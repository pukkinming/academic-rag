version: '3.8'

services:
  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: er_rag_qdrant
    ports:
      - "6333:6333"
      - "6334:6334"  # gRPC port
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    networks:
      - er_rag_network

  # RAG API service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: er_rag_api
    ports:
      - "8000:8000"
    environment:
      # Qdrant configuration
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION_NAME=emotion_recognition_papers
      
      # Embedding model
      # - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      
      # LLM configuration (override in .env file)
      # - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      # - LLM_MODEL=${LLM_MODEL:-gpt-4-turbo-preview}
      # - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.1}
      # - LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-2000}
      
      # OpenAI configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      
      # vLLM configuration
      # - VLLM_BASE_URL=${VLLM_BASE_URL:-http://host.docker.internal:8000/v1}
      - VLLM_BASE_URL=${VLLM_BASE_URL:-http://vllm:8000/v1}
      - VLLM_API_KEY=${VLLM_API_KEY:-EMPTY}
      
      # Chunking parameters
      - CHUNK_SIZE=${CHUNK_SIZE:-1000}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-200}
      
      # Retrieval parameters
      - DEFAULT_TOP_K=${DEFAULT_TOP_K:-8}
    volumes:
      # Mount for ingestion scripts (optional, for development)
      - ./extracted_data:/app/data:ro
    depends_on:
      - qdrant
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - er_rag_network

  # Optional: vLLM for local LLM (uncomment to use)
  # Requires NVIDIA GPU and nvidia-docker
  vllm:
    image: vllm/vllm-openai:latest
    container_name: er_rag_vllm
    runtime: nvidia
    ports:
      - "8001:8000"  # Using 8001 on host to avoid conflict with API
    volumes:
      - vllm_cache:/root/.cache/huggingface
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    command: >
      --model mistralai/Mistral-7B-Instruct-v0.2
      --gpu-memory-utilization 0.9
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - er_rag_network

volumes:
  qdrant_storage:
    driver: local
  vllm_cache:
    driver: local

networks:
  er_rag_network:
    driver: bridge


