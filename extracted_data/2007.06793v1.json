{
  "paper_id": "2007.06793v1",
  "title": "Tcgm: An Information-Theoretic Framework For Semi-Supervised Multi-Modality Learning",
  "published": "2020-07-14T03:32:03Z",
  "authors": [
    "Xinwei Sun",
    "Yilun Xu",
    "Peng Cao",
    "Yuqing Kong",
    "Lingjing Hu",
    "Shanghang Zhang",
    "Yizhou Wang"
  ],
  "keywords": [
    "Total Correlation",
    "Semi-supervised",
    "Multi-modality",
    "Conditional Independence",
    "Information intersection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Fusing data from multiple modalities provides more information to train machine learning systems. However, it is prohibitively expensive and timeconsuming to label each modality with a large amount of data, which leads to a crucial problem of semi-supervised multi-modal learning. Existing methods suffer from either ineffective fusion across modalities or lack of theoretical guarantees under proper assumptions. In this paper, we propose a novel information-theoretic approach -namely, Total Correlation Gain Maximization (TCGM) -for semisupervised multi-modal learning, which is endowed with promising properties: (i) it can utilize effectively the information across different modalities of unlabeled data points to facilitate training classifiers of each modality (ii) it has theoretical guarantee to identify Bayesian classifiers, i.e., the ground truth posteriors of all modalities. Specifically, by maximizing TC-induced loss (namely TC gain) over classifiers of all modalities, these classifiers can cooperatively discover the equivalent class of ground-truth classifiers; and identify the unique ones by leveraging limited percentage of labeled data. We apply our method to various tasks and achieve state-of-the-art results, including the news classification (Newsgroup dataset), emotion recognition (IEMOCAP and MOSI datasets), and disease prediction (Alzheimer's Disease Neuroimaging Initiative dataset).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Learning with data from multiple modalities has the advantage to facilitate information fusion from different perspectives and induce more robust models, compared with only using a single modality. For example, as shown in Figure  1 , to diagnose whether a patient has a certain disease or not, we can consult to its X-ray images, look into its medical records, or get results from clinical pathology. However, in many real applications, especially in some difficult ones (e.g. medical diagnosis), annotating such large-scale training data is prohibitively expensive and time-consuming. As a consequence, each modality of data may only contain a small proportion of labeled data from professional annotators, leaving a large proportion of unlabeled. This leads to an essential and challenging problem of semi-supervised multi-modality learning: how to effectively train accurate classifiers by aggregating unlabeled data of all modalities?\n\nTo achieve this goal, many methods have been proposed in the literature, which can be roughly categorized into two branches: (i) co-training strategy  [6] ; and (ii) learning joint representation across modalities in an unsupervised way  [26, 28] . These methods suffer from either too strong assumptions or loss of information during fusing. Specifically, the co-training strategy relies largely on the \"compatible\" assumption that the conditional distributions of the data point labels in each modality are the same, which may not be satisfied in the real settings, as self-claimed in  [6] ; while the latter branch of methods fails to capture the higher-order dependency among modalities, hence may end up in learning a trivial solution that maps all the data points to the same representation. A common belief in multi-modality learning  [22, 6, 13, 20]  is that conditioning on ground truth label Y , these modalities are conditionally independent, as illustrated in Figure  1 . For example, to diagnose if one suffers from a certain disease, an efficient way is to leverage as many as modalities that are related to the disease, e.g., X-ray image, medical records and the clinical pathology. Since each modality captures the characteristics of the disease from different aspects, the information extracted from these modalities, in addition to the label, are not necessarily correlated with each other. This suggests that the ground truth label can be regarded as the \"information intersection\" across all the modalities, i.e., the amount of agreement shared by all the modalities.\n\nInspired by such an assumption and the fact that the Total Correlation  [29]  can measure the amount of information shared by M (M ≥ 2) variables, in this paper, we propose Total Correlation Gain (TCG), which is a function of classifiers of all the modalities, as a surrogate goal for maximization of mutual information, in order to infer the ground-truth labels (i.e., information intersection among these modalities). Based on the proposed TCG, we devise an information-theoretic framework called Total Correlation Gain Maximization (TCGM) for semi-supervised multi-modal learning. By maximizing TCG among all the modalities, the classifiers for different modalities cooperatively discover the information intersection across all the modalities. It can be proved that the optimal classifiers for such a Total Correlation Gain are equivalent to the Bayesian posterior classifiers given each modality under some permutation function. With further leverage of labeled data, we can identify the Bayesian posterior classifiers. Furthermore, we devise an aggregator that employs all the modalities to forecast the labels of data. A simulated experiment is conducted to verify this theoretical result.\n\nWe apply TCGM on various tasks: (i) News classification with three pre-processing steps as different modalities, (ii) Emotion recognition with videos, audios, and texts as three modalities and (iii) disease prediction on medical imaging with the Structural magnetic resonance imaging (sMRI) and Positron emission tomography (PET) modalities. On these tasks, our method consistently outperforms the baseline methods especially when a limited percentage of labeled data are provided. To validate the benefit of jointly learning, we visualize that some cases of Alzheimer's Disease whose label are difficult to be predicted via supervised learning with single modality; while our jointly learned single modal classifier is able to correctly classify such hard samples.\n\nThe contributions can be summarized as follows: (i) We propose a novel informationtheoretic approach TCGM for semi-supervised multi-modality learning, which can effectively utilize information across all modalities. By maximizing the total correlation gain among all the modalities, the classifiers for different modalities cooperatively discover the information intersection across all the modalities -the ground truth. (ii) To the best of our knowledge, TCGM is the first in the literature that can be theoretically proved that, under the conditional independence assumption, it can identify the groundtruth Bayesian classifier given each modality. Further, by aggregating these classifiers, our method can learn the Bayesian classifier given all modalities. (iii) We achieve the state-of-the-art results on various semi-supervised multi-modality tasks including news classification, emotion recognition and disease prediction of medical imaging.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Semi-supervised multi-modal learning It is commonly believed in the literature that information of label is shared across all modalities. Existing work, which can be roughly categorized into two branches, suffers from either stronger but not reasonable assumptions or failure to capture the information (i.e., label) shared by all modalities. The first branch applies the co-training algorithm proposed by Blum et. al  [6] .  [17, 3, 12, 21, 11]  use weak classifiers trained by the labeled data from each modality to bootstrap each other by generating labels for the unlabeled data. However, the underlying compatible condition of such a method, which assumes the same conditional distributions for data point labels in each modality, may not be consistent with the real settings.\n\nThe second branch of work  [26, 28, 9, 31, 10, 18]  centers on learning joint representations that project unimodal representations all together into a multi-modal space in an unsupervised way and then using the labeled data from each modality to train a classifier to predict the label of the learned joint representation. A representative of such a framework is the soft-Hirschfeld-Gebelein-Rényi (HGR) framework  [31] , which proposed to maximize the correlation among non-linear representations of each modality. However, HGR only measures the linear dependence between pair modalities, since it follows the principle of maximizing the correlation between features of different modalities. In contrast, our framework, i.e., Total Correlation Gain Maximization can pursue information about higher-order dependence. Due to the above reasons, both branches can not avoid learning a naive solution that classifies all data points into the same class.\n\nTo overcome these limitations, we propose an information-theoretic loss function based on Total Correlation which can not only require the assumption in the first branch of work but also can be able to identify the ground-truth label, which is the information intersection among these modalities. Therefore, our method can avoid the trivial solution and can learn the optimal, i.e., the Bayesian Posterior classifiers of each modality.\n\nTotal Correlation/Mutual information maximization Total Correlation  [29] , as an extension of Mutual Information, measures the amount of information shared by M (M ≥ 2) variables. There are several works in the literature that have combined Mutual Information (M = 2) with deep learning algorithms and have shown superior performance on various tasks. Belghazi et allet@tokeeonedot  [4]  presents a mutual information neural estimator, which are utilized in a handful of applications based on the mutual information maximization (e.g., unsupervised learning of representations  [14] , learning node representations within graph-structured data  [30] ). Kong and Schoenebeck  [19]  provide another mutual information estimator in the co-training framework for the peer prediction mechanism, which has been combined with deep neural networks for crowdsourcing  [8] . Xu et.al  [32]  proposes an alternative definition of information, which is more effective for structure learning. However, those three estimators can only be applied to two-view settings. To the best of our knowledge, there are no similar studies that focus on a general number of modalities, which is very often in real applications. In this paper, we propose to leverage Total Correlation to fill in such a gap.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Preliminaries",
      "text": "Notations Given a random variable X, X denotes its realization space and x ∈ X denotes an instance. The P X denotes the probability distribution function over X and p(x) ∶= dP X (x) denotes the density function w.r.t the Lebesgue measure. Further, given a finite set X , ∆ X denotes the set of all distributions over X . For every integer M , [M ] denotes the set {1, 2, . . . , M }. For a vector v, v i denotes its i-th element.\n\nTotal Correlation The Total Correlation (TC), as an extension of mutual information, measures the \"amount of information\" shared by M (≥ 2) random variables:\n\nwhere H is the Shannon entropy. As defined, the TC degenerates to mutual information when M = 2. The ∑ M i=1 H(X i ) measures the total amount of information when treating X 1 , ..., X M independently; while the H(X 1 , ..., X M ) measures the counterpart when treating these M variables as a whole. Therefore, the difference between them implies the redundant information, i.e., the information shared by these M variables.\n\nSimilar to mutual information, the TC is equivalent to the Kullback-Leibler (KL)divergence between P X 1 ×...×X M and product of marginal distribution ⊗ M m=1 P X m :\n\nwhere D KL (P Q) = E P log dP dQ . Intuitively, larger KL divergence between joint and marginal distribution indicates more dependence among these M variables. To better characterize such a property, we give a formal definition of \"Point-wise Total Correlation\" (PTC):\n\nDefinition 1 (Point-wise Total Correlation). Given M random variables X 1 , ..., X M , the Point-wise Total Correlation on (x 1 , ..., x m ) ∈ X 1 × ... × X M , i.e., PTC(x 1 , ..., x M ) is defined as:\n\n) is denoted as the joint-margin ratio.\n\nRemark 1. The Point-wise Total Correlation can be understood as the point-wise distance between joint distribution and the marginal distribution. In more details, as noted from  [15] , by applying first-order Taylor-expansion, we have log p(x) q(x) ≈ log 1\n\n. Therefore, the expected value of PTC(⋅) can well measure the amount of information shared among these variables, which will be shown later in detailed.\n\nFor simplicity, we denote p\n\nAccording to dual representation in  [27] , we have the following lower bound for KL divergence between p and q, and hence TC.\n\nLemma 1 (Dual version of f -divergence  [27] ).\n\nwhere G is the set of functions that maps X 1 × X 2 × ⋅ ⋅ ⋅ × X M to R. The equality holds if and only if g(x 1 , x 2 , . . . , x M ) = 1 + PTC(x 1 , ..., x M ), and the supremum is\n\nThe Lemma 1 is commonly utilized for estimation of Mutual information  [4]  or optimization as variational lower bound in the machine learning literature. Besides, it also informs that the PTC is the optimal function to describe the amount of information shared by these M variables. Indeed, such shared information is the information intersection among these variables, i.e., conditional on such information, these M variables are independent of each other. To quantitatively describe this, we first introduce the conditional total correlation (CTC). Similar to TC, CTC measures the amount of information shared by these M variables conditioning on some variable Z:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "Problem statement In the semi-supervised muli-modal learning scenario, we have access to an unlabeled dataset D u = {x\n\nwhere X m denotes the domain of the m-th modality. Datapoints and labels in D l are i.i.d. samples drawn from the joint distribution\n\nDenote the prior of the ground truth labels by p * = (Pr(Y = c)) c . Upon the labeled and unlabeled datasets, our goal is to train M classifiers h [M ] ∶= {h 1 , h 2 , . . . , h M } and an aggregator ζ such that ∀m, h m ∶ X m → ∆ C predicts the ground truth y based on a m-th modality x m and ζ ∶ X 1 ×X 2 ×⋯×X M → ∆ C predicts the ground truth y based on all of the modalities x [M ] .\n\nOutline We will first introduce the assumptions regarding the ground truth label Y and prior distribution on (X 1 , ..., X M , Y ) in section 4.1. In section 4.2, we will present our method, i.e., maximize the total correlation gain on unlabeled dataset D u . Finally, we will introduce our algorithm for optimization in section 4.3.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Assumptions For Identification Of Y",
      "text": "In this section, we first introduce two basic assumptions to ensure that the ground-truth label can be identified. According to Proposition 2.1 in  [1] , the label Y can be viewed as the generating factor of data X. Such a result can be extended to multiple variables (please refer supplementary for details), which implies that Y is the common generating factor of X 1 , ..., X M . Motivated by this, it is natural to assume that the ground truth label Y is the \"information intersection\" among X 1 , ..., X M , i.e., all of the modalities are independent conditioning on the ground-truth:\n\nOn the basis of this assumption, one can immediately get the conditional total correlation gain CTC(X 1 , ..., X M Y ) = 0. In other words, conditioning on Y , there is no extra information shared by these M modalities, which is commonly assumed in the literature of semi-supervised learning  [22, 6, 13, 20] . However, the Y may not be the unique information intersection among these M modalities. Specifically, the following lemma establishes the rules for such information intersection to hold:\n\nFurther, the optimal g in lemma 1 satisfies g(x 1 , ...\n\nIn other words, in addition to {Pr(Y = c X m = x m )} m , p * c , there are other solutions {a x 1 , ..., a x M }, r with a x i ∈ ∆ C (for i ∈ C) and r ∈ ∆ C that can make the g optimal, as long as its joint-marginal ratio is equal to the ground-truth one:\n\nTo make {Pr(Y = c X m = x m )} m , p * c identifiable w.r.t a trivial permutation, we make the following trivial assumption on Pr(X 1 , ..., X M , Y ).\n\nAssumption 2 (Well-defined Prior). The solutions {a x 1 , ..., a x M }, r a and {b x 1 , ..., b x M }, r b for Eq. (4) are equivalent under the permutation ∏ ∶ C → C:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Total Correlation Gain Maximization (Tcgm)",
      "text": "Assumption 1 indicates that the label Y is the generating factor of all modalities, and assumption 2 further ensures its uniqueness under permutation. Our goal is to learn the ground-truth label Y which is the information intersection among M modalities. In this section, we propose a novel framework, namely Total Correlation Gain Maximization (TCGM) to capture such an information intersection, which is illustrated in Figure . 2. To the best of our knowledge, we are the first to theoretically prove the identification of ground truth classifiers on semi-supervised multi-modality data, by generalizing  [19, 8]  that can only handle two views in multi-view scenario. The high-level spirit is designing TC-induced loss over classifiers of every modality. By maximizing such a loss, these classifiers can converge to Bayesian posterior, which is the optimal solution of TC as expectation of the loss. First, we introduce the basic building blocks for our method.\n\nClassifiers h [M ] In order to leverage the powerful representation ability of deep neural network (DNN), each classifier h m (x m ; Θ m ) is modeled by a DNN with parameters Θ m . For each modality m, we denote the set of all such classifiers by H m and H [M ] ∶= {H 1 , H 2 , . . . , H M }.\n\nModality Classifiers-Aggregator ζ Given M classifiers for each modality h [M ] and a distribution p = (p c ) c ∈ ∆ C , the aggregator ζ which predicts the ground-truth label by aggregating classifiers of all modalities, is constructed by\n\nthat measures the \"amount of agreement\" among these classifiers. Note that the desired classifiers should satisfy Eq. (4).\n\nInspired by Lemma 1, we can take the empirical total correlation gain of N samples, i.e., the lower bound of Total Correlation as our maximization function. Specifically, given a reward function R, the empirical total correlation with respect to classifiers h [M ] , a prior p ∈ ∆ C measures the empirical \"amount of agreement\" for these M classifiers at the same sample (x 1 i , ..., x M i ) ∈ D u , with additionally a punishment of them on different samples:\n\nfor simplicity we denote T Cg[R]({x\n\n, p) as T Cg (N ) . Intuitively, we expect our classifiers to be consistent on the same sample; on the other hand, to disagree on different samples to avoid learning a trivial solution that classifies all data points into the same class.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Definition 3 (Bayesian Posterior Classifiers/Aggregator). The H",
      "text": ").\n\nNote that from Eq. (  5 ) that our maximization goal, i.e., T Cg (N ) relies on the form of reward function R. The following Lemma tells us the form of optimal reward function, with which we can finally give an explicit form of T Cg (N ) . Lemma 3. The R * that maximizes the expectation of T Cg (N ) can be represented as the Point-wise Total Correlation function, which is the function of Bayesian classifiers and the prior of ground truth labels (p * c ) c :\n\nTotal Correlation Gain Bring R * to Eq. (  5 ), we have:\n\nAs inspired by Lemma 3, we have that these Bayesian posterior classifiers are maximizers of the expected total correlation gain. Therefore, we can identify the equivalent class of Bayesian posteriors by minimizing -T Cg (N ) on unlabeled dataset D u . By additionally minimize expected cross entropy (CE) loss on D l , we can identify the unique Bayesian classifiers since they are respectively the minimizers of CE loss.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Theorem 3 (Main Theorem). Define The Expected Total Correlation Gain Et Cg(H",
      "text": "Given the conditional independence assumption 1 and well-defined prior assumption 2, we have that the maximum value of eT Cg is Total Correlation of M modalities, i.e., TC(X 1 , ..., X M ). Besides, Ground-truth → Maximizer (h\n\nMaximizer → (Permuted) Ground-truth If the prior is well defined, then for any maximizer of eT Cg, ( h[M] , p), there is a permutation ∏ ∶ C → C such that:\n\nThe proof is in Appendix A. Note from our main theorem that by maximizing the eT Cg, we can get the total correlation of M modalities, which is the ground-truth label Y , and also the equivalent class of Bayesian posterior classifier under permutation function. In order to identify the Bayesian posterior classifiers, we can further minimize cross-entropy loss on labeled data D l since the Bayesian posterior classifiers are the only minimizers of the expected cross-entropy loss. On the other hand, compared with only using D l to train classifiers, our method can leverage more information from D u , which can be shown in the experimental result later.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Optimization",
      "text": "Since eT Cg is intractable, we alternatively maximize the empirical total correlation gain, i.e., T Cg (N ) to learn the optimal classifiers. To identify the unique Bayesian posteriors, we should further utilize labeled dataset D l in a supervised way. Our whole optimization process is shown in Appendix, which adopts iteratively optimization strategy that is roughly contains two steps in each round: (i) We train the M classifiers using the classic cross entropy loss on the labeled dataset D l and (ii) using our information-theoretic loss function L TC on the unlabeled dataset D u . To learn the Bayesian posterior classifiers more accurately, the (ii) can help to learn the equivalent class of Bayesian Posterior Classifiers and (i) is to learn the correct and unique classifiers. As shown in Figure  2 , by optimizing L (B) TC (Eq. (  5 ) with B denoting the number of samples in each batch), we reward the M classifiers for their agreements on the same data point and punish the M classifiers for their agreements on different data points.\n\nLoss function L CE for labeled data We use the classic cross entropy loss for labeled data. Formally, for a batch of data points {x\n\ni=1 drawn from labeled data D l , the cross entropy loss L CE for each classifier h m is defined as L CE ({(x",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Loss Function L (B)",
      "text": "TC for unlabeled data For a batch of data points {x\n\nTC ∶= -T Cg (B) that is defined in Eq. (  6 ) with N replaced by number of batch size B. When N is large, we only sample a fixed number of samples from product of marginal distribution to estimate the second term in Eq. (  6 ), which makes training more amenable.\n\nPrediction After optimization, we can get the classifiers {h m } m . The prior p c can be estimated from data, i.e., p c =\n\n. Then based on Eq. (  7 ), we can get the aggregator classifier ζ for prediction. Specifically, given a new sample x[M] , the predicted label is ỹ ∶= arg max c ζ(x [M ] ) c .  We first conduct a simulated experiment on synthetic data to validate our theoretical result of TCGM. Specifically, we will show the effectiveness of Total Correlation Gain T Cg for unsupervised clustering of data. Further, with few labeled data, our TCGM can give accurate classification.\n\nIn more detail, we synthesize the data of three modalities from a specific Gaussian distribution P (X i y) (i = 1, 2, 3). The clustering accuracy is calculated as classification accuracy by assuming the label is known. As shown in Figure  3 , our method TCGM has competitive performance compared to well established clustering algorithms K-means++  [2]  and spectral clustering  [25] . Based on the promising unsupervised learning result, as shown by the light blue line (the top line) in Figure  3 , our method can accurately classify the data even with only a small portion of labeled data. In contrast, HGR  [31]  degrades since it can only capture the linear dependence and fail when higher-order dependency exists.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Applications",
      "text": "In this section, we evaluate our method on various multi-modal classification tasks: (i) News classification (Newsgroup) (ii) Emotion recognition: IEMOCAP, MOSI and (iii) Disease prediction of Alzheimer's Disease on 3D medical Imaging: Alzheimer's Disease Neuroimaging Initiative (ADNI). Our method TCGM is compared with : CE separately trains classifiers for each modality by minimizing cross entropy loss of only labeled data; HGR  [31]  learns representation by maximizing correlation of different modalities; and LMF  [23]  performs multimodal fusion using low-rank tensors. The optimal hyperparameters are selected according to validation accuracy, among which the learning rate is optimized from {0.1, 0.01, 0.001, 0.0001}. All experiments are repeated five times with different random seeds. The mean test accuracies and standard deviations of single classifiers the aggregator (ζ) are reported.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "News Classification",
      "text": "Dataset Newsgroup  [5]   8  is a group of news classification datasets. Following  [16] , each data point has three modalities, PAM, SMI and UMI, collected from three different preprocessing steps  9  . We evaluate TCGM and the baseline methods on 3 datasets from Newsgroup: News-M2, News-M5, News-M10. They contain 500, 500, 1000 data points with 2, 5, 10 categories respectively. Following  [33] , we use 60% for training, 20% for validation and 20% for testing for all of these three datasets.\n\nImplementation details We synthesize two different label rates (the percentage of labeled data points in each modality): {10%, 30%} for each dataset. We follow  [33]  for classifiers. Adam with default parameters and learning rate γ u = 0.0001, γ l = 0.01 is used as the optimizer during training. Batch size is set to 32. We further compare with two additional baselines: VAT  [24]  uses adversarial training for semi-supervised learning; PVCC  [33]  that considers the consistency of data points under different modalities. As shown in Fig.  4 , TCGM achieves the best classification accuracy for both single classifier and aggregators, especially when the label rate is small. This shows the efficacy of utilizing the cross-modal information during training as compared to others that are unable to utilize the cross-modal information. Moreover, we can achieve further improvement by aggregating classifiers on all modalities, which shows the benefit of aggregating knowledge from different modalities.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Dataset We evaluate our methods on two multi-modal emotion recognition datasets: IEMOCAP dataset  [7]  and MOSI dataset  [34] . The goal for both datasets is to identify speaker emotions based on the collected videos, audios and texts. The IEMOCAP consists of 151 sessions of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. The MOSI is composed of 93 opinion videos from YouTube movie reviews. We follow the settings in  [23]  for the data splits of training, validation and test set. For IEMOCAP, we conduct experiments on three different emotions: happy, angry and neutral emotions; for MOSI dataset we consider the binary classification of emotions: positive and negative.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Implementation Details",
      "text": "We synthesize three label rates for each dataset (the percentage of labeled data points in each modality): {0.5%, 1%, 1.5%} for IEMOCAP and {1%, 2%, 3%} for MOSI. For a fair comparison, we follow architecture setting in  [23] . We adopt the modality encoder architectures in  [23]  as the single modality classifiers for CE and TCGM, while adopting the aggregator on the top of modality encoders for LMF and HGR. Adam with default parameters and learning rate γ u = 0.0001, γ l = 0.001 is used as the optimizer. The batch size is set to 32.\n\nWe report the AUC (Area under ROC curve) for the aggregators on all the modalities and single modality classifiers by different methods. We only report the AUC of LMF and HGR on all modalities since they do not have single modality classifiers. For single modality classifiers, we show results on the text modality on happy emotion (d), audio modality on neutral emotion (e) the video modality on angry emotion on IEMOCAP; and (h) the video modality (i) the audio modality on MOSI. Please refer to supplementary material for complete experimental result. As shown in Figure  5 , aggregators trained by TCGM outperform all the baselines given only tiny fractions of labeled data. TCGM improves the AUC of the single modality classifiers significantly, which shows the efficacy of utilizing the cross-modal information during the training of our method. As label rates continue to grow, the advantage of our method over CE decreases since more information is provided for CE to learn the ground-truth label.\n\nOur method also outperforms other methods in terms of the prediction based on all the modalities, especially when the label rate is small. This shows the superiority of our method when dealing with a limited amount of annotations.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Disease Prediction Of Alzheimer'S Disease",
      "text": "Dataset Early prediction of Alzheimer's Disease (AD) is attracting increasing attention since it is irreversible and very challenging. Besides, due to privacy issues and high collecting costs, an efficient classifier with limited labeled data is desired. To validate the effectiveness of our method on this challenging task, we only keep labels of a limited percentage of data, which is obtained from the most popular Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset 10 , with 3D images sMRI and PET. DARTEL VBM pipeline  [2]  is implemented to pre-process the sMRI data, and then images of PET were reoriented into a standard 91 × 109 × 91 voxel image grid in MNI152 space, which is same with sMRIs'. To limit the size of images, only the hippocampus on both sides are extracted as input in the experiments. We denote subjects that convert to Alzheimer's disease (MCI c ) as AD, and subjects that remain stable (MCI s ) as NC (Normal Control). Our dataset contains 300 samples in total, with 144 AD and 156 NC. We randomly choose 60% for training, 20% for validation and 20% for testing stage. Implementation details We synthesize two different label rates (the percentage of labeled data points): {10%, 50%}. DenseNet is used as the classifier. Two 3D convolutional layers with the kernel size 3 × 3 × 3 are adopted to replace the first 2D convolutional layers with the kernel size 7 × 7. We use four dense blocks with the size of (6, 12, 24, 16).\n\nTo preserve more low-level local information, we discard the first max-pooling layer that follows after the first convolution layer. Adam with default parameters and learning rate γ u = γ l = 0.001 are used as the optimizer during training. We set Batch Size as only 12 due to the large memory usage of 3D images. Random crop of 64 × 64 × 64, random flip and random transpose are applied as data augmentation.\n\nFigure  6  shows the accuracy of classifiers for each modality and the aggregator. Our method TCGM outperforms the baseline methods in all settings especially when the label rate is small, which is desired since it is costly to label data. To further illustrate the advantage of our model over others in terms of leveraging the knowledge of another modality, we visualize two MCI c s, denoted as MCI 1 c and MCI 2 c , which are mistakenly classified as NC by CE's classifier for sMRI and PET modality, respectively. The volume and standardized uptake value (SUV) (a measurement of the degree of metabolism), whose information are respectively contained by sMRI and PET data, are linearly mapped to the darkness of the red and blue. Darker color implies smaller volume and SUV, i.e., more probability of being AD. As shown in Figure  7 , the volume (SUV) of MCI 1 c (MCI 2 c ) is similar to NC, hence it is reasonable for CE to mistakenly classify it by only using the information of volume (SUV). In contrast, TCGM for each modality can correctly classify both cases as AD, which shows the better learning of the information intersection (i.e., the ground truth) during training, facilitated by the leverage of knowledge from another modality.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose an information-theoretic framework on multi-modal data, Total Correlation Gain Maximization (TCGM), in the scenario of semi-supervised learning. Specifically, we learn to infer the ground truth labels shared by all modalities by maximizing the total correlation gain. Conditioning on a common assumption that all modalities are independent given the ground truth label, it can be theoretically proved our method can learn the Bayesian posterior classifier for each modality and the Bayesian posterior aggregator for all modalities. Extensive experiments on Newsgroup, IEMOCAP, MOSI and ADNI datasets are conducted and achieve promising results, which demonstrates the benefit and utility of our framework.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , to diagnose whether a patient",
      "page": 1
    },
    {
      "caption": "Figure 1: Multiple modalities are independent conditioning on the ground truth; Ground truth is the",
      "page": 2
    },
    {
      "caption": "Figure 1: For example, to diagnose if one suffers from a certain disease, an efﬁcient",
      "page": 2
    },
    {
      "caption": "Figure 2: Empirical Total Correlation Gain TCg(N)",
      "page": 10
    },
    {
      "caption": "Figure 3: Clustering and classiﬁcation ac-",
      "page": 10
    },
    {
      "caption": "Figure 4: Test accuracies (mean ± std. dev. ) on Newsgroups datasets",
      "page": 11
    },
    {
      "caption": "Figure 4: , TCGM achieves the best classiﬁcation accuracy for both single",
      "page": 11
    },
    {
      "caption": "Figure 5: , aggregators trained by",
      "page": 12
    },
    {
      "caption": "Figure 5: (a,b,c) AUC of Aggregators on happy, angry and neutral emotion recognition on",
      "page": 13
    },
    {
      "caption": "Figure 6: shows the accuracy of classiﬁers for each modality and the aggregator. Our",
      "page": 13
    },
    {
      "caption": "Figure 6: Test accuracies (mean ± std.) on ADNI dataset",
      "page": 14
    },
    {
      "caption": "Figure 7: Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1",
      "page": 14
    },
    {
      "caption": "Figure 8: AUC of single modality classiﬁers by CE and TCGM.",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM: An Information-Theoretic Framework for": "Semi-Supervised Multi-Modality Learning"
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": "2 Center on Frontiers of Computing Studies, Peking University"
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": "caopeng2016,"
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": ""
        },
        {
          "TCGM: An Information-Theoretic Framework for": "Abstract. Fusing data from multiple modalities provides more information to"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "train machine learning systems. However, it is prohibitively expensive and time-"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "consuming to label each modality with a large amount of data, which leads to a"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "crucial problem of semi-supervised multi-modal learning. Existing methods suffer"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "from either ineffective fusion across modalities or lack of theoretical guarantees"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "under proper assumptions. In this paper, we propose a novel information-theoretic"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "approach - namely, Total Correlation Gain Maximization (TCGM) – for semi-"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "supervised multi-modal learning, which is endowed with promising properties: (i)"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "it can utilize effectively the information across different modalities of unlabeled"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "data points to facilitate training classiﬁers of each modality (ii) it has theoreti-"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "cal guarantee to identify Bayesian classiﬁers, i.e., the ground truth posteriors of"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "all modalities. Speciﬁcally, by maximizing TC-induced loss (namely TC gain)"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "over classiﬁers of all modalities, these classiﬁers can cooperatively discover the"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "equivalent class of ground-truth classiﬁers; and identify the unique ones by lever-"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "aging limited percentage of labeled data. We apply our method to various tasks"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "and achieve state-of-the-art results, including the news classiﬁcation (Newsgroup"
        },
        {
          "TCGM: An Information-Theoretic Framework for": "dataset), emotion recognition (IEMOCAP and MOSI datasets), and disease pre-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nSun et al.": "records, or get\nresults from clinical pathology. However,\nin many real applications,"
        },
        {
          "2\nSun et al.": "especially in some difﬁcult ones (e.g. medical diagnosis), annotating such large-scale"
        },
        {
          "2\nSun et al.": "training data is prohibitively expensive and time-consuming. As a consequence, each"
        },
        {
          "2\nSun et al.": "modality of data may only contain a small proportion of labeled data from professional"
        },
        {
          "2\nSun et al.": "annotators,\nleaving a large proportion of unlabeled. This leads to an essential and"
        },
        {
          "2\nSun et al.": "challenging problem of semi-supervised multi-modality learning: how to effectively train"
        },
        {
          "2\nSun et al.": "accurate classiﬁers by aggregating unlabeled data of all modalities?"
        },
        {
          "2\nSun et al.": "To achieve this goal, many methods have been proposed in the literature, which can be"
        },
        {
          "2\nSun et al.": "roughly categorized into two branches: (i) co-training strategy [6]; and (ii) learning joint"
        },
        {
          "2\nSun et al.": "representation across modalities in an unsupervised way [26,28]. These methods suffer"
        },
        {
          "2\nSun et al.": "from either too strong assumptions or loss of information during fusing. Speciﬁcally, the"
        },
        {
          "2\nSun et al.": "co-training strategy relies largely on the “compatible” assumption that the conditional"
        },
        {
          "2\nSun et al.": "distributions of the data point labels in each modality are the same, which may not be"
        },
        {
          "2\nSun et al.": "satisﬁed in the real settings, as self-claimed in [6]; while the latter branch of methods"
        },
        {
          "2\nSun et al.": "fails to capture the higher-order dependency among modalities, hence may end up in"
        },
        {
          "2\nSun et al.": "learning a trivial solution that maps all the data points to the same representation."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM\n3": "proved that the optimal classiﬁers for such a Total Correlation Gain are equivalent to"
        },
        {
          "TCGM\n3": "the Bayesian posterior classiﬁers given each modality under some permutation function."
        },
        {
          "TCGM\n3": "With further leverage of labeled data, we can identify the Bayesian posterior classiﬁers."
        },
        {
          "TCGM\n3": "Furthermore, we devise an aggregator that employs all\nthe modalities to forecast\nthe"
        },
        {
          "TCGM\n3": "labels of data. A simulated experiment is conducted to verify this theoretical result."
        },
        {
          "TCGM\n3": "We apply TCGM on various tasks: (i) News classiﬁcation with three pre-processing"
        },
        {
          "TCGM\n3": "steps as different modalities, (ii) Emotion recognition with videos, audios, and texts as"
        },
        {
          "TCGM\n3": "three modalities and (iii) disease prediction on medical imaging with the Structural mag-"
        },
        {
          "TCGM\n3": "netic resonance imaging (sMRI) and Positron emission tomography (PET) modalities."
        },
        {
          "TCGM\n3": "On these tasks, our method consistently outperforms the baseline methods especially"
        },
        {
          "TCGM\n3": "when a limited percentage of labeled data are provided. To validate the beneﬁt of jointly"
        },
        {
          "TCGM\n3": "learning, we visualize that some cases of Alzheimer’s Disease whose label are difﬁcult"
        },
        {
          "TCGM\n3": "to be predicted via supervised learning with single modality; while our jointly learned"
        },
        {
          "TCGM\n3": "single modal classiﬁer is able to correctly classify such hard samples."
        },
        {
          "TCGM\n3": "The contributions can be summarized as follows: (i) We propose a novel information-"
        },
        {
          "TCGM\n3": "theoretic approach TCGM for semi-supervised multi-modality learning, which can"
        },
        {
          "TCGM\n3": "effectively utilize information across all modalities. By maximizing the total correlation"
        },
        {
          "TCGM\n3": "gain among all\nthe modalities,\nthe classiﬁers for different modalities cooperatively"
        },
        {
          "TCGM\n3": "discover the information intersection across all the modalities - the ground truth. (ii) To"
        },
        {
          "TCGM\n3": "the best of our knowledge, TCGM is the ﬁrst in the literature that can be theoretically"
        },
        {
          "TCGM\n3": "proved that, under the conditional independence assumption, it can identify the ground-"
        },
        {
          "TCGM\n3": "truth Bayesian classiﬁer given each modality. Further, by aggregating these classiﬁers,"
        },
        {
          "TCGM\n3": "our method can learn the Bayesian classiﬁer given all modalities. (iii) We achieve the"
        },
        {
          "TCGM\n3": "state-of-the-art results on various semi-supervised multi-modality tasks including news"
        },
        {
          "TCGM\n3": "classiﬁcation, emotion recognition and disease prediction of medical imaging."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "",
          "Sun et al.": "To overcome these limitations, we propose an information-theoretic loss function"
        },
        {
          "4": "",
          "Sun et al.": "based on Total Correlation which can not only require the assumption in the ﬁrst branch"
        },
        {
          "4": "",
          "Sun et al.": "of work but also can be able to identify the ground-truth label, which is the information"
        },
        {
          "4": "",
          "Sun et al.": "intersection among these modalities. Therefore, our method can avoid the trivial solution"
        },
        {
          "4": "and can learn the optimal, i.e., the Bayesian Posterior classiﬁers of each modality.",
          "Sun et al.": ""
        },
        {
          "4": "",
          "Sun et al.": "Total Correlation/Mutual information maximization Total Correlation [29], as an exten-"
        },
        {
          "4": "",
          "Sun et al.": "sion of Mutual Information, measures the amount of information shared by M (M ≥ 2)"
        },
        {
          "4": "",
          "Sun et al.": "variables. There are several works in the literature that have combined Mutual Informa-"
        },
        {
          "4": "",
          "Sun et al.": "tion (M = 2) with deep learning algorithms and have shown superior performance on"
        },
        {
          "4": "",
          "Sun et al.": "various tasks. Belghazi et allet@tokeeonedot[4] presents a mutual information neural"
        },
        {
          "4": "",
          "Sun et al.": "estimator, which are utilized in a handful of applications based on the mutual informa-"
        },
        {
          "4": "",
          "Sun et al.": "tion maximization (e.g., unsupervised learning of representations [14], learning node"
        },
        {
          "4": "",
          "Sun et al.": "representations within graph-structured data [30]). Kong and Schoenebeck [19] provide"
        },
        {
          "4": "",
          "Sun et al.": "another mutual information estimator in the co-training framework for the peer prediction"
        },
        {
          "4": "",
          "Sun et al.": "mechanism, which has been combined with deep neural networks for crowdsourcing [8]."
        },
        {
          "4": "Xu et.al [32] proposes an alternative deﬁnition of information, which is more effective",
          "Sun et al.": ""
        },
        {
          "4": "",
          "Sun et al.": "for structure learning. However, those three estimators can only be applied to two-view"
        },
        {
          "4": "",
          "Sun et al.": "settings. To the best of our knowledge, there are no similar studies that focus on a general"
        },
        {
          "4": "",
          "Sun et al.": "number of modalities, which is very often in real applications. In this paper, we propose"
        },
        {
          "4": "to leverage Total Correlation to ﬁll in such a gap.",
          "Sun et al.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM": "",
          "5": ""
        },
        {
          "TCGM": "",
          "5": ""
        },
        {
          "TCGM": "",
          "5": ""
        },
        {
          "TCGM": "",
          "5": ""
        },
        {
          "TCGM": "",
          "5": ""
        },
        {
          "TCGM": "",
          "5": ""
        },
        {
          "TCGM": "",
          "5": ""
        },
        {
          "TCGM": "",
          "5": ""
        },
        {
          "TCGM": "",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nSun et al.": "4\nMethod"
        },
        {
          "6\nSun et al.": "Problem statement\nIn the semi-supervised muli-modal learning scenario, we have access"
        },
        {
          "6\nSun et al.": ", yi)}i. Each la-\nto an unlabeled dataset Du = {x[M ]\n}i and a labeled dataset Dl = {(x[M ]\ni\ni"
        },
        {
          "6\nSun et al.": "∶= {x1\n∣xm\nbel yi ∈ C, where C denotes the set of classes. Each datapoint x[M ]"
        },
        {
          "6\nSun et al.": "i , x2\ni , . . . , xM\ni\ni"
        },
        {
          "6\nSun et al.": "X m} consists of M modalities, where X m denotes the domain of\nthe m-th modal-"
        },
        {
          "6\nSun et al.": "ity. Datapoints and labels in Dl are i.i.d. samples drawn from the joint distribution"
        },
        {
          "6\nSun et al.": "UX [M ],Y (x1, x2, . . . , xM , y) ∶= Pr(X 1 = x1, X 2 = x2, . . . , X M = xM , Y = y). Data"
        },
        {
          "6\nSun et al.": "points in Du are i.i.d. samples drawn from joint distribution UX [M ](x1, x2, . . . , xM ) ∶="
        },
        {
          "6\nSun et al.": "∑c∈C UX [M ],Y (x1, x2, . . . , xM , y = c). Denote the prior of the ground truth labels by"
        },
        {
          "6\nSun et al.": "is to train M\np∗ = (Pr(Y = c))c. Upon the labeled and unlabeled datasets, our goal"
        },
        {
          "6\nSun et al.": "classiﬁers h[M ] ∶= {h1, h2, . . . , hM } and an aggregator ζ such that ∀m, hm ∶ X m → ∆C"
        },
        {
          "6\nSun et al.": "predicts the ground truth y based on a m-th modality xm and ζ ∶ X 1×X 2×⋯×X M → ∆C"
        },
        {
          "6\nSun et al.": "predicts the ground truth y based on all of the modalities x[M ]."
        },
        {
          "6\nSun et al.": "Outline We will ﬁrst introduce the assumptions regarding the ground truth label Y and"
        },
        {
          "6\nSun et al.": "prior distribution on (X 1, ..., X M , Y ) in section 4.1. In section 4.2, we will present our"
        },
        {
          "6\nSun et al.": "method, i.e., maximize the total correlation gain on unlabeled dataset Du. Finally, we"
        },
        {
          "6\nSun et al.": "will introduce our algorithm for optimization in section 4.3."
        },
        {
          "6\nSun et al.": "4.1\nAssumptions for Identiﬁcation of Y"
        },
        {
          "6\nSun et al.": "In this section, we ﬁrst introduce two basic assumptions to ensure that the ground-truth"
        },
        {
          "6\nSun et al.": "label can be identiﬁed. According to Proposition 2.1 in [1], the label Y can be viewed"
        },
        {
          "6\nSun et al.": "as the generating factor of data X. Such a result can be extended to multiple variables"
        },
        {
          "6\nSun et al.": "(please refer supplementary for details), which implies that Y is the common generating"
        },
        {
          "6\nSun et al.": "factor of X 1, ..., X M . Motivated by this,\nit\nis natural\nto assume that\nthe ground truth"
        },
        {
          "6\nSun et al.": "label Y is the \"information intersection\" among X 1, ..., X M , i.e., all of the modalities"
        },
        {
          "6\nSun et al.": "are independent conditioning on the ground-truth:"
        },
        {
          "6\nSun et al.": "Assumption 1 (Conditional Independence). Conditioning on Y , X 1, X 2, . . . , X M are"
        },
        {
          "6\nSun et al.": "independent, i.e., ∀x1, . . . , xM ,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM\n7": "In other words, in addition to {Pr(Y = c∣X m = xm)}m, p∗\nc , there are other solutions"
        },
        {
          "TCGM\n7": "{ax1, ..., axM }, r with axi ∈ ∆C (for i ∈ C) and r ∈ ∆C that can make the g optimal, as"
        },
        {
          "TCGM\n7": "long as its joint-marginal ratio is equal to the ground-truth one:"
        },
        {
          "TCGM\n7": "∏m axm"
        },
        {
          "TCGM\n7": "= R(x1, ..., xM )\n(4)"
        },
        {
          "TCGM\n7": "∑ c\n(ra\n∈C\nc )M −1"
        },
        {
          "TCGM\n7": "identiﬁable w.r.t a trivial permutation, we make\nTo make {Pr(Y = c∣X m = xm)}m, p∗"
        },
        {
          "TCGM\n7": "the following trivial assumption on Pr(X 1, ..., X M , Y )."
        },
        {
          "TCGM\n7": "Assumption 2 (Well-deﬁned Prior). The solutions {ax1, ..., axM }, ra and {bx1 , ..., bxM }, rb"
        },
        {
          "TCGM\n7": "for Eq. (4) are equivalent under the permutation ∏ ∶ C → C: axm = ∏ bxm , ra = ∏ rb."
        },
        {
          "TCGM\n7": "4.2\nTotal Correlation Gain Maximization (TCGM)"
        },
        {
          "TCGM\n7": "Assumption 1 indicates that the label Y is the generating factor of all modalities, and"
        },
        {
          "TCGM\n7": "assumption 2 further ensures its uniqueness under permutation. Our goal is to learn the"
        },
        {
          "TCGM\n7": "ground-truth label Y which is the information intersection among M modalities. In this"
        },
        {
          "TCGM\n7": "section, we propose a novel framework, namely Total Correlation Gain Maximization"
        },
        {
          "TCGM\n7": "(TCGM) to capture such an information intersection, which is illustrated in Figure. 2."
        },
        {
          "TCGM\n7": "To the best of our knowledge, we are the ﬁrst to theoretically prove the identiﬁcation of"
        },
        {
          "TCGM\n7": "ground truth classiﬁers on semi-supervised multi-modality data, by generalizing [19,8]"
        },
        {
          "TCGM\n7": "that can only handle two views in multi-view scenario. The high-level spirit is designing"
        },
        {
          "TCGM\n7": "TC-induced loss over classiﬁers of every modality. By maximizing such a loss, these"
        },
        {
          "TCGM\n7": "classiﬁers can converge to Bayesian posterior, which is the optimal solution of TC as"
        },
        {
          "TCGM\n7": "expectation of the loss. First, we introduce the basic building blocks for our method."
        },
        {
          "TCGM\n7": "Classiﬁers h[M ]\nIn order to leverage the powerful representation ability of deep neural"
        },
        {
          "TCGM\n7": "network (DNN), each classiﬁer hm(xm; Θm) is modeled by a DNN with parameters"
        },
        {
          "TCGM\n7": "Θm. For each modality m, we denote the set of all such classiﬁers by H m and H [M ] ∶="
        },
        {
          "TCGM\n7": "{H 1, H 2, . . . , H M }."
        },
        {
          "TCGM\n7": "Modality Classiﬁers-Aggregator ζ Given M classiﬁers for each modality h[M ] and a"
        },
        {
          "TCGM\n7": "distribution p = (pc)c ∈ ∆C, the aggregator ζ which predicts the ground-truth label by"
        },
        {
          "TCGM\n7": "aggregating classiﬁers of all modalities, is constructed by"
        },
        {
          "TCGM\n7": ")c\n∏m hm(xm"
        },
        {
          "TCGM\n7": ")\n)\nζ(x[M ]; h[M ], p) = Normalize (("
        },
        {
          "TCGM\n7": "(pc)M −1"
        },
        {
          "TCGM\n7": "c"
        },
        {
          "TCGM\n7": "v"
        },
        {
          "TCGM\n7": "where Normalize(v) ∶=\nfor all v ∈ ∆C."
        },
        {
          "TCGM\n7": "∑c vc"
        },
        {
          "TCGM\n7": "M"
        },
        {
          "TCGM\n7": "‡„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„•„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„(cid:181)"
        },
        {
          "TCGM\n7": "Reward Function R We deﬁne a reward function R ∶\n∆C × ... × ∆C → R that measures"
        },
        {
          "TCGM\n7": "the \"amount of agreement\" among these classiﬁers. Note that\nthe desired classiﬁers"
        },
        {
          "TCGM\n7": "should satisfy Eq. (4)."
        },
        {
          "TCGM\n7": "Inspired by Lemma 1, we can take the empirical total correlation gain of N samples,"
        },
        {
          "TCGM\n7": "i.e.,\nthe lower bound of Total Correlation as our maximization function. Speciﬁcally,"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nSun et al.": "given a reward function R, the empirical total correlation with respect to classiﬁers h[M ],"
        },
        {
          "8\nSun et al.": "a prior p ∈ ∆C measures the empirical \"amount of agreement\" for these M classiﬁers at"
        },
        {
          "8\nSun et al.": "the same sample (x1\n) ∈ Du, with additionally a punishment of them on different\ni , ..., xM"
        },
        {
          "8\nSun et al.": "∈ X 1, ..., xM\nsamples: x1\n∈ X M with i1 ≠ i2 ≠ ... ≠ iM :"
        },
        {
          "8\nSun et al.": "i1\niM"
        },
        {
          "8\nSun et al.": "}N\nT Cg[R]({x[M ]"
        },
        {
          "8\nSun et al.": "1 N\n∑ i\n))\nR(h1(x1\ni ), ..., hM (xM\ni=1; h[M ], p) ∶=\ni"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM\n9": "Ground-truth → Maximizer\nIn other\n(h[M ]\n, p∗) is a maximizer of eT Cg(h[M ], p)."
        },
        {
          "TCGM\n9": "words, ∀h[M ]\n, p∗) ≥ eT Cg(h1, ..., hM , p). The\n∈ H [M ], p ∈ ∆C, eT Cg(h1\n∗, ..., hM"
        },
        {
          "TCGM\n9": "corresponding optimal aggregator is ζ⋆, i.e., ζ⋆(x[M ])c = Pr(Y = c∣X [M ] = x[M ])."
        },
        {
          "TCGM\n9": "Maximizer → (Permuted) Ground-truth\nIf the prior is well deﬁned, then for any maxi-"
        },
        {
          "TCGM\n9": "mizer of eT Cg, (˜h[M ], ˜p), there is a permutation ˜∏ ∶ C → C such that:"
        },
        {
          "TCGM\n9": "˜\n˜"
        },
        {
          "TCGM\n9": "(7)\n∏(c))\nhm(xm)c = P(Y =\n∏(c)∣X m = xm), ˜pc = P(Y ="
        },
        {
          "TCGM\n9": "The proof is in Appendix A. Note from our main theorem that by maximizing the"
        },
        {
          "TCGM\n9": "eT Cg, we can get\nthe total correlation of M modalities, which is the ground-truth"
        },
        {
          "TCGM\n9": "label Y , and also the equivalent class of Bayesian posterior classiﬁer under permutation"
        },
        {
          "TCGM\n9": "function. In order to identify the Bayesian posterior classiﬁers, we can further minimize"
        },
        {
          "TCGM\n9": "cross-entropy loss on labeled data Dl since the Bayesian posterior classiﬁers are the only"
        },
        {
          "TCGM\n9": "minimizers of the expected cross-entropy loss. On the other hand, compared with only"
        },
        {
          "TCGM\n9": "using Dl\nto train classiﬁers, our method can leverage more information from Du, which"
        },
        {
          "TCGM\n9": "can be shown in the experimental result later."
        },
        {
          "TCGM\n9": "4.3\nOptimization"
        },
        {
          "TCGM\n9": "Since eT Cg is intractable, we alternatively maximize the empirical total correlation gain,"
        },
        {
          "TCGM\n9": "i.e., T Cg(N ) to learn the optimal classiﬁers. To identify the unique Bayesian posteriors,"
        },
        {
          "TCGM\n9": "in a supervised way. Our whole optimization\nwe should further utilize labeled dataset Dl"
        },
        {
          "TCGM\n9": "process is shown in Appendix, which adopts iteratively optimization strategy that\nis"
        },
        {
          "TCGM\n9": "roughly contains two steps in each round: (i) We train the M classiﬁers using the classic"
        },
        {
          "TCGM\n9": "cross entropy loss on the labeled dataset Dl and (ii) using our information-theoretic loss"
        },
        {
          "TCGM\n9": "function LTC on the unlabeled dataset Du. To learn the Bayesian posterior classiﬁers"
        },
        {
          "TCGM\n9": "more accurately,\nthe (ii) can help to learn the equivalent class of Bayesian Posterior"
        },
        {
          "TCGM\n9": "Classiﬁers and (i) is to learn the correct and unique classiﬁers. As shown in Figure 2,"
        },
        {
          "TCGM\n9": "by optimizing L(B)\n(Eq. (5) with B denoting the number of samples in each batch), we"
        },
        {
          "TCGM\n9": "TC"
        },
        {
          "TCGM\n9": "reward the M classiﬁers for their agreements on the same data point and punish the M"
        },
        {
          "TCGM\n9": "classiﬁers for their agreements on different data points."
        },
        {
          "TCGM\n9": "Loss function LCE for labeled data We use the classic cross entropy loss for labeled"
        },
        {
          "TCGM\n9": "}B\ndata. Formally, for a batch of data points {x[M ]"
        },
        {
          "TCGM\n9": "the\ni=1 drawn from labeled data Dl,\ni"
        },
        {
          "TCGM\n9": ", yi)}B\ncross entropy loss LCE for each classiﬁer hm is deﬁned as LCE({(x[M ]\ni=1; hm) ∶="
        },
        {
          "TCGM\n9": "1B\n)yi).\n∑i − log(hm(xm"
        },
        {
          "TCGM\n9": "}B\nLoss function L(B)\nfor unlabeled data For a batch of data points {x[M ]"
        },
        {
          "TCGM\n9": "i=1 drawn from\ni\nTC"
        },
        {
          "TCGM\n9": "∶= −T Cg(B) that is deﬁned in Eq. (6) with N\nunlabeled data Du, our loss function L(B)"
        },
        {
          "TCGM\n9": "TC"
        },
        {
          "TCGM\n9": "replaced by number of batch size B. When N is large, we only sample a ﬁxed number"
        },
        {
          "TCGM\n9": "of samples from product of marginal distribution to estimate the second term in Eq. (6),"
        },
        {
          "TCGM\n9": "which makes training more amenable."
        },
        {
          "TCGM\n9": "Prediction After optimization, we can get\nthe classiﬁers {hm}m. The prior pc can"
        },
        {
          "TCGM\n9": "be estimated from data,\n. Then based on Eq.\n(7), we can get\ni.e., pc = ∑i∈[N ] 1(yi=c)"
        },
        {
          "TCGM\n9": "N"
        },
        {
          "TCGM\n9": "the aggregator classiﬁer ζ for prediction. Speciﬁcally, given a new sample ˜x[M ],\nthe"
        },
        {
          "TCGM\n9": "predicted label is ˜y ∶= arg maxc ζ(˜x[M ])c."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5\nPreliminary experiments": "We ﬁrst conduct a simulated experiment on syn-"
        },
        {
          "5\nPreliminary experiments": "thetic data to validate our\ntheoretical\nresult of"
        },
        {
          "5\nPreliminary experiments": "TCGM. Speciﬁcally, we will show the effective-"
        },
        {
          "5\nPreliminary experiments": "ness of Total Correlation Gain T Cg for unsuper-"
        },
        {
          "5\nPreliminary experiments": "vised clustering of data. Further, with few labeled"
        },
        {
          "5\nPreliminary experiments": ""
        },
        {
          "5\nPreliminary experiments": "data, our TCGM can give accurate classiﬁcation."
        },
        {
          "5\nPreliminary experiments": "In more detail, we synthesize the data of\nthree"
        },
        {
          "5\nPreliminary experiments": "modalities from a speciﬁc Gaussian distribution"
        },
        {
          "5\nPreliminary experiments": ""
        },
        {
          "5\nPreliminary experiments": "P (X i∣y) (i = 1, 2, 3). The clustering accuracy is"
        },
        {
          "5\nPreliminary experiments": ""
        },
        {
          "5\nPreliminary experiments": "calculated as classiﬁcation accuracy by assuming"
        },
        {
          "5\nPreliminary experiments": "the label\nis known. As\nshown in Figure 3, our"
        },
        {
          "5\nPreliminary experiments": ""
        },
        {
          "5\nPreliminary experiments": "method TCGM has competitive performance com-"
        },
        {
          "5\nPreliminary experiments": "pared to well established clustering algorithms K-"
        },
        {
          "5\nPreliminary experiments": ""
        },
        {
          "5\nPreliminary experiments": "means++ [2] and spectral clustering [25]. Based"
        },
        {
          "5\nPreliminary experiments": ""
        },
        {
          "5\nPreliminary experiments": "on the promising unsupervised learning result, as"
        },
        {
          "5\nPreliminary experiments": "shown by the light blue line (the top line) in Fig-"
        },
        {
          "5\nPreliminary experiments": "ure 3, our method can accurately classify the data"
        },
        {
          "5\nPreliminary experiments": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": "is a group of news classiﬁcation datasets. Following [16],"
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": "data points in each modality): {10%, 30%} for each dataset. We follow [33] for classiﬁers."
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        },
        {
          "TCGM\n11": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12\nSun et al.": "speaker emotions based on the collected videos, audios and texts. The IEMOCAP consists"
        },
        {
          "12\nSun et al.": "of 151 sessions of recorded dialogues, with 2 speakers per session for a total of 302"
        },
        {
          "12\nSun et al.": "videos across the dataset. The MOSI is composed of 93 opinion videos from YouTube"
        },
        {
          "12\nSun et al.": "movie reviews. We follow the settings in [23] for the data splits of training, validation"
        },
        {
          "12\nSun et al.": "and test set. For IEMOCAP, we conduct experiments on three different emotions: happy,"
        },
        {
          "12\nSun et al.": "angry and neutral emotions; for MOSI dataset we consider the binary classiﬁcation of"
        },
        {
          "12\nSun et al.": "emotions: positive and negative."
        },
        {
          "12\nSun et al.": "Implementation details We synthesize three label rates for each dataset (the percent-"
        },
        {
          "12\nSun et al.": "age of\nlabeled data points in each modality): {0.5%, 1%, 1.5%} for\nIEMOCAP and"
        },
        {
          "12\nSun et al.": "{1%, 2%, 3%} for MOSI. For a fair comparison, we follow architecture setting in [23]."
        },
        {
          "12\nSun et al.": "We adopt the modality encoder architectures in [23] as the single modality classiﬁers for"
        },
        {
          "12\nSun et al.": "CE and TCGM, while adopting the aggregator on the top of modality encoders for LMF"
        },
        {
          "12\nSun et al.": "and HGR. Adam with default parameters and learning rate γu = 0.0001, γl = 0.001 is"
        },
        {
          "12\nSun et al.": "used as the optimizer. The batch size is set to 32."
        },
        {
          "12\nSun et al.": "We report the AUC (Area under ROC curve) for the aggregators on all the modalities"
        },
        {
          "12\nSun et al.": "and single modality classiﬁers by different methods. We only report the AUC of LMF"
        },
        {
          "12\nSun et al.": "and HGR on all modalities since they do not have single modality classiﬁers. For single"
        },
        {
          "12\nSun et al.": "modality classiﬁers, we show results on the text modality on happy emotion (d), audio"
        },
        {
          "12\nSun et al.": "modality on neutral emotion (e) the video modality on angry emotion on IEMOCAP; and"
        },
        {
          "12\nSun et al.": "(h) the video modality (i) the audio modality on MOSI. Please refer to supplementary"
        },
        {
          "12\nSun et al.": "material for complete experimental result. As shown in Figure 5, aggregators trained by"
        },
        {
          "12\nSun et al.": "TCGM outperform all the baselines given only tiny fractions of labeled data. TCGM"
        },
        {
          "12\nSun et al.": "improves the AUC of\nthe single modality classiﬁers signiﬁcantly, which shows the"
        },
        {
          "12\nSun et al.": "efﬁcacy of utilizing the cross-modal information during the training of our method. As"
        },
        {
          "12\nSun et al.": "label rates continue to grow, the advantage of our method over CE decreases since more"
        },
        {
          "12\nSun et al.": "information is provided for CE to learn the ground-truth label."
        },
        {
          "12\nSun et al.": "Our method also outperforms other methods in terms of the prediction based on all"
        },
        {
          "12\nSun et al.": "the modalities, especially when the label rate is small. This shows the superiority of our"
        },
        {
          "12\nSun et al.": "method when dealing with a limited amount of annotations."
        },
        {
          "12\nSun et al.": "6.3\nDisease prediction of Alzheimer’s Disease"
        },
        {
          "12\nSun et al.": "Dataset Early prediction of Alzheimer’s Disease (AD) is attracting increasing attention"
        },
        {
          "12\nSun et al.": "since it\nis irreversible and very challenging. Besides, due to privacy issues and high"
        },
        {
          "12\nSun et al.": "collecting costs, an efﬁcient classiﬁer with limited labeled data is desired. To validate"
        },
        {
          "12\nSun et al.": "the effectiveness of our method on this challenging task, we only keep labels of a limited"
        },
        {
          "12\nSun et al.": "percentage of data, which is obtained from the most popular Alzheimer’s Disease"
        },
        {
          "12\nSun et al.": "Neuroimaging Initiative (ADNI) dataset10, with 3D images sMRI and PET. DARTEL"
        },
        {
          "12\nSun et al.": "VBM pipeline [2] is implemented to pre-process the sMRI data, and then images of PET"
        },
        {
          "12\nSun et al.": "were reoriented into a standard 91 × 109 × 91 voxel image grid in MNI152 space, which"
        },
        {
          "12\nSun et al.": "is same with sMRIs’. To limit the size of images, only the hippocampus on both sides"
        },
        {
          "12\nSun et al.": "are extracted as input in the experiments. We denote subjects that convert to Alzheimer’s"
        },
        {
          "12\nSun et al.": "disease (MCIc) as AD, and subjects that remain stable (MCIs) as NC (Normal Control)."
        },
        {
          "12\nSun et al.": "Our dataset contains 300 samples in total, with 144 AD and 156 NC. We randomly"
        },
        {
          "12\nSun et al.": "choose 60% for training, 20% for validation and 20% for testing stage."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "65": ""
        },
        {
          "65": ""
        },
        {
          "65": ""
        },
        {
          "65": "60"
        },
        {
          "65": ""
        },
        {
          "65": ""
        },
        {
          "65": "55"
        },
        {
          "65": ""
        },
        {
          "65": "50"
        },
        {
          "65": ""
        },
        {
          "65": ""
        },
        {
          "65": "45"
        },
        {
          "65": ""
        },
        {
          "65": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "Fig. 5.\n(a,b,c) AUC of Aggregators on happy, angry and neutral emotion recognition on"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "IEMOCAP data:\n(d, e,f) AUC on text, audio and video modality modality classiﬁers on"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "IEMOCAP: AUC of other composition of (modality, emotion) are listed in supplementary mate-"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "rial. (g,h,i) AUC on MOSI data: AUC of (g) Aggregators on all modalities and single classiﬁers"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "on (h) Video modality (i) Audio modality."
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "Implementation details We synthesize two different label rates (the percentage of labeled"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "data points): {10%, 50%}. DenseNet\nis used as the classiﬁer. Two 3D convolutional"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "layers with the kernel size 3 × 3 × 3 are adopted to replace the ﬁrst 2D convolutional"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "layers with the kernel size 7 × 7. We use four dense blocks with the size of (6, 12, 24, 16)."
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "To preserve more low-level local information, we discard the ﬁrst max-pooling layer that"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "follows after the ﬁrst convolution layer. Adam with default parameters and learning rate"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "γu = γl = 0.001 are used as the optimizer during training. We set Batch Size as only 12"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "due to the large memory usage of 3D images. Random crop of 64 × 64 × 64, random ﬂip"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "and random transpose are applied as data augmentation."
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "Figure 6 shows the accuracy of classiﬁers for each modality and the aggregator. Our"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "method TCGM outperforms the baseline methods in all settings especially when the"
        },
        {
          "(g) MOSI\n(h) MOSI (video)\n(i) MOSI (audio)": "label rate is small, which is desired since it is costly to label data."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "color implies smaller volume and SUV, i.e., more probability of being AD."
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "To further illustrate the advantage of our model over others in terms of leveraging the"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "knowledge of another modality, we visualize two MCIcs, denoted as MCI1\nc and MCI2\nc,"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "which are mistakenly classiﬁed as NC by CE’s classiﬁer for sMRI and PET modality,"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "respectively. The volume and standardized uptake value (SUV) (a measurement of the"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "degree of metabolism), whose information are respectively contained by sMRI and PET"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "data, are linearly mapped to the darkness of\nthe red and blue. Darker color\nimplies"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "smaller volume and SUV,\ni.e., more probability of being AD. As shown in Figure 7,"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "the volume (SUV) of MCI1\n(MCI2\nis reasonable for CE\nc\nc) is similar to NC, hence it"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "to mistakenly classify it by only using the information of volume (SUV). In contrast,"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "TCGM for each modality can correctly classify both cases as AD, which shows the"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "better learning of the information intersection (i.e.,\nthe ground truth) during training,"
        },
        {
          "Fig. 7. Volume (sMRI, top line) and SUV (PET, bottom line) of MCI1\nc, MCI2\nc, AD and NC. Darker": "facilitated by the leverage of knowledge from another modality."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM": "",
          "15": ""
        },
        {
          "TCGM": "",
          "15": ""
        },
        {
          "TCGM": "DFG TRR169/NSFC Major International Collaboration Project \"Crossmodal Learning\".",
          "15": ""
        },
        {
          "TCGM": "",
          "15": ""
        },
        {
          "TCGM": "New Generation of Artiﬁcial Intelligence\" Major Project No.2018AAA0100901, China.",
          "15": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "16": "References",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "1. Achille, A., Soatto, S.: Emergence of invariance and disentanglement in deep representations."
        },
        {
          "16": "",
          "Sun et al.": "The Journal of Machine Learning Research 19(1), 1947–1980 (2018) 6, 18"
        },
        {
          "16": "",
          "Sun et al.": "2. Ashburner, J.: A fast diffeomorphic image registration algorithm. Neuroimage 38(1), 95–113"
        },
        {
          "16": "",
          "Sun et al.": "(2007) 10, 12"
        },
        {
          "16": "",
          "Sun et al.": "3. Balcan, M.F., Blum, A., Yang, K.: Co-training and expansion: Towards bridging theory and"
        },
        {
          "16": "",
          "Sun et al.": "practice. In: Advances in neural information processing systems. pp. 89–96 (2005) 3"
        },
        {
          "16": "",
          "Sun et al.": "4. Belghazi, M.I., Baratin, A., Rajeswar, S., Ozair, S., Bengio, Y., Courville, A., Hjelm, R.D.:"
        },
        {
          "16": "",
          "Sun et al.": "Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062 (2018) 4, 5"
        },
        {
          "16": "",
          "Sun et al.": "5. Bisson, G., Grimal, C.: Co-clustering of multi-view datasets: a parallelizable approach. In:"
        },
        {
          "16": "",
          "Sun et al.": "2012 IEEE 12th International Conference on Data Mining. pp. 828–833. IEEE (2012) 11"
        },
        {
          "16": "",
          "Sun et al.": "6. Blum, A., Mitchell, T.: Combining labeled and unlabeled data with co-training. In: Proceed-"
        },
        {
          "16": "",
          "Sun et al.": "ings of the eleventh annual conference on Computational learning theory. pp. 92–100. ACM"
        },
        {
          "16": "",
          "Sun et al.": "(1998) 2, 3, 6"
        },
        {
          "16": "",
          "Sun et al.": "7. Busso, C., Bulut, M., Lee, C.C., Kazemzadeh, A., Provost, E.M., Kim, S., Chang, J.N., Lee, S.,"
        },
        {
          "16": "",
          "Sun et al.": "Narayanan, S.S.: Iemocap: interactive emotional dyadic motion capture database. Language"
        },
        {
          "16": "",
          "Sun et al.": "Resources and Evaluation 42, 335–359 (2008) 11"
        },
        {
          "16": "",
          "Sun et al.": "8. Cao, P., Xu, Y., Kong, Y., Wang, Y.: Max-mig: an information theoretic approach for joint"
        },
        {
          "16": "",
          "Sun et al.": "learning from crowds. ICLR2019 (2018) 4, 7"
        },
        {
          "16": "",
          "Sun et al.": "9. Chandar, S., Khapra, M.M., Larochelle, H., Ravindran, B.: Correlational neural networks."
        },
        {
          "16": "",
          "Sun et al.": "Neural computation 28(2), 257–285 (2016) 3"
        },
        {
          "16": "10. Chang, X., Xiang, T., Hospedales, T.M.: Scalable and effective deep cca via soft decorrelation.",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp."
        },
        {
          "16": "",
          "Sun et al.": "1488–1497 (2018) 3"
        },
        {
          "16": "11. Cheng, Y., Zhao, X., Huang, K., Tan, T.: Semi-supervised learning and feature evaluation for",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "rgb-d object recognition. Computer Vision and Image Understanding 139, 149–160 (2015) 3"
        },
        {
          "16": "12. Christoudias, C.M., Saenko, K., Morency, L.P., Darrell, T.: Co-adaptation of audio-visual",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "speech and gesture classiﬁers. In: Proceedings of the 8th international conference on Multi-"
        },
        {
          "16": "",
          "Sun et al.": "modal interfaces. pp. 84–91. ACM (2006) 3"
        },
        {
          "16": "13. Dasgupta, S., Littman, M.L., McAllester, D.A.: Pac generalization bounds for co-training. In:",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "Advances in neural information processing systems. pp. 375–382 (2002) 2, 6"
        },
        {
          "16": "14. Hjelm, R.D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Trischler, A., Bengio, Y.:",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "Learning deep representations by mutual information estimation and maximization. arXiv"
        },
        {
          "16": "",
          "Sun et al.": "preprint arXiv:1808.06670 (2018) 4"
        },
        {
          "16": "15. Huang, S.L., Suh, C., Zheng, L.: Euclidean information theory of networks. IEEE Transactions",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "on Information Theory 61(12), 6795–6814 (2015) 5"
        },
        {
          "16": "16. Hussain, S.F., Grimal, C., Bisson, G.: An improved co-similarity measure for document",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "clustering. In: International Conference on Machine Learning and Applications (December"
        },
        {
          "16": "",
          "Sun et al.": "2010) 11"
        },
        {
          "16": "17.",
          "Sun et al.": "Jones, R.: Learning to extract entities from labeled and unlabeled text. Ph.D. thesis, Citeseer"
        },
        {
          "16": "",
          "Sun et al.": "(2005) 3"
        },
        {
          "16": "18. Kim, D.H., Lee, M.K., Choi, D.Y., Song, B.C.: Multi-modal emotion recognition using semi-",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "supervised learning and multiple neural networks in the wild. In: Proceedings of the 19th"
        },
        {
          "16": "",
          "Sun et al.": "ACM International Conference on Multimodal Interaction. pp. 529–535. ACM (2017) 3"
        },
        {
          "16": "19. Kong, Y., Schoenebeck, G.: Water from two rocks: Maximizing the mutual information. In:",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "Proceedings of the 2018 ACM Conference on Economics and Computation. pp. 177–194."
        },
        {
          "16": "",
          "Sun et al.": "ACM (2018) 4, 7"
        },
        {
          "16": "20. Leskes, B.: The value of agreement, a new boosting algorithm. In: International Conference",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "on Computational Learning Theory. pp. 95–110. Springer (2005) 2, 6"
        },
        {
          "16": "21. Levin, A., Viola, P.A., Freund, Y.: Unsupervised improvement of visual detectors using",
          "Sun et al.": ""
        },
        {
          "16": "",
          "Sun et al.": "co-training. In: ICCV. vol. 1, p. 2 (2003) 3"
        },
        {
          "16": "",
          "Sun et al.": "22. Lewis, D.D.: Naive (bayes) at forty: The independence assumption in information retrieval."
        },
        {
          "16": "",
          "Sun et al.": "In: European conference on machine learning. pp. 4–15. Springer (1998) 2, 6"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM\n17": "23. Liu, Z., Shen, Y., Lakshminarasimhan, V.B., Liang, P.P., Zadeh, A., Morency, L.P.: Efﬁcient"
        },
        {
          "TCGM\n17": "low-rank multimodal fusion with modality-speciﬁc factors. In: ACL (2018) 10, 12"
        },
        {
          "TCGM\n17": "24. Miyato, T., ichi Maeda, S., Koyama, M., Ishii, S.: Virtual adversarial training: a regularization"
        },
        {
          "TCGM\n17": "method for supervised and semi-supervised learning. In: IEEE transactions on pattern analysis"
        },
        {
          "TCGM\n17": "and machine intelligence (2018). (2018) 11"
        },
        {
          "TCGM\n17": "25. Ng, A.Y., Jordan, M.I., Weiss, Y.: On spectral clustering: Analysis and an algorithm. In: NIPS"
        },
        {
          "TCGM\n17": "(2001) 10"
        },
        {
          "TCGM\n17": "26. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep learning."
        },
        {
          "TCGM\n17": "In: Proceedings of the 28th international conference on machine learning (ICML-11). pp."
        },
        {
          "TCGM\n17": "689–696 (2011) 2, 3"
        },
        {
          "TCGM\n17": "27. Nguyen, X., Wainwright, M.J., Jordan, M.I.: Estimating divergence functionals and the"
        },
        {
          "TCGM\n17": "likelihood ratio by convex risk minimization.\nIEEE Transactions on Information Theory"
        },
        {
          "TCGM\n17": "56(11), 5847–5861 (2010) 5"
        },
        {
          "TCGM\n17": "28. Sohn, K., Shang, W., Lee, H.: Improved multimodal deep learning with variation of infor-"
        },
        {
          "TCGM\n17": "mation. In: Advances in Neural Information Processing Systems. pp. 2141–2149 (2014) 2,"
        },
        {
          "TCGM\n17": "3"
        },
        {
          "TCGM\n17": "Studen`y, M., Vejnarová, J.: The multiinformation function as a tool for measuring stochastic\n29."
        },
        {
          "TCGM\n17": "dependence. In: Learning in graphical models, pp. 261–297. Springer (1998) 2, 4"
        },
        {
          "TCGM\n17": "30. Veliˇckovi´c, P., Fedus, W., Hamilton, W.L., Liò, P., Bengio, Y., Hjelm, R.D.: Deep graph"
        },
        {
          "TCGM\n17": "infomax. arXiv preprint arXiv:1809.10341 (2018) 4"
        },
        {
          "TCGM\n17": "31. Wang, L., Wu, J., Huang, S.L., Zheng, L., Xu, X., Zhang, L., Huang, J.: An efﬁcient approach"
        },
        {
          "TCGM\n17": "to informative feature extraction from multimodal data. arXiv preprint arXiv:1811.08979"
        },
        {
          "TCGM\n17": "(2018) 3, 10"
        },
        {
          "TCGM\n17": "32. Xu, Y., Zhao, S., Song, J., Stewart, R.J., Ermon, S.: A theory of usable information under"
        },
        {
          "TCGM\n17": "computational constraints. ArXiv abs/2002.10689 (2020) 4"
        },
        {
          "TCGM\n17": "33. Yang, Y., Zhan, D.C., Sheng, X.R., Jiang, Y.: Semi-supervised multi-modal\nlearning with"
        },
        {
          "TCGM\n17": "incomplete modalities. In: IJCAI. pp. 2998–3004 (2018) 11"
        },
        {
          "TCGM\n17": "34. Zadeh, A., Zellers, R., Pincus, E., Morency, L.P.: Mosi: Multimodal corpus of sentiment"
        },
        {
          "TCGM\n17": "intensity and subjectivity analysis in online opinion videos. ArXiv abs/1606.06259 (2016) 11"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "18\nSun et al.": "A\nProofs"
        },
        {
          "18\nSun et al.": "proof of Lemma 3. Note that"
        },
        {
          "18\nSun et al.": "Eq. (3), since {x[M ]"
        },
        {
          "18\nSun et al.": "i"
        },
        {
          "18\nSun et al.": "one can immediately get the conclusion."
        },
        {
          "18\nSun et al.": "Lemma 4. Given a joint distribution p(x1, ..., xM , y), where y is a discrete random"
        },
        {
          "18\nSun et al.": "variable, we can always ﬁnd M independent random variables z1, ..., zM such that"
        },
        {
          "18\nSun et al.": "zi ⊥⊥ y and xm = fm(y, zm), for m ∈ [M ]."
        },
        {
          "18\nSun et al.": "Proof. This proof"
        },
        {
          "18\nSun et al.": "U nif orm(0, 1),"
        },
        {
          "18\nSun et al.": "t∣y) where P(xm ≤ t∣y) is the cumulative distribution function of p(xm∣y)."
        },
        {
          "18\nSun et al.": "Lemma 5. Given assumption 1, then the Marginal-joint ratio (deﬁnition 1) R(x1, ..., xM )"
        },
        {
          "18\nSun et al.": "has"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TCGM\n19": "Ground-truth → Maximizer (h[M ]\n, p∗) is a maximizer of max∀m,hm∈Hm,p∈∆C T Cg(h1, ..., hM , p),"
        },
        {
          "TCGM\n19": "in other words, ∀h[M ] ∈ H [M ], p ∈ ∆C,"
        },
        {
          "TCGM\n19": "T Cg(h1\n, p⋆) ≥ T Cg(h1, ..., hM , p⋆)\n⋆, ..., hM"
        },
        {
          "TCGM\n19": "Maximizer → (Permuted) Ground-truth\nIf the prior is well deﬁned, then for any maxi-"
        },
        {
          "TCGM\n19": "mizer of T Cg, (˜h[M ], ˜p), there is a permutation ˜π ∶ C → C such that:"
        },
        {
          "TCGM\n19": "hm(xm)c = P(Y = ˜π(c)∣X m = xm), ˜pc = P(Y = ˜π(c))"
        },
        {
          "TCGM\n19": "Proof. We have"
        },
        {
          "TCGM\n19": "Πmhm\n⋆ (xm)"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "{hm(⋅; Θm)}M"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "for epoch t = 1 → T do"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "for m = 1 → M do"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "for batch b = 1 → ⌈∣Dl∣/B⌉ do"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "Randomly sample a batch of samples:"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": ", yi)}B\nBl = {(x[M ]\ni=1 from Dl\ni"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "Compute the LCE loss:"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "L ← LCE(Bl; hm(⋅; Θm))"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "∂L\nUpdate Θm: Θm ← Θm − γl"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "∂Θm"
        },
        {
          "Require: Unlabeled dataset Du = {x[M ]\ni": "end for"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": ""
        },
        {
          "Visual Modality (IEMOCAP-Happy)": "0.8\n1.0\n1.2"
        },
        {
          "Visual Modality (IEMOCAP-Happy)": "Percentage of labeled data"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Happy (text)": "Text Modality (IEMOCAP-Angry)",
          "(b) Happy (visual)": "Audio Modality (IEMOCAP-Angry)",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "TGCM",
          "(b) Happy (visual)": "TGCM",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": "TGCM"
        },
        {
          "(a) Happy (text)": "CE",
          "(b) Happy (visual)": "CE",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": "CE"
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "",
          "(b) Happy (visual)": "",
          "(c) Happy (audio)": ""
        },
        {
          "(a) Happy (text)": "0.6\n0.8\n1.0\n1.2\n1.4",
          "(b) Happy (visual)": "0.6\n0.8\n1.0\n1.2\n1.4",
          "(c) Happy (audio)": "0.6\n0.8"
        },
        {
          "(a) Happy (text)": "Percentage of Labeled Data",
          "(b) Happy (visual)": "Percentage of Labeled Data",
          "(c) Happy (audio)": ""
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emergence of invariance and disentanglement in deep representations",
      "authors": [
        "A Achille",
        "S Soatto"
      ],
      "year": "2018",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "2",
      "title": "A fast diffeomorphic image registration algorithm",
      "authors": [
        "J Ashburner"
      ],
      "year": "2007",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "3",
      "title": "Co-training and expansion: Towards bridging theory and practice",
      "authors": [
        "M Balcan",
        "A Blum",
        "K Yang"
      ],
      "year": "2005",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "Mine: mutual information neural estimation",
      "authors": [
        "M Belghazi",
        "A Baratin",
        "S Rajeswar",
        "S Ozair",
        "Y Bengio",
        "A Courville",
        "R Hjelm"
      ],
      "year": "2018",
      "venue": "Mine: mutual information neural estimation",
      "arxiv": "arXiv:1801.04062"
    },
    {
      "citation_id": "5",
      "title": "Co-clustering of multi-view datasets: a parallelizable approach",
      "authors": [
        "G Bisson",
        "C Grimal"
      ],
      "year": "2012",
      "venue": "IEEE 12th International Conference on Data Mining"
    },
    {
      "citation_id": "6",
      "title": "Combining labeled and unlabeled data with co-training",
      "authors": [
        "A Blum",
        "T Mitchell"
      ],
      "year": "1998",
      "venue": "Proceedings of the eleventh annual conference on Computational learning theory"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "8",
      "title": "Max-mig: an information theoretic approach for joint learning from crowds",
      "authors": [
        "P Cao",
        "Y Xu",
        "Y Kong",
        "Y Wang"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "9",
      "title": "Correlational neural networks",
      "authors": [
        "S Chandar",
        "M Khapra",
        "H Larochelle",
        "B Ravindran"
      ],
      "year": "2016",
      "venue": "Neural computation"
    },
    {
      "citation_id": "10",
      "title": "Scalable and effective deep cca via soft decorrelation",
      "authors": [
        "X Chang",
        "T Xiang",
        "T Hospedales"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Semi-supervised learning and feature evaluation for rgb-d object recognition",
      "authors": [
        "Y Cheng",
        "X Zhao",
        "K Huang",
        "T Tan"
      ],
      "year": "2015",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "12",
      "title": "Co-adaptation of audio-visual speech and gesture classifiers",
      "authors": [
        "C Christoudias",
        "K Saenko",
        "L Morency",
        "T Darrell"
      ],
      "year": "2006",
      "venue": "Proceedings of the 8th international conference on Multimodal interfaces"
    },
    {
      "citation_id": "13",
      "title": "Pac generalization bounds for co-training",
      "authors": [
        "S Dasgupta",
        "M Littman",
        "D Mcallester"
      ],
      "year": "2002",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Learning deep representations by mutual information estimation and maximization",
      "authors": [
        "R Hjelm",
        "A Fedorov",
        "S Lavoie-Marchildon",
        "K Grewal",
        "A Trischler",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Learning deep representations by mutual information estimation and maximization",
      "arxiv": "arXiv:1808.06670"
    },
    {
      "citation_id": "15",
      "title": "Euclidean information theory of networks",
      "authors": [
        "S Huang",
        "C Suh",
        "L Zheng"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "16",
      "title": "An improved co-similarity measure for document clustering",
      "authors": [
        "S Hussain",
        "C Grimal",
        "G Bisson"
      ],
      "year": "2010",
      "venue": "International Conference on Machine Learning and Applications"
    },
    {
      "citation_id": "17",
      "title": "Learning to extract entities from labeled and unlabeled text",
      "authors": [
        "R Jones"
      ],
      "year": "2005",
      "venue": "Citeseer"
    },
    {
      "citation_id": "18",
      "title": "Multi-modal emotion recognition using semisupervised learning and multiple neural networks in the wild",
      "authors": [
        "D Kim",
        "M Lee",
        "D Choi",
        "B Song"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "19",
      "title": "Water from two rocks: Maximizing the mutual information",
      "authors": [
        "Y Kong",
        "G Schoenebeck"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM Conference on Economics and Computation"
    },
    {
      "citation_id": "20",
      "title": "The value of agreement, a new boosting algorithm",
      "authors": [
        "B Leskes"
      ],
      "year": "2005",
      "venue": "International Conference on Computational Learning Theory"
    },
    {
      "citation_id": "21",
      "title": "Unsupervised improvement of visual detectors using co-training",
      "authors": [
        "A Levin",
        "P Viola",
        "Y Freund"
      ],
      "year": "2003",
      "venue": "ICCV"
    },
    {
      "citation_id": "22",
      "title": "Naive (bayes) at forty: The independence assumption in information retrieval",
      "authors": [
        "D Lewis"
      ],
      "year": "1998",
      "venue": "European conference on machine learning"
    },
    {
      "citation_id": "23",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "24",
      "title": "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
      "authors": [
        "T Miyato",
        "S Ichi Maeda",
        "M Koyama",
        "S Ishii"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "25",
      "title": "On spectral clustering: Analysis and an algorithm",
      "authors": [
        "A Ng",
        "M Jordan",
        "Y Weiss"
      ],
      "year": "2001",
      "venue": "NIPS"
    },
    {
      "citation_id": "26",
      "title": "Multimodal deep learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th international conference on machine learning (ICML-11)"
    },
    {
      "citation_id": "27",
      "title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization",
      "authors": [
        "X Nguyen",
        "M Wainwright",
        "M Jordan"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "28",
      "title": "Improved multimodal deep learning with variation of information",
      "authors": [
        "K Sohn",
        "W Shang",
        "H Lee"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "The multiinformation function as a tool for measuring stochastic dependence",
      "authors": [
        "M Studenỳ",
        "J Vejnarová"
      ],
      "year": "1998",
      "venue": "Learning in graphical models"
    },
    {
      "citation_id": "30",
      "title": "Deep graph infomax",
      "authors": [
        "P Veličković",
        "W Fedus",
        "W Hamilton",
        "P Liò",
        "Y Bengio",
        "R Hjelm"
      ],
      "year": "2018",
      "venue": "Deep graph infomax",
      "arxiv": "arXiv:1809.10341"
    },
    {
      "citation_id": "31",
      "title": "An efficient approach to informative feature extraction from multimodal data",
      "authors": [
        "L Wang",
        "J Wu",
        "S Huang",
        "L Zheng",
        "X Xu",
        "L Zhang",
        "J Huang"
      ],
      "year": "2018",
      "venue": "An efficient approach to informative feature extraction from multimodal data",
      "arxiv": "arXiv:1811.08979"
    },
    {
      "citation_id": "32",
      "title": "A theory of usable information under computational constraints",
      "authors": [
        "Y Xu",
        "S Zhao",
        "J Song",
        "R Stewart",
        "S Ermon"
      ],
      "year": "2020",
      "venue": "A theory of usable information under computational constraints"
    },
    {
      "citation_id": "33",
      "title": "Semi-supervised multi-modal learning with incomplete modalities",
      "authors": [
        "Y Yang",
        "D Zhan",
        "X Sheng",
        "Y Jiang"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "34",
      "title": "Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L Morency"
      ],
      "year": "2016",
      "venue": "Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos"
    }
  ]
}