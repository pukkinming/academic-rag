{
  "paper_id": "2310.14557v1",
  "title": "The Skipped Beat: A Study Of Sociopragmatic Understanding In Llms For 64 Languages",
  "published": "2023-10-23T04:22:44Z",
  "authors": [
    "Chiyu Zhang",
    "Khai Duy Doan",
    "Qisheng Liao",
    "Muhammad Abdul-Mageed"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPAR-ROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing opensource instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although Chat-GPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/ UBC-NLP/SPARROW",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multilingual LLMs have recently transformed NLP, due to their powerful capabilities on a ⋆ Equal contribution   et al., 2022) , UTCC  (Risch et al., 2021) ,  Nai-jaSenti (Muhammad et al., 2022) , AfriSenti  (Muhammad et al., 2023a) , SentiEval  (Zhang et al., 2023b) .\n\nwide range of tasks  (Xue et al., 2021; Scao et al., 2022) . Methods such instruction tuning and reinforcement learning from human feedback (RLHF)  (Ouyang et al., 2022)  have further enhanced the zero-shot generalizability of these models. Notably, ChatGPT exhibits impressive capabilities in this regard. Human language, however, is intrinsically ambiguous and far from solved. In fact, some forms of meaning are deeply embedded in social interactions. We collectively refer to this type of meaning as sociopragmatic meaning (SM). To appreciate SM, consider how the meaning of an utterance in social interaction (e.g., on social media) can be highly subtle and how it incorporates both the social variation related to language users (from a sociolinguistics perspective)  (Tagliamonte, 2015)  and their communicative intentions (from a pragmatics perspective)  (Boxer and Cortés-Conde, 2021) . Although SM is quite established within linguistics, NLP systems still struggle with this type of meaning that is intertwined in social and interactive contexts  (Zhang and Abdul-Mageed, 2022) . The extent to which instruction tuned models such as ChatGPT can capture SM across languages re-mains largely unclear as these models are yet to be evaluated on appropriate datasets under standardized conditions easy to replicate.\n\nTo facilitate evaluation of LLMs and enhance fairness of model comparisons and reproducibility, early work introduces evaluation benchmarks. Most existing benchmarks, however, focus on the monolingual setting. These include GLUE  (Wang et al., 2019) ,  SentEval (Conneau and Kiela, 2018), and TweetEval (Barbieri et al., 2020)  for English, ARLUE  (Abdul-Mageed et al., 2021)  for Arabic, CLUE  (Xu et al., 2020a)  for Chinese, and IndoNLU  (Wilie et al., 2020)  for Indonesian. Although XTREME  (Hu et al., 2020)  and XGLUE  (Liang et al., 2020)  introduce multilingual benchmarks, they only include a few SM tasks for a limited number of languages. They are also limited to standard language use (e.g., Wikipedia).  Barbieri et al. (2022)  propose a multilingual sentiment analysis benchmark (UMSAB), but it solely contains tweet sentiment analysis datasets in only eight languages. As such, absence of a unified, diverse, and comprehensive benchmark and a fragmented evaluation landscape hamper NLP work for cross-lingual SM.\n\nAnother challenge for SM research is the issue of data inaccessibility  (Assenmacher et al., 2022) . Although many studies release the IDs of posts (e.g., tweets), substantial amounts of these social posts become inaccessible over time due to deletion, etc.  (Zhang et al., 2022) . In our benchmark, we attempt to re-collect text contents of 25 datasets by using their IDs but can only retrieve 58% samples on average (see Table  8  in  Appendix) . This data decay also hinders fair comparisons in NLP for SM research. This issue has already become worse as corporations such as Twitter tighten access to their API, making it even harder to collect historical data. To address this bottleneck, we introduce a massively multilingual SM evaluation benchmark, dubbed SPAR-ROW, that comprises 169 datasets covering 64 languages from 12 language families, 16 types of scripts, across diverse online platforms (e.g., Twitter, YouTube, and Weibo). We then perform an extensive evaluation of ChatGPT, comparing it to 13 other models ranging in size between 110M-7B parameters. Our evaluations allow us to answer multiple questions related to how it is that LLMs fare across languages on SM. To facilitate future comparisons, we also design a modular, interac- tive leaderboard on top of our new benchmark.\n\nTo summarize, the contributions of this paper are as follows: (1) We collect, uniformize, and responsibly release massively multilingual SM datasets in a new benchamark; (2) Our SPAR-ROW benchmark is essentially an archive of SM datasets that alleviates the serious issue of data decay; (3) We evaluate a wide range of models on our SPARROW benchmark via fine-tuning SoTA encoder-only pretrained language models and zero-shot learning of a number of generative models, including instruction tuned models (e.g., BLOOMZ) as well as ChatGPT; and (4) We establish standard settings for future research in this area across a large number of languages and tasks. through a public leaderboard.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Evaluation of LLMs. There have been many attempts to evaluate ChatGPT and instruction tuned LLMs.  Qin et al. (2023) ;  Laskar et al. (2023) ;  Zhong et al. (2023) ;  Wu et al. (2023)  utilize existing English evaluation benchmarks, such as GLUE  (Wang et al., 2019)  and BigBench  (Srivastava et al., 2022) , to evaluate LLMs' capacities on various NLP tasks. These studies find that although ChatGPT performs less effectively than the models finetuned specifically for each task, it demonstrates superior capabilities compared to other instruction tuned LLMs (e.g.,  FLAN (Chung et al., 2022) ).  Ahuja et al. (2023) ;  Bang et al. (2023) ;  Laskar et al. (2023) ;  Lai et al. (2023) ;  Huang et al. (2023)  evaluate LLMs on more diverse languages using existing multilingual benchmarks (e.g., XNLI, PAWS-X, XLSum) involving monolingual NLP tasks and crosslingual tasks (e.g., machine translation). Their findings point to a large gap in performance of instruction tuned LLMs and ChatGPT, especially on low-resource languages and those with non-Latin scripts. SM is still not adequately represented in existing benchmarks, hindering comprehensive evaluations on more languages. As we summarize in Table  1 , benchmarks used for listed evaluations only include a few SM tasks focusing on sentiment analysis.  Wang et al. (2023) ;  Zhang et al. (2023b)  investigate LLMs on a number of SM tasks (e.g., offensive language detection), but only on English.  Ziems et al. (2023)  investigate Chat-GPT performance on a range of computational social science tasks covering subjects such as sociology, psychology, and linguistics, but they again focus only on English.  Das et al. (2023)  extend evaluation of ChatGPT on hate speech detection to 11 languages. Compared to these works, our objective is to investigate more diverse SM tasks on a massively multilingual setting. Sociopragmatic Meaning Benchmarks. Many previous works introduce unified benchmarks such as GLUE  (Wang et al., 2019) ,  SentEval (Conneau and Kiela, 2018) , XTREME  (Hu et al., 2020) , and XGLUE  (Liang et al., 2020) . These benchmarks include a wide range of NLP tasks, but comprise a sole SM task (i.e., sentiment analysis). Some recent studies started to construct benchmarks focusing on SM:  Barbieri et al. (2020)  introduce TweetEval benchmark that contains seven English datasets of six SM tasks;  Zhang et al. (2023b)  develop SentiEval that involves 26 English datasets of 13 sentimentrelated tasks. Beyond English, NusaX  (Winata et al., 2022) ,  NaijaSenti (Muhammad et al., 2022), and AfriSenti (Muhammad et al., 2023a)  propose benchmarks for sentiment analysis with eight Indonesian languages, four African languages, and 14 African languages, respectively. UMSAB introduced by  Barbieri et al. (2022)  contains 11 sentiment analysis datasets in 11 languages. For detecting antisocial online comments,  Risch et al. (2021)  introduces a toxic comment collection that contains 43 datasets of six antisocial detection tasks in 14 languages. Compared to these works, our SPARROW benchmark includes significantly more SM tasks and languages, from more diverse sources (refer to Figure  1  for a comparison).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sparrow Benchmark",
      "text": "In this section, we describe clusters of tasks in our benchmark as well as our preprocessing and unification. SPARROW consists of 13 types of tasks in six main categories. It contains 169 datasets from diverse online platforms and covers a wide period of time . We group different tasks in our benchmark by what we perceive to be an affinity between these tasks. For example, we group tasks of hate speech, offensive language, and dangerous language detection as anti-social language detection. Meanwhile, we keep particular tasks (such as sentiment analysis and emotion recognition) distinct due to the popularity of these tasks and since there are multiple datasets representing each of them. Table  2  summarizes statistics of SPARROW. We now briefly introduce our task clusters. We provide more information about languages in SPARROW in Table  7  of the Appendix. We also provide detailed descriptions with full citations of all our datasets in Tables 9, 10, 11, 12, 13, and 14 in Appendix.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Clusters",
      "text": "Antisocial Language Detection. The proliferation of antisocial language (e.g., hate speech) toxifies public discourse, incites violence, and undermines civil society  (Sap et al., 2019; Vidgen and Derczynski, 2020) . Antisocial language detection is thus a useful task. We include under the umbrella of antisocial language the following:\n\n(1) aggressive language  (Kumar et al., 2018) , (2) dangerous language  (Alshehri et al., 2020) , (3) hate speech (e.g.,  Waseem and Hovy (2016) ;  Deng et al. (2022) ), (4) offensive language (e.g.,  Mubarak et al. (2020) ; Kralj  Novak et al. (2021) ), (5) offense type identification (e.g.,  Zampieri et al. (2019) ), and (6) offense target identification (e.g.,  Ousidhoum et al. (2019) ;  Jeong et al. (2022) ).\n\nEmotion Recognition. Emotion affects our decision-making as well as mental and physical health  (Abdul-Mageed and Ungar, 2017) . SPAR-ROW includes 26 emotion datasets in 17 languages (e.g.,  Kajava (2018); Bianchi et al. (2021) ).\n\nHumor Detection. Humor is a type of figurative language which induces amusing effects, such as laughter or well-being sensations. We include four humor detection datasets in four languages (e.g.,  Blinov et al. (2019) ;  Meaney et al. (2021) ).\n\nIrony & Sarcasm Detection. Irony and sarcasm also involve figurative language. An ironic/sarcastic expression intentionally uses diametric language to signify implied meaning. We include (1) nine irony detection datasets in seven languages (e.g.,  Xiang et al. (2020) ), (2) ten sarcasm detection datasets in four languages (e.g.,  Walker et al. (2012) ), and (3) an irony type identification dataset  (Van Hee et al., 2018) .\n\nSubjectivity and Sentiment Analysis. Subjectivity analysis aims to understand the opinions, feelings, judgments, and speculations expressed via language  (Abdul-Mageed et al., 2014) . Our benchmark includes six subjectivity analysis datasets in five different languages (e.g.,  Pang and Lee (2004) ;  Pribán and Steinberger (2022) ). Subjectivity incorporates sentiment. Sentiment analysis  (Poria et al., 2020)  is one of the most popular tasks in SM understanding where the objective is to identify the underlying sentiment of a given text. Our benchmark contains 77 sentiment analysis datasets in 58 languages (e.g.,  Pang and Lee (2005) ;  Marreddy et al. (2022) ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preprocessing, Splits, And Metrics",
      "text": "We apply light normalization on all the samples by converting user mentions and hyperlinks to 'USER' and 'URL', respectively. We standardize label names for consistency across datasets without reassigning nor aggregating the original labels of the datasets. For instance, in certain sentiment analysis datasets, we map '0' and '1' to 'Negative' and 'Positive' respectively. Regarding data splits, if the dataset already has Train, Dev, and Test sets, we maintain the same splits. If the original dataset does not include a Dev set, we randomly select 10% of training data to be a Dev set. In cases without pre-defined splits, we use an 80% Train, 10% Dev, and 10% Test random split. For computing constraints, we also prepare a smaller Test set for each dataset by randomly sampling 500 samples from Test (if its size exceeds 500). We refer to this smaller test set as Test-S.\n\nWe evaluate on each dataset using its original metric as  Tables 9, 10, 11, 12, 13, and 14  in Appendix summarize.  1  We report the performance on individual datasets, aggregate datasets into 13 tasks, and report an average score over each task. Moreover, we introduce a metric for each main category, calculated as the average of dataset-specific metrics within that category. Inspired by previous evaluation benchmarks like GLUE  (Wang et al., 2019) , we define a global metric called SPARROW score, which represents the unweighted average of all dataset-specific metrics.\n\nThe SPARROW score provides an overall indication of performance on SM tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Methods",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Finetuning On Encoder-Only Models",
      "text": "We evaluate the following Transformer-encoderbased multilingual models on SPARROW:  Bernice (DeLucia et al., 2022) , a 270M-parameter model trained with 2.5B tweets in 66 languages, and (4) InfoDCL  (Zhang et al., 2023a) , a SoTA for SM understanding, which further trains XLM-R with 100M tweets in 66 languages with contrastive learning. More details about all models are in Appendix B.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Zero-And Few-Shot On Llms",
      "text": "We investigate zero-shot performance on a wide range of generative models, including pretrained generative models: (1) BLOOM  (Scao et al., 2022) , (2) mT5  (Xue et al., 2021) , (3) LLaMA  (Touvron et al., 2023) , instruction tuned models: (4)  BLOOMZ (Scao et al., 2022) , a BLOOM-initialized model tuned with multilingual xP3 corpus, (5)  BLOOMZ-P3 (Muennighoff et al., 2022) , a BLOOM-initialized model tuned with English-only P3 corpus, (6) BLOOM-Bactrian  (Li et al., 2023) , a BLOOM-initialized model tuned with 3.4M instruction-following samples in 52 languages, (7) mT0  (Muennighoff et al., 2022) , an mT5 model tuned with xP3 corpus, (8) Alpaca  (Taori et al., 2023) , a",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation",
      "text": "Finetuning. To keep computation reasonable, we randomly select 45 datasets for hyperparameter tuning and only tune the learning rate of each model.  3  For all experiments, we finetune a pretrained model with an arbitrary batch size of 32 and sequence length of 128 tokens. Each model is finetuned on the full Train set of each dataset for 20 epochs (with patience = 5) based on performance on Dev set. We run each experiment three times with different random seeds and identify the best model on Dev in each run. We report the average performance on Test-S over three runs.  4 Zero-shot Evaluation. We perform a zero-shot evaluation on SPARROW for BLOOM-, mT5-, and LLaMA-based models using language model evaluation harness (lm-evaluation-harness Gao et al. (  2021 )). 5  While we do not tailor prompts specifically for each model, customized prompts are employed for each set of tasks. These prompts follow the structure of question-and-answer tasks, where we present sample content alongside a taskspecific question, as shown in Figure  2 . The prompts are summarized in Appendix Table  15 .\n\nWe then instruct the model to generate an answer based on the provided option labels. Each option label represents a potential answer, and we calculate the log-likelihood for each candidate. The prediction with the highest log-likelihood is chosen as the model's final prediction. For the evaluation of ChatGPT, we draw inspiration from previous practices for prompt design  (Ziems et al., 2023) , and incorporate additional instructions to guide its generation of the desired labels. As shown in Figure  2 , we provide an instruction that compels ChatGPT to select a single label for the given input text without providing any additional explanation. We set the temperature to 0 to generate deterministic and reproducible results from ChatGPT. For a few instances, we observe that ChatGPT is unable to provide a direct answer. In these cases, we randomly assign a false label to each sample. In addition, we also use machine translation to translate English prompts and label names to the corresponding language of each dataset.  6 Few-shot Evaluation. We utilize lm-evaluationharness tool with the same prompts employed in zero-shot evaluation to explore the few-shot incontext learning abilities of open-source LLMs. Before the actual test examples, we prepend m examples from the Train set. Each example consists of an input text, task-specific instruction, and the corresponding answer. We set m to either 3 or 5.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "We present the aggregated performance of Test-S on each task and main category, respectively, in Table  3 . We also present test results on all datasets and compare to dataset-specific SoTA performance in  Tables 17, 18, 19, 20, 21, and 22  in Appendix.\n\n(1) How is the overall performance over different models? All the fully finetuned models surpass the zero-shot generative models as well as ChatGPT, as shown in Table  3 . The most superior among the finetuned models is InfoDCL, which achieves a SPARROW score of 71.60 and outperforms ChatGPT with 11.56 points SPAR-ROW score. On the other hand, the open-source models (i.e., BLOOM, mT5 and LLaMA) still significantly lag behind on multilingual SM understanding with performance close to a random baseline. Meanwhile, the instruction tuned multilingual LLMs (BLOOMZ and mT0) only slightly perform better than the random baseline.\n\n(2) Can instruction tuning enhance LLMs' ability on SM understanding? Yes, but it depends on the instruction training data. Following instruction tuning on the English-only P3 dataset, BLOOMZ-P3 demonstrates an improvement of 7.76 SPARROW score compared to BLOOM. Also, BLOOMZ improves 5.85 points over BLOOM (but falls short of BLOOMZ-P3). MT0 also outperforms mT5. However, there remains a substantial gap between all instruction tuned models and finetuned models. BLOOM-Bactrian performs worse than BLOOMZ and BLOOMZ-P3, which are instruction tuned with NLP tasks. This indicates that the general purpose instruction-response dataset is not very useful for SM understanding.\n\nTo further probe how instruction tuning improves BLOOM-based models, we compare BLOOM with BLOOMZ-P3 and BLOOMZ in terms of individual tasks, finding sentiment analysis to exhibit the most significant improvement. BLOOMZ-P3 and BLOOMZ achieve a sentiment score improvement of 16.37 and 12.36, respectively, based on average calculation across 77 sentiment analysis datasets. However, BLOOM-Bactrian obtains an improvement of only 1.79 sentiment score, perhaps implying that the Bactrian instruction-response data is not all that useful for some SM tasks. After tuning mT5 on xP3 dataset, mT0 also experiences a 13.88 improvement in the sentiment score. These may be stemming from inclusion of five English sentiment analysis datasets in both P3 and xP3 during the training phase. For example, we observe that BLOOM, BLOOMZ, BLOOMZ-P3, mT5, and mT0 obtain an accuracy of  56.4, 92.2, 93.00, 49.00, and 76.8  on Senteng Soc (not included in either xP3 or P3), respectively and that BLOOM-Bactrian still performs poorly (accuracy= 53.60) after instruction tuning. Again, these results indicate that it is still important to include task-related datasets in the instruction tuning stage.\n\n(3) How do LLMs perform across different SM tasks? They are inferior at humor and antisocial language detection while being relatively better at sentiment and emotion recognition tasks. BLOOMZ-P3, BLOOMZ, and mT0 exhibit considerable enhancements (> 5 points) on sentiment and emotion when compared to their respective initialization models. On the other hand, we find that instruction tuned models perform significantly worse on aggressive language detection and humor detection tasks. BLOOMZ-P3, BLOOMZ, BLOOM-Bactrian, and mT0 all incur a loss of more than 5 points on these two tasks. Upon investigating the predictions of instruction tuned models, we find that they tend to assign negative labels (i.e., non-aggressive or non-humor) which results in many false negative predictions. For a concrete example, we show that BLOOMZ-P3 predict most samples as non-humor in Figure  3a  shows.\n\nChatGPT outperforms the open-source LLMs on all tasks except dangerous language detection. Comparing ChatGPT to InfoDCL, we find gaps favoring InfoDCL in subjectivity analysis (a difference of 9.47), emotion recognition (a difference of 10.68), and irony & sarcasm detection (a difference of 10.70). ChatGPT also largely lags behind InfoDCL in humor detection (a difference of 15.40) and antisocial language detection (a difference of 14.06). As the example shows in Figure  3b , ChatGPT makes more false positive errors (classifies non-hateful as hateful).\n\n(4) How do LLMs perform across different languages? We now examine the impact of instruction finetuning on the model's language-wise performance. We categorize the performance of each      dataset based on language and calculate the average language scores across all datasets within a language. Since each language contains different tasks and datasets, a direct comparison across languages is not feasible. Therefore, we compare the relative performance between different models for each language. By comparing the instruction tuned models to their initial models, we observe that most languages experience improvement. However, we also observe a significant decline in performance for the Amharic (amh) dataset among these models. Specifically, BLOOMZ-P3, BLOOMZ, and mT0 experience a deterioration of 36.07, 24.99, and 26.12 points, respectively, compared to their respective initial models. We hypothesize that this deterioration can be attributed to catastrophic forgetting after instruction tuning, where Amharic was not included in the training set and does not share the writing scripts with the other included languages.  Similarly, the Filipino (fil) tasks exhibit an average decline of approximately 11 points on both BLOOMZ-P3 and BLOOMZ, as Filipino is not included in the xP3 dataset. Although Hindi is included in the xP3 dataset, the three instruction tuned models still show a decline in performance. Upon examining the individual performance of Hindi datasets, we find that the major deteriorations occur in the aggressive language detection and humor detection tasks, while the emotion recognition and sentiment analysis tasks show improvement. The instruction-response data for training Alpaca and Vicuna consist solely of English language. Therefore, we compare the performance of Alpaca and Vicuna to that of LLaMA using both English and non-English datasets. We observe that Alpaca and Vicuna outperform LLaMA when evaluated on English datasets, achieving average scores of 8.30 and 5.51, respectively. However, their performance declines when tested on non-English datasets, resulting in average decreases of 1.53 and 0.33, respectively. Compared to task-specific InfoDCL, ChatGPT performs poorly in 63 out of 64 languages, sometimes with a large gap (e.g., 45.06 lower on Amharic, 38.67 lower on Malayalam, and 36.91 lower on Buginese), as Table  4  shows. We also investigate how different models perform on SM tasks across various languages. Results for two tasks, hate speech detection (top) and humor detection (bottom), are presented in Fig-  ure 4 . The dataset for each task is grouped according to language, and the average score of each language is obtained. The relative gain of each model against the random baseline is shown, allowing us to compare across these languages.  7  We observe that InfoDCL is the best model across various tasks and languages, with the exception of hate speech in Polish where ChatGPT outperforms it. As Figure  4  shows, ChatGPT performs better for Western languages on hate speech detection. We can also observe wider gaps in hate speech detection between ChatGPT and InfoDCL on Arabic and Korean. Similarly, while ChatGPT demonstrates satisfactory performance in English humor, it remains at significant distance behind InfoDCL in Hindi humor.\n\n(5) Do machine translated prompts help LLMs? Not in general, but they do help in a few cases. We find, in Table  5 : Case study on using machine translated input and GPT-4 on samples mispredicted by ChatGPT.\n\nprompts is 6.14 points lower than ChatGPT with English prompts. Meanwhile, a few tasks such as humor and sarcasm acquire improvements. We also observe a similar pattern for BLOOMZ and mT0, as Table  3  shows. The low-resource languages with non-Latin scripts experience more performance drops in general, which is in line with findings by  Lai et al. (2023) . Hebrew (heb) and Greek (ell) get the largest performance drops (over 25 points in each case), as shown in Table  4 .\n\n(6) Does GPT-4 outperform ChatGPT? Yes, it does. We provide a study on probing GPT-4's capacities. We exploit 20 datasets from two tasks (i.e., hate speech and humor detection) in 12 languages, only choosing samples whose labels Chat-GPT predicted incorrectly. We refer to this test set as GPTHard and provide samples from it to GPT-4 in their original language, employing the same English prompts as those used by ChatGPT. As        when using the translated input. We also observe that when fed with these English-translated samples, ChatGPT is able to surpass GPT4 with the original inputs in three datasets (i.e., Hateara Ala , Hate-spa Bas , Hate-ara Mub ). These results suggest that although ChatGPT has inferior ability on several languages in terms of detecting SM, a translate-then-detect approach may be possible.\n\n(8) How do open-source LLMs perform with few-shot in-context learning? As Table  6  shows, we compare three-shot and five-shot results with zero-shot results. Based on SPAR-ROW score, we observe that few-shot learning does enhance the performance of BLOOM, mT5, LLaMA, and Vicuna. With the increasing number of shots, the performance of LLaMA and Vicuna increases. Vicuna obtains SPARROW scores of 29.36, 39.44, and 41.97 with zero, three, and five shots, respectively. However, BLOOMZ-P3 and mT0 do not improve with few-shot learning.\n\nWe suspect this is because the instruction finetuning of these two models only uses a zero-shot template that hurts their few-shot learning capacities. BLOOMZ-P3 and mT0 are also different from BLOOM and LLaMA in that they are finetuned on several NLP tasks only one of which is an SM task (i.e., sentiment analysis). This probably biases the behavior of these two models.\n\n(9) Are the open-source LLMs sensitive to prompts used? We carry out a study to probe the open-source LLMs' sensitivity to prompts. We curate 55 datasets across four tasks from SPARROW and evaluate six models with the prompts we used for evaluating ChatGPT. As Ta-ble 24 in Appendix shows, we find that BLOOM, LLaMA, and Vicuna incur sizable performance drops (> 6 points decrease across 55 datasets), while BLOOMZ-P3, mT5, and mT0 demonstrate performance levels akin to those observed in previous experiments (< 2 points different). We leave a more comprehensive evaluation of prompt sensitivity as future work.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Public Leaderboard",
      "text": "To facilitate future work, we design a public leaderboard for scoring models on SPARROW.\n\nOur leaderboard is interactive and offers rich metadata about the various datasets in our benchmark. It also encourages users to submit information about their models (e.g., number of parameters, time to convergence, pretraining datasets).\n\nWe also distribute a new modular toolkit for finetuning or evaluating models on SPARROW.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In order to understand the abilities of ChatGPT and other instruction tuned LLMs on capturing sociopragmatic meaning, we introduced a massively multilingual evaluation benchmark, dubbed SPARROW. The benchmark involves 169 datasets covering 64 languages from 12 language families and 16 scripts. Evaluating ChatGPT on SPAR-ROW, we find it struggles with different languages. We also reveal that task-specific models finetuned on SM (much smaller than ChatGPT) consistently outperform larger models by a significant margin even on English.\n\nBenchmark Construction. Our SPARROW benchmark only includes text classification tasks related to SM. Despite our best efforts, we acknowledge that our benchmark has not covered existing SM datasets exhaustively. We will continue expanding this benchmark and welcome future datasets or metric contributions to it. We also plan to extend SPARROW to more types of tasks related to SM, such as span-based sentiment analysis  (Xu et al., 2020b) , affective language generation (Goswamy et al., 2020), and conversational sentiment analysis  (Ojamaa et al., 2015) . We only include text-based SM tasks. Another improvement direction is to extend this benchmark to more tasks that involve more modalities, such affective image captioning  (Mohamed et al., 2022)  and multi-modal emotion recognition  (Firdaus et al., 2020) .\n\nModel Selection. Due to computation constraints, we cannot evaluate on model sizes > 7B. However, we hope SPARROW will be used in the future to evaluate larger-sized models. Again, due to budget constraints, we only conduct a relatively small case study on GPT-4 and do not evaluate more diverse commercial instruction tuned models that are more expensive (e.g., text-davinci-003 by OpenAI).\n\nExperiments. While we customize prompts employed for each task, we do not tailor prompts specifically for each model. We acknowledge that the performance of models may be influenced by different prompt variants. In future work, we will test diverse prompt variations for more robust results. We only experiment with machine translated prompts in our analyses and acknowledge that the performance drop may stem from the poor quality of machine translation. We will investigate the utility of human translated prompts in a future study. In this paper, we only evaluate LLMs on zero-shot learning. The adoption of few-shot incontext learning may enhance performance, which we also leave to future work.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ethics Statement And Broad Impacts",
      "text": "Data Collection and Releasing. All the 169 datasets are produced by previous research. Since there are large numbers of datasets and languages in SPARROW, it is hard to manually verify the quality of all the datasets. As a quality assurance measure, we only include in SPARROW datasets that are introduced in peer-reviewed published research. To facilitate access to information about each dataset, we link to each published paper describing each of these datasets inTables 9, 10, 11, 12, 13, and 14. Following privacy protection policies, we anonymize all SPARROW data as described in Section 3.2. With reference to accessibility of the original individual dataset, SPARROW data can be categorized into three releasing strategies: (1) In the case of datasets requiring approval by the original authors, we require future researchers to obtain approval first and will share our splits once approval has been obtained. We indicate these nine datasets in our data description tables. (2) For the 25 datasets (see Table  8  in Appendix) that are shared via tweet IDs, we share our obtained data for research use. By doing so, we expect to mitigate the issue of data decay and allow fair comparisons.\n\n(3) We will share the other 135 publicly accessible datasets upon request. We will also require a justification for responsible use of the datasets. Each dataset will be shared in our Train, Dev, and Test splits along with a dataset card to indicate the original publication of the dataset.\n\nIntended Use. The intended use of SPARROW benchmark is to construct a scoring board to facilitate model comparisons as well as enhance fairness and reproducibility across different languages and tasks. We also aim to mitigate data decay issues in social media research. SPARROW could help researchers investigate model's capacity on SM tasks across languages. SPARROW may also be used to investigate model transferability across a wide range of tasks and diverse languages in different settings (such as zero-or few-shot settings and prompting).\n\nPotential Misuse and Bias. We notice that some annotations in the datasets of SPARROW (e.g., for hate speech task  (Waseem and Hovy, 2016) ) can carry annotation and temporal biases. We recommend that any dataset in SPARROW not be used for research or in applications without careful consideration of internal biases of the datasets and potential biases of the resulting systems. We also suggest that users of SPARROW not only focus on the overall SPARROW score but also a model performance on each task and dataset. The SPARROW score is an unweighted average score over all the dataset-specific metrics, which may lose the fine-grained information and be dominated by the largest task cluster (i.e., sentiment analysis) or languages (e.g., languages from Indo-European language family).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Appendices A Benchmark",
      "text": "Table  7  summarizes language distribution of datasets in SPARROW and taxonomy of these language according to Ethnologue  (Gordon Jr, 2005)  and Glottolog  (Nordhoff and Hammarström, 2011) . Tables 9, 10, 11, 12, 13, and 14 describe the datasets in tasks of antisocial language detection, emotion recognition, humor detection, irony and sarcasm detection, sentiment analysis, and subjectivity analysis, respectively. We empirically characterize the issue of data inaccessibility by re-collecting tweets content via tweet IDs. Table  8  shows the data decay issue of 25 datasets.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B Models",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B.1 Finetuning On Encoder-Only Llms",
      "text": "We evaluate the following Transformer-encoderbased multilingual PLMs on SPARROW. We finetune each PLMs on the full training set and update all the parameters of the model during the training.\n\n(1) Multilingual-BERT (mBERT) (Devlin et al., 2019) is trained on a Wikipedia corpus including 104 languages with masked language modelling (MLM) and next sentence prediction objectives. It contains 110M parameters. mBERT tokenizes text by using WordPiece with a vocabulary size of 172K.\n\n(2) XLM-RoBERTa Base (XLM-R) (Conneau et al., 2020) is trained on CommonCrawl data involving 100 languages with MLM objective. It uses a SentencePiece tokenizer with a vocabulary size of 250K and contains 270M parameters.\n\n(3) Bernice (DeLucia et al., 2022) is trained with 2.5B tweets in 66 languages and MLM objective. Bernice consists of 270M parameters and a tweet-specific SentencePiece tokenizer including a vocabulary size of 250K.\n\n(4) InfoDCL  (Zhang et al., 2023a)  further trains XLM-R with 100M tweets in 66 languages with two contrastive learning, MLM, and distant label prediction objectives. InfoDCL shows that it effectively learns language representations for understanding SM.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B.2 Zero-Shot Setting On Llms",
      "text": "We also investigate the zero-shot performance on a wide range of LLMs:\n\n(1) BLOOM (Scao et al., 2022) is a Transformer decoder-only model trained on the ROOTS corpus consisting of 46 natural and 13 programming languages. BLOOM uses a multilingual vocabulary with 250K tokens and is trained with auto-regressive language modelling objectives.\n\n(2) Multilingual T5 (mT5)  (Xue et al., 2021 ) is Transformer encoder-decoder model trained on CommonCrawl data involving 101 languages and contains a vocabulary with 250K tokens. It trained with sequence-to-sequence MLM objective.\n\n(3) LLaMA  (Touvron et al., 2023 ) is a Transformer decoder-only model pretrained on 1.4T tokens where the majority are English and a small amount of data in 20 other languages. We utilize LLaMA with 7B parameters and a vocabulary with 30K tokens.\n\n(4) BLOOMZ  (Muennighoff et al., 2022)  is also an instruction finetune model. It further finetunes BLOOM on xP3 corpus that contains 13 type of tasks in 46 languages with English prompt. We benchmark SPARROW on the BLOOM-based models with a size of 7.1B parameters.\n\n(5) BLOOMZ-P3  (Muennighoff et al., 2022 ) is an instruction finetuned model. It is initialized by BLOOM and further finetunes on English-only P3 corpus  (Sanh et al., 2022)  containing 2, 073 natural language prompts for eight types of NLP tasks.\n\n(6) BLOOM-Bactrian  (Li et al., 2023 ) tune BLOOM on a 3.4M instruction-following dataset in 52 languages with low-rank adaptation modules.  Li et al. (2023)  translate the English 67K instructions from Alpaca and Dolly datasets into 51 languages and utilize ChatGPT API to generate responses in the corresponding language.\n\n(7) mT0  (Muennighoff et al., 2022)  is instruction fine-tuned mT5 model with xP3 corpus. We evaluate the mT5-based models with XL size (with 3.7B parameters).\n\n(8) Alpaca  (Taori et al., 2023)  further tune LLaMA on a 52K instruction-following dataset that is generated by gpt-3.5-turbo of Ope-nAI API. The dataset includes diverse English instruction-following tasks, e.g., question answering and programming.\n\n(9) Vicuna (Chiang et al., 2023) further tune LLaMA on 70K diverse user-shared conversations with ChatGPT in English.\n\n(10) ChatGPT is a conversation-based LLM trained  GTP-3 (Brown et al., 2020)  through reinforcement learning with human feedback  (Ouyang et al., 2022; Christiano et al., 2017) . We expolit gpt-3.5-turbo-0301 via OpenAI API. 11",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "C Experiments C.1 Hyperparameters",
      "text": "To be computation friendly, we only tune the peak learning rate of each model in a set of {1e-4, 5e-5, 3e -5, 1e -5} and randomly select 45 datasets for hyper-parameter tuning. We fine-tune a PLM with an arbitrary batch size of 32, sequence length of 128 tokens, and 20 epochs with patience of five epochs based on the model performance on Dev set. We fine-tune each dataset three time with different seeds and identify the best model based on Dev set performance. The best learning rate for each model is identified based on the average score of Dev set of the 45 datasets. The best peak learning rate is 3e-5 for mBERT, XLM-T, and Bernice and 1e -5 for other models.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "C.2 Prompts",
      "text": "The prompts we use in our experiments are summarized in Table  15 .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "C.3 Results",
      "text": "Table  16  shows aggregated performance of finetuned models on Dev and Test-S. We report the average of dataset-specific metrics and standard deviation in a task and a category. We also report the Test-S performance of tasks of antisocial language detection, emotion recognition, humor detection, irony and sarcasm detection, sentiment analysis, and subjectivity analysis in  Tables 17, 18, 19, 20, 21, and 22, respectively.  We provide a concise study to probe the sensitivity of open-source LLMs to prompts and present the results in Table  24",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of SM benchmarks with leader-",
      "page": 1
    },
    {
      "caption": "Figure 1: for a comparison).",
      "page": 3
    },
    {
      "caption": "Figure 2: Examples of prompts used for zero-shot evaluation with lm-evaluation-harness ( yellow ) and",
      "page": 5
    },
    {
      "caption": "Figure 2: , we provide an instruction that compels",
      "page": 5
    },
    {
      "caption": "Figure 3: Confusion matrices of two datasets.",
      "page": 7
    },
    {
      "caption": "Figure 4: A comparison of different models on multiple",
      "page": 8
    },
    {
      "caption": "Figure 4: shows, ChatGPT performs better",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Language-wise model performance for sam-",
      "data": [
        {
          "Antisocial\nDangerours\n42.06\nHate\n43.62\nOffense\n39.48\nH/O-Group\n14.82\nH/O-Target\n20.39\nAS\n35.20": "Emotion\n15.86",
          "67.13\n62.36\n63.57\n65.23\n75.85\n72.97\n74.37\n76.76\n78.88\n77.53\n75.88\n78.45\n51.15\n46.18\n42.39\n50.24\n60.96\n53.16\n57.67\n60.79\n71.14\n66.92\n67.99\n70.61": "69.27\n61.42\n66.87\n68.13",
          "50.84\n46.87\n46.87\n46.87\n46.87\n49.31\n46.87\n46.87\n46.87\n46.87\n46.87\n37.93\n33.68\n66.06\n39.83\n39.44\n37.76\n38.52\n42.23\n23.29\n37.33\n39.05\n37.80\n44.31\n41.59\n58.74\n67.31\n41.06\n40.42\n20.28\n38.59\n40.43\n24.99\n39.90\n21.11\n39.85\n16.82\n48.70\n52.70\n39.66\n13.63\n17.26\n14.23\n21.23\n14.81\n7.02\n16.25\n17.01\n12.35\n14.13\n9.26\n26.74\n35.89\n18.73\n19.03\n18.74\n16.89\n18.77\n6.69\n20.58\n17.99\n19.32\n16.83\n17.01\n28.67\n56.55\n33.70\n32.80\n27.93\n31.97\n33.79\n20.14\n32.02\n28.79\n31.68\n30.55\n34.50\n47.40": "59.58\n9.71\n17.18\n13.85\n15.07\n15.19\n7.75\n27.87\n24.21\n15.14\n31.80\n18.12\n50.85"
        },
        {
          "Antisocial\nDangerours\n42.06\nHate\n43.62\nOffense\n39.48\nH/O-Group\n14.82\nH/O-Target\n20.39\nAS\n35.20": "Humor\n49.65",
          "67.13\n62.36\n63.57\n65.23\n75.85\n72.97\n74.37\n76.76\n78.88\n77.53\n75.88\n78.45\n51.15\n46.18\n42.39\n50.24\n60.96\n53.16\n57.67\n60.79\n71.14\n66.92\n67.99\n70.61": "87.05\n84.35\n85.19\n86.75",
          "50.84\n46.87\n46.87\n46.87\n46.87\n49.31\n46.87\n46.87\n46.87\n46.87\n46.87\n37.93\n33.68\n66.06\n39.83\n39.44\n37.76\n38.52\n42.23\n23.29\n37.33\n39.05\n37.80\n44.31\n41.59\n58.74\n67.31\n41.06\n40.42\n20.28\n38.59\n40.43\n24.99\n39.90\n21.11\n39.85\n16.82\n48.70\n52.70\n39.66\n13.63\n17.26\n14.23\n21.23\n14.81\n7.02\n16.25\n17.01\n12.35\n14.13\n9.26\n26.74\n35.89\n18.73\n19.03\n18.74\n16.89\n18.77\n6.69\n20.58\n17.99\n19.32\n16.83\n17.01\n28.67\n56.55\n33.70\n32.80\n27.93\n31.97\n33.79\n20.14\n32.02\n28.79\n31.68\n30.55\n34.50\n47.40": "72.70\n41.78\n33.12\n33.82\n33.17\n33.04\n35.91\n43.60\n33.12\n39.78\n41.72\n46.19\n71.65"
        },
        {
          "Antisocial\nDangerours\n42.06\nHate\n43.62\nOffense\n39.48\nH/O-Group\n14.82\nH/O-Target\n20.39\nAS\n35.20": "I&S\nIrony\n42.39\nSarcasm\n45.48\nIrony-Type\n22.36\nI&S\n42.93",
          "67.13\n62.36\n63.57\n65.23\n75.85\n72.97\n74.37\n76.76\n78.88\n77.53\n75.88\n78.45\n51.15\n46.18\n42.39\n50.24\n60.96\n53.16\n57.67\n60.79\n71.14\n66.92\n67.99\n70.61": "68.38\n64.24\n65.53\n66.88\n74.94\n72.41\n73.40\n74.78\n57.58\n47.35\n46.43\n56.04\n71.12\n67.48\n68.51\n70.29",
          "50.84\n46.87\n46.87\n46.87\n46.87\n49.31\n46.87\n46.87\n46.87\n46.87\n46.87\n37.93\n33.68\n66.06\n39.83\n39.44\n37.76\n38.52\n42.23\n23.29\n37.33\n39.05\n37.80\n44.31\n41.59\n58.74\n67.31\n41.06\n40.42\n20.28\n38.59\n40.43\n24.99\n39.90\n21.11\n39.85\n16.82\n48.70\n52.70\n39.66\n13.63\n17.26\n14.23\n21.23\n14.81\n7.02\n16.25\n17.01\n12.35\n14.13\n9.26\n26.74\n35.89\n18.73\n19.03\n18.74\n16.89\n18.77\n6.69\n20.58\n17.99\n19.32\n16.83\n17.01\n28.67\n56.55\n33.70\n32.80\n27.93\n31.97\n33.79\n20.14\n32.02\n28.79\n31.68\n30.55\n34.50\n47.40": "58.23\n36.63\n35.15\n38.69\n44.46\n36.18\n36.52\n34.69\n33.99\n40.78\n27.49\n47.48\n56.24\n65.55\n43.00\n41.62\n32.23\n32.22\n41.68\n46.34\n36.09\n41.62\n41.17\n32.48\n47.67\n65.34\n30.81\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n30.81\n60.41\n38.92\n37.57\n34.46\n41.79\n40.39\n35.42\n37.36\n32.35\n39.87\n29.56\n46.14\n59.63"
        },
        {
          "Antisocial\nDangerours\n42.06\nHate\n43.62\nOffense\n39.48\nH/O-Group\n14.82\nH/O-Target\n20.39\nAS\n35.20": "Sentiment\n34.68",
          "67.13\n62.36\n63.57\n65.23\n75.85\n72.97\n74.37\n76.76\n78.88\n77.53\n75.88\n78.45\n51.15\n46.18\n42.39\n50.24\n60.96\n53.16\n57.67\n60.79\n71.14\n66.92\n67.99\n70.61": "71.64\n66.34\n69.58\n70.44",
          "50.84\n46.87\n46.87\n46.87\n46.87\n49.31\n46.87\n46.87\n46.87\n46.87\n46.87\n37.93\n33.68\n66.06\n39.83\n39.44\n37.76\n38.52\n42.23\n23.29\n37.33\n39.05\n37.80\n44.31\n41.59\n58.74\n67.31\n41.06\n40.42\n20.28\n38.59\n40.43\n24.99\n39.90\n21.11\n39.85\n16.82\n48.70\n52.70\n39.66\n13.63\n17.26\n14.23\n21.23\n14.81\n7.02\n16.25\n17.01\n12.35\n14.13\n9.26\n26.74\n35.89\n18.73\n19.03\n18.74\n16.89\n18.77\n6.69\n20.58\n17.99\n19.32\n16.83\n17.01\n28.67\n56.55\n33.70\n32.80\n27.93\n31.97\n33.79\n20.14\n32.02\n28.79\n31.68\n30.55\n34.50\n47.40": "60.34\n26.67\n39.03\n28.61\n43.03\n28.46\n20.77\n34.65\n32.76\n27.55\n25.84\n25.02\n54.94"
        },
        {
          "Antisocial\nDangerours\n42.06\nHate\n43.62\nOffense\n39.48\nH/O-Group\n14.82\nH/O-Target\n20.39\nAS\n35.20": "Subjectivity\n41.41",
          "67.13\n62.36\n63.57\n65.23\n75.85\n72.97\n74.37\n76.76\n78.88\n77.53\n75.88\n78.45\n51.15\n46.18\n42.39\n50.24\n60.96\n53.16\n57.67\n60.79\n71.14\n66.92\n67.99\n70.61": "75.73\n72.54\n74.45\n74.80",
          "50.84\n46.87\n46.87\n46.87\n46.87\n49.31\n46.87\n46.87\n46.87\n46.87\n46.87\n37.93\n33.68\n66.06\n39.83\n39.44\n37.76\n38.52\n42.23\n23.29\n37.33\n39.05\n37.80\n44.31\n41.59\n58.74\n67.31\n41.06\n40.42\n20.28\n38.59\n40.43\n24.99\n39.90\n21.11\n39.85\n16.82\n48.70\n52.70\n39.66\n13.63\n17.26\n14.23\n21.23\n14.81\n7.02\n16.25\n17.01\n12.35\n14.13\n9.26\n26.74\n35.89\n18.73\n19.03\n18.74\n16.89\n18.77\n6.69\n20.58\n17.99\n19.32\n16.83\n17.01\n28.67\n56.55\n33.70\n32.80\n27.93\n31.97\n33.79\n20.14\n32.02\n28.79\n31.68\n30.55\n34.50\n47.40": "66.26\n44.12\n29.45\n30.69\n30.73\n39.65\n37.35\n41.64\n36.16\n42.30\n30.44\n38.73\n59.33"
        },
        {
          "Antisocial\nDangerours\n42.06\nHate\n43.62\nOffense\n39.48\nH/O-Group\n14.82\nH/O-Target\n20.39\nAS\n35.20": "SPARROW\n33.47",
          "67.13\n62.36\n63.57\n65.23\n75.85\n72.97\n74.37\n76.76\n78.88\n77.53\n75.88\n78.45\n51.15\n46.18\n42.39\n50.24\n60.96\n53.16\n57.67\n60.79\n71.14\n66.92\n67.99\n70.61": "71.60\n66.60\n69.38\n70.85",
          "50.84\n46.87\n46.87\n46.87\n46.87\n49.31\n46.87\n46.87\n46.87\n46.87\n46.87\n37.93\n33.68\n66.06\n39.83\n39.44\n37.76\n38.52\n42.23\n23.29\n37.33\n39.05\n37.80\n44.31\n41.59\n58.74\n67.31\n41.06\n40.42\n20.28\n38.59\n40.43\n24.99\n39.90\n21.11\n39.85\n16.82\n48.70\n52.70\n39.66\n13.63\n17.26\n14.23\n21.23\n14.81\n7.02\n16.25\n17.01\n12.35\n14.13\n9.26\n26.74\n35.89\n18.73\n19.03\n18.74\n16.89\n18.77\n6.69\n20.58\n17.99\n19.32\n16.83\n17.01\n28.67\n56.55\n33.70\n32.80\n27.93\n31.97\n33.79\n20.14\n32.02\n28.79\n31.68\n30.55\n34.50\n47.40": "60.04\n27.94\n33.79\n27.17\n35.70\n29.45\n21.45\n33.63\n30.85\n28.75\n28.79\n29.36\n53.90"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: shows, we compare three-shot and five-shot re-",
      "data": [
        {
          "Antisocial\n49.31\nDangerours\n46.87\n46.87\n46.87\n46.87\n46.87\nHate\n39.83\n38.52\n23.29\n37.33\n37.80\n41.59\nOffense\n41.06\n38.59\n24.99\n39.90\n39.85\n48.70\n21.23\nH/O-Group\n13.63\n7.02\n16.25\n12.35\n9.26\nH/O-Target\n18.73\n16.89\n6.69\n20.58\n19.32\n17.01\nAS\n33.70\n31.97\n20.14\n32.02\n31.68\n34.50": "Emotion\n9.71\n15.07\n7.75\n27.87\n15.14\n18.12",
          "46.87\n46.87\n45.68\n46.87\n46.87\n46.87\n49.17\n38.83\n38.30\n39.43\n37.82\n43.51\n54.49\n43.94\n40.25\n21.99\n41.53\n46.36\n11.43\n13.04\n7.92\n15.98\n11.81\n14.19\n24.84\n17.09\n18.41\n10.48\n16.55\n20.56\n41.69\n33.14\n32.55\n27.19\n32.36\n36.42": "17.08\n12.17\n10.66\n23.12\n32.12\n40.28",
          "46.87\n46.87\n48.91\n46.87\n46.87\n46.87\n37.95\n37.14\n39.53\n37.70\n41.87\n48.37\n42.42\n40.59\n34.10\n41.67\n43.83\n51.72\n9.68\n11.76\n7.23\n14.50\n12.58\n16.27\n17.44\n17.45\n9.32\n16.56\n20.09\n23.60\n32.43\n31.88\n29.17\n32.09\n35.35\n40.99": "41.79\n18.48\n12.35\n10.07\n25.57\n34.20"
        },
        {
          "Antisocial\n49.31\nDangerours\n46.87\n46.87\n46.87\n46.87\n46.87\nHate\n39.83\n38.52\n23.29\n37.33\n37.80\n41.59\nOffense\n41.06\n38.59\n24.99\n39.90\n39.85\n48.70\n21.23\nH/O-Group\n13.63\n7.02\n16.25\n12.35\n9.26\nH/O-Target\n18.73\n16.89\n6.69\n20.58\n19.32\n17.01\nAS\n33.70\n31.97\n20.14\n32.02\n31.68\n34.50": "Humor\n41.78\n33.04\n43.60\n33.12\n39.78\n46.19",
          "46.87\n46.87\n45.68\n46.87\n46.87\n46.87\n49.17\n38.83\n38.30\n39.43\n37.82\n43.51\n54.49\n43.94\n40.25\n21.99\n41.53\n46.36\n11.43\n13.04\n7.92\n15.98\n11.81\n14.19\n24.84\n17.09\n18.41\n10.48\n16.55\n20.56\n41.69\n33.14\n32.55\n27.19\n32.36\n36.42": "33.67\n33.12\n44.70\n38.19\n55.20\n57.15",
          "46.87\n46.87\n48.91\n46.87\n46.87\n46.87\n37.95\n37.14\n39.53\n37.70\n41.87\n48.37\n42.42\n40.59\n34.10\n41.67\n43.83\n51.72\n9.68\n11.76\n7.23\n14.50\n12.58\n16.27\n17.44\n17.45\n9.32\n16.56\n20.09\n23.60\n32.43\n31.88\n29.17\n32.09\n35.35\n40.99": "58.75\n34.06\n33.12\n40.20\n37.08\n53.86"
        },
        {
          "Antisocial\n49.31\nDangerours\n46.87\n46.87\n46.87\n46.87\n46.87\nHate\n39.83\n38.52\n23.29\n37.33\n37.80\n41.59\nOffense\n41.06\n38.59\n24.99\n39.90\n39.85\n48.70\n21.23\nH/O-Group\n13.63\n7.02\n16.25\n12.35\n9.26\nH/O-Target\n18.73\n16.89\n6.69\n20.58\n19.32\n17.01\nAS\n33.70\n31.97\n20.14\n32.02\n31.68\n34.50": "I&S\n47.48\nIrony\n36.63\n44.46\n36.52\n34.69\n40.78\nSarcasm\n43.00\n41.68\n36.09\n41.62\n41.17\n47.67\nIrony-Type\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\nI&S\n46.14\n38.92\n41.79\n35.42\n37.36\n39.87",
          "46.87\n46.87\n45.68\n46.87\n46.87\n46.87\n49.17\n38.83\n38.30\n39.43\n37.82\n43.51\n54.49\n43.94\n40.25\n21.99\n41.53\n46.36\n11.43\n13.04\n7.92\n15.98\n11.81\n14.19\n24.84\n17.09\n18.41\n10.48\n16.55\n20.56\n41.69\n33.14\n32.55\n27.19\n32.36\n36.42": "42.21\n41.58\n42.61\n35.18\n36.76\n39.78\n52.55\n46.14\n42.91\n46.72\n48.42\n49.75\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n43.01\n41.11\n43.48\n40.98\n42.36\n45.12",
          "46.87\n46.87\n48.91\n46.87\n46.87\n46.87\n37.95\n37.14\n39.53\n37.70\n41.87\n48.37\n42.42\n40.59\n34.10\n41.67\n43.83\n51.72\n9.68\n11.76\n7.23\n14.50\n12.58\n16.27\n17.44\n17.45\n9.32\n16.56\n20.09\n23.60\n32.43\n31.88\n29.17\n32.09\n35.35\n40.99": "44.34\n44.14\n39.67\n34.82\n38.40\n41.61\n45.43\n43.05\n45.75\n39.88\n49.03\n52.51\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n43.61\n42.33\n41.67\n36.55\n42.74\n45.92"
        },
        {
          "Antisocial\n49.31\nDangerours\n46.87\n46.87\n46.87\n46.87\n46.87\nHate\n39.83\n38.52\n23.29\n37.33\n37.80\n41.59\nOffense\n41.06\n38.59\n24.99\n39.90\n39.85\n48.70\n21.23\nH/O-Group\n13.63\n7.02\n16.25\n12.35\n9.26\nH/O-Target\n18.73\n16.89\n6.69\n20.58\n19.32\n17.01\nAS\n33.70\n31.97\n20.14\n32.02\n31.68\n34.50": "Sentiment\n43.03\n26.67\n20.77\n34.65\n27.55\n25.02",
          "46.87\n46.87\n45.68\n46.87\n46.87\n46.87\n49.17\n38.83\n38.30\n39.43\n37.82\n43.51\n54.49\n43.94\n40.25\n21.99\n41.53\n46.36\n11.43\n13.04\n7.92\n15.98\n11.81\n14.19\n24.84\n17.09\n18.41\n10.48\n16.55\n20.56\n41.69\n33.14\n32.55\n27.19\n32.36\n36.42": "34.81\n35.79\n24.76\n31.37\n37.73\n34.53",
          "46.87\n46.87\n48.91\n46.87\n46.87\n46.87\n37.95\n37.14\n39.53\n37.70\n41.87\n48.37\n42.42\n40.59\n34.10\n41.67\n43.83\n51.72\n9.68\n11.76\n7.23\n14.50\n12.58\n16.27\n17.44\n17.45\n9.32\n16.56\n20.09\n23.60\n32.43\n31.88\n29.17\n32.09\n35.35\n40.99": "33.15\n37.71\n23.17\n29.25\n40.88\n39.37"
        },
        {
          "Antisocial\n49.31\nDangerours\n46.87\n46.87\n46.87\n46.87\n46.87\nHate\n39.83\n38.52\n23.29\n37.33\n37.80\n41.59\nOffense\n41.06\n38.59\n24.99\n39.90\n39.85\n48.70\n21.23\nH/O-Group\n13.63\n7.02\n16.25\n12.35\n9.26\nH/O-Target\n18.73\n16.89\n6.69\n20.58\n19.32\n17.01\nAS\n33.70\n31.97\n20.14\n32.02\n31.68\n34.50": "Subjectivity\n44.12\n30.73\n37.35\n41.64\n42.30\n38.73",
          "46.87\n46.87\n45.68\n46.87\n46.87\n46.87\n49.17\n38.83\n38.30\n39.43\n37.82\n43.51\n54.49\n43.94\n40.25\n21.99\n41.53\n46.36\n11.43\n13.04\n7.92\n15.98\n11.81\n14.19\n24.84\n17.09\n18.41\n10.48\n16.55\n20.56\n41.69\n33.14\n32.55\n27.19\n32.36\n36.42": "36.50\n30.66\n37.11\n34.36\n44.20\n54.77",
          "46.87\n46.87\n48.91\n46.87\n46.87\n46.87\n37.95\n37.14\n39.53\n37.70\n41.87\n48.37\n42.42\n40.59\n34.10\n41.67\n43.83\n51.72\n9.68\n11.76\n7.23\n14.50\n12.58\n16.27\n17.44\n17.45\n9.32\n16.56\n20.09\n23.60\n32.43\n31.88\n29.17\n32.09\n35.35\n40.99": "56.15\n33.70\n30.72\n39.36\n31.42\n46.53"
        },
        {
          "Antisocial\n49.31\nDangerours\n46.87\n46.87\n46.87\n46.87\n46.87\nHate\n39.83\n38.52\n23.29\n37.33\n37.80\n41.59\nOffense\n41.06\n38.59\n24.99\n39.90\n39.85\n48.70\n21.23\nH/O-Group\n13.63\n7.02\n16.25\n12.35\n9.26\nH/O-Target\n18.73\n16.89\n6.69\n20.58\n19.32\n17.01\nAS\n33.70\n31.97\n20.14\n32.02\n31.68\n34.50": "SPARROW\n27.94\n35.70\n21.45\n33.63\n28.75\n29.36",
          "46.87\n46.87\n45.68\n46.87\n46.87\n46.87\n49.17\n38.83\n38.30\n39.43\n37.82\n43.51\n54.49\n43.94\n40.25\n21.99\n41.53\n46.36\n11.43\n13.04\n7.92\n15.98\n11.81\n14.19\n24.84\n17.09\n18.41\n10.48\n16.55\n20.56\n41.69\n33.14\n32.55\n27.19\n32.36\n36.42": "32.76\n31.91\n26.12\n31.71\n37.82\n39.44",
          "46.87\n46.87\n48.91\n46.87\n46.87\n46.87\n37.95\n37.14\n39.53\n37.70\n41.87\n48.37\n42.42\n40.59\n34.10\n41.67\n43.83\n51.72\n9.68\n11.76\n7.23\n14.50\n12.58\n16.27\n17.44\n17.45\n9.32\n16.56\n20.09\n23.60\n32.43\n31.88\n29.17\n32.09\n35.35\n40.99": "41.97\n32.03\n32.84\n25.47\n30.44\n39.48"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 16: Performance of finetuned models on Dev and Test-S set. We finetune each model on each dataset for",
      "data": [
        {
          "Aggressive\n73.39±0.30\n73.92±0.50\n76.79±0.52\n76.09±0.38\nDangerours\n69.76±1.56\n69.53±1.04\n74.92±1.18\n73.42±0.80\nHate\n77.73±0.76\n79.40±0.85\n81.16±1.15\n80.62±0.60\nAntisocial\nOffense\n78.96±1.04\n80.21±0.92\n82.15±0.68\n81.55±0.46\nH/O-Group\n51.57±2.35\n41.24±3.16\n48.30±1.90\n46.05±2.25\nH/O-Target\n54.02±3.60\n59.23±1.71\n60.83±1.26\n60.14±1.21\nAS\n70.18±1.59\n71.47±1.24\n73.80±1.13\n73.04±0.85": "Emotion\n62.30±0.90\n67.43±0.68\n68.56±0.85\n69.34±0.53",
          "72.71±1.92\n74.64±0.14\n75.45±0.73\n73.96±0.91\n62.36±1.08\n63.57±1.15\n67.13±0.56\n65.23±1.60\n72.97±1.40\n74.37±1.48\n76.76±2.43\n75.85±0.90\n77.53±1.27\n75.88±2.43\n78.45±1.89\n78.88±2.63\n46.18±4.10\n42.39±3.30\n51.15±2.01\n50.24±4.19\n53.16±4.49\n57.67±1.79\n60.96±2.26\n60.79±1.38\n66.92±2.29\n67.99±1.84\n71.14±2.15\n70.61±1.64": "61.42±1.51\n66.87±0.99\n68.13±1.28\n69.27±1.03"
        },
        {
          "Aggressive\n73.39±0.30\n73.92±0.50\n76.79±0.52\n76.09±0.38\nDangerours\n69.76±1.56\n69.53±1.04\n74.92±1.18\n73.42±0.80\nHate\n77.73±0.76\n79.40±0.85\n81.16±1.15\n80.62±0.60\nAntisocial\nOffense\n78.96±1.04\n80.21±0.92\n82.15±0.68\n81.55±0.46\nH/O-Group\n51.57±2.35\n41.24±3.16\n48.30±1.90\n46.05±2.25\nH/O-Target\n54.02±3.60\n59.23±1.71\n60.83±1.26\n60.14±1.21\nAS\n70.18±1.59\n71.47±1.24\n73.80±1.13\n73.04±0.85": "Humor\n85.15±0.32\n85.83±0.57\n86.72±0.63\n86.74±0.42",
          "72.71±1.92\n74.64±0.14\n75.45±0.73\n73.96±0.91\n62.36±1.08\n63.57±1.15\n67.13±0.56\n65.23±1.60\n72.97±1.40\n74.37±1.48\n76.76±2.43\n75.85±0.90\n77.53±1.27\n75.88±2.43\n78.45±1.89\n78.88±2.63\n46.18±4.10\n42.39±3.30\n51.15±2.01\n50.24±4.19\n53.16±4.49\n57.67±1.79\n60.96±2.26\n60.79±1.38\n66.92±2.29\n67.99±1.84\n71.14±2.15\n70.61±1.64": "84.35±1.23\n85.19±1.62\n86.75±1.13\n87.05±0.82"
        },
        {
          "Aggressive\n73.39±0.30\n73.92±0.50\n76.79±0.52\n76.09±0.38\nDangerours\n69.76±1.56\n69.53±1.04\n74.92±1.18\n73.42±0.80\nHate\n77.73±0.76\n79.40±0.85\n81.16±1.15\n80.62±0.60\nAntisocial\nOffense\n78.96±1.04\n80.21±0.92\n82.15±0.68\n81.55±0.46\nH/O-Group\n51.57±2.35\n41.24±3.16\n48.30±1.90\n46.05±2.25\nH/O-Target\n54.02±3.60\n59.23±1.71\n60.83±1.26\n60.14±1.21\nAS\n70.18±1.59\n71.47±1.24\n73.80±1.13\n73.04±0.85": "Irony\n69.16±1.29\n69.94±1.02\n72.19±1.24\n71.12±0.72\nSarcasm\n74.65±1.14\n75.64±1.92\n77.82±1.56\n77.21±1.05\nI&S\nIrony-Type\n53.51±2.11\n52.18±2.15\n58.55±3.17\n57.72±2.92\nI&S\n71.13±1.26\n71.90±1.52\n74.32±1.50\n73.49±1.00",
          "72.71±1.92\n74.64±0.14\n75.45±0.73\n73.96±0.91\n62.36±1.08\n63.57±1.15\n67.13±0.56\n65.23±1.60\n72.97±1.40\n74.37±1.48\n76.76±2.43\n75.85±0.90\n77.53±1.27\n75.88±2.43\n78.45±1.89\n78.88±2.63\n46.18±4.10\n42.39±3.30\n51.15±2.01\n50.24±4.19\n53.16±4.49\n57.67±1.79\n60.96±2.26\n60.79±1.38\n66.92±2.29\n67.99±1.84\n71.14±2.15\n70.61±1.64": "64.24±1.16\n65.53±1.57\n66.88±1.23\n68.38±1.00\n72.41±1.38\n73.40±2.42\n74.78±1.69\n74.94±1.13\n47.35±1.89\n46.43±0.63\n56.04±1.87\n57.58±1.42\n67.48±1.31\n68.51±1.95\n70.29±1.49\n71.12±1.09"
        },
        {
          "Aggressive\n73.39±0.30\n73.92±0.50\n76.79±0.52\n76.09±0.38\nDangerours\n69.76±1.56\n69.53±1.04\n74.92±1.18\n73.42±0.80\nHate\n77.73±0.76\n79.40±0.85\n81.16±1.15\n80.62±0.60\nAntisocial\nOffense\n78.96±1.04\n80.21±0.92\n82.15±0.68\n81.55±0.46\nH/O-Group\n51.57±2.35\n41.24±3.16\n48.30±1.90\n46.05±2.25\nH/O-Target\n54.02±3.60\n59.23±1.71\n60.83±1.26\n60.14±1.21\nAS\n70.18±1.59\n71.47±1.24\n73.80±1.13\n73.04±0.85": "Sentiment\n69.29±1.14\n71.34±0.78\n72.95±0.88\n73.81±0.70",
          "72.71±1.92\n74.64±0.14\n75.45±0.73\n73.96±0.91\n62.36±1.08\n63.57±1.15\n67.13±0.56\n65.23±1.60\n72.97±1.40\n74.37±1.48\n76.76±2.43\n75.85±0.90\n77.53±1.27\n75.88±2.43\n78.45±1.89\n78.88±2.63\n46.18±4.10\n42.39±3.30\n51.15±2.01\n50.24±4.19\n53.16±4.49\n57.67±1.79\n60.96±2.26\n60.79±1.38\n66.92±2.29\n67.99±1.84\n71.14±2.15\n70.61±1.64": "66.34±1.92\n69.58±1.41\n70.44±1.61\n71.64±1.31"
        },
        {
          "Aggressive\n73.39±0.30\n73.92±0.50\n76.79±0.52\n76.09±0.38\nDangerours\n69.76±1.56\n69.53±1.04\n74.92±1.18\n73.42±0.80\nHate\n77.73±0.76\n79.40±0.85\n81.16±1.15\n80.62±0.60\nAntisocial\nOffense\n78.96±1.04\n80.21±0.92\n82.15±0.68\n81.55±0.46\nH/O-Group\n51.57±2.35\n41.24±3.16\n48.30±1.90\n46.05±2.25\nH/O-Target\n54.02±3.60\n59.23±1.71\n60.83±1.26\n60.14±1.21\nAS\n70.18±1.59\n71.47±1.24\n73.80±1.13\n73.04±0.85": "Subjectivity\n75.18±0.63\n77.28±0.74\n76.97±0.69\n77.78±0.87",
          "72.71±1.92\n74.64±0.14\n75.45±0.73\n73.96±0.91\n62.36±1.08\n63.57±1.15\n67.13±0.56\n65.23±1.60\n72.97±1.40\n74.37±1.48\n76.76±2.43\n75.85±0.90\n77.53±1.27\n75.88±2.43\n78.45±1.89\n78.88±2.63\n46.18±4.10\n42.39±3.30\n51.15±2.01\n50.24±4.19\n53.16±4.49\n57.67±1.79\n60.96±2.26\n60.79±1.38\n66.92±2.29\n67.99±1.84\n71.14±2.15\n70.61±1.64": "72.54±1.46\n74.45±1.18\n74.80±1.08\n75.73±1.33"
        },
        {
          "Aggressive\n73.39±0.30\n73.92±0.50\n76.79±0.52\n76.09±0.38\nDangerours\n69.76±1.56\n69.53±1.04\n74.92±1.18\n73.42±0.80\nHate\n77.73±0.76\n79.40±0.85\n81.16±1.15\n80.62±0.60\nAntisocial\nOffense\n78.96±1.04\n80.21±0.92\n82.15±0.68\n81.55±0.46\nH/O-Group\n51.57±2.35\n41.24±3.16\n48.30±1.90\n46.05±2.25\nH/O-Target\n54.02±3.60\n59.23±1.71\n60.83±1.26\n60.14±1.21\nAS\n70.18±1.59\n71.47±1.24\n73.80±1.13\n73.04±0.85": "SM\n69.29±1.17\n71.50±0.93\n73.16±0.98\n73.46±0.72",
          "72.71±1.92\n74.64±0.14\n75.45±0.73\n73.96±0.91\n62.36±1.08\n63.57±1.15\n67.13±0.56\n65.23±1.60\n72.97±1.40\n74.37±1.48\n76.76±2.43\n75.85±0.90\n77.53±1.27\n75.88±2.43\n78.45±1.89\n78.88±2.63\n46.18±4.10\n42.39±3.30\n51.15±2.01\n50.24±4.19\n53.16±4.49\n57.67±1.79\n60.96±2.26\n60.79±1.38\n66.92±2.29\n67.99±1.84\n71.14±2.15\n70.61±1.64": "66.60±1.83\n69.38±1.51\n70.85±1.63\n71.60±1.30"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table 16: Performance of finetuned models on Dev and Test-S set. We finetune each model on each dataset for",
      "data": [
        {
          "M-F1\n43.14\nAggr-hinKum\nM-F1\n42.06\nDang-araAls\nW-F1\n41.57\nHate-engWas\nW-F1\n38.97\nHate-engDav\nM-F1\n43.70\nHate-araAla\nM-F1\n47.00\nHate-itaBos\nM-F1\n52.37\nHate-filCab\nM-F1\n29.56\nHate-araMul\nM-F1\n52.61\nHate-engBas\nM-F1\n44.59\nHate-spaBas\nM-F1\n47.84\nHate-porFor\nM-F1\n44.15\nHate-polPta\nM-F1\n36.74\nHate-korMoo\nM-F1\n37.84\nHate-araMub\nM-F1\n48.72\nHate-zhoDen\nM-F1\n51.96\nHate-korJeo\nM-F1\n34.32\nHate-telMar\nM-F1\n46.05\nSexi-fraChi\nM-F1\n44.20\nOffe-engZam\nM-F1\n45.64\nOffe-araZam\nM-F1\n41.14\nOffe-danZam\nM-F1\n41.24\nOffe-ellZam\nM-F1\n41.11\nOffe-turZam\nM-F1\n46.28\nOffe-araMub\nM-F1\n16.75\nOffe-slvNov\n31.22\nOffe-G-engZam M-F1\nM-F1\n6.89\nHate-G-araOus\nM-F1\n6.36\nHate-G-fraOus\nM-F1\n39.77\nOffe-T-engZam\nM-F1\n21.42\nHate-T-araOus\nM-F1\n14.85\nHate-T-fraOus\nM-F1\n22.89\nHate-T-benKar\nM-F1\n14.73\nOffe-T-kanCha\nM-F1\n12.71\nOffe-T-malCha\nM-F1\n14.28\nOffe-T-tamCha\nM-F1\n22.51\nHate-T-korJeo": "Average\n—\n35.20",
          "75.45\n72.71\n74.64\n73.96\n67.13\n62.36\n63.57\n65.23\n89.33\n87.74\n88.79\n88.93\n92.96\n91.28\n91.99\n91.12\n85.95\n82.07\n81.23\n83.99\n81.17\n75.67\n76.96\n80.19\n79.50\n74.38\n78.40\n79.01\n76.53\n69.38\n67.12\n71.74\n50.24\n51.87\n54.17\n53.25\n78.20\n74.18\n76.26\n76.96\n74.40\n70.08\n70.02\n73.22\n69.69\n70.26\n71.68\n71.23\n64.80\n57.10\n63.17\n63.09\n82.00\n73.92\n79.67\n81.16\n84.86\n83.29\n83.36\n84.39\n80.19\n79.03\n78.50\n79.32\n58.22\n49.90\n49.78\n49.75\n81.99\n79.60\n81.01\n79.96\n78.67\n75.07\n75.10\n77.75\n91.55\n86.27\n86.88\n89.53\n82.09\n77.53\n76.11\n78.03\n80.64\n76.91\n79.63\n79.13\n77.07\n73.11\n76.90\n76.08\n93.16\n86.85\n86.44\n91.40\n63.23\n52.83\n51.98\n55.29\n61.93\n54.49\n55.44\n61.45\n52.38\n46.71\n37.11\n51.08\n39.13\n37.35\n34.61\n38.19\n78.16\n63.61\n64.04\n72.80\n53.84\n48.32\n52.94\n52.96\n48.18\n45.88\n43.39\n43.30\n86.30\n83.56\n85.31\n85.84\n46.47\n42.13\n38.78\n40.65\n81.53\n42.12\n74.22\n76.51\n39.98\n38.70\n36.25\n39.76\n66.43\n60.98\n64.47\n63.25": "71.14\n66.92\n67.99\n70.61",
          "51.06\n15.82\n15.82\n18.72\n16.37\n53.67\n15.82\n22.00\n18.31\n49.29\n25.07\n46.87\n46.87\n50.84\n46.87\n46.87\n49.31\n46.87\n46.87\n46.87\n46.87\n46.87\n58.26\n65.96\n65.96\n60.07\n60.17\n18.66\n59.75\n59.75\n60.42\n37.33\n61.66\n7.99\n10.78\n10.78\n25.43\n9.32\n13.36\n8.33\n8.33\n8.33\n73.10\n55.89\n54.28\n43.63\n43.63\n43.63\n52.08\n24.11\n43.63\n43.63\n43.63\n43.50\n43.63\n44.63\n40.26\n40.26\n40.26\n45.69\n25.55\n40.26\n40.26\n40.26\n42.34\n40.26\n46.79\n34.47\n45.46\n34.47\n46.80\n32.93\n34.47\n57.76\n34.47\n47.00\n34.47\n15.62\n25.46\n25.46\n37.90\n19.22\n14.50\n25.46\n25.46\n25.46\n17.16\n30.99\n51.11\n36.22\n36.22\n36.22\n52.86\n29.97\n36.22\n36.22\n36.65\n53.15\n37.96\n46.61\n37.50\n29.21\n37.50\n48.68\n31.06\n37.50\n37.50\n38.95\n59.16\n37.50\n48.01\n39.69\n39.90\n39.69\n51.53\n29.73\n39.69\n39.69\n40.09\n55.27\n39.69\n47.77\n46.47\n46.47\n46.47\n48.05\n12.41\n46.47\n46.47\n47.46\n53.11\n46.47\n18.03\n16.90\n21.94\n20.95\n16.90\n15.88\n16.90\n20.30\n16.90\n27.59\n16.90\n35.69\n48.67\n48.67\n48.67\n34.08\n8.36\n48.67\n48.67\n48.67\n48.51\n48.67\n69.31\n36.22\n30.17\n36.22\n60.96\n30.56\n36.22\n38.13\n36.73\n36.22\n36.22\n35.55\n33.69\n33.69\n33.69\n34.75\n35.77\n33.69\n34.05\n33.87\n40.06\n33.69\n32.67\n49.90\n49.90\n49.90\n48.19\n0.60\n49.90\n49.90\n49.90\n49.90\n49.90\n24.92\n65.25\n36.45\n25.26\n46.40\n49.17\n40.05\n38.65\n42.95\n25.60\n51.53\n42.06\n42.06\n42.06\n42.62\n42.06\n23.97\n42.06\n42.06\n42.06\n25.57\n59.67\n44.07\n44.07\n17.49\n44.07\n44.07\n27.25\n44.07\n17.49\n44.07\n17.49\n58.47\n46.68\n46.68\n11.08\n46.68\n46.68\n20.35\n46.68\n11.08\n46.68\n11.46\n51.84\n45.47\n45.47\n14.24\n46.71\n45.47\n33.99\n45.47\n14.24\n45.47\n14.24\n48.21\n44.38\n44.38\n19.92\n44.38\n44.38\n38.31\n44.38\n44.38\n44.38\n16.81\n51.12\n43.69\n43.69\n18.30\n43.69\n43.63\n22.86\n43.69\n18.30\n43.69\n18.59\n58.96\n21.02\n16.60\n18.88\n1.98\n16.68\n8.19\n12.93\n0.20\n12.57\n13.57\n12.59\n28.72\n36.00\n36.00\n44.00\n25.66\n19.73\n26.95\n26.95\n31.46\n30.73\n20.68\n3.87\n7.21\n0.07\n8.41\n7.55\n1.32\n14.83\n16.67\n0.00\n0.27\n1.43\n8.29\n8.57\n6.61\n11.28\n11.23\n0.00\n6.98\n7.42\n5.61\n11.39\n5.66\n47.02\n47.02\n47.02\n47.02\n47.63\n10.62\n22.75\n22.75\n58.67\n39.89\n37.48\n10.15\n19.64\n8.65\n15.58\n14.25\n16.76\n30.19\n11.07\n6.48\n7.64\n6.48\n4.11\n8.07\n19.58\n7.92\n2.11\n2.08\n18.84\n0.22\n2.08\n2.26\n3.15\n11.12\n15.52\n22.29\n7.53\n12.98\n12.11\n23.15\n42.74\n18.48\n12.76\n23.42\n16.69\n16.69\n9.50\n16.61\n16.69\n4.27\n8.09\n6.51\n20.01\n15.44\n14.16\n24.04\n24.04\n24.04\n20.81\n24.04\n1.17\n26.80\n24.04\n24.04\n24.04\n24.04\n17.32\n17.35\n4.39\n15.76\n17.35\n2.26\n17.90\n17.82\n17.22\n17.34\n17.35\n19.37\n3.87\n14.46\n3.87\n15.14\n4.28\n16.91\n18.80\n7.56\n15.27\n9.96": "33.70\n32.80\n27.93\n31.97\n33.79\n20.14\n32.02\n28.79\n31.68\n30.55\n34.50",
          "63.53\n54.36\n70.00 Kumar et al. (2018)\n37.93\n33.68\n59.60 Alshehri et al. (2020)\n79.98\n79.98\n73.62 Waseem and Hovy (2016)\n69.58\n69.58\n90.00 Davidson et al. (2017)\n63.96\n52.85\n-\n—\n77.98\n57.62\n79.93\nBosco et al. (2018)\nCabasag et al. (2019)\n69.13\n66.67\n71.12\n61.13\n32.01\n89.60 Mulki et al. (2019)\n63.69\n63.69\n65.10\nBasile et al. (2019)\n64.93\n54.43\n73.00\nBasile et al. (2019)\n68.45\n63.34\n72.00\nFortuna et al. (2019)\n77.02\n64.96\n50.30\nRybak et al. (2020)\n39.79\n46.90\n63.30 Moon et al. (2020)\n60.72\n52.04\n84.79 Abdul-Mageed et al. (2021)\n72.34\n74.24\n81.00 Deng et al. (2022)\n63.61\n57.09\n77.20\nJeong et al. (2022)\n49.90\n34.23\n60.00 Marreddy et al. (2022)\n74.81\n70.29\n76.20\nChiril et al. (2020)\nZampieri et al. (2019)\n67.90\n67.90\n82.90\n82.01\n67.52\n90.17\nZampieri et al. (2020)\n66.91\n66.79\n81.19\nZampieri et al. (2020)\n60.94\n34.98\n85.22\nZampieri et al. (2020)\n75.03\n45.68\n82.58\nZampieri et al. (2020)\n84.48\n69.36\n90.50 Mubarak et al. (2020)\n33.86\n16.67\n-\n—\n51.52\n51.52\n75.50\nZampieri et al. (2019)\n35.05\n10.90\n40.00 Ousidhoum et al. (2019)\n32.41\n17.79\n37.00 Ousidhoum et al. (2019)\n45.97\n45.97\n66.00\nZampieri et al. (2019)\n44.53\n40.74\n63.00 Ousidhoum et al. (2019)\n39.98\n41.14\n43.00 Ousidhoum et al. (2019)\n49.03\n46.60\n87.00 Karim et al. (2021)\n21.19\n9.63\n43.00\nChakravarthi et al. (2022)\n19.32\n4.00\n72.00\nChakravarthi et al. (2022)\n25.72\n11.61\n44.00\nChakravarthi et al. (2022)\n41.35\n29.67\n62.70\nJeong et al. (2022)": "56.55\n47.40\n—\n—"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table 16: Performance of finetuned models on Dev and Test-S set. We finetune each model on each dataset for",
      "data": [
        {
          "M-F1\n12.00\nEmot-engWal\nAcc.\n16.27\nEmot-zhoLee\nM-F1\n13.32\nEmot-finKaj\nM-F1\n12.85\nEmot-fraKaj\nM-F1\n13.28\nEmot-itaKaj\nM-F1\n13.16\nEmot-araAbd\n24.41\nEmot-engMoh M-F1\nM-F1\n22.98\nEmot-araMoh\nM-F1\n25.58\nEmot-spaMoh\nM-F1\n19.69\nEmot-indSap\nAcc.\n18.00\nEmot-turGuv\nM-F1\n18.34\nEmot-indWil\nW-F1\n16.12\nEmot-vieHo\nM-F1\n11.23\nEmot-engPla\nM-F1\n11.47\nEmot-spaPla\nM-F1\n13.60\nEmot-finOhm\n3.18\nEmot-engDem M-F1\nM-F1\n22.54\nEmot-itaBia\nM-F1\n23.99\nEmot-ronCio\nM-F1\n3.10\nEmot-hinDeb\nM-F1\n3.44\nEmot-porCor\nM-F1\n20.27\nEmot-fasSab\nM-F1\n15.84\nEmot-rusSbo\nM-F1\n15.97\nEmot-benIqb\nM-F1\n20.09\nEmot-fraBia\nM-F1\n21.56\nEmot-deuBia": "Average\n—\n15.86",
          "70.10\n65.35\n69.24\n69.51\n72.97\n65.87\n70.81\n67.30\n58.87\n46.15\n58.61\n49.54\n60.10\n49.99\n58.23\n60.04\n60.84\n54.61\n59.35\n60.68\n66.24\n57.77\n59.70\n65.69\n81.37\n70.90\n78.30\n80.86\n84.10\n72.93\n83.19\n83.29\n85.49\n78.53\n82.27\n83.33\n81.27\n63.58\n76.93\n78.37\n99.33\n98.25\n98.50\n98.92\n76.12\n63.10\n73.39\n75.18\n64.58\n54.63\n63.50\n63.12\n48.51\n39.09\n46.49\n47.45\n57.95\n50.89\n52.28\n54.25\n41.74\n49.82\n45.07\n50.36\n58.83\n53.75\n52.58\n57.50\n76.78\n66.47\n67.76\n75.58\n91.43\n87.63\n91.24\n89.26\n52.46\n45.42\n50.17\n48.52\n76.70\n72.09\n73.93\n75.16\n21.49\n21.32\n21.15\n26.21\n83.75\n76.49\n83.17\n81.96\n63.86\n53.79\n59.57\n63.69\n84.15\n75.07\n79.98\n83.56\n81.75\n71.35\n78.21\n79.18": "69.27\n61.42\n66.87\n68.13",
          "16.86\n21.13\n21.13\n11.05\n24.58\n4.54\n27.01\n27.01\n47.94\n60.08\n44.88\n14.11\n52.63\n56.94\n51.44\n18.42\n23.92\n55.50\n57.42\n16.03\n49.76\n19.14\n4.88\n3.45\n3.53\n3.45\n8.46\n3.61\n18.70\n8.89\n3.30\n3.86\n5.45\n4.24\n12.51\n12.04\n4.39\n8.45\n3.75\n20.70\n22.32\n8.82\n17.40\n14.23\n6.21\n8.78\n4.34\n5.87\n12.75\n5.62\n20.58\n14.28\n8.60\n14.08\n11.84\n5.08\n9.42\n10.93\n6.65\n8.92\n5.17\n13.98\n5.32\n3.19\n5.33\n9.49\n16.48\n9.63\n9.63\n29.70\n31.00\n15.51\n47.57\n47.57\n15.77\n59.71\n25.15\n19.12\n23.15\n34.83\n21.07\n22.68\n12.70\n35.32\n8.75\n26.18\n41.16\n25.85\n15.56\n25.14\n8.30\n19.50\n33.17\n12.53\n42.72\n23.43\n25.37\n54.06\n29.18\n8.85\n36.78\n7.69\n35.53\n15.55\n6.73\n29.03\n34.63\n26.94\n36.37\n24.04\n22.50\n20.25\n22.00\n20.25\n19.50\n21.75\n51.75\n49.00\n22.75\n36.50\n19.75\n8.00\n33.62\n8.00\n33.93\n16.30\n5.47\n30.50\n30.10\n29.79\n36.78\n25.26\n2.19\n14.90\n5.52\n12.22\n10.20\n1.76\n27.81\n24.03\n2.04\n16.34\n9.52\n5.30\n9.74\n9.74\n7.80\n10.12\n2.93\n19.29\n19.29\n7.96\n25.59\n20.60\n5.64\n12.63\n10.31\n7.77\n3.52\n3.36\n13.54\n13.45\n9.41\n27.48\n22.31\n5.96\n3.78\n2.39\n2.84\n7.67\n4.39\n14.47\n7.32\n3.73\n3.73\n6.04\n1.96\n5.40\n5.40\n1.90\n3.14\n0.31\n11.36\n11.36\n4.03\n15.91\n10.65\n16.21\n27.07\n15.42\n21.44\n22.65\n16.80\n38.69\n13.15\n27.47\n50.42\n24.10\n11.37\n19.59\n9.90\n14.75\n30.85\n10.25\n41.07\n33.29\n26.95\n52.76\n30.81\n1.17\n4.70\n3.03\n4.51\n3.67\n0.37\n9.07\n12.67\n2.44\n6.70\n2.88\n0.68\n2.62\n3.58\n1.12\n2.99\n0.18\n2.55\n0.65\n1.05\n6.70\n2.54\n5.23\n11.15\n3.35\n12.00\n5.24\n6.59\n14.03\n7.79\n5.13\n12.26\n4.47\n8.61\n15.05\n12.36\n11.02\n11.74\n3.57\n30.27\n21.97\n8.30\n51.06\n8.59\n10.43\n13.37\n15.01\n11.39\n9.36\n5.19\n17.90\n30.11\n2.98\n9.52\n2.98\n20.36\n29.15\n50.61\n20.00\n37.94\n12.18\n47.99\n54.75\n30.42\n63.48\n39.38\n15.48\n21.00\n14.22\n20.26\n15.96\n12.25\n43.21\n51.05\n26.96\n69.86\n32.10": "9.71\n17.18\n13.85\n15.07\n15.19\n7.75\n27.87\n24.21\n15.14\n31.80\n18.12",
          "68.06\n68.06\n57.00\nSuresh and Ong (2021)\n68.18\n65.55\n53.90\nLee and Wang (2015)\n52.79\n46.58\n-\n—\n58.29\n45.88\n-\n—\n60.53\n38.16\n-\n—\n32.09\n27.15\n60.32 Abdul-Mageed et al. (2020)\n74.44\n74.44\n78.50\nBarbieri et al. (2020)\n80.57\n71.69\n91.00\nBianchi et al. (2022)\nBianchi et al. (2022)\n75.89\n74.64\n91.00\n77.51\n60.20\n68.00\nSaputri et al. (2018)\n89.50\n84.00\n87.00 Güven et al. (2020)\n74.87\n58.25\n79.47 Wilie et al. (2020)\n54.69\n32.96\n66.34 Ho et al. (2019)\n40.73\n40.73\n32.00\nPlaza del Arco et al. (2020)\n40.51\n44.00\n44.00\nPlaza del Arco et al. (2020)\n52.94\n42.55\n-\n—\n32.61\n32.61\n64.80\nSuresh and Ong (2021)\n73.66\n73.19\n71.00\nBianchi et al. (2021)\n84.74\n60.97\n78.00\nCiobotaru and Dinu (2021)\n31.31\n19.91\n-\n—\n8.72\n8.98\n64.00\nCortiz et al. (2021)\n28.59\n27.80\n-\n—\n79.62\n76.53\n78.00\nSboev et al. (2020)\n53.23\n44.21\n-\n—\n79.02\n78.19\n88.00\nBianchi et al. (2022)\n76.07\n24.99\n87.00\nBianchi et al. (2022)": "59.58\n50.85\n—\n—"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "M-F1\n41.87\nIron-itaBas\nM-F1\n40.70\nIron-spaBar\nF1-iro.\n45.00\nIron-engHee\nM-F1\n51.80\nIron-itaCig\nM-F1\n46.88\nIron-hinVij\nM-F1\n48.39\nIron-araGha\nM-F1\n46.84\nIron-spaOrt\nAcc.\n48.30\nIron-fasGol\nM-F1\n11.71\nIron-zhoXia\nM-F1\n53.33\nSarc-engWal\nF1-sar.\n27.00\nSarc-engRil\nM-F1\n35.92\nSarc-cesPta\nM-F1\n49.85\nSarc-engPta\nAcc.\n52.20\nSarc-engBam\nAcc.\n48.80\nSarc-engRaj\nM-F1\n49.00\nSarc-engOra\nM-F1\n48.25\nSarc-zhoGon\nM-F1\n44.26\nSarc-araAbu\nM-F1\n46.22\nSarc-araFar\n22.36\nIron-T-engHee M-F1": "Average\n—\n42.93",
          "65.67\n63.38\n61.54\n63.16\n67.52\n57.90\n58.01\n61.66\n68.25\n59.99\n63.43\n67.43\n77.44\n70.37\n72.66\n75.92\n73.25\n70.90\n72.90\n71.16\n83.95\n82.19\n82.45\n83.08\n73.88\n67.42\n71.18\n72.79\n76.53\n74.04\n74.60\n73.58\n33.36\n31.96\n31.19\n30.52\n63.32\n65.36\n67.45\n63.65\n57.46\n46.76\n52.50\n54.89\n67.89\n66.12\n60.17\n65.97\n95.76\n94.28\n94.99\n95.56\n82.40\n79.73\n80.40\n82.27\n96.27\n94.20\n95.33\n95.67\n75.87\n72.73\n74.69\n75.64\n72.75\n71.01\n70.63\n70.53\n69.06\n69.57\n69.57\n71.87\n68.81\n66.87\n68.45\n68.80\n57.58\n47.35\n46.43\n56.04": "71.12\n67.48\n68.51\n70.29",
          "46.92\n46.92\n50.38\n50.97\n46.92\n49.59\n46.92\n46.92\n49.02\n13.27\n55.49\n46.47\n46.47\n48.65\n56.77\n46.47\n48.89\n46.47\n46.47\n49.34\n13.88\n52.78\n0.00\n0.00\n0.00\n25.41\n0.00\n1.03\n0.00\n0.00\n5.29\n55.06\n41.19\n34.67\n34.21\n45.82\n48.21\n34.21\n38.78\n34.21\n34.21\n48.22\n33.24\n56.22\n56.66\n43.99\n57.75\n51.30\n55.80\n46.28\n42.53\n46.63\n52.02\n23.61\n57.73\n40.10\n34.24\n32.61\n51.43\n32.61\n33.80\n32.61\n32.61\n39.17\n34.40\n35.94\n39.47\n39.47\n53.40\n54.46\n39.47\n40.22\n39.47\n39.47\n60.64\n29.47\n56.77\n51.70\n56.46\n56.46\n51.70\n56.46\n56.46\n56.46\n56.46\n49.66\n43.88\n57.82\n13.65\n14.56\n3.09\n9.89\n13.65\n13.65\n13.55\n3.17\n13.61\n0.63\n13.40\n45.21\n32.80\n32.80\n33.23\n41.54\n45.01\n32.80\n32.80\n34.55\n50.47\n33.38\n10.53\n0.00\n0.00\n0.00\n0.00\n36.36\n0.00\n0.00\n0.00\n38.97\n14.81\n46.90\n49.03\n3.68\n49.03\n49.75\n33.94\n49.03\n3.68\n34.46\n6.22\n55.00\n41.77\n38.42\n38.42\n39.49\n37.05\n45.25\n38.42\n38.42\n40.45\n33.01\n48.17\n48.60\n52.00\n52.00\n51.80\n50.60\n49.00\n52.00\n52.00\n49.20\n52.00\n52.20\n77.60\n91.20\n91.20\n90.80\n87.80\n19.60\n91.20\n91.20\n87.00\n15.00\n83.60\n41.48\n32.89\n32.89\n32.80\n42.11\n41.83\n32.89\n32.89\n33.71\n47.08\n36.08\n49.04\n33.29\n33.20\n33.20\n56.10\n38.89\n33.29\n33.29\n36.25\n41.41\n49.36\n27.16\n44.57\n16.39\n44.57\n45.15\n20.66\n44.57\n16.39\n53.38\n17.21\n53.74\n41.67\n42.00\n21.63\n41.93\n53.31\n30.34\n42.00\n21.63\n42.65\n23.47\n50.37\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83\n18.83": "38.92\n37.57\n34.46\n41.79\n40.39\n35.42\n37.36\n32.35\n39.87\n29.56\n46.14",
          "59.91\n55.16\n59.59\nBasile et al. (2014)\n66.10\n63.97\n54.12\nBarbieri et al. (2016)\n59.00\n59.00\n70.50 Van Hee et al. (2018)\n73.32\n74.20\n73.10\nCignarella et al. (2018)\n52.89\n57.92\n77.00 Vijay et al. (2018)\n68.78\n67.40\n84.40 Abdul-Mageed et al. (2020)\n62.92\n61.69\n71.67 Ortega-Bueno et al. (2019)\n62.59\n56.80\n83.10 Golazizian et al. (2020)\n18.58\n10.06\n57.20 Xiang et al. (2020)\n70.79\n70.79\n69.00\nFelbo et al. (2017)\n50.00\n50.00\n51.00\nRiloff et al. (2013)\n51.20\n52.48\n58.20\nPtáˇcek et al. (2014)\n74.30\n74.30\n92.37\nPtáˇcek et al. (2014)\n64.60\n64.60\n85.10\nBamman and Smith (2015)\n74.60\n74.60\n92.94\nRajadesingan et al. (2015)\n73.78\n73.78\n75.00\nFelbo et al. (2017)\n53.50\n51.05\n73.68 Gong et al. (2020)\n75.47\n74.38\n76.30 Abdul-Mageed et al. (2021)\n66.24\n68.43\n73.10\nFarha et al. (2021)\n30.81\n30.81\n50.70 Van Hee et al. (2018)": "60.41\n59.63\n—\n—"
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Acc.\n51.80\nSent-engPan\nM-F1\n49.04\nSent-zhoTan\nAcc.\n47.00\nSent-T-engThe\nAcc.\n50.20\nSent-Y-engThe\nAcc.\n19.80\nSent-5-engSoc\nM-F1\n20.79\nSent-korJan\nAcc.\n46.80\nSent-engSoc\nM-F1\n46.79\nSent-itaBas\nM-F1\n49.61\nSent-itaBas\nM-F1\n47.51\nSent-mltDin\nM-F1\n32.48\nSent-bulMoz\nM-F1\n34.37\nSent-bosMoz\nM-F1\n32.60\nSent-deuMoz\nM-F1\n30.33\nSent-engMoz\nM-F1\n29.19\nSent-spaMoz\nM-F1\n30.23\nSent-hrvMoz\nM-F1\n27.80\nSent-hunMoz\nM-F1\n30.95\nSent-polMoz\nM-F1\n29.81\nSent-porMoz\nM-F1\n32.33\nSent-rusMoz\nM-F1\n28.57\nSent-slkMoz\nM-F1\n35.90\nSent-slvMoz\nM-F1\n29.39\nSent-sqiMoz\nM-F1\n34.09\nSent-srpMoz\nM-F1\n33.35\nSent-sweMoz\nM-F1\n19.05\nSent-deuRei\nM-F1\n19.78\nSent-spaRei\nM-F1\n23.76\nSent-itaRei\nM-Rec\n36.77\nSent-engRos\nM-F1\n28.06\nSent-benPat\nM-F1\n31.67\nSent-hinPat\nAcc.\n47.60\nSent-hebAmr\nM-F1\n35.59\nSent-porBru\nM-F1\n50.11\nSent-finKaj\nM-F1\n47.70\nSent-fraKaj\nM-F1\n49.65\nSent-itaKaj\nM-F1\n16.24\nSent-norVel\nM-F1\n25.03\nSent-polKoc\nM-F1\n31.26\nSent-thaSur\nM-F1\n46.66\nSent-zhoWan\nM-F1\n30.51\nSent-fasAsh\nM-F1\n52.34\nSent-ronDum\nM-F1\n31.09\nSent-pcmOye\nM-F1\n21.56\nSent-polRyb\nM-F1\n30.52\nSent-indWil\nM-F1\n29.17\nSent-araAbd\nM-F1\n27.61\nSent-bamDia\nM-F1\n31.18\nSent-benIsl\nAcc.\n29.60\nSent-marKul\nM-F1\n49.55\nSent-kanCha\nM-F1\n50.66\nSent-malCha\nM-F1\n44.61\nSent-tamCha\nAcc.\n33.20\nSent-araMuh\nW-F1\n37.95\nSent-amhMuh\nW-F1\n35.94\nSent-aryMuh\nW-F1\n34.23\nSent-arqMuh\nW-F1\n35.22\nSent-hauMuh\nW-F1\n32.90\nSent-iboMuh\nW-F1\n35.62\nSent-pcmMuh\nW-F1\n35.83\nSent-kinMuh\nW-F1\n33.72\nSent-swhMuh\nW-F1\n34.03\nSent-tsoMuh\nW-F1\n34.04\nSent-twiMuh\nW-F1\n36.56\nSent-yorMuh\nM-F1\n46.80\nSent-yorSho\nAcc.\n18.00\nSent-jpnSuz\nM-F1\n34.78\nSent-aceWin\nM-F1\n30.14\nSent-banWin\nM-F1\n30.60\nSent-bbcWin\nM-F1\n30.77\nSent-bjnWin\nM-F1\n30.77\nSent-bugWin\nM-F1\n31.62\nSent-javWin\nM-F1\n28.64\nSent-madWin\nM-F1\n34.41\nSent-minWin\nM-F1\n34.86\nSent-nijWin\nM-F1\n32.18\nSent-sunWin\nM-F1\n30.27\nSent-telMar": "Average\n—\n34.68",
          "81.60\n86.07\n85.20\n86.93\n96.85\n95.72\n95.85\n95.92\n91.40\n79.20\n88.73\n89.60\n93.33\n86.00\n90.80\n92.87\n53.93\n49.73\n51.73\n53.67\n44.70\n38.64\n42.56\n41.05\n84.47\n88.40\n86.87\n88.73\n88.77\n78.06\n85.22\n85.33\n66.68\n78.51\n79.93\n84.73\n63.14\n39.79\n65.96\n68.01\n65.09\n62.01\n64.22\n63.09\n68.31\n64.75\n68.12\n65.83\n62.80\n61.63\n61.54\n62.53\n68.78\n62.81\n68.53\n68.59\n55.87\n51.39\n55.80\n55.79\n69.44\n64.79\n66.68\n67.83\n71.62\n67.27\n70.16\n68.37\n68.22\n67.09\n66.96\n67.91\n57.14\n56.21\n56.37\n56.71\n80.37\n75.24\n78.35\n78.84\n75.71\n71.78\n74.78\n71.82\n61.96\n59.53\n61.57\n61.27\n47.04\n43.07\n46.42\n45.69\n56.85\n53.16\n56.62\n52.51\n70.98\n64.61\n68.84\n69.93\n60.94\n47.05\n52.99\n55.93\n52.83\n39.59\n51.26\n51.53\n51.20\n47.29\n49.57\n47.14\n70.90\n64.52\n67.50\n70.39\n59.45\n54.20\n58.28\n57.56\n62.30\n55.46\n58.31\n59.76\n95.80\n93.27\n95.27\n95.40\n60.00\n59.05\n57.85\n59.29\n78.88\n83.48\n79.86\n82.32\n78.44\n84.21\n86.12\n87.50\n86.17\n79.21\n85.70\n84.59\n51.15\n41.05\n39.73\n42.36\n77.63\n70.99\n76.03\n77.05\n75.17\n65.07\n72.18\n71.99\n99.00\n98.80\n98.67\n98.93\n84.80\n82.70\n84.56\n84.54\n84.07\n88.60\n84.12\n88.23\n68.87\n65.11\n66.86\n68.84\n55.11\n45.45\n53.98\n51.12\n92.70\n88.28\n92.06\n92.52\n77.36\n71.96\n75.75\n77.30\n65.57\n64.28\n58.46\n65.46\n68.54\n62.95\n67.99\n66.74\n86.47\n84.00\n86.40\n84.40\n82.92\n78.92\n80.55\n82.35\n84.03\n84.00\n83.50\n83.88\n75.98\n70.91\n75.90\n75.87\n62.20\n48.73\n56.40\n61.20\n65.68\n24.93\n60.32\n57.21\n53.44\n46.56\n50.21\n52.61\n71.25\n57.16\n65.09\n70.21\n73.44\n69.94\n73.11\n72.18\n78.36\n76.37\n76.52\n76.75\n60.82\n62.44\n63.61\n64.45\n58.78\n55.32\n57.44\n56.69\n61.29\n53.04\n61.15\n56.19\n52.55\n42.64\n48.56\n52.47\n65.43\n63.32\n64.29\n64.51\n67.87\n64.38\n66.71\n64.84\n86.86\n83.69\n85.39\n84.93\n61.47\n54.33\n59.67\n61.13\n77.36\n76.59\n74.19\n75.74\n79.49\n75.96\n74.79\n76.86\n75.17\n72.66\n65.57\n73.58\n84.50\n75.81\n79.82\n82.58\n74.57\n67.72\n73.85\n71.55\n84.79\n76.08\n83.44\n84.38\n78.36\n70.56\n73.36\n77.36\n84.07\n77.25\n80.89\n80.19\n78.19\n73.82\n73.47\n77.22\n81.71\n75.32\n76.61\n77.70\n69.47\n65.20\n69.44\n67.14": "71.64\n66.34\n69.58\n70.44",
          "97.20\n51.60\n97.20\n96.80\n56.40\n51.40\n76.80\n76.80\n61.80\n60.20\n55.40\n36.40\n90.52\n87.19\n87.73\n58.32\n31.88\n86.57\n90.14\n56.83\n49.59\n45.31\n47.20\n75.60\n75.60\n78.40\n45.20\n41.20\n73.80\n73.80\n43.20\n54.00\n55.60\n42.40\n84.80\n84.80\n85.20\n36.80\n30.60\n81.60\n81.60\n32.60\n47.40\n33.20\n24.80\n48.60\n48.60\n48.40\n19.80\n27.60\n45.60\n45.60\n20.00\n40.40\n19.60\n2.96\n29.03\n3.70\n28.08\n1.11\n13.02\n25.91\n19.91\n12.78\n25.37\n8.01\n93.00\n56.40\n92.20\n92.20\n53.60\n49.00\n76.80\n76.80\n53.40\n58.20\n49.80\n40.60\n54.18\n39.08\n74.32\n41.25\n38.50\n53.07\n38.50\n54.20\n51.58\n60.85\n46.61\n45.37\n42.04\n63.71\n42.63\n40.76\n44.99\n40.76\n57.97\n52.71\n67.62\n39.36\n36.75\n39.36\n47.70\n41.14\n39.36\n39.25\n25.97\n53.99\n39.36\n48.98\n22.05\n22.61\n15.15\n27.35\n29.54\n11.75\n19.37\n29.02\n13.24\n16.88\n23.69\n24.58\n26.39\n16.31\n31.80\n24.05\n16.31\n21.16\n50.69\n27.92\n19.32\n16.72\n17.36\n26.96\n9.39\n26.13\n20.10\n9.79\n20.54\n21.93\n22.50\n15.88\n25.51\n26.73\n36.94\n36.94\n36.52\n27.25\n12.90\n22.92\n22.92\n15.77\n28.86\n23.87\n25.84\n31.85\n10.32\n33.31\n24.18\n7.25\n26.36\n7.25\n19.94\n14.69\n20.63\n19.35\n27.96\n11.02\n35.18\n20.68\n11.02\n27.47\n11.02\n24.32\n14.45\n13.67\n15.80\n28.87\n8.61\n30.31\n19.62\n6.93\n27.19\n39.94\n12.18\n8.66\n16.39\n18.69\n27.58\n14.36\n34.78\n26.65\n14.34\n25.57\n14.91\n24.98\n20.83\n14.91\n32.20\n26.22\n18.15\n27.65\n34.78\n18.22\n18.03\n18.22\n31.48\n26.59\n18.94\n26.77\n30.42\n28.64\n31.33\n30.70\n14.99\n20.60\n30.55\n18.19\n21.73\n22.35\n20.49\n27.36\n20.32\n31.89\n18.14\n14.09\n27.85\n23.54\n25.01\n17.93\n10.46\n26.34\n19.79\n14.66\n26.76\n22.95\n14.66\n20.75\n14.66\n27.09\n20.27\n20.69\n18.25\n26.88\n10.76\n31.78\n23.15\n9.98\n26.59\n41.09\n27.51\n9.98\n16.11\n28.06\n20.66\n19.82\n27.45\n24.20\n16.99\n20.55\n44.27\n31.95\n22.73\n19.05\n24.85\n25.84\n19.32\n29.31\n28.14\n19.66\n20.77\n26.30\n24.57\n22.56\n16.05\n11.67\n14.46\n3.01\n13.61\n16.78\n4.26\n15.78\n18.18\n13.82\n8.76\n30.32\n17.86\n6.70\n6.60\n8.12\n12.91\n1.69\n8.49\n1.69\n15.72\n14.71\n31.50\n27.92\n10.75\n3.30\n12.45\n25.39\n3.45\n11.30\n3.30\n14.80\n10.59\n31.17\n33.60\n52.75\n52.75\n57.60\n32.29\n33.33\n38.89\n38.89\n33.97\n38.72\n39.31\n23.88\n34.29\n30.34\n34.73\n22.12\n13.67\n28.91\n26.36\n19.53\n18.53\n21.01\n28.73\n28.74\n11.59\n30.45\n28.23\n11.57\n23.96\n26.66\n13.80\n20.58\n22.45\n64.80\n71.00\n32.20\n71.20\n60.80\n32.20\n76.60\n32.20\n67.00\n34.80\n40.80\n24.38\n26.73\n20.55\n29.33\n34.23\n19.68\n20.31\n19.65\n28.19\n25.63\n15.49\n35.48\n36.55\n35.65\n41.24\n41.33\n35.65\n58.17\n74.27\n38.38\n40.41\n57.76\n35.72\n71.71\n31.32\n70.88\n42.54\n35.23\n58.67\n63.50\n49.58\n60.49\n68.81\n59.43\n56.28\n35.98\n65.24\n36.48\n35.98\n64.76\n35.98\n57.65\n56.67\n64.59\n0.00\n18.43\n4.09\n19.84\n0.00\n1.93\n35.73\n39.75\n0.00\n11.76\n9.80\n10.17\n32.45\n13.02\n32.20\n9.23\n13.02\n39.51\n40.34\n21.75\n30.42\n9.63\n20.65\n20.51\n24.37\n20.16\n11.04\n13.33\n23.94\n29.83\n13.29\n14.21\n24.40\n46.49\n69.80\n49.89\n70.65\n61.31\n33.51\n56.51\n81.16\n53.89\n45.32\n65.70\n10.25\n33.77\n26.77\n41.34\n10.25\n9.98\n45.69\n49.56\n12.76\n10.25\n17.63\n37.82\n65.40\n30.26\n83.18\n44.66\n30.26\n93.21\n30.26\n62.09\n32.21\n30.26\n24.96\n33.63\n30.52\n38.99\n23.01\n8.19\n25.29\n25.29\n9.00\n15.17\n15.93\n8.72\n11.80\n3.83\n13.82\n5.92\n3.83\n16.28\n21.80\n10.58\n15.91\n5.58\n18.36\n58.25\n16.42\n59.11\n45.24\n16.19\n50.79\n36.23\n16.16\n23.89\n12.78\n28.05\n42.96\n24.15\n44.48\n32.36\n24.15\n26.60\n49.01\n28.43\n27.56\n8.21\n19.89\n31.54\n8.74\n36.70\n19.66\n8.74\n24.31\n8.74\n15.47\n9.94\n14.40\n18.50\n42.84\n29.71\n45.18\n29.11\n18.50\n30.75\n31.68\n18.50\n18.50\n15.08\n35.40\n46.20\n35.60\n43.80\n35.80\n35.00\n35.40\n39.20\n36.80\n38.20\n34.40\n27.44\n55.22\n22.77\n61.86\n23.82\n22.77\n47.51\n51.22\n29.28\n23.75\n24.07\n25.81\n62.16\n22.24\n66.87\n24.76\n22.24\n56.51\n64.97\n35.54\n23.18\n25.65\n26.73\n58.06\n44.93\n61.29\n19.37\n15.40\n50.64\n31.28\n22.52\n16.59\n17.85\n34.80\n51.60\n34.40\n52.80\n37.00\n34.40\n41.60\n53.40\n36.60\n37.80\n25.00\n54.53\n27.76\n22.74\n16.05\n49.04\n54.53\n22.49\n2.18\n54.53\n54.53\n2.99\n23.29\n34.49\n14.17\n37.40\n23.98\n14.17\n23.41\n32.35\n20.11\n18.02\n16.64\n35.26\n49.79\n34.23\n52.02\n37.83\n34.23\n18.05\n53.01\n34.91\n39.80\n5.33\n24.86\n20.66\n21.67\n30.14\n33.26\n16.55\n20.93\n16.55\n19.83\n16.97\n17.39\n25.58\n16.24\n27.92\n23.21\n37.48\n12.25\n11.52\n27.93\n14.25\n14.96\n28.37\n28.07\n53.47\n45.54\n55.33\n8.99\n39.73\n23.21\n22.73\n40.18\n42.36\n11.91\n19.99\n23.02\n18.33\n28.06\n26.01\n18.33\n14.84\n28.08\n24.32\n19.17\n22.23\n5.09\n17.80\n13.20\n17.60\n8.05\n1.96\n14.02\n13.20\n9.45\n2.36\n45.55\n21.65\n38.43\n30.48\n45.54\n25.42\n18.54\n30.74\n30.82\n24.58\n21.05\n6.50\n22.32\n37.26\n29.31\n46.56\n16.91\n20.74\n29.34\n28.99\n23.18\n24.48\n5.28\n14.14\n33.14\n14.12\n31.70\n18.87\n7.29\n28.58\n7.29\n9.27\n8.11\n19.01\n33.24\n56.04\n34.22\n54.50\n34.03\n33.33\n43.07\n35.52\n40.83\n33.33\n33.75\n32.00\n45.20\n26.00\n44.20\n33.40\n25.80\n47.60\n14.20\n32.00\n19.60\n26.80\n23.31\n28.22\n22.89\n37.89\n20.40\n18.44\n24.73\n20.99\n19.39\n19.38\n12.79\n21.96\n35.00\n31.84\n41.90\n23.12\n18.27\n29.91\n24.44\n19.80\n18.44\n13.82\n19.23\n24.61\n36.37\n36.42\n23.68\n18.48\n19.20\n18.27\n18.77\n18.44\n13.86\n20.52\n41.73\n43.86\n50.51\n34.89\n18.44\n27.78\n18.74\n18.44\n22.07\n14.89\n19.04\n20.69\n31.51\n34.60\n20.92\n18.39\n18.27\n18.27\n19.59\n18.44\n12.90\n20.11\n35.48\n18.44\n48.06\n23.68\n18.44\n37.65\n33.97\n18.95\n18.44\n15.21\n22.42\n31.23\n37.89\n45.44\n23.12\n18.44\n21.85\n18.27\n18.36\n18.92\n13.14\n18.39\n41.20\n35.18\n49.93\n28.76\n18.44\n32.41\n21.71\n18.44\n18.44\n14.95\n19.01\n35.18\n39.43\n42.86\n27.70\n18.44\n22.89\n19.65\n18.44\n18.44\n15.21\n18.87\n31.98\n18.44\n44.83\n27.14\n18.44\n37.65\n12.90\n18.86\n18.44\n12.93\n9.98\n38.81\n23.32\n32.08\n13.52\n9.99\n20.40\n36.96\n9.98\n9.98\n23.32": "26.67\n39.03\n28.61\n43.03\n28.46\n20.77\n34.65\n32.76\n27.55\n25.84\n25.02",
          "88.40\n88.40\n90.82 Ke et al. (2020)\n90.16\n88.39\n95.80\nSun et al. (2020)\n90.00\n90.00\n88.00\nFelbo et al. (2017)\nFelbo et al. (2017)\n90.00\n90.00\n93.00\n49.60\n49.60\n58.59 Ke et al. (2020)\n42.32\n36.49\n-\n—\n91.80\n91.80\n96.70\nTian et al. (2020)\nBasile et al. (2014)\n87.26\n87.01\n67.71\n85.21\n83.09\n66.38\nBarbieri et al. (2016)\n78.47\n77.45\n54.70 Dingli and Sant (2016)\n59.32\n54.39\n52.00 Mozetiˇc et al. (2016)\n63.22\n48.15\n60.60 Mozetiˇc et al. (2016)\n49.70\n47.64\n53.60 Mozetiˇc et al. (2016)\n60.61\n60.61\n63.00 Mozetiˇc et al. (2016)\n39.39\n42.48\n38.60 Mozetiˇc et al. (2016)\n62.02\n55.66\n60.60 Mozetiˇc et al. (2016)\n53.22\n43.89\n64.10 Mozetiˇc et al. (2016)\n61.84\n60.58\n67.70 Mozetiˇc et al. (2016)\n44.60\n42.76\n55.30 Mozetiˇc et al. (2016)\n61.67\n59.27\n61.50 Mozetiˇc et al. (2016)\n57.46\n56.18\n68.20 Mozetiˇc et al. (2016)\n58.64\n57.26\n55.30 Mozetiˇc et al. (2016)\n46.82\n33.84\n39.10 Mozetiˇc et al. (2016)\n55.84\n52.54\n60.60 Mozetiˇc et al. (2016)\n60.10\n62.58\n65.70 Mozetiˇc et al. (2016)\n30.95\n33.58\n-\n—\n25.01\n28.54\n-\n—\n25.39\n30.61\n-\n—\nBarbieri et al. (2020)\n69.94\n69.94\n72.60\n55.00\n29.14\n52.60\nPatra et al. (2018)\nPatra et al. (2018)\n59.40\n48.30\n56.90\n84.20\n57.40\n89.06 Amram et al. (2018)\nBrum and das Graças Volpe Nunes (2018)\n42.64\n42.39\n62.14\n83.77\n83.32\n-\n—\n87.96\n87.78\n-\n—\n86.03\n84.69\n-\n—\n42.11\n41.41\n-\n—\n50.86\n40.71\n-\n—\n52.81\n36.32\n-\n—\n77.83\n79.08\n91.20 Wan et al. (2020)\n73.87\n68.91\n80.00 Ashrafi Asli et al. (2020)\n93.72\n93.49\n-\n—\n52.19\n31.79\n-\n—\n43.30\n31.48\n-\n—\n73.49\n73.98\n92.72 Wilie et al. (2020)\n61.90\n60.56\n80.86\nElmadany et al. (2022)\n40.27\n37.11\n72.00 Diallo et al. (2021)\n55.76\n29.38\n64.61\nIslam et al. (2021)\n68.60\n50.20\n84.13 Kulkarni et al. (2021)\n68.44\n51.15\n68.50\nChakravarthi et al. (2022)\n68.75\n58.89\n60.50\nChakravarthi et al. (2022)\n65.98\n54.79\n59.00\nChakravarthi et al. (2022)\n58.40\n58.20\n75.16 Abdul-Mageed et al. (2022)\n20.62\n46.82\n78.42 Muhammad et al. (2023b)\n52.19\n51.66\n64.83 Muhammad et al. (2023b)\n63.89\n67.58\n74.20 Muhammad et al. (2023b)\n55.52\n34.13\n82.62 Muhammad et al. (2023b)\n57.55\n33.46\n82.96 Muhammad et al. (2023b)\n70.08\n53.71\n75.96 Muhammad et al. (2023b)\n53.78\n29.03\n72.63 Muhammad et al. (2023b)\n54.39\n53.84\n65.68 Muhammad et al. (2023b)\n42.58\n35.70\n60.67 Muhammad et al. (2023b)\n51.12\n32.06\n68.28 Muhammad et al. (2023b)\n57.57\n37.21\n80.16 Muhammad et al. (2022)\n71.97\n47.41\n87.20\nShode et al. (2022)\n55.80\n44.40\n61.50\nSuzuki et al. (2022)\n52.63\n58.05\n77.40 Winata et al. (2022)\n60.91\n42.28\n79.50 Winata et al. (2022)\n38.43\n40.65\n76.70 Winata et al. (2022)\n69.34\n75.43\n86.30 Winata et al. (2022)\n34.63\n30.86\n77.20 Winata et al. (2022)\n73.03\n78.56\n85.60 Winata et al. (2022)\n61.07\n61.14\n77.80 Winata et al. (2022)\n69.80\n62.91\n83.10 Winata et al. (2022)\n57.64\n57.07\n75.80 Winata et al. (2022)\n64.97\n68.76\n86.00 Winata et al. (2022)\n56.77\n43.14\n62.00 Marreddy et al. (2022)": "60.34\n54.9\n—\n—"
        }
      ],
      "page": 31
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lang Fam.\nLang\nRandom": "ara\n34.05\namh\n37.95\narq\n34.23\nAfro-Asiatic\nary\n35.94\nhau\n35.22\nheb\n47.60\nmlt\n47.51",
          "InfoDCL": "73.53\n65.68\n71.25\n53.44\n72.18\n95.80\n68.01",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "36.78\n35.61\n33.61\n60.81\n52.53\n46.82\n16.05\n22.49\n2.99\n20.62\n52.02\n18.05\n5.33\n63.89\n67.58\n37.40\n23.41\n16.64\n52.19\n51.66\n30.14\n20.93\n17.39\n55.52\n34.13\n71.20\n76.60\n40.80\n84.20\n57.40\n78.47\n77.45\n47.70\n39.25\n48.98"
        },
        {
          "Lang Fam.\nLang\nRandom": "bam\n27.61\nibo\n32.90\nkin\n35.83\nAtlantic-C.\nswh\n33.72\ntwi\n34.04\ntso\n34.03\nyor\n41.68",
          "InfoDCL": "65.57\n76.75\n56.69\n61.29\n64.51\n52.55\n74.88",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "36.70\n24.31\n14.40\n40.27\n37.11\n23.21\n11.52\n28.37\n57.55\n33.46\n28.06\n14.84\n22.23\n53.78\n29.03\n54.39\n53.84\n17.60\n14.02\n45.55\n46.56\n29.34\n5.28\n51.12\n32.06\n45.54\n30.74\n6.50\n42.58\n35.70\n64.77\n42.31\n43.10\n35.83\n26.38"
        },
        {
          "Lang Fam.\nLang\nRandom": "Austroasi.\nvie\n16.12",
          "InfoDCL": "64.58",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "12.22\n27.81\n9.52\n54.69\n32.96"
        },
        {
          "Lang Fam.\nLang\nRandom": "ace\n34.78\nban\n30.14\nbjn\n30.77\nbug\n30.77\nfil\n52.37\nind\n22.85\nAustrones.\njav\n31.62\nmad\n28.64\nmin\n34.41\nnij\n34.86\nsun\n32.18\nbbc\n30.60",
          "InfoDCL": "77.36\n79.49\n84.50\n71.55\n79.01\n83.05\n84.79\n78.36\n84.07\n77.22\n81.71\n73.58",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "37.89\n24.73\n12.79\n52.63\n58.05\n41.90\n29.91\n13.82\n60.91\n42.28\n50.51\n27.78\n14.89\n69.34\n75.43\n34.60\n18.27\n12.90\n34.63\n30.86\n69.13\n66.67\n34.47\n34.47\n34.47\n42.86\n36.77\n20.69\n75.29\n64.14\n48.06\n37.65\n15.21\n73.03\n78.56\n45.44\n21.85\n13.14\n61.07\n61.14\n69.80\n62.91\n49.93\n32.41\n14.95\n42.86\n22.89\n15.21\n57.64\n57.07\n44.83\n37.65\n12.93\n64.97\n68.76\n40.65\n36.42\n19.20\n13.86\n38.43"
        },
        {
          "Lang Fam.\nLang\nRandom": "kan\n32.14\nmal\n31.68\nDravidian\ntam\n29.45\ntel\n32.29",
          "InfoDCL": "61.79\n82.70\n57.81\n59.61",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "39.23\n27.80\n19.12\n44.81\n30.39\n43.84\n41.65\n24.85\n44.03\n31.44\n38.53\n34.27\n17.60\n45.85\n33.20\n40.99\n35.15\n36.61\n53.33\n38.68"
        },
        {
          "Lang Fam.\nLang\nRandom": "sqi\n29.39\nbos\n34.37\nbul\n32.48\nben\n24.53\nhrv\n30.23\nces\n41.06\ndan\n41.14\neng\n37.90\nfra\n24.65\ndeu\n24.40\nell\n41.24\nhin\n35.24\nIndo-Euro.\nita\n39.83\nmar\n29.60\npcm\n33.36\nnor\n16.24\nfas\n33.03\npor\n29.17\npol\n30.43\nron\n38.16\nrus\n31.85\nspa\n35.98\nsrp\n34.09\nslk\n28.57\nslv\n26.32\nswe\n33.35",
          "InfoDCL": "47.04\n68.31\n65.09\n69.49\n67.83\n80.15\n82.09\n75.48\n66.58\n67.55\n79.13\n67.55\n74.78\n86.47\n66.65\n42.36\n62.43\n66.10\n68.04\n89.83\n84.31\n70.20\n56.85\n75.71\n58.62\n70.98",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "31.78\n26.59\n16.11\n46.82\n33.84\n31.80\n21.16\n16.72\n63.22\n48.15\n27.35\n19.37\n23.69\n59.32\n54.39\n24.71\n25.18\n15.62\n53.25\n37.33\n35.18\n27.47\n13.67\n62.02\n55.66\n50.91\n46.61\n51.00\n65.30\n63.24\n46.68\n46.68\n51.84\n66.91\n66.79\n43.32\n39.23\n39.75\n66.51\n—\n23.29\n32.20\n30.46\n62.08\n56.84\n20.00\n26.51\n29.31\n52.24\n35.41\n46.71\n45.47\n48.21\n60.94\n34.98\n28.92\n26.20\n29.06\n52.63\n48.30\n38.76\n41.45\n45.05\n70.29\n64.37\n43.80\n35.40\n34.40\n68.60\n50.20\n47.16\n24.25\n13.92\n61.13\n42.75\n19.84\n35.73\n9.80\n42.11\n41.41\n35.01\n38.73\n26.64\n55.02\n51.17\n24.45\n20.14\n19.16\n41.10\n39.37\n31.82\n31.96\n19.15\n58.25\n49.43\n48.97\n67.14\n30.54\n89.11\n77.34\n25.49\n28.33\n28.51\n71.34\n69.08\n57.16\n56.64\n30.80\n32.17\n37.71\n27.45\n20.55\n19.05\n55.84\n52.54\n31.89\n27.85\n10.46\n57.46\n56.18\n14.37\n16.84\n16.64\n46.25\n36.97\n29.31\n20.77\n16.05\n60.10\n62.58"
        },
        {
          "Lang Fam.\nLang\nRandom": "Japonic\njpn\n18.00",
          "InfoDCL": "61.47",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "44.20\n47.60\n26.80\n55.80\n44.40"
        },
        {
          "Lang Fam.\nLang\nRandom": "Koreanic\nkor\n28.55",
          "InfoDCL": "57.46",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "19.48\n21.24\n16.26\n42.90\n37.06"
        },
        {
          "Lang Fam.\nLang\nRandom": "Sino-Tib.\nzho\n36.77",
          "InfoDCL": "76.10",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "48.19\n46.94\n38.19\n63.43\n61.39"
        },
        {
          "Lang Fam.\nLang\nRandom": "Tai-Kadai\ntha\n31.26",
          "InfoDCL": "75.17",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "20.16\n23.94\n24.40\n52.81\n36.32"
        },
        {
          "Lang Fam.\nLang\nRandom": "Turkic\ntur\n29.56",
          "InfoDCL": "87.71",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "32.32\n48.07\n35.44\n82.27\n64.84"
        },
        {
          "Lang Fam.\nLang\nRandom": "fin\n25.68\nUralic\nhun\n27.80",
          "InfoDCL": "63.85\n68.37",
          "BMZ-P3\nmT0\nVicuna\nCG\nCG-MT": "15.84\n30.45\n23.08\n63.17\n57.48\n30.31\n27.19\n16.39\n53.22\n43.89"
        }
      ],
      "page": 32
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "SAMAR: subjectivity and sentiment analysis for arabic social media",
      "authors": [
        "Muhammad Abdul-Mageed",
        "Mona Diab",
        "Sandra Kübler"
      ],
      "year": "2014",
      "venue": "Comput. Speech Lang",
      "doi": "10.1016/j.csl.2013.03.001"
    },
    {
      "citation_id": "2",
      "title": "AR-BERT & MARBERT: deep bidirectional transformers for arabic",
      "authors": [
        "Muhammad Abdul-Mageed",
        "Abdelrahim Elmadany",
        "El Moatez",
        "Billah Nagoudi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.551"
    },
    {
      "citation_id": "3",
      "title": "EmoNet: Fine-grained emotion detection with gated recurrent neural networks",
      "authors": [
        "Muhammad Abdul",
        "Lyle Ungar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1067"
    },
    {
      "citation_id": "4",
      "title": "NADI 2022: The third nuanced Arabic dialect identification shared task",
      "authors": [
        "Muhammad Abdul-Mageed",
        "Chiyu Zhang",
        "Abdel-Rahim Elmadany",
        "Houda Bouamor",
        "Nizar Habash"
      ],
      "year": "2022",
      "venue": "Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP)",
      "doi": "10.18653/v1/2022.wanlp-1.9"
    },
    {
      "citation_id": "5",
      "title": "AraNet: A deep learning toolkit for Arabic social media",
      "authors": [
        "Muhammad Abdul-Mageed",
        "Chiyu Zhang",
        "Azadeh Hashemi",
        "El Moatez",
        "Billah Nagoudi",
        "; Nguyen",
        "Kiet Van Nguyen",
        "Ngan Luu",
        "-Thuy Nguyen"
      ],
      "year": "2019",
      "venue": "Computational Linguistics -16th International Conference of the Pacific Association for Computational Linguistics",
      "doi": "10.1007/978-981-15-6168-9_27"
    },
    {
      "citation_id": "6",
      "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
      "authors": [
        "Junjie Hu",
        "Sebastian Ruder",
        "Aditya Siddhant",
        "Graham Neubig",
        "Orhan Firat",
        "Melvin Johnson"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020"
    },
    {
      "citation_id": "7",
      "title": "Not all languages are created equal in llms: Improving multilingual capability by crosslingual-thought prompting",
      "authors": [
        "Haoyang Huang",
        "Tianyi Tang",
        "Dongdong Zhang",
        "Wayne Zhao",
        "Ting Song",
        "Yan Xia",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Not all languages are created equal in llms: Improving multilingual capability by crosslingual-thought prompting",
      "doi": "10.48550/arXiv.2305.07004"
    },
    {
      "citation_id": "8",
      "title": "Bemoc: A corpus for identifying emotion in bengali texts",
      "authors": [
        "Asif Md",
        "Avishek Iqbal",
        "Omar Das",
        "Mohammed Sharif",
        "Iqbal Hoque",
        "Sarker"
      ],
      "year": "2022",
      "venue": "SN Comput. Sci",
      "doi": "10.1007/s42979-022-01028-w"
    },
    {
      "citation_id": "9",
      "title": "SentNoB: A dataset for analysing sentiment on noisy Bangla texts",
      "authors": [
        "Ittehadul Khondoker",
        "Sudipta Islam",
        "Kar"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": "10.18653/v1/2021.findings-emnlp.278"
    },
    {
      "citation_id": "10",
      "title": "KOSAC: A full-fledged korean sentiment analysis corpus",
      "authors": [
        "Hayeon Jang",
        "Munhyong Kim",
        "Hyopil Shin"
      ],
      "year": "2013",
      "venue": "Proceedings of the 27th Pacific Asia Conference on Language, Information and Computation"
    },
    {
      "citation_id": "11",
      "title": "KOLD: korean offensive language dataset",
      "authors": [
        "Younghoon Jeong",
        "Juhyun Oh",
        "Jaimeen Ahn",
        "Jongwon Lee",
        "Jihyung Moon",
        "Sungjoon Park",
        "Alice Oh"
      ],
      "year": "2022",
      "venue": "KOLD: korean offensive language dataset",
      "doi": "10.48550/arXiv.2205.11315"
    },
    {
      "citation_id": "12",
      "title": "Cross-lingual sentiment preservation and transfer learning in binary and multiclass classification. Master's thesis",
      "authors": [
        "Kaisla Kajava"
      ],
      "year": "2018",
      "venue": "th IEEE International Conference on Data Science and Advanced Analytics",
      "doi": "10.1109/DSAA53316.2021.9564230"
    },
    {
      "citation_id": "13",
      "title": "SentiLARE: Sentiment-aware language representation learning with linguistic knowledge",
      "authors": [
        "Pei Ke",
        "Haozhe Ji",
        "Siyang Liu",
        "Xiaoyan Zhu",
        "Minlie Huang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.567"
    },
    {
      "citation_id": "14",
      "title": "Multi-level sentiment analysis of PolEmo 2.0: Extended corpus of multi-domain consumer reviews",
      "authors": [
        "Jan Kocoń",
        "Piotr Miłkowski",
        "Monika Zaśko-Zielińska"
      ],
      "year": "2019",
      "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
      "doi": "10.18653/v1/K19-1092"
    },
    {
      "citation_id": "15",
      "title": "Slovenian twitter hate speech dataset IMSyPP-sl. Slovenian language resource repository CLARIN",
      "authors": [
        "Petra Kralj Novak",
        "Igor Mozetič",
        "Nikola Ljubešić"
      ],
      "year": "2021",
      "venue": "Slovenian twitter hate speech dataset IMSyPP-sl. Slovenian language resource repository CLARIN"
    },
    {
      "citation_id": "16",
      "title": "L3cubemahasent: A marathi tweet-based sentiment analysis dataset",
      "authors": [
        "Atharva Kulkarni",
        "Meet Mandhane",
        "Manali Likhitkar",
        "Gayatri Kshirsagar",
        "Raviraj Joshi"
      ],
      "year": "2021",
      "venue": "L3cubemahasent: A marathi tweet-based sentiment analysis dataset"
    },
    {
      "citation_id": "17",
      "title": "Aggressionannotated corpus of hindi-english code-mixed data",
      "authors": [
        "Ritesh Kumar",
        "Aishwarya Reganti",
        "Akshit Bhatia",
        "Tushar Maheshwari"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018"
    },
    {
      "citation_id": "18",
      "title": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
      "authors": [
        "Dac Viet",
        "Nghia Trung Lai",
        "Amir Ngo",
        "Ben Pouran",
        "Hieu Veyseh",
        "Franck Man",
        "Trung Dernoncourt",
        "Thien Huu Bui",
        "Nguyen"
      ],
      "year": "2023",
      "venue": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
      "doi": "10.48550/arXiv.2304.05613"
    },
    {
      "citation_id": "19",
      "title": "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
      "authors": [
        "Md",
        "M Tahmid Rahman Laskar",
        "Saiful",
        "Mizanur Bari",
        "Md Rahman",
        "Shafiq Amran Hossen Bhuiyan",
        "Jimmy Joty",
        "Huang"
      ],
      "year": "2023",
      "venue": "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
      "doi": "10.48550/arXiv.2305.18486"
    },
    {
      "citation_id": "20",
      "title": "Emotion in code-switching texts: Corpus construction and analysis",
      "authors": [
        "Sophia Lee",
        "Zhongqing Wang"
      ],
      "year": "2015",
      "venue": "Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing",
      "doi": "10.18653/v1/W15-3116"
    },
    {
      "citation_id": "21",
      "title": "Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation",
      "authors": [
        "Haonan Li",
        "Fajri Koto",
        "Minghao Wu",
        "Alham Fikri Aji",
        "Timothy Baldwin"
      ],
      "year": "2023",
      "venue": "Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation",
      "doi": "10.48550/arXiv.2305.15011"
    },
    {
      "citation_id": "22",
      "title": "XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation",
      "authors": [
        "Yaobo Liang",
        "Nan Duan",
        "Yeyun Gong",
        "Ning Wu",
        "Fenfei Guo",
        "Weizhen Qi",
        "Ming Gong",
        "Linjun Shou",
        "Daxin Jiang",
        "Guihong Cao",
        "Xiaodong Fan",
        "Ruofei Zhang",
        "Rahul Agrawal",
        "Edward Cui",
        "Sining Wei",
        "Taroon Bharti",
        "Ying Qiao",
        "Jiun-Hung Chen",
        "Winnie Wu",
        "Shuguang Liu",
        "Fan Yang",
        "Daniel Campos",
        "Rangan Majumder",
        "Ming Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2020.emnlp-main.484"
    },
    {
      "citation_id": "23",
      "title": "Am i a resource-poor language? data sets, embeddings, models and analysis for four different nlp tasks in telugu language",
      "authors": [
        "Mounika Marreddy",
        "Subba Reddy Oota",
        "Lakshmi Sireesha Vakada",
        "Charan Venkata",
        "Radhika Chinni",
        "Mamidi"
      ],
      "year": "2022",
      "venue": "ACM Trans. Asian Low-Resour. Lang. Inf. Process",
      "doi": "10.1145/3531535"
    },
    {
      "citation_id": "24",
      "title": "SemEval 2021 task 7: HaHackathon, detecting and rating humor and offense",
      "authors": [
        "J Meaney",
        "Steven Wilson",
        "Luis Chiruzzo",
        "Adam Lopez",
        "Walid Magdy"
      ],
      "year": "2021",
      "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
      "doi": "10.18653/v1/2021.semeval-1.9"
    },
    {
      "citation_id": "25",
      "title": "It is okay to not be okay: Overcoming emotional bias in affective image captioning by contrastive data collection",
      "authors": [
        "Youssef Mohamed",
        "Faizan Farooq Khan",
        "Kilichbek Haydarov",
        "Mohamed Elhoseiny"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR52688.2022.02058"
    },
    {
      "citation_id": "26",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1001"
    },
    {
      "citation_id": "27",
      "title": "BEEP! Korean corpus of online news comments for toxic speech detection",
      "authors": [
        "Jihyung Moon",
        "Won Ik Cho",
        "Junbum Lee"
      ],
      "year": "2020",
      "venue": "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media"
    },
    {
      "citation_id": "28",
      "title": "Multilingual twitter sentiment classification: The role of human annotators",
      "authors": [
        "Igor Mozetič",
        "Miha Grčar",
        "Jasmina Smailović"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "Overview of OSACT4 Arabic offensive language detection shared task",
      "authors": [
        "Hamdy Mubarak",
        "Kareem Darwish",
        "Walid Magdy",
        "Tamer Elsayed",
        "Hend Al-Khalifa"
      ],
      "year": "2020",
      "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection"
    },
    {
      "citation_id": "30",
      "title": "Crosslingual generalization through multitask finetuning",
      "authors": [
        "Niklas Muennighoff",
        "Thomas Wang",
        "Lintang Sutawika",
        "Adam Roberts",
        "Stella Biderman",
        "Teven Le Scao",
        "M Saiful",
        "Sheng Bari",
        "Shen"
      ],
      "year": "2022",
      "venue": "Crosslingual generalization through multitask finetuning",
      "doi": "10.48550/arXiv.2211.01786"
    },
    {
      "citation_id": "31",
      "title": "2023a. Afrisenti: A twitter sentiment analysis benchmark for african languages",
      "authors": [
        "Shamsuddeen Hassan"
      ],
      "venue": "2023a. Afrisenti: A twitter sentiment analysis benchmark for african languages",
      "doi": "10.48550/arXiv.2302.08956"
    },
    {
      "citation_id": "32",
      "title": "2023b. SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval)",
      "authors": [
        "Shamsuddeen Hassan"
      ],
      "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "33",
      "title": "Chris Chinenye Emezue, Saheed Salahuddeen Abdullahi, Anuoluwapo Aremu, Alipio Jeorge, and Pavel Brazdil. 2022. Naijasenti: A nigerian twitter sentiment corpus for multilingual sentiment analysis",
      "authors": [
        "Shamsuddeen Hassan",
        "David Adelani",
        "Sebastian Ruder",
        "Ibrahim Sa'id Ahmad",
        "Idris Abdulmumin",
        "Shehu Bello Bello",
        "Monojit Choudhury"
      ],
      "venue": "Proceedings of the 13th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "34",
      "title": "L-HSAB: A Levantine Twitter dataset for hate speech and abusive language",
      "authors": [
        "Hala Mulki",
        "Hatem Haddad",
        "Chedi Bechikh Ali",
        "Halima Alshabani"
      ],
      "year": "2019",
      "venue": "Proceedings of the Third Workshop on Abusive Language Online",
      "doi": "10.18653/v1/W19-3512"
    },
    {
      "citation_id": "35",
      "title": "Glottolog/langdoc: Defining dialects, languages, and language families as collections of resources",
      "authors": [
        "Sebastian Nordhoff",
        "Harald Hammarström"
      ],
      "year": "2011",
      "venue": "Proceedings of the First International Workshop on Linked Science"
    },
    {
      "citation_id": "36",
      "title": "XED: A multilingual dataset for sentiment analysis and emotion detection",
      "authors": [
        "Emily Öhman",
        "Marc Pàmies",
        "Kaisla Kajava",
        "Jörg Tiedemann"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020",
      "doi": "10.18653/v1/2020.coling-main.575"
    },
    {
      "citation_id": "37",
      "title": "Sentiment analysis on conversational texts",
      "authors": [
        "Birgitta Ojamaa",
        "Päivi Kristiina Jokinen",
        "Kadri Muischenk"
      ],
      "year": "2015",
      "venue": "Proceedings of the 20th Nordic Conference of Computational Linguistics, NODAL-IDA 2015, Institute of the Lithuanian Language"
    },
    {
      "citation_id": "38",
      "title": "Creating and characterizing a diverse corpus of sarcasm in dialogue",
      "authors": [
        "Shereen Oraby",
        "Vrindavan Harrison",
        "Lena Reed",
        "Ernesto Hernandez",
        "Ellen Riloff",
        "Marilyn Walker"
      ],
      "year": "2016",
      "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
      "doi": "10.18653/v1/W16-3604"
    },
    {
      "citation_id": "39",
      "title": "Overview of the task on irony detection in spanish variants",
      "authors": [
        "Reynier Ortega-Bueno",
        "Francisco Rangel",
        "D Hernández Farıas",
        "Paolo Rosso",
        "Manuel Montes-Y Gómez",
        "José E Medina Pagola"
      ],
      "year": "2019",
      "venue": "Proceedings of the Iberian Languages Evaluation Forum co-located with 35th Conference of the Spanish Society for Natural Language Processing, Iber-LEF@SEPLN 2019"
    },
    {
      "citation_id": "40",
      "title": "Multilingual and multi-aspect hate speech analysis",
      "authors": [
        "Nedjma Ousidhoum",
        "Zizheng Lin",
        "Hongming Zhang",
        "Yangqiu Song",
        "Dit-Yan Yeung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",
      "doi": "10.18653/v1/D19-1474"
    },
    {
      "citation_id": "41",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback"
    },
    {
      "citation_id": "42",
      "title": "Semantic enrichment of nigerian pidgin english for contextual sentiment classification",
      "authors": [
        "Fisayo Wuraola",
        "Olubayo Oyewusi",
        "Olalekan Adekanmbi",
        "Akinsande"
      ],
      "year": "2020",
      "venue": "Semantic enrichment of nigerian pidgin english for contextual sentiment classification",
      "doi": "10.48550/ARXIV.2003.12450"
    },
    {
      "citation_id": "43",
      "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "authors": [
        "Bo Pang",
        "Lillian Lee"
      ],
      "year": "2004",
      "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.3115/1218955.1218990"
    },
    {
      "citation_id": "44",
      "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "authors": [
        "Bo Pang",
        "Lillian Lee"
      ],
      "year": "2005",
      "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)",
      "doi": "10.3115/1219840.1219855"
    },
    {
      "citation_id": "45",
      "title": "Sentiment analysis of code-mixed indian languages: An overview of sail_code-mixed shared task @icon-2017",
      "authors": [
        "Gopal Braja",
        "Dipankar Patra",
        "Amitava Das",
        "Das"
      ],
      "year": "2018",
      "venue": "Sentiment analysis of code-mixed indian languages: An overview of sail_code-mixed shared task @icon-2017"
    },
    {
      "citation_id": "46",
      "title": "Emo-Event: A multilingual emotion corpus based on different events",
      "authors": [
        "Flor Miriam",
        "Plaza Del Arco",
        "Carlo Strapparava",
        "L Alfonso Urena",
        "Maite Lopez",
        "Martin"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "47",
      "title": "Navonil Majumder, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2020",
      "venue": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
      "arxiv": "arXiv:2005.00357"
    },
    {
      "citation_id": "48",
      "title": "Czech dataset for cross-lingual subjectivity classification",
      "authors": [
        "Pavel Pribán",
        "Josef Steinberger"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 2022"
    },
    {
      "citation_id": "49",
      "title": "Sarcasm detection on Czech and English Twitter",
      "authors": [
        "Tomáš Ptáček",
        "Ivan Habernal",
        "Jun Hong"
      ],
      "year": "2014",
      "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers"
    },
    {
      "citation_id": "50",
      "title": "Results of the PolEval 2019 shared task 6: First dataset and open shared task for automatic cyberbullying detection in Polish twitter. Proceedings of the PolEval 2019 Workshop",
      "authors": [
        "Michal Ptaszynski",
        "Agata Pieciukiewicz",
        "Paweł Dybała"
      ],
      "year": "2019",
      "venue": "Results of the PolEval 2019 shared task 6: First dataset and open shared task for automatic cyberbullying detection in Polish twitter. Proceedings of the PolEval 2019 Workshop"
    },
    {
      "citation_id": "51",
      "title": "Is chatgpt a general-purpose natural language processing task solver?",
      "authors": [
        "Chengwei Qin",
        "Aston Zhang",
        "Zhuosheng Zhang",
        "Jiaao Chen",
        "Michihiro Yasunaga",
        "Diyi Yang"
      ],
      "year": "2023",
      "venue": "Is chatgpt a general-purpose natural language processing task solver?",
      "doi": "10.48550/arXiv.2302.06476"
    },
    {
      "citation_id": "52",
      "title": "Sarcasm detection on twitter: A behavioral modeling approach",
      "authors": [
        "Ashwin Rajadesingan",
        "Reza Zafarani",
        "Huan Liu"
      ],
      "year": "2015",
      "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining",
      "doi": "10.1145/2684822.2685316"
    },
    {
      "citation_id": "53",
      "title": "Sarcasm as contrast between a positive sentiment and negative situation",
      "authors": [
        "Luis Rei",
        "Dunja Mladenic",
        "Simon Krek"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "54",
      "title": "Data integration for toxic comment classification: Making more than 40 datasets easily accessible in one unified format",
      "authors": [
        "Julian Risch",
        "Philipp Schmidt",
        "Ralf Krestel"
      ],
      "year": "2021",
      "venue": "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
      "doi": "10.18653/v1/2021.woah-1.17"
    },
    {
      "citation_id": "55",
      "title": "SemEval-2017 task 4: Sentiment analysis in Twitter",
      "authors": [
        "Sara Rosenthal",
        "Noura Farra",
        "Preslav Nakov"
      ],
      "year": "2017",
      "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S17-2088"
    },
    {
      "citation_id": "56",
      "title": "KLEJ: Comprehensive benchmark for polish language understanding",
      "authors": [
        "Piotr Rybak",
        "Robert Mroczkowski",
        "Janusz Tracz",
        "Ireneusz Gawlik"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "57",
      "title": "EmoPars: A collection of 30k emotion-annotated persian social media texts",
      "authors": [
        "Nazanin Sabri",
        "Reyhane Akhavan",
        "Behnam Bahrak"
      ],
      "year": "2021",
      "venue": "EmoPars: A collection of 30k emotion-annotated persian social media texts",
      "doi": "10.26615/issn.2603-2821.2021_023"
    },
    {
      "citation_id": "58",
      "title": "Multitask prompted training enables zero-shot task generalization",
      "authors": [
        "Victor Sanh",
        "Albert Webson",
        "Colin Raffel",
        "Stephen Bach",
        "Lintang Sutawika",
        "Zaid Alyafeai",
        "Antoine Chaffin",
        "Arnaud Stiegler",
        "Arun Raja",
        "Manan Dey",
        "M Saiful Bari",
        "Canwen Xu",
        "Urmish Thakker",
        "Shanya Sharma Sharma",
        "Eliza Szczechla",
        "Taewoon Kim",
        "Gunjan Chhablani",
        "V Nihal",
        "Debajyoti Nayak",
        "Jonathan Datta",
        "Mike Chang",
        "Tian-Jian",
        "Han Jiang",
        "Matteo Wang",
        "Sheng Manica",
        "Zheng Xin Shen",
        "Harshit Yong",
        "Rachel Pandey",
        "Thomas Bawden",
        "Trishala Wang",
        "Jos Neeraj",
        "Abheesht Rozen",
        "Andrea Sharma",
        "Thibault Santilli",
        "Jason Févry",
        "Alan Fries",
        "Ryan Teehan",
        "Le Teven",
        "Stella Scao",
        "Leo Biderman",
        "Thomas Gao",
        "Alexander Wolf",
        "Rush"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations"
    },
    {
      "citation_id": "59",
      "title": "The risk of racial bias in hate speech detection",
      "authors": [
        "Maarten Sap",
        "Dallas Card",
        "Saadia Gabriel",
        "Yejin Choi",
        "Noah Smith"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1163"
    },
    {
      "citation_id": "60",
      "title": "Sboev, Aleksandr Naumov, and Roman B. Rybka. 2020. Data-driven model for emotion detection in russian texts",
      "authors": [
        "Mei Silviana Saputri",
        "Rahmad Mahendra",
        "Mirna Adriani"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence, BICA 2020, Eleventh Annual Meeting of the BICA Society",
      "doi": "10.1016/j.procs.2021.06.075"
    },
    {
      "citation_id": "61",
      "title": "",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilic",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Matthias Yvon",
        "Jonathan Gallé",
        "Alexander Tow",
        "Stella Rush",
        "Albert Biderman",
        "Pawan Webson",
        "Thomas Sasanka Ammanamanchi",
        "Benoît Wang",
        "Niklas Sagot",
        "Albert Muennighoff",
        "Olatunji Villanova Del Moral",
        "Rachel Ruwase",
        "Stas Bawden",
        "Angelina Bekman",
        "Iz Mcmillan-Major",
        "Huu Beltagy",
        "Lucile Nguyen",
        "Samson Saulnier",
        "Pedro Tan",
        "Victor Suarez",
        "Hugo Sanh",
        "Yacine Laurençon",
        "Julien Jernite",
        "Margaret Launay",
        "Mitchell"
      ],
      "venue": "",
      "doi": "10.48550/arXiv.2211.05100"
    },
    {
      "citation_id": "62",
      "title": "YOSM: A new Yorùbá Sentiment Corpus for Movie Reviews",
      "authors": [
        "Iyanuoluwa Shode",
        "David Ifeoluwa Adelani",
        "Anna Feldman"
      ],
      "year": "2022",
      "venue": "AfricaNLP"
    },
    {
      "citation_id": "63",
      "title": "Emohind: Fine-grained multilabel emotion recognition from hindi texts with deep learning",
      "authors": [
        "@ Iclr",
        "Shome"
      ],
      "year": "2021",
      "venue": "12th International Conference on Computing Communication and Networking Technologies, ICCCNT 2021",
      "doi": "10.1109/ICCCNT51525.2021.9579886"
    },
    {
      "citation_id": "64",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "authors": [
        "Richard Socher",
        "Alex Perelygin",
        "Jean Wu",
        "Jason Chuang",
        "Christopher Manning",
        "Andrew Ng",
        "Christopher Potts"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "65",
      "title": "",
      "authors": [
        "Aarohi Srivastava",
        "Abhinav Rastogi",
        "Abhishek Rao",
        "Abu Awal",
        "Md Shoeb",
        "Abubakar Abid",
        "Adam Fisch",
        "Adam Brown",
        "Adam Santoro"
      ],
      "venue": ""
    },
    {
      "citation_id": "66",
      "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "authors": [
        "Andrew Stuhlmüller",
        "Andrew Dai",
        "Andrew La",
        "Andy Lampinen",
        "Angela Zou",
        "Angelica Jiang",
        "Anh Chen",
        "Animesh Vuong",
        "Anna Gupta",
        "Antonio Gottardi",
        "Anu Norelli",
        "Arash Venkatesh",
        "Arfa Gholamidavoodi",
        "Arul Tabassum",
        "Arun Menezes",
        "Asher Kirubarajan",
        "Ashish Mullokandov",
        "Austin Sabharwal",
        "Avia Herrick",
        "Aykut Efrat",
        "Ayla Erdem",
        "Karakas"
      ],
      "year": "2022",
      "venue": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "doi": "10.48550/arXiv.2206.04615"
    },
    {
      "citation_id": "67",
      "title": "Varsha Suresh and Desmond C. Ong. 2021. Not all negatives are equal: Label-aware contrastive loss for fine-grained text classification",
      "authors": [
        "Yu Sun",
        "Shuohuan Wang",
        "Yu-Kun Li",
        "Shikun Feng",
        "Hua Hao Tian",
        "Haifeng Wu",
        "Wang"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "68",
      "title": "Pythainlp/wisesight-sentiment: First release",
      "authors": [
        "Arthit Suriyawongkul",
        "Ekapol Chuangsuwanich",
        "Pattarawat Chormai",
        "Charin Polpanumas"
      ],
      "year": "2019",
      "venue": "Pythainlp/wisesight-sentiment: First release",
      "doi": "10.5281/zenodo.3457447"
    },
    {
      "citation_id": "69",
      "title": "Noriko Takemura, Yuta Nakashima, and Hajime Nagahara. 2022. A Japanese dataset for subjective and objective sentiment polarity classification in micro blog domain",
      "authors": [
        "Haruya Suzuki",
        "Yuto Miyauchi",
        "Kazuki Akiyama",
        "Tomoyuki Kajiwara",
        "Takashi Ninomiya"
      ],
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "70",
      "title": "Making waves: The story of variationist sociolinguistics",
      "authors": [
        "A Sali",
        "Tagliamonte"
      ],
      "year": "2015",
      "venue": "Making waves: The story of variationist sociolinguistics"
    },
    {
      "citation_id": "71",
      "title": "An empirical study of sentiment analysis for chinese documents",
      "authors": [
        "Songbo Tan",
        "Jin Zhang"
      ],
      "year": "2008",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2007.05.028"
    },
    {
      "citation_id": "72",
      "title": "Stanford alpaca: An instruction-following llama model",
      "authors": [
        "Rohan Taori",
        "Ishaan Gulrajani",
        "Tianyi Zhang",
        "Yann Dubois",
        "Xuechen Li",
        "Carlos Guestrin",
        "Percy Liang",
        "Tatsunori Hashimoto"
      ],
      "year": "2023",
      "venue": "Stanford alpaca: An instruction-following llama model"
    },
    {
      "citation_id": "73",
      "title": "SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis",
      "authors": [
        "Mike Thelwall",
        "Kevan Buckley",
        "Georgios Paltoglou",
        "; Hao Tian",
        "Can Gao",
        "Xinyan Xiao",
        "Hao Liu",
        "Bolei He",
        "Hua Wu",
        "Haifeng Wang",
        "Feng Wu"
      ],
      "year": "2012",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.374"
    },
    {
      "citation_id": "74",
      "title": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurélien Rodriguez",
        "Armand Joulin"
      ],
      "venue": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "doi": "10.48550/arXiv.2302.13971"
    },
    {
      "citation_id": "75",
      "title": "SemEval-2018 task 3: Irony detection in English tweets",
      "authors": [
        "Cynthia Van Hee",
        "Els Lefever",
        "Véronique Hoste"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1005"
    },
    {
      "citation_id": "76",
      "title": "Samia Touileb, and Fredrik Jørgensen",
      "authors": [
        "Erik Velldal",
        "Lilja Øvrelid",
        "Alexander Eivind",
        "Cathrine Bergem",
        "Stadsnes"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "77",
      "title": "Directions in abusive language training data, a systematic review: Garbage in, garbage out",
      "authors": [
        "Bertie Vidgen",
        "Leon Derczynski"
      ],
      "year": "2020",
      "venue": "Plos one"
    },
    {
      "citation_id": "78",
      "title": "A dataset for detecting irony in hindi-english code-mixed social media text",
      "authors": [
        "Deepanshu Vijay",
        "Aditya Bohra",
        "Vinay Singh",
        "Syed Sarfaraz Akhtar",
        "Manish Shrivastava"
      ],
      "year": "2018",
      "venue": "Proceedings of 4th Workshop on Sentic Computing, Sentiment Analysis, Opinion Mining, and Emotion Detection (EMSASW 2018) Co-located with the 15th Extended Semantic Web Conference 2018 (ESWC 2018)"
    },
    {
      "citation_id": "79",
      "title": "A corpus for research on deliberation and debate",
      "authors": [
        "Marilyn Walker",
        "Jean Fox Tree",
        "Pranav Anand",
        "Rob Abbott",
        "Joseph King"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "80",
      "title": "How universal and specific is emotional experience? evidence from 27 countries on five continents",
      "authors": [
        "G Harald",
        "Klaus Wallbott",
        "Scherer"
      ],
      "year": "1986",
      "venue": "Social science information"
    },
    {
      "citation_id": "81",
      "title": "S 2 ap: Sequential sentiweibo analysis platform",
      "authors": [
        "Bohan Shuo Wan",
        "Anman Li",
        "Wenhuan Zhang",
        "Donghai Wang",
        "Guan"
      ],
      "year": "2020",
      "venue": "Database Systems for Advanced Applications -25th International Conference, DASFAA 2020",
      "doi": "10.1007/978-3-030-59419-0_49"
    },
    {
      "citation_id": "82",
      "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "authors": [
        "Alex Springer",
        "Amanpreet Wang",
        "Julian Singh",
        "Felix Michael",
        "Omer Hill",
        "Samuel Levy",
        "Bowman"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations"
    },
    {
      "citation_id": "83",
      "title": "Is chatgpt a good sentiment analyzer? A preliminary study",
      "authors": [
        "Zengzhi Wang",
        "Qiming Xie",
        "Zixiang Ding",
        "Yi Feng",
        "Rui Xia"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good sentiment analyzer? A preliminary study",
      "doi": "10.48550/arXiv.2304.04339"
    },
    {
      "citation_id": "84",
      "title": "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
      "authors": [
        "Zeerak Waseem",
        "Dirk Hovy"
      ],
      "year": "2016",
      "venue": "Proceedings of the NAACL Student Research Workshop",
      "doi": "10.18653/v1/N16-2013"
    },
    {
      "citation_id": "85",
      "title": "Indonlu: Benchmark and resources for evaluating indonesian natural language understanding",
      "authors": [
        "Bryan Wilie",
        "Karissa Vincentio",
        "Genta Indra Winata",
        "Samuel Cahyawijaya",
        "Xiaohong Li",
        "Zhi Yuan Lim",
        "Sidik Soleman",
        "Rahmad Mahendra",
        "Pascale Fung",
        "Syafri Bahar",
        "Ayu Purwarianti"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2020"
    },
    {
      "citation_id": "86",
      "title": "Nusax: Multilingual parallel sentiment dataset for 10 indonesian local languages",
      "authors": [
        "Genta Indra Winata",
        "Alham Fikri Aji",
        "Samuel Cahyawijaya",
        "Rahmad Mahendra",
        "Fajri Koto",
        "Ade Romadhony",
        "Kemal Kurniawan",
        "David Moeljadi",
        "Radityo Prasojo",
        "Pascale Fung",
        "Timothy Baldwin",
        "Han Lau",
        "Rico Sennrich",
        "Sebastian Ruder"
      ],
      "year": "2022",
      "venue": "Nusax: Multilingual parallel sentiment dataset for 10 indonesian local languages",
      "doi": "10.48550/arXiv.2205.15960"
    },
    {
      "citation_id": "87",
      "title": "Lamini-lm: A diverse herd of distilled models from large-scale instructions",
      "authors": [
        "Minghao Wu",
        "Abdul Waheed",
        "Chiyu Zhang"
      ],
      "year": "2023",
      "venue": "Lamini-lm: A diverse herd of distilled models from large-scale instructions",
      "doi": "10.48550/arXiv.2304.14402"
    },
    {
      "citation_id": "88",
      "title": "Ciron: a new benchmark dataset for chinese irony detection",
      "authors": [
        "Rong Xiang",
        "Xuefeng Gao",
        "Yunfei Long",
        "Anran Li",
        "Emmanuele Chersoni",
        "Qin Lu",
        "Chu-Ren Huang"
      ],
      "year": "2020",
      "venue": "Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020"
    },
    {
      "citation_id": "89",
      "title": "CLUE: A chinese language understanding evaluation benchmark",
      "authors": [
        "Liang Xu",
        "Hai Hu",
        "Xuanwei Zhang",
        "Lu Li",
        "Chenjie Cao",
        "Yudong Li",
        "Yechen Xu",
        "Kai Sun",
        "Dian Yu",
        "Cong Yu",
        "Yin Tian",
        "Qianqian Dong",
        "Weitang Liu",
        "Bo Shi",
        "Yiming Cui",
        "Junyi Li",
        "Jun Zeng",
        "Rongzhao Wang",
        "Weijian Xie",
        "Yanting Li",
        "Yina Patterson",
        "Zuoyu Tian",
        "Yiwen Zhang",
        "He Zhou",
        "Shaoweihua Liu",
        "Zhe Zhao",
        "Qipeng Zhao",
        "Cong Yue",
        "Xinrui Zhang",
        "Zhengliang Yang",
        "Kyle Richardson",
        "Zhenzhong Lan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020",
      "doi": "10.18653/v1/2020.coling-main.419"
    },
    {
      "citation_id": "90",
      "title": "Aspect sentiment classification with aspect-specific opinion spans",
      "authors": [
        "Lu Xu",
        "Lidong Bing",
        "Wei Lu",
        "Fei Huang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.288"
    },
    {
      "citation_id": "91",
      "title": "Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer",
      "authors": [
        "Linting Xue",
        "Noah Constant",
        "Adam Roberts",
        "Mihir Kale",
        "Rami Al-Rfou",
        "Aditya Siddhant"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
      "doi": "10.18653/v1/2021.naacl-main.41"
    },
    {
      "citation_id": "92",
      "title": "Exploring Amharic sentiment analysis from social media texts: Building annotation tools and classification models",
      "authors": [
        "Hizkiel Seid Muhie Yimam",
        "Abinew Mitiku Alemayehu",
        "Chris Ayele",
        "Biemann"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.91"
    },
    {
      "citation_id": "93",
      "title": "Predicting the type and target of offensive posts in social media",
      "authors": [
        "Marcos Zampieri",
        "Shervin Malmasi",
        "Preslav Nakov",
        "Sara Rosenthal",
        "Noura Farra",
        "Ritesh Kumar"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1144"
    },
    {
      "citation_id": "94",
      "title": "Semeval-2020 task 12: Multilingual offensive language identification in social media (offenseval 2020)",
      "authors": [
        "Marcos Zampieri",
        "Preslav Nakov",
        "Sara Rosenthal",
        "Pepa Atanasova",
        "Georgi Karadzhov",
        "Hamdy Mubarak",
        "Leon Derczynski"
      ],
      "year": "2020",
      "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation, SemEval@COLING 2020",
      "doi": "10.18653/v1/2020.semeval-1.188"
    },
    {
      "citation_id": "95",
      "title": "Improving social meaning detection with pragmatic masking and surrogate fine-tuning",
      "authors": [
        "Chiyu Zhang",
        "Muhammad Abdul-Mageed"
      ],
      "year": "2022",
      "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis",
      "doi": "10.18653/v1/2022.wassa-1.14"
    },
    {
      "citation_id": "96",
      "title": "2023a. Contrastive learning of sociopragmatic meaning in social media",
      "authors": [
        "Chiyu Zhang",
        "Muhammad Abdul-Mageed",
        "Ganesh Jawahar"
      ],
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": "10.18653/v1/2023.findings-acl.152"
    },
    {
      "citation_id": "97",
      "title": "Decay no more: A persistent twitter dataset for learning social meaning",
      "authors": [
        "Chiyu Zhang",
        "Muhammad Abdul-Mageed",
        "El Moatez",
        "Billah Nagoudi"
      ],
      "year": "2022",
      "venue": "Workshop Proceedings of the 16th International AAAI Conference on Web and Social Media, ICWSM 2022 Workshops",
      "doi": "10.36190/2022.92"
    },
    {
      "citation_id": "98",
      "title": "Sinno Jialin Pan, and Lidong Bing. 2023b. Sentiment analysis in the era of large language models: A reality check",
      "authors": [
        "Wenxuan Zhang",
        "Yue Deng",
        "Bing Liu"
      ],
      "venue": "Sinno Jialin Pan, and Lidong Bing. 2023b. Sentiment analysis in the era of large language models: A reality check",
      "doi": "10.48550/arXiv.2305.15005"
    },
    {
      "citation_id": "99",
      "title": "Can chatgpt understand too? A comparative study on chatgpt and fine-tuned BERT",
      "authors": [
        "Qihuang Zhong",
        "Liang Ding",
        "Juhua Liu",
        "Bo Du",
        "Dacheng Tao"
      ],
      "year": "2023",
      "venue": "Can chatgpt understand too? A comparative study on chatgpt and fine-tuned BERT",
      "doi": "10.48550/arXiv.2302.10198"
    },
    {
      "citation_id": "100",
      "title": "Can large language models transform computational social science?",
      "authors": [
        "Caleb Ziems",
        "William Held",
        "Omar Shaikh",
        "Jiaao Chen",
        "Zhehao Zhang",
        "Diyi Yang"
      ],
      "year": "2023",
      "venue": "Can large language models transform computational social science?",
      "doi": "10.48550/arXiv.2305.03514"
    },
    {
      "citation_id": "101",
      "title": "The best performance in each language is bold, and the second best is in green highlight",
      "venue": "The red font denotes a performance lower than the random baseline. BMZ: BLOOMZ, CG: ChatGPT, MT: using machine"
    }
  ]
}