{
  "paper_id": "2503.22715v1",
  "title": "Hierarchical Adaptive Expert For Multimodal Sentiment Analysis",
  "published": "2025-03-25T09:52:08Z",
  "authors": [
    "Jiahao Qin",
    "Feng Liu",
    "Lu Zong"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal sentiment analysis has emerged as a critical tool for understanding human emotions across diverse communication channels. While existing methods have made significant strides, they often struggle to effectively differentiate and integrate modality-shared and modality-specific information, limiting the performance of multimodal learning. To address this challenge, we propose the Hierarchical Adaptive Expert for Multimodal Sentiment Analysis (HAEMSA), a novel framework that synergistically combines evolutionary optimization, cross-modal knowledge transfer, and multi-task learning. HAEMSA employs a hierarchical structure of adaptive experts to capture both global and local modality representations, enabling more nuanced sentiment analysis. Our approach leverages evolutionary algorithms to dynamically optimize network architectures and modality combinations, adapting to both partial and full modality scenarios. Extensive experiments demonstrate HAEMSA's superior performance across multiple benchmark datasets. On CMU-MOSEI, HAEMSA achieves a 2.6% increase in 7-class accuracy and a 0.059 decrease in MAE compared to the previous best method. For CMU-MOSI, we observe a 6.3% improvement in 7-class accuracy and a 0.058 reduction in MAE. On IEMOCAP, HAEMSA outperforms the state-of-the-art by 2.84% in weighted-F1 score for emotion recognition. These results underscore HAEMSA's effectiveness in capturing complex multimodal interactions and generalizing across different emotional contexts. The code will be available on GitHub soon.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal sentiment analysis has emerged as an important research area in recent years, aiming to interpret human emotions and sentiments by integrating information from multiple modalities, such as text, audio, and visual cues  [3, 13] . This field has significant implications for various applications, including human-computer interaction, customer analysis, and social media computing  [26, 48] . Recent advances in deep learning have enabled the development of more sophisticated multimodal sentiment analysis models  [42, 46] . These models attempt to capture complex interactions among modalities and leverage complementary information to improve sentiment prediction accuracy  [5, 20, 32] . However, existing approaches often struggle to effectively combine modality-specific representations and fully capture the intricate dynamics of multimodal data  [9, [29] [30] [31] 39] . To address these challenges, researchers have explored various approaches, in-cluding hierarchical architectures for learning representations at different granularities  [4, 21] , and multi-task learning (MTL) techniques for leveraging knowledge from related tasks  [1, 24] .\n\nHowever, designing effective hierarchical architectures and MTL frameworks often requires significant manual effort and domain expertise. This is where evolutionary algorithms may offer a potential solution. These algorithms, inspired by biological evolution, have been successfully applied to various aspects of deep learning, such as architecture search and hyperparameter optimization.\n\nIn the context of multimodal sentiment analysis, evolutionary algorithms present several potential advantages: They can automate the search for optimal architectures and modality combinations, potentially reducing the need for extensive manual design; Their population-based search and recombination mechanisms allow for the exploration of diverse architectural variants; They offer the potential to balance multiple optimization objectives, such as prediction accuracy, modality fusion effectiveness, and computational efficiency.\n\nInspired by these insights, we propose the Hierarchical Adaptive Expert for Multimodal Sentiment Analysis (HAEMSA) framework. HAEMSA leverages evolutionary algorithms to develop a hierarchical structure of specialized subnetworks, each serving as an adaptive expert. These experts collaboratively learn to represent and combine information from multiple modalities at varying levels of granularity, while also informing the learning process of modality-specific components. Figure  1  demonstrates the impact of different population sizes in the evolutionary process on model performance, highlighting the importance of this approach. The main contributions of our work are as follows:\n\n• We introduce HAEMSA, a novel framework that uses evolutionary algorithms to optimize hierarchical architectures for multimodal sentiment analysis. This approach enables the capture of intricate inter-modal interactions and dependencies at multiple granularities, improving upon existing flat or manually designed architectures.\n\n• We develop a targeted knowledge transfer mechanism that facilitates the sharing of learned representations across modalities and related tasks. This mechanism enhances multimodal learning by leveraging shared patterns and relationships, leading to more robust and generalizable representations.\n\n• We integrate HAEMSA with a multi-task learning approach, enabling the model to simultaneously learn from multiple sentiment analysis tasks. This integration allows for the capture of task interdependencies, resulting in improved performance across various sentiment analysis subtasks.\n\n• We conduct comprehensive experiments on three benchmark datasets: CMU-MOSI, CMU-MOSEI  [47] , and IEMOCAP  [2] . Our results demonstrate HAEMSA's effectiveness across various multimodal sentiment analysis tasks. Specifically, we achieve improvements of up to 6.3% in 7-class accuracy on CMU-MOSI, 5.6% on CMU-MOSEI compared to state-of-the-art methods, and a 2.84% increase in weighted-F1 score for emotion recognition on IEMO-CAP. These consistent improvements across diverse datasets underscore the robustness and generalizability of our approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Sentiment Analysis",
      "text": "Multimodal sentiment analysis has seen significant advancements with the advent of deep learning. Recent approaches focus on capturing complex interactions among modalities  [8, 27] . Notable works include the contextual inter-modal attention framework by Ghosal et al.  [5]  and the Multi-attention Recurrent Network (MARN) by Zadeh et al.  [47] .\n\nDespite these advancements, challenges persist in effectively combining modality-specific representations and generalizing to unseen data  [9, 38] . To address these limitations, researchers have explored multi-task learning  [1]  and adversarial training  [7]  to improve model generalization and adaptability.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Evolutionary Approaches And Hierarchical Architectures",
      "text": "Evolutionary algorithms have shown promise in multimodal learning, particularly in neural architecture search  [15, 33] . These methods automatically discover optimal architectures by evolving a population of candidates based on their performance. In multimodal sentiment analysis, evolutionary approaches offer the potential to reduce manual design efforts  [25] . Hierarchical architectures have been explored to capture the structure of multimodal data at different granularities  [22, 41] . These typically consist of modality-specific layers followed by cross-modal layers to capture inter-modal interactions  [16] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Task Learning In Sentiment Analysis",
      "text": "Multi-task learning (MTL) has been widely adopted in sentiment analysis to leverage knowledge from related tasks  [50] . In multimodal sentiment analysis, MTL approaches often incorporate hierarchical structures to capture crossmodal interactions effectively  [44] . However, designing ef-fective MTL architectures for multimodal sentiment analysis remains challenging. It requires balancing contributions from different modalities and tasks while capturing their intricate interactions  [38] . Careful selection and weighting of auxiliary tasks are crucial to avoid negative transfer and ensure positive knowledge sharing  [34] .\n\nOur proposed Hierarchical Adaptive Expert for Multimodal Sentiment Analysis (HAEMSA) framework addresses these challenges by integrating evolutionary algorithms with multi-task learning. HAEMSA evolves a hierarchical structure of specialized subnetworks, optimizing representations at various modality granularities. This approach enables effective learning of cross-modal interactions and task dependencies, advancing the state-of-the-art in multimodal sentiment analysis and emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we introduce the overall architecture and module details of the Hierarchical Adaptive Expert for Multimodal Sentiment Analysis (HAEMSA) framework for multi-modal sentiment analysis. Sequentially, we describe the hierarchical expert network structure, the evolutionbased adaptive structural parameter optimization process, and the cross-modal, cross-task knowledge transfer mechanism.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hierarchical Adaptive Expert Network",
      "text": "The hierarchical expert structure consists of multiple levels of experts that learn representations at different levels of granularity. Let x t , x a , and x v denote the input features for the text, audio, and visual modalities, respectively. The modality-specific representations learned by the finegrained experts can be expressed as:\n\nwhere f t , f a , and f v are the fine-grained expert functions parameterized by θ t , θ a , and θ v , respectively, and h t , h a , and h v are the resulting modality-specific representations.\n\nThe modality-shared representations learned by the unified experts can be expressed as:\n\nwhere f s is the unified expert function parameterized by θ s , [•; •; •] denotes concatenation, and h s is the resulting modality-shared representation. The hierarchical experts learn to fuse the modality-specific and modality-shared representations at different levels of granularity. Let h (i) l denote the fused representation at the i-th level of the hierarchy, where l ∈ t, a, v, s indicates the modality. The fused representations can be computed as:\n\nwhere g\n\nl is the fusion function at the i-th level for modality l, parameterized by ϕ (i) l , and h (i-1) l and h (i-1) s are the fused representations from the previous level.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evolutionary Optimization For Automated Mtl",
      "text": "The HAEMSA framework incorporates multi-task learning to leverage the complementary information from related sentiment analysis tasks. Let T denote the set of tasks, which includes sentiment classification, emotion recognition, and sentiment intensity prediction. For each task t ∈ T , a task-specific loss L t is defined based on the ground-truth labels and the predicted outputs. The multitask learning objective is formulated as a weighted sum of the task-specific losses:\n\nwhere w t is the weight assigned to task t. The weights w t are learned adaptively during the training process using a task-specific attention mechanism. The attention mechanism learns to assign higher weights to the tasks that are more relevant for sentiment analysis, allowing the model to focus on the most informative tasks. The total loss for the HAEMSA framework is a combination of the multi-task learning loss and the knowledge transfer loss:\n\nwhere L KT is the knowledge transfer loss, and β is a hyperparameter that balances the contributions of the multitask learning and knowledge transfer objectives. By jointly optimizing the multi-task learning and knowledge transfer objectives, the HAEMSA framework effectively leverages the complementary information from related tasks and the knowledge captured by the unified experts, leading to improved sentiment analysis performance.\n\nThe structure and weights of the hierarchical experts are optimized through an evolutionary process. Let P denote the population of candidate architectures, where each individual p ∈ P represents a specific configuration of the hierarchical experts. The evolutionary process iteratively updates the population to discover optimal architectures and modality combinations. The fitness of each individual p is evaluated based on its performance on a validation set Dval. The fitness function F(p) can be expressed as:  where L is a loss function that measures the discrepancy between the predicted sentiment p(x) and the ground-truth sentiment y. The evolutionary process employs genetic operators, such as mutation and crossover, to generate new individuals. The mutation operator introduces random per-turbations to the architecture and weights of an individual, while the crossover operator combines the genetic information of two parent individuals to create offspring. Let p i and p j be two parent individuals selected for crossover. The off-spring p o is generated as follows:\n\nwhere α ∈ [0, 1] is a random weight that determines the contribution of each parent. The mutation operator applies random modifications to an individual p. The modified individual p ′ is obtained as:\n\nwhere N (0, σ) is a Gaussian noise with mean 0 and standard deviation σ. The selection of individuals for the next generation is based on their fitness values. We employ tournament selection, where k individuals are randomly sampled from the population, and the one with the highest fitness is selected as a parent for the next generation. The evolutionary process continues for a fixed number of generations or until a convergence criterion is met. The fittest individual in the final population represents the optimized architecture and weights of the hierarchical experts.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Cross-Modal Joint Optimization",
      "text": "The HAEMSA framework employs a joint optimization approach to effectively integrate knowledge across modalities and tasks. This approach combines knowledge transfer between unified and modality-specific experts with a multitask learning objective.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Knowledge Transfer Optimization",
      "text": "The knowledge learned by the unified experts is transferred to the modality-specific experts to guide their learning. The knowledge transfer is achieved by minimizing the Kullback-Leibler (KL) divergence between the output probability distributions of the unified experts and the modalityspecific experts. The KL divergence loss for the text modality can be expressed as:\n\nwhere p s (y i |x i ) and p t (y i |x i ) are the output probability distributions of the unified expert and the text-specific expert, respectively, for the i-th sample x i . Similarly, the KL divergence losses for the audio and visual modalities can be defined as:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Task Learning Optimization",
      "text": "The HAEMSA framework incorporates a multi-task learning approach to leverage complementary information from related sentiment analysis tasks. For each task t ∈ T , a task-specific loss L t is defined based on the ground-truth labels and the predicted outputs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Joint Loss Function",
      "text": "The total loss for training the modality-specific experts is a weighted combination of the KL divergence losses and the task-specific losses:\n\nwhere L task1 , Ltask2, Ltask3, and Ltask 4 are the taskspecific losses, respectively, and λ 1 , λ 2 , λ 3 , λ 4 , and γ are hyperparameters that control the balance between the taskspecific losses and the KL divergence losses.\n\nThis joint optimization approach allows the HAEMSA framework to effectively learn from multiple modalities and tasks while ensuring that knowledge is shared across the hierarchical structure of experts.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "To evaluate the effectiveness of the proposed HAEMSA framework, we conduct extensive experiments on benchmark datasets for multimodal sentiment analysis. In this section, we describe the datasets, experimental setup, and results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We utilize three widely adopted datasets for multimodal sentiment analysis. CMU-MOSI  [47]  contains 2,199 video clips from 93 speakers, annotated for sentiment on a scale from -3 to +3, split into 1,284 training, 229 validation, and 686 testing clips. CMU-MOSEI  [47] , an extension of CMU-MOSI, comprises 23,454 clips from 1,000 speakers, with 16,326 for training, 1,871 for validation, and 4,659 for testing. IEMOCAP  [2]  includes 12 hours of audiovisual data from 10 actors, featuring 10,039 utterances annotated with categorical emotions and dimensional attributes, divided into 5 sessions. All datasets provide pre-extracted features for text, audio, and visual modalities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We compare the performance of the HAEMSA framework with several state-of-the-art methods for multimodal sentiment analysis, including:\n\n• TFN  [49] : Uses tensor factorization for multimodal fusion. • LMF  [17] : Employs low-rank tensors for efficient multimodal fusion.\n\n• MulT  [40] : Leverages cross-modal transformers for modality translation.\n\n• Self-MM  [43] : The Self-Supervised Multi-task Multimodal sentiment analysis network incorporates an unimodal label generation module, grounded in selfsupervised learning principles, to investigate the potential of unimodal supervision.\n\n• MMIM  [7] : The MultiModal InfoMax framework introduces a stratified approach for maximizing mutual information, facilitating the model's capacity to cultivate cohesive representations across various modalities.\n\n• TFR-Net  [45] : The Transformer-based Feature Reconstruction Network integrates intra-and inter-modal attention mechanisms alongside a feature reconstruction component to effectively address the issue of sporadic missing features in non-aligned multimodal sequences.\n\n• AMML  [38] : The Adaptive Multimodal Meta-Learning framework employs a methodology derived from meta-learning to refine unimodal representations, which are subsequently adjusted to enhance multimodal integration.\n\n• EMT  [37] : Utilizes dual-level feature restoration techniques to enhance the robustness and accuracy of multimodal sentiment analysis.\n\n• CRNet  [35] : Interacts coordinated audio-visual representations to construct enhanced linguistic representations for improved sentiment analysis.\n\n• BC-LSTM  [28] : A Bidirectional Contextual LSTM model that captures context in conversations to improve emotion recognition.\n\n• DialogueRNN  [23] : An attentive RNN designed for tracking emotional dynamics across conversational turns.\n\n• DialogueGCN  [6] : Employs a Graph Convolutional Network to model dialogue structure, enhancing emotion detection.\n\n• IterativeERC  [18] : An iterative network that refines emotion recognition predictions through successive stages.\n\n• QMNN  [14] : Utilizes quantum computing concepts to enhance conversational emotion recognition by capturing complex relationships in data.\n\n• MMGCN  [12] : A Multimodal Graph Convolution Network that integrates various modalities to improve emotion recognition accuracy in conversations.\n\n• MVN  [19] : Combines different perspectives for realtime emotion recognition, leveraging multiple data views for enhanced analysis.\n\n• UniMSE  [10] : Aims for unified multimodal sentiment analysis and emotion recognition, focusing on consistent performance across various metrics.\n\n• MultiEMO [36]: A novel attention-based relevanceaware multimodal fusion framework that effectively integrates cross-modal cues.\n\n• UniMEEC  [11] : A framework innovatively unifies multimodal emotion recognition in conversation (MERC) and multimodal emotion-cause pair extraction (MECPE) by reformulating them as mask prediction tasks, leveraging shared prompt learning across modalities, and employing task-specific hierarchical context aggregation.\n\nTo ensure robustness, we report results averaged over ten runs with different random seeds. This rigorous experimental setup allows for a fair and comprehensive evaluation of HAEMSA against a wide spectrum of existing approaches in the field. For MOSI and MOSEI, we report the binary accuracy (Acc-2), 7-class accuracy (Acc-7) and F1 score. For IEMOCAP, we report the weighted-F1 and unweighted accuracy (UA) for emotion classification. Weighted-F1 is the weighted average of F1 scores for all emotion categories while UA is the average accuracy of each class.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "We present a comprehensive evaluation of the proposed HAEMSA framework, comparing its performance against state-of-the-art methods and analyzing the contribution of each component through ablation studies.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison To State-Of-The-Art",
      "text": "As shown in Table  1  and 2 , HAEMSA consistently outperforms existing methods across all datasets and metrics. On CMU-MOSEI, HAEMSA achieves a 2.6% increase in Acc-7 and a 0.059 decrease in MAE compared to the next best method. For CMU-MOSI, we observe a 6.3% improvement in Acc-7 and a 0.058 reduction in MAE. On IEMO-CAP, HAEMSA surpasses the state-of-the-art by 2.84% in weighted-F1 score. These results demonstrate HAEMSA's effectiveness in capturing complex multimodal interactions and generalizing across different emotional contexts.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "To gain deeper insights into the efficacy of each component within the HAEMSA framework, we conducted comprehensive ablation studies across three benchmark datasets: CMU-MOSEI, CMU-MOSI, and IEMOCAP. This systematic analysis aims to quantify the individual contributions of key components and their synergistic effects on the overall performance of our proposed model. The detailed results of these ablation experiments are presented in Tables 3 and 4, offering a nuanced view of how each element impacts the model's effectiveness in multimodal sentiment analysis and emotion recognition tasks.\n\nThe ablation study conducted across CMU-MOSEI, CMU-MOSI, and IEMOCAP datasets consistently demonstrates the crucial role of each HAEMSA component in its overall performance. Results are presented in Tables  3  and 4 . Removing the hierarchical structure (w/o Hierarchy) leads to substantial performance drops across all metrics and emotion categories, underscoring the importance of learning representations at different granularity levels. The evolutionary optimization process (w/o Evolution) proves vital, with its removal resulting in decreased performance, particularly evident in complex emotions like excitement and frustration on IEMOCAP. Both cross-modal integration and multi-task learning components show significant contributions, as their individual removals lead to notable performance degradation across all datasets. These findings highlight the synergistic effect of HAEMSA's components in effectively capturing and utilizing multimodal information for sentiment analysis and emotion recognition tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization Analysis",
      "text": "To provide insights into HAEMSA's learned representations, we visualize the multimodal embeddings using t-SNE. Figure  3  presents the t-SNE plots for different ablation settings.\n\nThe full HAEMSA model (Figure  3 (a)) shows clear separation between sentiment classes on CMU-MOSEI, indicating its ability to learn discriminative features. In contrast, ablated versions exhibit less distinct clusters, highlighting the importance of each component in learning effective representations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "The HAEMSA framework, while demonstrating superior performance across multiple datasets, presents certain limitations that warrant discussion. The evolutionary optimization process, though effective, increases the computational cost compared to simpler fusion methods. This heightened complexity may restrict HAEMSA's applicability in resource-constrained or real-time scenarios, potentially limiting its use in certain practical applications.\n\nAnother consideration is the current implementation's assumption of complete modality availability for each data sample. In real-world applications, missing or incomplete modalities are common occurrences, presenting a challenge for the model's robustness and generalizability. This limitation highlights the need for adaptability in handling varied input conditions. Despite significant improvements in emotion recognition, HAEMSA still faces challenges in distinguishing between closely related emotional states, particularly evident in the IEMOCAP dataset results. This limitation points to the need for more nuanced emotional modeling and representation learning.\n\nFurthermore, the model's performance in cross-lingual or cross-cultural settings remains unexplored. Given the diverse nature of emotional expression across cultures, this aspect is crucial for the broader applicability of HAEMSA in global contexts. These limitations, while not diminishing the model's achievements, provide clear directions for future research and improvement.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces HAEMSA (Hierarchical Adaptive Expert for Multimodal Sentiment Analysis), a novel framework that integrates hierarchical mixture-of-experts networks, evolutionary optimization, and multi-task learning for multimodal sentiment analysis and emotion recogni-  tion. HAEMSA's hierarchical structure enables learning at various levels of modality granularity, while its evolutionary process optimizes expert architectures for effective crossmodal knowledge transfer. The incorporation of multi-task learning allows HAEMSA to capture task interdependencies, potentially improving overall performance.\n\nExperiments on CMU-MOSI, CMU-MOSEI, and IEMOCAP datasets demonstrate HAEMSA's effectiveness compared to existing methods, showing improvements in accuracy and MAE across different tasks. These results suggest that HAEMSA can effectively capture complex multimodal interactions and generalize across various emotional contexts.\n\nFuture work could focus on addressing current limitations and expanding HAEMSA's capabilities. This may include optimizing computational efficiency to enhance realworld applicability, developing mechanisms for handling missing or incomplete modalities, and refining the model's ability to distinguish between nuanced emotional states. Additionally, exploring HAEMSA's performance in cross-lingual and cross-cultural contexts could broaden its impact. Such advancements could contribute to the wider field of multimodal sentiment analysis and emotion recognition, potentially opening new avenues for applications in areas such as human-computer interaction, mental health monitoring, and social media analysis.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the Hierarchical Adaptive Expert Network",
      "page": 1
    },
    {
      "caption": "Figure 1: demonstrates the",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of the Hierarchical Adaptive Expert Network (HAEN) for MSA. The network consists of five main components: (A)",
      "page": 4
    },
    {
      "caption": "Figure 3: presents the t-SNE plots for different abla-",
      "page": 7
    },
    {
      "caption": "Figure 3: (a)) shows clear sep-",
      "page": 7
    },
    {
      "caption": "Figure 3: t-SNE visualization of multimodal embeddings on CMU-MOSEI.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "lu.zong@xjtlu.edu.cn": "Abstract"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "Multimodal sentiment analysis has emerged as a critical"
        },
        {
          "lu.zong@xjtlu.edu.cn": "tool for understanding human emotions across diverse com-"
        },
        {
          "lu.zong@xjtlu.edu.cn": "munication channels. While existing methods have made"
        },
        {
          "lu.zong@xjtlu.edu.cn": "significant strides,\nthey often struggle to effectively differ-"
        },
        {
          "lu.zong@xjtlu.edu.cn": "entiate and integrate modality-shared and modality-specific"
        },
        {
          "lu.zong@xjtlu.edu.cn": "information, limiting the performance of multimodal learn-"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "ing.\nTo address\nthis\nchallenge, we propose\nthe Hierar-"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "chical Adaptive Expert\nfor Multimodal Sentiment Analysis"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "(HAEMSA), a novel\nframework that\nsynergistically com-"
        },
        {
          "lu.zong@xjtlu.edu.cn": "bines\nevolutionary\noptimization,\ncross-modal\nknowledge"
        },
        {
          "lu.zong@xjtlu.edu.cn": "transfer, and multi-task learning. HAEMSA employs a hier-"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "archical structure of adaptive experts to capture both global"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "and local modality representations, enabling more nuanced"
        },
        {
          "lu.zong@xjtlu.edu.cn": "sentiment analysis. Our approach leverages evolutionary"
        },
        {
          "lu.zong@xjtlu.edu.cn": "algorithms to dynamically optimize network architectures"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "and modality combinations, adapting to both partial and"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "full modality scenarios. Extensive experiments demonstrate"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "HAEMSA’s\nsuperior performance across multiple bench-"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "mark datasets.\nOn CMU-MOSEI, HAEMSA achieves a"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "2.6% increase in 7-class accuracy and a 0.059 decrease in"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "MAE compared to the previous best method.\nFor CMU-"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "MOSI, we observe a 6.3% improvement in 7-class accuracy"
        },
        {
          "lu.zong@xjtlu.edu.cn": "and a 0.058 reduction in MAE. On IEMOCAP, HAEMSA"
        },
        {
          "lu.zong@xjtlu.edu.cn": "outperforms the state-of-the-art by 2.84% in weighted-F1"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "score for emotion recognition.\nThese results underscore"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "HAEMSA’s effectiveness in capturing complex multimodal"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "interactions\nand\ngeneralizing\nacross\ndifferent\nemotional"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "contexts. The code will be available on GitHub soon."
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "1. Introduction"
        },
        {
          "lu.zong@xjtlu.edu.cn": ""
        },
        {
          "lu.zong@xjtlu.edu.cn": "Multimodal\nsentiment analysis has emerged as an im-"
        },
        {
          "lu.zong@xjtlu.edu.cn": "portant\nresearch area in recent years,\naiming to interpret"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "follows:"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "• We introduce HAEMSA, a novel framework that uses"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "evolutionary algorithms to optimize hierarchical archi-"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "tectures for multimodal sentiment analysis.\nThis ap-"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "proach enables the capture of intricate inter-modal\nin-"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "teractions and dependencies at multiple granularities,"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "improving upon existing flat or manually designed ar-"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "chitectures."
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "• We develop a targeted knowledge transfer mechanism"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "that\nfacilitates\nthe sharing of\nlearned representations"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "across modalities and related tasks. This mechanism"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "enhances multimodal\nlearning by leveraging shared"
        },
        {
          "this approach. The main contributions of our work are as": "patterns and relationships,\nleading to more robust and"
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "generalizable representations."
        },
        {
          "this approach. The main contributions of our work are as": ""
        },
        {
          "this approach. The main contributions of our work are as": "• We integrate HAEMSA with a multi-task learning ap-"
        },
        {
          "this approach. The main contributions of our work are as": "proach,\nenabling the model\nto simultaneously learn"
        },
        {
          "this approach. The main contributions of our work are as": "from multiple sentiment analysis tasks. This integra-"
        },
        {
          "this approach. The main contributions of our work are as": "tion allows for the capture of task interdependencies,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "tions at different granularities [4, 21], and multi-task learn-",
          "resulting in improved performance across various sen-": "timent analysis subtasks."
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "ing (MTL)\ntechniques for\nleveraging knowledge from re-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "• We\nconduct\ncomprehensive\nexperiments\non\nthree"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "lated tasks [1, 24].",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "benchmark\ndatasets:\nCMU-MOSI,\nCMU-MOSEI"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "However, designing effective hierarchical architectures",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "[47],\nand IEMOCAP [2].\nOur\nresults demonstrate"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "and MTL frameworks often requires significant manual ef-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "HAEMSA’s effectiveness across various multimodal"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "fort and domain expertise. This is where evolutionary al-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "sentiment\nanalysis\ntasks.\nSpecifically, we\nachieve"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "gorithms may offer a potential solution. These algorithms,",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "improvements\nof\nup\nto\n6.3% in\n7-class\naccuracy"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "inspired by biological evolution, have been successfully ap-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "on CMU-MOSI,\n5.6% on CMU-MOSEI\ncompared"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "plied to various aspects of deep learning, such as architec-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "to state-of-the-art methods, and a 2.84% increase in"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "ture search and hyperparameter optimization.",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "weighted-F1 score for emotion recognition on IEMO-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "In the context of multimodal\nsentiment analysis,\nevo-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "CAP. These consistent\nimprovements across diverse"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "lutionary algorithms present\nseveral potential advantages:",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "datasets underscore the robustness and generalizabil-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "They can automate the search for optimal architectures and",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "ity of our approach."
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "modality combinations, potentially reducing the need for",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "extensive manual design; Their population-based search",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "2. Related Work"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "and recombination mechanisms allow for\nthe exploration",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "of diverse architectural variants; They offer the potential to",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "2.1. Multimodal Sentiment Analysis"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "balance multiple optimization objectives, such as prediction",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "accuracy, modality fusion effectiveness, and computational",
          "resulting in improved performance across various sen-": "Multimodal sentiment analysis has seen significant ad-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "efficiency.",
          "resulting in improved performance across various sen-": "vancements with the advent of deep learning. Recent ap-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "Inspired by these\ninsights, we propose\nthe Hierarchi-",
          "resulting in improved performance across various sen-": "proaches\nfocus on capturing complex interactions among"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "cal Adaptive Expert\nfor Multimodal Sentiment Analysis",
          "resulting in improved performance across various sen-": "modalities\n[8, 27].\nNotable works\ninclude the contextual"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "(HAEMSA)\nframework.\nHAEMSA leverages evolution-",
          "resulting in improved performance across various sen-": "inter-modal attention framework by Ghosal et al.\n[5] and"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "ary algorithms to develop a hierarchical structure of spe-",
          "resulting in improved performance across various sen-": "the Multi-attention Recurrent Network (MARN) by Zadeh"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "cialized subnetworks, each serving as an adaptive expert.",
          "resulting in improved performance across various sen-": "et al. [47]."
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "These experts collaboratively learn to represent and com-",
          "resulting in improved performance across various sen-": "Despite these advancements, challenges persist in effec-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "bine information from multiple modalities at varying levels",
          "resulting in improved performance across various sen-": "tively combining modality-specific representations and gen-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "of granularity, while also informing the learning process of",
          "resulting in improved performance across various sen-": "eralizing to unseen data [9,38]. To address these limitations,"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "modality-specific components.\nFigure 1 demonstrates the",
          "resulting in improved performance across various sen-": "researchers have explored multi-task learning [1] and ad-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "impact of different population sizes in the evolutionary pro-",
          "resulting in improved performance across various sen-": "versarial\ntraining [7] to improve model generalization and"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "cess on model performance, highlighting the importance of",
          "resulting in improved performance across various sen-": "adaptability."
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "this approach. The main contributions of our work are as",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "2.2. Evolutionary Approaches and Hierarchical Ar-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "follows:",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "chitectures"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "• We introduce HAEMSA, a novel framework that uses",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "Evolutionary algorithms have shown promise in multi-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "evolutionary algorithms to optimize hierarchical archi-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "modal\nlearning, particularly in neural architecture search"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "tectures for multimodal sentiment analysis.\nThis ap-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "[15, 33]. These methods automatically discover optimal ar-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "proach enables the capture of intricate inter-modal\nin-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "chitectures by evolving a population of candidates based on"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "teractions and dependencies at multiple granularities,",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "their performance.\nIn multimodal sentiment analysis, evo-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "improving upon existing flat or manually designed ar-",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "lutionary approaches offer\nthe potential\nto reduce manual"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "chitectures.",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "design efforts\n[25].\nHierarchical architectures have been"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "explored to capture the structure of multimodal data at dif-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "• We develop a targeted knowledge transfer mechanism",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "ferent granularities\n[22, 41].\nThese\ntypically consist of"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "that\nfacilitates\nthe sharing of\nlearned representations",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "modality-specific layers followed by cross-modal\nlayers to"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "across modalities and related tasks. This mechanism",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "capture inter-modal interactions [16]."
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "enhances multimodal\nlearning by leveraging shared",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "patterns and relationships,\nleading to more robust and",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "2.3. Multi-task Learning in Sentiment Analysis"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "generalizable representations.",
          "resulting in improved performance across various sen-": ""
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "",
          "resulting in improved performance across various sen-": "Multi-task learning (MTL) has been widely adopted in"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "• We integrate HAEMSA with a multi-task learning ap-",
          "resulting in improved performance across various sen-": "sentiment analysis to leverage knowledge from related tasks"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "proach,\nenabling the model\nto simultaneously learn",
          "resulting in improved performance across various sen-": "[50].\nIn multimodal sentiment analysis, MTL approaches"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "from multiple sentiment analysis tasks. This integra-",
          "resulting in improved performance across various sen-": "often incorporate hierarchical\nstructures\nto capture cross-"
        },
        {
          "cluding hierarchical architectures\nfor\nlearning representa-": "tion allows for the capture of task interdependencies,",
          "resulting in improved performance across various sen-": "modal interactions effectively [44]. However, designing ef-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "fective MTL architectures for multimodal sentiment analy-": "sis remains challenging. It requires balancing contributions",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "from different modalities and tasks while capturing their in-",
          "representations can be computed as:": "h(i)\n= g(i)\n([h(i−1)\n; h(i−1)\n]; ϕ(i)\n)\n(5)"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "s\nl\nl\nl\nl"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "tricate interactions [38].\nCareful selection and weighting",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "of auxiliary tasks are crucial\nto avoid negative transfer and",
          "representations can be computed as:": "where g(i)\nis the fusion function at the i-th level for modal-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "l"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "ensure positive knowledge sharing [34].",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "ity l, parameterized by ϕ(i)\n, and h(i−1)\nand h(i−1)\nare the"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "l\nl"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "Our proposed Hierarchical Adaptive Expert\nfor Mul-",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "fused representations from the previous level."
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "timodal Sentiment Analysis\n(HAEMSA)\nframework\nad-",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "dresses these challenges by integrating evolutionary algo-",
          "representations can be computed as:": "3.2.\nEvolutionary Optimization\nfor Automated"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "rithms with multi-task learning. HAEMSA evolves a hi-",
          "representations can be computed as:": "MTL"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "erarchical structure of specialized subnetworks, optimizing",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "The HAEMSA framework incorporates multi-task learn-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "representations at various modality granularities. This ap-",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "ing to leverage the complementary information from related"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "proach enables effective learning of cross-modal\ninterac-",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "sentiment analysis\ntasks.\nLet T\ndenote the set of\ntasks,"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "tions and task dependencies, advancing the state-of-the-art",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "which includes sentiment classification, emotion recogni-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "in multimodal sentiment analysis and emotion recognition.",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "tion,\nand sentiment\nintensity prediction.\nFor\neach task"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "t ∈ T ,\na\ntask-specific\nis defined based on the\nloss Lt"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "3. Methodology",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "ground-truth labels and the predicted outputs.\nThe multi-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "In this\nsection, we\nintroduce\nthe overall\narchitecture",
          "representations can be computed as:": "task learning objective is formulated as a weighted sum of"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "and module details of the Hierarchical Adaptive Expert for",
          "representations can be computed as:": "the task-specific losses:"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "Multimodal Sentiment Analysis (HAEMSA) framework for",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "(cid:88)"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "multi-modal sentiment analysis. Sequentially, we describe",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "(6)\nt ∈ T wtLt\nLMTL ="
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "the hierarchical\nexpert network structure,\nthe\nevolution-",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "based adaptive structural parameter optimization process,",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "where wt\nis the weight assigned to task t. The weights wt"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "and the cross-modal, cross-task knowledge transfer mech-",
          "representations can be computed as:": "are learned adaptively during the training process using a"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "anism.",
          "representations can be computed as:": "task-specific attention mechanism.\nThe attention mecha-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "nism learns to assign higher weights to the tasks that are"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "3.1. Hierarchical Adaptive Expert Network",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "more relevant\nfor\nsentiment analysis, allowing the model"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "The hierarchical expert structure consists of multiple lev-",
          "representations can be computed as:": "to focus on the most\ninformative tasks. The total\nloss for"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "els of experts that\nlearn representations at different\nlevels",
          "representations can be computed as:": "the HAEMSA framework is a combination of the multi-task"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "of granularity.\nfea-\nLet xt, xa, and xv denote the input",
          "representations can be computed as:": "learning loss and the knowledge transfer loss:"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "tures for the text, audio, and visual modalities, respectively.",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "The modality-specific representations learned by the fine-",
          "representations can be computed as:": "(7)\nLtotal = LMTL + βLKT"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "grained experts can be expressed as:",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "loss, and β is a hy-\nwhere LKT is the knowledge transfer"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "(1)\nht = ft(xt; θt)",
          "representations can be computed as:": "perparameter\nthat balances the contributions of\nthe multi-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "task learning and knowledge transfer objectives. By jointly"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "(2)\nha = fa(xa; θa)",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "optimizing the multi-task learning and knowledge transfer"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "objectives,\nthe HAEMSA framework effectively leverages"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "(3)\nhv = fv(xv; θv)",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "the complementary information from related tasks and the"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "functions\nwhere ft, fa, and fv are the fine-grained expert",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "knowledge captured by the unified experts,\nleading to im-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "parameterized by θt, θa, and θv,\nrespectively, and ht, ha,",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "proved sentiment analysis performance."
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "and hv are the resulting modality-specific representations.",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "The structure and weights of the hierarchical experts are"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "The modality-shared representations learned by the unified",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "optimized through an evolutionary process. Let P denote"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "experts can be expressed as:",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "the population of candidate architectures, where each indi-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "vidual p ∈ P represents a specific configuration of the hi-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "(4)\nhs = fs([xt; xa; xv]; θs)",
          "representations can be computed as:": ""
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "",
          "representations can be computed as:": "erarchical experts. The evolutionary process iteratively up-"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "is\nthe unified expert\nfunction parameterized by\nwhere fs",
          "representations can be computed as:": "dates the population to discover optimal architectures and"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "[·; ·; ·] denotes concatenation,\nis\nthe resulting\nθs,\nand hs",
          "representations can be computed as:": "modality combinations. The fitness of each individual p is"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "modality-shared representation.\nThe hierarchical experts",
          "representations can be computed as:": "evaluated based on its performance on a validation set Dval."
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "learn to fuse the modality-specific and modality-shared rep-",
          "representations can be computed as:": "The fitness function F(p) can be expressed as:"
        },
        {
          "fective MTL architectures for multimodal sentiment analy-": "resentations at different\nlevels of granularity. Let h(i)\nde-",
          "representations can be computed as:": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "embedding",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": "feature select module"
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "Fusion",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "embedding",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "Fusion",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "embedding",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "Fusion",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": "D"
        },
        {
          "Fusion": "embedding",
          "Task-driven Attention": ""
        },
        {
          "Fusion": "",
          "Task-driven Attention": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "CMU-MOSEI"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": ""
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "Acc-5(%)"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "-"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "-"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "54.1"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "55.4"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "55.0"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "54.3"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "-"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "56.3"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "-"
        },
        {
          "Table 1. Performance comparison on CMU-MOSEI and CMU-MOSI datasets": "58.2"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "The HAEMSA framework incorporates a multi-task learn-"
        },
        {
          "spring po is generated as follows:": "(9)\npo = α · pi + (1 − α) · pj",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "ing approach to leverage complementary information from"
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "related sentiment analysis tasks.\nFor each task t ∈ T , a"
        },
        {
          "spring po is generated as follows:": "where α ∈ [0, 1]\nis a random weight\nthat determines the",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "is defined based on the ground-truth\ntask-specific loss Lt"
        },
        {
          "spring po is generated as follows:": "contribution of each parent. The mutation operator applies",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "labels and the predicted outputs."
        },
        {
          "spring po is generated as follows:": "random modifications to an individual p. The modified in-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "dividual p′\nis obtained as:",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "3.3.3\nJoint Loss Function"
        },
        {
          "spring po is generated as follows:": "p′ = p + N (0, σ)\n(10)",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "The total loss for training the modality-specific experts is a"
        },
        {
          "spring po is generated as follows:": "where N (0, σ) is a Gaussian noise with mean 0 and stan-",
          "3.3.2\nMulti-Task Learning Optimization": "weighted combination of the KL divergence losses and the"
        },
        {
          "spring po is generated as follows:": "dard deviation σ. The selection of individuals for the next",
          "3.3.2\nMulti-Task Learning Optimization": "task-specific losses:"
        },
        {
          "spring po is generated as follows:": "generation is based on their fitness values. We employ tour-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "L = λ1Ltask1+λ2Ltask2+λ3Ltask3+λ4Ltask4+γ(LKLt+LKLa+Lv\nKL)"
        },
        {
          "spring po is generated as follows:": "nament selection, where k individuals are randomly sam-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "(14)"
        },
        {
          "spring po is generated as follows:": "pled from the population, and the one with the highest fit-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "are\nthe\ntask-\nand Ltask4\nwhere Ltask1, Ltask2, Ltask3,"
        },
        {
          "spring po is generated as follows:": "ness is selected as a parent\nfor\nthe next generation.\nThe",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "specific losses, respectively, and λ1, λ2, λ3, λ4, and γ are"
        },
        {
          "spring po is generated as follows:": "evolutionary process continues for a fixed number of gen-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "hyperparameters that control\nthe balance between the task-"
        },
        {
          "spring po is generated as follows:": "erations or until a convergence criterion is met. The fittest",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "specific losses and the KL divergence losses."
        },
        {
          "spring po is generated as follows:": "individual\nin the final population represents the optimized",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "This joint optimization approach allows the HAEMSA"
        },
        {
          "spring po is generated as follows:": "architecture and weights of the hierarchical experts.",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "framework to effectively learn from multiple modalities and"
        },
        {
          "spring po is generated as follows:": "3.3. Cross-Modal Joint Optimization",
          "3.3.2\nMulti-Task Learning Optimization": "tasks while ensuring that knowledge is\nshared across\nthe"
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "hierarchical structure of experts."
        },
        {
          "spring po is generated as follows:": "The HAEMSA framework employs a joint optimization",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "approach to effectively integrate knowledge across modali-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "4. Experiments"
        },
        {
          "spring po is generated as follows:": "ties and tasks. This approach combines knowledge transfer",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "between unified and modality-specific experts with a multi-",
          "3.3.2\nMulti-Task Learning Optimization": "To evaluate the effectiveness of the proposed HAEMSA"
        },
        {
          "spring po is generated as follows:": "task learning objective.",
          "3.3.2\nMulti-Task Learning Optimization": "framework, we conduct extensive experiments on bench-"
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "mark datasets\nfor multimodal\nsentiment analysis.\nIn this"
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "section, we describe the datasets, experimental setup, and"
        },
        {
          "spring po is generated as follows:": "3.3.1\nKnowledge Transfer Optimization",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "results."
        },
        {
          "spring po is generated as follows:": "The\nknowledge\nlearned\nby\nthe\nunified\nexperts\nis\ntrans-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "4.1. Datasets"
        },
        {
          "spring po is generated as follows:": "ferred to the modality-specific experts to guide their learn-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "ing. The knowledge transfer is achieved by minimizing the",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "We utilize three widely adopted datasets for multimodal"
        },
        {
          "spring po is generated as follows:": "Kullback-Leibler (KL) divergence between the output prob-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "sentiment analysis. CMU-MOSI [47] contains 2,199 video"
        },
        {
          "spring po is generated as follows:": "ability distributions of the unified experts and the modality-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "clips from 93 speakers, annotated for sentiment on a scale"
        },
        {
          "spring po is generated as follows:": "specific experts. The KL divergence loss for the text modal-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "from -3 to +3,\nsplit\ninto 1,284 training,\n229 validation,"
        },
        {
          "spring po is generated as follows:": "ity can be expressed as:",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "and 686 testing clips. CMU-MOSEI [47], an extension of"
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "CMU-MOSI, comprises 23,454 clips from 1,000 speakers,"
        },
        {
          "spring po is generated as follows:": "ps(yi|xi)",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "Lt",
          "3.3.2\nMulti-Task Learning Optimization": "with 16,326 for\ntraining, 1,871 for validation,\nand 4,659"
        },
        {
          "spring po is generated as follows:": "(cid:88) i\n(11)\nps(yi|xi) log\nKL =",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "pt(yi|xi)",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "for\ntesting.\nIEMOCAP [2]\nincludes 12 hours of audiovi-"
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "sual data from 10 actors, featuring 10,039 utterances anno-"
        },
        {
          "spring po is generated as follows:": "where ps(yi|xi) and pt(yi|xi) are the output probability",
          "3.3.2\nMulti-Task Learning Optimization": "tated with categorical emotions and dimensional attributes,"
        },
        {
          "spring po is generated as follows:": "distributions of the unified expert and the text-specific ex-",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "divided into 5 sessions. All datasets provide pre-extracted"
        },
        {
          "spring po is generated as follows:": "the KL\npert, respectively, for the i-th sample xi. Similarly,",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "features for text, audio, and visual modalities."
        },
        {
          "spring po is generated as follows:": "divergence losses for the audio and visual modalities can be",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "defined as:",
          "3.3.2\nMulti-Task Learning Optimization": "4.2. Experimental Setup"
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "We compare the performance of\nthe HAEMSA frame-"
        },
        {
          "spring po is generated as follows:": "ps(yi|xi)",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "La\n(12)",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "(cid:88) i\nKL =",
          "3.3.2\nMulti-Task Learning Optimization": "work with several state-of-the-art methods for multimodal"
        },
        {
          "spring po is generated as follows:": "pa(yi|xi)",
          "3.3.2\nMulti-Task Learning Optimization": ""
        },
        {
          "spring po is generated as follows:": "",
          "3.3.2\nMulti-Task Learning Optimization": "sentiment analysis, including:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "Happiness"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "34.43"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "33.18"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "51.87"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "53.17"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "39.71"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "42.34"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "55.75"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "-"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "65.77"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "69.52"
        },
        {
          "Table 2. Experimental results on IEMOCAP. The best results are highlighted in bold.": "71.95"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• LMF [17]: Employs low-rank tensors for efficient mul-",
          "75.17\n81.56\n75.53\n77.67": "• DialogueRNN [23]: An attentive RNN designed for"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "timodal fusion.",
          "75.17\n81.56\n75.53\n77.67": "tracking\nemotional\ndynamics\nacross\nconversational"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "turns."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• MulT [40]:\nLeverages cross-modal\ntransformers\nfor",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "• DialogueGCN [6]:\nEmploys a Graph Convolutional"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "modality translation.",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "Network to model dialogue structure, enhancing emo-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• Self-MM [43]: The Self-Supervised Multi-task Mul-",
          "75.17\n81.56\n75.53\n77.67": "tion detection."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "timodal\nsentiment\nanalysis network incorporates\nan",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "•\nIterativeERC [18]: An iterative network that\nrefines"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "unimodal\nlabel generation module, grounded in self-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "emotion\nrecognition\npredictions\nthrough\nsuccessive"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "supervised learning principles,\nto investigate the po-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "stages."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "tential of unimodal supervision.",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "• QMNN [14]: Utilizes quantum computing concepts to"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• MMIM [7]: The MultiModal InfoMax framework in-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "enhance conversational emotion recognition by captur-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "troduces a stratified approach for maximizing mutual",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "ing complex relationships in data."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "information,\nfacilitating the model’s capacity to cul-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "tivate cohesive representations across various modali-",
          "75.17\n81.56\n75.53\n77.67": "• MMGCN [12]:\nA Multimodal Graph Convolution"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "ties.",
          "75.17\n81.56\n75.53\n77.67": "Network that\nintegrates various modalities to improve"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "emotion recognition accuracy in conversations."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• TFR-Net [45]: The Transformer-based Feature Recon-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "• MVN [19]: Combines different perspectives for real-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "struction Network integrates intra- and inter-modal at-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "time\nemotion\nrecognition,\nleveraging multiple\ndata"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "tention mechanisms alongside a feature reconstruction",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "views for enhanced analysis."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "component to effectively address the issue of sporadic",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "missing features in non-aligned multimodal sequences.",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "• UniMSE [10]: Aims for unified multimodal sentiment"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "analysis and emotion recognition, focusing on consis-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• AMML\n[38]:\nThe\nAdaptive Multimodal Meta-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "tent performance across various metrics."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "Learning framework employs a methodology derived",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "from meta-learning to refine unimodal representations,",
          "75.17\n81.56\n75.53\n77.67": "• MultiEMO [36]: A novel attention-based relevance-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "which are\nsubsequently adjusted to\nenhance multi-",
          "75.17\n81.56\n75.53\n77.67": "aware multimodal\nfusion framework that effectively"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "modal integration.",
          "75.17\n81.56\n75.53\n77.67": "integrates cross-modal cues."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• EMT [37]: Utilizes dual-level feature restoration tech-",
          "75.17\n81.56\n75.53\n77.67": "• UniMEEC\n[11]:\nA framework\ninnovatively\nuni-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "niques to enhance the robustness and accuracy of mul-",
          "75.17\n81.56\n75.53\n77.67": "fies multimodal emotion recognition in conversation"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "timodal sentiment analysis.",
          "75.17\n81.56\n75.53\n77.67": "(MERC) and multimodal emotion-cause pair extrac-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "tion (MECPE) by reformulating them as mask predic-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• CRNet [35]:\nInteracts coordinated audio-visual repre-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "tion tasks,\nleveraging shared prompt\nlearning across"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "sentations to construct enhanced linguistic representa-",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "modalities,\nand employing task-specific hierarchical"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "tions for improved sentiment analysis.",
          "75.17\n81.56\n75.53\n77.67": ""
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "",
          "75.17\n81.56\n75.53\n77.67": "context aggregation."
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "• BC-LSTM [28]: A Bidirectional Contextual LSTM",
          "75.17\n81.56\n75.53\n77.67": "To ensure robustness, we report results averaged over ten"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "model\nthat captures context\nin conversations\nto im-",
          "75.17\n81.56\n75.53\n77.67": "runs with different random seeds. This rigorous experimen-"
        },
        {
          "71.95\n91.61\n72.18\nHAEMSA (Ours)": "prove emotion recognition.",
          "75.17\n81.56\n75.53\n77.67": "tal setup allows for a fair and comprehensive evaluation of"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HAEMSA against a wide spectrum of existing approaches": "in the field. For MOSI and MOSEI, we report the binary ac-",
          "highlight\nthe synergistic effect of HAEMSA’s components": "in effectively capturing and utilizing multimodal\ninforma-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "curacy (Acc-2), 7-class accuracy (Acc-7) and F1 score. For",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tion for sentiment analysis and emotion recognition tasks."
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "IEMOCAP, we report the weighted-F1 and unweighted ac-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "5.3. Visualization Analysis"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "curacy (UA) for emotion classification. Weighted-F1 is the",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "weighted average of F1 scores for all emotion categories",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "To provide insights\ninto HAEMSA’s\nlearned represen-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "while UA is the average accuracy of each class.",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tations, we visualize the multimodal embeddings using t-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "SNE. Figure 3 presents the t-SNE plots for different abla-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "5. Results",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tion settings."
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "The full HAEMSA model (Figure 3(a)) shows clear sep-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "We present a comprehensive evaluation of the proposed",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "aration between sentiment classes on CMU-MOSEI,\nindi-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "HAEMSA framework, comparing its performance against",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "cating its ability to learn discriminative features. In contrast,"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "state-of-the-art methods and analyzing the contribution of",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "ablated versions exhibit\nless distinct clusters, highlighting"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "each component through ablation studies.",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "the importance of each component in learning effective rep-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "5.1. Comparison to State-of-the-art",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "resentations."
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "As shown in Table 1 and 2, HAEMSA consistently out-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "6. Discussion"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "performs existing methods across all datasets and metrics.",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "On CMU-MOSEI, HAEMSA achieves a 2.6% increase in",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "The HAEMSA framework, while demonstrating supe-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "Acc-7 and a 0.059 decrease in MAE compared to the next",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "rior performance across multiple datasets, presents certain"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "best method. For CMU-MOSI, we observe a 6.3% improve-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "limitations that warrant discussion.\nThe evolutionary op-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "ment\nin Acc-7 and a 0.058 reduction in MAE. On IEMO-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "timization process,\nthough effective,\nincreases the compu-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "CAP, HAEMSA surpasses the state-of-the-art by 2.84% in",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tational cost compared to simpler\nfusion methods.\nThis"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "weighted-F1 score. These results demonstrate HAEMSA’s",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "heightened complexity may restrict HAEMSA’s applicabil-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "effectiveness in capturing complex multimodal interactions",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "ity in resource-constrained or\nreal-time scenarios, poten-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "and generalizing across different emotional contexts.",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tially limiting its use in certain practical applications."
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "Another consideration is\nthe current\nimplementation’s"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "5.2. Ablation Study",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "assumption of complete modality availability for each data"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "To gain deeper\ninsights into the efficacy of each com-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "sample.\nIn real-world applications, missing or incomplete"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "ponent within\nthe HAEMSA framework, we\nconducted",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "modalities are common occurrences, presenting a challenge"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "comprehensive\nablation\nstudies\nacross\nthree\nbenchmark",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "for the model’s robustness and generalizability. This limita-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "datasets: CMU-MOSEI, CMU-MOSI, and IEMOCAP. This",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tion highlights the need for adaptability in handling varied"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "systematic analysis aims to quantify the individual contribu-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "input conditions."
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "tions of key components and their synergistic effects on the",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "Despite significant\nimprovements\nin emotion recogni-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "overall performance of our proposed model. The detailed",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tion, HAEMSA still faces challenges in distinguishing be-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "results of\nthese ablation experiments are presented in Ta-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tween closely related emotional states, particularly evident"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "bles 3 and 4, offering a nuanced view of how each element",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "in the IEMOCAP dataset results. This limitation points to"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "impacts the model’s effectiveness in multimodal sentiment",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "the need for more nuanced emotional modeling and repre-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "analysis and emotion recognition tasks.",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "sentation learning."
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "The\nablation\nstudy\nconducted\nacross CMU-MOSEI,",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "Furthermore,\nthe model’s performance in cross-lingual"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "CMU-MOSI, and IEMOCAP datasets consistently demon-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "or cross-cultural settings remains unexplored. Given the di-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "strates\nthe crucial\nrole of\neach HAEMSA component\nin",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "verse nature of emotional expression across cultures,\nthis"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "its overall performance. Results are presented in Tables 3",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "aspect\nis crucial for the broader applicability of HAEMSA"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "and 4.\nRemoving the hierarchical\nstructure (w/o Hierar-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "in global contexts. These limitations, while not diminish-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "chy) leads to substantial performance drops across all met-",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "ing the model’s achievements, provide clear directions for"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "rics and emotion categories, underscoring the importance of",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "future research and improvement."
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "learning representations at different granularity levels. The",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "evolutionary optimization process (w/o Evolution) proves",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "",
          "highlight\nthe synergistic effect of HAEMSA’s components": "7. Conclusion"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "vital, with its removal resulting in decreased performance,",
          "highlight\nthe synergistic effect of HAEMSA’s components": ""
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "particularly evident\nin complex emotions\nlike excitement",
          "highlight\nthe synergistic effect of HAEMSA’s components": "This paper\nintroduces HAEMSA (Hierarchical Adap-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "and frustration on IEMOCAP. Both cross-modal\nintegra-",
          "highlight\nthe synergistic effect of HAEMSA’s components": "tive Expert\nfor Multimodal Sentiment Analysis),\na novel"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "tion and multi-task learning components show significant",
          "highlight\nthe synergistic effect of HAEMSA’s components": "framework that\nintegrates hierarchical mixture-of-experts"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "contributions, as their\nindividual\nremovals lead to notable",
          "highlight\nthe synergistic effect of HAEMSA’s components": "networks, evolutionary optimization, and multi-task learn-"
        },
        {
          "HAEMSA against a wide spectrum of existing approaches": "performance degradation across all datasets. These findings",
          "highlight\nthe synergistic effect of HAEMSA’s components": "ing for multimodal sentiment analysis and emotion recogni-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3. Ablation study results on CMU-MOSEI and CMU-MOSI datasets": ""
        },
        {
          "Table 3. Ablation study results on CMU-MOSEI and CMU-MOSI datasets": ""
        },
        {
          "Table 3. Ablation study results on CMU-MOSEI and CMU-MOSI datasets": "Acc-7(%)"
        },
        {
          "Table 3. Ablation study results on CMU-MOSEI and CMU-MOSI datasets": "57.1"
        },
        {
          "Table 3. Ablation study results on CMU-MOSEI and CMU-MOSI datasets": "55.8"
        },
        {
          "Table 3. Ablation study results on CMU-MOSEI and CMU-MOSI datasets": "53.6"
        },
        {
          "Table 3. Ablation study results on CMU-MOSEI and CMU-MOSI datasets": "52.2"
        },
        {
          "Table 3. Ablation study results on CMU-MOSEI and CMU-MOSI datasets": "51.7"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4. Ablation study results on IEMOCAP dataset": "Sadness"
        },
        {
          "Table 4. Ablation study results on IEMOCAP dataset": "91.61"
        },
        {
          "Table 4. Ablation study results on IEMOCAP dataset": "87.03"
        },
        {
          "Table 4. Ablation study results on IEMOCAP dataset": "85.17"
        },
        {
          "Table 4. Ablation study results on IEMOCAP dataset": "83.92"
        },
        {
          "Table 4. Ablation study results on IEMOCAP dataset": "83.29"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "tion. HAEMSA’s hierarchical structure enables learning at\nsuggest\nthat HAEMSA can\neffectively\ncapture\ncomplex"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "various levels of modality granularity, while its evolutionary\nmultimodal\ninteractions\nand\ngeneralize\nacross\nvarious"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "process optimizes expert architectures for effective cross-\nemotional contexts."
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "modal knowledge transfer. The incorporation of multi-task"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "Future work could focus on addressing current\nlimita-"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "learning allows HAEMSA to capture task interdependen-"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "tions and expanding HAEMSA’s capabilities. This may in-"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "cies, potentially improving overall performance."
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "clude optimizing computational efficiency to enhance real-"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "Experiments\non\nCMU-MOSI,\nCMU-MOSEI,\nand\nworld applicability, developing mechanisms\nfor handling"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "IEMOCAP datasets demonstrate HAEMSA’s effectiveness\nmissing or incomplete modalities, and refining the model’s"
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "compared to existing methods,\nshowing improvements\nin\nability to distinguish between nuanced emotional\nstates."
        },
        {
          "Figure 3.\nt-SNE visualization of multimodal embeddings on CMU-MOSEI.": "accuracy and MAE across different\ntasks.\nThese results\nAdditionally, exploring HAEMSA’s performance in cross-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "pact. Such advancements could contribute to the wider field",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Wu, and Yongbin Li. Unimse: Towards unified multimodal"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "sentiment analysis and emotion recognition.\nIn Proceed-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "of multimodal sentiment analysis and emotion recognition,",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "ings of\nthe 2022 Conference on Empirical Methods in Nat-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "potentially opening new avenues for applications in areas",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "ural Language Processing, pages 7837–7851, Abu Dhabi,"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "such as human-computer\ninteraction, mental health moni-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "United Arab Emirates, 2022. Association for Computational"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "toring, and social media analysis.",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Linguistics. 6"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[11] Guimin Hu, Zhihong Zhu, Daniel Hershcovich, Hasti Seifi,"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "References",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "and Jiayuan Xie.\nUnimeec:\nTowards unified multimodal"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "arXiv preprint\nemotion recognition and emotion cause."
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[1] Md Shad Akhtar, Dushyant Singh Chauhan, Deepanway",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "arXiv:2404.00403, 2024. 6"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Ghosal, Soujanya Poria, Asif Ekbal,\nand Pushpak Bhat-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[12]\nJingwen Hu, Yuchen Liu,\nJinming Zhao,\nand Qin\nJin."
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "tacharyya.\nMulti-task learning for multi-modal\nemotion",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Mmgcn: Multimodal fusion via deep graph convolution net-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "arXiv\npreprint\nrecognition\nand\nsentiment\nanalysis.\nIn",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "work for emotion recognition in conversation.\nIn Proceed-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "arXiv:1905.05812, 2019. 2",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "ings of the 59th Annual Meeting of the Association for Com-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[2] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "putational Linguistics and the 11th International Joint Con-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "ference on Natural Language Processing (Volume 1: Long"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Chang, Sungbok Lee, and Shrikanth S Narayanan.\nIemo-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Papers), pages 5666–5675, Online, 2021. Association for"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "cap:\nInteractive emotional dyadic motion capture database.",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Computational Linguistics. 6"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Language resources and evaluation, 42:335–359, 2008. 2, 5",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[13] Amandeep Kaur and Sanchita Kautish. Multimodal senti-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[3] Erik Cambria,\nSoujanya Poria, Rajiv Bajpai,\nand Bj¨orn",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "ment analysis: A survey and comparison. International Jour-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Schuller. A review of affective computing: From unimodal",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "nal of Service Science, Management, Engineering, and Tech-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "analysis to multimodal fusion.\nInformation Fusion, 37:98–",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "nology (IJSSMET), 10(2):38–58, 2019. 1"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "125, 2017. 1",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[14] Qiuchi Li, Dimitris Gkoumas, Alessandro Sordoni, JianYun"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[4]\nJianfeng Gao, Michel Galley, and Lihong Li.\nNeural ap-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Nie, and Massimo Melucci. Quantum-inspired neural net-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "proaches to conversational ai. In The 41st international ACM",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "work for conversational emotion recognition. In Proceedings"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "SIGIR conference on research & development in information",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "of the AAAI Conference on Artificial Intelligence, volume 35,"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "retrieval, pages 1371–1374, 2018. 2",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "pages 13270–13278, 2021. 6"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[5] Deepanway Ghosal, Md Shad Akhtar, Dushyant Chauhan,",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[15] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Soujanya Poria, Asif Ekbal,\nand Pushpak Bhattacharyya.",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Fernando,\nand Koray Kavukcuoglu.\nHierarchical\nrepre-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Contextual\ninter-modal attention for multi-modal sentiment",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "arXiv preprint\nsentations for efficient architecture search."
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "the 2018 Conference on Empir-\nanalysis.\nIn Proceedings of",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "arXiv:1711.00436, 2018. 2"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "ical Methods in Natural Language Processing, pages 3454–",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[16] Liangchen Liu, Youzhuo Sun, and Xiaodong Li. Hierarchical"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "3466, 2018. 1, 2",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "adaptive multimodal fusion for multimodal sentiment analy-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[6] Deepanway Ghosal, Navonil Majumder,\nSoujanya Poria,",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "sis.\nIn 2021 IEEE International Conference on Multimedia"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Niyati Chhaya, and Alexander Gelbukh.\nDialoguegcn: A",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "and Expo (ICME), pages 1–6. IEEE, 2021. 2"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "graph convolutional neural network for emotion recognition",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[17] Zhun\nLiu,\nYing\nShen,\nVarun\nBharadhwaj\nLakshmi-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "the 2019 Conference\nin conversation.\nIn Proceedings of",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "narasimhan, Paul Pu Liang, Amir Zadeh, and Louis-Philippe"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "on Empirical Methods in Natural Language Processing and",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Morency.\nEfficient\nlow-rank multimodal\nfusion with"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "the 9th International Joint Conference on Natural Language",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "modality-specific factors. arXiv preprint arXiv:1806.00064,"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Processing (EMNLP-IJCNLP), pages 154–164, Hong Kong,",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "2018. 6"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "China, 2019. Association for Computational Linguistics. 6",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": ""
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[18] Xin Lu, Yanyan Zhao, Yang Wu, Yijian Tian, Huipeng Chen,"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[7] Wei Han, Hui Chen, and Soujanya Poria.\nImproving mul-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "and Bing Qin. An iterative emotion interaction network for"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "timodal\nfusion with hierarchical mutual\ninformation maxi-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "emotion recognition in conversations.\nIn Proceedings of the"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "mization for multimodal sentiment analysis.\nIn Proceedings",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "28th International Conference on Computational Linguis-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "of\nthe 2021 Conference on Empirical Methods\nin Natural",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "tics, pages 4078–4088, Barcelona, Spain (Online), 2020. In-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Language Processing, pages 9180–9192, 2021. 2, 6",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "ternational Committee on Computational Linguistics. 6"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[8] Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea, Erik",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[19] Hui Ma, Jian Wang, Hongfei Lin, Xuejun Pan, Yijia Zhang,"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Cambria, and Roger Zimmermann. Icon: Interactive conver-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "and Zhihao Yang. A multi-view network for real-time emo-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "sational memory network for multimodal emotion detection.",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Knowledge-Based Sys-\ntion recognition in conversations."
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "In Proceedings of the 2018 conference on empirical methods",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "tems, 236:107751, 2022. 6"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "in natural language processing, pages 2594–2604, 2018. 2",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[20]\nSijie Mai, Haifeng Hu, and Songlong Xu. Analyzing mul-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "[9] Devamanyu Hazarika, Roger Zimmermann,\nand Soujanya",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "timodal sentiment via acoustic-informed fusion graph con-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "Poria. MISA: Modality-Invariant and -Specific Representa-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "IEEE/ACM Transactions on Audio,\nvolutional networks."
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "tions for Multimodal Sentiment Analysis.\nIn Proceedings of",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "Speech, and Language Processing, 28:2324–2335, 2020. 1"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "the 28th ACM International Conference on Multimedia, MM",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "[21] Navonil Majumder, Deepanway Ghosal, Alexander Gel-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "’20, pages 1122–1131, New York, NY, USA, Oct. 2020. As-",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "bukh, Rada Mihalcea, and Soujanya Poria. Hierarchical mul-"
        },
        {
          "lingual and cross-cultural contexts could broaden its\nim-": "sociation for Computing Machinery. 1, 2",
          "[10] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan": "timodal contextual attention for sentiment analysis in videos."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "1164, 2019. 2",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "based correlation-aware multimodal\nfusion framework for"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[22] Navonil Majumder, Soujanya Poria, Devamanyu Hazarika,",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "emotion recognition in conversations.\nIn Anna Rogers, Jor-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. Di-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "dan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "alogueRNN: An Attentive RNN for Emotion Detection in",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "of the 61st Annual Meeting of the Association for Computa-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "the AAAI Conference on Ar-\nConversations. Proceedings of",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "tional Linguistics (Volume 1: Long Papers), pages 14752–"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "tificial Intelligence, 33(01):6818–6825, July 2019. Number:",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "14766, Toronto, Canada, July 2023. Association for Compu-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "01. 2",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "tational Linguistics. 6"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[23] Navonil Majumder, Soujanya Poria, Devamanyu Hazarika,",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[37] Licai Sun, Zheng Lian, Bin Liu, and Jianhua Tao. Efficient"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. Di-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "multimodal\ntransformer with dual-level\nfeature restoration"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "aloguernn: An attentive rnn for emotion detection in conver-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "IEEE Transac-\nfor\nrobust multimodal\nsentiment analysis."
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "sations.\nIn Proceedings of the AAAI Conference on Artificial",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "tions on Affective Computing, pages 309–325, 2024. 6"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Intelligence, volume 33, pages 6818–6825, 2019. 6",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[38] Ya Sun, Sijie Mai, and Haifeng Hu. Learning to learn bet-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[24] Navonil Majumder, Soujanya Poria, Haiyun Peng, Niyati",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ter unimodal representations via adaptive multimodal meta-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Chhaya, Erik Cambria, and Alexander Gelbukh. Sentiment",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "learning.\nIEEE Transactions on Affective Computing, pages"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "and sarcasm classification with multitask learning.\nIn IEEE",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "2209–2223, 2023. 2, 3, 6"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Intelligent Systems, volume 34, pages 38–43. IEEE, 2019. 2",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[39] Zijun Sun, Pratyay Sarma, Jamin Wang, Nikhil Rao, Cheng-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[25] Ruben Martinez-Cantin.\nEvolutionary neural architecture",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "I Ling, and Louis-Philippe Morency. Progressively boosted"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "search: A survey. Neurocomputing, 507:273–296, 2022. 2",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "arXiv\nmodality fusion for multimodal sentiment analysis."
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[26]\nSoujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hus-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "preprint arXiv:2203.12119, 2022. 1"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "sain. A review of affective computing: From unimodal anal-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[40] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "ysis to multimodal fusion.\nInformation Fusion, 37:98–125,",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov."
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "2017. 1",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "Multimodal\ntransformer for unaligned multimodal\nlanguage"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[27]\nSoujanya\nPoria,\nErik\nCambria,\nDevamanyu\nHazarika,",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "the conference. Association\nsequences.\nIn Proceedings of"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Navonil Majumder,\nAmir\nZadeh,\nand\nLouis-Philippe",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "for computational\nlinguistics. Meeting, volume 2019, page"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Morency.\nContext-dependent\nsentiment\nanalysis\nin user-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "6558. NIH Public Access, 2019. 6"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "generated videos. In Proceedings of the 55th annual meeting",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[41] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "of\nthe association for computational\nlinguistics (volume 1:",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "Smola, and Eduard Hovy. Hierarchical attention networks"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Long papers), pages 873–883, 2017. 2",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "for document classification. In Proceedings of the 2016 con-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[28]\nSoujanya\nPoria,\nErik\nCambria,\nDevamanyu\nHazarika,",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ference of\nthe North American chapter of\nthe association"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Navonil Majumder,\nAmir\nZadeh,\nand\nLouis-Philippe",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "for\ncomputational\nlinguistics:\nhuman language\ntechnolo-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Morency.\nContext-dependent\nsentiment\nanalysis\nin user-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "gies, pages 1480–1489, 2016. 2"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "generated videos. In Proceedings of the 55th Annual Meeting",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[42] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu.\nLearn-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "of the Association for Computational Linguistics (Volume 1:",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ing modality-specific\nrepresentations with\nself-supervised"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Long Papers), pages 873–883, 2017. 6",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "multi-task learning for multimodal sentiment analysis. Pro-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[29]\nJiahao Qin. Zoom and shift are all you need, 2024. 1",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ceedings of\nthe AAAI Conference on Artificial\nIntelligence,"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[30]\nJiahao Qin, Yitao Xu, Zong Lu, and Xiaojun Zhang.\nStep",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "35(16):14102–14110, 2021. 1"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "fusion: Local and global mutual guidance, 2024. 1",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[43] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. Learning"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[31]\nJiahao Qin, Bihao You,\nand Feng Liu.\nIntelligent\nstock",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "modality-specific representations with self-supervised multi-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "forecasting by iterative global-local\nfusion.\nIn De-Shuang",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "task learning for multimodal sentiment analysis. In Proceed-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Huang, Chuanlei Zhang, and Jiayang Guo, editors, Advanced",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ings of\nthe AAAI conference on artificial\nintelligence, vol-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Intelligent Computing Technology and Applications, pages",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ume 35, pages 10790–10797, 2021. 6"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "285–295, Singapore, 2024. Springer Nature Singapore. 1",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[44] Yang Yu, Qi Bao, Wenbo Xu, Linlin Zheng, and Kaiyang"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[32]\nJiahao Qin, Lu Zong, and Feng Liu. Exploring inner speech",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "Wang. Multimodal multi-task financial risk forecasting.\nIn"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "recognition via cross-perception approach in eeg and fmri.",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "Proceedings of\nthe 44th International ACM SIGIR Confer-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Applied Sciences, 14(17), 2024. 1",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ence on Research and Development in Information Retrieval,"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[33] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "pages 1052–1061, 2021. 2"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Le. Regularized evolution for\nimage classifier architecture",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[45] Ziqi Yuan, Wei Li, Hua Xu, and Wenmeng Yu. Transformer-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "search. Proceedings of the AAAI conference on artificial in-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "based feature reconstruction network for robust multimodal"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "telligence, 33(01):4780–4789, 2019. 2",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "sentiment analysis. In Proceedings of the 29th ACM Interna-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[34]\nSebastian Ruder. An Overview of Multi-Task Learning in",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "tional Conference on Multimedia, pages 4400–4407, 2021."
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Deep Neural Networks, June 2017.\narXiv:1706.05098 [cs,",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "6"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "stat]. 3",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "[46] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria,"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "[35] Hang Shi, Yuanyuan Pu, Zhengpeng Zhao,\nJian Huang,",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "and Louis-Philippe Morency. Tensor fusion network for mul-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "Dongming Zhou, Dan Xu, and Jinde Cao. Co-space repre-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "timodal sentiment analysis. In Proceedings of the 2017 Con-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "sentation interaction network for multimodal sentiment anal-",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ference on Empirical Methods in Natural Language Process-"
        },
        {
          "IEEE Transactions\non Affective Computing,\n13(4):1151–": "ysis. Knowledge-Based Systems, 283:111149, 2024. 6",
          "[36] Tao Shi and Shao-Lun Huang. MultiEMO: An attention-": "ing, pages 1103–1114, 2017. 1"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "Erik Cambria, and Louis-Philippe Morency. Multi-attention"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "recurrent network for human communication comprehen-"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "the AAAI Conference on Artificial\nsion.\nIn Proceedings of"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "Intelligence, volume 32, pages 5642–5649, 2018. 2, 5"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "[48] AmirAli\nZadeh,\nChengfeng Mao, Kelly McCurry,\nand"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "Ramin Tong. Multimodal\nlanguage analysis in the wild: A"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "survey. arXiv preprint arXiv:2007.13700, 2020. 1"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "[49] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "Cambria,\nand Louis-Philippe Morency.\nMultimodal\nlan-"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "guage analysis\nin the wild: Cmu-mosei dataset and inter-"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "the 56th\npretable dynamic fusion graph.\nIn Proceedings of"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "Annual Meeting of\nthe Association for Computational Lin-"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "guistics (Volume 1: Long Papers), pages 2236–2246, 2018."
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "5"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "[50] Yu Zhang and Qiang Yang. A survey on multi-task learn-"
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "IEEE Transactions on Knowledge and Data Engineer-\ning."
        },
        {
          "[47] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij,": "ing, 34(12):5586–5609, 2011. 2"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Soujanya Poria, Asif Ekbal, and Pushpak Bhattacharyya. Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "Shad Md",
        "Dushyant Akhtar",
        "Deepanway Singh Chauhan",
        "Ghosal"
      ],
      "year": "2019",
      "venue": "Soujanya Poria, Asif Ekbal, and Pushpak Bhattacharyya. Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "arxiv": "arXiv:1905.05812"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Erik Cambria",
        "Soujanya Poria",
        "Rajiv Bajpai",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "4",
      "title": "Neural approaches to conversational ai",
      "authors": [
        "Jianfeng Gao",
        "Michel Galley",
        "Lihong Li"
      ],
      "year": "2018",
      "venue": "The 41st international ACM SIGIR conference on research & development in information retrieval"
    },
    {
      "citation_id": "5",
      "title": "Asif Ekbal, and Pushpak Bhattacharyya. Contextual inter-modal attention for multi-modal sentiment analysis",
      "authors": [
        "Deepanway Ghosal",
        "Shad Md",
        "Dushyant Akhtar",
        "Soujanya Chauhan",
        "Poria"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "7",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "9",
      "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, MM '20"
    },
    {
      "citation_id": "10",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Unimeec: Towards unified multimodal emotion recognition and emotion cause",
      "authors": [
        "Guimin Hu",
        "Zhihong Zhu",
        "Daniel Hershcovich",
        "Hasti Seifi",
        "Jiayuan Xie"
      ],
      "year": "2024",
      "venue": "Unimeec: Towards unified multimodal emotion recognition and emotion cause",
      "arxiv": "arXiv:2404.00403"
    },
    {
      "citation_id": "12",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Multimodal sentiment analysis: A survey and comparison",
      "authors": [
        "Amandeep Kaur",
        "Sanchita Kautish"
      ],
      "year": "2019",
      "venue": "International Journal of Service Science, Management, Engineering, and Technology (IJSSMET)"
    },
    {
      "citation_id": "14",
      "title": "Quantum-inspired neural network for conversational emotion recognition",
      "authors": [
        "Qiuchi Li",
        "Dimitris Gkoumas",
        "Alessandro Sordoni",
        "Jianyun Nie",
        "Massimo Melucci"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Hierarchical representations for efficient architecture search",
      "authors": [
        "Hanxiao Liu",
        "Karen Simonyan",
        "Oriol Vinyals",
        "Chrisantha Fernando",
        "Koray Kavukcuoglu"
      ],
      "year": "2018",
      "venue": "Hierarchical representations for efficient architecture search",
      "arxiv": "arXiv:1711.00436"
    },
    {
      "citation_id": "16",
      "title": "Hierarchical adaptive multimodal fusion for multimodal sentiment analysis",
      "authors": [
        "Liangchen Liu",
        "Youzhuo Sun",
        "Xiaodong Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "17",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "18",
      "title": "An iterative emotion interaction network for emotion recognition in conversations",
      "authors": [
        "Xin Lu",
        "Yanyan Zhao",
        "Yang Wu",
        "Yijian Tian",
        "Huipeng Chen",
        "Bing Qin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "A multi-view network for real-time emotion recognition in conversations. Knowledge-Based Systems",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Xuejun Pan",
        "Yijia Zhang",
        "Zhihao Yang"
      ],
      "year": "2022",
      "venue": "A multi-view network for real-time emotion recognition in conversations. Knowledge-Based Systems"
    },
    {
      "citation_id": "20",
      "title": "Analyzing multimodal sentiment via acoustic-informed fusion graph convolutional networks",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xu"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. Hierarchical multimodal contextual attention for sentiment analysis in videos",
      "authors": [
        "Navonil Majumder",
        "Deepanway Ghosal"
      ],
      "venue": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. Hierarchical multimodal contextual attention for sentiment analysis in videos"
    },
    {
      "citation_id": "22",
      "title": "IEEE Transactions on Affective Computing",
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria",
        "Di-Aloguernn"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria",
        "Dialoguernn"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Erik Cambria, and Alexander Gelbukh. Sentiment and sarcasm classification with multitask learning",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Haiyun Peng",
        "Niyati Chhaya"
      ],
      "year": "2019",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "26",
      "title": "Evolutionary neural architecture search: A survey",
      "authors": [
        "Ruben Martinez-Cantin"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "27",
      "title": "Rajiv Bajpai, and Amir Hussain. A review of affective From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "28",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "29",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "Zoom and shift are all you need",
      "authors": [
        "Jiahao Qin"
      ],
      "year": "2024",
      "venue": "Zoom and shift are all you need"
    },
    {
      "citation_id": "31",
      "title": "Step fusion: Local and global mutual guidance",
      "authors": [
        "Jiahao Qin",
        "Yitao Xu",
        "Zong Lu",
        "Xiaojun Zhang"
      ],
      "year": "2024",
      "venue": "Step fusion: Local and global mutual guidance"
    },
    {
      "citation_id": "32",
      "title": "Intelligent stock forecasting by iterative global-local fusion",
      "authors": [
        "Jiahao Qin",
        "Bihao You",
        "Feng Liu"
      ],
      "year": "2024",
      "venue": "Advanced Intelligent Computing Technology and Applications"
    },
    {
      "citation_id": "33",
      "title": "Exploring inner speech recognition via cross-perception approach in eeg and fmri",
      "authors": [
        "Jiahao Qin",
        "Lu Zong",
        "Feng Liu"
      ],
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "34",
      "title": "Regularized evolution for image classifier architecture search",
      "authors": [
        "Esteban Real",
        "Alok Aggarwal",
        "Yanping Huang",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "35",
      "title": "An Overview of Multi-Task Learning in Deep Neural Networks",
      "authors": [
        "Sebastian Ruder"
      ],
      "year": "2017",
      "venue": "An Overview of Multi-Task Learning in Deep Neural Networks",
      "arxiv": "arXiv:1706.05098"
    },
    {
      "citation_id": "36",
      "title": "MultiEMO: An attentionbased correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "Hang Shi",
        "Yuanyuan Pu",
        "Zhengpeng Zhao",
        "Jian Huang",
        "Dongming Zhou",
        "Dan Xu",
        "Jinde Cao",
        "; Tao Shi",
        "Shao-Lun Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Learning to learn better unimodal representations via adaptive multimodal metalearning",
      "authors": [
        "Ya Sun",
        "Sijie Mai",
        "Haifeng Hu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Progressively boosted modality fusion for multimodal sentiment analysis",
      "authors": [
        "Zijun Sun",
        "Pratyay Sarma",
        "Jamin Wang",
        "Nikhil Rao",
        "Cheng-I Ling",
        "Louis-Philippe Morency"
      ],
      "year": "2022",
      "venue": "Progressively boosted modality fusion for multimodal sentiment analysis",
      "arxiv": "arXiv:2203.12119"
    },
    {
      "citation_id": "40",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "41",
      "title": "Hierarchical attention networks for document classification",
      "authors": [
        "Zichao Yang",
        "Diyi Yang",
        "Chris Dyer",
        "Xiaodong He",
        "Alex Smola",
        "Eduard Hovy"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies"
    },
    {
      "citation_id": "42",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Learning modality-specific representations with self-supervised multitask learning for multimodal sentiment analysis",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "44",
      "title": "Multimodal multi-task financial risk forecasting",
      "authors": [
        "Yang Yu",
        "Qi Bao",
        "Wenbo Xu",
        "Linlin Zheng",
        "Kaiyang Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "45",
      "title": "Transformerbased feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Ziqi Yuan",
        "Wei Li",
        "Hua Xu",
        "Wenmeng Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "47",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "48",
      "title": "Multimodal language analysis in the wild: A survey",
      "authors": [
        "Amirali Zadeh",
        "Chengfeng Mao",
        "Kelly Mccurry",
        "Ramin Tong"
      ],
      "year": "2020",
      "venue": "Multimodal language analysis in the wild: A survey",
      "arxiv": "arXiv:2007.13700"
    },
    {
      "citation_id": "49",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "50",
      "title": "A survey on multi-task learning",
      "authors": [
        "Yu Zhang",
        "Qiang Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    }
  ]
}