{
  "paper_id": "2411.09339v1",
  "title": "Re-Parameterization Of Lightweight Transformer For On-Device Speech Emotion Recognition",
  "published": "2024-11-14T10:36:19Z",
  "authors": [
    "Zixing Zhang",
    "Zhongren Dong",
    "Weixiang Xu",
    "Jing Han"
  ],
  "keywords": [
    "Artificial Internet of Things",
    "Transformer Reconstruction",
    "Speech Emotion Recognition",
    "Model Compression"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the increasing implementation of machine learning models on edge or Internet-of-Things (IoT) devices, deploying advanced models on resource-constrained IoT devices remains challenging. Transformer models, a currently dominant neural architecture, have achieved great success in broad domains but their complexity hinders its deployment on IoT devices with limited computation capability and storage size. Although many model compression approaches have been explored, they often suffer from notorious performance degradation. To address this issue, we introduce a new method, namely Transformer Re-parameterization, to boost the performance of lightweight Transformer models. It consists of two processes: the High-Rank Factorization (HRF) process in the training stage and the de-High-Rank Factorization (deHRF) process in the inference stage. In the former process, we insert an additional linear layer before the Feed-Forward Network (FFN) of the lightweight Transformer. It is supposed that the inserted HRF layers can enhance the model learning capability. In the later process, the auxiliary HRF layer will be merged together with the following FFN layer into one linear layer and thus recover the original structure of the lightweight model. To examine the effectiveness of the proposed method, we evaluate it on three widely used Transformer variants, i. e., ConvTransformer, Conformer, and SpeechFormer networks, in the application of speech emotion recognition on the IEMOCAP, M 3 ED and DAIC-WOZ datasets. Experimental results show that our proposed method consistently improves the performance of lightweight Transformers, even making them comparable to large models. The proposed re-parameterization approach enables advanced Transformer models to be deployed on resource-constrained IoT devices.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we review Transformer-based SER, lightweight Transformers, and structural re-parameterization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Transformers-Based Speech Emotion Recognition",
      "text": "For SER, nowadays Transformer-based models have been widely employed due to their capability to capture short-and long-context information. From the feature aspect, it goes from the handcrafted frame-level features (e. g., the frame-level Mel-Frequency Cepstral Coefficient [MFCC] or Log Mel-filter Bank energies [LMFB])  [31]  and the segment-level features (e. g., IS09 or eGeMAPS)  [32] , to the raw speech signals that contain the complete information in an end-to-end way  [33] . The latter, however, often require a large number of emotional speech data for training.\n\nFrom the model architecture aspect, early studies often used the classic Transformer for SER  [31] ,  [33] . Recently, more and more advanced Transformer structures were designed for SER. For example, a 1D CNN network was inserted into the classic Transformer to extract information from raw speech signal  [33] . Besides, a SpeechFormer model was introduced, which optimized global attention to compute local attention and employed a hierarchical structure to extract frame-, phoneme-, word-, and sentence-level features, resulting in superior performance  [34] . Furthermore, a DWFormer model has been proposed, which takes dynamic windows to extract finegrained temporal features from speech samples for SER  [35] .\n\nNotably, all these aforementioned studies trained Transformer models from scratch in a supervised learning way. With the advent of pre-trained models, nowadays more and more research is focusing on the use of pre-trained models for SER  [12] ,  [36] . For example, Wang et al.  [36]  performed partial and overall fine-tuning of pre-trained models, including Wav2vec 2.0 and HuBERT, to study their feasibility in SER, speaker verification, and spoken language understanding tasks. Wagner et al.  [12]  conducted a comprehensive evaluation of several pre-trained variants of Wav2vec 2.0 and HuBERT for SER. Although these methods show promising performance, their high computational complexity, and particularly their model size, renders them unsuitable for edge devices.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Lightweight Transformers",
      "text": "With the popularity of mobile/edge devices, increasing interest is focusing on the reduction of the computational cost and model size of Transformers and have proposed many compression methods  [25] ,  [37] -  [51] . These methods can be mainly classified into three categories: pruning  [37] -  [42] , knowledge distillation  [43] -  [46]  and matrix decomposition  [25] ,  [47] -  [51] .\n\nPruning removes redundant connections from the model, and thus reduces the model size and computation operations, and speeds up the inference process. The simplified process of pruning refers to Figure  1(a) . Studies on Transformer-based pruning typically concentrate on different modules of Transformer, such as pruning the attention head  [37] ,  [38] , pruning the FFN  [39] ,  [40] , or even directly removing certain layers  [41] ,  [42] . Researchers often carefully eliminate a module or layer based on its contribution to the model size, ensuring that the overall performance of the model is not compromised by excessive removal.\n\nKnowledge distillation trains a smaller and less computationally intensive model by transferring knowledge from a large and more accurate model. Its streamlined procedure is illustrated in Figure  1      DistillBERT  [43]  and TinyBERT  [44]  both integrate standard BERT into a model with fewer layers, and MobileBERT  [45]  integrates standard BERT into a narrower network, with the same number of layers as BERT. Similarly, Distilhubert  [46]  has the standard structure of HuBERT but fewer layers, retaining almost the same performance as HuBERT. One feature of knowledge distillation is that no matter how small your student model is, we need to train a large teacher model. Matrix decomposition aims to reduce the number of parameters and computation by decomposing the weight matrix in the model into the product of multiple successive sub-matrices, and its simplified process refers to Figure  1(c) . Early work on transformer-based matrix decomposition typically focused on approximating the attention matrix using a low-rank matrix for the self-attention module  [25] ,  [47] ,  [48] . Subsequently, researchers went beyond the attention module and performed matrix decomposition on the entire transformer to compress the model, potentially achieving greater compression  [49] -  [51] . Matrix decomposition does not decrease the number of layers in the model but only approximates certain modules in the transformer, such as the self-attention module and the FFN module. Consequently, matrix decomposition methods usually do not compress the model to a significant extent. As the degree of compression increases, the model's performance tends to decrease substantially.\n\nIn any scenario, despite the reduction in the size of the network through the aforementioned compression methods, it not only fails to provide the flexibility to design a network with a specific architecture but also does not combine excessive parameterization to enhance the performance of training a compact network  [52] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Structural Re-Parameterization",
      "text": "Structural re-parameterization  [53]  is different from all aforementioned methods. It decouples the model training and inference stages, typically employing a multi-branch structure during the training process, and effectively reverts back to the original single-branch structure during the inference phase. The simplified process of structural re-parameterization refers to Figure  1(d) . Structural re-parameterization was previously investigated in CNN networks, such as using asymmetric convolution instead of regular convolutional layers  [54] , expand-ing a single regular convolution into multiple convolutional layers  [52] , and expanding a regular convolution into a multibranch convolution  [53] . A common feature of these networks is that the expanded modules can be shrunk back into a singlebranch standard convolution in the inference stage, which allows the model to be characterized by both representational power and computational efficiency. To date, no structural reparameterization techniques have been studied for boosting the lightweight Transformer performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Re-Parameterization Of Lightweight Transformer",
      "text": "In this section, we first formalize the problem statement, then present the framework overview, and finally describe the detailed process and implementation rationale for the Transformer re-parameterization.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "We evaluate the proposed Transformer reparameterization in the application of SER. Let D = {(x 1 , y 1 ) , (x 2 , y 2 ) , • • • , (x n , y n )} be the dataset consisting of n utterances, where x i denotes the i th utterance and y i denotes its corresponding emotion label (e. g., neutral, sadness, happiness, fear, anger, surprise, and disgust). The goal is to train a model M on D to predict the emotion labels of the test utterances.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Overview",
      "text": "The overall framework diagram is shown in Figure  2 , where we select three widely used Transformer structures in the speech domain, i. e., ConvTransformer  [55] , Conformer  [56] , and SpeechFormer  [34] . The introduced Transformer reparameterization includes High-Rank Factorization (HRF) process in the training stage and de-High-Rank Factorization (deHRF) process in the inference stage.\n\nSpecifically, in the training stage: For ConvTransformer or SpeechFormer, we perform HRF on QKV module, Project module, FFN module, CLS module, and all modules (ALL). As the FFN module is composed of two fully connected (FC) layers, HRF FFN will have three strategies, i. e., FFN-1, FFN-2, and FFN-1 & FFN-2. Detailed information is illustrated in In the inference stage: All the HRF modules will be reconstructed and converted back to the original structure, which is presented in the left subfigures of Figure  2(a",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Linear Over-Parameterization",
      "text": "Before introducing our method, let's first qualitatively discuss the importance of linear overparameterization, which is also the theoretical basis of HRF. Suppose we are learning a linear model parameterized by a matrix W , obtained through training min {L (W )}. Now, we use a linear neural network with N layers to implement the learning process, then W = W N W N -1 . . . W 1 , where W j is the weight matrix of a specific layer. We use a lower learning rate η for gradient descent, and according to  [57] , the weight matrix W = W N W N -1 . . . W 1 satisfies the dynamics of continuous gradient descent:\n\nThen the update rule for the end-to-end weight matrix can be expressed in the form of a differential equation:\n\nfractional power operators defined on positive semi-definite matrices. We left multiply by a matrix W t (W t )\n\nT , right multiply by a matrix (W t ) T W t , and then sum over j, which is essentially a special form of preconditioning that promotes movement along the optimization direction and can be used for optimizing deep networks. More importantly, the above update rule does not depend on the width of the hidden layers in the linear neural network, but on the depth (N ). This can be explained as overparameterization promoting movement along the direction already taken for optimization and can thus be viewed as a form of acceleration  [57] .\n\nIn addition to the qualitative discussion of overparameterization, in practice, the performance of large language models strongly depends on the number of model parameters N (excluding embeddings), the size of the dataset D, and the computational resources C used for training  [58] . As long as we simultaneously increase N and D, the performance will predictably improve, which is known as the scale law of large language models  [59] ,  [60] . The scale law in practice demonstrates the importance of overparameterization for model performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. High-Rank Factorization",
      "text": "Theoretically, with the HRF processing, a FC layer that serves as the foundational layer in Transformer can be expanded into multiple FC layers. Assuming a FC layer with m input neurons and n output neurons, the output Y can be represented as:\n\nwhere X is the input, Y is the output of the FC layer, σ() is the activation function, W ∈ R m×n is the weight matrix, and b ∈ R n is the bias. The HRF process is defined as follows:\n\nN can be arbitrarily large. But, due to the computational cost of FC layers, we set the maximum N to 3 in this paper.\n\nis the expansion scale and can be set to 2, 4, or 8 times the second dimension of W , i. e., r = 2n, 4n, or 8n. Notably and importantly, when expanding one FC layer into multiple FC layers, no activation functions are applied after each extended HRF layer. This process is similar to the low-rank matrix factorization/de-composition (LRF) that is widely used for model compression  [17] : given any matrix W * ∈ R m×n of full rank r * ≤ min{m, n}, it can be decomposed into W * ≃ AH, where A ∈ R m×r * and H ∈ R r * ×n . When r * is much smaller than m or n, the space complexity can be significantly reduced from O(mn) to O(r * (m + n)). Because both LRF and HRF use multiple matrices to represent a single matrix with the difference that LRF has r * ≤ min{m, n} whilst HRF normally has r > max{m, n}, we refer to our proposed method as HRF. The details of HRF are shown in Algorithm 1. return F C 10: end procedure",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. De-High-Rank Factorization",
      "text": "In the training phase, we employ HRF in the model; whilst in the inference phase, we reconstruct the HRF model back to its original size and structure. The reconstruction process involves a chain of matrix calculation as no non-linear activation functions are conducted between the adjacent FC layers. Given one FC layer with one HRF layer expansion (i. e., N = 1) for example:\n\nwhere X is the input and Y is the output. After applying HRF to FC layer, we obtain Y * that is calculated using the following equation:\n\nBy doing this calculation, we can contract the two layers back to one original FC layer. Algorithm 2 describes the details of the deHRF process. To be noted, when N > 1, a similar chain of matrix calculation can be conducted to reconstruct the HRF modules.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "In this section, we introduce the selected datasets, experiment implementation details, and selected Transformer models, and present the results followed by extensive discussions and ablation studies.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "We evaluated the introduced Transformer Reparameterization on two multimodal emotional datasets -IEMOCAP  [61] , M 3 ED  [62]  and DAIC-WOZ  [63] . In this work, we mainly focus on the speech signals for emotion recognition.\n\nIEMOCAP is the most widely exploited dataset for SER. It comprises 12.46 hours of audio data, divided into five sessions with one male and one female speaker each. The conversations were segmented into utterances and annotated by at least three annotators using the following discrete categories: anger, happiness, sadness, neutrality, excitement, frustration, fear, surprise, and others. For this study, we focused on the first four categories, and merged \"excitement\" into \"happiness\". Following previous research  [64] ,  [65] , we used the first four sessions for training and the last session for testing. To reduce randomness, we utilized five random seeds for training and testing, and averaged the final results over the five tests. We took unweighted accuracy (UA), weighted accuracy (WA), and weighted average F1 (WF1) as performance metrics for model evaluation.\n\nM 3 ED is a recently released and the first Chinese multimodal sentiment dialogue dataset, comprising 990 dyadic emotional dialogues from 56 different TV series, with a total of 9 082 turns and 24 449 utterances. The dataset was annotated with seven emotional categories (i. e., happy, surprised, sad, disgusted, angry, fearful, and neutral) at the verbal level, including sound, visual, and textual patterns. We followed the default data partition strategy and used five random seeds for training, validation, and testing, with the final results averaged over the five tests. We took WF1 as the performance metric to evaluate models. Note that the metrics of UA and WA were not considered for M 3 ED mainly due to the alignment with other SOTA approaches for performance comparison. The detailed emotion distribution for the IEMOCAP and M3ED datasets are listed in Table  I .\n\nThe DAIC-WOZ dataset, derived from The AVEC2017 series depression detection task, comprised 189 segments of clinical interviews. The dataset was divided into 107 segments for training, 35 segments for validation, and 47 segments for testing. These interviews were conducted to aid in diagnosing conditions such as depression.\n\nEach segment in the dataset included audio and video features, alongside transcripts of the interviews. Segment durations ranged from 7 to 33 minutes, with an average duration of 16 minutes. To ensure fair comparisons with other state-ofthe-art works, we utilized the training and validation sets and reported macro average F1 (MF1) scores on the validation set.\n\nFor training convenience, we extracted individual patient voices from the provided transcripts and segmented them into 10 s clips. The training set consisted of 5084 clips, while the",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Generally, there are three types of models: the original model (the original large one), the lightweight model (a lighter version of the original one), and the HRF model (the lightweight model with an HRF process). We implemented all models under the PyTorch framework.\n\nThe initial learning rate for the original and lightweight ConvTransformer and Conformer were the same with 1e-3; whilst for the original and lightweight Speechformer they were 1e-3 and 5e-4, respectively. The learning rate decays to half when the loss does not decrease. In addition, the initial learning rate for the HRF model was identical to the corresponding lightweight model. All models used the AdamW optimizer, with the decay rate set to 1e-6 and the epoch set to 120 (for IEMOCAP) or 100 (for M 3 ED and DAIC-WOZ). The original ConvTransformer, Conformer, and SpeechFormer had the structures of  [8, 80, 320] ,  [4, 80, 320] , and  [8, 80, 64] , where the three values in each bracket denote the number of Transformer blocks (n layer ), the dimensionality of the input data for each Transformer block (d model ), and the dimensionality of the first FC layer in FFN (d f f n ), respectively. These original models were then compressed into tiny ones for the on-device deployment purpose, with new structures of  [1, 16, 4] ,  [1, 16, 2] , and  [1, 16, 4]  for lightweight ConvTransformer, Conformer, and SpeechFormer, respectively. All models hold four attention heads. Notably, all HRF models had the same parameters as their corresponding lightweight models during the inference stage. The default activation functions for ConvTransformer and SpeechFormer are ReLU, and for Conformer it is Swish.\n\nRegarding to the acoustic features, we followed previous work  [66]  and extracted 78-dimensional features from speech   signals using 26 filter banks with a window length of 25 ms and a hop size of 10 ms. These 78-dimensional features comprise 26 original LMFBs, and their first and second derivatives.\n\nPlease note that we do not directly employ the pre-trained models for feature extraction, as these pre-trained models are too large to be deployed on the device, or it is too sensitive to send the users' private data to the cloud. Besides, we segmented the sentences into 5 s and 4 s for IEMOCAP and M 3 ED datasets, respectively, and pursued a zero-filling strategy if the sentence was insufficiently long.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Baseline Models",
      "text": "Three typical Transformer models in the speech domain were selected to examine the performance of the introduced Transformer re-parameterization, and are listed as follows.\n\n• ConvTransformer  [55] : A classic Transformer architecture for speech recognition. On top of the classic Transformer, it employs a VGG-style convolutional network that is able to potentially extract high-level representations of speech signals and their relative positions.\n\n• Conformer  [56] : A frequently used model in speech domain. The Conformer is a Macaron-style structure which exploits FFN before and after the self-attention module.\n\n• SpeechFormer  [34] : A latest SOTA Transformer variant that extracts different levels (i. e., frame, phoneme, and word) of acoustic representations for SER. Compared to the original SpeechFormer, we made slight modifications. That is, we insert a linear layer to map the highdimensional features of model inputs into a smaller one, which makes the d model reduced.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Results And Discussions",
      "text": "As aforementioned in Section III-B, we investigated the modules of FFN, QKV, Project, CLS, and ALL for both ConvTransformer and SpeechFormer, and the one additional module of FFN-M for Conformer. Again, since an FFN is composed of two FC layers, it can be divided into FFN1, FFN2, and FFN1 & FFN2. It is the same with FFN-M (i. e., FFN-M1, FFN-M2, and FFN-M1 & FFN-M2).\n\nTable  III , Table  IV , and Table  V  present the results evaluated on the IEMOCAP, M 3 ED and DAIC-WOZ datasets for Con-vTransformer, Conformer, and SpeechFormer, respectively. All these results were obtained when the HRF expansion ratio is eight. First of all, we can see that the original Transformer models, some of which have already been compressed, have hundreds of thousands of parameters and FLOPs (i. e., computational complexity). With such kinds of model parameters and FLOPs, it is challenging to deploy these models on IoT devices.\n\nFor this reason, we largely compressed the original Con-vTransformer, Conformer, and SpeechFormer into tiny ones by reducing the depth and width of these models (shown in the second row of Table III-V) to make it more feasible under the on-device scenario. However, we also observe that their performance is greatly degraded, which is consistent with our expectations.\n\nWhen applying HRF to the three selected Transformers, it is found that HRF-plugged models outperform the ones without HRF layers in most cases. Even in some cases, they are comparable to the original models. Taking ConvTransformer for example, the best WF1s of the HRF model on IEMOCAP and M 3 ED datasets are 0.580 and 0.387, which are obviously higher than 0.550 and 0.372 for the lightweight model and are close to 0.586 and 0.399 for the original model, respectively. Similarly, on the DAIC-WOZ dataset, the best MF1 score of the HRF model is 0.580, markedly higher than 0.533 for the lightweight model and exceeds 0.572 for the original model. Therefore, the Transformer re-parameterization process can generally well fill the performance gap between the large models and their lightweight versions. This is largely attributed to the expansion of model size which plays an important role in model performance as discussed in previous work  [67] ,  [68] .\n\nIn addition, we also notice that SpeechFormer performs less well compared to the other two Transformers on IEMOCAP and M 3 ED datasets. This is possibly due to the specific design of SpeechFormer with four stages (each stage has a different number of layers of Transformer blocks) to extract features at different levels of speech (i. e., frames, phonemes, words, and utterances). However, the lightweight model has only one stage and can only extract features at the frame level, which leads to a decrease in the overall learning ability. On the contrary, SpeechFormer performs the best on the DAIC-WOZ dataset. This is because the DAIC-WOZ dataset has too few samples, and the parameter count of ConvTransformer and Conformer is approximately three times that of SpeechFormer, making it prone to overfitting issues. SpeechFormer, on the other hand, is easier to train. Moreover, when comparing different modules of FFN, QKV, Project, CLS, and ALL with HRF, we can generally find that the HRF FNN performs better than the other modules in five out of six cases with ConvTransformer, Conformer, and SpeechFormer on IEMOCAP, M 3 ED and DAIC-WOZ datasets. This finding basically suggests that FFN layers play a vital role in Transformer, which consecutively maps the representations from the low-level space to a high-level space in a non-linear way. However, it is worth noting that FFN often holds a large ratio of the total parameters of Transformers due to its full-connection characteristics. Therefore, it is important to balance the size of FFN and its overall performance, which further raises the importance of the introduced reparameterization approach.\n\nWe further observe that the HRF FFN2 is superior to the HRF FFN1 or HRF FFN1 & FFN2 in most cases. This indicates that mapping representations from high dimension to low dimension often lose a lot of information. This lost information can be well captured by inserting an HRF layer. Besides, it can be seen that when inserting an HRF layer for QKV attention modules, less performance improvement is observed. This is possible because of the redundancy of classic QKV matrics, which is consistent with previous findings in this work  [37] ,  [38] . Furthermore, we also notice that combining different modules for HRF does not seem to help much. One potential assumption might be that the inserted HRF layers have no activation functions, and thus they have limited capability for the model to learn non-linearly. Such a kind of linear transformation might not need several times in one Transformer block.\n\nIn addition to the above experiments, we also investigated how well the selected Transformer models are to confirm that these models are reliable representatives for SER. We compared the original three models with some other recent studies on SER. It is important to note that our proposed models are designed for mobile/edge device scenarios, which prevents us to exploit large pre-trained models, such as Wav2vec and HuBERT, to extract high-level representations. To ensure fair comparisons between these models, we only considered handcrafted acoustic features, including Mel spectrum, Logmel spectrum, LMFB, MFCC, etc. The results of these comparisons are shown in Table  VI    VIII , when using manual features, the original models can achieve comparable performance to other work for IEMOCAP and DAIC-WOZ. However, for M 3 ED, our baseline was not as good as the SOTA model  [62] . This is due to the pre-trained model utilized for feature extraction in  [62] . For a fair performance comparison, we further extracted speech features using HuBERT, and the results of all three selected baseline models were better than the models in  [62] 's. All these results indicate the reliability of the selected baselines.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Ablation Studies",
      "text": "Impact of different expansion ratios (r) of HRF: In order to examine the impact of HRF expansion at various ratios on the results, we conducted expansions of 0 (ratio0, original model), 2 (ratio2), 4 (ratio4), and 8 (ratio8) for all lightweight models in different modules. The obtained WF1 versus different expansion ratios of HRF for the three Transformers with different modules on the IEMOCAP, M 3 ED and DAIC-WOZ datasets are plotted in Figure  4 , Figure  5  and Figure  6  . When varying the expansion ratio, we can see that the HRF Transformers outperform their lightweight versions in most cases. This finding shows the robustness of the Transformer re-parameterization approach. Additionally, the different ratios display a slight double-increase trend, i. e., when r=2, 4, or 8, the performance shows an increase, decrease, and increase. We hypothesize that this outcome indicates the phenomenon of double descent  [74] ,  [75] , which has been proven to exist in image classification tasks. That is, the test error decreases, increases, and finally decreases again as the number of model parameters increases.\n\nImpact of different numbers of HRF layers (n): To quantitatively analyze the impact of different numbers of HRF layers, we extend one HRF layer into two and three layers.\n\nTo reduce the experiments, we took the best experimental setting for ConvTransformer, Conformer, and SpeechFormer. That is, we fixed the HRF expansion ratio to eight in all cases. Then, for IEMOCAP, we applied multiple HRF layers to the modules of FFN2, FFN M1, and FFN2, for ConvTransformer, Conformer, and SpeechFormer, respectively; whilst for M 3 ED and DAIC-WOZ, we applied HRF to FFN1, FFN M1, and FFN1 for ConvTransformer, Conformer, and SpeechFormer, respectively. The results versus different numbers of HRF layers are illustrated in Figure  7 , Figure  8  and Figure  9 . The results indicate that extending only one layer achieves the best results. This means that without a non-linear activation function, one HRF layer can be sufficiently well-trained. Increasing the depth of HRF layers may bring more noise and confusion into the system when doing gradient backpropagation. Impact of different activation functions for HRF: Different activation functions were used for various Transformer variants. We tested four commonly used activation functions for the FFN layer, i. e., ReLU, GELU, Swish, and Squared ReLU. The performance comparison between the lightweight Transformer and the HRF Transformer under different activation functions is depicted in Figure  10 , Figure  11  and Figure  12 . It was observed that under all scenarios the HRF Transformer outperform the lightweight Transformer. This finding further demonstrates the robustness of the Transformer re-parameterization approach.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "V. Conclusions",
      "text": "In the present paper, we introduce a structure reparameterization method to boost the performance of compressed Transformers. The Transformer architecture can be expanded via a high-rank factorization process into a bigger model in the training, which helps increase the model learning capability. In the inference, the expanded model is shrunk into the original one via a reverse mathematical calculation, while retaining the boosted model performance. Our approach is validated on the IEMOCAP, M 3 ED and DAIC-WOZ datasets for the speech emotion recognition task. Experimental results show the effectiveness of the structure re-parameterization method, which can enhance the lightweight Transformers, and even can make them comparable to larger models with significantly more parameters. The results also empirically reveal the robustness via investigating different numbers of expansion ratios and layers, and different activation functions.\n\nAlthough the initial findings are promising, further investigation of the introduced method is necessary to unlock its full potential. Firstly, except for the speech modality in this work, other modalities of text or video will be explored as well for multimodal learning in future work. Secondly, we will extend the evaluation task into a broader range, such as image recognition, speech recognition, and target detection.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a). Studies on Transformer-based",
      "page": 2
    },
    {
      "caption": "Figure 1: (b). One of the most basic purposes of",
      "page": 2
    },
    {
      "caption": "Figure 1: Four categories of model compression methods. (a) Pruning: Removing some weights or connections from the network.",
      "page": 3
    },
    {
      "caption": "Figure 1: (c). Early work on",
      "page": 3
    },
    {
      "caption": "Figure 1: (d). Structural re-parameterization was previously",
      "page": 3
    },
    {
      "caption": "Figure 2: Framework of Transformer re-parameterisation in different modules with the examples of ConvTransformer, Speech-",
      "page": 4
    },
    {
      "caption": "Figure 2: (a). Similar to ConvTransformer",
      "page": 4
    },
    {
      "caption": "Figure 3: Detailed illustration of the re-parameterization process",
      "page": 5
    },
    {
      "caption": "Figure 4: , Figure 5 and Figure 6",
      "page": 9
    },
    {
      "caption": "Figure 7: , Figure 8 and Figure 9. The",
      "page": 9
    },
    {
      "caption": "Figure 10: , Figure 11 and",
      "page": 9
    },
    {
      "caption": "Figure 12: It was observed that under all scenarios the HRF",
      "page": 9
    },
    {
      "caption": "Figure 4: Results of WF1 when applying different expansion ratios of HRF to different modules of ConvTransformer (a),",
      "page": 10
    },
    {
      "caption": "Figure 5: Results of WF1 when applying different expansion ratios of HRF to different modules of ConvTransformer (a),",
      "page": 10
    },
    {
      "caption": "Figure 6: Results of MF1 when applying different expansion ratios of HRF to different modules of ConvTransformer (a),",
      "page": 10
    },
    {
      "caption": "Figure 7: Results when applying different numbers of HRF layer to the second feedforward layers of ConvTransformer (a),",
      "page": 11
    },
    {
      "caption": "Figure 8: Results when applying different numbers of HRF layer to the second feedforward layers of ConvTransformer (a),",
      "page": 11
    },
    {
      "caption": "Figure 9: Results when applying different numbers of HRF layer to the second feedforward layers of ConvTransformer (a),",
      "page": 11
    },
    {
      "caption": "Figure 10: Performance of re-parameterized lightweight ConvTransformer (a), Conformer (b), or SpeechFromer (c) with diverse",
      "page": 12
    },
    {
      "caption": "Figure 11: Performance of re-parameterized lightweight ConvTransformer (a), Conformer (b), or SpeechFromer (c) with diverse",
      "page": 12
    },
    {
      "caption": "Figure 12: Performance of re-parameterized lightweight ConvTransformer (a), Conformer (b), or SpeechFromer (c) with diverse",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "ieving greater comp",
          "Column_2": "",
          "Column_3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "–": "A.\nf",
          "Column_2": "Problem F",
          "Column_3": "",
          "Column_4": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Classification head HRF Classification head\nAdd & Norm Add & Norm\nFeed Forward HRF Feed Forward\nAdd & Norm Add & Norm\nProject HRF Project\nConcat Concat\nScaled Dot-Product Attention Scaled Dot-Product Attention\nV K Q HRF V HRF K HRF Q\nPositional Positional\nEncoding Encoding\nConv/Linear Conv/Linear\nOriginal ConvTransformer/SpeechFormer HRF ConvTransformer/SpeechFor\n(a) ConvTransformer/SpeechFormer\nConvTransformer / SpeechFormer\nFig. 2: Framework of Transformer re-parameterisation in differe\nFormer, and Conformer.",
          "Column_2": "",
          "Column_3": ""
        },
        {
          "Column_1": "",
          "Column_2": "mer\nnt m",
          "Column_3": "4\nClassification head HRF Classification head\nAdd Add\nFeed Forward HRF Feed Forward\nNorm Norm\nAdd Add\nConv Module Conv Module\nNorm Norm\nAdd Add\nProject HRF Project\nConcat Concat\nScaled Dot-Product Attention Scaled Dot-Product Attention\nV K Q HRF V HRF K HRF Q\nNorm Norm\nAdd Add\nFeed Forward HRF Feed Forward\nNorm Norm\nPositional Positional\nEncoding Encoding\nConv Conv\nOriginal Conformer HRF Conformer\n(b) Conformer\nodules with the exampCleosnfoorfmeCronvTransformer, Speech-"
        },
        {
          "Column_1": "therightsubfigureofFigure2(a).SimilartoConvTransformer with\nand SpeechFomer, we apply HRF on the same modules for W\nN\nConformer.Nevertheless,asaMacaronstructureisperformed layer.\nfor Conformer, i.e., two FFN networks are used before and accor\naftertheself-attentionblock,weadditionallyconductHRFon satisfi\nthe additional FFN module (FFN M) as well, i.e., FFN M-\n1, FFN M-2, and FFN M-1 & FFN M-2. More details about\nthe HRF Conformer can be found in the right subfigure of\nTh\nFigure 2(b).\nbe ex\nIn the inference stage: All the HRF modules will be\nreconstructed and converted back to the original structure,\nwhich is presented in the left subfigures of Figure 2(a)\nWt+\nfor ConvTransformer or SpeechFormer and Figure 2(b) for\nConformer.\nTaking an FFN module of Transformer for example, Fig-\nwher\nure 3 depicts detailed HRF and deHRF processes. In the\n1,···\ntraining phase, three types of HRF expansions are performed,\ntive\ni.e.,FC1only(FFN-1),FC2only(FFN-2),andbothFC1and (cid:104)\nWt\nFC2 (FFN-1 & FFN-2). In the inference phase, the expanded\nthen\nFClayersareshrunktoasingleFClayerbyadeHRFprocess.\ncondi\nThe principles of HRF and deHRF are explained in Section\ndirect\nIII-D and Section III-E.\nimpo\nwidth\nC. Linear Over-Parameterization\nthe d\nBefore introducing our method, let’s first qualitatively dis- prom\ncuss the importance of linear overparameterization, which is optim\nalso the theoretical basis of HRF. Suppose we are learning a [57].\nlinear model parameterized by a matrix W, obtained through In\ntraining min{L(W)}. Now, we use a linear neural network zatio",
          "Column_2": "",
          "Column_3": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Add": "Feed Forward"
        },
        {
          "Add": "Norm"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Add": "HRF Feed Forward"
        },
        {
          "Add": "Norm"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Add": "Conv Module"
        },
        {
          "Add": "Norm"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Add": "Conv Module"
        },
        {
          "Column_1": "",
          "Add": "Norm"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Project\nConcat\nScaled Dot-Product Attention": "V K Q"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Nonlin",
          "Column_2": "",
          "Column_3": "earity"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Nonlin",
          "Column_2": "earity"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5.2, 12.2 13.1, 12.2 20.1, 12.2\n10": "5.2, 12.2 (a)ConvTransformer 13.1, 12.2 (b)Conformer 20.1, 12.2 (c)SpeechFormer\nFig. 4: Results of WF1 when applying different expansion ratios of HRF to different modules of ConvTransformer (a),\nConformer (b), or SpeechFromer (c) on the IEMOCAP dataset.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "5.2, 12.2 13.1, 12.2 20.1, 12.2\n10": "5.2, 12.2 (a)ConvTransformer 13.1, 12.2 (b)Conformer 20.1, 12.2 (c)SpeechFormer\nFig. 5: Results of WF1 when applying different expansion ratios of HRF to different modules of ConvTransformer (a),\nConformer (b), or SpeechFromer (c) on the M3ED dataset.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "5.2, 12.2 13.1, 12.2 20.1, 12.2\n10": "(a)ConvTransformer (b)Conformer (c)SpeechFormer\nFig. 6: Results of MF1 when applying different expansion ratios of HRF to different modules of ConvTransformer (a),\nConformer (b), or SpeechFromer (c) on the DAIC-WOZ dataset.\nREFERENCES usingosmoticcomputing,”IEEETransactionsonIndustrialInformatics,\nvol.18,no.5,pp.3377–3386,May2022.\n[1] V. Sharma, T. G. Tan, S. Singh, and P. K. Sharma, “Optimal and\nprivacy-aware resource management in artificial intelligence of things",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "11": "(a)ConvTransformer (b)Conformer (c)SpeechFormer\nFig. 7: Results when applying different numbers of HRF layer to the second feedforward layers of ConvTransformer (a),\nConformer (b), or SpeechFromer (c) on the IEMOCAP dataset.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "11": "(a)ConvTransformer (b)Conformer (c)SpeechFormer\nFig. 8: Results when applying different numbers of HRF layer to the second feedforward layers of ConvTransformer (a),\nConformer (b), or SpeechFromer (c) on the M3ED dataset.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "11": "(a)ConvTransformer (b)Conformer (c)SpeechFormer\nFig. 9: Results when applying different numbers of HRF layer to the second feedforward layers of ConvTransformer (a),\nConformer (b), or SpeechFromer (c) on the DAIC-WOZ dataset.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "(a) ConvTransformer (b) Conformer (c) SpeechFormer\nFig. 10: Performance of re-parameterized lightweight ConvTransformer (a), Conformer (b), or SpeechFromer (c) with diverse\nactivation functions on the IEMOCAP dataset.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "12": "(a) ConvTransformer (b) Conformer (c) SpeechFormer\nFig. 11: Performance of re-parameterized lightweight ConvTransformer (a), Conformer (b), or SpeechFromer (c) with diverse\nactivation functions on the M3ED dataset.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "12": "(a)ConvTransformer (b)Conformer (c)SpeechFormer\nFig. 12: Performance of re-parameterized lightweight ConvTransformer (a), Conformer (b), or SpeechFromer (c) with diverse\nactivation functions on the DAIC-WOZ dataset.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Optimal and privacy-aware resource management in artificial intelligence of things using osmotic computing",
      "authors": [
        "V Sharma",
        "T Tan",
        "S Singh",
        "P Sharma"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "2",
      "title": "Empowering things with intelligence: A survey of the progress, challenges, and opportunities in artificial intelligence of things",
      "authors": [
        "J Zhang",
        "D Tao"
      ],
      "year": "2021",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "3",
      "title": "Enable deep learning on mobile devices: Methods, systems, and applications",
      "authors": [
        "H Cai",
        "J Lin",
        "Y Lin",
        "Z Liu",
        "H Tang",
        "H Wang",
        "L Zhu",
        "S Han"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Design Automation of Electronic Systems"
    },
    {
      "citation_id": "4",
      "title": "Tinyad: Memoryefficient anomaly detection for time series data in industrial iot",
      "authors": [
        "Y Sun",
        "T Chen",
        "Q Nguyen",
        "H Yin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "5",
      "title": "Real-time speech emotion analysis for smart home assistants",
      "authors": [
        "R Chatterjee",
        "S Mazumdar",
        "R Sherratt",
        "R Halder",
        "T Maitra",
        "D Giri"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion detection using iot based deep learning for health care",
      "authors": [
        "Z Tariq",
        "S Shah",
        "Y Lee"
      ],
      "year": "2019",
      "venue": "Proc. 6th Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "7",
      "title": "Iot-enabled WBAN and machine learning for speech emotion recognition in patients",
      "authors": [
        "D Olatinwo",
        "A Abu-Mahfouz",
        "G Hancke",
        "H Myburgh"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "Emotion-aware connected healthcare big data towards 5g",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2018",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "9",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. 31th Annual Conference on Neural Information Processing Systems (NeuIPS)"
    },
    {
      "citation_id": "10",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": [
        "X Qiu",
        "T Sun",
        "Y Xu",
        "Y Shao",
        "N Dai",
        "X Huang"
      ],
      "year": "2020",
      "venue": "Science China Technological Sciences"
    },
    {
      "citation_id": "11",
      "title": "Transformers in vision: A survey",
      "authors": [
        "S Khan",
        "M Naseer",
        "M Hayat",
        "S Zamir",
        "F Khan",
        "M Shah"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "12",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "13",
      "title": "A survey of large language models",
      "authors": [
        "W Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "14",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Proc. 34th Annual Conference on Neural Information Processing Systems (NeuIPS)"
    },
    {
      "citation_id": "15",
      "title": "Pangu-σ: Towards trillion parameter language model with sparse heterogeneous computing",
      "authors": [
        "X Ren",
        "P Zhou",
        "X Meng",
        "X Huang",
        "Y Wang",
        "W Wang",
        "P Li",
        "X Zhang",
        "A Podolskiy",
        "G Arshinov"
      ],
      "year": "2023",
      "venue": "Pangu-σ: Towards trillion parameter language model with sparse heterogeneous computing",
      "arxiv": "arXiv:2303.10845"
    },
    {
      "citation_id": "16",
      "title": "Training compute-optimal large language models",
      "authors": [
        "J Hoffmann",
        "S Borgeaud",
        "A Mensch",
        "E Buchatskaya",
        "T Cai",
        "E Rutherford",
        "D Casas",
        "L Hendricks",
        "J Welbl",
        "A Clark"
      ],
      "year": "2022",
      "venue": "Training compute-optimal large language models",
      "arxiv": "arXiv:2203.15556"
    },
    {
      "citation_id": "17",
      "title": "Model compression and hardware acceleration for neural networks: A comprehensive survey",
      "authors": [
        "L Deng",
        "G Li",
        "S Han",
        "L Shi",
        "Y Xie"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "18",
      "title": "Federated learning: Challenges, methods, and future directions",
      "authors": [
        "T Li",
        "A Sahu",
        "A Talwalkar",
        "V Smith"
      ],
      "year": "2020",
      "venue": "IEEE signal processing magazine"
    },
    {
      "citation_id": "19",
      "title": "Efficient acceleration of deep learning inference on resource-constrained edge devices: A review",
      "authors": [
        "M Shuvo",
        "S Islam",
        "J Cheng",
        "B Morshed"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "20",
      "title": "Toward tailored models on private aiot devices: Federated direct neural architecture search",
      "authors": [
        "C Zhang",
        "X Yuan",
        "Q Zhang",
        "G Zhu",
        "L Cheng",
        "N Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "21",
      "title": "Compressing large-scale transformer-based models: A case study on bert",
      "authors": [
        "P Ganesh",
        "Y Chen",
        "X Lou",
        "M Khan",
        "Y Yang",
        "H Sajjad",
        "P Nakov",
        "D Chen",
        "M Winslett"
      ],
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Efficient transformers: A survey",
      "authors": [
        "Y Tay",
        "M Dehghani",
        "D Bahri",
        "D Metzler"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "23",
      "title": "Pruning deep neural networks by optimal brain damage",
      "authors": [
        "C Liu",
        "Z Zhang",
        "D Wang"
      ],
      "year": "2014",
      "venue": "Proc. 15th Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "24",
      "title": "An efficient framework for counting pedestrians crossing a line using low-cost devices: the benefits of distilling the knowledge in a neural network",
      "authors": [
        "Y Lin",
        "C Wang",
        "C Chang",
        "H Sun"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "25",
      "title": "Linformer: Selfattention with linear complexity",
      "authors": [
        "S Wang",
        "B Li",
        "M Khabsa",
        "H Fang",
        "H Ma"
      ],
      "year": "2020",
      "venue": "Linformer: Selfattention with linear complexity",
      "arxiv": "arXiv:2006.04768"
    },
    {
      "citation_id": "26",
      "title": "Wakeupnet: A mobiletransformer based framework for end-to-end streaming voice trigger",
      "authors": [
        "Z Zhang",
        "T Farnsworth",
        "S Lin",
        "S Karout"
      ],
      "year": "2022",
      "venue": "Wakeupnet: A mobiletransformer based framework for end-to-end streaming voice trigger",
      "arxiv": "arXiv:2210.02904"
    },
    {
      "citation_id": "27",
      "title": "From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty",
      "authors": [
        "J Han",
        "Z Zhang",
        "M Schmitt",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proc. 25th ACM international conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "28",
      "title": "Scaling laws for neural language models",
      "authors": [
        "J Kaplan",
        "S Mccandlish",
        "T Henighan",
        "T Brown",
        "B Chess",
        "R Child",
        "S Gray",
        "A Radford",
        "J Wu",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "29",
      "title": "Scaling laws for autoregressive generative modeling",
      "authors": [
        "T Henighan",
        "J Kaplan",
        "M Katz",
        "M Chen",
        "C Hesse",
        "J Jackson",
        "H Jun",
        "T Brown",
        "P Dhariwal",
        "S Gray"
      ],
      "year": "2020",
      "venue": "Scaling laws for autoregressive generative modeling",
      "arxiv": "arXiv:2010.14701"
    },
    {
      "citation_id": "30",
      "title": "The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning",
      "authors": [
        "S Ma",
        "R Bassily",
        "M Belkin"
      ],
      "year": "2018",
      "venue": "Proc. the 35th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "31",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "Proc. 45th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "32",
      "title": "The INTER-SPEECH 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi"
      ],
      "year": "2013",
      "venue": "Pro. 14th Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "33",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. 20th Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "34",
      "title": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2022",
      "venue": "Proc. 23th Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "35",
      "title": "Dwformer: Dynamic window transformer for speech emotion recognition",
      "authors": [
        "S Chen",
        "X Xing",
        "W Zhang",
        "W Chen",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Proc. 48th IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "37",
      "title": "Are sixteen heads really better than one",
      "authors": [
        "P Michel",
        "O Levy",
        "G Neubig"
      ],
      "year": "2019",
      "venue": "Proc. 33th Annual Conference on Neural Information Processing Systems (NeuIPS)"
    },
    {
      "citation_id": "38",
      "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "authors": [
        "E Voita",
        "D Talbot",
        "F Moiseev",
        "R Sennrich",
        "I Titov"
      ],
      "year": "2019",
      "venue": "Proc. 57th Conference of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "39",
      "title": "When BERT plays the lottery, all tickets are winning",
      "authors": [
        "S Prasanna",
        "A Rogers",
        "A Rumshisky"
      ],
      "year": "2020",
      "venue": "Proc. 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Virtual: ACL"
    },
    {
      "citation_id": "40",
      "title": "Earlybert: Efficient bert training via early-bird lottery tickets",
      "authors": [
        "X Chen",
        "Y Cheng",
        "S Wang",
        "Z Gan",
        "Z Wang",
        "J Liu"
      ],
      "year": "2021",
      "venue": "Proc. 59th Conference of the Association for Computational Linguistics (ACL). Virtual: ACL"
    },
    {
      "citation_id": "41",
      "title": "Reducing transformer depth on demand with structured dropout",
      "authors": [
        "A Fan",
        "E Grave",
        "A Joulin"
      ],
      "year": "2019",
      "venue": "Reducing transformer depth on demand with structured dropout",
      "arxiv": "arXiv:1909.11556"
    },
    {
      "citation_id": "42",
      "title": "On the effect of dropping layers of pre-trained transformer models",
      "authors": [
        "H Sajjad",
        "F Dalvi",
        "N Durrani",
        "P Nakov"
      ],
      "year": "2023",
      "venue": "Computer Speech and Language"
    },
    {
      "citation_id": "43",
      "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "arxiv": "arXiv:1910.01108"
    },
    {
      "citation_id": "44",
      "title": "Tinybert: Distilling BERT for natural language understanding",
      "authors": [
        "X Jiao",
        "Y Yin",
        "L Shang",
        "X Jiang",
        "X Chen",
        "L Li",
        "F Wang",
        "Q Liu"
      ],
      "year": "2020",
      "venue": "Proc. 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Virtual: ACL"
    },
    {
      "citation_id": "45",
      "title": "Mobilebert: a compact task-agnostic BERT for resource-limited devices",
      "authors": [
        "Z Sun",
        "H Yu",
        "X Song",
        "R Liu",
        "Y Yang",
        "D Zhou"
      ],
      "year": "2020",
      "venue": "Proc. 58th Conference of the Association for Computational Linguistics (ACL). Virtual: ACL"
    },
    {
      "citation_id": "46",
      "title": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "authors": [
        "H Chang",
        "S Yang",
        "H Lee"
      ],
      "year": "2022",
      "venue": "Proc. 47th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "47",
      "title": "Fmmformer: Efficient and flexible transformer via decomposed nearfield and far-field attention",
      "authors": [
        "T Nguyen",
        "V Suliafu",
        "S Osher",
        "L Chen",
        "B Wang"
      ],
      "year": "2021",
      "venue": "Proc. 35th Annual Conference on Neural Information Processing Systems (NeuIPS)"
    },
    {
      "citation_id": "48",
      "title": "Is attention better than matrix decomposition?",
      "authors": [
        "Z Geng",
        "M Guo",
        "H Chen",
        "X Li",
        "K Wei",
        "Z Lin"
      ],
      "year": "2021",
      "venue": "Proc. 9th International Conference on Learning Representations (ICLR). Virtual: OpenReview.net"
    },
    {
      "citation_id": "49",
      "title": "DRONE: dataaware low-rank compression for large NLP models",
      "authors": [
        "P Chen",
        "H Yu",
        "I Dhillon",
        "C Hsieh"
      ],
      "year": "2021",
      "venue": "Proc. 34th Annual Conference on Neural Information Processing Systems (NeuIPS)"
    },
    {
      "citation_id": "50",
      "title": "Lightweight and efficient end-to-end speech recognition using low-rank transformer",
      "authors": [
        "G Winata",
        "S Cahyawijaya",
        "Z Lin",
        "Z Liu",
        "P Fung"
      ],
      "year": "2020",
      "venue": "Proc. 45th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "51",
      "title": "A t 2 -tensor-aided multiscale transformer for remaining useful life prediction in iiot",
      "authors": [
        "L Ren",
        "Z Jia",
        "X Wang",
        "J Dong",
        "W Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "52",
      "title": "Expandnets: Linear overparameterization to train compact convolutional networks",
      "authors": [
        "S Guo",
        "J Alvarez",
        "M Salzmann"
      ],
      "year": "2020",
      "venue": "Proc. 34th Annual Conference on Neural Information Processing Systems (NeuIPS)"
    },
    {
      "citation_id": "53",
      "title": "Repvgg: Making vgg-style convnets great again",
      "authors": [
        "X Ding",
        "X Zhang",
        "N Ma",
        "J Han",
        "G Ding",
        "J Sun"
      ],
      "year": "2021",
      "venue": "Proc. 34th IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Virtual: IEEE"
    },
    {
      "citation_id": "54",
      "title": "Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks",
      "authors": [
        "X Ding",
        "Y Guo",
        "G Ding",
        "J Han"
      ],
      "year": "2019",
      "venue": "Proc. 32th International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "55",
      "title": "Transformer-transducer: End-to-end speech recognition with self-attention",
      "authors": [
        "C Yeh",
        "J Mahadeokar",
        "K Kalgaonkar",
        "Y Wang",
        "D Le",
        "M Jain",
        "K Schubert",
        "C Fuegen",
        "M Seltzer"
      ],
      "year": "2019",
      "venue": "Transformer-transducer: End-to-end speech recognition with self-attention",
      "arxiv": "arXiv:1910.12977"
    },
    {
      "citation_id": "56",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu",
        "R Pang"
      ],
      "year": "2020",
      "venue": "Proc. 21st Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "57",
      "title": "On the optimization of deep networks: Implicit acceleration by overparameterization",
      "authors": [
        "S Arora",
        "N Cohen",
        "E Hazan"
      ],
      "year": "2018",
      "venue": "Proc. the 35th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "58",
      "title": "Scaling laws for autoregressive generative modeling",
      "authors": [
        "T Henighan",
        "J Kaplan",
        "M Katz",
        "M Chen",
        "C Hesse",
        "J Jackson",
        "H Jun",
        "T Brown",
        "P Dhariwal",
        "S Gray"
      ],
      "year": "2020",
      "venue": "Scaling laws for autoregressive generative modeling",
      "arxiv": "arXiv:2010.14701"
    },
    {
      "citation_id": "59",
      "title": "Scaling laws for neural language models",
      "authors": [
        "J Kaplan",
        "S Mccandlish",
        "T Henighan",
        "T Brown",
        "B Chess",
        "R Child",
        "S Gray",
        "A Radford",
        "J Wu",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "60",
      "title": "Scaling laws for transfer",
      "authors": [
        "D Hernandez",
        "J Kaplan",
        "T Henighan",
        "S Mccandlish"
      ],
      "year": "2021",
      "venue": "Scaling laws for transfer",
      "arxiv": "arXiv:2102.01293"
    },
    {
      "citation_id": "61",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "62",
      "title": "M3ed: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "J Zhao",
        "T Zhang",
        "J Hu",
        "Y Liu",
        "Q Jin",
        "X Wang",
        "H Li"
      ],
      "year": "2022",
      "venue": "Proc. 60th Conference of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "63",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "J Gratch",
        "R Artstein",
        "G Lucas",
        "G Stratou",
        "S Scherer",
        "A Nazarian",
        "R Wood",
        "J Boberg",
        "D Devault",
        "S Marsella"
      ],
      "year": "2014",
      "venue": "Proc. 9th International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "64",
      "title": "Emotion recognition with multimodal transformer fusion framework based on acoustic and lexical information",
      "authors": [
        "L Guo",
        "L Wang",
        "J Dang",
        "Y Fu",
        "J Liu",
        "S Ding"
      ],
      "year": "2022",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "65",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "66",
      "title": "Semantic alignment network for multi-modal emotion recognition",
      "authors": [
        "M Hou",
        "Z Zhang",
        "C Liu",
        "G Lu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "67",
      "title": "Learning and generalization in overparameterized neural networks",
      "authors": [
        "Z Allen-Zhu",
        "Y Li",
        "Y Liang"
      ],
      "year": "2019",
      "venue": "Proc. 33th Annual Conference on Neural Information Processing Systems (NeuIPS)"
    },
    {
      "citation_id": "68",
      "title": "A convergence theory for deep learning via over-parameterization",
      "authors": [
        "Z Allen-Zhu",
        "Y Li",
        "Z Song"
      ],
      "year": "2019",
      "venue": "Proc. 36th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "69",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. 46th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "70",
      "title": "Exploiting vocal tract coordination using dilated cnns for depression detection in naturalistic environments",
      "authors": [
        "Z Huang",
        "J Epps",
        "D Joachim"
      ],
      "year": "2020",
      "venue": "Proc. 45th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "71",
      "title": "Towards robust deep neural networks for affect and depression recognition from speech",
      "authors": [
        "A Othmani",
        "D Kadoch",
        "K Bentounes",
        "E Rejaibi",
        "R Alfred",
        "A Hadid"
      ],
      "year": "2021",
      "venue": "Proc. 25th International Conference on Pattern Recognition (ICPR). ICPR International Workshops and Challenges"
    },
    {
      "citation_id": "72",
      "title": "Hybrid cnn-svm classifier for efficient depression detection system",
      "authors": [
        "A Saidi",
        "S Othman",
        "S Saoud"
      ],
      "year": "2020",
      "venue": "Proc. 4th International Conference on Advanced Systems and Emergent Technologies (IC ASET)"
    },
    {
      "citation_id": "73",
      "title": "The detection of depression using multimodal models based on text and voice quality features",
      "authors": [
        "H Solieman",
        "E Pustozerov"
      ],
      "year": "2021",
      "venue": "Proc. 2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus)"
    },
    {
      "citation_id": "74",
      "title": "Double trouble in double descent: Bias and variance(s) in the lazy regime",
      "authors": [
        "S Ascoli",
        "M Refinetti",
        "G Biroli",
        "F Krzakala"
      ],
      "year": "2020",
      "venue": "Proc. 37th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "75",
      "title": "Deep double descent: Where bigger models and more data hurt",
      "authors": [
        "P Nakkiran",
        "G Kaplun",
        "Y Bansal",
        "T Yang",
        "B Barak",
        "I Sutskever"
      ],
      "year": "2020",
      "venue": "Proc. 8th International Conference on Learning Representations (ICLR)"
    }
  ]
}