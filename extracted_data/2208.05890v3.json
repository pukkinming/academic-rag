{
  "paper_id": "2208.05890v3",
  "title": "Speech Synthesis With Mixed Emotions",
  "published": "2022-08-11T15:45:58Z",
  "authors": [
    "Kun Zhou",
    "Berrak Sisman",
    "Rajib Rana",
    "B. W. Schuller",
    "Haizhou Li"
  ],
  "keywords": [
    "Emotional speech synthesis",
    "mixed emotions",
    "sequence-to-sequence",
    "the ordinal nature of emotions",
    "relative difference",
    "emotion attribute vector"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional speech synthesis aims to synthesize human voices with various emotional effects. The current studies are mostly focused on imitating an averaged style belonging to a specific emotion type. In this paper, we seek to generate speech with a mixture of emotions at run-time. We propose a novel formulation that measures the relative difference between the speech samples of different emotions. We then incorporate our formulation into a sequence-to-sequence emotional text-to-speech framework. During the training, the framework does not only explicitly characterize emotion styles but also explores the ordinal nature of emotions by quantifying the differences with other emotions. At run-time, we control the model to produce the desired emotion mixture by manually defining an emotion attribute vector. The objective and subjective evaluations have validated the effectiveness of the proposed framework. To our best knowledge, this research is the first study on modelling, synthesizing, and evaluating mixed emotions in speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "H UMANS can feel multiple emotional states at the same time  [1] . Consider some bittersweet moments such as remembering a lost love with warmth or the first time leaving home for college, it is possible to experience the co-occurrence of different types of emotions -even two oppositely valenced emotions (e. g., happy and sad)  [2] ,  [3] . Emotional speech synthesis aims to add emotional effects to a synthesized voice  [4] . Synthesizing mixed emotions will mark a milestone for achieving human-like emotions in speech synthesis, thus enabling a higher level of emotional intelligence in human-computer interaction  [5] ,  [6] ,  [7] .\n\nSpeech synthesis aims to generate human-like voices from input text  [8] ,  [9] ,  [10] . With the advent of deep learning, the state-of-the-art speech synthesis systems  [11] ,  [12] ,  [13]  are able to produce speech of high naturalness and intelligibility. However, most of them do not convey the omnipresent emotional contexts in human-human interaction  [14] ,  [15] ,  [16] . The lack of expressiveness limits the emotional intelligence of current speech synthesis systems  [17] . Emotional speech synthesis aims to fill this gap  [18] ,  [19] ,  [20] .\n\nSynthesizing a mixed emotional effect is a challenging task. One of the reasons is the subtle nature of human emotions  [21] . Therefore, it is not straightforward to precisely characterize speech emotion. Besides, speech emotion is inherently supra-segmental, complex with multiple acoustic cues such as timbre, pitch and rhythm  [22] ,  [23] . Both spectral and prosodic variants need to be studied when modelling speech emotion. The early studies on emotional speech synthesis rely on statistical modelling of different speech parameters with hidden Markov models (HMM)  [24] ,  [25]  and Gaussian mixture model (GMM)  [26] ,  [27] . Deep neural networks (DNN)  [28] ,  [29]  and deep bi-directional long-short-term memory network (DBLSTM)  [30] ,  [31]  represent the recent advances. The end-to-end neural architecture  [32] ,  [33]  becomes popular because of its superior performance. We note that there are generally two types of methods in the literature to learn emotion information: one uses auxiliary emotion labels as the condition of the framework  [34] ,  [35] , and the other imitates the emotion style of the reference speech  [36] ,  [37] . However, these methods learn the global temporal structure of speech emotion, resulting in a monotonous expressiveness in synthesized speech. In this way, these frameworks can only synthesize several emotion types exhibited in the database. These disadvantages limit the flexibility and controllability of the above frameworks. For example, it is hard to synthesize mixed emotional effects with existing emotional speech synthesis frameworks.\n\nFor the first time, we study the modelling of mixed emotions in speech synthesis. In psychology, there have been studies  [38] ,  [39]  to understand the paradigms and measures of mixed emotions. However, the study of mixed emotions in speech synthesis is not given attention yet, where there exist two main research problems:  (1)  how to characterize and quantify the mixture of speech emotions, and (2) how to evaluate the synthesized speech. In this article, we will address these two challenges.\n\nThe main contributions of this article are listed as follows:\n\n• For the first time, we study the modelling of mixed emotions for speech synthesis, which brings us a step closer to achieving emotional intelligence; • We introduce a novel scheme to measure the relative difference between emotion categories, with which the emotional text-to-speech framework learns to quantify the differences between the emotion styles of speech samples during the training. At run-time, we control the model to produce the desired emotion mixture by manually defining an emotion attribute vector; • We carefully devise objective and subjective evaluations to confirm the effectiveness of the proposed framework and the emotional expressiveness of the speech.\n\nThis paper is organized as follows: In Section 2, we motivate our study by introducing the background and related work. In Section 3, we present the details of our proposed framework, and we introduce our experiments in Section 4. We provide further investigations in Section 5. The study is concluded in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Related Work",
      "text": "This work is built on several previous studies on the characterization of emotions, sequence-to-sequence emotion modelling for speech synthesis and controllable emotional speech synthesis. We briefly introduce the related studies to set the stage for our research and rationalize the novelty of our contributions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Characterization Of Emotions",
      "text": "Understanding human emotions (e. g., their nature and functions) has been gaining lots of attention in psychology  [40] ,  [41] ,  [42] . This study is inspired by several previous research, including the theory of the emotion wheel and the ordinal nature of emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Theory Of The Emotion Wheel",
      "text": "Humans can experience around 34, 000 different emotions  [43] . While it is hard to understand all these distinct emotions, Plutchik proposed 8 primary emotions: anger, fear, sadness, disgust, surprise, anticipation, trust and joy, and arranged them in an emotion wheel  [44]  as shown in Figure  1 . All other emotions can be regarded as mixed or derivative states of these primary emotions  [44] . According to the theory of the emotion wheel, the changes in intensity could produce the diverse amount of emotions we can feel. Besides, the adding up of primary emotions could produce new emotion types. For example, delight can be produced by combining joy and surprise  [45] .\n\nDespite these efforts in psychology, there is almost no attempt to model the mixed emotions in the literature of speech synthesis. Inspired by the theory of the emotion wheel, we believe it is possible to combine different primary emotions and synthesize mixed emotions in speech. This technique will also allow us to create new emotion types that are hard to collect in real life, which could help us better mimic human emotions and further enhance the engagement in human-robot interaction. Fig.  1 : An illustration of the theory of the emotion wheel  [44] , where all emotions occur as the mixed or derivative states of eight primary emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Ordinal Nature Of Emotions",
      "text": "Emotions are intrinsically relative, and their annotations and analysis should follow the ordinal path  [46] ,  [47] . Instead of assigning an absolute score or an emotion category, ordinal methods characterize emotions through comparative assessments (e. g., is sentence one happier than sentence two?).\n\nOrdinal methods have shown remarkable performance, especially in speech emotion recognition  [48] ,  [49] ,  [50] .\n\nThe key idea of ordinal methods is to learn a ranking according to the given criterion. An example is preference learning  [51] , where the task is to establish preferences between samples. Once the preferences are established, ranking samples  [52] ,  [53] ,  [54]  is straightforward. Other rankbased methods  [55] ,  [56] ,  [57]  also show the effectiveness of modelling the affect for speech emotion recognition. As for emotional speech synthesis, researchers also explore the ordinal nature of emotions to model the emotion intensity  [58] ,  [59] ,  [60] ,  [61] , where the intensity of an emotion is treated as the relative difference between neutral and emotional samples. Inspired by the previous studies, we aim to study rank-based methods to quantify the relative differences between the speech samples from different emotion categories, which we discuss later.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Sequence-To-Sequence Emotion Modelling For Speech Synthesis",
      "text": "The sequence-to-sequence model with attention mechanism was first studied in machine translation  [62]  and later on found effective in speech synthesis  [12] ,  [63] . We consider that sequence-to-sequence models are suitable for modelling speech emotion. Sequence-to-sequence models are more effective in modelling the long-term dependencies at different temporal levels such as word, phrase and utterance  [64] . By learning attention alignment, sequence-to-sequence models can capture the dynamic prosodic variants within an utterance  [65] . They also allow for the prediction of the speech duration at run-time, which is a critical prosodic factor of the speech emotion  [66] .\n\nThere are generally two types of methods in the literature to model speech emotions: 1) explicit label-based and 2) reference-based approaches. Next, we will briefly introduce these two approaches in sequence-to-sequence modelling.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Learn To Associate With Explicit Labels",
      "text": "It is the most straightforward to characterize emotion by using explicit emotion labels  [34] ,  [35] , where the model learns to associate labels with emotion styles. In  [34] , an emotion label vector is taken by the attention-based decoder to produce the desired emotion. In  [35] , a low-resourced emotional text-to-speech is built using model adaptation with a few emotion labels. In addition to the explicit labels of discrete emotion categories, there are attempts to condition the decoder with continuous variables  [67] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Learn To Imitate A Reference",
      "text": "Another approach is to use a style encoder to imitate and transplant the reference style  [32] . Global style token (GST)  [36]  is an example to learn style embeddings from the reference audio in an unsupervised manner. Some studies incorporate additional emotion recognition loss  [33] ,  [68] , perceptual loss  [60] ,  [69]  or adversarial training  [70]  to help with the emotion rendering. Other studies  [71] ,  [72] ,  [73] ,  [74]  replace the global style embedding with phoneme or segmental level prosody embedding to capture multiscale emotion variants. Similar approaches have also been applied to emotional voice conversion research. In  [75] , the style encoder further acts as the emotion encoder to learn actual emotion information through a two-stage training. In  [76] , a speaker encoder is further introduced to preserve the speaker information.\n\nThese successful attempts motivate us to leverage the sequence-to-sequence mechanism to enable emotion modelling for speech synthesis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Controllable Emotional Speech Synthesis",
      "text": "Speech emotion is often manifested in various prosody aspects  [77] . Emotion rendering can be controlled by modifying different prosodic cues. Current studies  [78] ,  [79]  mainly focus on designing the prosody embedding as a control vector that is derived from a representation learning framework. For example, style tokens  [36]  are designed to represent high-level styles such as speaker style, pitch range and speaking rate. Emotion rendering can be controlled by choosing specific tokens. Recent attempts  [80] ,  [81]  study a way to include a hierarchical, fine-grained prosody representation into the style token-based diagram  [36] . Some other studies also use variational autoencoders (VAE)  [82]  to control the speech style by learning, scaling or combining disentangled representations  [83] ,  [84] .\n\nRecently, emotion intensity control has attracted much attention in emotional speech synthesis. Emotion intensity is considered to be correlated with all the acoustic cues that contribute to speech emotion  [85] , which makes itself even more subjective and challenging to model. Some studies use Fig.  2 : Block diagram of our proposed relative scheme applied to emotional text-to-speech at run-time.\n\nauxiliary features such as a state of voiced, unvoiced and silence (VUS)  [86] , attention weights or a saliency map  [87]  to control the emotion intensity. Other studies manipulate the internal emotion representations through interpolation  [88] , scaling  [76]  or distance-based quantization  [89] . In  [58] ,  [59] ,  [60] ,  [61] , relative attributes are introduced to learn a more interpretable representation of emotion intensity. However, none of these frameworks studied the correlation and interplay between different emotions. This contribution aims to fill this research gap.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Summary Of Research Gap",
      "text": "We briefly summarize the gaps in the current literature on speech synthesis that we aim to address in this study:\n\n• The synthesis of mixed emotions has not been studied in speech synthesis, which limits the capability of current systems to imitate human emotions; • Despite much progress in psychology, it is still challenging to characterize and quantify the mixture of emotions in speech; • Current evaluation methods are inadequate to assess mixed emotional effects. The rethinking of the current evaluation for mixed emotions is needed. This study is a departure from the current studies on emotional speech synthesis. We seek to display the possibilities to synthesize mixed emotions that are subtle but do exist in our real life.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mixed Emotion Modelling And Synthesis",
      "text": "We propose a novel relative scheme that allows for manually manipulating the synthesized emotion, i.e. mixing multiple different emotion styles. As shown in Figure  2 , the proposed scheme allows for flexible control of the extent of each contributing emotion in the speech. At run-time, the framework transfers the reference emotion into a new utterance with the text input, also known as emotional text-to-speech.\n\nWe first describe our method of characterizing mixed emotions in speech and highlight our contributions to designing a novel relative scheme. Then, we present the details of the sequence-to-sequence emotion training with the proposed relative scheme. Lastly, we show the flexible control of the proposed framework for synthesizing mixed emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Characterization Of Mixed Emotions In Speech",
      "text": "Emotion can be characterized with either categorical  [90] ,  [91]  or dimensional representations  [92] ,  [93] . With designated emotion labels, the emotion category approach is the  most straightforward way to represent emotions. However, such representation ignores the subtle variations of emotions. Another approach seeks to model the physical properties of speech emotion with dimensional representations. An example is Russell's circumplex model  [92] , where emotions are distributed in a two-dimensional circular space, containing arousal and valence dimensions.\n\nOne of the most straightforward ways to characterize mixed emotions is to inject different emotion styles into a continuous space. Mixed emotions could be synthesized by adjusting each dimension carefully. However, only a few emotional speech databases  [94] ,  [95]  provide such annotations. These dimensional annotations are subjective and expensive to collect. Therefore, we only utilize discrete emotion labels available in most databases. We first make an assumption based on the theory of the emotion wheel  [44] : Mixed emotions are characterized by combinations, mixtures, or compounds of primary emotions. While it is not straightforward to add up emotions, we explore the ordinal nature of emotions instead.\n\nWe propose a rank-based relative scheme to quantify the relative difference between speech recordings with different emotion types. Mixed emotions can be characterized by adjusting the relative difference with other emotion types. The relative difference value can also quantify the level of engagement of each emotion. We introduce our design of a novel relative scheme next.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Design Of A Novel Relative Scheme",
      "text": "One of the challenges of synthesizing mixed emotions is quantifying the association or the interplay between different emotions. Inspired by the ordinal nature of emotions, we propose a novel relative scheme to address this challenge. We first make two assumptions according to the theory of the emotion wheel:  (1)  all emotions are related to some extent; (2) each emotion has stereotypical styles. In our proposal, we not only characterize the identifiable styles of each emotion but also seek to quantify the similarity between different emotion styles. Fig.  4:  The training diagram of the proposed framework. The pre-trained relative scheme learns to generate an emotion attribute vector that measures the relative difference between the input emotion style ('Happy') and other primary emotion styles ('Angry', 'Sad', 'Surprise' and 'Neutral').\n\nWe study a rank-based method to measure the relative difference between emotion categories, which can offer more informative descriptions and thus be closer to human supervision  [96] . In computer vision, the relative attribute  [96]  represents an effective way to model the relative difference between two categories of data. Inspired by the success in various computer vision tasks  [97] ,  [98] ,  [99] , we believe relative attributes bridge between the low-level features and high-level semantic meanings, which allows us to model the relative difference between emotions only with discrete emotion labels. In this way, we regard the identifiable emotion style as an attribute of speech data, which can be represented with a rich set of emotion-related acoustic features. The relative difference of the emotion styles can be modelled as a relative attribute, which is called \"emotion attribute\" in this article. The emotion attribute can be learned through a max-margin optimization problem as explained below:\n\nGiven a training set T = {x n }, where x n is the acoustic features of the n th training sample, and T = A ∪ B, where A and B are two different emotion sets, we aim to learn a ranking function given as below:\n\nwhere W is a weighting matrix indicating the difference in emotion styles. According to hypotheses (1) and (  2 ), we propose the following constraints:\n\nThe weighting matrix W is estimated by solving the following problem similar to that of a support vector machine  [100] :\n\nwhere C is the trade-off between the margin and the size of slack variables ξ i,j and γ i,j . Through Eq. (  4 ) -(  7 ), we learn a wide-margin ranking function that enforces the ordering on each training point. As shown in Figure  3 (a), we train a relative ranking function f (x) between each emotion pair. At the inference phase, the trained function can estimate an emotion attribute of unseen data, as shown in Figure  3(b) . In practice, each emotion attribute value is normalized to [0, 1], where a smaller value indicates a similar emotional style. All the normalized emotion attributes form an emotion attribute vector. The emotion attribute vector bridges the discrete primary emotion labels and is further incorporated in sequence-tosequence emotion training.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Training Strategy",
      "text": "We adopt an emotional text-to-speech framework with the joint training of voice conversion as in  [75] . As both textto-speech and voice conversion share a common goal of generating realistic speech from the internal representations, the joint training was shown effective  [101] ,  [102] ,  [103] ,  [104] . The text-to-speech task could benefit from the phone-embedding vectors  [105] ,  [106] , or the prosody style introduced by a reference encoder  [32] . A shared decoder between text-to-speech and voice conversion contributes to a robust decoding process  [107] ,  [108] ,  [109] .\n\nThe overall emotional text-to-speech framework is an encoder-decoder model that is trained as a sequence-tosequence system, as shown in Figure  4 , where the text Fig.  5 : The run-time diagram of the proposed emotional textto-speech framework. The emotion rendering can be manually controlled via the relative scheme. By assigning the appropriate percentage to the attribute vector, we produce a target emotion mixture. encoder and linguistic encoder generate an embedding sequence for the input, while the emotion encoder generates one embedding that encapsulates the whole reference speech sample.\n\nGiven the text or speech as input, the text and the linguistic encoder learn to predict the linguistic embedding from the text or speech, respectively. The decoder takes the linguistic embedding from the text or speech in an alternative manner, depending on whether the epoch number is odd or even. Similar to  [102] , a contrastive loss is used to ensure the similarity between these two types of linguistic embeddings. The adversarial training strategy with an emotion classifier is employed on the acoustic linguistic embedding to eliminate the residual emotion information.\n\nAn emotion encoder is used to extract an emotion embedding vector from the input speech under the supervision of an emotion label. Meanwhile, an emotion attribute vector is generated by the pre-trained relative scheme described in Section 3.2, and then produced by a fully connected (FC) layer, resulting in a relative embedding. The emotion embedding describes the emotion styles of the input speech, while the emotion attribute vector indicates the difference between the input emotion style and other emotion styles. Finally, the decoder learns to reconstruct the input emotion style from a combination of emotion and relative embeddings.\n\nThe whole training procedure can be viewed as a recognition-synthesis process at the sequence level. Our proposed framework does not only learn the abundant emotion variance that is exhibited in a database but also the correlation or association across different emotion categories. It allows us to explicitly adjust the difference level at run-time and further enables mixed emotion synthesis and the flexible control of emotion rendering at the same time, which will be discussed next.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Control Of Emotion Rendering",
      "text": "We illustrate our proposed emotional text-to-speech framework in Figure  5 , which renders controllable emotional speech at run-time. The framework consists of three main modules, the content encoder, the emotion controller, and the decoder.\n\nThe text encoder projects the linguistic information from the input text into an internal representation. The emotion encoder captures the emotion style in an embedding from the reference speech, while the relative scheme further introduces the characteristics of other emotion types with a manually assigned attribute vector. By varying the percentage for each primary emotion in the attribute vector, we can easily synthesize the desired emotional effects and control the emotion rendering in synthesized speech.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Evaluations",
      "text": "In this section, we report our experimental settings and results. As shown in Table  1 , for all the experiments, we synthesize mixed emotional effects by mixing a primary emotion (Surprise) with three reference emotions (Happy, Angry and Sad) respectively. We expect to synthesize mixed emotional effects similar with the secondary emotions such as Delight, Outrage and Disappointment, respectively. We choose these three combinations because they are thought to be easier to perceive for the listeners and have been studied in psychology  [1] ,  [44] .\n\nSince this contribution serves as a pioneer in related fields, there is no literature or reference method before this study, to our best knowledge. Therefore, we could not include any baselines in our experiments. Instead, we adopt objective and subjective metrics widely used in previous literature and carefully design evaluation methods to show the effectiveness of our proposal. We have made the source codes and speech demos available to the public 1 . We encourage readers to listen to the speech samples on our demo website to best understand this work.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "We use acoustic features and phoneme sequences as inputs to the proposed framework during the training. The acoustic features are 80-dimensional logarithm Mel-spectrograms extracted every 12.5 ms with a frame size of 50 ms for shorttime Fourier transform (STFT). We convert text to phoneme with the Festival  [110]  G2P tool to serve as the input to the text encoder. At run-time, we synthesize emotional speech from the text input.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Network Configuration",
      "text": "Our proposed framework can be regarded as a sequencelevel recognition-synthesis structure similar to that of  [102] ,  [111] . Both the linguistic encoder and the decoder have a sequence-to-sequence encoder-decoder structure. The linguistic encoder consists of an encoder, a 2-layer 256-cell BLSTM and a decoder, a 1-layer 512-cell BLSTM with an attention layer followed by a full-connected (FC) layer with  an output channel of 512. The decoder has the same model architecture as that of Tacotron  [12] . The text encoder is a 3-layer 1D CNN with a kernel size of 5 and a channel number of 512. The text encoder is followed by a 1-layer of 256-cell BLSTM and an FC layer with an output channel number of 512. The style encoder is a 2-layer 128-cell BLSTM followed by an FC layer with an output channel number of 64. The classifier is a 4-layer FC with channel numbers of {512, 512, 512, 5}.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Pipeline",
      "text": "We first pre-train a relative ranking function between each emotion pair using an emotional speech dataset. We implement the relative ranking function following an opensource repository 2 . We use a standardized set of 384 acoustic features extracted with openSMILE  [112]  as the input features. These features include zero-crossing rate, frame energy, pitch frequency, and Mel-frequency cepstral coefficient (MFCC) used in the Interspeech Emotion Challenge  [113] . The trained ranking functions reported a classification accuracy of 97% on the test set.\n\nWe then conduct a two-stage training strategy to train our text-to-speech framework, which consists of (1) Multispeaker text-to-speech training with the VCTK Corpus  [114]  and (2) Emotion Adaptation for text-to-speech with a single speaker from the ESD dataset  [115] ,  [116] . The proposed text-to-speech framework learns abundant speaker styles with a multi-speaker corpus and then learns the actual emotion information with a small amount of emotional speech data. The training strategy we used is similar to that of  [75] . During the training, we use the Adam optimizer  [117]  and set the batch size to 64 and 4 for multi-speaker text-tospeech training and emotion adaptation, respectively. We set the learning rate to 0.001 and the weight decay to 0.0001 for multi-speaker text-to-speech training. We halve the learning rate every seven epochs during the emotion adaptation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Preparation",
      "text": "We select the VCTK Corpus  [114]  to perform multi-speaker text-to-speech training, where we use 99 speakers and the total duration of training speech data is about 30 hours. We select the ESD dataset  [115] ,  [116]  to perform emotion adaptation and relative ranking training. We choose one English male ('0013') and one English female ('0019') speaker from the ESD. We consider five emotions: Neutral, Angry, Happy, Sad and Surprise, and for each emotion, we follow the data partition given in the ESD. For each speaker and each emotion, we use 300, 30 and 20 utterances for training, testing, and evaluation, respectively. The total duration of emotional speech training data is around 50 minutes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Objective Evaluation",
      "text": "We first perform objective evaluations to validate the proposed mixed emotion synthesis. We demonstrate the effectiveness of our proposals and provide analysis with a pretrained speech emotion recognition (SER) model. We calculate Mel-cepstral distortion (MCD) and Pearson correlation coefficient (PCC) as objective evaluation metrics.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Analysis With Speech Emotion Recognition",
      "text": "We train a speech emotion recognition model on the ESD dataset  [115]  with the same data partition described in Section 4.1.3. To improve the robustness of SER, data augmentation is performed by adding white Gaussian noise during the SER training  [118] ,  [119] ,  [120] ,  [121] .\n\nThe SER architecture is the same as that in  [122] , which includes: 1) a three-dimensional (3-D) CNN layer; 2) a BLSTM; 3) an attention layer; and 4) a fully connected (FC) layer. We evaluate our synthesized mixed emotions with the pre-trained SER. We use the classification probabilities derived from the softmax layer of the SER to analyze the effects of mixed emotions. As a high-level feature, the classification probabilities summarize the useful emotion information from the previous layers for final decisionmaking. The classification probabilities offer us an effective tool to justify how well each emotional component can be perceptually recognized by the SER from the emotion mixture.\n\nWe first report the classification probabilities for a male speaker ('0013') in Figure  6 . We evaluate four different combinations where we gradually increase the percentage (0%, 30%, 60%, 90%) of Angry, Happy or Sad while keeping that of Surprise always being 100%. As shown in Figure  6 (a), we observe that the probability of Angry increases while we increase the percentage of Angry from 0% to 90%. In the meanwhile, the probability of Surprise decreases but still remains to be higher than for others. The probability of Angry achieves 0.25 when the percentage of Angry reaches 90%. We also note similar observations for Happy and Sad as shown in Figure  6(b)  and (c) .\n\nWe then report the classification probabilities for a female speaker ('0019') in Figure  7 . Similar to that of the male speaker, we report four different percentages (0%, 30%, 60%, 90%) of Angry, Happy or Sad while keeping that of Surprise being 100%. For Happy, we observe the probability of Happy considerably increases while we increase the percentage of Happy in mixed emotions as shown in Figure  7(b) . For Angry and Sad, we find similar observations as in Figure  7 (a) and (c). These observations indicate that the mixed emotions can be perceptually recognized by a pre-trained SER.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Mel-Cepstral Distorion",
      "text": "Spectral features, based on the short-term power spectrum of sound, such as Mel-cepstral coefficients (MCEPs), contain rich information about expressivity and emotion  [123] . Melcepstral Distortion (MCD)  [124]  is a widely adopted metric 0&'>G%@ )HPDOH6SHDNHU 0DOH6SHDNHU (a) Mixing Surprise (100%) with different percentages of Angry (0%, 30%, 60%, 90%) 3HUFHQWDJHRI+DSS\\ 0&'>G%@ (b) Mixing Surprise (100%) with different percentages of Happy (0%, 30%, 60%, 90%) 3HUFHQWDJHRI6DG 0&'>G%@ (c) Mixing Surprise (100%) with different percentages of Sad (0%, 30%, 60%, 90%) to measure the spectrum similarity, which is calculated between the synthesized (ŷ = {ŷ m }) and the target MCEPs (y = {y m }):\n\nwhere M represents the dimension of the MCEPs. A lower value of MCD indicates a higher degree of in the spectrum.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Pearson Correlation Coefficient",
      "text": "Pitch is considered a major prosodic factor contributing to speech emotion, closely correlated to the activity level  [125] ,  [126] . In practice, the pitch is often represented by the fundamental frequency (F0), which can be estimated with the harvest algorithm  [127] . We calculate the Pearson Correlation Coefficient (PCC) of F0 to measure the linear dependency between two F0 sequences, which has been used in previous studies  [128] ,  [129] ,  [130] . The PCC between two F0 sequences is given as:\n\nwhere cov(•) represents the covariance function, σ F s 0 and σ F t 0 are the standard deviations of the synthesized sequences (F s 0 ) and the target F0 sequences (F t 0 ), respectively.\n\nA higher PCC value represents a higher degree of similarity in prosody.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion Of The Mcd And Pcc Results",
      "text": "To show the effectiveness of synthesizing mixed emotions, we calculate MCD and PCC between the synthesized results and the reference emotions (Angry, Happy and Sad). We choose one male ('0013') and one female speaker ('0019') from the ESD dataset  [115] . For each speaker, we use 20 utterances for evaluation. We report four different percentages of Angry, Happy and Sad that are: 0%, 30%, 60% and 90%. Again, we keep Surprise as the primary emotion that has a percentage of Surprise is always 100%. We first compare spectrum similarity as shown in Figure  8 . For all three different combinations, we observe that the MCD values decrease as the percentage of reference emotions (Angry, Happy and Sad) increases as shown in Figure  8 (a), (b) and (c). These results show that the synthesized emotion becomes more similar to the reference emotions in the spectrum as we increase the percentage of the reference emotions.\n\nWe have similar observations for prosody similarity as shown in Figure  9 . As the percentage of reference emotions (Angry, Happy and Sad) increases, we observe that the PCC value consistently increases. It indicates that the synthesized mixed emotions have a stronger correlation with the reference emotions (Angry, Happy and Sad) in terms of the prosody variance. These results show that we can effectively synthesize and further control the rendering of mixed emotions in terms of the spectrum and prosody.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "We conduct subjective evaluations with human listeners, whom we ask to focus on two aspects: (1) Speech Quality and (2) Emotion Perception.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Speech Quality",
      "text": "We first conduct the Mean Opinion Score (MOS) test to evaluate speech quality, covering the speech's naturalness, intelligibility and listening efforts. All participants are asked to listen to the reference speech (\"Ground truth\") and the synthesized speech with mixed emotions and score the \"quality\" of each speech sample on a 5-point scale ('5' for excellent, '4' for good, '3' for fair, '2' for poor, and '1' for bad). 20 subjects listened to 80 speech samples in total (80 = 5 x 4 (# of percentages) x 3 (Angry, Happy and Sad) + 20 (# of Ground truth)). The actual speech samples can be found in our demo website. We report the MOS results in Table  2 , which show that our synthesized mixed emotions retain the speech quality between fair and good.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotion Perception",
      "text": "We then conduct the best-worst scaling (BWS) test to evaluate the emotion perception of synthesized mixed emotions. All participants are asked to listen to the speech samples and choose the best and the worst one according to their perception of a specific emotion type. 20 subjects listened to 168 speech samples in total (168 = 7 x 4 (# of percentages) x 6 (Angry, Happy, Sad, Outrage, Delight and Disappointment)).\n\nThe actual speech samples can be found on our demo website.\n\nWe first evaluate the perception of the reference emotions (Angry, Happy and Sad) that are mixed with Surprise. As shown in Table  3a , 3b and 3c, the mixed emotion with  90% of the reference emotions consistently achieves the highest percentage of the \"Best\" score; also, the \"Best\" score increases as the percentage of reference emotion increases.\n\nSimilarly, the highest \"Worst\" score is observed when the reference emotion is added at the lowest percentage (0%). These results confirm the effectiveness of controlling the rendering of mixed emotions. We also observe a slight rise of the worst rating when the percentage of Happy and Sad exceeds 60% in Table  3b , and 3c. This observation we attribute to the unnatural emotional expressions that may be created to influence listeners' preferences.\n\nWe then take one step further to evaluate the perception of Outrage, Delight and Disappointment in synthesized speech. In psychology, there is evidence that those feelings could be produced by combining several emotions. We observe that participants can perceive such feelings, and most of them choose those with 90% of reference emotions as the \"Best\", as shown in Table  4a , 4b and 4c. As for the rating of \"Worst\", we also have similar observations to those in Table  3 . These results show that we can synthesize new emotion types that are subtle and hard to collect in real life, which will significantly benefit the research community.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "We further conduct ablations studies to validate the contributions of the proposed relative scheme on emotional expression. We compare the proposed framework with or without the relative scheme through several XAB preference tests, where the participants are asked to listen to the reference emotional speech first, then choose the one closer to the reference in terms of emotional expression. 20 subjects listened to 60 speech samples in total (60 = 5 x 2 (# of frameworks) x 4 (# of emotions) + 20 (# of ground truth)).\n\nWe report the XAB results in Figure  10  where we observe that \"Proposed w/ Relative Scheme\" consistently and considerably outperforms \"Proposed w/o Relative Scheme\" for all emotions (Angry, Happy, Sad and Surprise). Besides, the p values calculated between those two pairs (\"Proposed w/ Relative Scheme\" and \"Proposed w/o Relative Scheme\") are always lower than 0.05, indicating that the out-performance did not occur by chance. These results demonstrate that our relative scheme can improve emotional intelligibility in synthesized emotional speech.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Further Investigations And Discussion",
      "text": "In this section, we expand our experiments and show the ability of our proposed methods on other interesting topics. We first investigate the mixed emotional effects of Happy and Sad, which are two oppositely valenced emotions. We then build an emotion transition system with our proposed method. We do not seek to conduct comprehensive evaluations but to provide some interesting insights into mixed emotion synthesis and its applications. All the speech samples are provided on the demo page.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Oppositely Valenced Emotions: Happy And Sad",
      "text": "In our experiments, we mostly focus on mixing Surprise with other emotions (Angry, Happy and Sad), which is thought to be easier to perceive for human listeners. Here, we move one step further to study a more challenging task, which is to synthesize mixed effects of Happy and Sad. In Russell's valence-arousal model  [92] , Happy and Sad are two conflicting emotions with opposite valance (Pleasant and Unpleasant). There are some debates that agree with the co-existence of conflicting emotions  [131] ,  [132] . In real life, there are also some terms to describe such feelings in different cultures, for example, \"Bittersweet\" in English. Professional actors are thought to be able to deliver such",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "An Emotion Transition System",
      "text": "One potential application of mixed emotion synthesis is building an emotion transition system  [133] . Emotion transition aims to gradually transition the emotion state from one to another. One similar study is emotional voice conversion  [116] , which aims to convert the emotional state. Compared with emotional voice conversion, the key challenge of emotion transition is to synthesize internal states between different emotion types. With our proposed methods, we are able to model these internal states by mixing them with different emotions. To achieve this, the sum up of the percentages of each emotion needs to be 100% (e. g., 80% Surprise with 20% Angry; 40% Happy with 60% Sad). Then, we can synthesize various internal emotion states by adjusting the percentages.\n\nCompared with traditional methods such as interpolation, our proposed system is data-driven, and the synthesized emotions are more natural.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "This study serves as the first attempt to model and synthesize mixed emotions for speech synthesis. Although we have shown the effectiveness of our methods, the related problems have not been completely solved. We provide a discussion to address the concerns, show our findings, and inspire future studies.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Category Vs. Dimensional Emotion Models",
      "text": "Our assumptions, formulation, and evaluation of mixed emotions are all based on categorical emotion studies. We note that mixed emotions can also be modelled with dimensional representations such as arousal, valence, and dominance. A dimensional model can capture a wide range of emotional concepts, which offers a means of measuring the similarity of different emotional states  [134] . However, several problems need to be adequately dealt with when modelling mixed emotions with a dimensional model. As mentioned in Section 3.1, the significant challenge for using dimensional representations comes from the lack of labels. Besides, humans are more efficient at discriminating among options than giving an absolute score  [135] , which adds challenges to the evaluation process. Furthermore, dimensional models are restricted to modelling the co-occurrence of like-valenced discrete emotions  [136] . For these reasons, we refrain from applying dimensional emotions to the current framework.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Remaining Challenges",
      "text": "There are a few remaining challenges that need attention from the community. As mentioned in Section 4.3.2, increasing the percentage of adding emotions may result in unnatural emotional expressions. If the synthesized emotion sounds unnatural or is difficult to understand, it may not be effective in achieving the desired outcome. Additionally, the human voice is a complex and highly variable instrument, and different people can produce the same emotional state in very different ways. This can make it difficult to accurately capture and reproduce a desired mix of emotions. At last, human raters are asked to evaluate the mixed emotions totally based on their personal experiences because of the lack of \"ground truth\" emotions. People from different cultures may have different experiences and backgrounds that can influence their emotional responses, and having a diverse group of evaluators can provide a more wellrounded perspective on the synthesized emotions.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Potential Improvements",
      "text": "We discuss several potential improvements to inspire future studies on mixed emotion synthesis: 1) Selection of ranking functions: adopt deep learning-based ranking methods  [137]  to improve the performance of ranking; 2) Multispeaker studies: add training data from multiple speakers; 3) Non-autoregressive backbone frameworks: use nonautoregressive TTS framework as the backbone to avoid the misalignment of attention and improve the naturalness of synthesized speech.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "This contribution fills the gap on mixed emotion synthesis in the literature on speech synthesis. We proposed an emotional speech synthesis framework that is based on a sequence-to-sequence model. For the first time, with the proposed framework, we are able to synthesize mixed emotions and further control the rendering of mixed emotions at runtime. The key highlights are as follows:\n\n1) We proposed a novel relative scheme to measure the difference between each emotion pair. We demonstrate that our proposed relative scheme enables the effective synthesis and control of the rendering of mixed emotions. Through ablation studies, we also show that the proposed relative scheme improves emotional intelligibility in synthesized speech; 2) We presented a comprehensive study to evaluate mixed emotions for the first time. Through both objective and subjective evaluations, we validated our idea and showed the effectiveness of our proposed framework in terms of synthesizing mixed emotions; 3) We present further investigations on synthesising a bittersweet feeling and an emotion triangle. The investigation study serves as an additional contribution to the article, which could broaden the scope of the study. In this article, we only focused on studying mixed emotions for emotional text-to-speech. We believe that our proposed relative scheme could enable mixed emotion synthesis in most existing emotional speech synthesis frameworks, including but not limited to emotional text-to-speech. We will expand our experiments to include emotional voice conversion in our future studies.\n\nThe future work includes: 1) a comparison with other ranking methods such as metric learning  [138]  and Siamese neural networks [137]; 2) conducting experiments for more emotion combinations, speakers, and other languages. Our future directions also include the study of cross-lingual emotion style modeling and transfer. Besides, a closer look at linguistic prosody for emotional speech synthesis is foreseen; for example, different semantic meanings can affect the way of expressing an emotion.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: All other emotions can be regarded as mixed or deriva-",
      "page": 2
    },
    {
      "caption": "Figure 1: An illustration of the theory of the emotion wheel",
      "page": 2
    },
    {
      "caption": "Figure 2: Block diagram of our proposed relative scheme",
      "page": 3
    },
    {
      "caption": "Figure 2: , the proposed",
      "page": 3
    },
    {
      "caption": "Figure 3: The illustration of the proposed relative scheme at (a) training and (b) run-time phase. A relative ranking function is",
      "page": 4
    },
    {
      "caption": "Figure 4: The training diagram of the proposed framework. The",
      "page": 4
    },
    {
      "caption": "Figure 3: (a), we train a relative ranking function",
      "page": 5
    },
    {
      "caption": "Figure 3: (b). In practice, each",
      "page": 5
    },
    {
      "caption": "Figure 4: , where the text",
      "page": 5
    },
    {
      "caption": "Figure 5: The run-time diagram of the proposed emotional text-",
      "page": 5
    },
    {
      "caption": "Figure 5: , which renders controllable emotional",
      "page": 6
    },
    {
      "caption": "Figure 6: Classiﬁcation probabilities derived from the pre-trained SER model for a male speaker (’0013’) from the ESD dataset.",
      "page": 7
    },
    {
      "caption": "Figure 7: Classiﬁcation probabilities derived from the pre-trained SER model for a female speaker (’0019’) from the ESD",
      "page": 7
    },
    {
      "caption": "Figure 6: We evaluate four different",
      "page": 7
    },
    {
      "caption": "Figure 6: (b) and (c).",
      "page": 7
    },
    {
      "caption": "Figure 7: Similar to that of the male",
      "page": 7
    },
    {
      "caption": "Figure 7: (b). For Angry",
      "page": 7
    },
    {
      "caption": "Figure 8: Mel-cepstral distortion (MCD) [dB] calculated between the Mel-cepstral coefﬁcients (MCEPs) of mixed emotions",
      "page": 8
    },
    {
      "caption": "Figure 9: Pearson Correlation Coefﬁcient (PCC) calculated between the fundamental frequency (F0) of mixed emotions and",
      "page": 8
    },
    {
      "caption": "Figure 8: For all three different combinations, we observe that the",
      "page": 8
    },
    {
      "caption": "Figure 8: (a), (b) and (c). These results show that the synthesized",
      "page": 8
    },
    {
      "caption": "Figure 9: As the percentage of reference emo-",
      "page": 8
    },
    {
      "caption": "Figure 10: where we observe",
      "page": 10
    },
    {
      "caption": "Figure 10: XAB preference test results with 95% conﬁdence",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "Bj ¨orn W. Schuller, Fellow,\nIEEE, and Haizhou Li, Fellow,\nIEEE"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "Abstract—Emotional speech synthesis aims to synthesize human voices with various emotional effects. The current studies are mostly"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "focused on imitating an averaged style belonging to a speciﬁc emotion type.\nIn this paper, we seek to generate speech with a mixture"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "of emotions at run-time. We propose a novel\nformulation that measures the relative difference between the speech samples of different"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "emotions. We then incorporate our formulation into a sequence-to-sequence emotional\ntext-to-speech framework. During the training,"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "the framework does not only explicitly characterize emotion styles but also explores the ordinal nature of emotions by quantifying the"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "differences with other emotions. At run-time, we control\nthe model\nto produce the desired emotion mixture by manually deﬁning an"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "emotion attribute vector. The objective and subjective evaluations have validated the effectiveness of\nthe proposed framework. To our"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "best knowledge,\nthis research is the ﬁrst study on modelling, synthesizing, and evaluating mixed emotions in speech."
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "the ordinal nature of emotions, relative\nIndex Terms—Emotional speech synthesis, mixed emotions, sequence-to-sequence,"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "difference, emotion attribute vector"
        },
        {
          "Kun Zhou, Student Member,\nIEEE, Berrak Sisman, Member,\nIEEE, Rajib Rana, Member,\nIEEE,": "(cid:70)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "H UMANS can feel multiple emotional states at the same"
        },
        {
          "1\nINTRODUCTION": "as remembering a lost\nlove with warmth or\nthe ﬁrst\ntime"
        },
        {
          "1\nINTRODUCTION": "leaving home for\ncollege,\nit\nis possible to experience the"
        },
        {
          "1\nINTRODUCTION": "co-occurrence of different\ntypes of\nemotions\n-\neven two"
        },
        {
          "1\nINTRODUCTION": "oppositely valenced emotions (e. g., happy and sad) [2], [3]."
        },
        {
          "1\nINTRODUCTION": "Emotional speech synthesis aims to add emotional effects"
        },
        {
          "1\nINTRODUCTION": "to a\nsynthesized voice\n[4]. Synthesizing mixed emotions"
        },
        {
          "1\nINTRODUCTION": "will mark a milestone for achieving human-like emotions in"
        },
        {
          "1\nINTRODUCTION": "speech synthesis, thus enabling a higher level of emotional"
        },
        {
          "1\nINTRODUCTION": "intelligence in human-computer interaction [5], [6], [7]."
        },
        {
          "1\nINTRODUCTION": "Speech synthesis\naims\nto generate human-like voices"
        },
        {
          "1\nINTRODUCTION": "from input\ntext\n[8],\n[9],\n[10]. With\nthe\nadvent\nof deep"
        },
        {
          "1\nINTRODUCTION": "learning,\nthe state-of-the-art speech synthesis systems [11],"
        },
        {
          "1\nINTRODUCTION": "[12],\n[13] are able to produce speech of high naturalness"
        },
        {
          "1\nINTRODUCTION": "and intelligibility. However, most of\nthem do not\nconvey"
        },
        {
          "1\nINTRODUCTION": "the omnipresent emotional contexts in human-human inter-"
        },
        {
          "1\nINTRODUCTION": "action [14],\n[15],\n[16]. The lack of expressiveness limits the"
        },
        {
          "1\nINTRODUCTION": "emotional\nintelligence of current speech synthesis systems"
        },
        {
          "1\nINTRODUCTION": "[17]. Emotional speech synthesis aims to ﬁll\nthis gap [18],"
        },
        {
          "1\nINTRODUCTION": "[19], [20]."
        },
        {
          "1\nINTRODUCTION": "Synthesizing a mixed emotional effect\nis a challenging"
        },
        {
          "1\nINTRODUCTION": "task. One of\nthe\nreasons\nis\nthe\nsubtle nature of human"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "•\nKun\nZhou\nis\nwith\nthe\nDepartment\nof\nElectrical\nand\nComputer"
        },
        {
          "1\nINTRODUCTION": "Engineering, National University\nof\nSingapore,\nSingapore.\nE-mail:"
        },
        {
          "1\nINTRODUCTION": "zhoukun@u.nus.edu"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "•\nBerrak Sisman is with the Department of Electrical and Computer En-"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "gineering at\nthe University of Texas\nat Dallas, United States. E-mail:"
        },
        {
          "1\nINTRODUCTION": "berraksisman@u.nus.edu"
        },
        {
          "1\nINTRODUCTION": "•\nRajib Rana is with the University of Southern Queensland, Australia."
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "E-mail: Rajib.Rana@usq.edu.au"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "•\nBj¨orn W. Schuller\nis with GLAM – the Group on Language, Audio,"
        },
        {
          "1\nINTRODUCTION": "& Music,\nImperial College London, U. K., and the Chair of Embedded"
        },
        {
          "1\nINTRODUCTION": "Intelligence\nfor Health Care\nand Wellbeing, University\nof Augsburg,"
        },
        {
          "1\nINTRODUCTION": "Germany. Email: bjoern.schuller@imperial.ac.uk"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "•\nHaizhou Li\nis with the Shenzhen Research Institute of Big Data, and the"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "School of Data Science, The Chinese University of Hong Kong, Shenzhen,"
        },
        {
          "1\nINTRODUCTION": "China; the Department of Electrical and Computer Engineering, National"
        },
        {
          "1\nINTRODUCTION": "University of Singapore, Singapore; and the University of Bremen, Bre-"
        },
        {
          "1\nINTRODUCTION": "men, Germany. Email: haizhouli@cuhk.edu.cn"
        },
        {
          "1\nINTRODUCTION": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotions for speech synthesis, which brings us a step": "closer to achieving emotional intelligence;"
        },
        {
          "emotions for speech synthesis, which brings us a step": "• We introduce a novel scheme to measure the relative"
        },
        {
          "emotions for speech synthesis, which brings us a step": ""
        },
        {
          "emotions for speech synthesis, which brings us a step": "difference between emotion categories, with which the"
        },
        {
          "emotions for speech synthesis, which brings us a step": ""
        },
        {
          "emotions for speech synthesis, which brings us a step": "emotional\ntext-to-speech framework learns to quantify"
        },
        {
          "emotions for speech synthesis, which brings us a step": "the differences between the emotion styles of\nspeech"
        },
        {
          "emotions for speech synthesis, which brings us a step": "samples during the training. At\nrun-time, we control"
        },
        {
          "emotions for speech synthesis, which brings us a step": ""
        },
        {
          "emotions for speech synthesis, which brings us a step": "the model\nto produce the desired emotion mixture by"
        },
        {
          "emotions for speech synthesis, which brings us a step": "manually deﬁning an emotion attribute vector;"
        },
        {
          "emotions for speech synthesis, which brings us a step": "• We carefully devise objective and subjective evaluations"
        },
        {
          "emotions for speech synthesis, which brings us a step": "to conﬁrm the effectiveness of the proposed framework"
        },
        {
          "emotions for speech synthesis, which brings us a step": "and the emotional expressiveness of the speech."
        },
        {
          "emotions for speech synthesis, which brings us a step": "This paper\nis organized as\nfollows:\nIn Section 2, we"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• We carefully devise objective and subjective evaluations": ""
        },
        {
          "• We carefully devise objective and subjective evaluations": ""
        },
        {
          "• We carefully devise objective and subjective evaluations": "This paper"
        },
        {
          "• We carefully devise objective and subjective evaluations": ""
        },
        {
          "• We carefully devise objective and subjective evaluations": "motivate\nour"
        },
        {
          "• We carefully devise objective and subjective evaluations": "related work."
        },
        {
          "• We carefully devise objective and subjective evaluations": "proposed framework, and we introduce our experiments in"
        },
        {
          "• We carefully devise objective and subjective evaluations": "Section 4. We provide further"
        },
        {
          "• We carefully devise objective and subjective evaluations": "The study is concluded in Section 6."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "The study is concluded in Section 6.",
          "Sadness + Disgust\nSurprise + Sadness": "= Remorse\n= Disappointment"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "2\nBACKGROUND AND RELATED WORK",
          "Sadness + Disgust\nSurprise + Sadness": "Fig. 1: An illustration of\nthe theory of\nthe emotion wheel"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "[44], where all emotions occur as the mixed or derivative"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "This work\nis\nbuilt\non\nseveral\nprevious\nstudies\non\nthe",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "states of eight primary emotions."
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "characterization of emotions, sequence-to-sequence emotion",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "modelling for speech synthesis and controllable emotional",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "speech synthesis. We brieﬂy introduce the related studies to",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "2.1.2\nThe Ordinal Nature of Emotions"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "set the stage for our research and rationalize the novelty of",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "Emotions are intrinsically relative, and their annotations and"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "our contributions.",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "analysis should follow the ordinal path [46], [47]. Instead of"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "assigning an absolute score or an emotion category, ordinal"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "methods characterize emotions through comparative assess-"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "2.1\nCharacterization of Emotions",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "ments\n(e. g.,\nis\nsentence one happier\nthan sentence\ntwo?)."
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "Understanding\nhuman\nemotions\n(e. g.,\ntheir\nnature\nand",
          "Sadness + Disgust\nSurprise + Sadness": "Ordinal methods have shown remarkable performance, es-"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "functions) has been gaining lots of attention in psychology",
          "Sadness + Disgust\nSurprise + Sadness": "pecially in speech emotion recognition [48], [49], [50]."
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "[40],\n[41],\n[42]. This\nstudy is\ninspired by several previous",
          "Sadness + Disgust\nSurprise + Sadness": "The key idea of ordinal methods is to learn a ranking"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "research, including the theory of the emotion wheel and the",
          "Sadness + Disgust\nSurprise + Sadness": "according to the given criterion. An example is preference"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "ordinal nature of emotions.",
          "Sadness + Disgust\nSurprise + Sadness": "learning [51], where the task is to establish preferences be-"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "tween samples. Once the preferences are established, rank-"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "ing samples [52],\n[53],\n[54]\nis straightforward. Other rank-"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "2.1.1\nTheory of\nthe Emotion Wheel",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "based methods\n[55],\n[56],\n[57] also show the effectiveness"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "Humans can experience around 34, 000 different emotions",
          "Sadness + Disgust\nSurprise + Sadness": "of modelling the affect\nfor speech emotion recognition. As"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "[43]. While it\nis hard to understand all\nthese distinct emo-",
          "Sadness + Disgust\nSurprise + Sadness": "for emotional speech synthesis, researchers also explore the"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "tions, Plutchik proposed 8 primary emotions: anger,\nfear,",
          "Sadness + Disgust\nSurprise + Sadness": "ordinal nature of emotions to model\nthe emotion intensity"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "sadness, disgust,\nsurprise, anticipation,\ntrust and joy, and",
          "Sadness + Disgust\nSurprise + Sadness": "[58],\n[59],\n[60],\n[61], where\nthe\nintensity\nof\nan\nemotion"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "arranged them in an emotion wheel [44] as shown in Figure",
          "Sadness + Disgust\nSurprise + Sadness": "is\ntreated as\nthe\nrelative difference between neutral\nand"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "1. All other emotions can be regarded as mixed or deriva-",
          "Sadness + Disgust\nSurprise + Sadness": "emotional samples. Inspired by the previous studies, we aim"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "tive\nstates of\nthese primary emotions\n[44]. According to",
          "Sadness + Disgust\nSurprise + Sadness": "to study rank-based methods to quantify the relative differ-"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "the theory of\nthe emotion wheel,\nthe changes in intensity",
          "Sadness + Disgust\nSurprise + Sadness": "ences between the speech samples from different emotion"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "could produce the diverse amount of emotions we can feel.",
          "Sadness + Disgust\nSurprise + Sadness": "categories, which we discuss later."
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "Besides, the adding up of primary emotions could produce",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "new emotion types. For example, delight can be produced",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "2.2\nSequence-to-Sequence\nEmotion\nModelling\nfor"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "by combining joy and surprise [45].",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "",
          "Sadness + Disgust\nSurprise + Sadness": "Speech Synthesis"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "Despite these efforts in psychology,\nthere is almost no",
          "Sadness + Disgust\nSurprise + Sadness": ""
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "attempt\nto model\nthe mixed emotions\nin the literature of",
          "Sadness + Disgust\nSurprise + Sadness": "The sequence-to-sequence model with attention mechanism"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "speech synthesis.\nInspired by the\ntheory of\nthe\nemotion",
          "Sadness + Disgust\nSurprise + Sadness": "was ﬁrst studied in machine translation [62] and later on"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "wheel, we believe it is possible to combine different primary",
          "Sadness + Disgust\nSurprise + Sadness": "found effective in speech synthesis [12],\n[63]. We consider"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "emotions and synthesize mixed emotions\nin speech. This",
          "Sadness + Disgust\nSurprise + Sadness": "that sequence-to-sequence models are suitable for modelling"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "technique will also allow us\nto create new emotion types",
          "Sadness + Disgust\nSurprise + Sadness": "speech emotion. Sequence-to-sequence models are more ef-"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "that are hard to collect\nin real\nlife, which could help us",
          "Sadness + Disgust\nSurprise + Sadness": "fective in modelling the long-term dependencies at different"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "better mimic\nhuman\nemotions\nand further\nenhance\nthe",
          "Sadness + Disgust\nSurprise + Sadness": "temporal levels such as word, phrase and utterance [64]. By"
        },
        {
          "Section 4. We provide further\ninvestigations\nin Section 5.": "engagement in human-robot interaction.",
          "Sadness + Disgust\nSurprise + Sadness": "learning attention alignment, sequence-to-sequence models"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Neutral:             0%": "+ 30% Happy)"
        },
        {
          "Neutral:             0%": "Happy:            30%"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "Surprise:       100%\nReference Speech"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "(Label: Surprise)"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "Fig.\n2: Block diagram of\nour proposed relative\nscheme"
        },
        {
          "Neutral:             0%": "applied to emotional text-to-speech at run-time."
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "auxiliary features such as a state of voiced, unvoiced and"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "silence (VUS) [86], attention weights or a saliency map [87]"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "to control\nthe emotion intensity. Other studies manipulate"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "the internal emotion representations through interpolation"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "[88], scaling [76] or distance-based quantization [89]. In [58],"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "[59],\n[60],\n[61],\nrelative attributes are\nintroduced to learn"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "a more\ninterpretable\nrepresentation of\nemotion intensity."
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "However, none of these frameworks studied the correlation"
        },
        {
          "Neutral:             0%": "and interplay between different emotions. This contribution"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "aims to ﬁll this research gap."
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "2.4\nSummary of Research Gap"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "We brieﬂy summarize the gaps in the current\nliterature on"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "speech synthesis that we aim to address in this study:"
        },
        {
          "Neutral:             0%": ""
        },
        {
          "Neutral:             0%": "• The synthesis of mixed emotions has not been studied"
        },
        {
          "Neutral:             0%": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "We brieﬂy summarize the gaps in the current\nliterature on"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "speech synthesis that we aim to address in this study:"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "• The synthesis of mixed emotions has not been studied"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "in speech synthesis, which limits the capability of cur-"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "rent systems to imitate human emotions;"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "• Despite much progress\nin psychology,\nit\nis\nstill\nchal-"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "lenging to characterize\nand quantify the mixture of"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "emotions in speech;"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "• Current evaluation methods are inadequate to assess"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "mixed emotional effects. The rethinking of\nthe current"
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "evaluation for mixed emotions is needed."
        },
        {
          "2.4\nSummary of Research Gap": ""
        },
        {
          "2.4\nSummary of Research Gap": "This study is a departure from the current studies on emo-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "Manually Controlled\nText Input"
        },
        {
          "3": "Attribute Vector"
        },
        {
          "3": ""
        },
        {
          "3": "Angry:               0%"
        },
        {
          "3": "Synthesized"
        },
        {
          "3": "Sad:                  0%"
        },
        {
          "3": "Proposed \nEmotional \nSpeech"
        },
        {
          "3": ""
        },
        {
          "3": "Relative Scheme\nText-to-Speech\n(100% Surprise \nNeutral:             0%"
        },
        {
          "3": "+ 30% Happy)"
        },
        {
          "3": "Happy:            30%"
        },
        {
          "3": ""
        },
        {
          "3": "Surprise:       100%\nReference Speech"
        },
        {
          "3": ""
        },
        {
          "3": "(Label: Surprise)"
        },
        {
          "3": ""
        },
        {
          "3": "Fig.\n2: Block diagram of\nour proposed relative\nscheme"
        },
        {
          "3": "applied to emotional text-to-speech at run-time."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "auxiliary features such as a state of voiced, unvoiced and"
        },
        {
          "3": ""
        },
        {
          "3": "silence (VUS) [86], attention weights or a saliency map [87]"
        },
        {
          "3": ""
        },
        {
          "3": "to control\nthe emotion intensity. Other studies manipulate"
        },
        {
          "3": ""
        },
        {
          "3": "the internal emotion representations through interpolation"
        },
        {
          "3": ""
        },
        {
          "3": "[88], scaling [76] or distance-based quantization [89]. In [58],"
        },
        {
          "3": ""
        },
        {
          "3": "[59],\n[60],\n[61],\nrelative attributes are\nintroduced to learn"
        },
        {
          "3": ""
        },
        {
          "3": "a more\ninterpretable\nrepresentation of\nemotion intensity."
        },
        {
          "3": ""
        },
        {
          "3": "However, none of these frameworks studied the correlation"
        },
        {
          "3": "and interplay between different emotions. This contribution"
        },
        {
          "3": ""
        },
        {
          "3": "aims to ﬁll this research gap."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "2.4\nSummary of Research Gap"
        },
        {
          "3": ""
        },
        {
          "3": "We brieﬂy summarize the gaps in the current\nliterature on"
        },
        {
          "3": ""
        },
        {
          "3": "speech synthesis that we aim to address in this study:"
        },
        {
          "3": ""
        },
        {
          "3": "• The synthesis of mixed emotions has not been studied"
        },
        {
          "3": ""
        },
        {
          "3": "in speech synthesis, which limits the capability of cur-"
        },
        {
          "3": ""
        },
        {
          "3": "rent systems to imitate human emotions;"
        },
        {
          "3": ""
        },
        {
          "3": "• Despite much progress\nin psychology,\nit\nis\nstill\nchal-"
        },
        {
          "3": ""
        },
        {
          "3": "lenging to characterize\nand quantify the mixture of"
        },
        {
          "3": ""
        },
        {
          "3": "emotions in speech;"
        },
        {
          "3": ""
        },
        {
          "3": "• Current evaluation methods are inadequate to assess"
        },
        {
          "3": ""
        },
        {
          "3": "mixed emotional effects. The rethinking of\nthe current"
        },
        {
          "3": ""
        },
        {
          "3": "evaluation for mixed emotions is needed."
        },
        {
          "3": ""
        },
        {
          "3": "This study is a departure from the current studies on emo-"
        },
        {
          "3": "tional speech synthesis. We seek to display the possibilities"
        },
        {
          "3": "to synthesize mixed emotions that are subtle but do exist in"
        },
        {
          "3": "our real life."
        },
        {
          "3": "3\nMIXED EMOTION MODELLING AND SYNTHESIS"
        },
        {
          "3": ""
        },
        {
          "3": "We propose a novel relative scheme that allows for manually"
        },
        {
          "3": ""
        },
        {
          "3": "manipulating the synthesized emotion,\ni.e. mixing multiple"
        },
        {
          "3": ""
        },
        {
          "3": "different emotion styles. As shown in Figure 2, the proposed"
        },
        {
          "3": ""
        },
        {
          "3": "scheme allows for ﬂexible control of the extent of each con-"
        },
        {
          "3": ""
        },
        {
          "3": "tributing emotion in the speech. At run-time, the framework"
        },
        {
          "3": ""
        },
        {
          "3": "transfers the reference emotion into a new utterance with the"
        },
        {
          "3": ""
        },
        {
          "3": "text input, also known as emotional text-to-speech."
        },
        {
          "3": ""
        },
        {
          "3": "We ﬁrst describe our method of\ncharacterizing mixed"
        },
        {
          "3": ""
        },
        {
          "3": "emotions\nin\nspeech\nand\nhighlight\nour\ncontributions\nto"
        },
        {
          "3": ""
        },
        {
          "3": "designing a novel\nrelative\nscheme. Then, we present\nthe"
        },
        {
          "3": ""
        },
        {
          "3": "details of\nthe sequence-to-sequence emotion training with"
        },
        {
          "3": ""
        },
        {
          "3": "the proposed relative scheme. Lastly, we show the ﬂexible"
        },
        {
          "3": ""
        },
        {
          "3": "control of\nthe proposed framework for synthesizing mixed"
        },
        {
          "3": ""
        },
        {
          "3": "emotions."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "3.1\nCharacterization of Mixed Emotions in Speech"
        },
        {
          "3": ""
        },
        {
          "3": "Emotion can be characterized with either categorical\n[90],"
        },
        {
          "3": "[91] or dimensional\nrepresentations\n[92],\n[93]. With desig-"
        },
        {
          "3": "nated emotion labels, the emotion category approach is the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ". \nFC": "."
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": "Fig. 4: The training diagram of the proposed framework. The"
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": "pre-trained relative scheme learns to generate an emotion"
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": "attribute vector\nthat measures\nthe\nrelative difference be-"
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": "tween the input emotion style (’Happy’) and other primary"
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": "emotion styles (’Angry’,\n’Sad’,\n’Surprise’ and ’Neutral’)."
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": "We\nstudy a\nrank-based method to measure\nthe\nrela-"
        },
        {
          ". \nFC": ""
        },
        {
          ". \nFC": "tive difference between emotion categories, which can offer"
        },
        {
          ". \nFC": "more informative descriptions and thus be closer to human"
        },
        {
          ". \nFC": "supervision [96].\nIn computer vision,\nthe relative attribute"
        },
        {
          ". \nFC": "[96]\nrepresents an effective way to model\nthe relative dif-"
        },
        {
          ". \nFC": "ference between two categories of data.\nInspired by the"
        },
        {
          ". \nFC": "success\nin various\ncomputer vision tasks\n[97],\n[98],\n[99],"
        },
        {
          ". \nFC": "we believe relative attributes bridge between the low-level"
        },
        {
          ". \nFC": "features and high-level\nsemantic meanings, which allows"
        },
        {
          ". \nFC": "us to model\nthe relative difference between emotions only"
        },
        {
          ". \nFC": "with discrete\nemotion labels.\nIn this way, we\nregard the"
        },
        {
          ". \nFC": "identiﬁable\nemotion style as an attribute of\nspeech data,"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Training Phase": "Fig. 3: The illustration of the proposed relative scheme at (a) training and (b) run-time phase. A relative ranking function is",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "trained between each emotion pair and automatically predicts an emotion attribute at run-time. A smaller emotion attribute",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "value represents a similar emotional style between the pairs. All the emotion attributes form an emotion attribute vector.",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "most straightforward way to represent emotions. However,",
          "(b) Inference Phase": "Linguistic  \nLinguistic Space"
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "Embedding"
        },
        {
          "(a) Training Phase": "such representation ignores\nthe subtle variations of emo-",
          "(b) Inference Phase": "(Text)"
        },
        {
          "(a) Training Phase": "tions. Another approach seeks to model\nthe physical prop-",
          "(b) Inference Phase": "Text"
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "Encoder\nTask"
        },
        {
          "(a) Training Phase": "erties of speech emotion with dimensional representations.",
          "(b) Inference Phase": "Switch\nLinguistic"
        },
        {
          "(a) Training Phase": "An example is Russell’s circumplex model [92], where emo-",
          "(b) Inference Phase": "Embedding"
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "(Acoustic)"
        },
        {
          "(a) Training Phase": "tions are distributed in a two-dimensional\ncircular\nspace,",
          "(b) Inference Phase": "Linguistic"
        },
        {
          "(a) Training Phase": "containing arousal and valence dimensions.",
          "(b) Inference Phase": "Encoder\nEmotion"
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "Classiﬁer"
        },
        {
          "(a) Training Phase": "One of\nthe most\nstraightforward ways\nto characterize",
          "(b) Inference Phase": "Label: Happy"
        },
        {
          "(a) Training Phase": "mixed emotions\nis\nto inject different\nemotion styles\ninto",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "Emotion"
        },
        {
          "(a) Training Phase": "a continuous space. Mixed emotions could be synthesized",
          "(b) Inference Phase": "Emotion Space\nEmbedding"
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "by adjusting each dimension carefully. However, only a",
          "(b) Inference Phase": "Emotion"
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "Encoder"
        },
        {
          "(a) Training Phase": "few emotional\nspeech databases\n[94],\n[95] provide\nsuch",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "annotations. These dimensional annotations are subjective",
          "(b) Inference Phase": "Label: Happy"
        },
        {
          "(a) Training Phase": "and expensive to collect. Therefore, we only utilize discrete",
          "(b) Inference Phase": "Concat\nPre-trained"
        },
        {
          "(a) Training Phase": "emotion labels available in most databases. We ﬁrst make",
          "(b) Inference Phase": "Relative Scheme\nEmotion"
        },
        {
          "(a) Training Phase": "an assumption based on the theory of\nthe emotion wheel",
          "(b) Inference Phase": "Attribute Vector"
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "."
        },
        {
          "(a) Training Phase": "[44]: Mixed emotions\nare\ncharacterized by combinations,",
          "(b) Inference Phase": ". \nFC"
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "."
        },
        {
          "(a) Training Phase": "mixtures, or compounds of primary emotions. While it is not",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "straightforward to add up emotions, we explore the ordinal",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "nature of emotions instead.",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "We propose a rank-based relative scheme to quantify the",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "Fig. 4: The training diagram of the proposed framework. The"
        },
        {
          "(a) Training Phase": "relative difference between speech recordings with different",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "pre-trained relative scheme learns to generate an emotion"
        },
        {
          "(a) Training Phase": "emotion types. Mixed emotions\ncan be\ncharacterized by",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "that measures\nthe"
        },
        {
          "(a) Training Phase": "adjusting the relative difference with other emotion types.",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "tween the input emotion style (’Happy’) and other primary"
        },
        {
          "(a) Training Phase": "The relative difference value can also quantify the level of",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "’Sad’,"
        },
        {
          "(a) Training Phase": "engagement of each emotion. We introduce our design of a",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "novel relative scheme next.",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "study a"
        },
        {
          "(a) Training Phase": "3.2\nDesign of a Novel Relative Scheme",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "",
          "(b) Inference Phase": "tive difference between emotion categories, which can offer"
        },
        {
          "(a) Training Phase": "One of\nthe challenges of\nsynthesizing mixed emotions\nis",
          "(b) Inference Phase": "more informative descriptions and thus be closer to human"
        },
        {
          "(a) Training Phase": "quantifying the association or the interplay between differ-",
          "(b) Inference Phase": "In computer vision,"
        },
        {
          "(a) Training Phase": "ent emotions. Inspired by the ordinal nature of emotions, we",
          "(b) Inference Phase": "represents an effective way to model"
        },
        {
          "(a) Training Phase": "propose a novel relative scheme to address this challenge.",
          "(b) Inference Phase": "ference between two categories of data."
        },
        {
          "(a) Training Phase": "We ﬁrst make two assumptions according to the theory of",
          "(b) Inference Phase": "in various\ncomputer vision tasks"
        },
        {
          "(a) Training Phase": "the\nemotion wheel:\n(1)\nall\nemotions\nare\nrelated to some",
          "(b) Inference Phase": "we believe relative attributes bridge between the low-level"
        },
        {
          "(a) Training Phase": "extent;\n(2)\neach emotion has\nstereotypical\nstyles.\nIn our",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "proposal, we not only characterize\nthe\nidentiﬁable\nstyles",
          "(b) Inference Phase": ""
        },
        {
          "(a) Training Phase": "of\neach emotion but\nalso seek to quantify the\nsimilarity",
          "(b) Inference Phase": "emotion labels."
        },
        {
          "(a) Training Phase": "between different emotion styles.",
          "(b) Inference Phase": "emotion style as an attribute of"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Synthesized"
        },
        {
          "5": ""
        },
        {
          "5": "Speech"
        },
        {
          "5": ""
        },
        {
          "5": "(100% Surprise"
        },
        {
          "5": "+ 30% Happy)"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: , for all the experiments, we",
      "data": [
        {
          "6": "TABLE 1: Our experimental settings of one primary emotion"
        },
        {
          "6": ""
        },
        {
          "6": "(A),\nthree reference emotions\n(B) and the expected mixed"
        },
        {
          "6": ""
        },
        {
          "6": "emotional effects (A+B)."
        },
        {
          "6": ""
        },
        {
          "6": "Primary Emotion (A)\nReference Emotion (B)\nMixed Effects (A+B)"
        },
        {
          "6": ""
        },
        {
          "6": "Surprise\nHappy\nDelight"
        },
        {
          "6": "Surprise\nAngry\nOutrage"
        },
        {
          "6": "Surprise\nSad\nDisappointment"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "an output channel of 512. The decoder has the same model"
        },
        {
          "6": ""
        },
        {
          "6": "architecture as that of Tacotron [12]."
        },
        {
          "6": ""
        },
        {
          "6": "The\ntext\nencoder\nis a 3-layer 1D CNN with a kernel"
        },
        {
          "6": ""
        },
        {
          "6": "size of 5 and a channel number of 512. The text encoder"
        },
        {
          "6": ""
        },
        {
          "6": "is followed by a 1-layer of 256-cell BLSTM and an FC layer"
        },
        {
          "6": ""
        },
        {
          "6": "with an output channel number of 512. The style encoder is"
        },
        {
          "6": ""
        },
        {
          "6": "a 2-layer 128-cell BLSTM followed by an FC layer with an"
        },
        {
          "6": "output channel number of 64. The classiﬁer is a 4-layer FC"
        },
        {
          "6": "with channel numbers of {512, 512, 512, 5}."
        },
        {
          "6": ""
        },
        {
          "6": "4.1.2\nTraining Pipeline"
        },
        {
          "6": ""
        },
        {
          "6": "We ﬁrst pre-train a relative ranking function between each"
        },
        {
          "6": ""
        },
        {
          "6": "emotion pair using an emotional\nspeech dataset. We im-"
        },
        {
          "6": ""
        },
        {
          "6": "plement\nthe relative ranking function following an open-"
        },
        {
          "6": ""
        },
        {
          "6": "source repository2. We use a standardized set of 384 acous-"
        },
        {
          "6": ""
        },
        {
          "6": "tic\nfeatures extracted with openSMILE [112] as\nthe input"
        },
        {
          "6": ""
        },
        {
          "6": "features. These\nfeatures\ninclude zero-crossing rate,\nframe"
        },
        {
          "6": ""
        },
        {
          "6": "energy, pitch frequency, and Mel-frequency cepstral\ncoef-"
        },
        {
          "6": ""
        },
        {
          "6": "ﬁcient\n(MFCC) used in the Interspeech Emotion Challenge"
        },
        {
          "6": ""
        },
        {
          "6": "[113]. The trained ranking functions reported a classiﬁcation"
        },
        {
          "6": ""
        },
        {
          "6": "accuracy of 97% on the test set."
        },
        {
          "6": ""
        },
        {
          "6": "We then conduct a two-stage training strategy to train"
        },
        {
          "6": ""
        },
        {
          "6": "our text-to-speech framework, which consists of\n(1) Multi-"
        },
        {
          "6": ""
        },
        {
          "6": "speaker text-to-speech training with the VCTK Corpus [114]"
        },
        {
          "6": ""
        },
        {
          "6": "and (2) Emotion Adaptation for text-to-speech with a single"
        },
        {
          "6": ""
        },
        {
          "6": "speaker\nfrom the ESD dataset\n[115],\n[116]. The proposed"
        },
        {
          "6": ""
        },
        {
          "6": "text-to-speech framework learns\nabundant\nspeaker\nstyles"
        },
        {
          "6": ""
        },
        {
          "6": "with a multi-speaker corpus and then learns the actual emo-"
        },
        {
          "6": ""
        },
        {
          "6": "tion information with a small amount of emotional speech"
        },
        {
          "6": ""
        },
        {
          "6": "data. The\ntraining strategy we used is\nsimilar\nto that of"
        },
        {
          "6": "[75]. During the training, we use the Adam optimizer [117]"
        },
        {
          "6": "and set the batch size to 64 and 4 for multi-speaker text-to-"
        },
        {
          "6": "speech training and emotion adaptation, respectively. We set"
        },
        {
          "6": ""
        },
        {
          "6": "the learning rate to 0.001 and the weight decay to 0.0001 for"
        },
        {
          "6": ""
        },
        {
          "6": "multi-speaker text-to-speech training. We halve the learning"
        },
        {
          "6": ""
        },
        {
          "6": "rate every seven epochs during the emotion adaptation."
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "4.1.3\nData Preparation"
        },
        {
          "6": ""
        },
        {
          "6": "We select the VCTK Corpus [114] to perform multi-speaker"
        },
        {
          "6": ""
        },
        {
          "6": "text-to-speech training, where we use 99 speakers and the"
        },
        {
          "6": ""
        },
        {
          "6": "total duration of\ntraining speech data is about 30 hours."
        },
        {
          "6": "We select\nthe ESD dataset\n[115],\n[116]\nto perform emotion"
        },
        {
          "6": ""
        },
        {
          "6": "adaptation and relative ranking training. We choose one En-"
        },
        {
          "6": "glish male (’0013’) and one English female (’0019’) speaker"
        },
        {
          "6": "from the ESD. We consider ﬁve emotions: Neutral, Angry,"
        },
        {
          "6": "Happy, Sad and Surprise, and for each emotion, we follow"
        },
        {
          "6": "the data partition given in the ESD. For each speaker and"
        },
        {
          "6": "each emotion, we use 300, 30 and 20 utterances for training,"
        },
        {
          "6": "testing, and evaluation,\nrespectively. The total duration of"
        },
        {
          "6": "emotional speech training data is around 50 minutes."
        },
        {
          "6": "2. https://github.com/chaitanya100100/Relative-Attributes-Zero-"
        },
        {
          "6": "Shot-Learning"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": ""
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": "\u0000\u0013\u0000\u0011\u0000\u001b"
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": ""
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": "\u0000\u0013\u0000\u0011\u0000\u0019"
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": "\u00003\u0000U\u0000R\u0000E\u0000D\u0000E\u0000L\u0000O\u0000L\u0000W\u0000\\"
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": "\u0000\u0013\u0000\u0011\u0000\u0017"
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": "\u0000\u0013\u0000\u0011\u0000\u0015"
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": "\u0000\u0013\u0000\u0011\u0000\u0013"
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": "\u0000\u000e\u0000\u0013\u0000\b"
        },
        {
          "\u0000\u0014\u0000\u0011\u0000\u0013": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "4.2\nObjective Evaluation"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "We ﬁrst perform objective evaluations to validate the pro-"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "posed mixed emotion synthesis. We demonstrate the effec-"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "tiveness of our proposals and provide analysis with a pre-"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "trained speech emotion recognition (SER) model. We calcu-"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "late Mel-cepstral distortion (MCD) and Pearson correlation"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "coefﬁcient (PCC) as objective evaluation metrics."
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "4.2.1\nAnalysis with Speech Emotion Recognition"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "We train a speech emotion recognition model on the ESD"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "dataset\n[115] with the\nsame data partition described in"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "Section 4.1.3. To improve the robustness of SER, data aug-"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "mentation is performed by adding white Gaussian noise"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "during the SER training [118], [119], [120], [121]."
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "The SER architecture is the same as that in [122], which"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "includes:\n1)\na\nthree-dimensional\n(3-D) CNN layer;\n2)\na"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "BLSTM; 3) an attention layer; and 4) a fully connected (FC)"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "layer. We evaluate our\nsynthesized mixed emotions with"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "the pre-trained SER. We use the classiﬁcation probabilities"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "derived from the softmax layer of\nthe SER to analyze the"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "effects\nof mixed emotions. As\na\nhigh-level\nfeature,\nthe"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "classiﬁcation probabilities\nsummarize\nthe useful\nemotion"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": ""
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "information from the previous\nlayers\nfor ﬁnal decision-"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "making. The classiﬁcation probabilities offer us an effective"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "tool\nto justify how well\neach emotional\ncomponent\ncan"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "be perceptually recognized by the SER from the emotion"
        },
        {
          "dataset. Each point represents an averaged probability value of 20 utterances with mixed emotions.": "mixture."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": ""
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": "\u0000\u0017\u0000\u0011\u0000\u001b"
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": ""
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": "\u0000\u0017\u0000\u0011\u0000\u0019"
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": "\u0000\u0017\u0000\u0011\u0000\u0017"
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": "\u00000\u0000&\u0000'\u0000\u0003\u0000>\u0000G\u0000%\u0000@\n\u0000\u0017\u0000\u0011\u0000\u0015"
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": "\u0000\u0017\u0000\u0011\u0000\u0013"
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": "\u0000\u0016\u0000\u0011\u0000\u001b"
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": "\u0000\u0016\u0000\u0011\u0000\u0019"
        },
        {
          "\u0000\u0018\u0000\u0011\u0000\u0013": "\u0000\u000e\u0000\u0013\u0000\b"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0016\u0000\u0011\u0000\u001b": "\u0000\u0016\u0000\u0011\u0000\u0019"
        },
        {
          "\u0000\u0016\u0000\u0011\u0000\u001b": "\u0000\u000e\u0000\u0013\u0000\b\n\u0000\u000e\u0000\u0016\u0000\u0013\u0000\b\n\u0000\u000e\u0000\u0019\u0000\u0013\u0000\b"
        },
        {
          "\u0000\u0016\u0000\u0011\u0000\u001b": "\u00003\u0000H\u0000U\u0000F\u0000H\u0000Q\u0000W\u0000D\u0000J\u0000H\u0000\u0003\u0000R\u0000I\u0000\u0003\u00006\u0000D\u0000G"
        },
        {
          "\u0000\u0016\u0000\u0011\u0000\u001b": ""
        },
        {
          "\u0000\u0016\u0000\u0011\u0000\u001b": "ages of Sad (0%, 30%, 60%, 90%)"
        },
        {
          "\u0000\u0016\u0000\u0011\u0000\u001b": ""
        },
        {
          "\u0000\u0016\u0000\u0011\u0000\u001b": ""
        },
        {
          "\u0000\u0016\u0000\u0011\u0000\u001b": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "emotions."
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "to measure\nthe\nspectrum similarity, which is\ncalculated"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "between the synthesized (ˆy = {ˆym}) and the target MCEPs"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "(y = {ym}):"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "√"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "(cid:118)(cid:117)(cid:117)(cid:116)\n2\n10"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "1 M\nM(cid:88) m\n(8)\nMCD [dB] =\n(ym − ˆym)2,"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "ln 10"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "=1"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": ""
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "where M represents the dimension of the MCEPs. A lower"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": ""
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "value of MCD indicates a higher degree of in the spectrum."
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": ""
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "4.2.3\nPearson Correlation Coefﬁcient"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "Pitch is\nconsidered a major prosodic\nfactor\ncontributing"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "to speech emotion,\nclosely correlated to the activity level"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "[125],\n[126].\nIn practice,\nthe pitch is often represented by"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "the\nfundamental\nfrequency (F0), which can be\nestimated"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "with the harvest algorithm [127]. We calculate the Pearson"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "Correlation Coefﬁcient (PCC) of F0 to measure the linear de-"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "pendency between two F0 sequences, which has been used"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "in previous studies [128], [129], [130]. The PCC between two"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "F0 sequences is given as:"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": ""
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "cov(F s\n0 , F t\n0)"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "ρ(F s\n,\n(9)\n0 , F t\n0) ="
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "σF s\nσF t\n0"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "0"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "where cov(·)\nrepresents\nand\nthe covariance function, σF s\n0"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "are\nthe\nstandard deviations\nof\nthe\nsynthesized se-\nσF t"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "0"
        },
        {
          "the reference emotions (Angry, Happy and Sad). Each point represents an averaged PCC value of 20 utterances with mixed": "quences (F s\n0 ) and the target F0 sequences (F t\n0), respectively."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Best-worst scaling (BWS) test results to evaluate",
      "data": [
        {
          "+ 30% Angry": "+ 60% Angry",
          "3.79 ± 0.37": "3.81 ± 0.35",
          "with Angry": "",
          "+ 60% Angry": "+ 90% Angry",
          "24.8": "60.9",
          "11.3": "7.5"
        },
        {
          "+ 30% Angry": "+ 90% Angry",
          "3.79 ± 0.37": "3.76 ± 0.35",
          "with Angry": "",
          "+ 60% Angry": "",
          "24.8": "",
          "11.3": ""
        },
        {
          "+ 30% Angry": "",
          "3.79 ± 0.37": "",
          "with Angry": "",
          "+ 60% Angry": "(b) Perception of Happy",
          "24.8": "",
          "11.3": ""
        },
        {
          "+ 30% Angry": "Ground truth (Happy)",
          "3.79 ± 0.37": "4.93 ± 0.05",
          "with Angry": "",
          "+ 60% Angry": "",
          "24.8": "",
          "11.3": ""
        },
        {
          "+ 30% Angry": "+ 0% Happy",
          "3.79 ± 0.37": "3.21 ± 0.41",
          "with Angry": "",
          "+ 60% Angry": "",
          "24.8": "",
          "11.3": ""
        },
        {
          "+ 30% Angry": "",
          "3.79 ± 0.37": "",
          "with Angry": "Conﬁguration",
          "+ 60% Angry": "",
          "24.8": "Best (%)",
          "11.3": "Worst (%)"
        },
        {
          "+ 30% Angry": "+ 30% Happy",
          "3.79 ± 0.37": "3.36 ± 0.36",
          "with Angry": "",
          "+ 60% Angry": "",
          "24.8": "",
          "11.3": ""
        },
        {
          "+ 30% Angry": "+ 60% Happy",
          "3.79 ± 0.37": "3.39 ± 0.39",
          "with Angry": "",
          "+ 60% Angry": "+ 0% Happy",
          "24.8": "8.3",
          "11.3": "44.4"
        },
        {
          "+ 30% Angry": "+ 90% Happy",
          "3.79 ± 0.37": "3.52 ± 0.42",
          "with Angry": "Mixing Surprise (100%)",
          "+ 60% Angry": "+ 30% Happy",
          "24.8": "24.0",
          "11.3": "25.6"
        },
        {
          "+ 30% Angry": "",
          "3.79 ± 0.37": "",
          "with Angry": "with Happy",
          "+ 60% Angry": "+ 60% Happy",
          "24.8": "27.1",
          "11.3": "11.2"
        },
        {
          "+ 30% Angry": "Ground truth (Sad)",
          "3.79 ± 0.37": "4.84 ± 0.15",
          "with Angry": "",
          "+ 60% Angry": "",
          "24.8": "",
          "11.3": ""
        },
        {
          "+ 30% Angry": "",
          "3.79 ± 0.37": "",
          "with Angry": "",
          "+ 60% Angry": "+ 90% Happy",
          "24.8": "40.6",
          "11.3": "18.8"
        },
        {
          "+ 30% Angry": "+ 0% Sad",
          "3.79 ± 0.37": "3.64 ± 0.35",
          "with Angry": "",
          "+ 60% Angry": "",
          "24.8": "",
          "11.3": ""
        },
        {
          "+ 30% Angry": "+ 30% Sad",
          "3.79 ± 0.37": "3.73 ± 0.32",
          "with Angry": "",
          "+ 60% Angry": "",
          "24.8": "",
          "11.3": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Best-worst scaling (BWS) test results to evaluate",
      "data": [
        {
          "interval to evaluate the speech quality of synthesized mixed": "emotions.",
          "the perception of the reference emotions (Angry, Happy, and": "Sad) in synthesized mixed emotions."
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "(a) Perception of Angry"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "Conﬁguration\nMOS",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "Ground truth (Surprise)\n4.83 ± 0.16",
          "the perception of the reference emotions (Angry, Happy, and": "Conﬁguration\nBest (%)\nWorst (%)"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "Ground truth (Angry)\n4.81 ± 0.19",
          "the perception of the reference emotions (Angry, Happy, and": "+ 0% Angry\n8.3\n61.7"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "Mixing Surprise (100%)\n+ 0% Angry\n3.51 ± 0.36",
          "the perception of the reference emotions (Angry, Happy, and": "Mixing Surprise (100%)\n+ 30% Angry\n6.0\n19.5"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "+ 30% Angry\n3.79 ± 0.37",
          "the perception of the reference emotions (Angry, Happy, and": "with Angry\n+ 60% Angry\n24.8\n11.3"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "with Angry\n+ 60% Angry\n3.81 ± 0.35",
          "the perception of the reference emotions (Angry, Happy, and": "60.9\n7.5\n+ 90% Angry"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "+ 90% Angry\n3.76 ± 0.35",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "(b) Perception of Happy"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "Ground truth (Happy)\n4.93 ± 0.05",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "Mixing Surprise (100%)\n+ 0% Happy\n3.21 ± 0.41",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "Conﬁguration\nBest (%)\nWorst (%)"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "+ 30% Happy\n3.36 ± 0.36",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "with Happy\n+ 60% Happy\n3.39 ± 0.39",
          "the perception of the reference emotions (Angry, Happy, and": "+ 0% Happy\n8.3\n44.4"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "+ 90% Happy\n3.52 ± 0.42",
          "the perception of the reference emotions (Angry, Happy, and": "Mixing Surprise (100%)\n+ 30% Happy\n24.0\n25.6"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "11.2\nwith Happy\n+ 60% Happy\n27.1"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "Ground truth (Sad)\n4.84 ± 0.15",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "40.6\n+ 90% Happy\n18.8"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "Mixing Surprise (100%)\n+ 0% Sad\n3.64 ± 0.35",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "+ 30% Sad\n3.73 ± 0.32",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "with Sad\n+ 60% Sad\n3.74 ± 0.31",
          "the perception of the reference emotions (Angry, Happy, and": "(c) Perception of Sad"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "+ 90% Sad\n3.60 ± 0.38",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "Conﬁguration\nBest (%)\nWorst (%)"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "+ 0% Sad\n9.0\n57.1"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "Mixing Surprise (100%)\n+ 30% Sad\n9.0\n29.3"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "synthesized mixed emotions have\na\nstronger\ncorrelation",
          "the perception of the reference emotions (Angry, Happy, and": ""
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "",
          "the perception of the reference emotions (Angry, Happy, and": "3.8\nwith Sad\n+ 60% Sad\n20.3"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "with the reference emotions (Angry, Happy and Sad) in terms",
          "the perception of the reference emotions (Angry, Happy, and": "61.7\n+ 90% Sad\n9.8"
        },
        {
          "interval to evaluate the speech quality of synthesized mixed": "of\nthe prosody variance. These results\nshow that we can",
          "the perception of the reference emotions (Angry, Happy, and": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: a, 4b and 4c. As for the 0.2",
      "data": [
        {
          "10": "Proposed w/o Relative Scheme\nNo Difference\nProposed w/ Relative Scheme"
        },
        {
          "10": ""
        },
        {
          "10": "*\n*\n*\n*\n*\n*"
        },
        {
          "10": "0.7"
        },
        {
          "10": ""
        },
        {
          "10": "0.6"
        },
        {
          "10": "0.5"
        },
        {
          "10": "Preference Percentage\n0.4"
        },
        {
          "10": ""
        },
        {
          "10": "0.3"
        },
        {
          "10": ""
        },
        {
          "10": "0.2"
        },
        {
          "10": ""
        },
        {
          "10": "0.1"
        },
        {
          "10": ""
        },
        {
          "10": "0"
        },
        {
          "10": "Angry\nHappy\nSad\nSurprise"
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "Fig. 10: XAB preference test\nresults with 95% conﬁdence"
        },
        {
          "10": "interval to evaluate the emotion similarity with the ground"
        },
        {
          "10": ""
        },
        {
          "10": "truth emotions. The marker * indicates p < 0.05 for paired"
        },
        {
          "10": "t-test scores (pairs between “Proposed w/ Relative Scheme”"
        },
        {
          "10": "and the others)."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "feelings to the audience through both actions and speech."
        },
        {
          "10": ""
        },
        {
          "10": "With our proposed methods, we are able to synthesize such"
        },
        {
          "10": ""
        },
        {
          "10": "mixed feelings of the oppositely valenced emotions such as"
        },
        {
          "10": ""
        },
        {
          "10": "Happy and Sad. Readers are suggested to refer to the demo"
        },
        {
          "10": ""
        },
        {
          "10": "page."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "5.2\nAn Emotion Transition System"
        },
        {
          "10": ""
        },
        {
          "10": "One potential\napplication of mixed emotion synthesis\nis"
        },
        {
          "10": ""
        },
        {
          "10": "building an emotion transition system [133]. Emotion transi-"
        },
        {
          "10": ""
        },
        {
          "10": "tion aims to gradually transition the emotion state from one"
        },
        {
          "10": ""
        },
        {
          "10": "to another. One similar study is emotional voice conversion"
        },
        {
          "10": ""
        },
        {
          "10": "[116], which aims to convert the emotional state. Compared"
        },
        {
          "10": ""
        },
        {
          "10": "with emotional voice conversion, the key challenge of emo-"
        },
        {
          "10": ""
        },
        {
          "10": "tion transition is to synthesize internal states between differ-"
        },
        {
          "10": ""
        },
        {
          "10": "ent emotion types. With our proposed methods, we are able"
        },
        {
          "10": "to model these internal states by mixing them with different"
        },
        {
          "10": "emotions. To achieve this, the sum up of the percentages of"
        },
        {
          "10": "each emotion needs to be 100% (e. g., 80% Surprise with 20%"
        },
        {
          "10": ""
        },
        {
          "10": "Angry; 40% Happy with 60% Sad). Then, we can synthesize"
        },
        {
          "10": ""
        },
        {
          "10": "various internal emotion states by adjusting the percentages."
        },
        {
          "10": ""
        },
        {
          "10": "Compared with traditional methods such as interpolation,"
        },
        {
          "10": ""
        },
        {
          "10": "our proposed system is data-driven, and the synthesized"
        },
        {
          "10": ""
        },
        {
          "10": "emotions are more natural."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "5.3\nDiscussion"
        },
        {
          "10": ""
        },
        {
          "10": "This\nstudy serves as\nthe ﬁrst attempt\nto model and syn-"
        },
        {
          "10": "thesize mixed emotions for speech synthesis. Although we"
        },
        {
          "10": "have shown the effectiveness of our methods,\nthe related"
        },
        {
          "10": "problems have not been completely solved. We provide a"
        },
        {
          "10": ""
        },
        {
          "10": "discussion to address the concerns, show our ﬁndings, and"
        },
        {
          "10": ""
        },
        {
          "10": "inspire future studies."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "5.3.1\nCategory vs. Dimensional Emotion Models"
        },
        {
          "10": ""
        },
        {
          "10": "Our\nassumptions,\nformulation,\nand evaluation of mixed"
        },
        {
          "10": "emotions are all based on categorical emotion studies. We"
        },
        {
          "10": "note that mixed emotions\ncan also be modelled with di-"
        },
        {
          "10": "mensional\nrepresentations\nsuch as\narousal,\nvalence,\nand"
        },
        {
          "10": "dominance. A dimensional model can capture a wide range"
        },
        {
          "10": "of emotional concepts, which offers a means of measuring"
        },
        {
          "10": "the similarity of different emotional states [134]. However,"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dimensional representations comes from the lack of\nlabels.": "Besides, humans are more efﬁcient at discriminating among",
          "bittersweet feeling and an emotion triangle. The inves-": "tigation study serves as an additional contribution to"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "options\nthan giving an absolute\nscore\n[135], which adds",
          "bittersweet feeling and an emotion triangle. The inves-": "the article, which could broaden the scope of the study."
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "challenges to the evaluation process. Furthermore, dimen-",
          "bittersweet feeling and an emotion triangle. The inves-": "In this article, we only focused on studying mixed emo-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "sional models are restricted to modelling the co-occurrence",
          "bittersweet feeling and an emotion triangle. The inves-": "tions for emotional text-to-speech. We believe that our pro-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "of\nlike-valenced discrete emotions [136]. For these reasons,",
          "bittersweet feeling and an emotion triangle. The inves-": "posed relative scheme could enable mixed emotion synthe-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "we refrain from applying dimensional emotions to the cur-",
          "bittersweet feeling and an emotion triangle. The inves-": "sis in most existing emotional speech synthesis frameworks,"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "rent framework.",
          "bittersweet feeling and an emotion triangle. The inves-": "including but not\nlimited to emotional\ntext-to-speech. We"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "will\nexpand our\nexperiments\nto include\nemotional voice"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "5.3.2\nRemaining Challenges",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "conversion in our future studies."
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "There are a few remaining challenges\nthat need attention",
          "bittersweet feeling and an emotion triangle. The inves-": "The future work includes: 1) a comparison with other"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "from the\ncommunity. As mentioned in Section 4.3.2,\nin-",
          "bittersweet feeling and an emotion triangle. The inves-": "ranking methods such as metric learning [138] and Siamese"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "creasing the percentage of adding emotions may result\nin",
          "bittersweet feeling and an emotion triangle. The inves-": "neural networks [137]; 2) conducting experiments for more"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "unnatural emotional expressions. If the synthesized emotion",
          "bittersweet feeling and an emotion triangle. The inves-": "emotion combinations, speakers, and other languages. Our"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "sounds unnatural or is difﬁcult to understand, it may not be",
          "bittersweet feeling and an emotion triangle. The inves-": "future directions\nalso\ninclude\nthe\nstudy\nof\ncross-lingual"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "effective in achieving the desired outcome. Additionally, the",
          "bittersweet feeling and an emotion triangle. The inves-": "emotion style modeling and transfer. Besides, a closer look"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "human voice is a complex and highly variable instrument,",
          "bittersweet feeling and an emotion triangle. The inves-": "at linguistic prosody for emotional speech synthesis is fore-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "and different people can produce the same emotional state",
          "bittersweet feeling and an emotion triangle. The inves-": "seen;\nfor example, different\nsemantic meanings can affect"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "in very different ways. This can make it difﬁcult\nto accu-",
          "bittersweet feeling and an emotion triangle. The inves-": "the way of expressing an emotion."
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "rately capture and reproduce a desired mix of emotions. At",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "last, human raters are asked to evaluate the mixed emotions",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "ACKNOWLEDGMENTS"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "totally based on their personal experiences because of\nthe",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "lack\nof\n“ground truth”\nemotions. People\nfrom different",
          "bittersweet feeling and an emotion triangle. The inves-": "The\nresearch by Kun Zhou is\nsupported by the\nScience"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "cultures may have different experiences and backgrounds",
          "bittersweet feeling and an emotion triangle. The inves-": "and\nEngineering Research Council, Agency\nof\nScience,"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "that\ncan inﬂuence their emotional\nresponses, and having",
          "bittersweet feeling and an emotion triangle. The inves-": "Technology\nand Research (A*STAR),\nSingapore,\nthrough"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "a diverse group of\nevaluators\ncan provide\na more well-",
          "bittersweet feeling and an emotion triangle. The inves-": "the National Robotics Program under Human-Robot\nInter-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "rounded perspective on the synthesized emotions.",
          "bittersweet feeling and an emotion triangle. The inves-": "action Phase\n1\n(Grant No.\n192\n25\n00054); Human-Robot"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "Collaborative AI under\nits AME Programmatic Funding"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "5.3.3\nPotential\nImprovements",
          "bittersweet feeling and an emotion triangle. The inves-": "Scheme\n(Project No. A18A2b0046);\nand the National Re-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "search Foundation Singapore under\nits AI Singapore Pro-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "We discuss several potential improvements to inspire future",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "gramme (Award Grant No. AISG-100E-2018-006). The work"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "studies on mixed emotion synthesis: 1) Selection of\nrank-",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "by Haizhou Li\nis supported by the Guangdong Provincial"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "ing functions: adopt deep learning-based ranking methods",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "Key Laboratory of Big Data Computing, The Chinese Uni-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "[137]\nto\nimprove\nthe performance\nof\nranking;\n2) Multi-",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "versity of Hong Kong, Shenzhen (Grant No. B10120210117-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "speaker\nstudies:\nadd training data\nfrom multiple\nspeak-",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "KP02),\nthe Research Foundation of Guangdong Province"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "ers; 3) Non-autoregressive backbone frameworks: use non-",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "(Grant No. 2019A050505001) and the National Natural Sci-"
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "autoregressive TTS framework as the backbone to avoid the",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "",
          "bittersweet feeling and an emotion triangle. The inves-": "ence Foundation of China (Grant No. 62271432)."
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "misalignment of attention and improve the naturalness of",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        },
        {
          "dimensional representations comes from the lack of\nlabels.": "synthesized speech.",
          "bittersweet feeling and an emotion triangle. The inves-": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "11": "showed the effectiveness of our proposed framework in"
        },
        {
          "11": "terms of synthesizing mixed emotions;"
        },
        {
          "11": "3) We present\nfurther\ninvestigations\non synthesising\na"
        },
        {
          "11": "bittersweet feeling and an emotion triangle. The inves-"
        },
        {
          "11": "tigation study serves as an additional contribution to"
        },
        {
          "11": "the article, which could broaden the scope of the study."
        },
        {
          "11": "In this article, we only focused on studying mixed emo-"
        },
        {
          "11": "tions for emotional text-to-speech. We believe that our pro-"
        },
        {
          "11": "posed relative scheme could enable mixed emotion synthe-"
        },
        {
          "11": "sis in most existing emotional speech synthesis frameworks,"
        },
        {
          "11": "including but not\nlimited to emotional\ntext-to-speech. We"
        },
        {
          "11": "will\nexpand our\nexperiments\nto include\nemotional voice"
        },
        {
          "11": ""
        },
        {
          "11": "conversion in our future studies."
        },
        {
          "11": "The future work includes: 1) a comparison with other"
        },
        {
          "11": "ranking methods such as metric learning [138] and Siamese"
        },
        {
          "11": "neural networks [137]; 2) conducting experiments for more"
        },
        {
          "11": "emotion combinations, speakers, and other languages. Our"
        },
        {
          "11": "future directions\nalso\ninclude\nthe\nstudy\nof\ncross-lingual"
        },
        {
          "11": "emotion style modeling and transfer. Besides, a closer look"
        },
        {
          "11": "at linguistic prosody for emotional speech synthesis is fore-"
        },
        {
          "11": "seen;\nfor example, different\nsemantic meanings can affect"
        },
        {
          "11": "the way of expressing an emotion."
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": "ACKNOWLEDGMENTS"
        },
        {
          "11": ""
        },
        {
          "11": "The\nresearch by Kun Zhou is\nsupported by the\nScience"
        },
        {
          "11": "and\nEngineering Research Council, Agency\nof\nScience,"
        },
        {
          "11": "Technology\nand Research (A*STAR),\nSingapore,\nthrough"
        },
        {
          "11": "the National Robotics Program under Human-Robot\nInter-"
        },
        {
          "11": "action Phase\n1\n(Grant No.\n192\n25\n00054); Human-Robot"
        },
        {
          "11": "Collaborative AI under\nits AME Programmatic Funding"
        },
        {
          "11": "Scheme\n(Project No. A18A2b0046);\nand the National Re-"
        },
        {
          "11": "search Foundation Singapore under\nits AI Singapore Pro-"
        },
        {
          "11": ""
        },
        {
          "11": "gramme (Award Grant No. AISG-100E-2018-006). The work"
        },
        {
          "11": ""
        },
        {
          "11": "by Haizhou Li\nis supported by the Guangdong Provincial"
        },
        {
          "11": ""
        },
        {
          "11": "Key Laboratory of Big Data Computing, The Chinese Uni-"
        },
        {
          "11": ""
        },
        {
          "11": "versity of Hong Kong, Shenzhen (Grant No. B10120210117-"
        },
        {
          "11": ""
        },
        {
          "11": "KP02),\nthe Research Foundation of Guangdong Province"
        },
        {
          "11": ""
        },
        {
          "11": "(Grant No. 2019A050505001) and the National Natural Sci-"
        },
        {
          "11": ""
        },
        {
          "11": "ence Foundation of China (Grant No. 62271432)."
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "posed framework, we are able to synthesize mixed emotions": "and further control the rendering of mixed emotions at run-"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "time. The key highlights are as follows:"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "1) We proposed a novel\nrelative scheme to measure the"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "difference between each emotion pair. We demonstrate"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "that our proposed relative scheme enables the effective"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "synthesis and control of\nthe rendering of mixed emo-"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "tions. Through ablation studies, we also show that\nthe"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "proposed relative scheme improves emotional intelligi-"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "bility in synthesized speech;"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "2) We presented a comprehensive study to evaluate mixed"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "emotions\nfor\nthe ﬁrst\ntime. Through both objective"
        },
        {
          "posed framework, we are able to synthesize mixed emotions": "and subjective evaluations, we validated our idea and"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "R. Aihara, R. Takashima, T. Takiguchi,\nand Y. Ariki,\n“Gmm-"
        },
        {
          "12": "based emotional voice conversion using spectrum and prosody"
        },
        {
          "12": "features,” American Journal of Signal Processing, 2012."
        },
        {
          "12": "H. Kawanami, Y. Iwami, T. Toda, H. Saruwatari, and K. Shikano,"
        },
        {
          "12": "“Gmm-based voice conversion applied to emotional speech syn-"
        },
        {
          "12": "thesis,” 2003."
        },
        {
          "12": "J.\nLorenzo-Trueba,\nG.\nE. Henter,\nS.\nTakaki,\nJ.\nYamagishi,"
        },
        {
          "12": "Y. Morino, and Y. Ochiai, “Investigating different representations"
        },
        {
          "12": "for modeling and controlling multiple emotions\nin dnn-based"
        },
        {
          "12": "speech synthesis,” Speech Communication, 2018."
        },
        {
          "12": "Z. Luo,\nJ. Chen, T. Takiguchi, and Y. Ariki, “Emotional voice"
        },
        {
          "12": "conversion with adaptive scales f0 based on wavelet\ntransform"
        },
        {
          "12": "using limited amount of\nemotional data.”\nin INTERSPEECH,"
        },
        {
          "12": "2017, pp. 3399–3403."
        },
        {
          "12": "H. Ming, D. Huang, L. Xie,\nJ. Wu, M. Dong, and H. Li, “Deep"
        },
        {
          "12": "bidirectional lstm modeling of timbre and prosody for emotional"
        },
        {
          "12": "voice conversion,” Interspeech 2016, pp. 2453–2457, 2016."
        },
        {
          "12": "S. An, Z. Ling,\nand L. Dai,\n“Emotional\nstatistical parametric"
        },
        {
          "12": "speech synthesis using lstm-rnns,” in 2017 Asia-Paciﬁc Signal and"
        },
        {
          "12": "Information Processing Association Annual Summit and Conference"
        },
        {
          "12": "(APSIPA ASC).\nIEEE, 2017, pp. 1613–1616."
        },
        {
          "12": "R.\nSkerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D.\nStanton,"
        },
        {
          "12": "J. Shor, R. Weiss, R. Clark, and R. A. Saurous, “Towards end-"
        },
        {
          "12": "to-end prosody transfer\nfor\nexpressive\nspeech synthesis with"
        },
        {
          "12": "tacotron,” in International Conference on Machine Learning, 2018,"
        },
        {
          "12": "pp. 4693–4702."
        },
        {
          "12": "P\n. Wu, Z. Ling, L. Liu, Y.\nJiang, H. Wu, and L. Dai, “End-to-"
        },
        {
          "12": "end emotional\nspeech synthesis using style\ntokens and semi-"
        },
        {
          "12": "supervised training,” in 2019 Asia-Paciﬁc Signal and Information"
        },
        {
          "12": "Processing Association Annual\nSummit\nand Conference\n(APSIPA"
        },
        {
          "12": "ASC).\nIEEE, 2019, pp. 623–627."
        },
        {
          "12": "Y. Lee, S.-Y. Lee, and A. Rabiee, “Emotional end-to-end neural"
        },
        {
          "12": "speech synthesizer,” in NIPS2017.\nNeural Information Process-"
        },
        {
          "12": "ing Systems Foundation, 2017."
        },
        {
          "12": "N. Tits, K. El Haddad, and T. Dutoit, “Exploring transfer learning"
        },
        {
          "12": "Intelligent\nfor low resource emotional\ntts,” in Proceedings of SAI"
        },
        {
          "12": "Systems Conference.\nSpringer, 2019, pp. 52–60."
        },
        {
          "12": "Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor,"
        },
        {
          "12": "Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, “Style tokens: Unsuper-"
        },
        {
          "12": "vised style modeling, control and transfer in end-to-end speech"
        },
        {
          "12": "synthesis,” in International Conference on Machine Learning, 2018,"
        },
        {
          "12": "pp. 5180–5189."
        },
        {
          "12": "D. Stanton, Y. Wang, and R. Skerry-Ryan, “Predicting expressive"
        },
        {
          "12": "speaking style from text in end-to-end speech synthesis,” in 2018"
        },
        {
          "12": "IEEE Spoken Language Technology Workshop (SLT).\nIEEE, 2018, pp."
        },
        {
          "12": "595–602."
        },
        {
          "12": "S. D. Kreibig and J.\nJ. Gross, “Understanding mixed emotions:"
        },
        {
          "12": "paradigms and measures,” Current opinion in behavioral sciences,"
        },
        {
          "12": "vol. 15, pp. 62–71, 2017."
        },
        {
          "12": "R. Berrios, P. Totterdell, and S. Kellett, “Eliciting mixed emotions:"
        },
        {
          "12": "a meta-analysis comparing models,\ntypes, and measures,” Fron-"
        },
        {
          "12": "tiers in psychology, vol. 6, p. 428, 2015."
        },
        {
          "12": "P\n. M. Niedenthal and M. Brauer, “Social\nfunctionality of human"
        },
        {
          "12": "emotion,” Annual review of psychology, vol. 63, pp. 259–285, 2012."
        },
        {
          "12": "P\n. C. Hogan, The mind and its stories: Narrative universals and human"
        },
        {
          "12": "emotion.\nCambridge University Press, 2003."
        },
        {
          "12": "P\n. E. Ekman and R. J. Davidson, The nature of emotion: Fundamental"
        },
        {
          "12": "questions.\nOxford University Press, 1994."
        },
        {
          "12": "R. Plutchik,\n“The nature of\nemotions: Human emotions have"
        },
        {
          "12": "deep evolutionary roots, a fact that may explain their complexity"
        },
        {
          "12": "and provide tools for clinical practice,” American scientist, vol. 89,"
        },
        {
          "12": "no. 4, pp. 344–350, 2001."
        },
        {
          "12": "R. Plutchik and H. Kellerman, Theories of\nemotion.\nAcademic"
        },
        {
          "12": "Press, 2013, vol. 1."
        },
        {
          "12": "M. Cross and C. Hanrahan, Changing Minds: The Go-to Guide to"
        },
        {
          "12": "Mental Health for Family and Friends.\nHarperCollins Australia,"
        },
        {
          "12": "2016."
        },
        {
          "12": "G. N. Yannakakis, R. Cowie, and C. Busso, “The ordinal nature"
        },
        {
          "12": "of emotions,” in 2017 Seventh International Conference on Affective"
        },
        {
          "12": "Computing and Intelligent Interaction (ACII).\nIEEE, 2017, pp. 248–"
        },
        {
          "12": "255."
        },
        {
          "12": "——, “The ordinal nature of emotions: An emerging approach,”"
        },
        {
          "12": "IEEE Transactions on Affective Computing, vol. 12, no. 1, pp. 16–35,"
        },
        {
          "12": "2018."
        },
        {
          "12": "J.\nB. Harvill,\nS.-G.\nLeem, M. Abdelwahab, R.\nLotﬁan,\nand"
        },
        {
          "12": "IEEE\nC. Busso,\n“Quantifying emotional\nsimilarity in speech,”"
        },
        {
          "12": "Transactions on Affective Computing, 2021."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13": "S. Ma, D. Mcduff,\nand Y.\nSong,\n“Neural\ntts\nstylization with"
        },
        {
          "13": "adversarial and collaborative games,” in International Conference"
        },
        {
          "13": "on Learning Representations, 2018."
        },
        {
          "13": "T. Cornille,\nF. Wang,\nand\nJ.\nBekker,\n“Interactive multi-level"
        },
        {
          "13": "prosody\ncontrol\nfor\nexpressive\nspeech synthesis,”\nin ICASSP"
        },
        {
          "13": "2022-2022 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "13": "Signal Processing (ICASSP).\nIEEE, 2022, pp. 8312–8316."
        },
        {
          "13": "V\n. Klimkov,\nS. Ronanki,\nJ. Rohnke,\nand T. Drugman,\n“Fine-"
        },
        {
          "13": "Grained Robust Prosody Transfer for Single-Speaker Neural Text-"
        },
        {
          "13": "To-Speech,” in Proc. Interspeech 2019, 2019, pp. 4440–4444."
        },
        {
          "13": "X. Li, C. Song, J. Li, Z. Wu, J. Jia, and H. Meng, “Towards Multi-"
        },
        {
          "13": "Scale Style Control\nfor Expressive Speech Synthesis,” in Proc."
        },
        {
          "13": "Interspeech 2021, 2021, pp. 4673–4677."
        },
        {
          "13": "G. Zhang, Y. Qin, and T. Lee, “Learning syllable-level discrete"
        },
        {
          "13": "prosodic\nrepresentation for\nexpressive\nspeech generation.”\nin"
        },
        {
          "13": "INTERSPEECH, 2020, pp. 3426–3430."
        },
        {
          "13": "K. Zhou, B. Sisman, and H. Li, “Limited Data Emotional Voice"
        },
        {
          "13": "Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-"
        },
        {
          "13": "Sequence Training,” in Proc. Interspeech 2021, 2021, pp. 811–815."
        },
        {
          "13": "H. Choi and M. Hahn, “Sequence-to-sequence emotional voice"
        },
        {
          "13": "conversion with strength control,” IEEE Access, vol. 9, pp. 42 674–"
        },
        {
          "13": "42 687, 2021."
        },
        {
          "13": "S. Mozziconacci, “Prosody and emotions,” in Speech Prosody 2002,"
        },
        {
          "13": ""
        },
        {
          "13": "International Conference, 2002."
        },
        {
          "13": ""
        },
        {
          "13": "Y. Lee and T. Kim, “Robust and ﬁne-grained prosody control"
        },
        {
          "13": ""
        },
        {
          "13": "of end-to-end speech synthesis,” in ICASSP 2019-2019 IEEE In-"
        },
        {
          "13": ""
        },
        {
          "13": "ternational Conference\non Acoustics, Speech and Signal Processing"
        },
        {
          "13": ""
        },
        {
          "13": "(ICASSP).\nIEEE, 2019, pp. 5911–5915."
        },
        {
          "13": ""
        },
        {
          "13": "D. Tan and T. Lee, “Fine-Grained Style Modeling, Transfer and"
        },
        {
          "13": ""
        },
        {
          "13": "Prediction in Text-to-Speech Synthesis via Phone-Level Content-"
        },
        {
          "13": ""
        },
        {
          "13": "Style Disentanglement,” in Proc. Interspeech 2021, 2021, pp. 4683–"
        },
        {
          "13": ""
        },
        {
          "13": "4687."
        },
        {
          "13": ""
        },
        {
          "13": "G. Sun, Y. Zhang, R. J. Weiss, Y. Cao, H. Zen, and Y. Wu, “Fully-"
        },
        {
          "13": ""
        },
        {
          "13": "hierarchical\nﬁne-grained\nprosody modeling\nfor\ninterpretable"
        },
        {
          "13": ""
        },
        {
          "13": "speech synthesis,” in ICASSP 2020-2020 IEEE International Con-"
        },
        {
          "13": ""
        },
        {
          "13": "ference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "13": ""
        },
        {
          "13": "2020, pp. 6264–6268."
        },
        {
          "13": ""
        },
        {
          "13": "G. Sun, Y. Zhang, R.\nJ. Weiss, Y. Cao, H. Zen, A. Rosenberg,"
        },
        {
          "13": ""
        },
        {
          "13": "B. Ramabhadran, and Y. Wu, “Generating diverse and natural"
        },
        {
          "13": ""
        },
        {
          "13": "text-to-speech samples using a quantized ﬁne-grained vae and"
        },
        {
          "13": ""
        },
        {
          "13": "IEEE In-\nautoregressive prosody prior,”\nin ICASSP 2020-2020"
        },
        {
          "13": ""
        },
        {
          "13": "ternational Conference\non Acoustics, Speech and Signal Processing"
        },
        {
          "13": ""
        },
        {
          "13": "(ICASSP).\nIEEE, 2020, pp. 6699–6703."
        },
        {
          "13": ""
        },
        {
          "13": "D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv preprint arXiv:1312.6114, 2013."
        },
        {
          "13": ""
        },
        {
          "13": "Y.-J. Zhang, S. Pan, L. He, and Z.-H. Ling, “Learning latent rep-"
        },
        {
          "13": ""
        },
        {
          "13": "resentations for style control and transfer in end-to-end speech"
        },
        {
          "13": ""
        },
        {
          "13": "synthesis,” in ICASSP 2019-2019 IEEE International Conference on"
        },
        {
          "13": ""
        },
        {
          "13": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp."
        },
        {
          "13": ""
        },
        {
          "13": "6945–6949."
        },
        {
          "13": ""
        },
        {
          "13": "T. Kenter, V. Wan, C.-A. Chan, R. Clark,\nand J. Vit,\n“Chive:"
        },
        {
          "13": ""
        },
        {
          "13": "Varying prosody in speech synthesis with a linguistically driven"
        },
        {
          "13": ""
        },
        {
          "13": "dynamic hierarchical conditional variational network,” in Inter-"
        },
        {
          "13": ""
        },
        {
          "13": "national Conference on Machine Learning.\nPMLR, 2019, pp. 3331–"
        },
        {
          "13": "3340."
        },
        {
          "13": "N. H. Frijda, A. Ortony, J. Sonnemans, and G. L. Clore, “The com-"
        },
        {
          "13": "plexity of\nintensity:\nIssues concerning the structure of emotion"
        },
        {
          "13": "intensity.” 1992."
        },
        {
          "13": "K. Matsumoto, S. Hara, and M. Abe, “Controlling the Strength"
        },
        {
          "13": "of Emotions\nin\nSpeech-Like Emotional\nSound Generated by"
        },
        {
          "13": "WaveNet,” in Proc. Interspeech 2020, 2020, pp. 3421–3425."
        },
        {
          "13": "B. Schnell and P. N. Garner, “Improving emotional\ntts with an"
        },
        {
          "13": "emotion intensity input\nfrom unsupervised extraction,” in Proc."
        },
        {
          "13": "11th ISCA Speech Synthesis Workshop (SSW 11), pp. 60–65."
        },
        {
          "13": "S.-Y. Um, S. Oh, K. Byun,\nI.\nJang, C. Ahn,\nand H.-G. Kang,"
        },
        {
          "13": "“Emotional speech synthesis with rich and granularized control,”"
        },
        {
          "13": "in ICASSP 2020-2020 IEEE International Conference on Acoustics,"
        },
        {
          "13": "Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 7254–7258."
        },
        {
          "13": "C.-B. Im, S.-H. Lee, S.-B. Kim, and S.-W. Lee, “Emoq-tts: Emotion"
        },
        {
          "13": "intensity\nquantization\nfor ﬁne-grained controllable\nemotional"
        },
        {
          "13": "text-to-speech,” in ICASSP 2022-2022 IEEE International Confer-"
        },
        {
          "13": "ence on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "13": "2022, pp. 6317–6321."
        },
        {
          "13": "C. M. Whissell, “The dictionary of affect\nin language,” in The"
        },
        {
          "13": "measurement of emotions.\nElsevier, 1989, pp. 113–131."
        },
        {
          "13": "P\n. Ekman, “An argument\nfor basic emotions,” Cognition & emo-"
        },
        {
          "13": "tion, 1992."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "[115] K. Zhou, B. Sisman, R. Liu, and H. Li, “Seen and unseen emo-"
        },
        {
          "14": "tional style transfer for voice conversion with a new emotional"
        },
        {
          "14": "speech dataset,” in ICASSP 2021-2021 IEEE International Confer-"
        },
        {
          "14": "ence on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "14": "2021, pp. 920–924."
        },
        {
          "14": "[116] ——, “Emotional voice conversion: Theory, databases and esd,”"
        },
        {
          "14": "Speech Communication, vol. 137, pp. 1–18, 2022."
        },
        {
          "14": "[117] D. P. Kingma\nand J. L. Ba,\n“Adam: Amethod for\nstochastic"
        },
        {
          "14": "optimization.”"
        },
        {
          "14": "[118] P. Heracleous,\nK.\nYasuda,\nF.\nSugaya,\nA.\nYoneyama,\nand"
        },
        {
          "14": "M. Hashimoto, “Speech emotion recognition in noisy and rever-"
        },
        {
          "14": "berant environments,” in 2017 Seventh International Conference on"
        },
        {
          "14": "Affective Computing and Intelligent Interaction (ACII).\nIEEE, 2017,"
        },
        {
          "14": "pp. 262–266."
        },
        {
          "14": "[119] U. Tiwari, M. Soni, R. Chakraborty, A. Panda, and S. K. Kop-"
        },
        {
          "14": "parapu, “Multi-conditioning and data augmentation using gen-"
        },
        {
          "14": "erative noise model\nfor\nspeech emotion recognition in noisy"
        },
        {
          "14": "conditions,” in ICASSP 2020-2020 IEEE International Conference"
        },
        {
          "14": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020,"
        },
        {
          "14": "pp. 7194–7198."
        },
        {
          "14": "[120] B. J. Abbaschian, D. Sierra-Sosa, and A. Elmaghraby, “Deep learn-"
        },
        {
          "14": "ing techniques for speech emotion recognition, from databases to"
        },
        {
          "14": "models,” Sensors, vol. 21, no. 4, p. 1249, 2021."
        },
        {
          "14": ""
        },
        {
          "14": "[121] H. Muthusamy, K. Polat,\nand S. Yaacob,\n“Improved emotion"
        },
        {
          "14": ""
        },
        {
          "14": "recognition using gaussian mixture model and extreme learning"
        },
        {
          "14": "machine in speech and glottal signals,” Mathematical Problems in"
        },
        {
          "14": "Engineering, vol. 2015, 2015."
        },
        {
          "14": "[122] M. Chen, X. He, J. Yang, and H. Zhang, “3-d convolutional recur-"
        },
        {
          "14": "rent neural networks with attention model\nfor speech emotion"
        },
        {
          "14": "recognition,” IEEE Signal Processing Letters, vol. 25, no. 10, pp."
        },
        {
          "14": "1440–1444, 2018."
        },
        {
          "14": ""
        },
        {
          "14": "[123] D. Bitouk, R. Verma,\nand A. Nenkova,\n“Class-level\nspectral"
        },
        {
          "14": ""
        },
        {
          "14": "features for emotion recognition,” Speech communication, vol. 52,"
        },
        {
          "14": ""
        },
        {
          "14": "no. 7-8, pp. 613–625, 2010."
        },
        {
          "14": ""
        },
        {
          "14": "[124] R. Kubichek, “Mel-cepstral distance measure for objective speech"
        },
        {
          "14": ""
        },
        {
          "14": "quality assessment,” in Proceedings of IEEE Paciﬁc Rim Conference"
        },
        {
          "14": ""
        },
        {
          "14": "on Communications Computers and Signal Processing, vol. 1.\nIEEE,"
        },
        {
          "14": ""
        },
        {
          "14": "1993, pp. 125–128."
        },
        {
          "14": ""
        },
        {
          "14": "[125] W. F.\nJohnson, R. N. Emde, K. R. Scherer, and M. D. Klinnert,"
        },
        {
          "14": ""
        },
        {
          "14": "of General\n“Recognition of emotion from vocal\ncues,” Archives"
        },
        {
          "14": ""
        },
        {
          "14": "Psychiatry, vol. 43, no. 3, pp. 280–283, 1986."
        },
        {
          "14": ""
        },
        {
          "14": "[126] M. J. Owren and J.-A. Bachorowski, “Measuring emotion-related"
        },
        {
          "14": ""
        },
        {
          "14": "vocal acoustics,” Handbook of emotion elicitation and assessment, pp."
        },
        {
          "14": ""
        },
        {
          "14": "239–266, 2007."
        },
        {
          "14": ""
        },
        {
          "14": "et\n[127] M. Morise\nal.,\n“Harvest: A high-performance\nfundamental"
        },
        {
          "14": ""
        },
        {
          "14": "frequency estimator\nfrom speech signals.”\nin INTERSPEECH,"
        },
        {
          "14": ""
        },
        {
          "14": "2017, pp. 2321–2325."
        },
        {
          "14": ""
        },
        {
          "14": "[128] K. Zhou, B. Sisman, M. Zhang, and H. Li, “Converting Anyone’s"
        },
        {
          "14": ""
        },
        {
          "14": "Emotion: Towards Speaker-Independent Emotional Voice Con-"
        },
        {
          "14": ""
        },
        {
          "14": "version,” in Proc. Interspeech 2020, 2020, pp. 3416–3420."
        },
        {
          "14": ""
        },
        {
          "14": "[129] K. Zhou, B. Sisman, and H. Li, “Transforming Spectrum and"
        },
        {
          "14": ""
        },
        {
          "14": "Prosody\nfor\nEmotional Voice Conversion with Non-Parallel"
        },
        {
          "14": ""
        },
        {
          "14": "Training Data,” in Proc. Odyssey 2020 The Speaker and Language"
        },
        {
          "14": ""
        },
        {
          "14": "Recognition Workshop, 2020, pp. 230–237."
        },
        {
          "14": ""
        },
        {
          "14": "[130] ——, “Vaw-gan for disentanglement and recomposition of emo-"
        },
        {
          "14": ""
        },
        {
          "14": "tional elements in speech,” in 2021 IEEE Spoken Language Technol-"
        },
        {
          "14": ""
        },
        {
          "14": "ogy Workshop (SLT).\nIEEE, 2021, pp. 415–422."
        },
        {
          "14": ""
        },
        {
          "14": "[131] P. Williams and J. L. Aaker, “Can mixed emotions peacefully"
        },
        {
          "14": ""
        },
        {
          "14": "coexist?” Journal of consumer research, vol. 28, no. 4, pp. 636–649,"
        },
        {
          "14": ""
        },
        {
          "14": "2002."
        },
        {
          "14": ""
        },
        {
          "14": "[132] Y. Miyamoto, Y. Uchida, and P. C. Ellsworth, “Culture and mixed"
        },
        {
          "14": "emotions:\nco-occurrence of positive and negative\nemotions\nin"
        },
        {
          "14": "japan and the united states.” Emotion, vol. 10, no. 3, p. 404, 2010."
        },
        {
          "14": "[133]\nS. Hareli, S. David, and U. Hess, “The role of emotion transition"
        },
        {
          "14": "for the perception of social dominance and afﬁliation,” Cognition"
        },
        {
          "14": "and Emotion, vol. 30, no. 7, pp. 1260–1270, 2016."
        },
        {
          "14": "[134]\nS. PS and G. Mahalakshmi, “Emotion models: a review,” Interna-"
        },
        {
          "14": "tional Journal of Control Theory and Applications, vol. 10, no. 8, pp."
        },
        {
          "14": "651–657, 2017."
        },
        {
          "14": "[135] G. A. Miller, “The magical number\nseven, plus or minus\ntwo:"
        },
        {
          "14": "Some limits on our capacity for processing information.” Psycho-"
        },
        {
          "14": "logical review, vol. 63, no. 2, p. 81, 1956."
        },
        {
          "14": "[136] L.\nF. Barrett,\n“Discrete\nemotions\nor dimensions?\nthe\nrole\nof"
        },
        {
          "14": "valence focus and arousal\nfocus,” Cognition & Emotion, vol. 12,"
        },
        {
          "14": "no. 4, pp. 579–599, 1998."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "15": "IEEE) received the B. Sc.\nRajib Rana (Member,"
        },
        {
          "15": "degree\nin\ncomputer\nscience\nand\nengineering"
        },
        {
          "15": "from Khulna University, with the Prime Minis-"
        },
        {
          "15": "ter and President’s Gold Medal\nfor outstanding"
        },
        {
          "15": "achievements, and the Ph. D. degree in com-"
        },
        {
          "15": "puter science and engineering from the Univer-"
        },
        {
          "15": "sity of New South Wales, Sydney, Australia,\nin"
        },
        {
          "15": "2011. He received his Postdoctoral Training with"
        },
        {
          "15": "the Autonomous System Laboratory, CSIRO, be-"
        },
        {
          "15": "fore joining the University of Southern Queens-"
        },
        {
          "15": "land, as a Faculty Member,\nin 2015. He is cur-"
        },
        {
          "15": "rently a Senior Advance Queensland Research Fellow and an Associate"
        },
        {
          "15": "Professor at\nthe University of Southern Queensland. He is also the Di-"
        },
        {
          "15": "rector of the IoT Health Research Program with the University of South-"
        },
        {
          "15": "ern Queensland, which capitalizes on advancements in technology and"
        },
        {
          "15": "sophisticated information and data processing to understand disease"
        },
        {
          "15": "progression in chronic health conditions better and develop predictive"
        },
        {
          "15": "algorithms for chronic diseases, such as mental\nillness and cancer. His"
        },
        {
          "15": "current research interests include unsupervised representation learning,"
        },
        {
          "15": "Adversarial Machine Learning, Re-enforcement Learning, Federated"
        },
        {
          "15": "Learning, Emotional Speech Generation, and Domain Adaptation."
        },
        {
          "15": "received\nBj ¨orn W. Schuller (M’05-SM’15-F’18)"
        },
        {
          "15": "the diploma degree,\nthe doctoral degree in au-"
        },
        {
          "15": "tomatic\nspeech and emotion recognition, and"
        },
        {
          "15": "the habilitation and Adjunct Teaching Professor"
        },
        {
          "15": ""
        },
        {
          "15": "in signal processing and machine intelligence"
        },
        {
          "15": ""
        },
        {
          "15": "(TUM),\nfrom Technische Universit ¨at M ¨unchen"
        },
        {
          "15": ""
        },
        {
          "15": "Munich, Germany,\nin 1999, 2006, and 2012, re-"
        },
        {
          "15": ""
        },
        {
          "15": "spectively, all\nin electrical engineering and in-"
        },
        {
          "15": ""
        },
        {
          "15": "formation technology. He is currently a Profes-"
        },
        {
          "15": ""
        },
        {
          "15": "sor of Artiﬁcial\nIntelligence with the Department"
        },
        {
          "15": ""
        },
        {
          "15": "of Computing,\nImperial College London, U.K.,"
        },
        {
          "15": ""
        },
        {
          "15": "where he heads the Group on Language, Audio, & Music (GLAM), a"
        },
        {
          "15": ""
        },
        {
          "15": "Full Professor and the Head of\nthe Chair of Embedded Intelligence for"
        },
        {
          "15": ""
        },
        {
          "15": "Health Care and Wellbeing with the University of Augsburg, Germany,"
        },
        {
          "15": ""
        },
        {
          "15": "and the founding CEO/CSO of audEERING. He was previously a Full"
        },
        {
          "15": ""
        },
        {
          "15": "Professor and the Head of the Chair of Complex and Intelligent Systems"
        },
        {
          "15": ""
        },
        {
          "15": "at\nthe University of Passau, Germany. He has (co-)authored ﬁve books"
        },
        {
          "15": ""
        },
        {
          "15": "and more than 1 000 publications in peer-reviewed books,\njournals, and"
        },
        {
          "15": ""
        },
        {
          "15": "conference proceedings leading to more than overall 40,000 citations"
        },
        {
          "15": ""
        },
        {
          "15": "(H-index=97). He was an Elected Member of\nthe IEEE Speech and"
        },
        {
          "15": ""
        },
        {
          "15": "Language Processing Technical Committee. He is a Golden Core Mem-"
        },
        {
          "15": ""
        },
        {
          "15": "ber of\nthe IEEE Computer Society, a Fellow of\nthe IEEE, AAAC, BCS,"
        },
        {
          "15": ""
        },
        {
          "15": "and ISCA, as well as a Senior Member of\nthe ACM, and the President-"
        },
        {
          "15": ""
        },
        {
          "15": "Emeritus of the Association of the Advancement of Affective Computing"
        },
        {
          "15": ""
        },
        {
          "15": "(AAAC). He was the General Chair of ACII 2019, a Co-Program Chair"
        },
        {
          "15": "of\nInterspeech,\nin 2019, and ICMI,\nin 2019, a repeated Area Chair of"
        },
        {
          "15": "ICASSP, next\nto a multitude of\nfurther Associate and Guest Editor roles"
        },
        {
          "15": "and functions in Technical and Organisational Committees. He is the"
        },
        {
          "15": "Field Chief Editor of\nthe Frontiers in Digital Health and a former Editor-"
        },
        {
          "15": "in-Chief of\nthe IEEE Transactions on Affective Computing."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "B. Sc., M. Sc., and Ph.D degrees in electrical and"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "electronic engineering from South China Univer-"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "sity of Technology, Guangzhou, China in 1984,"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "1987 and 1990 respectively. Dr Li\nis currently"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "a Presidential Chair Professor and the Execu-"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "tive Dean of\nthe School of Data Science, The"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "Chinese University of Hong Kong, Shenzhen,"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "China. He is also with the Department of Electri-"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "cal and Computer Engineering, National Univer-"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "sity of Singapore (NUS). His research interests"
        },
        {
          "(M’91-SM’01-F’14)\nreceived\nthe\nHaizhou\nLi": "include automatic speech recognition, speaker and language recogni-"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tive Dean of\nthe School of Data Science, The": "Chinese University of Hong Kong, Shenzhen,"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "China. He is also with the Department of Electri-"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "cal and Computer Engineering, National Univer-"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "sity of Singapore (NUS). His research interests"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "include automatic speech recognition, speaker and language recogni-"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "tion and natural\nlanguage processing. Prior to joining NUS, he taught at"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "the University of Hong Kong (1988-1990) and the South China University"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "of Technology (1990-1994). He was a Visiting Professor at CRIN in"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "France (1994-1995), Research Manager at\nthe Apple-ISS Research"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Centre\n(1996-1998), Research Director\nat\nLernout & Hauspie Asia"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Paciﬁc (1999-2001), Vice President at\nInfoTalk Corp. Ltd.\n(2001-2003)"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "and the Principal Scientist and Department Head of Human Language"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Technology in the Institute for\nInfocomm Research, Singapore (2003-"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "2016). Dr Li served as the Editor-in-Chief of\nIEEE/ACM Transactions"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "on Audio, Speech and Language Processing (2015-2018) and as a"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Member of\nthe Editorial Board of Computer Speech and Language"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "(2012-2018). He was an elected Member of\nthe IEEE Speech and"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Language Processing Technical Committee (2013-2015),\nthe President"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "of\nthe International Speech Communication Association (2015-2017),"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "the President of\nthe Asia Paciﬁc Signal and Information Processing"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Association (2015-2016) and the President of\nthe Asian Federation of"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Natural Language Processing (2017-2018). He was the General Chair"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "of ACL 2012,\nINTERSPEECH 2014, ASRU 2019 and ICASSP 2022. Dr"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Li\nis a Fellow of\nthe IEEE,\nthe ISCA and the Academy of Engineering"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Singapore. He was a recipient of\nthe National\nInfocomm Award in 2002"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "and the President’s Technology Award in 2013 in Singapore. He was"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "named one of\nthe two Nokia Visiting Professors in 2009 by the Nokia"
        },
        {
          "tive Dean of\nthe School of Data Science, The": "Foundation, and Bremen Excellence Chair Professor in 2019."
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mixed emotions and coping: The benefits of secondary emotions",
      "authors": [
        "A Braniecka",
        "E Trzebi Ńska",
        "A Dowgiert",
        "A Wytykowska"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "2",
      "title": "The case for mixed emotions",
      "authors": [
        "J Larsen",
        "A Mcgraw"
      ],
      "year": "2014",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "3",
      "title": "Further evidence for mixed emotions",
      "year": "2011",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "4",
      "title": "Emotional speech synthesis: A review",
      "authors": [
        "M Schr Öder"
      ],
      "year": "2001",
      "venue": "Seventh European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "5",
      "title": "Handling emotions in human-computer dialogues",
      "authors": [
        "J Pittermann",
        "A Pittermann",
        "W Minker"
      ],
      "year": "2010",
      "venue": "Handling emotions in human-computer dialogues"
    },
    {
      "citation_id": "6",
      "title": "A survey of using vocal prosody to convey emotion in robot speech",
      "authors": [
        "J Crumpton",
        "C Bethel"
      ],
      "year": "2016",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "7",
      "title": "Prosodic aspects of the attractive voice",
      "authors": [
        "A Rosenberg",
        "J Hirschberg"
      ],
      "year": "2021",
      "venue": "Voice Attractiveness"
    },
    {
      "citation_id": "8",
      "title": "A survey on neural speech synthesis",
      "authors": [
        "X Tan",
        "T Qin",
        "F Soong",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "A survey on neural speech synthesis",
      "arxiv": "arXiv:2106.15561"
    },
    {
      "citation_id": "9",
      "title": "Progress in speech synthesis",
      "authors": [
        "J Van Santen",
        "R Sproat",
        "J Olive",
        "J Hirschberg"
      ],
      "year": "2013",
      "venue": "Progress in speech synthesis"
    },
    {
      "citation_id": "10",
      "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
      "authors": [
        "B Sisman",
        "J Yamagishi",
        "S King",
        "H Li"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Char2wav: End-to-end speech synthesis",
      "authors": [
        "J Sotelo",
        "S Mehri",
        "K Kumar",
        "J Santos",
        "K Kastner",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Char2wav: End-to-end speech synthesis"
    },
    {
      "citation_id": "12",
      "title": "Tacotron: Towards endto-end speech synthesis",
      "authors": [
        "Y Wang",
        "R Skerry-Ryan",
        "D Stanton",
        "Y Wu",
        "R Weiss",
        "N Jaitly",
        "Z Yang",
        "Y Xiao",
        "Z Chen",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Tacotron: Towards endto-end speech synthesis",
      "arxiv": "arXiv:1703.10135"
    },
    {
      "citation_id": "13",
      "title": "Fastspeech: Fast, robust and controllable text to speech",
      "authors": [
        "Y Ren",
        "Y Ruan",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T.-Y Liu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Statistical parametric speech synthesis using deep neural networks",
      "authors": [
        "H Ze",
        "A Senior",
        "M Schuster"
      ],
      "year": "2013",
      "venue": "2013 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "15",
      "title": "Analysis of speaker adaptation algorithms for hmm-based speech synthesis and a constrained smaplr adaptation algorithm",
      "authors": [
        "J Yamagishi",
        "T Kobayashi",
        "Y Nakano",
        "K Ogata",
        "J Isogai"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "From hmms to dnns: where do the improvements come from?",
      "authors": [
        "O Watts",
        "G Henter",
        "T Merritt",
        "Z Wu",
        "S King"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "The age of artificial emotional intelligence",
      "authors": [
        "D Schuller",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Computer"
    },
    {
      "citation_id": "18",
      "title": "An hmm-based speech synthesis system applied to english",
      "authors": [
        "K Tokuda",
        "H Zen",
        "A Black"
      ],
      "year": "2002",
      "venue": "IEEE speech synthesis workshop"
    },
    {
      "citation_id": "19",
      "title": "Emotional transplant in statistical speech synthesis based on emotion additive model",
      "authors": [
        "Y Ohtani",
        "Y Nasu",
        "M Morita",
        "M Akamine"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "20",
      "title": "Model architectures to extrapolate emotional expressions in dnn-based text-to-speech",
      "authors": [
        "K Inoue",
        "S Hara",
        "M Abe",
        "N Hojo",
        "Y Ijima"
      ],
      "year": "2021",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "21",
      "title": "The emotions",
      "authors": [
        "R Plutchik"
      ],
      "year": "1991",
      "venue": "The emotions"
    },
    {
      "citation_id": "22",
      "title": "Speech prosody: A methodological review",
      "authors": [
        "Y Xu"
      ],
      "year": "2011",
      "venue": "Journal of Speech Sciences"
    },
    {
      "citation_id": "23",
      "title": "Multilevel parametric-base f0 model for speech synthesis",
      "authors": [
        "J Latorre",
        "M Akamine"
      ],
      "year": "2008",
      "venue": "Ninth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "24",
      "title": "Modeling of various speaking styles and emotions for hmm-based speech synthesis",
      "authors": [
        "J Yamagishi",
        "K Onishi",
        "T Masuko",
        "T Kobayashi"
      ],
      "year": "2003",
      "venue": "Eighth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised clustering of emotion and voice styles for expressive tts",
      "authors": [
        "F Eyben",
        "S Buchholz",
        "N Braunschweiler",
        "J Latorre",
        "V Wan",
        "M Gales",
        "K Knill"
      ],
      "year": "2012",
      "venue": "ICASSP 2012 -2012 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Gmmbased emotional voice conversion using spectrum and prosody features",
      "authors": [
        "R Aihara",
        "R Takashima",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2012",
      "venue": "American Journal of Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Gmm-based voice conversion applied to emotional speech synthesis",
      "authors": [
        "H Kawanami",
        "Y Iwami",
        "T Toda",
        "H Saruwatari",
        "K Shikano"
      ],
      "year": "2003",
      "venue": "Gmm-based voice conversion applied to emotional speech synthesis"
    },
    {
      "citation_id": "28",
      "title": "Investigating different representations for modeling and controlling multiple emotions in dnn-based speech synthesis",
      "authors": [
        "J Lorenzo-Trueba",
        "G Henter",
        "S Takaki",
        "J Yamagishi",
        "Y Morino",
        "Y Ochiai"
      ],
      "year": "2018",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "29",
      "title": "Emotional voice conversion with adaptive scales f0 based on wavelet transform using limited amount of emotional data",
      "authors": [
        "Z Luo",
        "J Chen",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2017",
      "venue": "Emotional voice conversion with adaptive scales f0 based on wavelet transform using limited amount of emotional data"
    },
    {
      "citation_id": "30",
      "title": "Deep bidirectional lstm modeling of timbre and prosody for emotional voice conversion",
      "authors": [
        "H Ming",
        "D Huang",
        "L Xie",
        "J Wu",
        "M Dong",
        "H Li"
      ],
      "year": "2016",
      "venue": "Deep bidirectional lstm modeling of timbre and prosody for emotional voice conversion"
    },
    {
      "citation_id": "31",
      "title": "Emotional statistical parametric speech synthesis using lstm-rnns",
      "authors": [
        "S An",
        "Z Ling",
        "L Dai"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "32",
      "title": "Towards endto-end prosody transfer for expressive speech synthesis with tacotron",
      "authors": [
        "R Skerry-Ryan",
        "E Battenberg",
        "Y Xiao",
        "Y Wang",
        "D Stanton",
        "J Shor",
        "R Weiss",
        "R Clark",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "33",
      "title": "End-toend emotional speech synthesis using style tokens and semisupervised training",
      "authors": [
        "P Wu",
        "Z Ling",
        "L Liu",
        "Y Jiang",
        "H Wu",
        "L Dai"
      ],
      "year": "2019",
      "venue": "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "34",
      "title": "Emotional end-to-end neural speech synthesizer",
      "authors": [
        "Y Lee",
        "S.-Y Lee",
        "A Rabiee"
      ],
      "year": "2017",
      "venue": "NIPS2017. Neural Information Processing Systems Foundation"
    },
    {
      "citation_id": "35",
      "title": "Exploring transfer learning for low resource emotional tts",
      "authors": [
        "N Tits",
        "K Haddad",
        "T Dutoit"
      ],
      "year": "2019",
      "venue": "Proceedings of SAI Intelligent Systems Conference"
    },
    {
      "citation_id": "36",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R.-S Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "Y Jia",
        "F Ren",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "37",
      "title": "Predicting expressive speaking style from text in end-to-end speech synthesis",
      "authors": [
        "D Stanton",
        "Y Wang",
        "R Skerry-Ryan"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "38",
      "title": "Understanding mixed emotions: paradigms and measures",
      "authors": [
        "S Kreibig",
        "J Gross"
      ],
      "year": "2017",
      "venue": "Understanding mixed emotions: paradigms and measures"
    },
    {
      "citation_id": "39",
      "title": "Eliciting mixed emotions: a meta-analysis comparing models, types, and measures",
      "authors": [
        "R Berrios",
        "P Totterdell",
        "S Kellett"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "40",
      "title": "Social functionality of human emotion",
      "authors": [
        "P Niedenthal",
        "M Brauer"
      ],
      "year": "2012",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "41",
      "title": "The mind and its stories: Narrative universals and human emotion",
      "authors": [
        "P Hogan"
      ],
      "year": "2003",
      "venue": "The mind and its stories: Narrative universals and human emotion"
    },
    {
      "citation_id": "42",
      "title": "The nature of emotion: Fundamental questions",
      "authors": [
        "P Ekman",
        "R Davidson"
      ],
      "year": "1994",
      "venue": "The nature of emotion: Fundamental questions"
    },
    {
      "citation_id": "43",
      "title": "The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "R Plutchik"
      ],
      "year": "2001",
      "venue": "American scientist"
    },
    {
      "citation_id": "44",
      "title": "Theories of emotion",
      "authors": [
        "R Plutchik",
        "H Kellerman"
      ],
      "year": "2013",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "45",
      "title": "Changing Minds: The Go-to Guide to Mental Health for Family and Friends",
      "authors": [
        "M Cross",
        "C Hanrahan"
      ],
      "year": "2016",
      "venue": "Changing Minds: The Go-to Guide to Mental Health for Family and Friends"
    },
    {
      "citation_id": "46",
      "title": "The ordinal nature of emotions",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "47",
      "title": "The ordinal nature of emotions: An emerging approach",
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Quantifying emotional similarity in speech",
      "authors": [
        "J Harvill",
        "S.-G Leem",
        "M Abdelwahab",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "49",
      "title": "Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech",
      "authors": [
        "H Cao",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2015",
      "venue": "Computer speech & language"
    },
    {
      "citation_id": "50",
      "title": "Retrieving speech samples with similar emotional content using a triplet loss function",
      "authors": [
        "J Harvill",
        "M Wahab",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Pairwise preference learning and ranking",
      "authors": [
        "E Üllermeier"
      ],
      "year": "2003",
      "venue": "European conference on machine learning"
    },
    {
      "citation_id": "52",
      "title": "Retrieving categorical emotions using a probabilistic framework to define preference learning samples",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "53",
      "title": "Using agreement on direction of change to build rank-based emotion classifiers",
      "authors": [
        "S Parthasarathy",
        "R Cowie",
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "54",
      "title": "Don't classify ratings of affect; rank them",
      "authors": [
        "H Martinez",
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "55",
      "title": "Grounding truth via ordinal annotation",
      "authors": [
        "G Yannakakis",
        "H Martinez"
      ],
      "year": "2015",
      "venue": "2015 international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "56",
      "title": "Speech emotion recognition from variable-length inputs with triplet loss function",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "57",
      "title": "A siamese neural network with modified distance loss for transfer learning in speech emotion recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "A siamese neural network with modified distance loss for transfer learning in speech emotion recognition",
      "arxiv": "arXiv:2006.03001"
    },
    {
      "citation_id": "58",
      "title": "Controlling emotion strength with relative attribute for end-to-end speech synthesis",
      "authors": [
        "X Zhu",
        "S Yang",
        "G Yang",
        "L Xie"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "59",
      "title": "Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "L Xie"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "60",
      "title": "Emotion intensity and its control for emotional voice conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "61",
      "title": "Msemotts: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "X Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "62",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "63",
      "title": "Char2wav: End-to-end speech synthesis",
      "authors": [
        "K Kyle",
        "K Jose",
        "S Sotelo"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations, workshop"
    },
    {
      "citation_id": "64",
      "title": "A review on five recent and near-future developments in computational processing of emotion in the human voice",
      "authors": [
        "D Schuller",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "65",
      "title": "Limited data emotional voice conversion leveraging text-to-speech: Two-stage sequence-tosequence training",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "Limited data emotional voice conversion leveraging text-to-speech: Two-stage sequence-tosequence training",
      "arxiv": "arXiv:2103.16809"
    },
    {
      "citation_id": "66",
      "title": "Acoustic feature analysis in speech emotion primitives estimation",
      "authors": [
        "D Wu",
        "T Parsons",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "67",
      "title": "Adjusting pleasure-arousaldominance for continuous emotional text-to-speech synthesizer",
      "authors": [
        "A Rabiee",
        "T.-H Kim",
        "S.-Y Lee"
      ],
      "year": "2019",
      "venue": "Adjusting pleasure-arousaldominance for continuous emotional text-to-speech synthesizer"
    },
    {
      "citation_id": "68",
      "title": "Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition",
      "authors": [
        "X Cai",
        "D Dai",
        "Z Wu",
        "X Li",
        "J Li",
        "H Meng"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "69",
      "title": "Controllable emotion transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "S Yang",
        "L Xue",
        "L Xie"
      ],
      "year": "2021",
      "venue": "2021 12th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "70",
      "title": "Neural tts stylization with adversarial and collaborative games",
      "authors": [
        "S Ma",
        "D Mcduff",
        "Y Song"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "71",
      "title": "Interactive multi-level prosody control for expressive speech synthesis",
      "authors": [
        "T Cornille",
        "F Wang",
        "J Bekker"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "72",
      "title": "Fine-Grained Robust Prosody Transfer for Single-Speaker Neural Text-To-Speech",
      "authors": [
        "V Klimkov",
        "S Ronanki",
        "J Rohnke",
        "T Drugman"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "73",
      "title": "Towards Multi-Scale Style Control for Expressive Speech Synthesis",
      "authors": [
        "X Li",
        "C Song",
        "J Li",
        "Z Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "74",
      "title": "Learning syllable-level discrete prosodic representation for expressive speech generation",
      "authors": [
        "G Zhang",
        "Y Qin",
        "T Lee"
      ],
      "year": "2020",
      "venue": "Learning syllable-level discrete prosodic representation for expressive speech generation"
    },
    {
      "citation_id": "75",
      "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "76",
      "title": "Sequence-to-sequence emotional voice conversion with strength control",
      "authors": [
        "H Choi",
        "M Hahn"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "77",
      "title": "Prosody and emotions",
      "authors": [
        "S Mozziconacci"
      ],
      "year": "2002",
      "venue": "Speech Prosody 2002, International Conference"
    },
    {
      "citation_id": "78",
      "title": "Robust and fine-grained prosody control of end-to-end speech synthesis",
      "authors": [
        "Y Lee",
        "T Kim"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "79",
      "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
      "authors": [
        "D Tan",
        "T Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "80",
      "title": "Fullyhierarchical fine-grained prosody modeling for interpretable speech synthesis",
      "authors": [
        "G Sun",
        "Y Zhang",
        "R Weiss",
        "Y Cao",
        "H Zen",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "81",
      "title": "Generating diverse and natural text-to-speech samples using a quantized fine-grained vae and autoregressive prosody prior",
      "authors": [
        "G Sun",
        "Y Zhang",
        "R Weiss",
        "Y Cao",
        "H Zen",
        "A Rosenberg",
        "B Ramabhadran",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "82",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "83",
      "title": "Learning latent representations for style control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y.-J Zhang",
        "S Pan",
        "L He",
        "Z.-H Ling"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "84",
      "title": "Chive: Varying prosody in speech synthesis with a linguistically driven dynamic hierarchical conditional variational network",
      "authors": [
        "T Kenter",
        "V Wan",
        "C.-A Chan",
        "R Clark",
        "J Vit"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "85",
      "title": "The complexity of intensity: Issues concerning the structure of emotion intensity",
      "authors": [
        "N Frijda",
        "A Ortony",
        "J Sonnemans",
        "G Clore"
      ],
      "year": "1992",
      "venue": "The complexity of intensity: Issues concerning the structure of emotion intensity"
    },
    {
      "citation_id": "86",
      "title": "Controlling the Strength of Emotions in Speech-Like Emotional Sound Generated by WaveNet",
      "authors": [
        "K Matsumoto",
        "S Hara",
        "M Abe"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "87",
      "title": "Improving emotional tts with an emotion intensity input from unsupervised extraction",
      "authors": [
        "B Schnell",
        "P Garner"
      ],
      "venue": "Proc. 11th ISCA Speech Synthesis Workshop"
    },
    {
      "citation_id": "88",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "S.-Y Um",
        "S Oh",
        "K Byun",
        "I Jang",
        "C Ahn",
        "H.-G Kang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "89",
      "title": "Emoq-tts: Emotion intensity quantization for fine-grained controllable emotional text-to-speech",
      "authors": [
        "C.-B Im",
        "S.-H Lee",
        "S.-B Kim",
        "S.-W Lee"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "90",
      "title": "The dictionary of affect in language",
      "authors": [
        "C Whissell"
      ],
      "year": "1989",
      "venue": "The measurement of emotions"
    },
    {
      "citation_id": "91",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "92",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "93",
      "title": "Expressing degree of activation in synthetic speech",
      "authors": [
        "M Schroder"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "94",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "95",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "96",
      "title": "Relative attributes",
      "authors": [
        "D Parikh",
        "K Grauman"
      ],
      "year": "2011",
      "venue": "2011 International Conference on Computer Vision. IEEE"
    },
    {
      "citation_id": "97",
      "title": "Whittlesearch: Image search with relative attribute feedback",
      "authors": [
        "A Kovashka",
        "D Parikh",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "98",
      "title": "Robust relative attributes for human action recognition",
      "authors": [
        "Z Zhang",
        "C Wang",
        "B Xiao",
        "W Zhou",
        "S Liu"
      ],
      "year": "2015",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "99",
      "title": "Relative attributes for largescale abandoned object detection",
      "authors": [
        "Q Fan",
        "P Gabbur",
        "S Pankanti"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "100",
      "title": "Training a support vector machine in the primal",
      "authors": [
        "O Chapelle"
      ],
      "year": "2007",
      "venue": "Neural computation"
    },
    {
      "citation_id": "101",
      "title": "Joint training framework for text-to-speech and voice conversion using multi-source tacotron and wavenet",
      "authors": [
        "M Zhang",
        "X Wang",
        "F Fang",
        "H Li",
        "J Yamagishi"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "102",
      "title": "Non-parallel sequenceto-sequence voice conversion with disentangled linguistic and speaker representations",
      "authors": [
        "J.-X Zhang",
        "Z.-H Ling",
        "L.-R Dai"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "103",
      "title": "Transfer learning from speech synthesis to voice conversion with non-parallel training data",
      "authors": [
        "M Zhang",
        "Y Zhou",
        "L Zhao",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "104",
      "title": "Attention-based wavenet autoencoder for universal voice conversion",
      "authors": [
        "A Polyak",
        "L Wolf"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "105",
      "title": "Deep voice 3: 2000-speaker neural textto-speech",
      "authors": [
        "W Ping",
        "K Peng",
        "A Gibiansky",
        "S Arik",
        "A Kannan",
        "S Narang",
        "J Raiman",
        "J Miller"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "106",
      "title": "Close to human quality tts with transformer",
      "authors": [
        "N Li",
        "S Liu",
        "Y Liu",
        "S Zhao",
        "M Liu",
        "M Zhou"
      ],
      "year": "2018",
      "venue": "Close to human quality tts with transformer",
      "arxiv": "arXiv:1809.08895"
    },
    {
      "citation_id": "107",
      "title": "Bootstrapping non-parallel voice conversion from speaker-adaptive text-to-speech",
      "authors": [
        "H.-T Luong",
        "J Yamagishi"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "108",
      "title": "Voice transformer network: Sequence-to-sequence voice conversion using transformer with text-to-speech pretraining",
      "authors": [
        "W.-C Huang",
        "T Hayashi",
        "Y.-C Wu",
        "H Kameoka",
        "T Toda"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "109",
      "title": "Nautilus: a versatile voice cloning system",
      "authors": [
        "H.-T Luong",
        "J Yamagishi"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "110",
      "title": "The festival speech synthesis system, version 1.4.2",
      "authors": [
        "A Black",
        "P Taylor",
        "R Caley",
        "R Clark",
        "K Richmond",
        "S King",
        "V Strom",
        "H Zen"
      ],
      "year": "2001",
      "venue": "The festival speech synthesis system, version 1.4.2"
    },
    {
      "citation_id": "111",
      "title": "Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning",
      "authors": [
        "J.-X Zhang",
        "Z.-H Ling",
        "L.-R Dai"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "112",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "113",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "114",
      "title": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit",
      "authors": [
        "C Veaux",
        "J Yamagishi",
        "K Macdonald"
      ],
      "year": "2016",
      "venue": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit"
    },
    {
      "citation_id": "115",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "116",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "117",
      "title": "Adam: Amethod for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "venue": "Adam: Amethod for stochastic optimization"
    },
    {
      "citation_id": "118",
      "title": "Speech emotion recognition in noisy and reverberant environments",
      "authors": [
        "P Heracleous",
        "K Yasuda",
        "F Sugaya",
        "A Yoneyama",
        "M Hashimoto"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "119",
      "title": "Multi-conditioning and data augmentation using generative noise model for speech emotion recognition in noisy conditions",
      "authors": [
        "U Tiwari",
        "M Soni",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "120",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "B Abbaschian",
        "D Sierra-Sosa",
        "A Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "121",
      "title": "Improved emotion recognition using gaussian mixture model and extreme learning machine in speech and glottal signals",
      "authors": [
        "H Muthusamy",
        "K Polat",
        "S Yaacob"
      ],
      "year": "2015",
      "venue": "Mathematical Problems in Engineering"
    },
    {
      "citation_id": "122",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "123",
      "title": "Class-level spectral features for emotion recognition",
      "authors": [
        "D Bitouk",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2010",
      "venue": "Speech communication"
    },
    {
      "citation_id": "124",
      "title": "Mel-cepstral distance measure for objective speech quality assessment",
      "authors": [
        "R Kubichek"
      ],
      "year": "1993",
      "venue": "Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing"
    },
    {
      "citation_id": "125",
      "title": "Recognition of emotion from vocal cues",
      "authors": [
        "W Johnson",
        "R Emde",
        "K Scherer",
        "M Klinnert"
      ],
      "year": "1986",
      "venue": "Archives of General Psychiatry"
    },
    {
      "citation_id": "126",
      "title": "Measuring emotion-related vocal acoustics",
      "authors": [
        "M Owren",
        "J.-A Bachorowski"
      ],
      "year": "2007",
      "venue": "Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "127",
      "title": "Harvest: A high-performance fundamental frequency estimator from speech signals",
      "authors": [
        "M Morise"
      ],
      "year": "2017",
      "venue": "Harvest: A high-performance fundamental frequency estimator from speech signals"
    },
    {
      "citation_id": "128",
      "title": "Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "M Zhang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "129",
      "title": "Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. Odyssey 2020 The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "130",
      "title": "Vaw-gan for disentanglement and recomposition of emotional elements in speech",
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "131",
      "title": "Can mixed emotions peacefully coexist?",
      "authors": [
        "P Williams",
        "J Aaker"
      ],
      "year": "2002",
      "venue": "Journal of consumer research"
    },
    {
      "citation_id": "132",
      "title": "Culture and mixed emotions: co-occurrence of positive and negative emotions in japan and the united states",
      "authors": [
        "Y Miyamoto",
        "Y Uchida",
        "P Ellsworth"
      ],
      "year": "2010",
      "venue": "Emotion"
    },
    {
      "citation_id": "133",
      "title": "The role of emotion transition for the perception of social dominance and affiliation",
      "authors": [
        "S Hareli",
        "S David",
        "U Hess"
      ],
      "year": "2016",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "134",
      "title": "Emotion models: a review",
      "authors": [
        "S Ps",
        "G Mahalakshmi"
      ],
      "year": "2017",
      "venue": "International Journal of Control Theory and Applications"
    },
    {
      "citation_id": "135",
      "title": "The magical number seven, plus or minus two: Some limits on our capacity for processing information",
      "authors": [
        "G Miller"
      ],
      "year": "1956",
      "venue": "Psychological review"
    },
    {
      "citation_id": "136",
      "title": "Discrete emotions or dimensions? the role of valence focus and arousal focus",
      "authors": [
        "L Barrett"
      ],
      "year": "1998",
      "venue": "Cognition & Emotion"
    }
  ]
}