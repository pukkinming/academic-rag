{
  "paper_id": "2408.13019v1",
  "title": "Vcemo: Multi-Modal Emotion Recognition For Chinese Voiceprints",
  "published": "2024-08-23T12:14:18Z",
  "authors": [
    "Jinghua Tang",
    "Liyun Zhang",
    "Yu Lu",
    "Dian Ding",
    "Lanqing Yang",
    "YiChao Chen",
    "Minjie Bian",
    "Xiaoshan Li",
    "Guangtao Xue"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Multi-modal",
    "Chinese Voiceprints"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition can enhance humanized machine responses to user commands, while voiceprint-based perception systems can be easily integrated into commonly used devices like smartphones and stereos. Despite having the largest number of speakers, there's a noticeable absence of high-quality corpus datasets for emotion recognition using Chinese voiceprints. Hence, this paper introduces the VCEMO dataset to address this deficiency. The proposed dataset is constructed from everyday conversations and comprises over 100 users and 7,747 textual samples. Furthermore, this paper proposes a multimodal-based model as a benchmark, which effectively fuses speech, text, and external knowledge using a co-attention structure. The system employs contrastive learning-based regulation for the uneven distribution of the dataset and the diversity of emotional expressions. The experiments demonstrate the significant improvement of the proposed model over SOTA on the VCEMO and IEMOCAP datasets. Code and dataset will be released for research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Audio data plays a fundamental and irreplaceable role in our comprehension of the world. It encapsulates not only words and language but also the intricate tapestry of human experiences and emotions. Consequently, audio data exhibits an astonishingly diverse array of applications that span the entire spectrum of human endeavor. Specifically, voiceprint-based emotion recognition from audio data is paramount for assistance in communicating with people and many human-computer interaction applications. In call centers, employees can make informed decisions by receiving timely feed-back on customers' moods or assess business interactions based on customers' emotional states. Simultaneously, the software application can adapt and enhance user experiences by implementing appropriate behaviors through real-time monitoring of the user's emotions  [24] . With over a billion native speakers and a rich cultural heritage, Chinese has undeniably emerged as a highly popular language on the global stage. Hence, the application of emotion recognition for Chinese audio holds great promise. However, there exists a restricted amount of Chinese corpus data available for model training in the context of emotion recognition. Furthermore, current methods are typically trained and evaluated on English datasets, lacking specific processing and optimization for Chinese data.\n\nConsequently, we collected a large Chinese conversation sentiment corpus called VCEMO for the single-sentence Chinese emotion recognition task. The dataset consists of single-sentence conversations of everyday life and has several advantages:\n\n1). Rich voiceprint information: Considering that the collection of voice information in previous datasets (e.g., CASIA  [18] , IEMOCAP  [1] ) has often relied on a specific few professional readers or professional actors, only a few people's pronunciation information as well as voiceprint features are present in the datasets. Our dataset contains daily speech data from more than 100 people, including a wide range of Chinese pronunciation accents and spoken language features.\n\n2). Abundant text information: The textual content of the dataset is exclusively sourced from spontaneous conversations in everyday life. Consequently, there exist substantial disparities between these texts, and they are abundant in information.\n\n3). Adaptability to multi-modal fusion: Given that the data originate exclusively from everyday conversations and individuals naturally employ various textual expressions to convey their inner sentiments based on their emotions, we can effectively leverage the multi-modal fusion of audio signals and textual information for the emotion recognition task.\n\nContemporary methods  [5, 4, 15, 20, 21, 19]  commonly employ neural networks for tasks such as emotion recognition, as well as for effective feature extraction and classification of data. Given the notable distinctions between Chinese and English, these methods lack specific processing tailored to Chinese information. Hence, leveraging the extensive Chinese corpus dataset VCEMO, we introduce a novel multimodal model for emotion recognition. Automatic speech recognition (ASR)  [11]  is adapted to convert audio signals into Chinese text messages. For Chinese text, we use the pre-trained Chinese BERT architecture for processing. In addition, we utilize text embedding for additional emotion feature extraction from Chinese text. Finally, the co-attention structure is employed to fuse multi-modal data features.\n\nFurthermore, given that the VCEMO dataset originates from everyday conversations, there is an uneven distribution of emotional data within the dataset. Additionally, a single audio sentence may encompass diverse emotional expressions, making it challenging for a singular emotional label to fully convey its comprehensive emotional information. To tackle the aforementioned issue, we employ a contrastive-learning-based regulation for training our model. Eventually, experimental tests have demonstrated that our model has significantly better emotion recognition performance on VCEMO than previously studied models.\n\nOverall, our contributions are as follows: -We produce a new Chinese daily conversational corpus dataset for emotion recognition, called VCEMO, containing 7477 samples of audio signals from over 100 individuals.\n\n-We propose a multi-modal model for acoustic data and text data (word embeddings and pre-trained BERT embeddings) using the co-attention structure for multi-modal feature fusion.\n\n-We employ a contrastive-learning-based regulation to train and optimize models, mitigating issues related to sample imbalance and under-representation of individual labels.\n\n-Extensive experiments show that our model has SOTA performance on the VCEMO and IEMOCAP datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Speech emotion recognition has been studied for multiple decades within both the machine learning and speech communities. In alignment with the prevailing research approach, scholars extract feature insights from audio data and subsequently employ these insights across a range of classifiers, including: hidden Markov models  [12] , convolutional recurrent network  [16] , SVM  [13] , hierarchical binary decision tree  [8] , gaussian mixture  [3] , nerual network  [14] . Much of the aforementioned works relied on context to furnish additional information for correcting and inferring emotional content extracted from the data. The mining and analysis of emotional information from singlesentence audio data can pose more significant challenges. Xu et al.  [20]  introduced an attention-based network designed for aligning textual and audio information, along with feature extraction. Yoon  [21, 22]    Our Approach\n\nIn this section, we describe our emotion recognition model. This model employs three distinct modalities of data as input sources: acoustic signals, word embeddings, and BERT-encoded embeddings. Initially, each modality is processed separately. Subsequently, all the features from the various input modalities are combined using a coattention layer. Finally, Linear layers are employed to produce the predictions. The overall model structure is shown in Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Input",
      "text": "This model has three modalities as input. Regarding the acoustic input, we utilized the mel-spectrogram, which is generated by applying a Short-Time Fourier Transform (STFT) to the audio signal. The mel-spectrogram provides a visual representation of the energy in different frequency bands of an audio signal changing over time, with the frequency axis adjusted to better match the human auditory perception. ASR is a technology that converts audio data into text data, facilitating the transcription and understanding of spoken words by machines. We use the ASR module to extract recognized text from audio signals. For text input, we use text embedding to learn text features directly. Meanwhile, we incorporate pre-trained BERT to extract transcription features from external knowledge.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modality Pre-Process",
      "text": "After retrieving the mel-spectrogram of the audio signals, we apply a classic Conv-BatchNorm-ReLU structure to extract features in both the time and frequency dimensions. Then, an LSTM layer is applied to extract deeper features in the time dimension. Additionally, the word embeddings have a better time structure and are more straightforward in each time slot. Hence, an LSTM is applied to the word embeddings before using a 1D-convolution layer to incorporate the information from the entire timeline. The feature extracted from BERT is a 768-dimensional vector. As it is already wellstructured and contains abundant information, we applied a Linear layer to modify its size for subsequent multi-modal fusion and information compression.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Modal Fusion",
      "text": "Given the presence of three modalities, we need two rounds of fusion to comprehensively combine all the information extracted from these different modalities, and determining the order of fusion is a significant consideration. In our model, we first fuse the audio features and word embedding features. Their akin temporal structures make them suitable for initial fusion, as this process enhances the temporal dimension by leveraging their shared characteristics to amplify common information and compensate for missing data unique to one modality. Subsequently, the time-structured feature mentioned earlier is fused with the BERT-encoded feature, incorporating external knowledge from the outside world to in-dataset knowledge. In each fusion, there are two stages: extracting additional features from one modality with knowledge from another modality and then merging these additionally extracted features into a single representation.\n\nIn the first stage, we employed the co-attention layer to convey the presence of another modality to each modality. The structure of co-attention layer is as shown in Fig.  2 . Inspired by  [23] , we employed the Encoder-Decoder structure to stack multiple layers of attention modules. In the co-attention layer, the first modality employs self-attention alone to extract deeper information from itself. Following that, the second modality goes through a self-attention operation, during which a guided-attention step is conducted to extract more information while considering both modalities. In contrast to simply using the output of the self-attention from another modality at the same depth as the input for guidedattention, leveraging the final output of the Self-attention layers can offer more enriched information and a more accurate guide. Both self-attention and guidedattention are based on the attention mechanism  [17] . The attention module aids in constructing a holistic perspective of the entire period during the speech. The attention consists of a query q, a key k and a value v:\n\nIn the self-attention, all of q, k, and v are from the same modality. However, in guidedattention, the v and k are from the same modality while q is from another modality. The first stage of the two fusion is the same, yet they diverge in the second stage. Considering the similarity of time structures, for the fusion between features from audio data and word embeddings, we employ a straightforward element-wise addition. This approach enhances their temporal structure and reduces the feature size compared to concatenation. In the second fusion, the features are dissimilar and lack a shared temporal structure, which leads to lossy and disorganized information when using elementwise addition. Consequently, concatenation is employed to retain more information, which is crucial for effectively leveraging both in-dataset knowledge and externalworld knowledge. Following the ultimate fusion, we applied additional self-attention to comprehensively process the collective information from all modalities and proceed to make predictions using a two-layer MLP.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Fig. 2. The Architecture Of The Co-Attention Layer",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Self-Attention",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Features Of Modality 1",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Features Of Modality 2",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Co-Attention Layer",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Guided-Attention",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Self-Attention",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Guided-Attention",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Self-Attention",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Guided-Attention",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Self-Attention",
      "text": "Self-Attention Self-Attention",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Contrastive Learning",
      "text": "Through our examination of misclassified cases in current state-of-the-art models, we identified that the ambiguity in the emotions expressed by actors is another factor hindering the model from learning accurate features. It is common to observe that a person's emotions can be complex, even involving contradictory feelings simultaneously. However, datasets with labels assigned to a single emotion as the ground truth may be misleading in capturing the presence of other coexisting emotions. Furthermore, employing traditional cross-entropy loss during model training mechanically steers the model to predict a probability of 1 only for the labeled emotion, penalizing predictions with non-zero probabilities for other emotions. This situation can significantly perplex the model, especially in cases where multiple emotions coexist. Moreover, stemming from naturalistic conversations in daily life, our dataset exhibits an imbalanced distribution of labels. Specifically, there is a pronounced prevalence of sentences labeled as neutral, contrasting with a scarcity of instances labeled as surprise. Consequently, we advocate for the implementation of a contrastive learning loss as a regulatory measure to alleviate the impact of multiple emotions and mitigate data imbalances. Contrastive learning is a training technique that originated from unsupervised learning. Supervised learning studies  [7]  have also demonstrated their effectiveness, utilizing samples from the same class as positive samples and others as negative samples. The loss used is the following:\n\nHere, I is the set of classes, A(i) is the batch of samples contrasting with feature zi, P(i) is the set of positive samples of feature zi in A(i), i.e. samples with the same label.\n\nThe loss function is characterized by a vague description, suggesting that the feature extracted from a given sample should exhibit proximity to features extracted from positive samples while maintaining distance from features of other negative samples. Unlike traditional supervised learning, which prescribes a specific point in a lower dimension for a sample, contrastive learning defines positions in high-dimensional space that a sample should either approach or diverge from. This can mitigate the impact of labels, thereby diminishing the influence of multiple emotions.\n\nAs depicted in Fig.  3  and Fig.  1 , the contrastive learning loss is computed from the feature projector's output, whereas the conventional cross-entropy loss relies on the output of the predictor. The feature projector and the predictor are both one-layer MLP. Therefore, the final loss can be represented as\n\n1 + ùõº where Œ± is a hyperparameter to control the importance of contrastive learning loss in the final loss.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Data Augmentation",
      "text": "We formulate data augmentation strategies to mitigate the impact of noise, thereby improving the overall generalization of the model. In detail, we augment the audio signals in three ways: adding noise based on SNR, applying pitch shifts, and employing time stretching. When adding noise to the audio feature, we use an SNR of 30dB and randomly initialize the noise in Gaussian distribution. The pitch shift and time stretch are implemented by the librosa. In IEMOCAP, to increase the contrastive samples, we take advantage of the Dropout layers in our model. We run the prediction twice in one epoch to generate different features from the same sample. Also, as described in the previous section, we adopted MoCo  [6]  with size 16384.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "In addition to utilizing Chinese as the primary language, a key distinction between our dataset and existing ones is that we gather real-world data rather than employing actors to simulate various emotions. We collected 7,477 daily conversations from over 100 different people to create the dataset. For each sample, we hired several professional emotion analysis experts to analyze the data emotion and get artificial emotion classification labels (i.e., angry, fear, happy, neutral, sad, surprise) as ground truth. Fig.  4  presented the distribution of our dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Negative SampleÔºö",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multi-Modal Model",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this study, we implement a prototype of a multi-modal emotion recognition algorithm and evaluate the performance on the VCEMO dataset. A server equipped with 188GB RAM and a 48.0GB VRAM's NVIDIA TESLA A40 is used for the whole computation for the system.\n\nTraining We trained the models in both our proposed dataset VCEMO and another public dataset IEMOCAP. In both datasets, the model was trained for 50 steps with a batch size of 256. The optimizer used is Adam. Also, as a mostly used setting, the feature projector projects the feature into a 128-d vector. The temperature of contrastive learning loss t is 1. In VCEMO, we set learning rate to 1e-5, weight decay to 1e-3 and Œ± to 0.1 while using 1e-4, 0 and 100 in IEMOCAP.\n\nWord Embeddings We utilize a 300-dimensional GloVe  [10]  pre-trained embedding obtained from spaCy to encode the transcription into fixed-length vectors.\n\nEvaluation Metrics Our dataset supports two kinds of setups. The first setup uses all samples for a 6-way classification, while the other setup only uses 4 classes. The 4 classes include angry, happy, neutral, and sad, which is a common setting for emotion recognition. We use two metrics to measure the performance of models comprehensively. The first metric is the accuracy of classification. Besides that, we adopt the F1score as another metric to provide a more balanced evaluation of the model's performance. The F1-score is of the form: ùêπ1 = 2 ‚ãÖ (ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ ‚ãÖ ùëüùëíùëêùëéùëôùëô) ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ + ùëüùëíùëêùëéùëôùëô By considering both precision and recall, the F1-score can reflect the bias of model prediction to show whether a model achieves high accuracy by predicting those majority classes. In IEMOCAP, we measured metrics following existing works. Weighted accuracy and unweighted accuracy are both considered to evaluate our model and existing works.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Micro Benchmark",
      "text": "Model Comparison In VCEMO, we split the dataset into 8/1/1 for train/val/test setting. We trained our model in the training part consisting 80% data of the dataset. The final model is chosen according to their performance on the 10% validation part. The test model is only used when the final model is determined. To show the effectiveness of our dataset and evaluate the performance of our model, we implement three other models as a comparison (i.e., Xu's model  [20] , UMONS  [2]  and Yoon's model  [21] ). The result of different models is shown in Table  1 . Our model surpasses all other models in performance. In contrast to these models, our model can leverage external knowledge provided by BERT embeddings. Furthermore, our model enables improved multimodal fusion through enhanced attention mechanisms. In IEMOCAP, we followed the previous works using a 5-fold cross-validation. Each session in IEMOCAP will be used as a validation set once when training on the other 4 sessions. The final result is the average of all 5-fold results. Furthermore, when using contrastive learning regulation, our model is even better than CME  [9]  which requires addition alignment between transcription and acoustic signal.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Experiment",
      "text": "To further understand the effect of each modality, we performed an ablation study based on the 4-classes setup. The result is presented in Table  3 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Impact Of Transcription Modality",
      "text": "Theoretically, all the information presented in the text should also be contained within audio signals, suggesting that using only audio signals ought to outperform using only text modality. However, it's noteworthy that using only word embeddings outperformed using only one of the other two modalities. The disappointment with the result of using only acoustic signals may be due to that the information in audio signals is more challenging to extract, making it harder for the model to discern what is essential from the abundance of information. And using only BERT embeddings is slightly worse than using only acoustic signals. The reason is that encoding transcription with a pre-trained BERT model could cause a loss of information that is helpful in downstream tasks while trivial in upstream tasks. Therefore, word embeddings contain origin features and are easiest to extract, leading to a significant improvement in performance by around 4%. This gives us a hint that utilizing a text modality could help the model effectively extract features from the audio signals.\n\nImpact of BERT When comparing experiments that only differ in the use of word embeddings or BERT embeddings, it's evident that using word embeddings outperforms using BERT embeddings in both single-modal and multimodal settings with the audio signal. This indicates that the knowledge within the database is still more important than external knowledge. However, adding BERT embeddings to word embeddings consistently improves performance by 1.5%, demonstrating that external knowledge can compensate for missing features from internal knowledge.\n\nImpact of contrastive learning regulation In all benchmarks, additional contrastive learning regulation does improve the performance of our model. Especially in IEMOCAP, we can see it can significantly improve the performance of our model by over 1.5%. This is consistent with our expectation that contrastive learning regulation can reduce the effect of multi-label emotion recognition. Considering that IEMOCAP is using a much larger Œ±, the result indirectly suggests that the ground truth of samples of IEMOCAP is more vague than our dataset VCEMO.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose the emotion recognition dataset VCEMO for Chinese voiceprints. Compared with existing Chinese datasets, the proposed dataset is richer and more diversified in terms of voice tones and textual contents, containing more than 100 users and 7747 textual contents; the samples are all from daily conversations, which is closer to real-life scenarios. In addition, this paper proposes a multimodal emotion recognition model, which utilizes the co-attention structure for multimodal fusion. The contrastive-learning-based regulation training system achieves significantly better performance than SOTA on the VCEMO and IEMOCAP datasets.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The multi-modal model for emotion recognition.",
      "page": 3
    },
    {
      "caption": "Figure 2: Inspired by [23], we employed the Encoder-Decoder structure to stack multiple lay-",
      "page": 5
    },
    {
      "caption": "Figure 2: The architecture of the co-attention layer",
      "page": 5
    },
    {
      "caption": "Figure 3: and Fig.  1, the contrastive learning loss is computed from the",
      "page": 6
    },
    {
      "caption": "Figure 3: The pipeline of the contrastive learning.",
      "page": 7
    },
    {
      "caption": "Figure 4: presented the distribution of our dataset.",
      "page": 7
    },
    {
      "caption": "Figure 4: Dataset distribution.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": "dingdian94@sjtu.edu.cn; lixiaoshan@shdatagroup.com"
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": "model  as  a  benchmark,  which  effectively  fuses  speech,"
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": ""
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": "Code and dataset will be released for research."
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints."
        },
        {
          "4 Shanghai Data Group Co., Ltd, Shanghai, China 200011": "Introduction"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": "Introduction"
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": "Audio  data  plays  a  fundamental  and  irreplaceable  role  in  our  comprehension  of  the"
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": ""
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": ""
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": ""
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": ""
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": ""
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": ""
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": ""
        },
        {
          "Keywords: Speech Emotion Recognition, Multi-modal, Chinese Voiceprints.": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "back on customers‚Äô moods or assess business interactions based on customers‚Äô emo-",
          "J. Tang et al.": ""
        },
        {
          "2": "tional states. Simultaneously, the software application can adapt and enhance user ex-",
          "J. Tang et al.": ""
        },
        {
          "2": "periences by implementing appropriate behaviors through real-time monitoring of the",
          "J. Tang et al.": ""
        },
        {
          "2": "user‚Äôs emotions [24].",
          "J. Tang et al.": ""
        },
        {
          "2": "",
          "J. Tang et al.": "With over a billion native speakers and a rich cultural heritage, Chinese has undeni-"
        },
        {
          "2": "ably emerged as a highly popular language on the global stage. Hence, the application",
          "J. Tang et al.": ""
        },
        {
          "2": "of emotion recognition for Chinese audio holds great promise. However, there exists a",
          "J. Tang et al.": ""
        },
        {
          "2": "restricted amount of Chinese corpus data available for model training in the context of",
          "J. Tang et al.": ""
        },
        {
          "2": "emotion recognition. Furthermore, current methods are typically trained and evaluated",
          "J. Tang et al.": ""
        },
        {
          "2": "on English datasets, lacking specific processing and optimization for Chinese data.",
          "J. Tang et al.": ""
        },
        {
          "2": "",
          "J. Tang et al.": "Consequently,  we  collected  a  large  Chinese  conversation  sentiment  corpus  called"
        },
        {
          "2": "VCEMO for the single-sentence Chinese emotion recognition task. The dataset consists",
          "J. Tang et al.": ""
        },
        {
          "2": "of single-sentence conversations of everyday life and has several advantages:",
          "J. Tang et al.": ""
        },
        {
          "2": "",
          "J. Tang et al.": "1).  Rich  voiceprint  information:  Considering  that  the  collection  of  voice  infor-"
        },
        {
          "2": "mation in previous datasets (e.g., CASIA [18], IEMOCAP [1]) has often relied on a",
          "J. Tang et al.": ""
        },
        {
          "2": "specific few professional readers or professional actors, only a few people‚Äôs pronunci-",
          "J. Tang et al.": ""
        },
        {
          "2": "ation information as well as voiceprint features are present in the datasets. Our dataset",
          "J. Tang et al.": ""
        },
        {
          "2": "contains daily speech data from more than 100 people, including a wide range of Chi-",
          "J. Tang et al.": ""
        },
        {
          "2": "nese pronunciation accents and spoken language features.",
          "J. Tang et al.": ""
        },
        {
          "2": "",
          "J. Tang et al.": "2). Abundant text information: The textual content of the dataset is exclusively"
        },
        {
          "2": "sourced  from  spontaneous  conversations  in  everyday  life.  Consequently,  there  exist",
          "J. Tang et al.": ""
        },
        {
          "2": "substantial disparities between these texts, and they are abundant in information.",
          "J. Tang et al.": ""
        },
        {
          "2": "",
          "J. Tang et al.": "3). Adaptability to multi-modal fusion: Given that the data originate exclusively"
        },
        {
          "2": "from everyday conversations and individuals naturally employ various textual expres-",
          "J. Tang et al.": ""
        },
        {
          "2": "sions to convey their inner sentiments based on their emotions, we can effectively lev-",
          "J. Tang et al.": ""
        },
        {
          "2": "erage the multi-modal fusion of audio signals and textual information for the emotion",
          "J. Tang et al.": ""
        },
        {
          "2": "recognition task.",
          "J. Tang et al.": ""
        },
        {
          "2": "",
          "J. Tang et al.": "Contemporary methods [5, 4, 15, 20, 21,19] commonly employ neural networks for"
        },
        {
          "2": "tasks such as emotion recognition, as well as for effective feature extraction and clas-",
          "J. Tang et al.": ""
        },
        {
          "2": "sification of data. Given the notable distinctions between Chinese and English, these",
          "J. Tang et al.": ""
        },
        {
          "2": "methods lack specific processing tailored to Chinese information. Hence, leveraging",
          "J. Tang et al.": ""
        },
        {
          "2": "the extensive Chinese corpus dataset VCEMO, we introduce a novel multimodal model",
          "J. Tang et al.": ""
        },
        {
          "2": "for emotion recognition. Automatic speech recognition (ASR) [11] is adapted to con-",
          "J. Tang et al.": ""
        },
        {
          "2": "vert audio signals into Chinese text messages. For Chinese text, we use the pre-trained",
          "J. Tang et al.": ""
        },
        {
          "2": "Chinese BERT architecture for processing. In addition, we utilize text embedding for",
          "J. Tang et al.": ""
        },
        {
          "2": "additional emotion feature extraction from Chinese text. Finally, the co-attention struc-",
          "J. Tang et al.": ""
        },
        {
          "2": "ture is employed to fuse multi-modal data features.",
          "J. Tang et al.": ""
        },
        {
          "2": "",
          "J. Tang et al.": "Furthermore, given that the VCEMO dataset originates from everyday conversations,"
        },
        {
          "2": "there  is  an  uneven  distribution  of  emotional  data  within  the  dataset.  Additionally,  a",
          "J. Tang et al.": ""
        },
        {
          "2": "single audio sentence may encompass diverse emotional expressions, making it chal-",
          "J. Tang et al.": ""
        },
        {
          "2": "lenging for a singular emotional label to fully convey its comprehensive emotional in-",
          "J. Tang et al.": ""
        },
        {
          "2": "formation. To tackle the aforementioned issue, we employ a contrastive-learning-based",
          "J. Tang et al.": ""
        },
        {
          "2": "regulation  for  training  our  model.  Eventually,  experimental  tests  have  demonstrated",
          "J. Tang et al.": ""
        },
        {
          "2": "that our model has significantly better emotion recognition performance on VCEMO",
          "J. Tang et al.": ""
        },
        {
          "2": "than previously studied models.",
          "J. Tang et al.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "O\nverall, our contributions are as follows:": "‚Äì We produce a new Chinese daily conversational corpus dataset for emotion recog-"
        },
        {
          "O\nverall, our contributions are as follows:": "nition, called VCEMO, containing 7477 samples of audio signals from over 100 indi-"
        },
        {
          "O\nverall, our contributions are as follows:": "viduals."
        },
        {
          "O\nverall, our contributions are as follows:": "‚Äì We propose a multi-modal model for acoustic data and text data (word embeddings"
        },
        {
          "O\nverall, our contributions are as follows:": "and  pre-trained  BERT  embeddings)  using  the  co-attention  structure  for  multi-modal"
        },
        {
          "O\nverall, our contributions are as follows:": "feature fusion."
        },
        {
          "O\nverall, our contributions are as follows:": "‚Äì We employ a contrastive-learning-based regulation to train and optimize models,"
        },
        {
          "O\nverall, our contributions are as follows:": "mitigating issues related to sample imbalance and under-representation of individual"
        },
        {
          "O\nverall, our contributions are as follows:": "labels."
        },
        {
          "O\nverall, our contributions are as follows:": "‚Äì  Extensive  experiments  show  that  our  model  has  SOTA  performance  on  the"
        },
        {
          "O\nverall, our contributions are as follows:": "VCEMO and IEMOCAP datasets."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "3",
          "J. Tang et al.": "Our Approach"
        },
        {
          "4": "In this section, we describe our emotion recognition model. This model employs three",
          "J. Tang et al.": ""
        },
        {
          "4": "distinct  modalities  of  data  as  input  sources:  acoustic  signals,  word  embeddings,  and",
          "J. Tang et al.": ""
        },
        {
          "4": "BERT-encoded  embeddings.  Initially,  each  modality  is  processed  separately.  Subse-",
          "J. Tang et al.": ""
        },
        {
          "4": "quently, all the features from the various input modalities are combined using a co-",
          "J. Tang et al.": ""
        },
        {
          "4": "attention  layer.  Finally,  Linear  layers  are  employed  to  produce  the  predictions.  The",
          "J. Tang et al.": ""
        },
        {
          "4": "overall model structure is shown in Fig.  1.",
          "J. Tang et al.": ""
        },
        {
          "4": "3.1",
          "J. Tang et al.": "Modality Input"
        },
        {
          "4": "This model has three modalities as input. Regarding the acoustic input, we utilized the",
          "J. Tang et al.": ""
        },
        {
          "4": "mel-spectrogram,  which  is  generated  by  applying  a  Short-Time  Fourier  Transform",
          "J. Tang et al.": ""
        },
        {
          "4": "(STFT) to the audio signal. The mel-spectrogram provides a visual representation of",
          "J. Tang et al.": ""
        },
        {
          "4": "the energy in different frequency bands of an audio signal changing over time, with the",
          "J. Tang et al.": ""
        },
        {
          "4": "frequency axis adjusted to better match the human auditory perception.",
          "J. Tang et al.": ""
        },
        {
          "4": "",
          "J. Tang et al.": "ASR is a technology that converts audio data into text data, facilitating the transcrip-"
        },
        {
          "4": "tion and understanding of spoken words by machines. We use the ASR module to ex-",
          "J. Tang et al.": ""
        },
        {
          "4": "tract recognized text from audio signals. For text input, we use text embedding to learn",
          "J. Tang et al.": ""
        },
        {
          "4": "text features directly. Meanwhile, we incorporate pre-trained BERT to extract transcrip-",
          "J. Tang et al.": ""
        },
        {
          "4": "tion features from external knowledge.",
          "J. Tang et al.": ""
        },
        {
          "4": "3.2",
          "J. Tang et al.": "Modality Pre-process"
        },
        {
          "4": "After  retrieving  the  mel-spectrogram  of  the  audio  signals,  we  apply  a  classic  Conv-",
          "J. Tang et al.": ""
        },
        {
          "4": "BatchNorm-ReLU structure to extract features in both the time and frequency dimen-",
          "J. Tang et al.": ""
        },
        {
          "4": "sions. Then, an LSTM layer is applied to extract deeper features in the time dimension.",
          "J. Tang et al.": ""
        },
        {
          "4": "Additionally, the word embeddings have a better time structure and are more straight-",
          "J. Tang et al.": ""
        },
        {
          "4": "forward in each time slot. Hence, an LSTM is applied to the word embeddings before",
          "J. Tang et al.": ""
        },
        {
          "4": "using a 1D-convolution layer to incorporate the information from the entire timeline.",
          "J. Tang et al.": ""
        },
        {
          "4": "The feature extracted from BERT is a 768-dimensional vector. As it is already well-",
          "J. Tang et al.": ""
        },
        {
          "4": "structured and contains abundant information, we applied a Linear layer to modify its",
          "J. Tang et al.": ""
        },
        {
          "4": "size for subsequent multi-modal fusion and information compression.",
          "J. Tang et al.": ""
        },
        {
          "4": "3.3",
          "J. Tang et al.": "Multi-modal fusion"
        },
        {
          "4": "Given the presence of three modalities, we need two rounds of fusion to comprehen-",
          "J. Tang et al.": ""
        },
        {
          "4": "sively combine all the information extracted from these different modalities, and deter-",
          "J. Tang et al.": ""
        },
        {
          "4": "mining the order of fusion is a significant consideration. In our model, we first fuse the",
          "J. Tang et al.": ""
        },
        {
          "4": "audio features and word embedding features. Their akin temporal structures make them",
          "J. Tang et al.": ""
        },
        {
          "4": "suitable for initial fusion, as this process enhances the temporal dimension by leverag-",
          "J. Tang et al.": ""
        },
        {
          "4": "ing  their  shared  characteristics  to  amplify  common  information  and  compensate  for",
          "J. Tang et al.": ""
        },
        {
          "4": "missing data unique to one modality. Subsequently, the time-structured feature men-",
          "J. Tang et al.": ""
        },
        {
          "4": "tioned  earlier",
          "J. Tang et al.": "is \nfused  with"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "In the first stage, we employed the co-attention layer to convey the presence of an-",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "ùëûùëò!",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëû, ùëò, ùë£) = ùë†ùëúùëìùë°ùëöùëéùë•(\n)ùë£",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "‚àöùëë",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "The first stage of the two fusion is the same, yet they diverge in the second stage.",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "5": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "Features of \nFeatures of",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "3.4",
          "J. Tang et al.": "Contrastive Learning"
        },
        {
          "6": "Through our examination of misclassified cases in current state-of-the-art models, we",
          "J. Tang et al.": ""
        },
        {
          "6": "identified that the ambiguity in the emotions expressed by actors is another factor hin-",
          "J. Tang et al.": ""
        },
        {
          "6": "dering the model from learning accurate features. It is common to observe that a per-",
          "J. Tang et al.": ""
        },
        {
          "6": "son‚Äôs emotions can be complex, even involving contradictory feelings simultaneously.",
          "J. Tang et al.": ""
        },
        {
          "6": "However, datasets with labels assigned to a single emotion as the ground truth may be",
          "J. Tang et al.": ""
        },
        {
          "6": "misleading in capturing the presence of other coexisting emotions. Furthermore, em-",
          "J. Tang et al.": ""
        },
        {
          "6": "ploying  traditional  cross-entropy  loss  during  model  training  mechanically  steers  the",
          "J. Tang et al.": ""
        },
        {
          "6": "model to predict a probability of 1 only for the labeled emotion, penalizing predictions",
          "J. Tang et al.": ""
        },
        {
          "6": "with non-zero probabilities for other emotions. This situation can significantly perplex",
          "J. Tang et al.": ""
        },
        {
          "6": "the model, especially in cases where multiple emotions coexist. Moreover, stemming",
          "J. Tang et al.": ""
        },
        {
          "6": "from naturalistic conversations in daily life, our dataset exhibits an imbalanced distri-",
          "J. Tang et al.": ""
        },
        {
          "6": "bution of labels. Specifically, there is a pronounced prevalence of sentences labeled as",
          "J. Tang et al.": ""
        },
        {
          "6": "neutral, contrasting with a scarcity of instances labeled as surprise.",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": "Consequently, we advocate for the implementation of a contrastive learning loss as"
        },
        {
          "6": "a  regulatory  measure  to  alleviate  the  impact  of  multiple  emotions  and  mitigate  data",
          "J. Tang et al.": ""
        },
        {
          "6": "imbalances. Contrastive learning is a training technique that originated from unsuper-",
          "J. Tang et al.": ""
        },
        {
          "6": "vised learning. Supervised learning studies [7] have also demonstrated their effective-",
          "J. Tang et al.": ""
        },
        {
          "6": "ness, utilizing samples from the same class as positive samples and others as negative",
          "J. Tang et al.": ""
        },
        {
          "6": "samples. The loss used is the following:",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": "ùêø\"#$%&‚Äô = ‚àí 7"
        },
        {
          "6": "",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": "(‚àà/"
        },
        {
          "6": "Here, I is the set of classes, A(i) is the batch of samples contrasting with feature zi, P(i)",
          "J. Tang et al.": ""
        },
        {
          "6": "is the set of positive samples of feature zi in A(i), i.e. samples with the same label.",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": "The loss function is characterized by a vague description, suggesting that the feature"
        },
        {
          "6": "extracted from a given sample should exhibit proximity to features extracted from pos-",
          "J. Tang et al.": ""
        },
        {
          "6": "itive samples while maintaining distance from features of other negative samples. Un-",
          "J. Tang et al.": ""
        },
        {
          "6": "like traditional supervised learning, which prescribes a specific point in a lower dimen-",
          "J. Tang et al.": ""
        },
        {
          "6": "sion for a sample, contrastive learning defines positions in high-dimensional space that",
          "J. Tang et al.": ""
        },
        {
          "6": "a sample should either approach or diverge from. This can mitigate the impact of labels,",
          "J. Tang et al.": ""
        },
        {
          "6": "thereby diminishing the influence of multiple emotions.",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": "As depicted in Fig.  3 and Fig.  1, the contrastive learning loss is computed from the"
        },
        {
          "6": "feature  projector‚Äôs  output,  whereas  the  conventional  cross-entropy  loss  relies  on  the",
          "J. Tang et al.": ""
        },
        {
          "6": "output of the predictor. The feature projector and the predictor are both one-layer MLP.",
          "J. Tang et al.": ""
        },
        {
          "6": "Therefore, the final loss can be represented as",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": ""
        },
        {
          "6": "",
          "J. Tang et al.": ""
        },
        {
          "6": "where Œ± is a hyperparameter to control the importance of contrastive learning loss in",
          "J. Tang et al.": ""
        },
        {
          "6": "the final loss.",
          "J. Tang et al.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig.  3. The pipeline of the contrastive learning.": "Data Augmentation"
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        },
        {
          "Fig.  3. The pipeline of the contrastive learning.": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": "2 ‚ãÖ (ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ ‚ãÖ ùëüùëíùëêùëéùëôùëô)"
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": "ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ + ùëüùëíùëêùëéùëôùëô"
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        },
        {
          "Fig.  4. Dataset distribution.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: Our model surpasses all other models in",
      "data": [
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "n IEMOCAP, we measured metrics following existing works. Weighted accuracy",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "Micro Benchmark",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "In IEMOCAP, we followed the previous works using a 5-fold cross-validation. Each",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "",
          "9": ""
        },
        {
          "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints": "Table 1. Comparison of different models on VCEMO.",
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: Our model surpasses all other models in",
      "data": [
        {
          "Table 1. Comparison of different models on VCEMO.": ""
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "Accuracy"
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "67.40%"
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "66.99%"
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "63.27%"
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "60.96%"
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "59.42%"
        },
        {
          "Table 1. Comparison of different models on VCEMO.": ""
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "Xu‚Äôs"
        },
        {
          "Table 1. Comparison of different models on VCEMO.": ""
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "70.41%"
        },
        {
          "Table 1. Comparison of different models on VCEMO.": "69.52%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Impact of Transcription Modality Theoretically, all the information presented in",
      "data": [
        {
          "10": "4.4",
          "J. Tang et al.": "Ablation Experiment"
        },
        {
          "10": "",
          "J. Tang et al.": "To further understand the effect of each modality, we performed an ablation study"
        },
        {
          "10": "based on the 4-classes setup. The result is presented in Table 3.",
          "J. Tang et al.": ""
        },
        {
          "10": "",
          "J. Tang et al.": "Impact of Transcription Modality Theoretically, all the information presented in"
        },
        {
          "10": "the text should also be contained within audio signals, suggesting that using only audio",
          "J. Tang et al.": ""
        },
        {
          "10": "signals ought to outperform using only text modality. However, it‚Äôs noteworthy that",
          "J. Tang et al.": ""
        },
        {
          "10": "using only word embeddings outperformed using only one of the other two modalities.",
          "J. Tang et al.": ""
        },
        {
          "10": "The disappointment with the result of using only acoustic signals may be due to that",
          "J. Tang et al.": ""
        },
        {
          "10": "the information in audio signals is more challenging to extract, making it harder for the",
          "J. Tang et al.": ""
        },
        {
          "10": "model to discern what is essential from the abundance of information. And using only",
          "J. Tang et al.": ""
        },
        {
          "10": "BERT embeddings is slightly worse than using only acoustic signals. The reason is that",
          "J. Tang et al.": ""
        },
        {
          "10": "encoding  transcription  with  a  pre-trained  BERT  model  could  cause  a  loss  of  infor-",
          "J. Tang et al.": ""
        },
        {
          "10": "mation that is helpful in downstream tasks while trivial in upstream tasks. Therefore,",
          "J. Tang et al.": ""
        },
        {
          "10": "word embeddings contain origin features and are easiest to extract, leading to a signif-",
          "J. Tang et al.": ""
        },
        {
          "10": "icant improvement in performance by around 4%. This gives us a hint that utilizing a",
          "J. Tang et al.": ""
        },
        {
          "10": "text modality could help the model effectively extract features from the audio signals.",
          "J. Tang et al.": ""
        },
        {
          "10": "",
          "J. Tang et al.": "Impact of BERT When comparing experiments that only differ in the use of word"
        },
        {
          "10": "embeddings  or  BERT  embeddings,  it‚Äôs  evident  that  using  word  embeddings  outper-",
          "J. Tang et al.": ""
        },
        {
          "10": "forms using BERT embeddings in both single-modal and multimodal settings with the",
          "J. Tang et al.": ""
        },
        {
          "10": "audio signal. This indicates that the knowledge within the database is still more im-",
          "J. Tang et al.": ""
        },
        {
          "10": "portant than external knowledge. However, adding BERT embeddings to word embed-",
          "J. Tang et al.": ""
        },
        {
          "10": "dings  consistently",
          "J. Tang et al.": "improves  performance  by  1.5%,  demonstrating"
        },
        {
          "10": "knowledge can compensate for missing features from internal knowledge.",
          "J. Tang et al.": ""
        },
        {
          "10": "",
          "J. Tang et al.": "Impact of contrastive learning regulation In all benchmarks, additional contras-"
        },
        {
          "10": "tive  learning  regulation  does  improve  the  performance  of  our  model.  Especially  in",
          "J. Tang et al.": ""
        },
        {
          "10": "IEMOCAP, we can see it can significantly improve the performance of our model by",
          "J. Tang et al.": ""
        },
        {
          "10": "over 1.5%. This is consistent with our expectation that contrastive learning regulation",
          "J. Tang et al.": ""
        },
        {
          "10": "can reduce the effect of multi-label emotion recognition. Considering that IEMOCAP",
          "J. Tang et al.": ""
        },
        {
          "10": "is using a much larger Œ±, the result indirectly suggests that the ground truth of samples",
          "J. Tang et al.": ""
        },
        {
          "10": "of IEMOCAP is more vague than our dataset VCEMO.",
          "J. Tang et al.": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: Impact of Transcription Modality Theoretically, all the information presented in",
      "data": [
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": ""
        },
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": "Used modality"
        },
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": "Embeddings"
        },
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": "BERT"
        },
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": "Acoustic"
        },
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": "Embeddings + BERT"
        },
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": "Embedding + Acoustic"
        },
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": "BERT + Acoustic"
        },
        {
          "Table 3. Ablation study of using different modalities: Embeddings means the simple transcrip-": "Embeddings + BERT + Acoustic"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "",
          "Conclusion": "In  this  paper,  we  propose  the  emotion  recognition  dataset  VCEMO  for  Chinese"
        },
        {
          "5": "voiceprints. Compared with existing Chinese datasets, the proposed dataset is richer",
          "Conclusion": ""
        },
        {
          "5": "and more diversified in terms of voice tones and textual contents, containing more than",
          "Conclusion": ""
        },
        {
          "5": "100 users and 7747 textual contents; the samples are all from daily conversations, which",
          "Conclusion": ""
        },
        {
          "5": "is closer to real-life scenarios. In addition, this paper proposes a multimodal emotion",
          "Conclusion": ""
        },
        {
          "5": "recognition model, which utilizes the co-attention structure for multimodal fusion. The",
          "Conclusion": ""
        },
        {
          "5": "contrastive-learning-based regulation training system achieves significantly better per-",
          "Conclusion": ""
        },
        {
          "5": "formance than SOTA on the VCEMO and IEMOCAP datasets.",
          "Conclusion": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "1.  Busso, C., Bulut, M., Lee, C.C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J.N., Lee, S.,"
        },
        {
          "References": "Narayanan, S.S.: Iemocap: Interactive emotional dyadic motion capture database. Language"
        },
        {
          "References": "resources and evaluation 42, 335-359 (2008)"
        },
        {
          "References": "2.  Delbrouck, J.B., Tits, N., Brousmiche, M., Dupont, S.: A transformer-based joint-encoding"
        },
        {
          "References": "for emotion recognition and sentiment analysis. ACL 2020 p. 1 (2020)"
        },
        {
          "References": "3.  El Ayadi, M.M., Kamel, M.S., Karray, F.: Speech emotion recognition using Gaussian mix-"
        },
        {
          "References": "ture vector autoregressive models. In: ICASSP. vol. 4, pp. IV-957. IEEE (2007)"
        },
        {
          "References": "4.  Gat, I., Aronowitz, H., Zhu, W., Morais, E., Hoory, R.: Speaker normalization for self-su-"
        },
        {
          "References": "pervised speech emotion recognition. In: ICASSP 2022. pp. 7342-7346. IEEE (2022)"
        },
        {
          "References": "5.  Ghosh, S., Tyagi, U., Ramaneswaran, S., Srivastava, H., Manocha, D.: Mmer: Multimodal"
        },
        {
          "References": "multi-task learning for speech emotion recognition. arXiv preprint arXiv:2203.16794 (2022)"
        },
        {
          "References": "6.  He,  K.,  Fan,  H.,  Wu,  Y.,  Xie,  S.,  Girshick,  R.:Momentumcontrastforunsupervised  visual"
        },
        {
          "References": "representation learning. In: CVPR. pp. 9729‚Äì9738 (2020)"
        },
        {
          "References": "7.  Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C.,"
        },
        {
          "References": "Krishnan, D.: Supervised contrastive learning. NeurlPS 33, 18661‚Äì18673 (2020)"
        },
        {
          "References": "8.  Lee, C.C., Mower, E., Busso, C., Lee, S., Narayanan, S.: Emotion recognition using a hier-"
        },
        {
          "References": "archical binary decision tree approach. Speech Communication 53(9-10), 1162‚Äì1171 (2011)"
        },
        {
          "References": "9.  Li,  H.,  Ding,  W.,  Wu,  Z.,  Liu,  Z.:  Learning  fine-grained  cross-modality  excitement  for"
        },
        {
          "References": "speech emotion recognition. arXiv preprint arXiv:2010.12733 (2020)"
        },
        {
          "References": "10.  Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word representation."
        },
        {
          "References": "In: EMNLP. pp. 1532‚Äì1543 (2014)"
        },
        {
          "References": "11.  Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutskever, I.:Robust speech"
        },
        {
          "References": "recognition via large-scale weak supervision. In: ICML. pp. 28492‚Äì28518. PMLR (2023)"
        },
        {
          "References": "12.  Schuller, B., Rigoll, G., Lang, M.: Hidden Markov model-based speech emotion recognition."
        },
        {
          "References": "In: ICASSP. vol. 2, pp. II‚Äì1. IEEE (2003)"
        },
        {
          "References": "13.  Seehapoch, T., Wongthanavasu, S.: Speech emotion recognition using support vector ma-"
        },
        {
          "References": "chines. In: 2013 KST. pp. 86‚Äì91. IEEE (2013)"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "",
          "J. Tang et al.": "14.  Stuhlsatz, A., Meyer, C., Eyben, F., Zielke, T., Meier, G., Schuller, B.: Deep neural networks"
        },
        {
          "12": "",
          "J. Tang et al.": "for  acoustic  emotion  recognition:  Raising  the  benchmarks.  In:  ICASSP.  pp.  5688‚Äì5691."
        },
        {
          "12": "IEEE (2011)",
          "J. Tang et al.": ""
        },
        {
          "12": "15.  Triantafyllopoulos, A., Liu, S., Schuller, B.W.: Deep speaker conditioning for speech emo-",
          "J. Tang et al.": ""
        },
        {
          "12": "",
          "J. Tang et al.": "tion recognition. In: ICME. pp. 1‚Äì6. IEEE (2021)"
        },
        {
          "12": "",
          "J. Tang et al.": "16.  Trigeorgis,  G.,  Ringeval,  F.,  Brueckner,  R.,  Marchi,  E.,  Nicolaou,  M.A.,  Schuller,  B.,"
        },
        {
          "12": "",
          "J. Tang et al.": "Zafeiriou, S.: Adieu features? end-to-end speech emotion recognition using a deep convolu-"
        },
        {
          "12": "",
          "J. Tang et al.": "tional recurrent network. In: ICASSP. pp. 5200‚Äì5204. IEEE (2016)"
        },
        {
          "12": "",
          "J. Tang et al.": "17.  Vaswani,  A.,  Shazeer,  N.,  Parmar,  N.,  Uszkoreit,  J.,  Jones,  L.,  Gomez,  A.N.,  Kaiser,  L.,"
        },
        {
          "12": "",
          "J. Tang et al.": "Polosukhin, I.: Attention is all you need. NeurlPS 30 (2017)"
        },
        {
          "12": "",
          "J. Tang et al.": "18.  Wang, K., An, N., Li, B.N., Zhang, Y., Li, L.: Speech emotion recognition using Fourier"
        },
        {
          "12": "",
          "J. Tang et al.": "parameters. IEEE Transactions on affective computing 6(1), 69‚Äì75 (2015)"
        },
        {
          "12": "",
          "J. Tang et al.": "19.  Wang,  Y.,  Boumadane,  A.,  Heba,  A.:  A  fine-tuned  wav2vec  2.0/Hubert  benchmark  for"
        },
        {
          "12": "",
          "J. Tang et al.": "speech emotion recognition, speaker verification and spoken language understanding. arXiv"
        },
        {
          "12": "",
          "J. Tang et al.": "preprint arXiv:2111.02735 (2021)"
        },
        {
          "12": "",
          "J. Tang et al.": "20.  Xu, H., Zhang, H., Han, K., Wang, Y., Peng, Y., Li, X.: Learning alignment for multimodal"
        },
        {
          "12": "",
          "J. Tang et al.": "emotion recognition from speech. Proc. Interspeech 2019 pp. 3569‚Äì3573 (2019)"
        },
        {
          "12": "",
          "J. Tang et al.": "21.  Yoon, S., Byun, S., Dey, S., Jung, K.: Speech emotion recognition using multi-hop attention"
        },
        {
          "12": "",
          "J. Tang et al.": "mechanism. In: ICASSP 2019. pp. 2822‚Äì2826. IEEE (2019)"
        },
        {
          "12": "",
          "J. Tang et al.": "22.  Yoon, S., Byun, S., Jung, K.: Multimodal speech emotion recognition using audio and text."
        },
        {
          "12": "",
          "J. Tang et al.": "In: 2018 SLT. pp. 112‚Äì118. IEEE (2018)"
        },
        {
          "12": "",
          "J. Tang et al.": "23.  Yu, Z., Yu, J., Cui, Y., Tao, D., Tian, Q.: Deep modular co-attention networks for visual"
        },
        {
          "12": "",
          "J. Tang et al.": "question answering. In: CVPR. pp. 6281‚Äì6290 (2019)"
        },
        {
          "12": "",
          "J. Tang et al.": "24.  Zhao, M., Adib, F., Katabi, D.: Emotion recognition using wireless signals. In: Mobicom."
        },
        {
          "12": "pp. 95‚Äì108 (2016)",
          "J. Tang et al.": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "J Delbrouck",
        "N Tits",
        "M Brousmiche",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using Gaussian mixture vector autoregressive models",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2007",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "I Gat",
        "H Aronowitz",
        "W Zhu",
        "E Morais",
        "R Hoory"
      ],
      "year": "2022",
      "venue": "ICASSP 2022"
    },
    {
      "citation_id": "5",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran",
        "H Srivastava",
        "D Manocha"
      ],
      "year": "2022",
      "venue": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "arxiv": "arXiv:2203.16794"
    },
    {
      "citation_id": "6",
      "title": "Momentumcontrastforunsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Momentumcontrastforunsupervised visual representation learning"
    },
    {
      "citation_id": "7",
      "title": "Supervised contrastive learning",
      "authors": [
        "P Khosla",
        "P Teterwak",
        "C Wang",
        "A Sarna",
        "Y Tian",
        "P Isola",
        "A Maschinot",
        "C Liu",
        "D Krishnan"
      ],
      "year": "2020",
      "venue": "NeurlPS"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C Lee",
        "E Mower",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "9",
      "title": "Learning fine-grained cross-modality excitement for speech emotion recognition",
      "authors": [
        "H Li",
        "W Ding",
        "Z Wu",
        "Z Liu"
      ],
      "year": "2020",
      "venue": "Learning fine-grained cross-modality excitement for speech emotion recognition",
      "arxiv": "arXiv:2010.12733"
    },
    {
      "citation_id": "10",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Glove: Global vectors for word representation"
    },
    {
      "citation_id": "11",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "12",
      "title": "Hidden Markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "ICASSP"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using support vector machines",
      "authors": [
        "T Seehapoch",
        "S Wongthanavasu"
      ],
      "year": "2013",
      "venue": "KST"
    },
    {
      "citation_id": "14",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Deep speaker conditioning for speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "S Liu",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Deep speaker conditioning for speech emotion recognition"
    },
    {
      "citation_id": "16",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "NeurlPS"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using Fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "19",
      "title": "A fine-tuned wav2vec 2.0/Hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/Hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "20",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "22",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "Multimodal speech emotion recognition using audio and text"
    },
    {
      "citation_id": "23",
      "title": "Deep modular co-attention networks for visual question answering",
      "authors": [
        "Z Yu",
        "J Yu",
        "Y Cui",
        "D Tao",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "Deep modular co-attention networks for visual question answering"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition using wireless signals",
      "authors": [
        "M Zhao",
        "F Adib",
        "D Katabi"
      ],
      "year": "2016",
      "venue": "Mobicom"
    }
  ]
}