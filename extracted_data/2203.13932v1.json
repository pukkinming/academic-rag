{
  "paper_id": "2203.13932v1",
  "title": "A Cross-Domain Approach For Continuous Impression Recognition From Dyadic Audio-Visual-Physio Signals",
  "published": "2022-03-25T22:40:53Z",
  "authors": [
    "Yuanchao Li",
    "Catherine Lai"
  ],
  "keywords": [
    "impression recognition",
    "affective computing",
    "multi-task learning",
    "attention",
    "fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The impression we make on others depends not only on what we say, but also, to a large extent, on how we say it. As a sub-branch of affective computing and social signal processing, impression recognition has proven critical in both humanhuman conversations and spoken dialogue systems. However, most research has studied impressions only from the signals expressed by the emitter, ignoring the response from the receiver. In this paper, we perform impression recognition using a proposed cross-domain architecture on the dyadic IMPRESSION dataset. This improved architecture makes use of cross-domain attention and regularization. The cross-domain attention consists of intra-and inter-attention mechanisms, which capture intra-and inter-domain relatedness, respectively. The crossdomain regularization includes knowledge distillation and similarity enhancement losses, which strengthen the feature connections between the emitter and receiver. The experimental evaluation verified the effectiveness of our approach. Our approach achieved a concordance correlation coefficient of 0.770 in competence dimension and 0.748 in warmth dimension.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Besides the carefully conceived speech content, psychological research indicates that people also intentionally control their appearances and behaviors to leave different impressions on the interaction partners according to different scenarios, such as selection, leader-subordinate relationships, job performance, and so on. For example, a person tends to express extroversion during job interviews and show agreeableness in dating  [1, 2, 3] . Social research has pointed out the importance of impressions in human interactions, where it is natural to perceive impressions from the partners via non-verbal behaviors, such as eye gaze, body pose, speaking activity, and prosody variation  [4, 5] . Impressions are often categorized in terms of \"Big Five\" personality traits: extroversion, agreeableness, conscientiousness, neuroticism, and openness. But an impression is more like a formed opinion and some research also uses two dimensions of social perception/cognition: warmth and competence to describe impression. Warmth reflects the intentions of others and includes the meanings such as good-natured, trustworthy, tolerant, friendly, and sincere. Competence reflects the ability of the other to enact his/her intentions and means capable, skillful, intelligent, and confident  [6, 7] .\n\nThe procedure of impression recognition is similar to that of emotion recognition, consisting of two steps: feature extraction and classification/regression. Researchers have used many approaches to predict impressions from human behaviors, such as facial, vocal, and bodily expression.  [8, 9]  used high-level features obtained from body and head motion, speaking activ-ity, along with low-level features extracted from audio to predict the five personality traits.  [10]  extracted visual and vocal features (e.g., eye gaze, head pose, speaking activity, and prosody variation) to characterize the social interaction.  [11]  used Electroencephalogram (EEG), Electrocardiogram (ECG), Galvanic Skin Response (GSR), and facial activity data to recognize personality. Furthermore, human-like speech has been designed for robots and virtual agents, to express different impressions to conduct specific roles. For example,  [12]  examined the influence of backchannel selection on extroversion expression using the virtual agent SAL, which acts as an interlocutor in interaction.  [13]  analyzed the relationship between personality traits and dialogue behaviors, to control utterance amount, backchannel, filler, and switching pause length for the humanoid robot ERICA to express extroversion, emotional instability, and politeness. However, most research has ignored the fact that impression depends not only on the emitter (system/speaker) but also largely on the perception by the receiver (user/listener). That is, impressions should be formed by both of the interaction partners.\n\nTherefore, in this paper, we propose a dyadic impression recognition approach using cross-domain attention and regularization to capture and strengthen related information from the emitter and receiver as there is a perception gap that separates them into two domains of signal sources (explained in Sec 2.2).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Impression Recognition",
      "text": "The majority of impression research focuses on personality impression. Many corpora have been presented and adopted for personality prediction. YouTube Personality  [5]  contains 442 vlogs and 2,210 annotations, providing new findings regarding the suitability of collecting personality impressions from crowdsourcing, the types of personality impressions, their association with social attention, and the level of utilization of non-verbal cues. AMIGOS  [14]  collected participants' EEG, ECG, and GSR signals using wearable sensors, and recorded participants' frontal high definition videos and both RGB and depth full-body videos. Self-assessment of affective levels (valence, arousal, control, familiarity, liking, and basic emotions) were annotated. The SSPNet Speaker Personality  [15]  consists of 640 speech clips for a total of 322 subjects. Each speech clip lasts 10 seconds and was scored by 11 annotators in terms of the Big Five traits using the BFI-10 questionnaire  [16, 17] .\n\nCompared to personality impression, recognition of warmth and competence is understudied. The Noxi corpus  [18]  was collected to investigate the relationship between observed nonverbal cues and first impression formation of warmth and competence  [19] . Noxi is a multilingual dataset of natural dyadic novice-expert interactions, where participants interacted through a screen in different rooms and the experts talked more arXiv:2203.13932v1 [cs.MM] 25 Mar 2022 during the interaction. However, the annotations are from external annotators and do not take into consideration speech content or prosody. Furthermore, there are no physiological signals, eye movements, or self-reported annotations of the receiver.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Fusion",
      "text": "Besides corpora, another major issue of impression recognition is the data fusion approach -a general problem for multimodal affective computing tasks. Traditional early and late modality/feature fusions are being replaced or enhanced by the popular tensor fusion  [20] . Rather than operations on feature level or decision level, tensor fusion deals with features on the hidden-state level, which can better model synchrony, relatedness, and hierarchy  [21] . For example, attention-based and hierarchical tensor fusion approaches have been investigated for features of different levels and proven useful in emotion recognition  [22, 23] . However, we regard the fusion problem as more complex in impression recognition for the following reasons: 1) Unlike emotion expression that almost relies on the emitter, impression relies more on the receiver, which means there is a perception gap between the receiver and emitter. 2) Some existing experiments did not conduct real-time interaction, making it difficult for two signal sources to achieve consistency towards the impression. Thus, we follow prior paths to investigate effective fusion approaches for dyadic impression recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Corpus Description",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dyadic Impression Dataset",
      "text": "To address the issues mentioned in Sec 2.1, the dyadic IM-PRESSION dataset that contains audio, visual, and physiological signals from both the emitter and receiver has been newly publicized  [7] . The dataset consists of 31 dyads, in total 1,890 minutes of synchronized recordings of face videos, speech clips, eye gaze data, and peripheral nervous system physiological signals. Participants reported their formed impressions in the warmth and competence dimensions in real time. In this work, we use Session 1 of the dataset, where participants (receivers) watched Noxi stimuli (emitters) and annotated the stimuli with respect to warmth and competence. The labels are represented by a stepwise continuous groundtruth, which means the participants were allowed to increase/decrease the label value once they felt an impression change. As a result, there is no value range limitation. Session 1 is separated as a trainingvalidation set and a testing set, which has 40 and 10 participants respectively, and each participant watch 13 stimuli. We ignored the testing set because the labels are not yet provided.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Processing",
      "text": "First of all, we selected the features that we need from each modality, as shown in Table  1 . Most of the features we selected are the same as  [7] , except for the physiological modality. Some prior research has proved that raw features can have better performance than hand-crafted features  [24, 25] . Thus, for physiological signals, we used the raw signals without further feature extraction. We expect the neural networks can extract better representations, considering results reported on extracted hand-crafted features have low correlations with the labels  [7] .\n\nSince the sample numbers of each modality are different, we first conducted resampling to make them the same as the label numbers. For the modalities whose sample numbers can be divided by integer, we directly applied the decimate function. Otherwise, the decimate function was first applied to downsample the features to approximate the label sample size. Then a Fast Fourier Transform (FFT) was applied to further downsample the features with a step of current length/target length. Since the window is applied every step, it can also serve as a denoising function to remove some outliers. Taking the eye movement modality of the receiver as an example, decimate function with a factor of two or ten (depending on original sample numbers) was directly applied. For the audio modality of the emitter, decimate function with a factor of three was applied, followed by an FFT with the number of samples that equals the label numbers. Finally, the 13 emitters have a total of 44,923 samples, each with 412 feature dimensions. The 40 receivers has 44,923 * 40 samples, where each sample has 68 feature dimensions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "To address the fusion problem stated in Sec 2.2, we propose a cross-domain approach for dyadic impression recognition, shown in Fig  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Concatenation",
      "text": "We concatenated the features from the emitter and receiver respectively, using early fusion. Though prior research has proposed State-Of-The-Art (SOTA) tensor fusion approaches for multimodal features  [20] , we used early fusion here because the feature dimension of each modality is relatively small, which is not suitable for use alone in deep learning. We denoted the concatenated features of the emitter and receiver as E = {e1, e2, • • • , eK } and R = {r1, r2, • • • , rL}. K and L are the lengths (dimensions) of the emitter features and the receiver features, which equal to 412 and 68. Then we encoded the inputs E and R as fixed-length vectors Z e and Z r .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Blstm Network",
      "text": "We fed the vectors Z e and Z r to a Bi-directional Long Short-Term Memory (BLSTM) network. The BLSTM network encodes global contexts by updating its hidden states recurrently. It takes as input vectors and outputs a sequence of hidden states\n\nis the concatenation of the i-th forward hidden state and the i-th backward hidden state. The forward LSTM reads the input from left to right and the backward LSTM reads the input in reverse.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Domain Attention",
      "text": "Following the BLSTM network, a structured cross-domain attention consisting of two intra-attention and inter-attention net-Figure  1 : Architecture using the proposed cross-domain approach.\n\nworks, aggregates information from the BLSTM hidden states H blstm , and produce four fixed-length encodings: H e , H r , H e→r , and H r→e . For both intra-and inter-attention, we used multi-head self-attention mechanism as follows:\n\nwhere W O , W Q i , W K i , and W V i are trainable parameters. Q, K, and V represent the query, key, and value, respectively, and d is their common dimension. For inter-attention, Q is from one domain (the emitter or receiver), while K and V are from the other. For intra-attention, the three parameters are from the same domain. The value of n is 16, and H denotes the concatenated multi-head representations: H e , H r , H e→r , and H r→e . Next, we concatenated the four representations and passed them to a Fully-Connected (FC) network with a ReLU activation function to generate the recognition outputs Cp and Wp which are competence and warmth predictions.\n\nWe used the cross-domain attention network because impression recognition from the emitter and the receiver can be regarded as different domains in this dataset. In  [7] , the impression labels have high correlations with receiver features, but low correlations with emitter features, indicating that the relatedness between the emitter and receiver is not obvious. This phenomenon is plausible because the receiver responds to the emitter videos without real interaction. Therefore, we need the interattention to find relevant features between the two domains.\n\nInter-attention (also known as co-attention)  [26]  has been adopted in affective computing over the recent years  [22, 27, 28] . It exchanges key-value pairs in multi-head self-attention. As shown in Fig  1,   H e→r denotes emitter-attended receiver features and H r→e is the reverse. However, there may be other useful information from individual modalities ignored by the inter-attention. We use intra-attention to resolve this issue. The intra-attention focuses on salient information in each signal domain towards the impression recognition and generates H e and H r as the hidden representations. The four representations are then concatenated for the final non-linear combination.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Cross-Domain Regularization",
      "text": "To reduce the discrepancy and further regulate the relatedness between the two different domains, we propose a cross-domain regularization which has a Knowledge Distillation (KD) loss L kd and a Similarity Enhancement (SE) loss Lse.\n\nKnowledge Distillation Loss. Knowledge distillation is a deep learning technique used for training a small network (student) under the guidance of a trained network (teacher)  [29, 30] . Though this technique is widely used in model training, recent work has been inspired to apply it to transfer multimodal knowledge between hidden representations  [28] . Considering the fact that the multimodal signals from emitter and receiver have weak relatedness, we design a KD loss to transfer the information from the other domain. Unlike the inter-attention which directly attends one domain to the other, which we call \"hard relatedness\", the knowledge distillation enables indirectly learning multimodal knowledge with minor changes in a \"soft\" way: H r and H e can absorb information from each other to some extent while still maintaining their independence. We calculate the Mean Squared Error (MSE) and represent the KD loss as:\n\nwhere h A i , h B i denote the hidden states from two representations, and m is the sequence length.\n\nSimilarity Enhancement Loss. To ensure the inter-attention representations have successfully learned the information from the other domain, we apply a similarity enhancement loss. For example, minimizing the distance between H r→e and H r to align together the two representations means that the receiver information has been attended to in the emitter information by inter-attention. We use Kullback-Leibler (KL) divergence for this purpose:\n\nwhere P (H) denotes the softened probability vector of the representation H, and m is the sequence length.\n\nThe reasons that why we choose MSE and KL divergence are: 1) MSE generally outperforms KL divergence in knowledge distillation  [31] . 2) KL divergence is good at calculating the distance of two distributions on the same probability space and is popular for similarity measurement  [32, 33] , so we expect it to enhance the similarity in the cross-domain situation. We also exchanged MSE and KL divergence for KD and SE but found a small decrease in the warmth dimension.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Fully-Connected Network",
      "text": "Finally, the concatenated representations are fed to an FC network containing a linear dense layer with 16 neurons, followed by a ReLU activation function, a dropout layer, and a linear dense layer with a single neuron to generate predictions. The prediction task is optimized by the following objective function:\n\nwhere Cp and Wp are the predictions of competence and warmth, and C l and W l are the corresponding labels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Evaluation",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation",
      "text": "The model was built using Pytorch and optimized using the Adam method. The learning rate was set as 1e-3 and reduced by half every 20 epochs. 40 epochs were used for training and validation. The full model was trained by minimizing the overall loss:\n\nWe mixed up all the data and used 80% for training, 10% for validation and 10% for testing. We evaluated the performance using Concordance Correlation Coefficient (CCC).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "First, we compare our results with the original IMPRESSION paper and present ablation study where we removed each component from concatenation (for the attentions) or from back propagation (for the losses) without changing the architecture. So even when intra-attention was removed, the losses still worked.\n\nTable  2  shows that: 1) our proposed full model achieves a similar result in warmth as the original work using LSTM, but a significant increase in competence. 2) The removal of each of the components results in a decrease in performance, which in turn proves their effectiveness. 3) There is no clear difference between removing inter-attention and intra-attention, suggesting that even without the inter-representations, the intra-'s do learn cross-domain relevance with the help of KD loss and SE loss. 4) SE loss shows the least effect among all the components, which is reasonable since the two targets originally have similarities (for instance, H e→r denotes the receiver information attended by the emitter information which shares similarity with H e ). On top of that, SE loss reinforces the connection even further. From Fig  2 , we can observe that both KD and SE losses show a decreasing trend. However, they showed an increasing trend (not shown here) when removed from back propagation in the ablation study, which also demonstrates their usefulness.\n\nNext, since the IMPRESSION dataset is just publicized and there are few dimensional impression recognition studies, our  results are not directly comparable to the literature. Thus, we compared with some dimensional emotion recognition work. Our prior work  [34]  incorporated sentiment analysis using a weighted linear regression, achieving a CCC of 0.560 in valence estimation. A recent SOTA work  [35]  obtained a CCC of 0.680 for arousal using multi-stage fusion and multi-task learning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a cross-domain architecture to address the dyadic impression recognition problem. This architecture consists of cross-domain attention and regularization to capture and strengthen the relatedness between the emitter and receiver. The cross-domain attention has an intra-and an inter-attention, which focus on the salient information in each domain and the relevant information between two domains. The cross-domain regularization has a knowledge distillation loss and a similarity enhancement loss, \"softly\" transferring multimodal information and minimizing the distance between two domains. The experimental evaluation proves that both components are useful for performance improvement. We expect to test the approach on a second database (e.g., AMIGOS) for more result comparisons in the near future.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 4.1. Feature Concatenation",
      "page": 2
    },
    {
      "caption": "Figure 1: Architecture using the proposed cross-domain approach.",
      "page": 3
    },
    {
      "caption": "Figure 1: , He→r denotes emitter-attended receiver fea-",
      "page": 3
    },
    {
      "caption": "Figure 2: , we can observe that both KD and SE losses",
      "page": 4
    },
    {
      "caption": "Figure 2: Trends of regularization losses.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Centre for Speech Technology Research, University of Edinburgh": "y.li-385@sms.ed.ac.uk,\nc.lai@ed.ac.uk"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "ity, along with low-level features extracted from audio to predict\nAbstract"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "the ﬁve personality traits.\n[10] extracted visual and vocal fea-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "The impression we make on others depends not only on what"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "tures (e.g., eye gaze, head pose, speaking activity, and prosody"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "we say, but also,\nto a large extent, on how we say it.\nAs a"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "variation) to characterize the social interaction. [11] used Elec-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "sub-branch of affective computing and social\nsignal process-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "troencephalogram (EEG), Electrocardiogram (ECG), Galvanic"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "ing,\nimpression recognition has proven critical\nin both human-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "Skin Response (GSR), and facial activity data to recognize per-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "human conversations and spoken dialogue systems. However,"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "sonality.\nFurthermore, human-like speech has been designed"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "most research has studied impressions only from the signals ex-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "for robots and virtual agents, to express different impressions to"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "pressed by the emitter, ignoring the response from the receiver."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "conduct speciﬁc roles.\nFor example,\n[12] examined the inﬂu-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "In this paper, we perform impression recognition using a pro-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "ence of backchannel selection on extroversion expression using"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "posed cross-domain architecture on the dyadic IMPRESSION"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "the virtual agent SAL, which acts as an interlocutor in interac-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "dataset. This improved architecture makes use of cross-domain"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "tion.\n[13] analyzed the relationship between personality traits"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "attention and regularization. The cross-domain attention con-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "and dialogue behaviors, to control utterance amount, backchan-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "sists of\nintra- and inter-attention mechanisms, which capture"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "nel, ﬁller, and switching pause length for\nthe humanoid robot"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "intra- and inter-domain relatedness,\nrespectively.\nThe cross-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "ERICA to express extroversion, emotional\ninstability, and po-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "domain regularization includes knowledge distillation and sim-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "liteness. However, most research has ignored the fact\nthat\nim-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "ilarity enhancement\nlosses, which strengthen the feature con-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "pression depends not only on the emitter (system/speaker) but"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "nections between the emitter and receiver.\nThe experimental"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "also largely on the perception by the receiver\n(user/listener)."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "evaluation veriﬁed the effectiveness of our approach. Our ap-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "That\nis,\nimpressions should be formed by both of\nthe interac-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "proach achieved a concordance correlation coefﬁcient of 0.770"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "tion partners."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "in competence dimension and 0.748 in warmth dimension."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "Therefore,\nin this paper, we propose a dyadic impression"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "Index Terms:\nimpression recognition,\naffective\ncomputing,"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "recognition approach using cross-domain attention and regular-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "multi-task learning, attention, fusion"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "ization to capture and strengthen related information from the"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "emitter and receiver as there is a perception gap that separates"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "1.\nIntroduction\nthem into two domains of signal sources (explained in Sec 2.2)."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "Besides the carefully conceived speech content, psychological"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "research indicates that people also intentionally control their ap-\n2. Related Work"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "pearances and behaviors to leave different\nimpressions on the"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "2.1.\nImpression Recognition"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "interaction partners according to different scenarios, such as se-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "The majority of impression research focuses on personality im-\nlection, leader–subordinate relationships, job performance, and"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "pression. Many corpora have been presented and adopted for\nso on. For example, a person tends to express extroversion dur-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "personality prediction. YouTube Personality [5] contains 442\ning job interviews and show agreeableness in dating [1, 2, 3]."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "vlogs and 2,210 annotations, providing new ﬁndings\nregard-\nSocial research has pointed out\nthe importance of impressions"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "ing the suitability of collecting personality impressions\nfrom\nin human interactions, where it\nis natural\nto perceive impres-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "crowdsourcing,\nthe types of personality impressions,\ntheir as-\nsions from the partners via non-verbal behaviors, such as eye"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "sociation with social attention, and the level of utilization of\ngaze, body pose, speaking activity, and prosody variation [4, 5]."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "non-verbal cues. AMIGOS [14] collected participants’ EEG,\nImpressions are often categorized in terms of “Big Five” per-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "ECG, and GSR signals using wearable sensors, and recorded\nsonality traits: extroversion, agreeableness, conscientiousness,"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "participants’\nfrontal high deﬁnition videos and both RGB and\nneuroticism, and openness. But an impression is more like a"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "depth full-body videos. Self-assessment of affective levels (va-\nformed opinion and some research also uses two dimensions"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "lence, arousal, control, familiarity,\nliking, and basic emotions)\nof social perception/cognition: warmth and competence to de-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "were annotated. The SSPNet Speaker Personality [15] consists\nscribe impression. Warmth reﬂects the intentions of others and"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "of 640 speech clips for a total of 322 subjects. Each speech clip\nincludes the meanings such as good-natured,\ntrustworthy,\ntol-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "lasts 10 seconds and was scored by 11 annotators in terms of\nerant, friendly, and sincere. Competence reﬂects the ability of"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "the Big Five traits using the BFI-10 questionnaire [16, 17].\nthe other to enact his/her intentions and means capable, skillful,"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "Compared to personality impression, recognition of warmth\nintelligent, and conﬁdent [6, 7]."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "and competence is understudied.\nThe Noxi corpus\n[18] was\nThe procedure of impression recognition is similar to that"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "collected to investigate the relationship between observed non-\nof emotion recognition, consisting of two steps:\nfeature extrac-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "verbal\ncues\nand ﬁrst\nimpression\nformation\nof warmth\nand\ntion and classiﬁcation/regression. Researchers have used many"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "competence\n[19].\nNoxi\nis\na multilingual dataset of natural\napproaches to predict impressions from human behaviors, such"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "dyadic novice-expert interactions, where participants interacted\nas facial, vocal, and bodily expression.\n[8, 9] used high-level"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh": "through a screen in different rooms and the experts talked more\nfeatures obtained from body and head motion, speaking activ-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: Most of the features we se-",
      "data": [
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "Modality\nEmitter\nReceiver"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "−\nAudio\nMFCC, VoiceProb,"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "RMS energy, ZCR"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "Eye\n2D&3D gaze directions\nGaze duration,"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "2D gaze locations,"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "3D eye locations"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "Facial\nAU presence,\nAU presence,"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "AU intensity\nAU intensity"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "−\nPhysio\nBVP, ECG, GSR"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "* AU: Action Units. MFCC: Mel-Frequency Cepstral Coefﬁcients."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "VoiceProb: Voicing Probability. RMS: Root-Mean-Square. ZCR:"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "Zero-Crossing Rate. BVP: Blood Volume Pulse. ECG: Electro-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "cardiogram. GSR: Galvanic Skin Response."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "Otherwise, the decimate function was ﬁrst applied to downsam-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "ple the features to approximate the label sample size. Then a"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "Fast Fourier Transform (FFT) was applied to further downsam-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "ple the features with a step of current length/target length. Since"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "the window is applied every step,\nit can also serve as a denois-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "ing function to remove some outliers. Taking the eye movement"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "modality of the receiver as an example, decimate function with"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "a factor of two or ten (depending on original sample numbers)"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "was directly applied. For the audio modality of the emitter, dec-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "imate function with a factor of three was applied, followed by"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "an FFT with the number of samples that equals the label num-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "bers.\nFinally,\nthe 13 emitters have a total of 44,923 samples,"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "each with 412 feature dimensions. The 40 receivers has 44,923"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "* 40 samples, where each sample has 68 feature dimensions."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "4. Proposed Approach"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "To address the fusion problem stated in Sec 2.2, we propose"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "a\ncross-domain approach for dyadic\nimpression recognition,"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "shown in Fig 1."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "4.1. Feature Concatenation"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "We concatenated the features from the emitter and receiver re-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "spectively, using early fusion. Though prior research has pro-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "posed State-Of-The-Art\n(SOTA)\ntensor\nfusion approaches for"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "multimodal features [20], we used early fusion here because the"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "feature dimension of each modality is relatively small, which is"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "not suitable for use alone in deep learning."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "We denoted the concatenated features of the emitter and re-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "ceiver as E = {e1, e2, · · ·\n, eK } and R = {r1, r2, · · ·\n, rL}."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "K and L are the lengths (dimensions) of\nthe emitter\nfeatures"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "and the receiver features, which equal\nto 412 and 68. Then we"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "encoded the inputs E and R as ﬁxed-length vectors Z e and Z r."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "4.2. BLSTM Network"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "We fed the vectors Z e and Z r to a Bi-directional Long Short-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "Term Memory (BLSTM) network.\nThe BLSTM network en-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "codes global contexts by updating its hidden states recurrently."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "It takes as input vectors and outputs a sequence of hidden states"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "H blstm = {hblstm\n, hblstm\n∈ R2dlstm is"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ", · · ·\nL/npw }, where hblstm"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "the concatenation of the i-th forward hidden state and the i-th"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "backward hidden state. The forward LSTM reads the input from"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "left to right and the backward LSTM reads the input in reverse."
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "4.3. Cross-Domain Attention"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": ""
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "Following the BLSTM network, a structured cross-domain at-"
        },
        {
          "Table 1: Selected features from the emitter and receiver.": "tention consisting of two intra-attention and inter-attention net-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "works, aggregates information from the BLSTM hidden states"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "H blstm,\nand produce four ﬁxed-length encodings: H e, H r,"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "H e→r, and H r→e. For both intra- and inter-attention, we used"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "multi-head self-attention mechanism as follows:"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "H = M ultiHead(Q, K, V )W O\n(1)"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "(2)\n= Concat(head1, ..., headn)"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ", KW K\n, V W V\n)\n(3)\nheadi = Attention(QW Q\ni"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "QK T"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "√\n)V\nAttention(Q, K, V ) = Sof tmax(\n(4)"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "d"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "are trainable parameters. Q,\n, and W V\nwhere W O, W Q\n, W K\ni"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "K, and V represent the query, key, and value, respectively, and"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "d is their common dimension.\nFor\ninter-attention, Q is from"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "one domain (the emitter or receiver), while K and V are from"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "the other. For intra-attention, the three parameters are from the"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "same domain. The value of n is 16, and H denotes the concate-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "nated multi-head representations: H e, H r, H e→r, and H r→e."
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "Next, we concatenated the four representations and passed them"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "to a Fully-Connected (FC) network with a ReLU activation"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "function to generate the recognition outputs Cp and Wp which"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "are competence and warmth predictions."
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "We used the cross-domain attention network because im-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "pression recognition from the emitter and the receiver can be"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "regarded as different domains in this dataset. In [7], the impres-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "sion labels have high correlations with receiver features, but low"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "correlations with emitter\nfeatures,\nindicating that\nthe related-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "ness between the emitter and receiver is not obvious. This phe-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "nomenon is plausible because the receiver responds to the emit-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "ter videos without real interaction. Therefore, we need the inter-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "attention to ﬁnd relevant features between the two domains."
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "Inter-attention (also known as co-attention)\n[26] has been"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "adopted in affective computing over\nthe recent years [22, 27,"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "28].\nIt exchanges key-value pairs in multi-head self-attention."
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "As shown in Fig 1, H e→r denotes emitter-attended receiver fea-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "tures and H r→e\nis the reverse. However,\nthere may be other"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "useful\ninformation from individual modalities ignored by the"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "inter-attention. We use intra-attention to resolve this issue. The"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "intra-attention focuses on salient information in each signal do-"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "main towards the impression recognition and generates H e and"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "H r as the hidden representations. The four representations are"
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": ""
        },
        {
          "Figure 1: Architecture using the proposed cross-domain approach.": "then concatenated for the ﬁnal non-linear combination."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where P (H) denotes the softened probability vector of the rep-": "resentation H, and m is the sequence length."
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "The reasons that why we choose MSE and KL divergence"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "are:\n1) MSE generally outperforms KL divergence in knowl-"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "edge distillation [31]. 2) KL divergence is good at calculating"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "the distance of two distributions on the same probability space"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "and is popular for similarity measurement [32, 33], so we ex-"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "pect\nit\nto enhance the similarity in the cross-domain situation."
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "We also exchanged MSE and KL divergence for KD and SE but"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "found a small decrease in the warmth dimension."
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "4.5. Fully-Connected Network"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "Finally,\nthe concatenated representations are fed to an FC net-"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "work containing a linear dense layer with 16 neurons, followed"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "by a ReLU activation function, a dropout\nlayer, and a linear"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "dense layer with a single neuron to generate predictions. The"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "prediction task is optimized by the following objective function:"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "(10)\nLtask = M SE(Cp, Cl) + M SE(Wp, Wl)"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "are\nthe\npredictions\nof\ncompetence\nand\nwhere Cp\nand Wp"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "warmth, and Cl and Wl are the corresponding labels."
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "5. Experimental Evaluation"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "5.1.\nImplementation"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "The model was built using Pytorch and optimized using the"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "Adam method. The learning rate was set as 1e-3 and reduced by"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "half every 20 epochs. 40 epochs were used for training and val-"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "idation. The full model was trained by minimizing the overall"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "loss:"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "(11)\nL = Ltask + Lkd + Lse"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "We mixed up all\nthe data and used 80% for training, 10% for"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "validation and 10% for testing. We evaluated the performance"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "using Concordance Correlation Coefﬁcient (CCC)."
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "5.2. Results and Discussion"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "First, we compare our results with the original IMPRESSION"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "paper and present ablation study where we removed each com-"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "ponent\nfrom concatenation (for\nthe attentions) or\nfrom back"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "propagation (for the losses) without changing the architecture."
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "So\neven when\nintra-attention was\nremoved,\nthe\nlosses\nstill"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "worked."
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "Table 2 shows that: 1) our proposed full model achieves a"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "similar result\nin warmth as the original work using LSTM, but"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "a signiﬁcant increase in competence. 2) The removal of each of"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "the components results in a decrease in performance, which in"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "turn proves their effectiveness. 3) There is no clear difference"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "between removing inter-attention and intra-attention, suggest-"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "ing that even without\nthe inter-representations,\nthe intra-’s do"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "learn cross-domain relevance with the help of KD loss and SE"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "loss.\n4) SE loss shows the least effect among all\nthe compo-"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "nents, which is reasonable since the two targets originally have"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "similarities (for instance, H e→r denotes the receiver informa-"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "tion attended by the emitter information which shares similarity"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": ""
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "with H e). On top of that, SE loss reinforces the connection even"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "further. From Fig 2, we can observe that both KD and SE losses"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "show a decreasing trend. However,\nthey showed an increasing"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "trend (not shown here) when removed from back propagation in"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "the ablation study, which also demonstrates their usefulness."
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "Next, since the IMPRESSION dataset is just publicized and"
        },
        {
          "where P (H) denotes the softened probability vector of the rep-": "there are few dimensional\nimpression recognition studies, our"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "pressions of warmth and competence from observable nonverbal"
        },
        {
          "8. References": "[1] B. W. Swider, T. B. Harris, and Q. Gong, “First\nimpression ef-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "the 19th\ncues\nin expert-novice interactions,”\nin Proceedings of"
        },
        {
          "8. References": "fects in organizational psychology.” Journal of Applied Psychol-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "ACM International Conference on Multimodal Interaction, 2017,"
        },
        {
          "8. References": "ogy, 2021.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "pp. 341–349."
        },
        {
          "8. References": "[2]\nJ. Willis and A. Todorov, “First\nimpressions: Making up your",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[20]\nS. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of"
        },
        {
          "8. References": "mind after a 100-ms exposure to a face,” Psychological science,",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "affective computing: From unimodal analysis to multimodal fu-"
        },
        {
          "8. References": "vol. 17, no. 7, pp. 592–598, 2006.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "sion,” Information Fusion, vol. 37, pp. 98–125, 2017."
        },
        {
          "8. References": "[3] N. Ambady and J. J. Skowronski, First\nimpressions.\nGuilford",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[21] Y. Li, T. Zhao, and X. Shen, “Attention-based multimodal fusion"
        },
        {
          "8. References": "Press, 2008.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "for estimating human emotion in real-world hri,” in Companion of"
        },
        {
          "8. References": "[4]\nL. P. Naumann, S. Vazire, P. J. Rentfrow, and S. D. Gosling, “Per-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "the 2020 ACM/IEEE International Conference on Human-Robot"
        },
        {
          "8. References": "sonality judgments based on physical appearance,” Personality",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "Interaction, 2020, pp. 340–342."
        },
        {
          "8. References": "and social psychology bulletin, vol. 35, no. 12, pp. 1661–1671,",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[22] Y. Li, P. Bell, and C. Lai, “Fusing asr outputs in joint training for"
        },
        {
          "8. References": "2009.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "speech emotion recognition,” arXiv preprint arXiv:2110.15684,"
        },
        {
          "8. References": "[5]\nJ.-I. Biel and D. Gatica-Perez, “The youtube lens: Crowdsourced",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "2021."
        },
        {
          "8. References": "personality impressions and audiovisual analysis of vlogs,” IEEE",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[23]\nL. Tian,\nJ. Moore, and C. Lai, “Recognizing emotions\nin spo-"
        },
        {
          "8. References": "Transactions on Multimedia, vol. 15, no. 1, pp. 41–55, 2012.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "ken dialogue with hierarchically fused acoustic and lexical\nfea-"
        },
        {
          "8. References": "[6] A. J. Cuddy, S. T. Fiske, and P. Glick, “Warmth and competence",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "tures,”\nin 2016 IEEE Spoken Language Technology Workshop"
        },
        {
          "8. References": "as universal dimensions of social perception: The stereotype con-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "(SLT).\nIEEE, 2016, pp. 565–572."
        },
        {
          "8. References": "tent model and the bias map,” Advances in experimental social",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[24] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient emotion recogni-"
        },
        {
          "8. References": "psychology, vol. 40, pp. 61–149, 2008.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "tion from speech using deep learning on spectrograms.” in Inter-"
        },
        {
          "8. References": "[7] C. Wang and G. Chanel, “An open dataset for impression recog-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "speech, 2017, pp. 1089–1093."
        },
        {
          "8. References": "nition from multimodal bodily responses,” in 2021 9th Interna-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "tional Conference on Affective Computing and Intelligent\nInter-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[25] Y. Li, T. Zhao, and T. Kawahara, “Improved end-to-end speech"
        },
        {
          "8. References": "action (ACII).\nIEEE, 2021, pp. 1–8.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "emotion recognition using self attention mechanism and multitask"
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "learning.” in Interspeech, 2019, pp. 2803–2807."
        },
        {
          "8. References": "[8] O. Aran and D. Gatica-Perez, “One of a kind:\nInferring person-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "the 15th ACM\nality impressions in meetings,” in Proceedings of",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[26]\nJ. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-"
        },
        {
          "8. References": "on International conference on multimodal interaction, 2013, pp.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "agnostic visiolinguistic\nrepresentations\nfor vision-and-language"
        },
        {
          "8. References": "11–18.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "tasks,” arXiv preprint arXiv:1908.02265, 2019."
        },
        {
          "8. References": "[9]\nS. Okada, O. Aran, and D. Gatica-Perez, “Personality trait classi-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[27]\nJ. Huang, Y. Li, J. Tao, Z. Lian et al., “Speech emotion recog-"
        },
        {
          "8. References": "ﬁcation via co-occurrent multiparty multimodal event discovery,”",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "nition from variable-length inputs with triplet\nloss function.” in"
        },
        {
          "8. References": "the 2015 ACM on International Conference on\nin Proceedings of",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "Interspeech, 2018, pp. 3673–3677."
        },
        {
          "8. References": "Multimodal Interaction, 2015, pp. 15–22.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[28]\nZ. Huang,\nF. Liu, X. Wu,\nS. Ge, H. Wang, W.\nFan,\nand"
        },
        {
          "8. References": "[10]\nT. A. Judge, C. A. Higgins, C. J. Thoresen, and M. R. Barrick,",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "Y\n. Zou, “Audio-oriented multimodal machine comprehension via"
        },
        {
          "8. References": "“The big ﬁve personality traits, general mental ability, and career",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "dynamic inter-and intra-modality attention,” in Proceedings of the"
        },
        {
          "8. References": "success across the life span,” Personnel psychology, vol. 52, no. 3,",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "AAAI Conference on Artiﬁcial Intelligence, vol. 35, no. 14, 2021,"
        },
        {
          "8. References": "pp. 621–652, 1999.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "pp. 13 098–13 106."
        },
        {
          "8. References": "[11] R. Subramanian, J. Wache, M. K. Abadi, R. L. Vieriu, S. Win-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[29] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and"
        },
        {
          "8. References": "kler, and N. Sebe, “Ascertain: Emotion and personality recogni-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "Y\n. Bengio,\n“Fitnets: Hints\nfor\nthin deep nets,” arXiv preprint"
        },
        {
          "8. References": "tion using commercial sensors,” IEEE Transactions on Affective",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "arXiv:1412.6550, 2014."
        },
        {
          "8. References": "Computing, vol. 9, no. 2, pp. 147–160, 2016.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[30]\nF. Tung and G. Mori, “Similarity-preserving knowledge distilla-"
        },
        {
          "8. References": "[12]\nE. De Sevin, S. J. Hyniewska, and C. Pelachaud, “Inﬂuence of per-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "the IEEE/CVF International Conference\ntion,” in Proceedings of"
        },
        {
          "8. References": "sonality traits on backchannel selection,” in International Confer-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "on Computer Vision, 2019, pp. 1365–1374."
        },
        {
          "8. References": "ence on Intelligent Virtual Agents.\nSpringer, 2010, pp. 187–193.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[31]\nT. Kim,\nJ. Oh, N. Kim,\nS. Cho,\nand S.-Y. Yun,\n“Compar-"
        },
        {
          "8. References": "[13] K. Yamamoto, K.\nInoue,\nS. Nakamura, K. Takanashi,\nand",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "ing Kullback-Leibler Divergence and Mean Squared Error Loss"
        },
        {
          "8. References": "T. Kawahara, “Dialogue behavior control model\nfor expressing",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "in Knowledge Distillation,”\narXiv preprint arXiv:210/5.08919,"
        },
        {
          "8. References": "a character of humanoid robots,” in 2018 Asia-Paciﬁc Signal and",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "2021."
        },
        {
          "8. References": "Information Processing Association Annual Summit and Confer-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[32]\nJ. Goldberger, S. Gordon, H. Greenspan et al.,\n“An Efﬁcient"
        },
        {
          "8. References": "ence (APSIPA ASC).\nIEEE, 2018, pp. 1732–1737.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "Image Similarity Measure Based\non Approximations\nof KL-"
        },
        {
          "8. References": "[14]\nJ. A. M. Correa, M. K. Abadi, N. Sebe, and I. Patras, “Amigos:",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "Divergence Between Two Gaussian Mixtures,” in ICCV, vol. 3,"
        },
        {
          "8. References": "A dataset for affect, personality and mood research on individuals",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "2003, pp. 487–493."
        },
        {
          "8. References": "and groups,” IEEE Transactions on Affective Computing, 2018.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[33]\nZ. Yao, Z. Lai, and W. Liu, “A symmetric KL divergence based"
        },
        {
          "8. References": "[15] G. Mohammadi and A. Vinciarelli, “Automatic personality per-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "spatiogram similarity measure,” in 2011 18th IEEE International"
        },
        {
          "8. References": "ception: Prediction of trait attribution based on prosodic features,”",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "Conference on Image Processing.\nIEEE, 2011, pp. 193–196."
        },
        {
          "8. References": "IEEE Transactions on Affective Computing, vol. 3, no. 3, pp. 273–",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "284, 2012.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[34] Y. Li, C. T. Ishi, N. Ward, K. Inoue, S. Nakamura, K. Takanashi,"
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "and T. Kawahara, “Emotion recognition by combining prosody"
        },
        {
          "8. References": "[16]\nS. D. Gosling, P. J. Rentfrow, and W. B. Swann Jr, “A very brief",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "and sentiment analysis\nfor expressing reactive emotion by hu-"
        },
        {
          "8. References": "measure of the big-ﬁve personality domains,” Journal of Research",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "manoid robot,” in 2017 Asia-Paciﬁc Signal and Information Pro-"
        },
        {
          "8. References": "in personality, vol. 37, no. 6, pp. 504–528, 2003.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "cessing Association Annual\nSummit\nand Conference\n(APSIPA"
        },
        {
          "8. References": "[17] B. Rammstedt\nand O. P.\nJohn,\n“Measuring personality in one",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "ASC).\nIEEE, 2017, pp. 1356–1359."
        },
        {
          "8. References": "minute or less: A 10-item short version of the big ﬁve inventory in",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "[35] B. T. Atmaja\nand M. Akagi,\n“Multitask\nlearning\nand multi-"
        },
        {
          "8. References": "english and german,” Journal of research in Personality, vol. 41,",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "stage fusion for dimensional audiovisual emotion recognition,” in"
        },
        {
          "8. References": "no. 1, pp. 203–212, 2007.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "ICASSP 2020-2020 IEEE International Conference on Acoustics,"
        },
        {
          "8. References": "[18] A. Cafaro, J. Wagner, T. Baur, S. Dermouche, M. Torres Torres,",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 4482–"
        },
        {
          "8. References": "C. Pelachaud, E. Andr´e, and M. Valstar, “The NoXi database:",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": "4486."
        },
        {
          "8. References": "multimodal recordings of mediated novice-expert interactions,” in",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "Proceedings of the 19th ACM International Conference on Multi-",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        },
        {
          "8. References": "modal Interaction, 2017, pp. 350–359.",
          "[19] B. Biancardi, A. Cafaro, and C. Pelachaud, “Analyzing ﬁrst\nim-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "First impression effects in organizational psychology",
      "authors": [
        "B Swider",
        "T Harris",
        "Q Gong"
      ],
      "year": "2021",
      "venue": "Journal of Applied Psychology"
    },
    {
      "citation_id": "3",
      "title": "First impressions: Making up your mind after a 100-ms exposure to a face",
      "authors": [
        "J Willis",
        "A Todorov"
      ],
      "year": "2006",
      "venue": "Psychological science"
    },
    {
      "citation_id": "4",
      "title": "First impressions",
      "authors": [
        "N Ambady",
        "J Skowronski"
      ],
      "year": "2008",
      "venue": "First impressions"
    },
    {
      "citation_id": "5",
      "title": "Personality judgments based on physical appearance",
      "authors": [
        "L Naumann",
        "S Vazire",
        "P Rentfrow",
        "S Gosling"
      ],
      "year": "2009",
      "venue": "Personality and social psychology bulletin"
    },
    {
      "citation_id": "6",
      "title": "The youtube lens: Crowdsourced personality impressions and audiovisual analysis of vlogs",
      "authors": [
        "J.-I Biel",
        "D Gatica-Perez"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "Warmth and competence as universal dimensions of social perception: The stereotype content model and the bias map",
      "authors": [
        "A Cuddy",
        "S Fiske",
        "P Glick"
      ],
      "year": "2008",
      "venue": "Advances in experimental social psychology"
    },
    {
      "citation_id": "8",
      "title": "An open dataset for impression recognition from multimodal bodily responses",
      "authors": [
        "C Wang",
        "G Chanel"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "9",
      "title": "One of a kind: Inferring personality impressions in meetings",
      "authors": [
        "O Aran",
        "D Gatica-Perez"
      ],
      "year": "2013",
      "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction"
    },
    {
      "citation_id": "10",
      "title": "Personality trait classification via co-occurrent multiparty multimodal event discovery",
      "authors": [
        "S Okada",
        "O Aran",
        "D Gatica-Perez"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "11",
      "title": "The big five personality traits, general mental ability, and career success across the life span",
      "authors": [
        "T Judge",
        "C Higgins",
        "C Thoresen",
        "M Barrick"
      ],
      "year": "1999",
      "venue": "Personnel psychology"
    },
    {
      "citation_id": "12",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Influence of personality traits on backchannel selection",
      "authors": [
        "E De Sevin",
        "S Hyniewska",
        "C Pelachaud"
      ],
      "year": "2010",
      "venue": "International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "14",
      "title": "Dialogue behavior control model for expressing a character of humanoid robots",
      "authors": [
        "K Yamamoto",
        "K Inoue",
        "S Nakamura",
        "K Takanashi",
        "T Kawahara"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "15",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Automatic personality perception: Prediction of trait attribution based on prosodic features",
      "authors": [
        "G Mohammadi",
        "A Vinciarelli"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "A very brief measure of the big-five personality domains",
      "authors": [
        "S Gosling",
        "P Rentfrow",
        "W Swann"
      ],
      "year": "2003",
      "venue": "Journal of Research in personality"
    },
    {
      "citation_id": "18",
      "title": "Measuring personality in one minute or less: A 10-item short version of the big five inventory in english and german",
      "authors": [
        "B Rammstedt",
        "O John"
      ],
      "year": "2007",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "19",
      "title": "The NoXi database: multimodal recordings of mediated novice-expert interactions",
      "authors": [
        "A Cafaro",
        "J Wagner",
        "T Baur",
        "S Dermouche",
        "M Torres",
        "C Pelachaud",
        "E André",
        "M Valstar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "20",
      "title": "Analyzing first impressions of warmth and competence from observable nonverbal cues in expert-novice interactions",
      "authors": [
        "B Biancardi",
        "A Cafaro",
        "C Pelachaud"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "21",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "22",
      "title": "Attention-based multimodal fusion for estimating human emotion in real-world hri",
      "authors": [
        "Y Li",
        "T Zhao",
        "X Shen"
      ],
      "year": "2020",
      "venue": "Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "23",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2021",
      "venue": "Fusing asr outputs in joint training for speech emotion recognition",
      "arxiv": "arXiv:2110.15684"
    },
    {
      "citation_id": "24",
      "title": "Recognizing emotions in spoken dialogue with hierarchically fused acoustic and lexical features",
      "authors": [
        "L Tian",
        "J Moore",
        "C Lai"
      ],
      "year": "2016",
      "venue": "2016 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "25",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "J Lu",
        "D Batra",
        "D Parikh",
        "S Lee"
      ],
      "year": "2019",
      "venue": "Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks",
      "arxiv": "arXiv:1908.02265"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition from variable-length inputs with triplet loss function",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Audio-oriented multimodal machine comprehension via dynamic inter-and intra-modality attention",
      "authors": [
        "Z Huang",
        "F Liu",
        "X Wu",
        "S Ge",
        "H Wang",
        "W Fan",
        "Y Zou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Fitnets: Hints for thin deep nets",
      "authors": [
        "A Romero",
        "N Ballas",
        "S Kahou",
        "A Chassang",
        "C Gatta",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Fitnets: Hints for thin deep nets",
      "arxiv": "arXiv:1412.6550"
    },
    {
      "citation_id": "31",
      "title": "Similarity-preserving knowledge distillation",
      "authors": [
        "F Tung",
        "G Mori"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "32",
      "title": "Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation",
      "authors": [
        "T Kim",
        "J Oh",
        "N Kim",
        "S Cho",
        "S.-Y Yun"
      ],
      "year": "2021",
      "venue": "Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation"
    },
    {
      "citation_id": "33",
      "title": "An Efficient Image Similarity Measure Based on Approximations of KL-Divergence Between Two Gaussian Mixtures",
      "authors": [
        "J Goldberger",
        "S Gordon",
        "H Greenspan"
      ],
      "year": "2003",
      "venue": "ICCV"
    },
    {
      "citation_id": "34",
      "title": "A symmetric KL divergence based spatiogram similarity measure",
      "authors": [
        "Z Yao",
        "Z Lai",
        "W Liu"
      ],
      "year": "2011",
      "venue": "2011 18th IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "35",
      "title": "Emotion recognition by combining prosody and sentiment analysis for expressing reactive emotion by humanoid robot",
      "authors": [
        "Y Li",
        "C Ishi",
        "N Ward",
        "K Inoue",
        "S Nakamura",
        "K Takanashi",
        "T Kawahara"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "36",
      "title": "Multitask learning and multistage fusion for dimensional audiovisual emotion recognition",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}