{
  "paper_id": "2004.06657v1",
  "title": "A Transfer Learning Approach To Heatmap Regression For Action Unit Intensity Estimation",
  "published": "2020-04-14T16:51:13Z",
  "authors": [
    "Ioanna Ntinou",
    "Enrique Sanchez",
    "Adrian Bulat",
    "Michel Valstar",
    "Georgios Tzimiropoulos"
  ],
  "keywords": [
    "Facial Action Unit Intensity Estimation",
    "Heatmap Regression",
    "Transfer Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Action Units (AUs) are geometrically-based atomic facial muscle movements known to produce appearance changes at specific facial locations. Motivated by this observation we propose a novel AU modelling problem that consists of jointly estimating their localisation and intensity. To this end, we propose a simple yet efficient approach based on Heatmap Regression that merges both problems into a single task. A Heatmap models whether an AU occurs or not at a given spatial location. To accommodate the joint modelling of AUs intensity, we propose variable size heatmaps, with their amplitude and size varying according to the labelled intensity. Using Heatmap Regression, we can inherit from the progress recently witnessed in facial landmark localisation. Building upon the similarities between both problems, we devise a transfer learning approach where we exploit the knowledge of a network trained on large-scale facial landmark datasets. In particular, we explore different alternatives for transfer learning through a) fine-tuning, b) adaptation layers, c) attention maps, and d) reparametrisation. Our approach effectively inherits the rich facial features produced by a strong face alignment network, with minimal extra computational cost. We empirically validate that our system sets a new state-of-the-art on three popular datasets, namely BP4D, DISFA, and FERA2017.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A UTOMATIC facial expression analysis is important for detect- ing, recognising and interpreting the emotional state underlying a given facial image. One of the most common descriptors of facial expressions are the Action Units (AUs,  [13] ). The Facial Action Coding System (FACS,  [13] ) defines Action Units as atomic non-overlapping facial muscle actions that when combined in different configurations can describe any facial expression. There are 32 AUs in total. The Facial Action Coding System also establishes a six-point ordinal ranking of intensities which ranges from 0 to 5, with 0 denoting the absence of a specific Action Unit, and 5 referring to the maximum level of expressivity.\n\nAction Units are in many ways inherently correlated: in time, as facial actions vary smoothly within a sequence, in their cooccurrence, as Action Units are often activated in certain meaningful combinations, and spatially, as they adhere to anatomically defined local and global geometric structure. In the field of automatic facial expression analysis, these correlations can indeed be regarded as a line of research, either alone or in combination with others. For example, co-occurrence correlation is exploited to perform joint prediction of multiple AUs, either through shared feature representations  [61] ,  [69]  or through methods that impose correlations among labels, usually by employing graphs  [56] ,  [57] . Furthermore, a significant amount of works attempt to exploit spatial correlations of AUs by extracting local representations in the facial regions where AUs are known to produce appearance Manuscript submitted on April 14, 2020 Fig.  1 . Facial AUs have strong spatial correlations and co-dependent structure. While the spatial correlation comes from their definition, we often find AUs that tend to co-occur. For instance, AU 6 (cheek raising), and AU 12 (lip corner pulling) are known to correlate. In this paper, we propose to jointly model their localisation and their intensity in a simple yet efficient Heatmap Regression manner, where the Heatmaps are chosen to depend on the corresponding AU intensities.\n\nchanges, namely Regions of Interest (ROIs)  [19] ,  [25] ,  [26] . Typically spatially-aware approaches employ a two-stage pipeline where facial landmarks, are firstly detected in order to define AU locations, and then, local features for each AU are extracted and, adaptation or fusion mechanisms are introduced to jointly predict AU intensity levels. Our method significantly simplifies the aforementioned two-stage pipeline by localising AUs and estimating their intensity, while also modelling their inter-dependencies, in a single step.\n\nIn this paper, we firstly make the simple observation that AU recognition should be treated as localisation problem where the task is to both localise and classify the Action Units. Motivated by this observation, we propose a new task that consists of jointly localising and estimating the intensity of Action Units, and we formulate this problem using Heatmap Regression. Heatmap Regression is arguably the most successful approach to landmark (key-Fig.  2 . Overview of the main approaches compared in this paper. i) This paper poses the problem of AU intensity estimation in a Heatmap Regression framework, where the AU network, (referred for simplicity as AU-Net) is trained to jointly localise and estimate the intensity of Action Units. We then observe the similarities between this approach and that known to deliver state of the art results in the task of Face Alignment (FAN), and propose three alternatives to incrementally learn our AU-Net. In particular, we propose. ii) an approach where one can use the early features given by a FAN, along with the generated facial landmarks; iii) an approach where the facial landmarks are used to produce AU attention maps, to be fed to the AU-Net alongside with the early features of FAN and; iv) an approach where the original FAN is reparameterised using a small set of additional parameters that can be projected onto the weights of the filters of the FAN. point) localisation  [8] ,  [30] . It boils down to a pixel-wise regression task that indicates the likelihood of a landmark being present at the corresponding spatial location. However, unlike landmarks, AUs can be present or absent in a given image with their intensity spanning from 0 to 5. Hence, their amplitude modelling cannot be treated with the standard probabilistic approach of Heatmap Regression (i.e. with the heatmaps relating to the confidence of a detected landmark). To overcome such limitation, we extend Heatmap Regression to include maps that are modelled according to the corresponding AU intensity. In particular, motivated by the fact that Action Units produce appearance changes around the facial region where they are known to occur, we propose to model the size of a heatmap (i.e. its amplitude and extend) according to the intensity of a given Action Unit. Under this setting, our idea boils down to a simple yet efficient Heatmap Regression approach. This approach is efficient in a way that not only merges the co-occurrence and the spatial correlation of AUs in a single task, but also in a way that bypasses all complexities associated to the typical two-stage pipeline, consisting of registration followed by local feature extraction and classification, often found in AU related works.\n\nBy jointly tackling the problem of AU intensity estimation and localisation using Heatmap Regression one could choose to dismiss the commonly required step of face alignment. Rather doing so, in this work we further propose to integrate this task into our system using transfer learning. In particular, on one hand it is known that AU annotations are scarce and hence it is difficult to train a system for AU recognition that can work well across all types of facial variability (e.g. facial pose, illumination, occlusion). On the other hand, there is a large pool of facial landmark annotations available for all types of facial variability. Since heatmap regression can be used to tackle both tasks, we investigate how and to what extent one can transfer knowledge from a network trained for landmark localisation into a network for AU localisation and intensity estimation. We explore several alternatives for transfer learning through a) fine-tuning, b) adaptation layers, c) attention maps, and d) reparameterisation (see Fig.  2 ). We show that our approach allows for robust AU modelling across a wide range of poses and illumination conditions. This paper reformulates our previous manuscript on Action Unit intensity estimation using Heatmap Regression  [48]  and devises a more robust approach using transfer learning. We show that this approach benefits from the robustness of the facial features given by a similar network trained for the task of facial landmark localisation, yielding state of the art results in three different datasets (FERA 2015, DISFA, FERA 2017). The contributions of this paper are as follows:\n\n• We propose to reformulate the problem of Action Unit intensity estimation in a way that absorbs their localisation. In particular, we are the first to propose to jointly localise and estimate the intensity of Action Units using Heatmap Regression. The use of variable size heatmaps allows the joint modelling of AUs localisation and intensity estimation in a single yet efficient way.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We propose the use of transfer learning to exploit the knowledge of a network trained for a similar task, that of face alignment, in a large-scale of images ranging a wide variety of poses, expressions, and illumination, conditions often hard to find in AU datasets. To this end, we explore several variants and identify an incremental learning approach which significantly reduces the number of weights to be trained, and increases robustness against different views.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "We extensively validate our approach in three challenging datasets, namely BP4D  [54] , DISFA  [28] , and FERA 2017  [55] , yielding state of the art results with an approach that requires little complexity.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "We firstly review the closely related work in the area of facial Action Unit detection and intensity estimation, and then we provide some insight into existing approaches to transfer learning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Action Unit Detection And Intensity Estimation",
      "text": "Facial Action Unit modelling is a longstanding problem in Computer Vision and Affective Computing, that is often split into works that either target their detection, i.e. estimating whether an AU is present or not on a given facial image  [1] ,  [2] ,  [4] ,  [10] ,  [11] ,  [15] ,  [16] ,  [19] ,  [21] ,  [25] ,  [26] ,  [33] ,  [50] ,  [59] ,  [60] , or the more challenging task of estimating their intensity  [14] ,  [20] ,  [39] ,  [40] ,  [41] ,  [42] ,  [49] ,  [53] ,  [56] ,  [57] , given as a value that ranges from 0 (i.e. absence of an AU), to 5 (maximum intensity). Regardless the task, many methods partially share the underlying methodology: some works attempt to leverage cooccurrence and static dependencies among AUs  [15] ,  [57] , some exploit their geometric structure  [26] ,  [67] , or their temporal correlation in time  [19]  and works that combine different means of correlation  [12] ,  [19] ,  [25] ,  [29] .\n\nOne of the most exploited means of AU correlation refers to their spatial structure. Action Units have a geometric structure, i.e. they are spatially correlated to specific facial regions. Early works on AU modelling were targeting the design of some handcrafted features that can inform about local appearance variations that are ultimately related to each AU  [2] . With the development of Convolutional Neural Nets this design was no longer needed, and other techniques to extract local features appeared. A simple approach to extracting local features is that of  [19] , where the face is first registered according to some landmark detection, and each part is cropped independently. Then, CNN-based features can be extracted independently. In a similar fashion,  [67]  proposed to incorporate an intermediate region-specific layer to a CNN to extract separate features at different facial sub-areas, while  [26]  incorporated to a pre-trained CNN two extra layers -coined as the enhancing and the cropping layer -to enforce the network pay more attention to spatial regions with high AU correlation. With such locally-based modelling,  [10]  proposed to introduce a temporal model in a hybrid manner, to jointly exploit the local and temporal correlation of AUs. Building on top of region layers,  [25]  introduced the CNN-based Region of Interest (ROI) detection, which was then incorporated into the local modelling of AUs. In particular,  [25]  proposed independent ROI networks to learn separate filters for different facial regions, which were later used to feed a fully-connected LSTM network to also exploit the temporal correlation. More recently,  [53]  proposed to model AU regions by incorporating a Variational Autoencoder framework (VAE,  [23] ), and  [60]  combined a 2D with 3D CNN for frame-level AU detection at an attempt to leverage spatiotemporal dependencies. All of these works require, however, a good pre-processing step that consists of registering the input image according to some detected facial landmarks. In this paper, we observe that both tasks can be performed together in a rather simple way. While the methodology proposed in this paper is completely new, some works have attempted to jointly detect facial landmarks and perform AU modelling in a unified framework. In an early work,  [59]  proposed an iterative framework whereby a cascaded regression approach was used to detect facial landmarks, and where a Restricted Boltzmann Machine was used to detect the Action Units. Recently,  [50]  proposed to jointly perform facial landmark localisation and AU intensity estimation through a hierarchical, multi-scale region learning pipeline that employs attention maps refinement to ease the learning process. In  [33]  the landmarks are instead used to regularise the features extracted by a CNN, towards driving these to be person-specific. Both  [33]  and  [50]  observe that landmarks carry over important information, either to regularise the features or to generate attention maps. In a more recent approach,  [51]  proposed the use of attention maps that are landmark-free, showing how locally-based features can better model the AU occurrence. In this paper, we propose a rather less computationally expensive method that can deal with estimating the AU intensity by first re-formulating the problem in a way that includes their localisation, and then by incorporating the rich features acquired by a network trained to detect facial landmarks. Our approach offers a significantly less complex yet efficient method that delivers state-of-the-art results in the more challenging problem of estimating the intensity of Action Units.\n\nFinally, it is worth mentioning the recent appearance of methods that work on a weakly supervised or even unsupervised manner  [27] ,  [32] ,  [58] ,  [62] ,  [63] ,  [64] ,  [65] . While these works have shown some interesting advances, they are still behind the performance achieved by fully supervised methods. Although it is out of the scope of this paper, the low computational complexity of our method suggests that it could also be a good approach for learning with scarce labels. We leave this problem for future work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transfer Learning",
      "text": "The goal of transfer learning, often found in the literature as incremental learning, is to adapt the knowledge acquired for a strong task for which a large pool of labels is available to learn a set of potentially unrelated tasks  [35] ,  [36] ,  [38] . One of the simplest approaches to transfer learning consists of finetuning  [18] , often used in face analysis works. For example, it is a common practice to initialise a network with the weights of the pre-trained VGG-Face2  [9] , trained with thousands of images for the task of face recognition. Some examples of this can be found in  [25] ,  [26] ,  [33] . Other works have proposed to add knowledge incrementally, i.e by extracting features from a model trained on a specific task and use them to train another model for a new task. This method advances over fine-tuning in the sense that a model is trained for a new task without forgetting old representations. An example is the progressive networks  [43] , or the adaptive filters  [35] ,  [36] .\n\nIn this paper, we explore different alternatives for transfer feature learning, as well as an approach based on network reparameterisation, which consists of applying a transformation over the Fig.  3 . Dots correspond to the facial landmarks and circles correspond to the location of various AUs. First, facial landmarks are used to define the location of the AUs. Then, Gaussians with varying amplitude are generated in the spatial location of AUs. Some AUs share the same spatial location and hence their location is defined by the same landmarks. For example, AUs 9 and 10 share two Gaussians, but each of them will activate according to their intensity. See Table  1  for all the correspondences between AU spatial locations and facial landmarks.\n\nexisting weights of the pre-trained network. Network reparametrisation is often found in incremental learning approaches, where new tasks are added sequentially to a strong core. For instance,  [24]  applies a tensor decomposition that allows each of the tensor dimensions to adapt a new task. A simple approach that has been shown to perform well in practice consists of projecting the given weights in a convolutional network with a simple projection matrix, learned over the new task  [38] . This approach was also recently applied to the unsupervised adaptation of object landmark detectors  [45] . We observe that we can use such a framework in a supervised manner to transfer the knowledge of a face alignment network to our proposed AU intensity estimation network. We observe that this approach offers significant computational benefits whilst yielding state of the art results.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Joint Au Localisation And Intensity Esti-",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mation",
      "text": "In this Section, we present a novel approach to AU intensity estimation using heatmap regression. The main novelty of this approach lies in the fact that heatmap regression allows joint localisation and intensity estimation of the AUs, as the machine learning task gains a spatial aspect. By using an encoder-decoder approach, we are able to gather features at different spatial levels to yield a dense pixel-wise prediction, facilitating inference, and allowing the network to learn both the spatial relation and cooccurrence of AUs. Our approach differentiates from previous works that apply multi-task learning through joint estimation of AU intensities and landmark localisation, in the sense that AUs' spatial relation and co-occurrence is inherently embedded in the heatmap regression method. In addition, Heatmap Regression does not make use of attention maps to predict the score, it just regresses the heatmaps. Our approach offers significant advances: it yields state of the art results without requiring any complex pre-processing or face alignment, thus effectively reducing the computational cost of inference.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Formulation",
      "text": "Our goal is to train a network that can predict the intensity of a set of AUs. To do so, we propose to reformulate the training process in a way that makes the network jointly detect the location of the AUs, as well as their intensity. This way, we can formulate the joint problem using Heatmap Regression, thus relaxing the training process. This relaxation comes from the fact that Heatmap Regression boils down to local pixel-wise regression, making the network penalise local errors more efficiently, rather than in a global way, as is the case of direct regression. While some works use the facial landmarks to perform Multi-Task learning or generate some attention, we want to formulate a joint training, where the set of images and AUs is augmented by the locations of the latter. Formally, let D = {I i ∈ R 3×W ×H } N i=1 be a set of N RGB W × H images, for which the corresponding AU intensities a i = (a i,1 , . . . , a i,Naus ) are known, with N aus the number of annotated AUs in the dataset. Our first goal is to extend the training annotations with a set of locations p i = (p i,1 , . . . , p i,Naus ), where each p i,j ∈ R 2×Ni,j corresponds to the x, y coordinates of the N i,j locations where an AU j is known to produce appearance changes for that particular AU when it is activated. Each AU will often be placed at two locations, symmetric with respect to the facial axis of symmetry. Some of the AUs will also produce changes at a third location, placed along the symmetry axis. Given that there are no annotations available to place the ground-truth locations, we place them using a priori knowledge about their location and with respect to the 68 facial landmarks used for landmark localisation. The positions of the AUs as derived from the given landmarks is given in Fig.  3 . The exact correspondences are illustrated in Table  1 . After having generated the AU positions, our training set is now defined as\n\nTABLE 1 Correspondences between landmarks and AUs. In our setting each AU can be described with the activation of up to three Gaussians on the right, left and centre of the face. Landmark indexing is as noted by  [",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Heatmap Regression",
      "text": "Once the training set has been augmented with the AU locations, we can define the training procedure. Inspired by the success of Heatmap Regression for facial landmark localisation, we propose to formulate the training problem in a similar way. However, locating the AUs only, would not solve the problem of estimating their intensity, and thus we need to accommodate the latter into the localisation problem. To do so, we propose to attach each AU to a corresponding heatmap. Each heatmap will contain one, two or three Gaussians, according to the number of points defined in p i (see Table  1  for all different correspondences). Following existing works in Heatmap Regression, we will work with heatmaps that are quarter the size of the input image, i.e. p i,j ← 0.25•p i,j . Each Gaussian g i,j is defined as a 2D map having the same size as that of the input image, where the value at each position x is given by:\n\nwith a i,j the intensity of the j-th AU for image i. The heatmap for AU j is thus defined as H i,j (x) = max k g i,j (x). Under this representation, the heatmaps form an N aus × W/4 × H/4 tensor, with N aus the number of AUs, and W and H the width and height of the images, respectively (256 in our setting).\n\nOur goal is then to train a network Φ(• , θ) that, given an image I, regresses a set of heatmaps H ∈ R Naus×W/4×H/4 . The network is parametrised by the parameters θ. The learning is formulated in a standard heatmap regression fashion, i.e. as finding the weights θ that minimise the squared loss between the output and the ground-truth maps:\n\n(2)",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Network",
      "text": "For the sake of clarity, we introduce the network description and training herein, as it will constitute what we refer to as the backbone throughout the next section. In particular, the network Φ follows a similar architecture as that of the Face Alignment Network (FAN,  [8] ), with small differences. In our setting, the network Φ receives an input image I ∈ R 3×256×256 , and first applies a downsampling convolutional 7 × 7 filter to it, halving its resolution and increasing the number of channels to 64. Then, a set of 3 Convolutional Blocks (referred to as ConvBlock  [7] , see Fig.  4 ), are used to bring the number of channels to 128 and the spatial resolution to 64 × 64. We will refer to these layers as conv2, conv3, and conv4, respectively. Then, the features after the conv4 layer are fed into a single Hourglass network  [30]  (Fig.  4 ), which is an encoder-decoder network, composed of several ConvBlock and skip connections that aggregate the features at different scales. While  [6] ,  [30]  used a set of",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Inference",
      "text": "Inference in this setting is straightforward. To get the AU intensities, one simply needs to crop the face image according to some face detection and forward it to the trained network. The network returns a set of heatmaps, from which the AU intensities can be inferred by just finding the maximum of each map. Note that our method does not require to register the face image before inserting it to the network.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Incremental Heatmap Regression For Au Localisation",
      "text": "Using heatmap regression for AU recognition and localisation allows us to make use of the great progress that we have witnessed recently for the problem of facial landmark localisation. More specifically, we propose to transfer knowledge from a network trained for face alignment with hundreds of thousands in-thewild images spanning a large set of poses, expressions, and illumination into the proposed network for AU intensity estimation and localisation. This in turn allows us to overcome to some extent the limitations of existing AU datasets related to facial variability (e.g. number of subjects, facial pose, occlusions etc.).\n\nIn contrast to the previous works that have attempted to exploit the correlation between facial expressions and localisation  [50] ,  [59]  through a multi-task learning framework, we propose to use transfer learning to learn AU intensities from rich facial features retrieved from a pre-trained face alignment model.\n\nThe first and simplest approach to the proposed transfer learning approach consists of fine-tuning the pre-trained network for the target task. Besides fine-tuning, we propose and explore three different alternatives to accomplish the task of transfer learning, which are described below. First, we briefly describe the architecture and pre-training of the face alignment network (Sec. 4.1) and then, we explain how we fine-tune an AU estimation model from a face alignment one (Sec. 4.2). Finally, we describe our three alternatives, namely that of adaptation layers (Sec. 4.3), attention maps (Sec. 4.4) and network reparametrisation (Sec.  4.5) . In what follows, we will refer to the face alignment network as FAN, whereas the corresponding part of the network targeted with the AU heatmap regression will be referred, for simplicity, as AU-Net.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Fan Pre-Training",
      "text": "Our in-house implementation of FAN follows that of the AU-Net described in Sec. 3.3. The output of the network is a set of 68 heatmaps, each corresponding to a single landmark. The FAN is trained for 80 epochs on LS3D-W  [8]  training set which is the largest and most challenging facial landmark dataset to date (approximately 230, 000 images). The network yields a validation accuracy of 9.03 point-to-point Euclidean distance, on par with that reported in  [8] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Method 1: Fine -Tuning",
      "text": "Our first and simplest approach to transfer learning consists of fine-tuning the pre-trained network. In particular, we observe that one can depart from the face alignment network and finetune it for the proposed AU localisation and intensity estimation by using a small learning rate. This will allow the network to slightly move from a very similar problem (that of facial landmark localisation) to our target task. We experimentally validate that, in line with existing works that suggest fine-tuning as a strong adaptation mechanism, such a simple technique already improves performance over training the network from scratch.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Method 2: Adaptation Layers",
      "text": "Our second approach to incremental learning consists of transferring the features generated from the Face Alignment Network (FAN) to a second network, targeted with producing the Action Unit heatmaps. In this paper, we conjecture that the early features produced by a strong FAN provide with rich facial representations, and we thus propose to inject this knowledge into a second learnable network. In addition to the early features, we also inject the produced heatmaps, as these are nothing but a geometric representation of the face. The heatmaps consolidate the spatial configuration of all landmarks, and hence encode information regarding location, pose, shape and expression of a face in an image. Finally, given that they are probabilistic maps, they provide both coordinate and confidence information which can be useful for understanding spatial context and modelling part relationships. Overall, we posit that the generated landmark heatmaps encode rich facial geometry representations that could operate as an attention mechanism that drives focus on regions of the face that are very informative for the task of AU prediction. Hence, it is reasonable to attempt to incorporate to the new task of AU estimation, this rich facial geometry information from face alignment. We study the impact of these heatmaps on the task of AU intensity estimation through several ablation studies in Sec.  6 .\n\nThe AU network has a similar structure than that of the Hourglass described in Sec. 3.3. However, rather than using as input the facial image used to extract the facial landmarks, we use as input a combination of the features produced after the conv4 block of the FAN, and the produced heatmaps  (68) .\n\nIn order to inject the early features and produced heatmaps into the AU network, we use an adaptation layer, as depicted in Fig.  6, a ). This adaptation layer is composed of a branch that processes the early features coming from the FAN, and another branch that processes the generated landmark heatmaps. Let H ∈ R 68×64×64 be the output heatmaps corresponding to the facial landmarks, and let f conv4 ∈ R 128×64×64 be the features from the conv4 layer of the FAN. In order to integrate these two tensors, we apply to each of them a 1 × 1 filter, followed by a Batch Normalisation layer and a ReLU activation layer. The 1 × 1 filters produce, for both cases, a 128 × 64 × 64 tensor. The output of both branches are then added and sent to the AU network. The AU network then receives the combined features, and passes it through an Hourglass network with all filters set to have a kernel size of 1, rather than the 3 of the original FAN network. The output of this network is then a set of N aus heatmaps. The training is done through the classical heatmap regression depicted in Sec. 3. With the 1 × 1 filters, and the removal of the first convolutional blocks, the new network comprises only ∼ 1M parameters.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Method 3: Attention Maps",
      "text": "A different alternative consists of generating attention maps from the generated heatmaps. This approach is depicted in Fig.  6, b ). In particular, it is important to recall, from Sec. 3, that at training time the target heatmaps, in the original setting, are located according to the ground-truth landmarks. In other words, there is a clear relation between the facial landmarks and the location of the Action Unit heatmaps. Thus, if we are to transfer the knowledge from the FAN network to the AU network, it is natural to explore the use of attention maps, generated from the heatmaps produced for the facial landmarks.\n\nIn this setting, we use the corresponding heatmaps from the FAN, and extract the corresponding landmarks by applying an argmax operator. Then, using the method described in Sec. 3.2, we generate a new set of heatmaps Ĥ ∈ R Naus×64×64 . The heatmaps Ĥ are then forwarded to the corresponding branch described above. However, it is worth noting that we do not have the ground-truth labels to produce a set of heatmaps that vary according to the Action Unit intensities. Instead, we are interested in generating an attention map, i.e. a heatmap that locates the Action Units, without regarding to whether a given AU is actually present or not. To this end, the heatmaps in Ĥ are generated using a fixed intensity a = 1.\n\nNote that the attention maps are generated on the fly i.e during training the generated heatmaps from the FAN are used to generate the attention maps. After having acquired the attention maps we follow the process described in Sec. 4.3. The new network is again ∼ 1M parameters.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Method 4: Reparametrisation Of Fan",
      "text": "A different alternative consists of using the reparametrisation approach of  [37] ,  [45] . A visual representation is depicted in Fig.  2 , iv). In particular, we depart from the FAN network Φ(• , θ FAN ), with parameters θ FAN , trained to detect facial landmarks, as described in Sec. 4.1. We now wish to adapt the model Φ for the task of AU intensity estimation using heatmap regression, to yield a new set of weights θ AU-Net , so that Φ(• , θ θAU-Net ) would produce the desired AU heatmaps. As previously mentioned, Φ is chosen to be an Hourglass, which is uniquely parametrised by convolutional and batch normalisation layers. The network Φ is modified to return N aus heatmaps, rather than the 68 heatmaps of facial landmarks, i.e. Φ(• , θ θAU-Net ) replicates the same structure for all layers but the very last one. Under this setting, the adaptation method proposed in  [45]  boils down to reparameterising the convolutional layers by learning a series of weights that are projected onto the original filters, to yield a new set of weights for the target task. Let us denote the weights of the convolutional layer L of the original network as θ L FAN ∈ R Cin×Cout×k×k , with C in and C out the number of input and output channels, respectively, and k the kernel size. Then, following  [45]  we use the following reparametrisation of the weights θ L :\n\nwhere W L ∈ R Cout×Cout is the learnable projection matrix, and × n denotes the n-mode product of tensors. The set of weights θ L AU-Net are of the same size than those of θ L FAN , and can thus be replaced into the original network. Then, the learning is formulated in a heatmap regression fashion, although now the weights θ L FAN remain frozen, and only the weights W L are to be learned. This approach, besides advancing the field of unsupervised adaptation, offers significant computational savings, as now the learnable weights have only C 2 out parameters, contrary to the C out × C in × k 2 original set of parameters. Considering that the majority of filters in the Hourglass are of k = 3, the computational saving is, for C in = C out , about 9 times the number of parameters. We observe that, while the original FAN network Φ comprises ∼ 1.6M parameters, the new set of learnable weights W L reduce to only ∼ 130K parameters. This method allows to efficiently transfer the knowledge from the pretrained network to the target one.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Training And Implementation Details",
      "text": "In the following Sections, we evaluate each of the discussed alternatives for transfer learning, and compare them to training a model from scratch, as presented in Sec. 3. We then compare our method against existing works reporting AU intensity estimation. Note that, while we show qualitatively how our method is capable of localising the Action Units across a wide span of poses and expressions, we are primarily interested in demonstrating the superiority of our method at the task of estimating the intensity of AUs, and we are not interested in the precise localisation error.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Databases",
      "text": "We evaluate our models on three benchmark databases -FERA 2015  [54] , DISFA  [28]  and FERA 2017  [55] . All these datasets contain a set of videos each showing an individual responding to emotion-elicitation tasks. FERA2015  [54] : The corpus of the FERA2015 challenge is based on the BP4D dataset  [61] , which is composed of 41 subjects performing 8 tasks, plus an extra test set of 20 subjects, performing additional tasks. The original corpus was released as part of the training and development partitions of the FERA 2015 challenge, whereas the test set, which is not publicly available, was used to rank participants. The training and development partitions are split into 21 and 20 subjects. In total, there are 328 videos corresponding to the training/validation partitions, and 160 videos corresponding to the test partition. In this paper, we use the official partitions, and report results on both the validation and test set. Given that the test set is not publicly available, we compare our results with those of the challenge winners. All partitions are annotated with 5 Action Units intensity levels. The training set comprises ∼ 75k frames, whereas the validation and test set contain ∼ 69k and ∼ 76K frames, respectively. DISFA  [28] : The DISFA dataset contains video recordings of 27 subjects while watching Youtube videos. Each clip is ∼ 4 length, and has been manually annotated with the intensity of 12 AU. Given that no official partitions are defined for DISFA, we follow existing works and perform a three-fold cross validation An additional set of 20 videos was added as the official test, emanating from the BP4D+ dataset  [66] . This dataset poses a great challenge in multi-view facial expression recognition, which we prove can be efficiently performed with a computationally simple model. The AU intensity annotations were extended to cover a total number of 7. For FERA 2017, we use again the official partitions and we report the predictions against the corresponding annotations. FERA 2017 contains roughly ∼ 1.29M frames for training, ∼ 696k frames for validation, and ∼ 363k for testing.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We use standard error measures to evaluate AU intensity estimation models. The first measure is the intra-class correlation (ICC(3,1),  [52] ), commonly used in behavioural sciences to measure agreement between annotators, and used to rank participants in the FERA challenges. The second measure is the mean squared error (MSE) mainly used for prediction problems.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Set-Up",
      "text": "All experiments are carried out using the Pytorch library for Python  [34] . The adaptation layers along with all versions of the AU estimation network (AU-NET) are trained from scratch with Adam optimiser  [22]  and batch size 48. The weight decay is set to 10 -6 and momentum to 0.9. We additionally use cosine annealing scheduler with step 5. Note that during training, FAN weights remain frozen. The ground-truth target heatmaps are generated according to the method described in Sec. 3. For BP4D and DISFA, we use the landmarks extracted from the publicly available code of iCCR  [46] ,  [47] , whereas for FERA2017 we used the official implementation of FAN  [8] . These landmarks are used to define the heatmaps for training. Note that the landmarks are not needed at test time. The facial images are then tightly cropped to 256 × 256 resolution to be passed through the corresponding networks. In addition, we use some random augmentation, consiting of flipping, rotation (from -30°to 30°), color jittering, scale noise (from 0.8 to 1.2) and random occlusion. In order to ensure a fair comparison, we re-implemented and re-evaluated the Heatmap Regression model proposed in Sec. 3 and  [48] . Our new results account for the stronger augmentation and training strategy applied herein. In this Section we analyse and evaluate all our proposed approaches for AU prediction. To do so, we experimentally evaluate the performance of all our methods using ICC score and Mean Square Error on all three aforementioned datasets (FERA2015, FERA2017 and DISFA). Results on FERA 2015 and DISFA are shown in Table  2 , while results on FERA 2017 are on Table  3 . Also, to further allow for a comprehensive evaluation of the strengths and weaknesses of each of the methods we include a thorough evaluation of their complexity, including the capacity of each model, the number of floating point operations per second, and the average time per forward pass. The summary of complexity is illustrated in Table  5 , along with the performance each method reports on the validation set of BP4D, often used as the referent benchmark for comparison in existing works. In addition to our proposed methods, we include a strong baseline based on a ResNet-18  [17] , which is a rather deep network of approximately 11M parameters. For the task of AU intensity estimation, we modify the last layer to generate predictions that match the number of Action Units. Then, we simply regress AU intensity levels. All",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "In-House Evaluation",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Heatmap Regression Vs. Regression",
      "text": "We approach the task AU intensity estimation from a geometric perspective. We train our models in a Heatmap Regression fashion which allows us to jointly localise the AUs and estimate their intensity. This approach is simple and straightforward: an image is passed forward to the AU-net that generates a set of heatmaps from which simple retrieval of the maximum value gives AUs predictions. To evaluate the impact of heatmap regression, we train a model to simply regress AU intensity levels rather than regressing heatmaps. This model is ResNet-18 which also serves as our baseline. As shown in Table  2  and Table  3 , Heatmap Regression significantly outperforms ResNet-18 both in terms of ICC score and Mean Square Error in all three datasets. Notably, our method achieves better results than direct regression with a model of much less capacity.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Heatmap Regression Vs Transfer Learning",
      "text": "We extend our approach of AU intensity estimation through Heatmap Regression by proposing methods that absorb knowledge of a pre-existing network for face alignment. Thus, we further investigate whether our four different methods, i.e finetuning, adaptation layers, reparametrisation and attention maps, that leverage information from FAN, benefit the task of Heatmap Regression for AU estimation. To evaluate their impact, we first train all methods under the very same training scenario, i.e same data augmentation, same training configuration etc., and test the performance of each in terms of ICC and MSE score. Then, we further evaluate their computational requirements with regards results in Table  5 . We refer to the Heatmap Regression method presented in Sec. 3 as trained from scratch, and to each of the transfer learning methods by their corresponding technique.\n\nAs shown in Table  2  and Table  3  transfer learning improves over training from scratch in almost all three datasets in terms of both ICC and MSE. For FERA 2015, all transfer learning methods yield an ICC score that ranges between 0.70 and 0.72, while the model trained from scratch achieves an ICC score of 0.69. The same behaviour is observed for DISFA, where transfer learning improves over training from scratch, with ICC scores ranging from 0.54 to 0.57 for the former, vs. the 0.53 given by the latter. However, we observe that for FERA 2017 both training from scratch and applying transfer learning appear to deliver similar results, which we attribute to the fact that indeed FERA 2017 is a large-scale dataset that includes a large variety of poses. Under such a large pool of videos, training the model from scratch suffices to yield competitive results.\n\nRegarding complexity (see Table  5 ), we observe that all our methods deploy models with a small number of parameters that roughly range between 1.6M to 2.65M . The transition from having a model trained from scratch to transfer learning requires no extra parameters when fine-tuning, and a negligible number of extra parameters for the reparametrisation approach. The use of adaptation layers and attention maps incur in only an extra 1M number of parameters. We can observe that this increase is negligible compared to the original Hourglass of  [8] , which comprises 3M . Similarly, the number of slightly increase in the case of adaptation layers and attention maps.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Comparison Between Transfer Learning Methods",
      "text": "We now turn our analysis to the comparison between each of the proposed methods for transfer learning. While the discussed approaches deliver state of the art results (see Sec. 7) it is worth discussing the pros and cons of each, according to their performance and complexity.\n\nThe first proposed approach to transfer learning, that of finetuning, is undoubtedly the simplest in terms of complexity, which matches that of training the network from scratch. We observe that fine-tuning brings a considerable gain in performance w.r.t. training from scratch, especially in DISFA, which shows to be an effective and efficient way to transfer learning.\n\nIn the same line, we can observe that, while the reparametrisation approach results in an even more efficient approach to that of fine-tuning (much less number of learnable parameters, with same inference complexity), its performance is slightly worse. Arguably, the best gain in performance comes from the adaptation layers and the attention maps. While the latter includes an extra step to convert the facial landmark heatmaps into AUattention maps, the complexity can be said to be the same. However, we observe that using the adaptation layer directly from the heatmaps returned by the FAN outperforms the results given by the attention maps. We attribute this to the fact that the detected heatmaps provide some confidence, which can be more effectively used by the network to automatically infer the attention.\n\nIn summary, while fine-tuning and reparametrisation seem to be the methods with the least complexity, the adaptation layers method yields the best performance. However, it is worth noting that, regardless this method being the most complex from the proposed ones, its complexity and number of parameters compared to those of the Resnet-18 suggest this as an efficient method for AU localisation, and thus we choose it for comparison w.r.t. state of the art works.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Core Task With Random Weights",
      "text": "The proposed transfer learning methods depart from a core network pre-trained for the task of Face Alignment, and include an extra set of learnable weights to perform the target task of AU localisation and intensity estimation. With the great success of the Heatmap Regression method proposed in Sec. 3, it is natural to explore whether the gain in performance comes from having a more constrained network, or from the actual features inherited from the FAN network. To validate that the contribution of the transfer learning methods does not come from the little capacity added to the core network, we study the performance of using the adaptation layers using as a core network a FAN-like network that is initialised with random weights, and that remains frozen with these randomly initialised weights. The results of this study are those referred to as random backbone in Table  2  and Table  3 . It can be seen that, while learning only the extra network still produces competitive results, having the rich representations given by the FAN is crucial to achieve state of the art results. Note that, despite the network receiving the features from a randomly initialised FAN, the generated features after the conv4 layer are still conditioned to the input image, through some fixed random non-linear projections.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Fine-Tuning After Transfer Learning",
      "text": "In addition to the aforementioned studies, we also explored an alternative approach that consists of fine-tuning the whole pipeline after the transfer learning step. In particular, we unfreeze the FAN network and we fine-tune the whole pipeline. We, however, observed no improvement in the performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Comparison With State Of The Art",
      "text": "In this Section, we report the results of our proposed approach w.r.t. state of the art results in both the validation and test partitions of FERA 2015 and FERA 2017, as well as after the 3-fold crossvalidation experiment on DISFA. Similarly to Sec. 6, we use as a baseline a ResNet-18  [17]  which is trained to directly regress AU intensity levels. In addition to that, we report the results of the network trained from scratch, herein referred simply as HR. Finally, for the sake of clarity, we report the results of the best performing method from the proposed transfer learning approaches, that of the Adaptation Layers (Sec. 4.3), herein simply referred to as Ours.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Fera 2015 Dataset",
      "text": "The results for FERA 2015 -Development are shown in Table  4 , whereas those regarding the test partition are shown in Table  7 . It is important to recall that, given that FERA 2015 is not publicly available, current works report on the development set, hence the lack of up-to-date results on the test partition. Despite the recent advances and the improved results on the development set, our method outperforms state of the art results by a considerable margin. We observe that the transfer learning approach results crucial to attain a new state of the art result in both partitions, proving the effectiveness of such approach when working with small datasets.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Disfa Dataset",
      "text": "We report the results of the 3-fold cross-validation experiment on DISFA in Table  4 . We can observe that both Heatmap Regression  and transfer learning outperform existing methods in such a challenging dataset. We attribute this gain to the fact that localising the AUs is more effective than resorting to complex Autoencoder networks such as the one proposed in the 2DC  [53] . With Heatmap Regression, the network returns a structured representation that already captures the AU dependencies in a geometric way, and thus no additional dependencies need to be learned.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Fera2017 Dataset",
      "text": "Results on FERA 2017 validation and test set are given in Table  6 .\n\nNote that for FERA 2017 we choose to report the Root Mean Squared Error as this was the measure of choice for the challenge. Our approach achieves an ICC score of 0.64 on the validation set which is by 7% better than the ICC score of FERA 2017 challenge winners, that attained an ICC score of 0.60. Similarly, in terms of RMSE score our method method outperforms challenge winners by a 5% margin. The same pattern is also found on the test set, where our method reports an ICC score of 0.50, which surpasses 0.45 ICC reported by the challenge winners. We can also observe that both HR-Scratch and our transfer learning approach deliver similar results, which we attribute to the fact that FERA 2017 is already a large-scale dataset.\n\nQualitative evaluation: In addition to the reported results, we show the capabilities of our method to actually infer both the location and the intensity of Action Units in Fig.  7 . We observe that for FERA 2017, that spans a large set of poses, our method is capable of estimating the location and intensity accurately, thus proving the efficacy of Heatmap Regression for the task of AU intensity estimation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "We have proposed a simple yet efficient approach for the problem of facial Action Unit intensity estimation: that of joint localisation and intensity estimation through heatmap regression. To accommodate the varying AU levels in the framework of heatmap regression, we modify the ground-truth heatmaps by changing their size and amplitude according to the corresponding AU intensity. Then, motivated by the similarities of our approach with these of the face alignment task, along with the fact that the task of face alignment is equipped with rich annotations, we reform the task of AU heatmap regression with an incremental learning approach. To do so, we incorporate to our setting a pre-trained facial landmark network that provides us with rich face related features across a variety of poses and illuminations. We conducted extensive experiments illustrating how the proposed approach systematically improves Intra Class Correlation (ICC) and thus achieve state of the art results on three benchmark datasets: FERA2015, DISFA and FERA2017.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Facial AUs have strong spatial correlations and co-dependent",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview of the main approaches compared in this paper. i) This paper poses the problem of AU intensity estimation in a Heatmap Regression",
      "page": 2
    },
    {
      "caption": "Figure 3: Dots correspond to the facial landmarks and circles correspond",
      "page": 4
    },
    {
      "caption": "Figure 3: The exact correspondences",
      "page": 4
    },
    {
      "caption": "Figure 4: ), are used to bring the number of",
      "page": 5
    },
    {
      "caption": "Figure 4: ), which is an encoder-decoder",
      "page": 5
    },
    {
      "caption": "Figure 4: Convolutional Block, main building block in all the networks in",
      "page": 5
    },
    {
      "caption": "Figure 5: Main pipeline used for Heatmap Regression. The network re-",
      "page": 5
    },
    {
      "caption": "Figure 6: Proposed feature adaptation from FAN to the AU estimation",
      "page": 6
    },
    {
      "caption": "Figure 7: Visual examples of AU localisation for three different AUs on FERA 2017. We superimpose to the input images the regressed heatmaps",
      "page": 11
    },
    {
      "caption": "Figure 7: We observe",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: for all the will often be placed at two locations, symmetric with respect to",
      "data": [
        {
          "AU 1": "AU 2",
          "21\n22\n21,22,27": "18\n25\n-"
        },
        {
          "AU 1": "AU 4",
          "21\n22\n21,22,27": "21\n22\n21,22, 27"
        },
        {
          "AU 1": "AU 5",
          "21\n22\n21,22,27": "37,38\n43,44\n-"
        },
        {
          "AU 1": "AU 6",
          "21\n22\n21,22,27": "1,41,31\n15,46,35\n-"
        },
        {
          "AU 1": "AU 9",
          "21\n22\n21,22,27": "31\n35\n28"
        },
        {
          "AU 1": "AU 10",
          "21\n22\n21,22,27": "31\n35\n51"
        },
        {
          "AU 1": "AU 12",
          "21\n22\n21,22,27": "48\n54\n-"
        },
        {
          "AU 1": "AU 14",
          "21\n22\n21,22,27": "48\n54\n-"
        },
        {
          "AU 1": "AU 15",
          "21\n22\n21,22,27": "48\n54\n-"
        },
        {
          "AU 1": "AU 17",
          "21\n22\n21,22,27": "57\n8\n-"
        },
        {
          "AU 1": "AU 20",
          "21\n22\n21,22,27": "48\n54\n51"
        },
        {
          "AU 1": "AU 25",
          "21\n22\n21,22,27": "-\n-\n61,64"
        },
        {
          "AU 1": "AU 26",
          "21\n22\n21,22,27": "-\n-\n61,64"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning to transfer: transferring latent task structures and its application to person-specific facial action unit detection",
      "authors": [
        "T Almaev",
        "B Martinez",
        "M Valstar"
      ],
      "year": "2015",
      "venue": "International Conference on Computer Vision"
    },
    {
      "citation_id": "2",
      "title": "Local gabor binary patterns from three orthogonal planes for automatic facial expression recognition",
      "authors": [
        "T Almaev",
        "M Valstar"
      ],
      "year": "2013",
      "venue": "Affective Computer and Intelligent Interaction"
    },
    {
      "citation_id": "3",
      "title": "Support vector regression of sparse dictionary-based features for view-independent action unit intensity estimation",
      "authors": [
        "M Amirian",
        "M Kchele",
        "G Palm",
        "F Schwenker"
      ],
      "year": "2017",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "4",
      "title": "Cross-dataset learning and person-specific normalisation for automatic action unit detection",
      "authors": [
        "T Baltrusaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "5",
      "title": "Aumpnet: Simultaneous action units detection and intensity estimation on multipose facial images using a single convolutional neural network",
      "authors": [
        "J Batista",
        "V Albiero",
        "O Bellon",
        "L Silva"
      ],
      "year": "2017",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "6",
      "title": "Human pose estimation via convolutional part heatmap regression",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "7",
      "title": "Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "How far are we from solving the 2d & 3d face alignment problem?",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "10",
      "title": "Learning spatial and temporal cues for multi-label facial action unit detection",
      "authors": [
        "W Chu",
        "F De La Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "11",
      "title": "Selective transfer machine for personalized facial action unit detection",
      "authors": [
        "W.-S Chu",
        "F La Torre",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Learning facial action units with spatiotemporal cues and multi-label sampling",
      "authors": [
        "W.-S Chu",
        "F La Torre",
        "J Cohn"
      ],
      "year": "2019",
      "venue": "Image and vision computing"
    },
    {
      "citation_id": "13",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen",
        "J Hager"
      ],
      "year": "2002",
      "venue": "A Human Face"
    },
    {
      "citation_id": "14",
      "title": "Variational gaussian process auto-encoder for ordinal prediction of facial action units",
      "authors": [
        "S Eleftheriadis",
        "O Rudovic",
        "M Deisenroth",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Asian Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Pattnet: Patch-attentive deep network for action unit detection",
      "authors": [
        "I Ertugrul",
        "L Jeni",
        "J Cohn"
      ],
      "year": "2019",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "16",
      "title": "Multi view facial action unit detection based on cnn and blstm-rnn",
      "authors": [
        "J He",
        "D Li",
        "B Yang",
        "S Cao",
        "B Sun",
        "L Yu"
      ],
      "year": "2017",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "17",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "What makes imagenet good for transfer learning? arXiv preprint",
      "authors": [
        "M Huh",
        "P Agrawal",
        "A Efros"
      ],
      "year": "2016",
      "venue": "What makes imagenet good for transfer learning? arXiv preprint",
      "arxiv": "arXiv:1608.08614"
    },
    {
      "citation_id": "19",
      "title": "Deep learning the dynamic appearance and shape of facial action units",
      "authors": [
        "S Jaiswal",
        "M Valstar"
      ],
      "year": "2016",
      "venue": "Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Continuous au intensity estimation using localized, sparse facial feature space",
      "authors": [
        "L Jeni",
        "J Girard",
        "J Cohn",
        "F La Torre"
      ],
      "year": "2013",
      "venue": "EmoSPACE"
    },
    {
      "citation_id": "21",
      "title": "Facial action detection using blockbased pyramid appearance descriptors",
      "authors": [
        "B Jiang",
        "M Valstar",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "Social Computing"
    },
    {
      "citation_id": "22",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "23",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "24",
      "title": "T-net: Parametrizing fully convolutional nets with a single high-order tensor",
      "authors": [
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "T-net: Parametrizing fully convolutional nets with a single high-order tensor",
      "arxiv": "arXiv:1904.02698"
    },
    {
      "citation_id": "25",
      "title": "Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Eac-net: A region-based deep enhancing and cropping approach for facial action unit detection",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu",
        "L Yin"
      ],
      "year": "2017",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "27",
      "title": "Self-supervised representation learning from videos for facial action unit detection",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Facial action units intensity estimation by the fusion of features with multi-kernel support vector machine",
      "authors": [
        "Z Ming",
        "A Bugeau",
        "J Rouas",
        "T Shochi"
      ],
      "year": "2015",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "30",
      "title": "Stacked hourglass networks for human pose estimation",
      "authors": [
        "A Newell",
        "K Yang",
        "J Deng"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "Facial action unit intensity prediction via hard multi-task metric learning for kernel regression",
      "authors": [
        "J Nicolle",
        "K Bailly",
        "M Chetouani"
      ],
      "year": "2015",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "32",
      "title": "Multi-label co-regularization for semi-supervised facial action unit recognition",
      "authors": [
        "X Niu",
        "H Han",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Automatic differentiation in pytorch",
      "authors": [
        "A Paszke",
        "S Gross",
        "S Chintala",
        "G Chanan",
        "E Yang",
        "Z Devito",
        "Z Lin",
        "A Desmaison",
        "L Antiga",
        "A Lerer"
      ],
      "year": "2017",
      "venue": "Autodiff workshop -NeurIPS"
    },
    {
      "citation_id": "35",
      "title": "Learning multiple visual domains with residual adapters",
      "authors": [
        "S.-A Rebuffi",
        "H Bilen",
        "A Vedaldi"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "36",
      "title": "Efficient parametrization of multi-domain deep neural networks",
      "authors": [
        "S.-A Rebuffi",
        "H Bilen",
        "A Vedaldi"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Incremental learning through deep adaptation",
      "authors": [
        "A Rosenfeld",
        "J Tsotsos"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Incremental learning through deep adaptation",
      "authors": [
        "A Rosenfeld",
        "J Tsotsos"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Multi-output laplacian dynamic ordinal regression for facial expression recognition and intensity estimation",
      "authors": [
        "O Rudovic",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Automatic pain intensity estimation with heteroscedastic conditional ordinal random fields",
      "authors": [
        "O Rudovic",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "Advances in Visual Computing"
    },
    {
      "citation_id": "41",
      "title": "Context-sensitive conditional ordinal random fields for facial action intensity estimation",
      "authors": [
        "O Rudovic",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "Int'l Conf. Computer Vision -Workshop"
    },
    {
      "citation_id": "42",
      "title": "Context-sensitive dynamic ordinal regression for intensity estimation of facial action units",
      "authors": [
        "O Rudovic",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Progressive neural networks",
      "authors": [
        "A Rusu",
        "N Rabinowitz",
        "G Desjardins",
        "H Soyer",
        "J Kirkpatrick",
        "K Kavukcuoglu",
        "R Pascanu",
        "R Hadsell"
      ],
      "year": "2016",
      "venue": "Progressive neural networks",
      "arxiv": "arXiv:1606.04671"
    },
    {
      "citation_id": "44",
      "title": "300 faces in-the-wild challenge: database and results. Image and Vision Computing, 2016. 300-W, the First Automatic Facial Landmark Detection in-the-Wild Challenge",
      "authors": [
        "C Sagonas",
        "E Antonakos",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "venue": "300 faces in-the-wild challenge: database and results. Image and Vision Computing, 2016. 300-W, the First Automatic Facial Landmark Detection in-the-Wild Challenge"
    },
    {
      "citation_id": "45",
      "title": "Object landmark discovery through unsupervised adaptation",
      "authors": [
        "E Sanchez",
        "G Tzimiropoulos"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "Cascaded continuous regression for real-time incremental face tracking",
      "authors": [
        "E Sánchez-Lozano",
        "B Martinez",
        "G Tzimiropoulos",
        "M Valstar"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "A functional regression approach to facial landmark tracking",
      "authors": [
        "E Sánchez-Lozano",
        "G Tzimiropoulos",
        "B Martinez",
        "F De La Torre",
        "M Valstar"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "48",
      "title": "Joint action unit localisation and intensity estimation through heatmap regression",
      "authors": [
        "E Sánchez-Lozano",
        "G Tzimiropoulos",
        "M Valstar"
      ],
      "year": "2018",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "49",
      "title": "Markov random field structures for facial action unit intensity estimation",
      "authors": [
        "G Sandbach",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "Int'l Conf. Computer Vision -Workshop"
    },
    {
      "citation_id": "50",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "L Ma"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "51",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "Y Wu",
        "L Ma"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Intraclass correlations: uses in assessing rater reliability",
      "authors": [
        "P Shrout",
        "J Fleiss"
      ],
      "year": "1979",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "53",
      "title": "Deepcoder: Semi-parametric variational autoencoders for automatic facial action coding",
      "authors": [
        "D Tran",
        "R Walecki",
        "O Rudovic",
        "S Eleftheriadis",
        "B Schuller",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision"
    },
    {
      "citation_id": "54",
      "title": "Fera 2015 -second facial expression recognition and analysis challenge",
      "authors": [
        "M Valstar",
        "T Almaev",
        "J Girard",
        "G Mckeown",
        "M Mehu",
        "L Yin",
        "M Pantic",
        "J Cohn"
      ],
      "year": "2015",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "55",
      "title": "Fera 2017-addressing head pose in the third facial expression recognition and analysis challenge",
      "authors": [
        "M Valstar",
        "E Sánchez-Lozano",
        "J Cohn",
        "L Jeni",
        "J Girard",
        "Z Zhang",
        "L Yin",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "56",
      "title": "Copula ordinal regression for joint estimation of facial action unit intensity",
      "authors": [
        "R Walecki",
        "O Rudovic",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "Deep structured learning for facial action unit intensity estimation",
      "authors": [
        "R Walecki",
        "O Rudovic",
        "V Pavlovic",
        "B Schuller",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "58",
      "title": "Weakly supervised dual learning for facial action unit recognition",
      "authors": [
        "S Wang",
        "G Peng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "59",
      "title": "Constrained joint cascade regression framework for simultaneous facial action unit recognition and facial landmark detection",
      "authors": [
        "Y Wu",
        "Q Ji"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Facs3d-net: 3d convolution based spatiotemporal representation for action unit detection",
      "authors": [
        "L Yang",
        "I Ertugrul",
        "J Cohn",
        "Z Hammal",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2019",
      "venue": "Affective Computer and Intelligent Interaction"
    },
    {
      "citation_id": "61",
      "title": "spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "62",
      "title": "Classifier learning with prior probabilities for facial action unit recognition",
      "authors": [
        "Y Zhang",
        "W Dong",
        "B.-G Hu",
        "Q Ji"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "Weakly-supervised deep convolutional neural network learning for facial action unit intensity estimation",
      "authors": [
        "Y Zhang",
        "W Dong",
        "B.-G Hu",
        "Q Ji"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "64",
      "title": "Context-aware feature and label fusion for facial action unit intensity estimation with partially labeled data",
      "authors": [
        "Y Zhang",
        "H Jiang",
        "B Wu",
        "Y Fan",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "International Conference on Computer Vision"
    },
    {
      "citation_id": "65",
      "title": "Bilateral ordinal relevance multi-instance regression for facial action unit intensity estimation",
      "authors": [
        "Y Zhang",
        "R Zhao",
        "W Dong",
        "B.-G Hu",
        "Q Ji"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "Multi modal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang",
        "J Cohn",
        "Q Ji",
        "L Yin"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "67",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W Chu",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "68",
      "title": "Pose-independent facial action unit intensity regression based on multi-task deep transfer learning",
      "authors": [
        "Y Zhou",
        "J Pi",
        "B Shi"
      ],
      "year": "2017",
      "venue": "Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "69",
      "title": "Multiple-facial action unit recognition by shared feature learning and semantic relation modeling",
      "authors": [
        "Y Zhu",
        "S Wang",
        "L Yue",
        "Q Ji"
      ],
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    }
  ]
}