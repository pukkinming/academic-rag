{
  "paper_id": "2001.06103v1",
  "title": "An Adversarial Learning Framework For Preserving Users' Anonymity In Face-Based Emotion Recognition",
  "published": "2020-01-16T22:45:52Z",
  "authors": [
    "Vansh Narula",
    "Zhangyang",
    "Wang",
    "Theodora Chaspari"
  ],
  "keywords": [
    "Emotion",
    "privacy preservation",
    "anonymity",
    "user identity",
    "adversarial learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Image and video-capturing technologies have permeated our everyday life. Such technologies can continuously monitor individuals' expressions in real-life settings, affording us new insights into their emotional states and transitions, thus paving the way to novel wellbeing and healthcare applications. Yet, due to the strong privacy concerns, the use of such technologies is met with strong skepticism, since current face-based emotion recognition systems relying on deep learning techniques tend to preserve substantial information related to the identity of the user, apart from the emotion-specific information. This paper proposes an adversarial learning framework which relies on a convolutional neural network (CNN) architecture trained through an iterative procedure for minimizing identityspecific information and maximizing emotion-dependent information. The proposed approach is evaluated through emotion classification and face identification metrics, and is compared against two CNNs, one trained solely for emotion recognition and the other trained solely for face identification. Experiments are performed using the Yale Face Dataset and Japanese Female Facial Expression Database. Results indicate that the proposed approach can learn a convolutional transformation for preserving emotion recognition accuracy and degrading face identity recognition, providing a foundation toward privacy-aware emotion recognition technologies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Image and video-capturing devices have become increasingly ubiquitous and pervasive. From the millions of surveillance cameras installed all over the world to the newly introduced smart home devices, such ambulatory recording technologies allow the continuous monitoring of individuals over long periods of time. Beyond wellestablished applications related to security monitoring and community safety, the continuous capturing of human expression in reallife environments can promote healthcare and well-being applications  [1, 2] . For example, the monitoring of facial expressions and body gestures in a continuous manner can capture momentary and longitudinal patterns of human emotion, which can be reflective of users' stress, depression, or even suicidal risk, therefore rendering such information a valuable biomarker for predicting and potentially intervening upon individuals mental and emotional health  [3] .\n\nDespite the premises, the barrier of confidentiality and anonymity inherent in these smart-monitoring applications is an issue with various social and cultural implications preventing their Thanks to the Texas A&M University Program to Enhance Scholarly and Creative Activities (PESCA) for supporting this research. wide adoption. Users are often skeptical of such technologies, since they are afraid that facial information relevant to their identity will be permanently stored in third-party servers or will be abused by hacker attacks  [4] . This does not come as a surprise, since state-of-the art computer vision systems for emotion recognition relying on rich facial features, such as the histograms of oriented gradients (HoG)  [5] , and representation learning models, such as convolutional neural networks (CNN)  [6] , tend to preserve a significant amount of facial information related to the identity of the user. This privacy compromising landscape renders essential the design of novel machine learning systems that conceal one's identity, while at the same time preserve useful information for emotion recognition.\n\nExtensive work on human activity recognition has leveraged CNN-based architectures to learn image transformations for a given classification or regression task  [7] . In fact, previous work suggests that the convolutional transformations of the CNN are able to capture general and highly reusable information  [8] , thus the CNN layers can be pre-trained on one task and subsequently fine-tuned to another proximal one  [9] . Although this is a highly desirable property for many applications, the ability of CNNs to retain reusable information can pose an innate threat in cases with high sensitive data, since the learning of the convolutional transformation in terms of emotionspecific information might also preserve information relevant to an individual's identity. To overcome this challenge, prior work has used pre-defined image degradation transformations in order to reduce the total amount of information preserved in an image, as well as an optimization framework to learn an image transformation from a set of data for the corresponding tasks of interest  [10, 11] . Despite the promising results, this work has solely focused on the task of human activity identification, which involves the presence of multiple individuals in a frame recorded from a long distance. In contrast, the problem of privacy-preserving emotion recognition presents an additional set of unique challenges, as it depends on learning subtle facial emotional expressions and usually relies on data from cameras placed at a close proximity to one's face.\n\nIn order to balance the trade-off between user anonymity and emotion-specific face characteristics, we propose an adversarial learning algorithm that learns an image transformation to degrade sensitive information relevant to the user identity and preserve emotion-dependent information. A hybrid neural network architecture is comprised of convolutional layers fW c , followed by two parts of fully-connected layers, one for emotion classification fW e and another for face identity recognition fW i . The convolutional layers fW c are learned so that they can degrade face identity-dependent information for any possible user-dependent transformation fW i and at the same time preserve emotion-dependent information in fW e . Our results in two datasets indicate that the proposed approach can arXiv:2001.06103v1 [cs.LG] 16 Jan 2020 achieve emotion recognition performance equivalent to the one of a CNN fully trained on emotion recognition and at the same time, significant degradation in face identification, indicating the feasibility of the proposed framework for promoting user privacy in visual applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Relation To Prior Work",
      "text": "The automatic recognition of facial expressions has always been an interesting problem in computer vision. Prior work has tackled this problem by engineering appropriate features, such as Histogram of Oriented Gradients (HOG) and eigen-faces  [12] . Other approaches have leveraged the frequency characteristics of an image through Gabor filters and Wavelets  [13] . Finally, more recent techniques focus on the automatic learning of features through CNN-based architectures, such as the Resnet, MobileNet and Inception Network  [7] . These approaches tend to capture facial features that are considered important for both emotion recognition and face identification.\n\nIn the light of these, prior work has approached the problem of privacy preservation in general human activity recognition from two different approaches, one relying on pre-defined transformations of an image, while the other leveraging supervised optimization approaches. Image transformation approaches have attempted to increase the amount of uncertainty throughout the image by adding noise  [14]  or performing filtering operations  [15] . They have further tried to decrease the resolution of the facial area of a person  [11] , as well as to encode the change in successive images as the input to the system, compared to the image pixels themselves  [16] . Supervised learning approaches have formulated this as an optimization problem, leveraging an adversarial learning framework for learning appropriate degradations of images to increase a target utility metric and minimize a privacy-based metric  [17, 10, 18, 19] . . The proposed work advances existing literature in the following ways:  (1)  In contrast to previous work on privacy preservation for general human activity recognition, this paper proposes a privacypreservation system specifically for the task of emotion recognition. This task is highly dependent on subtle facial characteristics, for which it is much more difficult to learn appropriate degradation transformations; (2) While previous work has focused on data obtained with surveillance cameras or distant cameras capturing the entire body from one or multiple users  [10, 18, 19] , this paper relies on cameras placed in close proximity to a user's face, therefore preserving a high amount of identity-specific information; (3) Most of the work does not to provide a clear method of evaluating the trade-off between the degradation of utility-based information and preservation of user identity  [15, 14] . The proposed adversarial learning framework results in a convolutional transformation that attempts to degrade user-specific information for any of the subsequent fully-connected layers. The output of the convolution is fed into two classifiers, one for emotion recognition and the other for face identification. These accuracies can quantify the amount of identity-and emotion-specific information preserved in the CNN.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology 3.1. Quantifying The Identity-Specific Information Of An Emotion-Specific Convolutional Base",
      "text": "We first train two separate CNNs, one for emotion classification, referred to as \"Emotion\" (Fig.  1a ), and another for face identity recognition, referred to as \"Face\" (Fig.  1b ). Each of the CNNs includes a set of convolutional layers, followed by fully-connected ones. The Require: Image x, emotion label ye, user label yi, hyperparameters α, β, T 1: Initialize Wc (convolutional weights), We (emotion classification weights), Wi (face identificaiton weights) with multi-task learning min {Wc,We,W i } {Le (fW e (fW c (x)), ye) + αLi (fW i (fW c (x)), yi)} 2: for t = 1, . . . , T do 3:\n\nFreeze Wi",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "4:",
      "text": "Learn Wc and We using adversarial learning min {Wc,We} {Le (fW e (fW c (x)), ye) -βLi (fW i (fW c (x)), yi)} 5:\n\nFreeze Wc and We",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "6:",
      "text": "Learn Wi minW i {Li (fW i (fW c (x)), yi)} 7: end for Algorithm 1: Adversarial learning for anonymity preserving emotion recognition Face model serves as a baseline to quantify the amount of information specific to the user identity that can be captured through the convolutional layers fully trained for face identification, as measured by the corresponding face identification accuracy. Similarly, the emotion classification accuracy of the Emotion model can approximate the degree of emotion-specific information present in the convolutional layers of a CNN fully trained on emotion. These accuracies will serve as baselines, against which the proposed adversarial framework (Section 3.2) will be compared for its ability to preserve the emotional information and eliminate user-specific information.\n\nIn order to quantify the amount of identity-specific information built-in in the emotion recognition model Emotion, we keep its convolutional layers frozen and fine-tune its fully-connected layers for the task of face identification. This model, which will be referred to as \"Emotion2Face\" (Fig.  1c ), can capture the amount of identityspecific information present in the convolutional base of the CNN trained for emotion recognition. The face identification accuracy of the Emotion2Face model will be employed as an approximate measure for that. High values of this measure suggest that the convolutional base of the emotion-specific CNN retains a large degree of face identity information, while low values indicate that the convolutional base does not preserve much user-specific information.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "User Anonymity Preserving Emotion Classification",
      "text": "We propose an adversarial framework which learns a transformation to maximize the emotion-specific information and minimize the identity-specific information related to the privacy-preservation task (Algorithm 1). We will use a hybrid architecture including a shared convolutional base, connected to two different sets of fully connected layers, one for emotion classification and one for face recognition, which will be referred to as \"Hybrid\" (Fig.  1d ). The proposed adversarial approach contains two key components: (1) We learn appropriate transformations both at the convolutional and fully-connected layers. Since the convolutional layers preserve a large portion of the information in an image  [8] , they also tend to be more prone in capturing the user identity. If we apply the adversarial learning solely on the fully-connected layers, then the corresponding weights will quickly reach to zero, while the identity-dependent information will be preserved in the convolutional part; (2) We further aim to learn a convolutional base that is able to eliminate identityspecific information when followed by any fully-connected layer. The intuition behind this lies in that the fully-connected layers of a reliable face recognition model will be able to extract information specific to the user identity from a given convolutional base. For this reason, we employ an iterative procedure to learn a convolutional transformation, based on which there is no fully connected layer able to extract face identity-specific information.\n\nIn the following, let x be an input image with face identity label yi and emotion label ye. Also let fW c (x) be the transformation of the shared convolutional layers, represented by the weights Wc. The output of the convolutional layers is fed to two fully connected layers, fW e (fW c (x)) and fW i (fW c (x)), performing the emotionspecific and face identity-specific transformations using weights We and Wi. We further assume that Le and Li are the cross-entropy loss of emotion classification and face identity recognition. We initialize the weights Wc, We, and Wi by jointly training the hybrid architecture as a multi-task learning task:\n\nmin {Wc,We,W i } {Le (fW e (fW c (x)), ye) + αLi (fW i (fW c (x)), yi)} (1\n\nwhere α is the hyper-parameter that balances between positive emotion loss and positive face identity loss. We then freeze the weights Wi of fully-connected layers for identity recognition and perform an adversarial learning process to optimize the following criterion: min {Wc,We} {Le (fW e (fW c (x)), ye) -βLi (fW i (fW c (x)), yi)} (2)\n\nwhere β balances between positive emotion loss and negative face identity loss. This allows to initiate the adversarial training with a reliable set of weights Wi for face identification. At the same time, this prevents Wi from becoming zero, which would result in an \"artificially\" successful adversarial learning with the face identity information being likely to remain in the convolutional base.\n\nThe above adversarial learning ensures that the model is trained in way that it conceals face identity-specific information from the current transformation fW i , but we would like the model to generalize and hence it should able to \"fool\" any transformation, i.e. any possible value of Wi should not be able to extract any face specific information from the convolutional base. In order to achieve that, we would ideally have to perform the adversarial training for all possible values of Wi, which is computationally not feasible. For this reason, we freeze the convolutional base Wc and emotion-specific fully-connected layers We, and learn the face identity weights Wi, such that minW i {Li (fW i (fW c (x)), yi)}. After this we have another reliable estimate of Wi for face identity recognition, and thus can restart the adversarial training to further learn the weights Wc and We. This process is repeated T times until the transformation fW i is no longer able to achieve good face recognition accuracy, suggesting that the face identity-specific information is either lost or hard to be recovered.\n\nGiven the final learned convolutional transformation fW * c (x), we evaluate its ability to preserve emotion-specific information and eliminate information related to the identity of a speaker. For this reason, we add two different sets of feedforward layers f W e and f W i , whose weights W e and W i are learned for emotion recognition (i.e., min W e Le(f W e (fW * c (x)), ye)) and face identification (i.e., min W i Li(f W i (fW * c (x)), ye)), respectively. These models will be referred to as \"Hybrid2Emotion\" and \"Hybrid2Face\" and their final accuracies will be reported.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Description And Pre-Processing",
      "text": "We tested our approach on the Japanese Female Facial Expression (JAFFE) database  [20]  and the Yale Face Dataset (YALE)  [21] . JAFFE included 10 female users and 7 emotions (neutral, sadness, surprise, happiness, fear, anger, and disgust), with a total of 213 static images. For YALE, we used only the images which include labels for both the user and emotion categories. These include 15 male and female users and 4 emotion classes (happy, sad, normal, surprised), resulting in a total of 60 images. Since the number of images in both datasets was small for a deep learning model to be adequately trained, we used data augmentation techniques related to random rotation, horizontal flip, and random noise addition to generate a total of 3038 and 3033 images for JAFFE and YALE, respectively  [22] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setting",
      "text": "For all experiments we used a 10-fold cross-validation, with the same set of train and test images per fold across all systems and making sure that no samples generated from the same original image after data augmentation are present in the test set concurrently. The problem formulation does not qualify for a leave-one-subjectout cross-validation, since we need the convolutional transformation to learn from every subject in order to be able to calculate the corresponding face-recognition accuracy. Similarly we need the model to be trained for every emotion. All the considered models (i.e., Emotion, Face, Emotion2Face, and Hybrid) included 3 convolutional followed by 3 fully-connected layers. For the case of the Hybrid model, two streams of fully connected layers were included, one for emotion and the other for user classification. The activation function of all the hidden layers was ReLU, while the output layer had a softmax activation. The filter of the convolution operation was equal to 3 with a stride length of 1. The number of nodes for each layer is depicted in Fig.  1 . The hyper-parameters balancing the Hybrid model's ability to learn between the emotion and the user categories in (1) and (2) were set to to α = 0.5 and β = 1, respectively. The number of iterations for the adversarial learning optimization was 50 and 20 for the JAFFE and YALE, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Our results for the JAFFE and YALE datasets are presented in Tables 2 and 1. All results reflect simple classification accuracies, since the distribution of samples for the user and emotion categories was balanced for both datasets. Face identity recognition appears to be an easy task for both datasets, yielding classification accuracies of 99% and 87%. Emotion classification depicts higher accuracy for JAFFE compared to YALE, potentially due to the high variability",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Discussion",
      "text": "Although the proposed framework provides encouraging results, the current study presents the following limitations. The YALE and JAFFE datasets are collected in laboratory conditions and include acted emotions. It would be beneficial to test the proposed approach in real-life applications with spontaneous emotional expressions and a larger number of samples. Surprisingly, we were not able to find many image datasets which contain labels of user identity and emotion for the same image, comprising an impediment for this type of work. Preserving information related to the user identity when designing emotional datasets would provide beneficial benchmarks for this task. Another limitation of this study lies in the fact that static images were taken into account. However, emotion is a dynamically changing state, therefore future work will concentrate on extending these techniques to video signals. Finally, the inherently unstable nature of adversarial learning can present various challenges related to finding the optimal number of iterations during the optimization for achieve a close-to-optimal solution. We used techniques like early stopping and random initialization of layers to tackle this, but more experimentation in terms of such hyper-parameters is needed in order to obtain more robust results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "To the best of our knowledge, this work is the first attempt to design an adversarial learning framework for preserving users' anonymity in face-based emotion recognition. We first provide a method to evaluate the amount of identity-specific information available in convolutional bases learned for emotion recognition. We then designed an adversarial learning framework through an approximate procedure which reduces the amount of user-dependent information in the convolutional layer, which is iteratively learned for various userspecific fully connected layers in an attempting to yield a robust emotion-specific and anonymity-preserving transformation. As part of our future work, we will expand this study to larger datasets and explore additional methods, such as adversarial reinforcement learning and self-attention mechanisms.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: a), and another for face identity recog-",
      "page": 2
    },
    {
      "caption": "Figure 1: b). Each of the CNNs includes",
      "page": 2
    },
    {
      "caption": "Figure 1: c), can capture the amount of identity-",
      "page": 2
    },
    {
      "caption": "Figure 1: Schematic representation of the: (a) Emotion model, trained on emotion recognition; (b) Face model, trained on face identiﬁcation;",
      "page": 3
    },
    {
      "caption": "Figure 1: The hyper-parameters balancing the Hybrid model’s abil-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Computer Science & Engineering, Texas A&M University": "wide adoption. Users are often skeptical of such technologies, since"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "they are afraid that facial information relevant to their identity will be"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "permanently stored in third-party servers or will be abused by hacker"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "attacks [4]. This does not come as a surprise, since state-of-the art"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "computer vision systems for emotion recognition relying on rich fa-"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "cial features, such as the histograms of oriented gradients (HoG) [5],"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "and representation learning models,\nsuch as convolutional neural"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "networks (CNN) [6],\ntend to preserve a signiﬁcant amount of facial"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "information related to the identity of\nthe user.\nThis privacy com-"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "promising landscape renders essential\nthe design of novel machine"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "learning systems that conceal one’s identity, while at\nthe same time"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "preserve useful information for emotion recognition."
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "Extensive work on human activity recognition has\nleveraged"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "CNN-based architectures to learn image transformations for a given"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "classiﬁcation or regression task [7].\nIn fact, previous work suggests"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "that the convolutional transformations of the CNN are able to capture"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "general and highly reusable information [8], thus the CNN layers can"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "be pre-trained on one task and subsequently ﬁne-tuned to another"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "proximal one [9]. Although this is a highly desirable property for"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "many applications,\nthe ability of CNNs to retain reusable informa-"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "tion can pose an innate threat in cases with high sensitive data, since"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "the learning of the convolutional transformation in terms of emotion-"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "speciﬁc information might also preserve information relevant\nto an"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "individual’s identity.\nTo overcome this challenge, prior work has"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "used pre-deﬁned image degradation transformations in order to re-"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "duce the total amount of information preserved in an image, as well"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "as an optimization framework to learn an image transformation from"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "a set of data for the corresponding tasks of interest [10, 11]. Despite"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "the promising results, this work has solely focused on the task of hu-"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "man activity identiﬁcation, which involves the presence of multiple"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "individuals in a frame recorded from a long distance.\nIn contrast,"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "the problem of privacy-preserving emotion recognition presents an"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "additional set of unique challenges, as it depends on learning subtle"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "facial emotional expressions and usually relies on data from cameras"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "placed at a close proximity to one’s face."
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "In order\nto balance the trade-off between user anonymity and"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "emotion-speciﬁc\nface\ncharacteristics, we\npropose\nan\nadversarial"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "learning algorithm that\nlearns an image transformation to degrade"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "sensitive\ninformation\nrelevant\nto\nthe\nuser\nidentity\nand\npreserve"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "emotion-dependent\ninformation. A hybrid neural network architec-"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "ture is comprised of convolutional layers fWc , followed by two parts"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "of\nand\nfully-connected layers, one for emotion classiﬁcation fWe"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "layers\nanother for face identity recognition fWi . The convolutional"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "fWc are learned so that they can degrade face identity-dependent in-"
        },
        {
          "Computer Science & Engineering, Texas A&M University": ""
        },
        {
          "Computer Science & Engineering, Texas A&M University": "formation for any possible user-dependent\ntransformation fWi and"
        },
        {
          "Computer Science & Engineering, Texas A&M University": "at\nthe same time preserve emotion-dependent\ninformation in fWe ."
        },
        {
          "Computer Science & Engineering, Texas A&M University": "Our results in two datasets indicate that\nthe proposed approach can"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "achieve emotion recognition performance equivalent\nto the one of": "a CNN fully trained on emotion recognition and at\nthe same time,",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": "hyperparameters α, β, T"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "signiﬁcant degradation in face identiﬁcation, indicating the feasibil-",
          "Require:": "1:",
          "Image x, emotion label ye, user label yi,": "Initialize Wc (convolutional weights), We (emotion"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "ity of the proposed framework for promoting user privacy in visual",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": "classiﬁcation weights), Wi (face identiﬁcaiton weights) with"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "applications.",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": "multi-task learning"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": "min{Wc,We,Wi}{Le (fWe (fWc (x)), ye) + αLi (fWi (fWc (x)), yi)}"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "2. RELATION TO PRIOR WORK",
          "Require:": "2:",
          "Image x, emotion label ye, user label yi,": "for t = 1, . . . , T do"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:": "3:",
          "Image x, emotion label ye, user label yi,": "Freeze Wi"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "The automatic recognition of facial expressions has always been an",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:": "4:",
          "Image x, emotion label ye, user label yi,": "Learn Wc and We using adversarial learning"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "interesting problem in computer vision. Prior work has tackled this",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "problem by engineering appropriate features, such as Histogram of",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": "min{Wc,We}{Le (fWe (fWc (x)), ye) − βLi (fWi (fWc (x)), yi)}"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:": "5:",
          "Image x, emotion label ye, user label yi,": "Freeze Wc and We"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "Oriented Gradients (HOG) and eigen-faces [12]. Other approaches",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:": "6:",
          "Image x, emotion label ye, user label yi,": "Learn Wi minWi {Li (fWi (fWc (x)), yi)}"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "have leveraged the frequency characteristics of an image through Ga-",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:": "7:",
          "Image x, emotion label ye, user label yi,": "end for"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "bor ﬁlters and Wavelets [13]. Finally, more recent\ntechniques focus",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": "Algorithm 1: Adversarial learning for anonymity preserving emo-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "on the automatic learning of features through CNN-based architec-",
          "Require:": "",
          "Image x, emotion label ye, user label yi,": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "achieve emotion recognition performance equivalent\nto the one of": "a CNN fully trained on emotion recognition and at\nthe same time,",
          "Require:\nImage x, emotion label ye, user label yi,": "hyperparameters α, β, T"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "signiﬁcant degradation in face identiﬁcation, indicating the feasibil-",
          "Require:\nImage x, emotion label ye, user label yi,": "1:\nInitialize Wc (convolutional weights), We (emotion"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "ity of the proposed framework for promoting user privacy in visual",
          "Require:\nImage x, emotion label ye, user label yi,": "classiﬁcation weights), Wi (face identiﬁcaiton weights) with"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "applications.",
          "Require:\nImage x, emotion label ye, user label yi,": "multi-task learning"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "min{Wc,We,Wi}{Le (fWe (fWc (x)), ye) + αLi (fWi (fWc (x)), yi)}"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "2. RELATION TO PRIOR WORK",
          "Require:\nImage x, emotion label ye, user label yi,": "2:\nfor t = 1, . . . , T do"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "3:\nFreeze Wi"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "The automatic recognition of facial expressions has always been an",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "4:\nLearn Wc and We using adversarial learning"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "interesting problem in computer vision. Prior work has tackled this",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "problem by engineering appropriate features, such as Histogram of",
          "Require:\nImage x, emotion label ye, user label yi,": "min{Wc,We}{Le (fWe (fWc (x)), ye) − βLi (fWi (fWc (x)), yi)}"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "5:\nFreeze Wc and We"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "Oriented Gradients (HOG) and eigen-faces [12]. Other approaches",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "6:\nLearn Wi minWi {Li (fWi (fWc (x)), yi)}"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "have leveraged the frequency characteristics of an image through Ga-",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "7:\nend for"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "bor ﬁlters and Wavelets [13]. Finally, more recent\ntechniques focus",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "Algorithm 1: Adversarial learning for anonymity preserving emo-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "on the automatic learning of features through CNN-based architec-",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "tion recognition"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "tures,\nsuch as\nthe Resnet, MobileNet and Inception Network [7].",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "These approaches tend to capture facial features that are considered",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "important for both emotion recognition and face identiﬁcation.",
          "Require:\nImage x, emotion label ye, user label yi,": ""
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "In the light of\nthese, prior work has approached the problem",
          "Require:\nImage x, emotion label ye, user label yi,": "Face model serves as a baseline to quantify the amount of informa-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "of privacy preservation in general human activity recognition from",
          "Require:\nImage x, emotion label ye, user label yi,": "tion speciﬁc to the user\nidentity that can be captured through the"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "two different approaches, one relying on pre-deﬁned transformations",
          "Require:\nImage x, emotion label ye, user label yi,": "convolutional\nlayers\nfully trained for\nface identiﬁcation,\nas mea-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "of an image, while the other leveraging supervised optimization ap-",
          "Require:\nImage x, emotion label ye, user label yi,": "sured by the corresponding face identiﬁcation accuracy. Similarly,"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "proaches.\nImage transformation approaches have attempted to in-",
          "Require:\nImage x, emotion label ye, user label yi,": "the emotion classiﬁcation accuracy of\nthe Emotion model can ap-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "crease the amount of uncertainty throughout\nthe image by adding",
          "Require:\nImage x, emotion label ye, user label yi,": "proximate the degree of emotion-speciﬁc information present in the"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "noise [14] or performing ﬁltering operations [15]. They have further",
          "Require:\nImage x, emotion label ye, user label yi,": "convolutional layers of a CNN fully trained on emotion. These accu-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "tried to decrease the resolution of\nthe facial area of a person [11],",
          "Require:\nImage x, emotion label ye, user label yi,": "racies will serve as baselines, against which the proposed adversarial"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "as well as to encode the change in successive images as the input to",
          "Require:\nImage x, emotion label ye, user label yi,": "framework (Section 3.2) will be compared for its ability to preserve"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "the system, compared to the image pixels themselves [16]. Super-",
          "Require:\nImage x, emotion label ye, user label yi,": "the emotional information and eliminate user-speciﬁc information."
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "vised learning approaches have formulated this as an optimization",
          "Require:\nImage x, emotion label ye, user label yi,": "In order to quantify the amount of identity-speciﬁc information"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "problem,\nleveraging an adversarial\nlearning framework for learning",
          "Require:\nImage x, emotion label ye, user label yi,": "built-in in the emotion recognition model Emotion, we keep its con-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "appropriate degradations of images to increase a target utility metric",
          "Require:\nImage x, emotion label ye, user label yi,": "volutional\nlayers frozen and ﬁne-tune its fully-connected layers for"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "and minimize a privacy-based metric [17, 10, 18, 19].\n.",
          "Require:\nImage x, emotion label ye, user label yi,": "the task of\nface identiﬁcation. This model, which will be referred"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "The proposed work advances existing literature in the following",
          "Require:\nImage x, emotion label ye, user label yi,": "to as “Emotion2Face” (Fig. 1c), can capture the amount of identity-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "ways:\n(1) In contrast\nto previous work on privacy preservation for",
          "Require:\nImage x, emotion label ye, user label yi,": "speciﬁc information present\nin the convolutional base of\nthe CNN"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "general human activity recognition,\nthis paper proposes a privacy-",
          "Require:\nImage x, emotion label ye, user label yi,": "trained for emotion recognition. The face identiﬁcation accuracy of"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "preservation system speciﬁcally for\nthe task of emotion recogni-",
          "Require:\nImage x, emotion label ye, user label yi,": "the Emotion2Face model will be employed as an approximate mea-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "tion. This task is highly dependent on subtle facial characteristics,",
          "Require:\nImage x, emotion label ye, user label yi,": "sure for\nthat. High values of\nthis measure suggest\nthat\nthe convo-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "for which it\nis much more difﬁcult\nto learn appropriate degradation",
          "Require:\nImage x, emotion label ye, user label yi,": "lutional base of the emotion-speciﬁc CNN retains a large degree of"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "transformations;\n(2) While previous work has focused on data ob-",
          "Require:\nImage x, emotion label ye, user label yi,": "face identity information, while low values indicate that\nthe convo-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "tained with surveillance cameras or distant cameras capturing the",
          "Require:\nImage x, emotion label ye, user label yi,": "lutional base does not preserve much user-speciﬁc information."
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "entire body from one or multiple users [10, 18, 19],\nthis paper\nre-",
          "Require:\nImage x, emotion label ye, user label yi,": "3.2. User anonymity preserving emotion classiﬁcation"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "lies on cameras placed in close proximity to a user’s\nface,\nthere-",
          "Require:\nImage x, emotion label ye, user label yi,": "We propose an adversarial\nframework which learns a transforma-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "fore preserving a high amount of identity-speciﬁc information;\n(3)",
          "Require:\nImage x, emotion label ye, user label yi,": "tion to maximize the emotion-speciﬁc information and minimize"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "Most of the work does not\nto provide a clear method of evaluating",
          "Require:\nImage x, emotion label ye, user label yi,": "the identity-speciﬁc information related to the privacy-preservation"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "the trade-off between the degradation of utility-based information",
          "Require:\nImage x, emotion label ye, user label yi,": "task (Algorithm 1). We will use a hybrid architecture including a"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "and preservation of user identity [15, 14]. The proposed adversarial",
          "Require:\nImage x, emotion label ye, user label yi,": "shared convolutional base, connected to two different sets of\nfully"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "learning framework results in a convolutional transformation that at-",
          "Require:\nImage x, emotion label ye, user label yi,": "connected layers, one for emotion classiﬁcation and one for\nface"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "tempts to degrade user-speciﬁc information for any of the subsequent",
          "Require:\nImage x, emotion label ye, user label yi,": "recognition, which will be referred to as “Hybrid” (Fig. 1d).\nThe"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "fully-connected layers. The output of the convolution is fed into two",
          "Require:\nImage x, emotion label ye, user label yi,": "proposed adversarial approach contains\ntwo key components:\n(1)"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "classiﬁers, one for emotion recognition and the other for face identi-",
          "Require:\nImage x, emotion label ye, user label yi,": "We learn appropriate transformations both at\nthe convolutional and"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "ﬁcation. These accuracies can quantify the amount of identity- and",
          "Require:\nImage x, emotion label ye, user label yi,": "fully-connected layers.\nSince the convolutional\nlayers preserve a"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "emotion-speciﬁc information preserved in the CNN.",
          "Require:\nImage x, emotion label ye, user label yi,": "large portion of the information in an image [8], they also tend to be"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "",
          "Require:\nImage x, emotion label ye, user label yi,": "more prone in capturing the user identity. If we apply the adversarial"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "3. METHODOLOGY",
          "Require:\nImage x, emotion label ye, user label yi,": "learning solely on the fully-connected layers, then the corresponding"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "3.1. Quantifying\nthe\nidentity-speciﬁc\ninformation\nof\nan",
          "Require:\nImage x, emotion label ye, user label yi,": "weights will quickly reach to zero, while the identity-dependent\nin-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "emotion-speciﬁc convolutional base",
          "Require:\nImage x, emotion label ye, user label yi,": "formation will be preserved in the convolutional part; (2) We further"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "We ﬁrst train two separate CNNs, one for emotion classiﬁcation, re-",
          "Require:\nImage x, emotion label ye, user label yi,": "aim to learn a convolutional base that\nis able to eliminate identity-"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "ferred to as “Emotion” (Fig. 1a), and another for face identity recog-",
          "Require:\nImage x, emotion label ye, user label yi,": "speciﬁc information when followed by any fully-connected layer."
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "nition, referred to as “Face” (Fig. 1b). Each of the CNNs includes",
          "Require:\nImage x, emotion label ye, user label yi,": "The intuition behind this lies in that\nthe fully-connected layers of"
        },
        {
          "achieve emotion recognition performance equivalent\nto the one of": "a set of convolutional layers, followed by fully-connected ones. The",
          "Require:\nImage x, emotion label ye, user label yi,": "a reliable face recognition model will be able to extract information"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(c) Emotion2Face model": "Fig. 1. Schematic representation of the:\n(a) Emotion model,",
          "(d) Hybrid model": "trained on emotion recognition; (b) Face model,\ntrained on face identiﬁcation;"
        },
        {
          "(c) Emotion2Face model": "(c) Emotion2Face model,\ntrained on emotion and ﬁne-tuned on face identiﬁcation; and (d) Hybrid model,",
          "(d) Hybrid model": "trained on an iterative adversarial"
        },
        {
          "(c) Emotion2Face model": "framework for user identity-preserving emotion recognition.",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "speciﬁc to the user identity from a given convolutional base. For this",
          "(d) Hybrid model": "where β balances between positive emotion loss and negative face"
        },
        {
          "(c) Emotion2Face model": "reason, we employ an iterative procedure to learn a convolutional",
          "(d) Hybrid model": "identity loss.\nThis allows\nto initiate the adversarial\ntraining with"
        },
        {
          "(c) Emotion2Face model": "transformation, based on which there is no fully connected layer able",
          "(d) Hybrid model": "for\nface identiﬁcation.\nAt\nthe same\na reliable set of weights Wi"
        },
        {
          "(c) Emotion2Face model": "to extract face identity-speciﬁc information.",
          "(d) Hybrid model": "time,\nfrom becoming zero, which would result\nin\nthis prevents Wi"
        },
        {
          "(c) Emotion2Face model": "In the following,\nlet x be an input\nimage with face identity la-",
          "(d) Hybrid model": "an “artiﬁcially” successful adversarial learning with the face identity"
        },
        {
          "(c) Emotion2Face model": "bel yi and emotion label ye. Also let fWc (x) be the transformation",
          "(d) Hybrid model": "information being likely to remain in the convolutional base."
        },
        {
          "(c) Emotion2Face model": "of the shared convolutional\nlayers, represented by the weights Wc.",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "The above adversarial learning ensures that the model is trained"
        },
        {
          "(c) Emotion2Face model": "The output of the convolutional layers is fed to two fully connected",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "in way that\nit conceals face identity-speciﬁc information from the"
        },
        {
          "(c) Emotion2Face model": "layers, fWe (fWc (x)) and fWi (fWc (x)), performing the emotion-",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "current\nto gener-\ntransformation fWi , but we would like the model"
        },
        {
          "(c) Emotion2Face model": "speciﬁc and face identity-speciﬁc transformations using weights We",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "alize and hence it should able to “fool” any transformation,\ni.e. any"
        },
        {
          "(c) Emotion2Face model": "and Wi. We further assume that Le and Li are the cross-entropy",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "possible value of Wi should not be able to extract any face speciﬁc"
        },
        {
          "(c) Emotion2Face model": "loss of emotion classiﬁcation and face identity recognition. We ini-",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "information from the convolutional base. In order to achieve that, we"
        },
        {
          "(c) Emotion2Face model": "tialize the weights Wc, We, and Wi by jointly training the hybrid",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "would ideally have to perform the adversarial\ntraining for all possi-"
        },
        {
          "(c) Emotion2Face model": "architecture as a multi-task learning task:",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "feasible.\nFor\nthis\nble values of Wi, which is computationally not"
        },
        {
          "(c) Emotion2Face model": "(1)\nmin{Wc,We,Wi}{Le (fWe (fWc (x)), ye) + αLi (fWi (fWc (x)), yi)}",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "reason, we freeze the convolutional base Wc and emotion-speciﬁc"
        },
        {
          "(c) Emotion2Face model": "where α is the hyper-parameter that balances between positive emo-",
          "(d) Hybrid model": "fully-connected layers We, and learn the face identity weights Wi,"
        },
        {
          "(c) Emotion2Face model": "tion loss and positive face identity loss. We then freeze the weights",
          "(d) Hybrid model": ""
        },
        {
          "(c) Emotion2Face model": "",
          "(d) Hybrid model": "this we have an-\nsuch that minWi {Li (fWi (fWc (x)), yi)}. After"
        },
        {
          "(c) Emotion2Face model": "Wi of fully-connected layers for identity recognition and perform an",
          "(d) Hybrid model": "for face identity recognition, and thus\nother reliable estimate of Wi"
        },
        {
          "(c) Emotion2Face model": "adversarial learning process to optimize the following criterion:",
          "(d) Hybrid model": "can restart\nthe adversarial\ntraining to further\nlearn the weights Wc"
        },
        {
          "(c) Emotion2Face model": "(2)\nmin{Wc,We}{Le (fWe (fWc (x)), ye) − βLi (fWi (fWc (x)), yi)}",
          "(d) Hybrid model": "the transformation\nand We. This process is repeated T times until"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "suggesting that the face identity-speciﬁc information is either lost or"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "hard to be recovered."
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "Given the ﬁnal\nlearned convolutional"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "transformation fW ∗\nc (x),"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "we evaluate its ability to preserve emotion-speciﬁc information and"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "eliminate information related to the identity of a speaker.\nFor\nthis"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "reason, we add two different\nsets of\nand\nfeedforward layers fW (cid:48)"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "e"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ", whose weights W (cid:48)\nand W (cid:48)\nare learned for emotion recog-\nfW (cid:48)"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "i"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "(fW ∗\nnition (i.e., minW (cid:48)\nLe(fW (cid:48)\nc (x)), ye)) and face identiﬁcation"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "e\ne"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "respectively.\nThese models\n(fW ∗\n(i.e., minW (cid:48)"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "c (x)), ye)),\ni\ni"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "will be referred to as “Hybrid2Emotion” and “Hybrid2Face” and"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "their ﬁnal accuracies will be reported."
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "4. EXPERIMENTS"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "4.1. Data description and pre-processing"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "We tested our approach on the Japanese Female Facial Expression"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "(JAFFE) database\n[20]\nand the Yale Face Dataset\n(YALE)\n[21]."
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "JAFFE included 10 female users and 7 emotions (neutral, sadness,"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "surprise, happiness,\nfear, anger, and disgust), with a total of 213"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "static images.\nFor YALE, we used only the images which include"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "labels for both the user and emotion categories. These include 15"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "male and female users and 4 emotion classes (happy, sad, normal,"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "surprised),\nresulting in a total of 60 images.\nSince the number of"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "images in both datasets was small for a deep learning model\nto be"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "adequately trained, we used data augmentation techniques related to"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "random rotation, horizontal ﬂip, and random noise addition to gen-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "erate a total of 3038 and 3033 images for JAFFE and YALE, respec-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "tively [22]."
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "4.2. Experimental setting"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "For all experiments we used a 10-fold cross-validation, with the"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "same set of\ntrain and test\nimages per\nfold across all\nsystems and"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "making sure that no samples generated from the same original\nim-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "age after data augmentation are present\nin the test set concurrently."
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "The problem formulation does not qualify for a leave-one-subject-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "out cross-validation, since we need the convolutional transformation"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "to learn from every subject in order to be able to calculate the corre-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "sponding face-recognition accuracy. Similarly we need the model to"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "be trained for every emotion. All\nthe considered models (i.e., Emo-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "tion, Face, Emotion2Face, and Hybrid) included 3 convolutional fol-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "lowed by 3 fully-connected layers. For the case of the Hybrid model,"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "two streams of fully connected layers were included, one for emo-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "tion and the other for user classiﬁcation. The activation function of"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "all the hidden layers was ReLU, while the output layer had a softmax"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "activation. The ﬁlter of the convolution operation was equal to 3 with"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "a stride length of 1. The number of nodes for each layer is depicted"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "in Fig. 1. The hyper-parameters balancing the Hybrid model’s abil-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "ity to learn between the emotion and the user categories in (1) and"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "(2) were set\nto to α = 0.5 and β = 1, respectively. The number of"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "iterations for the adversarial learning optimization was 50 and 20 for"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "the JAFFE and YALE, respectively."
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": ""
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "4.3. Results"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "Our results for the JAFFE and YALE datasets are presented in Ta-"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "bles 2 and 1. All results reﬂect simple classiﬁcation accuracies, since"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "the distribution of samples for the user and emotion categories was"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "balanced for both datasets. Face identity recognition appears to be"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "an easy task for both datasets, yielding classiﬁcation accuracies of"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "99% and 87%.\nEmotion classiﬁcation depicts higher accuracy for"
        },
        {
          "is no longer able to achieve good face recognition accuracy,\nfWi": "JAFFE compared to YALE, potentially due to the high variability"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Susskind, and Javier Movellan, “Dynamics of facial expression"
        },
        {
          "7. REFERENCES": "[1] Enrique\nLeon, Manuel Montejo,\nand\nInigo Dorronsoro,",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "extracted automatically from video,”\nin IEEE Conference on"
        },
        {
          "7. REFERENCES": "“Prospect of\nsmart home-based detection of\nsubclinical de-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Computer Vision and Pattern Recognition: Workshop on Face"
        },
        {
          "7. REFERENCES": "pressive disorders,”\nin 2011 5th International Conference on",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Processing in Video, 2004."
        },
        {
          "7. REFERENCES": "Pervasive Computing Technologies for Healthcare (Pervasive-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "[14] Yogachandran Rahulamathavan and Muttukrishnan Rajarajan,"
        },
        {
          "7. REFERENCES": "Health) and Workshops. IEEE, 2011, pp. 452–457.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "“Efﬁcient privacy-preserving facial expression classiﬁcation,”"
        },
        {
          "7. REFERENCES": "[2] Barbara Preschl, Birgit Wagner, Simon Forstmeier, and An-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "in Transactions on Dependable and Secure Computing. IEEE,"
        },
        {
          "7. REFERENCES": "dreas Maercker,\n“E-health interventions for depression, anx-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "2015."
        },
        {
          "7. REFERENCES": "iety disorder, dementia, and other disorders in old age: A re-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "[15] G. Letournel, A. Bugeau, V.-T. Ta, and J.-P. Domenger, “Face"
        },
        {
          "7. REFERENCES": "view,” Journal of CyberTherapy and Rehabilitation, vol. 4, pp.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "de-identiﬁcation with expressions preservation,”\nin Interna-"
        },
        {
          "7. REFERENCES": "371–86, 2011.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "tional Conference on Image Processing (ICIP). IEEE, 2015."
        },
        {
          "7. REFERENCES": "[3]\nShrikanth Narayanan and Panayiotis G Georgiou,\n“Behav-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Julian Steil, Marion Koelle, Wilko Heuten, Susanne Boll, and"
        },
        {
          "7. REFERENCES": "ioral signal processing: Deriving human behavioral\ninformat-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "the 11th ACM\nAndreas Bulling,\n“Privaceye,” Proceedings of"
        },
        {
          "7. REFERENCES": "ics from speech and language,” Proceedings of the IEEE, vol.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Symposium on Eye Tracking Research & Applications - ETRA"
        },
        {
          "7. REFERENCES": "101, no. 5, pp. 1203–1233, 2013.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "19, 2019."
        },
        {
          "7. REFERENCES": "[4] Nir Kshetri and Jeffrey M. Voas, “Cyberthreats under the bed,”",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Seyed Osia, Ali Shahin Shamsabadi, Sina Sajadmanesh, Ali"
        },
        {
          "7. REFERENCES": "IEEE Computer, vol. 51, no. 5, pp. 92–95, 2018.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Taheri, Kleomenis Katevas, Hamid Rabiee, Nicholas Lane,"
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "and Hamed Haddadi, “A hybrid deep learning architecture for"
        },
        {
          "7. REFERENCES": "[5] Mohamed Dahmane and Jean Meunier,\n“Emotion recognition",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "privacy-preserving mobile analytics,” 03 2017."
        },
        {
          "7. REFERENCES": "using dynamic grid-based hog features,”\nin Face and Gesture",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "2011. IEEE, 2011, pp. 884–888.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Jihun Hamm,\n“Minimax ﬁlter:\nlearning to preserve privacy"
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "from inference attacks,” The Journal of Machine Learning Re-"
        },
        {
          "7. REFERENCES": "[6] Dung Nguyen,\nKien Nguyen,\nSridha\nSridharan,\nAfsane",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "search, vol. 18, no. 1, pp. 4704–4734, 2017."
        },
        {
          "7. REFERENCES": "Ghasemi, David Dean,\nand Clinton Fookes,\n“Deep spatio-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "[19] Nisarg Raval, Ashwin Machanavajjhala, and Landon P Cox,"
        },
        {
          "7. REFERENCES": "temporal\nfeatures\nfor multimodal emotion recognition,”\nin",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "“Protecting visual\nsecrets using adversarial nets,”\nin 2017"
        },
        {
          "7. REFERENCES": "2017 IEEE Winter Conference on Applications of Computer",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
          "7. REFERENCES": "Vision (WACV). IEEE, 2017, pp. 1215–1223.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Workshops (CVPRW). IEEE, 2017, pp. 1329–1332."
        },
        {
          "7. REFERENCES": "[7]\nShreya Ghosh, Abhinav Dhall, and Nicu Sebe,\n“Automatic",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "J Lyons, Shigeru Akamatsu, Miyuki Kamachi,\nJiro"
        },
        {
          "7. REFERENCES": "group affect analysis in images via visual attribute and feature",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Gyoba, and Julien Budynek,\n“The Japanese female facial ex-"
        },
        {
          "7. REFERENCES": "networks,”\nin 25th IEEE International Conference on Image",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "third interna-\npression (JAFFE) database,”\nin Proceedings of"
        },
        {
          "7. REFERENCES": "Processing (ICIP), 2018.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "tional conference on automatic face and gesture recognition,"
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "1998, pp. 14–16."
        },
        {
          "7. REFERENCES": "[8] Leon A Gatys, Alexander S Ecker, and Matthias Bethge, “Im-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "age style transfer using convolutional neural networks,” in Pro-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "http://vision.ucsd.edu/\n“Yale\nface\ndatabase,”"
        },
        {
          "7. REFERENCES": "ceedings of the IEEE Conference on Computer Vision and Pat-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "content/yale-face-database,\nLast\naccessed\non"
        },
        {
          "7. REFERENCES": "tern Recognition, 2016, pp. 2414–2423.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "2019-10-17."
        },
        {
          "7. REFERENCES": "[9]\nJiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "[22] Connor Shorten and Taghi M. Khoshgoftaar,\n“A survey on"
        },
        {
          "7. REFERENCES": "Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Journal of Big\nimage data augmentation for deep learning,”"
        },
        {
          "7. REFERENCES": "Wang, Jianfei Cai, et al.,\n“Recent advances in convolutional",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": "Data, vol. 6, no. 1, pp. 60, Jul 2019."
        },
        {
          "7. REFERENCES": "neural networks,” Pattern Recognition, vol. 77, pp. 354–377,",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "2018.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "[10] Zhenyu Wu, Zhangyang Wang, Zhaowen Wang, and Hailin Jin,",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "“Towards privacy-preserving visual recognition via adversarial",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "training: A pilot study,” in The European Conference on Com-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "puter Vision (ECCV), September 2018.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "[11] Michael S. Ryoo, Brandon Rothrock, Charles Fleming,\nand",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "Hyun Jong Yang,\n“Privacy-preserving human activity recog-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "nition from extreme low resolution,”\nin AAAI Publications,",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "[12] Matthew A. Turk and Alex P. Pentland,\n“Face recognition us-",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        },
        {
          "7. REFERENCES": "ing eigenfaces,” in Pattern Recognition, 1991.",
          "[13] Gwen Littlewort, Marian Stewart Bartlett,\nIan Fasel,\nJoshua": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Prospect of smart home-based detection of subclinical depressive disorders",
      "authors": [
        "Enrique Leon",
        "Manuel Montejo",
        "Inigo Dorronsoro"
      ],
      "year": "2011",
      "venue": "2011 5th International Conference on Pervasive Computing Technologies for Healthcare"
    },
    {
      "citation_id": "3",
      "title": "E-health interventions for depression, anxiety disorder, dementia, and other disorders in old age: A review",
      "authors": [
        "Barbara Preschl",
        "Birgit Wagner",
        "Simon Forstmeier",
        "Andreas Maercker"
      ],
      "year": "2011",
      "venue": "Journal of CyberTherapy and Rehabilitation"
    },
    {
      "citation_id": "4",
      "title": "Behavioral signal processing: Deriving human behavioral informatics from speech and language",
      "authors": [
        "Shrikanth Narayanan",
        "Panayiotis G Georgiou"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "5",
      "title": "Cyberthreats under the bed",
      "authors": [
        "Nir Kshetri",
        "Jeffrey Voas"
      ],
      "year": "2018",
      "venue": "IEEE Computer"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition using dynamic grid-based hog features",
      "authors": [
        "Mohamed Dahmane",
        "Jean Meunier"
      ],
      "year": "2011",
      "venue": "Face and Gesture"
    },
    {
      "citation_id": "7",
      "title": "Deep spatiotemporal features for multimodal emotion recognition",
      "authors": [
        "Dung Nguyen",
        "Kien Nguyen",
        "Sridha Sridharan",
        "Afsane Ghasemi",
        "David Dean",
        "Clinton Fookes"
      ],
      "year": "2017",
      "venue": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "8",
      "title": "Automatic group affect analysis in images via visual attribute and feature networks",
      "authors": [
        "Shreya Ghosh",
        "Abhinav Dhall",
        "Nicu Sebe"
      ],
      "year": "2018",
      "venue": "25th IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "9",
      "title": "Image style transfer using convolutional neural networks",
      "authors": [
        "Leon Gatys",
        "Alexander Ecker",
        "Matthias Bethge"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Recent advances in convolutional neural networks",
      "authors": [
        "Jiuxiang Gu",
        "Zhenhua Wang",
        "Jason Kuen",
        "Lianyang Ma",
        "Amir Shahroudy",
        "Bing Shuai",
        "Ting Liu",
        "Xingxing Wang",
        "Gang Wang",
        "Jianfei Cai"
      ],
      "year": "2018",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Towards privacy-preserving visual recognition via adversarial training: A pilot study",
      "authors": [
        "Zhenyu Wu",
        "Zhangyang Wang",
        "Zhaowen Wang",
        "Hailin Jin"
      ],
      "year": "2018",
      "venue": "The European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "12",
      "title": "Privacy-preserving human activity recognition from extreme low resolution",
      "authors": [
        "Michael Ryoo",
        "Brandon Rothrock",
        "Charles Fleming",
        "Hyun Yang"
      ],
      "year": "2017",
      "venue": "AAAI Publications, Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Face recognition using eigenfaces",
      "authors": [
        "Matthew Turk",
        "Alex Pentland"
      ],
      "year": "1991",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Dynamics of facial expression extracted automatically from video",
      "authors": [
        "Gwen Littlewort",
        "Marian Bartlett",
        "Ian Fasel",
        "Joshua Susskind",
        "Javier Movellan"
      ],
      "year": "2004",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition: Workshop on Face Processing in Video"
    },
    {
      "citation_id": "15",
      "title": "Efficient privacy-preserving facial expression classification",
      "authors": [
        "Yogachandran Rahulamathavan",
        "Muttukrishnan Rajarajan"
      ],
      "year": "2015",
      "venue": "Transactions on Dependable and Secure Computing"
    },
    {
      "citation_id": "16",
      "title": "Face de-identification with expressions preservation",
      "authors": [
        "G Letournel",
        "A Bugeau",
        "V.-T Ta",
        "J.-P Domenger"
      ],
      "year": "2015",
      "venue": "International Conference on Image Processing"
    },
    {
      "citation_id": "17",
      "title": "Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications -ETRA 19",
      "authors": [
        "Julian Steil",
        "Marion Koelle",
        "Wilko Heuten",
        "Susanne Boll",
        "Andreas Bulling",
        "\" Privaceye"
      ],
      "year": "2019",
      "venue": "Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications -ETRA 19"
    },
    {
      "citation_id": "18",
      "title": "A hybrid deep learning architecture for privacy-preserving mobile analytics",
      "authors": [
        "Seyed Osia",
        "Ali Shahin Shamsabadi",
        "Sina Sajadmanesh",
        "Ali Taheri",
        "Kleomenis Katevas",
        "Hamid Rabiee",
        "Nicholas Lane",
        "Hamed Haddadi"
      ],
      "venue": "A hybrid deep learning architecture for privacy-preserving mobile analytics"
    },
    {
      "citation_id": "19",
      "title": "Minimax filter: learning to preserve privacy from inference attacks",
      "authors": [
        "Jihun Hamm"
      ],
      "year": "2017",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "20",
      "title": "Protecting visual secrets using adversarial nets",
      "authors": [
        "Nisarg Raval",
        "Ashwin Machanavajjhala",
        "Landon Cox"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "21",
      "title": "The Japanese female facial expression (JAFFE) database",
      "authors": [
        "J Michael",
        "Shigeru Lyons",
        "Miyuki Akamatsu",
        "Jiro Kamachi",
        "Julien Gyoba",
        "Budynek"
      ],
      "year": "1998",
      "venue": "Proceedings of third international conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "22",
      "title": "Yale face database",
      "year": "2019",
      "venue": "Yale face database"
    },
    {
      "citation_id": "23",
      "title": "A survey on image data augmentation for deep learning",
      "authors": [
        "Connor Shorten",
        "M Taghi",
        "Khoshgoftaar"
      ],
      "year": "2019",
      "venue": "Journal of Big Data"
    }
  ]
}