{
  "paper_id": "2506.15834v1",
  "title": "Machine Learning-Based Context Aware Emas: An Offline Feasibility Study",
  "published": "2025-06-18T19:20:47Z",
  "authors": [
    "Zachary D King",
    "Maryam Khalid",
    "Han Yu",
    "Kei Shibuya",
    "Khadija Zanna",
    "Marzieh Majd",
    "Ryan L Brown",
    "Yufei Shen",
    "Thomas Vaessen",
    "George Kypriotakis",
    "Christopher P Fagundes",
    "Akane Sano"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Mobile health (mHealth) systems help researchers monitor and care for patients in real-world settings. Many studies that utilize mHealth applications use Ecological Momentary Assessments (EMAs), passive sensing, and contextual features to develop emotion recognition models, which rely on EMA responses as ground truth. Due to this, it is crucial to consider EMA compliance when conducting a successful mHealth study. Utilizing machine learning is one approach that can solve this problem by sending EMAs based on the predicted likelihood of a response. However, literature suggests that this approach may lead to prompting participants more frequently during emotions associated with responsiveness, thereby narrowing the range of emotions collected. We propose a multi-objective function that utilizes machine learning to identify optimal times for sending EMAs, thereby increasing the likelihood of response and capturing a broader range of emotions. The function identifies optimal moments by combining predicted response likelihood with model uncertainty in emotion predictions. Uncertainty would lead the function to prioritize time points when the model is less confident, which often corresponds to underrepresented emotions. We demonstrate that using this objective function to guide EMA delivery would result in prompts being sent when participants are responsive and experiencing less commonly observed emotions. Prioritizing less commonly observed emotions promotes emotional diversity, defined as the range of emotions a person experiences over time, leading to a more comprehensive picture of overall well-being.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Mobile health (mHealth) applications and systems have seen a remarkable rise in popularity. mHealth research has demonstrated significant promise in aiding healthcare providers in managing various mental health conditions, offering cost-effective, easily accessible, and minimally intrusive interventions  [46] . The primary advantage of these mHealth systems lies in their capacity to gather data (through wearable or phone sensors) that can help predict adverse health outcomes. This, in turn, enables researchers to develop interventions through reminders, education, or motivation  [26]  in real-time.\n\nEcological Momentary Assessments (EMA) capture daily in-situ snapshots of individuals' emotions and behaviors through recurring surveys  [49]  and are commonly used in mHealth applications. EMAs are particularly important in mental health and well-being applications and serve as the gold standard for measuring emotional state. Without EMA responses, mHealth applications will struggle to accurately assess users' emotional states. However, EMAs can be burdensome to users as they require active responses, especially when prompts are frequent. Rather than prompting users more often, mHealth studies can collect more responses by improving participant compliance. Some researchers have utilized machine learning techniques to determine the most appropriate time to send an EMA or intervention, which has proven effective in improving receptivity rates for both EMAs  [31]  and interventions  [30] . Several researchers have explored alternative mechanisms to enhance EMA receptivity, including reducing the complexity or frequency of prompts or increasing incentives  [10, 19, 52 ]. Yet, these approaches may limit the number of responses, alter survey content, or increase study costs. Conversely, machine learning-based interruptions can maintain higher compliance rates without these drawbacks.\n\nHowever, a significant challenge in using machine learning to schedule EMAs is its potential unintended consequences on reported responses. Several researchers have explored factors influencing interruptibility, such as respondents' emotional state  [14] . Specifically, there is a well-documented link between emotional state and EMA responsiveness  [34, 45, 52] , with several studies indicating that participants are less likely to respond when experiencing negative emotions. Emotional state and several other factors can be used as a proxy for receptivity; however, these factors would affect the prediction of a receptivity model and lead to the model avoiding prompts when participants are experiencing emotions associated with non-responses  [22] . This poses a challenge for emotion recognition models. If machine learning-based EMA triggers disproportionately send notifications during certain emotional states, it may inadvertently hinder the development of an emotion recognition model.\n\nTo address this challenge, we propose a multi-objective function designed to optimize the timing of EMAs using data from wearable devices and phone sensors. This function incorporates two machine learning models: one estimates the likelihood of an EMA response, and the other captures uncertainty in predicting the study-specific emotional state. A trigger would then deliver the EMA at the time that maximizes the objective function's output. The potential framework of this Smart Trigger is shown in Figure  1 . This approach balances both the likelihood of participant response (EMA receptivity) and the uncertainty in predicting the target emotion. The inclusion of the receptivity model in the multi-objective function aims to improve the participant's receptivity rate. While prompting during periods of high model uncertainty encourages EMA delivery across a broader range of emotional states, as uncertainty tends to be higher for emotions that are less frequently represented in the training data. Accounting for uncertainty when sending EMAs helps counterbalance any potential biases the receptivity model may have on reported emotions. As stated, a system that prompts users based solely on their likelihood of responding would send EMAs more frequently during emotions associated with responsiveness. In contrast, using the proposed objective function would minimize this effect.\n\nThe main objective of this work is to demonstrate the feasibility of our multi-objective function in two distinct populations. Insights gained from this work will inform the development of a future ML-based context aware EMA and the design of a future validation study. A significant gap in current research lies in the lack of adaptive methods for improving EMA receptivity rates. Existing machine learning-based EMA triggers primarily focus on responsiveness as the sole criterion for delivering EMAs, often overlooking the collection of responses during varying emotions. This   [20] . To predict momentary emotions, researchers use a wide range of sensors, signals, and features including, respiration  [3, 13, 18] , electrocardiogram (ECG)  [17, 47, 54] , Galvanic Skin Response (GSR)  [55] , skin temperature  [18] , accelerometers  [44] , and phone-based data including Global Positioning System (GPS) and call logs  [37] . The ability to predict these constructs varies based on the sensor suite, the complexity of the intended construct, and the algorithm. The significant pitfall of these momentary emotion recognition models is their dependency on EMA responses as ground truth. As sensors generate increasingly rich data, sparse EMA responses continue to be a bottleneck. Addressing this mismatch will require innovations in labeling methods and improvements in EMA design or delivery to support the development of more effective and scalable emotion-aware systems.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ema",
      "text": "As mentioned, many researchers have studied the factors influencing receptivity or interuptability. Ho et al. described 11 components that affect the perceived burden of an interruption, including present and future activity, frequency and modality of the interruption, the utility of the message, the required effort, the interface, social aspects, history of responding, and the user's emotional state  [14] . Some of the more common solutions proposed to improve receptivity and limit the burden of an interruption that we found in the literature include changing the sampling rate (frequency), reducing the length of the EMA (complexity), and increasing the compensation of the study or increasing compensation per response (incentivization).\n\nResearchers often explore adjusting EMA frequency or complexity to improve compliance, but the effects on receptivity remain mixed. Some studies, including Eisele et al. and Field et al., found little to no relationship between EMA frequency and perceived burden  [10, 21] , while others, such as Wen et al., reported higher compliance when EMA frequency is reduced among Youth in non-clinical studies  [52] . Despite these findings, reducing EMA frequency limits data density, posing challenges for machine learning applications. An alternative strategy employed by several researchers is to reduce the size or complexity of the EMA. Eisele et al. showed that smaller question sets reduce perceived burden  [10] . Intille et al. introduced the ùúá-EMA, which prompted users more frequently with fewer questions and maintained stable compliance over four weeks, unlike traditional EMAs, which saw declines  [19] . Li et al. proposed a similar method but used a dynamic prompt that only provided questions based on the potential information gained  [25]  from each question. Still, reducing or omitting items from validated instruments may compromise construct validity and limit the interpretability of results. Furthermore, EMAs with fewer questions also provide less information and context, which may reduce the overall value of the collected data.\n\nIncentives are another strategy used to improve EMA receptivity. Wrzus et al. found that 15% of EMA studies offered no incentives, while 27% tied incentives to compliance  [53] .  Ottenstein et al.  found that EMA studies offering incentives tend to achieve higher compliance rates than those that do not  [38] . Although effective for recruitment, incentive-based approaches raise ethical concerns about coercion  [51]  and may be unfair if participants miss prompts due to scheduling conflicts. A more sustainable approach would focus on improving compliance without relying on compensation, especially for applications intended for the consumer market, where users engage with mHealth applications for their own well-being.\n\nA more recent method employed by researchers involves utilizing sensors and contextual data to help determine when to send EMAs. Studies have shown that factors like location, personality traits, physical activity, and time of day  [5, 23, 29, 33, 42]  play a role in receptivity. Mishra et al. proposed using machine learning to improve the receptivity of Just-in-time interventions (JITI)  [30] . They tested a static model using only the previously collected data and an adaptive model, which expanded on the static model using information gathered from the user. Similar to the previous methods discussed, this method has drawbacks. The primary issue with an ML-based EMA trigger is mentioned by King et al., who detailed the relationship between emotional state and EMA receptivity  [22] . Their findings suggest that sending EMAs based on the predicted likelihood of a response would introduce bias into participant responses. This bias is that notifications would be sent during more positive emotions compared to if the EMAs were sent randomly.\n\nHowever, this method avoids the drawbacks of the previous three methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Objective Function For Optimal Ema Delivery",
      "text": "The proposed multi-objective function combines two models: a receptivity model that predicts the probability of an EMA response and an emotion recognition model that provides the uncertainty of an emotion prediction at any given moment.\n\nThe objective function integrates the outputs of both models to deliver the EMA at a time that maximizes the combined likelihood of response and the uncertainty of the emotion recognition model. This could improve receptivity rates and emotion prediction accuracy by targeting moments of high model uncertainty. This strategy also helps the model to personalize and converge more rapidly than if EMAs were sent randomly. An example of when model uncertainty may be higher is when the model encounters less familiar or previously unobserved patterns in the data. These moments may reflect atypical behaviors, such as attending a unique event, exercising more than usual, or experiencing an emotion that has not been reported before.\n\nThis section includes the methods used for model development and evaluation. In the data processing subsection, we discuss feature processing and extraction, timezone drift correction, and class labels. The model development subsection describes the design of the emotion recognition and the receptivity models. We then introduce the proposed multi-objective function. And finally, the Evaluation subsection discusses our methods for evaluating model performance and the multi-objective function.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Receptivity And Emotion Models",
      "text": "The receptivity model is trained and tested using binary receptivity labels, whereas the emotional state model is trained and tested on sequential labels representing emotion. The input features for these two models are computed from sensor signals. These models operate on segments of data that are 30 minutes long. Each segment is assigned a label: non-responsive, responsive, or unlabeled. An emotion score is also assigned to that segment if there was a response.\n\nMore information on the reported emotional states can be found in section Datasets.\n\n3.1.1 Class Labels. Receptivity was labeled based on the time of the initial notification of the EMA and when the user responded, as shown in Fig.  2 . If a person did not respond to the EMA, the entire 60 minutes after the time of notification was labeled as non-receptive. Since participants have 60 minutes to respond to the survey before it closes.\n\nIf the participant responds to the survey, the 30 minutes before the response are labeled as receptive. We label the prior 30 minutes as receptive rather than the entire 60 minutes because we cannot assume that the participant will remain receptive after responding to an EMA. We chose a 30-minute window to increase the number of labeled data points.\n\nFurther increasing the window size would make it more challenging to argue that the labeled data points are associated with receptivity and emotional state. However, in the Appendix (Tables  S5  and S7 , we present the performance metrics of both the receptivity and emotion recognition models using varying window sizes  (10, 15, 20 , and 60 minutes). After comparing different window sizes, we found that the 30-minute window consistently yielded the best performance for both datasets in predicting emotional state and receptivity. Like labeling receptivity, the segments within the 30-minute window are assigned emotion scores based on the response.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Receptivity Model.",
      "text": "The class labels for the receptivity model are binary, indicating whether a response was collected from a prompted EMA or not. We test several machine-learning algorithms selected based on past research for both studies, including Support Vector Machine  [23, 48] , Random Forest  [23, 29, 30, 39] , Boosting Algorithms  [29, 39] ,\n\nNaive Bayes  [29, 39] , and Neural Network  [7, 36] . These prior works predict receptivity; however, some focus on EMA responsiveness, while others target JITAI responsiveness. In addition to these models, we employ a simple neural In addition to modeling emotion using the neural network, we also model emotion using linear regression and a random baseline. The baseline model for emotional state is chosen randomly based on the mean and standard deviation of emotion scores in the training set. These models are used only for performance evaluation, as the proposed objective function relies on uncertainty for decision-making. Most emotion recognition studies use classification models, simplifying continuous emotion into categories (e.g., low, medium, high). Prior work in regression-based emotion recognition often uses inputs like speech or images to predict valence and arousal  [1, 12, 32] . Our approach uses regression to generate uncertainty estimates that highlight underrepresented emotional states. This avoids sacrificing the nuance of our labels that occurs when categorizing emotion scores.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Multi-Objective Function.",
      "text": "The multi-objective function helps determine the optimal time for an EMA based on the output of the receptivity model (likelihood for a response) and the uncertainty of the predicted emotional states. This is similar to the function presented by Kuo et al.  [24] . Equation 1 represents the multi-objective function. The ùë§ ùë¢ and ùë§ ùëü variables are weights that allow the researchers to prioritize either uncertainty or receptivity. The functions ùëà (ùë°) and ùëÖ(ùë°) are computed from the receptivity and emotion model outputs at time ùë°, where ùë° ‚àà ùëá represents the time frame for sending an EMA (ex., between 9 AM and 12 PM). For receptivity, ùëÖ(ùë°) is the probabilistic prediction of a response, and ùëà (ùë°) represents the uncertainty measure. Uncertainty and receptivity are maximized with respect to ùë°, corresponding to the ideal time to send an EMA.\n\n(1)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate the multi-objective function using data from two studies, each of which collected wearable and EMA data.\n\nOne study involved ADRD spousal caregivers (hereafter referred to as the 'ADRD study'), while the other involved healthy participants (hereafter referred to as the 'Healthy study').",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Considerations. The Adrd Study (Irb-Fy2021-65) Has Been Reviewed And Approved By The Rice University",
      "text": "Institutional Review Board. The Healthy study was granted ethical approval by the Sociaal-Maatschappelijke Etische Commissie of Katholieke Universiteit Leuven (G-201809 1339).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Adrd Study. Participants And Study Design:",
      "text": "The ADRD study aimed to understand the mental and physical effects associated with ADRD spousal and family caregivers (N=91). We included 73 participants in our analysis after removing participants with no EMA responses overlapping with sensor data (n=14) and those who dropped out (n=4).\n\nExcluding the 4 participants who dropped out, the average age was 62 (SD = 11.8), and 87% were female. The gender disparity reflects that women make up about two-thirds of spousal caregivers for Alzheimer's patients  [2] . The remaining demographic information for the ADRD dataset is found in the appendix (Table  S3 ). The ADRD study included three blocks of data collection, each lasting one week. During each data collection block, participants were asked to respond to 5 EMAs daily (1 morning, 1 bedtime, and 3 random), wear a Fitbit Inspire 2, and download the AWARE application  [11] .\n\nEMA: Each EMA was randomly sent within one of five scheduled time blocks spaced throughout the day via text and email. Each message contained a link to a Qualtrics survey. The survey was available for 1 hour, and participants received a reminder text after 30 minutes. The morning EMA was sent between 8 and 9 AM and included questions about the quality and length of their sleep. The bedtime EMA was sent between 8 and 9 PM and included questions reflecting the day. The remaining three EMAs were composed of the Positive and Negative Affect Schedule (PANAS)  [27] , along with questions regarding depression and loneliness. We use the shortened 10-item PANAS questionnaire, consisting of five positive affect (PA) and five negative affect (NA) items rated on a scale from 1 to 5. PA and NA composite scores are calculated by summing the respective items, resulting in scores ranging from 5 to 25.\n\nSensor Data and Processing: Using Fitbit's intra-day API, we extracted minute-by-minute data for steps, heart rate, sleep, and Heart Rate Variability (HRV). We extracted sleep duration, efficiency, and regularity  [40] . HRV features were calculated during a 5-minute window and included the root mean sum of square difference (RMSSD), High Frequency (HF), and Low Frequency (LF). HRV data was collected exclusively during sleep periods lasting more than three hours when the participant was relatively still.\n\nThe AWARE app collects four data modalities: phone logs, message logs, screen usage, and location. From these, we calculate several features, including clustered location, screen time, call duration, number of calls (incoming, outgoing, and missed), and number of SMS messages (incoming and outgoing). The data was temporarily stored on the phone and then sent to a MySQL server at Rice University when the participant's phone was charging and connected to WiFi.\n\nA list of features extracted can be found in Table  S8 , and a more detailed description of data processing and feature extraction can be found in the appendix (section A).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Healthy Study.",
      "text": "Participants and Study Design: Forty-five healthy participants enrolled in the study to assess the impact of laboratory stress tests on participants' daily lives, which lasted 10 days  [8, 9] . However, this study is a secondary analysis that focuses solely on data collected during the real-world phase of the original study. Participants were recruited in Leuven, Belgium, and the majority (60%) were college students enrolled at KU Leuven. The participants had an average age of 24.5 years (SD = 3), with 38 individuals (84%) identifying as female. Race and ethnicity were not collected for the Healthy Study.\n\nEMA: Participants were given a dedicated smartphone to respond to 10 EMAs daily, which were sent at random intervals between 15 and 90 minutes apart. EMAs included 10 mood-assessment questions  [35] . The mood question set covered nine NA components (worried, stressed, anxious, annoyed, down, restless, tense, under pressure, ashamed) and four PA components (relaxed, cheerful, confident, in control). Each component was prefaced with \"At the moment, I feel. . . \" and participants rated their responses on a scale from 1 (not at all) to 7 (very much). Similarly to PANAS, PA (range: 4-28) and NA (range: 9-63) were calculated by summing the respective individual responses.\n\nSensor Data and Processing: The participants wore a 2-electrode ECG chest patch and a wrist-worn sensor that collected electrodermal activity (EDA) at 256 Hz, skin temperature (ST) at 1 Hz, and accelerometer (ACC) at 32 Hz.\n\nWe extracted features from ST, ECG, EDA, and ACC. The data is segmented into 30-minute segments, and features are extracted from these segments. We utilized the Python package biosppy  [6]  to process the ECG data. Biosppy uses a bandpass filter with 3 Hz and 45 Hz frequencies, a sampling rate of 256, and the Hamilton segmentation algorithm to extract R peaks. We then validated the R peaks using the algorithm by Hovsepian et al.  [16] ; this algorithm uses the criterion beat difference based on the maximum expected difference for a beat and the minimal artifact difference.\n\nWe then used the Python package hrvanalysis  [41]  to extract heart rate and heart rate variability features. We used the method proposed by Taylor et al. to process and extract the statistical and wavelet features from EDA  [50] . For ST, we filtered out outliers using the IQR and then extracted statistical features during each segment. We smoothed the accelerometer signal using a fourth-order 10-Hz low-pass Butterworth filter and obtained a baseline. Then, we used the sensormotion Python package  [15]  to extract activity features. A detailed list of features can be found in Table  S9 .\n\n3.2.4 EMA Class Label Distribution. Figure  3  illustrates the distribution of responses. PA shows a wide distribution range for both studies. In contrast, NA responses are strongly skewed with low variability, making accurate prediction challenging for an emotion model, as it lacks references to higher levels of NA. Given this distribution, our Emotion model will focus solely on PA scores.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Evaluation",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Receptivity And Emotion",
      "text": "Model Settings and Evaluation. We employ a cross-validation approach, which we refer to as semi-personalized. The semi-personalized approach begins similarly to Leave-One-Subject-Out (LOSO), but gradually incorporates personalized data. The first day is tested using a model trained only with other participants' data. For each of the following days, the previous days from the test participant are added to the training set. This method replicates how a researcher may personalize an emotion or receptivity prediction model. This method outperforms a purely personalized approach due to the insufficient data available from each individual participant. We utilize a min-max normalization method. On the first day, we normalize the features using the data from the other participants.\n\nSubsequent days are normalized with respect to the participant's previous days of data. We test the performance of several machine learning algorithms and a random baseline model. For the receptivity model, we calculate the weighted F1 score (harmonic mean of precision and recall), Accuracy (proportion of correctly classified instances), and weighted Precision (weighted average of precision for both classes). For the emotion recognition model, we calculate Root Mean Square Error (RMSE) and ùëÖ 2 . Additionally, we conduct an ANOVA to compare performance metrics across the different machine learning algorithms, evaluated at the participant level. If the ANOVA test indicated a significant effect, we conducted post hoc Tukey tests to identify specific differences between the algorithms.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Trigger Evaluation.",
      "text": "In an ideal situation, the multi-objective function would be tested against a control (randomly sending EMAs throughout the day) in a real-world study. However, to demonstrate the feasibility of our system before deploying it in a real-world setting, we test the multi-objective function offline using the high-resolution sensing and EMA data described in previous sections.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Analyzing The Output Of The Multi-Objective Function (ùêΩ ):",
      "text": "This evaluation method assesses the relationship between ùêΩ (Eq. 1) and reported emotional state and receptivity. By examining these relationships, we can assess whether the proposed trigger would likely prompt users when they are more receptive or during emotional states that are more beneficial to model development.\n\n3.3.4 RQ1 (Would the proposed multi-objective function send EMAs when people are more responsive?): To address RQ1, we use a mixed effect model to determine the statistical relationship between ùêΩ and receptivity labels, where the response label is the dependent variable, ùêΩ is the fixed effect, and participant ID is the random effect. This will demonstrate how the output of the multi-objective function (ùêΩ ) behaves during responsive and non-responsive states, while accounting for individual differences across participants. We also utilize a repeated-measure ANOVA. However, since we are collecting repeated measures and repeated outcomes, the dependent variable (ùêΩ ) will be aggregated among responses and non-responses for each participant. Using these two statistical methods, we show that ùêΩ is higher during receptive time points, indicating the proposed objective function prioritizes moments when participants are more likely to respond.\n\n3.3.5 RQ2 (Would the proposed multi-objective function send EMAs when participants are experiencing emotional states that are less observed?): We employed a linear mixed-effect model to address RQ2. Rather than directly analyzing the relationship between PA and ùêΩ , we investigated the relationship between ùêΩ and the absolute standardized score of PA, representing how far each value is from the participant's mean. To focus on the magnitude of deviations, regardless of direction, we used the absolute value of the z-scores. Finally, we fit a mixed-effect model with ùêΩ as the fixed effect, participant ID as the random effect, and the absolute z-scores as the dependent variable.\n\nWe also visualize the relationship between reported PA and ùêΩ by plotting the distribution of PA (y-axis) against a fitted line representing the average value of ùêΩ (x-axis) during varying levels of PA. This highlights the potential of the multi-objective function to prioritize EMA delivery during moments of higher model uncertainty. Emotion scores further from the mean are more likely to be underrepresented in the dataset, are generally harder to predict, and are more beneficial to incremental learning.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Rq3 (",
      "text": "To what extent would the multi-objective function improve participant compliance and enhance the distribution of captured emotional responses compared to traditional EMA delivery methods?): To assess the potential impact of the multi-objective function on EMA delivery and data collection, we compare its performance with that of a traditional random sampling approach across multiple time windows. Each day for each participant is divided into five evenly spaced three-hour windows, and within each window, a delivery time point is selected using the two EMA delivery methods. The two EMA delivery strategies are: (1) the Smart Trigger, which selects the delivery time point that maximizes the output of the multi-objective function (ùêΩ ), and (2) a random trigger, which selects a delivery time point at random.\n\nFor the random trigger, the selected time point is chosen randomly during the scheduled window. In contrast, the Smart Trigger selected the time point that maximized the value of ùêΩ (Eq. 1). We utilize our receptivity and emotion recognition models to obtain predicted values for responsiveness (response or non-response) and emotion (PA score) at each selected time point. We then calculate each participant's response rate using both triggers. A paired t-test is used to assess whether the difference in participant response rates between the two EMA triggers is statistically significant.\n\nAdditionally, we analyze the distribution of predicted emotional states captured by each strategy. For both the Smart Trigger and the random trigger, we predict emotion scores at each selected time point to compare the distributions of participants' predicted emotions captured by each method. This comparison is done using the Kolmogorov-Smirnov (KS) Test, which assesses whether the distributions differ significantly. We also conducted a paired t-test to compare the within-participant variance in predicted emotions between the two triggers. This allows us to determine whether one strategy captures a broader range of emotions within individuals.",
      "page_start": 3,
      "page_end": 11
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Model Evaluation",
      "text": "The results of our receptivity model for the ADRD and Healthy datasets using the semi-personalized cross-validation approach are shown in Table  1 . We also analyzed the model performance using a Leave-One-Subject-Out (LOSO) cross-validation approach; for more information, see Table  S4 . The neural network is the best overall performer for the ADRD dataset, achieving higher accuracy and F1 scores compared to the other algorithms. However, this performance comes at the cost of reduced precision in identifying the positive (Responsive) class. We conducted an ANOVA to compare model performance across participants and found no significant differences in F1 score or precision. However, there was a significant effect for accuracy (F = 8.2, p < 0.001). A post hoc Tukey test revealed that the neural network's accuracy was significantly different from all other models, and that each model, excluding SVM, was statistically different from the baseline.\n\nFor the Healthy dataset, the random forest model performs best overall, achieving the highest accuracy and F1 scores.\n\nWhile its precision is comparable to the other models, it offers a clear advantage in overall predictive performance.\n\nIn the Healthy dataset, the ANOVA results showed that only Accuracy demonstrated a significant difference across algorithms (F = 8.0, p < 0.001). The post hoc Tukey test revealed that all algorithms except SVM were significantly different from the Baseline. Additionally, the neural network and random forest models differed significantly from all other algorithms, except for each other.\n\nTable  2  shows RMSE results for the personalized models predicting emotional state. For results using the LOSO cross-validation approach, see Table  S6 . The neural network outperforms both linear regression and the baseline model, achieving an RMSE of approximately 3.5 for the ADRD dataset and 4.1 for the Healthy dataset. It is also the only model to achieve a positive ùëÖ 2 , with values of 0.48 for the ADRD study and 0.27 for the Healthy study. All other models produced negative ùëÖ 2 values, indicating performance worse than predicting the mean. This was partly due to low variability in reported emotions for some participants. For both datasets, RMSE was statistically different across algorithms (ADRD Dataset; F = 13.9, p < 0.001, Healthy Dataset; F = 11.5, p < 0.001). For both datasets, the Tukey test showed that the neural network's RMSE was significantly different from the Linear Regression and Baseline models.\n\nThe results discussed in this section utilize data segmented into 30-minute-long windows. Model performance across multiple window lengths is detailed in S5 for the receptivity model and in S7 for the Emotion model. These findings indicate that 30-minute segments outperform other window lengths, especially the 10-and 15-minute windows. While performance differences between the 30-and 60-minute windows are smaller, longer segments place the data further from the actual EMA response time.  mixed linear model, we found that ùêΩ was a significant predictor of receptivity for both the ADRD caregiver and the Healthy population. A few participants from each study were excluded from this analysis because their labeled data consisted entirely of responses or entirely of non-responses.\n\nADRD Study: The estimated coefficient for ùêΩ was 0.04 (p < 0.001, 95% CI: [0.027:0.047]). The intercept was also significant (z = 0.89, p < 0.001), highlighting a strong baseline likelihood of response across participants. The random effect variance for participants was 0.04, indicating some variability between participants in their response tendencies.\n\nHealthy Study: The estimated coefficient for ùêΩ was 0.03 (p < 0.001, 95% CI: [0.027:0.037]), suggesting a positive relationship between ùêΩ and the probability of response. The intercept was also significant (z = 0.98, p < 0.001), highlighting a strong baseline likelihood of response across participants. The random effect variance for participants was 0.02, indicating some variability between participants in their response tendencies.\n\nWe also evaluated the statistical difference in ùêΩ between responses and non-responses using repeated measures ANOVA. For both studies, we can visually notice the differences in ùêΩ during responses and non-responses. However, the repeated measure ANOVA takes into account the participant-specific differences. For the ADRD study, the results of the repeated-measures ANOVA indicate a statistically significant effect on responsiveness (F=4.5, p = 0.04), suggesting that ùêΩ differs significantly between instances of responses and non-responses. For the Healthy study, we found that this relationship was not significant. This is partly due to the number of participants in the Healthy dataset. Since we aggregate values of ùêΩ during responses and non-responses, we effectively reduce the number of observations to the number of participants, which may result in insignificant findings.   1 ) during responses and non-responses for each population.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Relationship Between The Output Of The Multi-Objective Function ùêΩ And Reported Emotional State (Rq2)",
      "text": ". Figure  5a  illustrates the relationship between ùêΩ and reported PA for the ADRD caregiver study, and Figure  5b  illustrates the same relationship for the Healthy study. Notably, ùêΩ is larger at PA scores where there are fewer observations, as seen on the edges of the distribution. This suggests that an EMA trigger using the multi-objective function would prioritize sending EMAs when participants are experiencing less frequently observed emotions, potentially capturing atypical emotional states that are more beneficial for model development.\n\nWe also found that the relationship we described between ùêΩ and the distribution of PA (Figure  5 ) for both studies is statistically meaningful. As mentioned, we converted the reported PA values to participant-specific absolute z-scores.\n\nThis means that our mixed-effects model is examining the relationship between ùêΩ and the degree to which the value deviates from the participant's average reported PA score. For the ADRD study, the effect of J on the absolute z-score was statistically significant (p < 0.001), with a coefficient of 0.08 (95% CI: [0.095, 0.11]). For the Healthy Study, the effect of J on absolute z-scores was statistically significant (p < 0.001), with a coefficient of 0.12 (95% CI: [0.09, 0.14]).\n\nThe results from both studies indicate that higher values of ùêΩ were associated with PA values further from the mean.\n\nAnd because of the distribution of reported PA, that would mean that values of ùêΩ are generally larger during reported emotions that are less represented in the dataset. The resulting coefficients for both studies are relatively small, which can be explained by the range and distribution of the absolute z-scores. ADRD participants with labeled data, the average receptivity rate for the random trigger was 0.82 (SD = 0.28), while the Smart Trigger achieved a higher rate of 0.93 (SD = 0.20). A paired t-test revealed that this difference in participant receptivity rates was statistically significant (t = 2.47, p = 0.01). For the Healthy dataset, the average receptivity rate was 0.87 (SD = 0.19) for the random trigger and 0.94 (SD = 0.10) for the Smart Trigger. These findings were also statistically significant based on a paired t-test (t = 2.14, p = 0.04).\n\nThe findings also suggest meaningful differences in emotional states at the time points selected by the two EMA triggers. For the ADRD population, a comparison of the PA score distributions revealed statistically significant differences, as determined by the KS test. However, the test statistic indicates that these differences were relatively subtle. On average, PA values at time points selected by the Smart Trigger were 13.7 (SD = 4.2), compared to 13.4 (SD = 4.0) for the random trigger. Notably, both averages are higher than the overall average PA score in the original dataset, which was 11.8 (SD = 4.9). For the Healthy dataset, a comparison of PA score distributions revealed statistically significant differences, as determined by the KS test. On average, PA values at time points selected by the Smart Trigger were 16.9\n\n(SD = 4.1), while those selected by the random trigger averaged slightly higher at 17.1 (SD = 3.5).\n\nFinally, we observed a significant difference in participant-specific emotion score variance between the Smart Trigger and the random trigger for the ADRD population, as shown by a paired t-test (t = -1.97, p < 0.05). On average, the within-participant variance was slightly higher for the Smart Trigger (M = 3.04, SD = 0.92) than for the random trigger (M = 2.83, SD = 0.93). For the Healthy population, we also found a significant difference in participant-specific emotion score variance between the Smart Trigger and the random trigger, as indicated by a paired t-test (t = -2.44, p < 0.05).\n\nThe average within-participant variance was higher for the Smart Trigger (M = 2.76, SD = 1.45) compared to the random trigger (M = 2.15, SD = 1.01). These results suggest that the Smart Trigger may capture a wider range of emotional states within individuals.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Model Performance",
      "text": "The performance of both the receptivity and emotion recognition models aligns with results reported in prior research.\n\nNotably, the receptivity model surpasses the Just-In-Time receptivity approach proposed by Kunzler et al., who developed a similar system for delivering interventions at moments when participants are most likely to respond  [23] . In their study, they achieved an F1-score of 0.31. Our receptivity model also outperforms several other works that aim to send messages, specifically interventions, at more responsive time points  [30, 31] . However, none of the mentioned studies incorporate time-series wearable data; instead, they focus on passively collected contextual information from participants' smartphones. This underscores the value of time-series wearable data in accurately predicting a participant's receptivity state. Directly comparing the performance of our emotion recognition model to previous work is challenging, primarily because our approach focuses on regression rather than binary or multi-class classification. Moreover, among the few studies that employ regression-based emotion recognition, most rely on data from facial expressions captured by cameras or participant speech or text, rather than wearable sensor data.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Implications Of The Multi-Objective Function",
      "text": "This paper presents a new method that can significantly improve data collection in mHealth studies, particularly for those developing machine learning (ML) models. While our community has continuously strived to make the objective measures (wearable devices) more accurate, efficient, and robust, comparatively less attention has been given\n\nto advancing the subjective data collection methods, such as EMAs. Our evaluation demonstrates the potential of using machine learning techniques to distribute EMAs. Many mHealth studies, particularly those focused on well-being, rely on subjective measures that can only be sampled a few times a day. Maximizing the utility of each EMA can enhance the quality of our data and models, allowing us to intervene when needed.\n\nThe objective function we have presented is modular, wherein any construct can be seamlessly integrated into Equation 1. Its inherent flexibility allows for tailored adaptations to suit any population. The devices, sensors, and EMA constructs should be aligned with the population's characteristics. Specifically, for leveraging uncertainty, it is beneficial to collect data that is known to relate to the construct a researcher aims to predict. Additionally, the weights in Equation 1 must be adjusted based on the unique needs and differences of each population. For example, populations that struggle with EMA adherence might benefit from a higher weight of receptivity (ùë§ ùëÖ ). These weights could also be adjusted adaptively using a reinforcement learning approach. In this method, rewards are defined for EMAs that prompt a response or yield valuable information for the model, while penalties are applied to non-responses. The model would then explore various weight combinations for uncertainty and receptivity and, over time, converge on the optimal weights for each specific participant or population. Fitbit device is relatively easy for any population to use, regardless of age and technological intelligence. Furthermore, our results may be influenced by population bias. By testing the trigger on two distinct populations, including a healthy general population, we aimed to improve the generalizability of our findings to other groups.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Limitations",
      "text": "However, the differences in data collection methods between the two populations can also present a potential limitation. While these differences demonstrate the generalizability of the multi-objective function, they also introduce variability. Specifically, the data collected and the study duration differ: for ADRD caregivers, we collect minute-byminute Fitbit data and contextual data from the AWARE app over three weeks, whereas for the healthy population, we gather high-frequency ST, EDA, ECG, and ACC data over 10 consecutive days. While these variations highlight that the multi-objective function can be applied across diverse data collection protocols, they may also pose challenges in interpreting results, as differences in data resolution and study duration can influence model performance and impact efficacy.\n\n5.3.2 Distribution of Class Labels. The distribution of PA (our target variable) in both studies shows a central tendency, with occurrences being more frequent near the mean and decreasing as values deviate further from it. This allows us to assume that model uncertainty will be greater for emotions further from the mean, as model uncertainty is generally higher for less common occurrences. Additionally, participants reported varying intensities of positive emotions, unlike NA. A higher variance is needed to develop robust machine learning models that accurately predict emotions. When testing this trigger using NA as the target variable, we were unable to develop a model that was accurate enough to meet the project's needs. Nevertheless, the model would find that most negative emotions would have higher values of ùêΩ , albeit there is only a small subset of higher values of reported NA. This lack of higher-intensity negative emotions may not solely reflect participants' true emotions but could instead stem from psychological factors influencing their willingness to report intense negative emotions. Social desirability bias may be one factor influencing this lack of variability in reported NA; Bergen et al. describes social desirability bias as the tendency to present oneself in a more socially acceptable manner  [4] .\n\nWhile our findings demonstrate success in targeting moments of responsiveness and model uncertainty, they cannot fully address external psychological factors, such as social desirability bias, that influence participants' reported emotions.\n\nHowever, due to the modularity of the objective function, additional variables could be incorporated to account for such factors. For instance, introducing a third variable that measures the difference between the model's emotion prediction and the participant's average reported emotion could help identify moments when participants are more likely to deviate from their typical patterns of reporting. This approach could increase the likelihood of capturing genuine emotional states, including those participants who may otherwise hesitate to disclose.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Future Work",
      "text": "In future work, we plan to investigate the weights of the proposed objective function. While decisions can be made before the study's deployment, they depend on the intended outcome and population specific to each study. Consequently, it may prove beneficial to explore adaptive weights that fit the evolving needs of each participant at different time points throughout a study.\n\nFinally, we would like to conduct a study to test the efficacy of a trigger using the proposed multi-objective function against a control group that received the EMAs randomly throughout the day and a comparison group that would receive EMAs based only on the predicted likelihood of a response. This study will evaluate the Trigger's impact on EMA receptivity, model performance, and its effectiveness compared to a traditional method and an alternative method that does not account for model uncertainty.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "In In conclusion, based on the outcomes presented in this work, we believe that the demonstrated feasibility and potential enhancements in data collection support incorporating the proposed multi-objective function into a clinical study. While our findings demonstrate the potential of this system, further testing is necessary to establish the statistical significance between our system and a control.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "A Adrd Study Feature Extraction",
      "text": "Fitbit: Four features are extracted from the Fitbit intraday data: heart rate, steps, sleep, and heart rate variability. To ensure the quality and reliability of the Fitbit data, we filtered out data points that fell outside our pre-defined expected ranges for steps (0-400), heart rate (40-200 beats per minute), and sleep stages. We also filtered out minutes where no data had been collected. This step was crucial because the Fitbit API may return a step count of 0, even when the device is charging or not worn. These minutes are identified by a lack of movement (0 steps) and the absence of heart rate (HR) or sleep data.\n\nAfter validation, we segment the steps and heart rate data into 5-and 30-minute windows to extract statistical features such as mean, minimum, standard deviation, and more. Using segments shorter than 5 minutes poses challenges due to the low sampling rate available for Fitbit (minute-by-minute). HRV features are derived from the R-R interval  [28]  and calculated by Fitbit. The HRV features are only obtained when participants are asleep and are relatively still while sleeping. RMSSD (root mean square of successive differences between normal heartbeats) is one such feature, reflecting parasympathetic nervous system activity. Fitbit also calculated HF (0.15 to 0.40 Hz) and LF (0.04 to 0.15 Hz), which are frequency domain features. These HRV features can be predictive of stress and emotion  [43] . When asleep immediately after going to bed and do not wake up during the night. Phone Data: The Aware app collects four key data elements: location, call logs, message logs, and screen usage  [11] . From these elements, we calculate seven features. Concerning participant privacy, we only gather metadata (e.g., timestamps of calls and messages, not phone or text conversations). We segment the data using the same segments from Fitbit (i.e., minute-by-minute and historical data). From there, we calculate many features, including messages received and sent, the length of conversations (phone and text), screen time, etc. When processing and extracting features from location, we cluster the locations and label the most frequent location as their home. We employ the k-means clustering technique, wherein the value of k is chosen for each participant through optimization of the silhouette score. However, we introduce a constraint to this selection process, stipulating that the number of clusters must exceed five. This constraint is implemented to ensure that our analysis maintains a level of granularity that prevents overgeneralization. We also clean the data by removing outliers or noisy locations that are unlikely or unavailable.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This approach balances both the likelihood of participant response",
      "page": 2
    },
    {
      "caption": "Figure 1: Framework for an EMA Trigger that would utilize the proposed multi-objective function.",
      "page": 3
    },
    {
      "caption": "Figure 2: If a person did not respond to the EMA, the entire 60 minutes after the time of",
      "page": 5
    },
    {
      "caption": "Figure 2: Receptivity Labeling Methodology for Responses (top) and Non-Responses (bottom). In the figure, ùë°1 refers to the time a",
      "page": 6
    },
    {
      "caption": "Figure 3: illustrates the distribution of responses. PA shows a wide distribution",
      "page": 8
    },
    {
      "caption": "Figure 3: Distribution of Positive Affect (PA) and Negative Affect (NA) among all participants.",
      "page": 9
    },
    {
      "caption": "Figure 4: illustrates the",
      "page": 11
    },
    {
      "caption": "Figure 4: Differences in ùêΩ(Equation 1) during responses and non-responses for each population.",
      "page": 13
    },
    {
      "caption": "Figure 5: b illustrates the same",
      "page": 13
    },
    {
      "caption": "Figure 5: ) for both studies is",
      "page": 13
    },
    {
      "caption": "Figure 5: The dashed line represents the distribution of collected positive affect (PA) labels during the day. Note that the y-axis does not",
      "page": 14
    },
    {
      "caption": "Figure 5: a and 5b). Our evaluation method provides",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ADRD Study": "Algorithm"
        },
        {
          "ADRD Study": "Random Forest"
        },
        {
          "ADRD Study": "Neural Network"
        },
        {
          "ADRD Study": "SVM"
        },
        {
          "ADRD Study": "Naive Bayes"
        },
        {
          "ADRD Study": "Baseline"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Healthy Study": "Algorithm"
        },
        {
          "Healthy Study": "Random Forest"
        },
        {
          "Healthy Study": "Neural Network"
        },
        {
          "Healthy Study": "SVM"
        },
        {
          "Healthy Study": "Naive Bayes"
        },
        {
          "Healthy Study": "Baseline"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ADRD Study": "Algorithm"
        },
        {
          "ADRD Study": "Random Forest"
        },
        {
          "ADRD Study": "Neural Network"
        },
        {
          "ADRD Study": "SVM"
        },
        {
          "ADRD Study": "Naive Bayes"
        },
        {
          "ADRD Study": "Baseline"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Healthy Study": "Algorithm"
        },
        {
          "Healthy Study": "Random Forest"
        },
        {
          "Healthy Study": "Neural Network"
        },
        {
          "Healthy Study": "SVM"
        },
        {
          "Healthy Study": "Naive Bayes"
        },
        {
          "Healthy Study": "Baseline"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ADRD Study": "Window Length"
        },
        {
          "ADRD Study": "10-Minutes"
        },
        {
          "ADRD Study": "15-Minutes"
        },
        {
          "ADRD Study": "30-Minutes"
        },
        {
          "ADRD Study": "60-Minutes"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Healthy Study": "Window Length"
        },
        {
          "Healthy Study": "10-Minutes"
        },
        {
          "Healthy Study": "15-Minutes"
        },
        {
          "Healthy Study": "30-Minutes"
        },
        {
          "Healthy Study": "60-Minutes"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Models": "Algorithm",
          "ADRD Study": "ùëÖ2\nRMSE",
          "Healthy Study": "ùëÖ2\nRMSE"
        },
        {
          "Emotion Models": "Neural Network",
          "ADRD Study": "4.47 (1.78)\n-1.4(2.06)",
          "Healthy Study": "4.32 (1.13)\n-0.90 (0.38)"
        },
        {
          "Emotion Models": "Linear Regression",
          "ADRD Study": "4.8(1.9)\n-3.3(5.0)",
          "Healthy Study": "5.30 (2.22)\n-21.12 (18.97)"
        },
        {
          "Emotion Models": "Baseline",
          "ADRD Study": "6.6(0.8)\n-3.7(1.8)",
          "Healthy Study": "6.79 (1.29)\n-1.11 (1.02)"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ADRD Study": "Window Length"
        },
        {
          "ADRD Study": "10-Minutes"
        },
        {
          "ADRD Study": "15-Minutes"
        },
        {
          "ADRD Study": "30-Minutes"
        },
        {
          "ADRD Study": "60-Minutes"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Healthy Study": "Window Length"
        },
        {
          "Healthy Study": "10-Minutes"
        },
        {
          "Healthy Study": "15-Minutes"
        },
        {
          "Healthy Study": "30-Minutes"
        },
        {
          "Healthy Study": "60-Minutes"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Signal": "Steps",
          "Features": "Mean, Median, Max, Standard De-\nviation, 75/25 percentile, maximum,\nKurtosis, and Skew",
          "Description": "Features are calculated using the total number of steps from\neach minute window across a five-minute and 30-minute\ninterval."
        },
        {
          "Signal": "Heart Rate",
          "Features": "Mean, Median, Max, Min,\nStan-\ndard Deviation,\n75/25\npercentile,\nInterquartile\nRange,\nRoot Mean\nSquare, Kurtosis, and Skew",
          "Description": "Features are calculated using the heart rate values from each\nminute window across a five-minute and 30-minute interval."
        },
        {
          "Signal": "Heart Rate\nVariability\n(HRV)",
          "Features": "Mean, Median, Max, Standard Devi-\nation, 75/25 percentile, Interquartile\nRange, and Root Mean Square",
          "Description": "Features are calculated using the heart rate variability from\nthe entire night. There are three HRV signals recorded Root\nMean Square of Successive Differences between normal\nheartbeats (RMSSD), High Frequency (HF), and Low Fre-\nquency (LF)."
        },
        {
          "Signal": "Sleep",
          "Features": "Time in Bed, Time Asleep, Sleep Ef-\nficiency, and Sleep Regularity Index",
          "Description": ""
        },
        {
          "Signal": "AWARE",
          "Features": "Number of Text Messages Received,\nNumber\nof Text Messages\nSent,\nNumber of Phone Calls, Number\nof missed calls, On-phone (Binary),\nScreen On (Locked vs. Unlocked),\nLocation (Cluster Number)",
          "Description": "AWARE data is collected periodically, log files for messages\nand calls contain the time of the event, type (received or\nmade), and for calls the end time. Geolocation is sampled\nregularly and does not necessarily gather data when partici-\npants are in transit. Screen usage logs transition points, i.e.,\nif the phone goes from locked to unlocked or vice versa."
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Signal": "Skin\nTemperature\n(ST)",
          "Features": "mean, median, mode, minimum,\nrange,\nroot mean square,\nzero\ncross, Kurtosis,\nskew,\nIQR 25th\nand 75th Percentile",
          "Description": "Zero-cross here is based on the number of times ST crosses\nover the mean ST. Kurtosis measures the extremity of the\ndata in the segment and skew is the measure of asymme-\ntry."
        },
        {
          "Signal": "Electrocardiogram\n(ECG)",
          "Features": "mean, median, mode, minimum,\nrange,\nroot mean square,\nzero\ncross Kurtosis, skew, IQR 25th and\n75th\nPercentile RMSSD, CVSD,\nCVNNI\nSDNN, NNI50, NNI20,\nPNNI50, PNNI50, low frequency\n(lf),\nvery\nlow frequency\n(vlf),\nhigh frequency (hf), high/low fre-\nquency ratio (hf/lf)",
          "Description": "NN (N-N or R-R interval) indicates the time between heart-\nbeats. NNI20/50 refers to the number of successive inter-\nvals that differ by more than 20 or 50 ms. P indicates\nthe proportion of NNI20/50 in the segment. RMSSD is\nthe root mean square of successive differences between\nheartbeats. CVNNI and CVSD are the coefficients of vari-\nation (sdnn/mean) and (rmssd/mean), respectively. Our\nfrequency domain features are based on how much of the\nsignal lies between 0.003 to 0.04 Hz (vlf), 0.04 to 0.15 Hz\n(lf), 0.15 to 0.40 Hz (hf)"
        },
        {
          "Signal": "Electrodermal Activ-\nity (EDA)",
          "Features": "Wavelet: max, mean, std, median,\nabove\nzero (1\nsecond and half\nsecond wavelet) Raw: amplitude,\nmax, min, mean Filtered: ampli-\ntude, max, min, average",
          "Description": "A 1-second and a half-second window were used for\nwavelet features. Features were calculated for both the\nfirst and second derivatives of each window size."
        }
      ],
      "page": 23
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Md Abdus Samad Kamal, and Tetsuya Shimamura. 2021. Facial emotion recognition using transfer learning in the deep CNN",
      "authors": [
        "Shuvendu Mah Akhand",
        "Nazmul Roy",
        "Siddique"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "2",
      "title": "Alzheimer's disease facts and figures",
      "year": "2019",
      "venue": "Alzheimer's & dementia"
    },
    {
      "citation_id": "3",
      "title": "Automated detection of stressful conversations using wearable physiological and inertial sensors",
      "authors": [
        "Rummana Bari",
        "Md Mahbubur Rahman",
        "Nazir Saleheen",
        "Megan Parsons",
        "Eugene Buder",
        "Santosh Kumar"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies"
    },
    {
      "citation_id": "4",
      "title": "Everything is perfect, and we have no problems\": detecting and limiting social desirability bias in qualitative research",
      "authors": [
        "Nicole Bergen",
        "Ronald Labont√©"
      ],
      "year": "2020",
      "venue": "Qualitative health research"
    },
    {
      "citation_id": "5",
      "title": "To prompt or not to prompt? A microrandomized trial of time-varying push notifications to increase proximal engagement with a mobile health app",
      "authors": [
        "Niranjan Bidargaddi",
        "Daniel Almirall",
        "Susan Murphy",
        "Inbal Nahum-Shani",
        "Michael Kovalcik",
        "Timothy Pituch",
        "Haitham Maaieh",
        "Victor Strecher"
      ],
      "year": "2018",
      "venue": "JMIR mHealth and uHealth"
    },
    {
      "citation_id": "6",
      "title": "BioSPPy: Biosignal Processing in Python",
      "authors": [
        "Carlos Carreiras",
        "Ana Alves",
        "Andr√© Louren√ßo",
        "Filipe Canento",
        "Hugo Silva",
        "Ana Fred"
      ],
      "year": "2015",
      "venue": "BioSPPy: Biosignal Processing in Python"
    },
    {
      "citation_id": "7",
      "title": "Are you killing time? Predicting smartphone users' time-killing moments via fusion of smartphone sensor data and screenshots",
      "authors": [
        "Yu-Chun Chen",
        "Yu-Jen Lee",
        "Kuei-Chun Kao",
        "Jie Tsai",
        "En-Chi Liang",
        "Wei-Chen Chiu",
        "Faye Shih",
        "Yung-Ju Chang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "8",
      "title": "Investigating adverse daily life effects following a psychosocial laboratory stress task, and the moderating role of Psychopathology",
      "authors": [
        "Joana De",
        "Calheiros Velozo",
        "Thomas Vaessen",
        "Stephan Claes",
        "Inez Myin-Germeys"
      ],
      "year": "2024",
      "venue": "StreSS"
    },
    {
      "citation_id": "9",
      "title": "Is daily-life stress reactivity a measure of stress recovery? An investigation of laboratory and daily-life stress",
      "authors": [
        "Joana De",
        "Calheiros Velozo",
        "Thomas Vaessen",
        "Ginette Lafit",
        "Stephan Claes",
        "Inez Myin-Germeys"
      ],
      "year": "2023",
      "venue": "Stress and Health"
    },
    {
      "citation_id": "10",
      "title": "Inez Myin-Germeys, and Wolfgang Viechtbauer. 2020. The effects of sampling frequency and questionnaire length on perceived burden, compliance, and careless responding in experience sampling data in a student population",
      "authors": [
        "Gudrun Eisele",
        "Hugo Vachon",
        "Ginette Lafit",
        "Peter Kuppens",
        "Marlies Houben"
      ],
      "year": "2020",
      "venue": "Assessment"
    },
    {
      "citation_id": "11",
      "title": "AWARE: mobile context instrumentation framework",
      "authors": [
        "Denzil Ferreira",
        "Vassilis Kostakos",
        "Anind Dey"
      ],
      "year": "2015",
      "venue": "Frontiers in ICT"
    },
    {
      "citation_id": "12",
      "title": "Predicting exact valence and arousal values from EEG",
      "authors": [
        "Filipe Galv√£o",
        "M Soraia",
        "Manuel Alarc√£o",
        "Fonseca"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "13",
      "title": "An emotion recognition system based on physiological signals obtained by wearable sensors",
      "authors": [
        "Cheng He",
        "Yun-Jin Yao",
        "Xue-Song Ye"
      ],
      "year": "2017",
      "venue": "Wearable sensors and robots"
    },
    {
      "citation_id": "14",
      "title": "Using context-aware computing to reduce the perceived burden of interruptions from mobile devices",
      "authors": [
        "Joyce Ho",
        "Stephen Intille"
      ],
      "year": "2005",
      "venue": "Proceedings of the SIGCHI conference on Human factors in computing systems"
    },
    {
      "citation_id": "15",
      "title": "Sensor Motion",
      "authors": [
        "Simon Ho"
      ],
      "year": "2018",
      "venue": "Sensor Motion"
    },
    {
      "citation_id": "16",
      "title": "cStress: towards a gold standard for continuous stress assessment in the mobile environment",
      "authors": [
        "Karen Hovsepian",
        "Mustafa Al'absi",
        "Emre Ertin",
        "Thomas Kamarck",
        "Motohiro Nakajima",
        "Santosh Kumar"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing"
    },
    {
      "citation_id": "17",
      "title": "SCAI-SVSC: Smart clothing for effective interaction with a sustainable vital sign collection",
      "authors": [
        "Long Hu",
        "Jun Yang",
        "Min Chen",
        "Yongfeng Qian",
        "Joel Jpc Rodrigues"
      ],
      "year": "2018",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "18",
      "title": "Stressnas: Affect state and stress detection using neural architecture search",
      "authors": [
        "Lam Huynh",
        "Tri Nguyen",
        "Thu Nguyen",
        "Susanna Pirttikangas",
        "Pekka Siirtola"
      ],
      "year": "2021",
      "venue": "Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "19",
      "title": "ùúáEMA: Microinteraction-based ecological momentary assessment (EMA) using a smartwatch",
      "authors": [
        "Stephen Intille",
        "Caitlin Haynes",
        "Dharam Maniar",
        "Aditya Ponnada",
        "Justin Manjourides"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "20",
      "title": "mHealth technology to assess, monitor and treat daily functioning difficulties in people with severe mental illness: A systematic review",
      "authors": [
        "Leila Jameel",
        "Lucia Valmaggia",
        "Georgina Barnes",
        "Matteo Cella"
      ],
      "year": "2022",
      "venue": "Journal of psychiatric research"
    },
    {
      "citation_id": "21",
      "title": "Compliance with ecological momentary assessment protocols in substance users: a meta-analysis",
      "authors": [
        "Andrew Jones",
        "Danielle Remmerswaal",
        "Ilse Verveer",
        "Eric Robinson",
        "Ingmar Ha Franken",
        "Matt Cheng K Fred Wen",
        "Field"
      ],
      "year": "2019",
      "venue": "Addiction"
    },
    {
      "citation_id": "22",
      "title": "Investigating Receptivity and Affect Using Machine Learning: Ecological Momentary Assessment and Wearable Sensing Study",
      "authors": [
        "Zachary D King",
        "Han Yu",
        "Thomas Vaessen",
        "Inez Myin-Germeys",
        "Akane Sano"
      ],
      "year": "2024",
      "venue": "JMIR mHealth and uHealth"
    },
    {
      "citation_id": "23",
      "title": "Exploring the state-of-receptivity for mhealth interventions",
      "authors": [
        "Florian K√ºnzler",
        "Varun Mishra",
        "Jan-Niklas Kramer",
        "David Kotz",
        "Elgar Fleisch",
        "Tobias Kowatsch"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "24",
      "title": "Cost-sensitive active learning for intracranial hemorrhage detection",
      "authors": [
        "Weicheng Kuo",
        "Christian H√§ne",
        "Esther Yuh",
        "Pratik Mukherjee",
        "Jitendra Malik"
      ],
      "year": "2018",
      "venue": "Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference"
    },
    {
      "citation_id": "25",
      "title": "Ask Less, Learn More: Adapting Ecological Momentary Assessment Survey Length by Modeling Question-Answer Information Gain",
      "authors": [
        "Jixin Li",
        "Aditya Ponnada",
        "Wei-Lin Wang",
        "Genevieve Dunton",
        "Stephen Intille"
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies"
    },
    {
      "citation_id": "26",
      "title": "mHealth for mental health: Integrating smartphone technology in behavioral healthcare",
      "authors": [
        "Russell David D Luxton",
        "Nigel Mccann",
        "Matthew Bush",
        "Greg Mishkind",
        "Reger"
      ],
      "year": "2011",
      "venue": "mHealth for mental health: Integrating smartphone technology in behavioral healthcare"
    },
    {
      "citation_id": "27",
      "title": "A short form of the Positive and Negative Affect Schedule: Evaluation of factorial validity and invariance across demographic variables in a community sample",
      "authors": [
        "Andrew Mackinnon",
        "Anthony Jorm",
        "Helen Christensen",
        "Patricia Ailsa E Korten",
        "Bryan Jacomb",
        "Rodgers"
      ],
      "year": "1999",
      "venue": "Personality and Individual differences"
    },
    {
      "citation_id": "28",
      "title": "Components of heart rate variability-what they really mean and what we really measure",
      "authors": [
        "Marek Malik",
        "Camm John"
      ],
      "year": "1993",
      "venue": "The American journal of cardiology"
    },
    {
      "citation_id": "29",
      "title": "Designing content-driven intelligent notification mechanisms for mobile applications",
      "authors": [
        "Abhinav Mehrotra",
        "Mirco Musolesi",
        "Robert Hendley",
        "Veljko Pejovic"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "30",
      "title": "Detecting Receptivity for mHealth Interventions in the Natural Environment",
      "authors": [
        "Varun Mishra",
        "Florian K√ºnzler",
        "Jan-Niklas Kramer",
        "Elgar Fleisch",
        "Tobias Kowatsch",
        "David Kotz"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "31",
      "title": "Investigating contextual cues as indicators for EMA delivery",
      "authors": [
        "Varun Mishra",
        "Byron Lowens",
        "Sarah Lord",
        "Kelly Caine",
        "David Kotz"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "32",
      "title": "Valence and arousal estimation in-the-wild with tensor methods",
      "authors": [
        "Anna Mitenkova",
        "Jean Kossaifi",
        "Yannis Panagakis",
        "Maja Pantic"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "33",
      "title": "The effect of timing and frequency of push notifications on usage of a smartphone-based stress management intervention: an exploratory trial",
      "authors": [
        "Charlie Leanne G Morrison",
        "Veljko Hargood",
        "Adam Pejovic",
        "Scott Geraghty",
        "Natalie Lloyd",
        "Goodman",
        "T Danius",
        "Anna Michaelides",
        "Mirco Weston",
        "Mark Musolesi",
        "Weal"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "34",
      "title": "Prompt-level predictors of compliance in an ecological momentary assessment study of young adults' mental health",
      "authors": [
        "Aja Louise Murray",
        "Ruth Brown",
        "Xinxin Zhu",
        "Gabriela Speyer",
        "Yi Yang",
        "Zhouni Xiao",
        "Denis Ribeaud",
        "Manuel Eisner"
      ],
      "year": "2023",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "35",
      "title": "Emotional reactivity to daily life stress in psychosis",
      "authors": [
        "Inez Myin-Germeys",
        "Jim Van Os",
        "Joseph Schwartz",
        "Arthur Stone",
        "Philippe Delespaul"
      ],
      "year": "2001",
      "venue": "Archives of general psychiatry"
    },
    {
      "citation_id": "36",
      "title": "Transformers for prompt-level EMA non-response prediction",
      "authors": [
        "Supriya Nagesh",
        "Alexander Moreno",
        "Stephanie Carpenter",
        "Jamie Yap",
        "Soujanya Chatterjee",
        "Steven Lloyd Lizotte",
        "Neng Wan",
        "Santosh Kumar",
        "Cho Lam",
        "David Wetter"
      ],
      "year": "2021",
      "venue": "Transformers for prompt-level EMA non-response prediction",
      "arxiv": "arXiv:2111.01193"
    },
    {
      "citation_id": "37",
      "title": "Analysis and use of the emotional context with wearable devices for games and intelligent assistants",
      "authors": [
        "J Grzegorz",
        "Krzysztof Nalepa",
        "Barbara Kutt",
        "Pawe≈Ç Gi≈ºycka",
        "Szymon Jemio≈Ço",
        "Bobek"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "38",
      "title": "Compliance in ambulatory assessment studies: Investigating study and sample characteristics as predictors",
      "authors": [
        "Charlotte Ottenstein",
        "Linda Werner"
      ],
      "year": "2022",
      "venue": "Compliance in ambulatory assessment studies: Investigating study and sample characteristics as predictors"
    },
    {
      "citation_id": "39",
      "title": "InterruptMe: designing intelligent prompting mechanisms for pervasive applications",
      "authors": [
        "Veljko Pejovic",
        "Mirco Musolesi"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "40",
      "title": "Irregular sleep/wake patterns are associated with poorer academic performance and delayed circadian and sleep/wake timing",
      "authors": [
        "Andrew Jk Phillips",
        "William Clerx",
        "S Conor",
        "Akane O'brien",
        "Laura Sano",
        "Rosalind Barger",
        "Steven Picard",
        "Elizabeth Lockley",
        "Charles Klerman",
        "Czeisler"
      ],
      "year": "2017",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "41",
      "title": "HRVanalysis: a free software for analyzing cardiac autonomic activity",
      "authors": [
        "Vincent Pichot",
        "Fr√©d√©ric Roche",
        "S√©bastien Celle",
        "Jean-Claude Barth√©l√©my",
        "Florian Chouchou"
      ],
      "year": "2016",
      "venue": "Frontiers in physiology"
    },
    {
      "citation_id": "42",
      "title": "Beyond interruptibility: Predicting opportune moments to engage mobile phone users",
      "authors": [
        "Martin Pielot",
        "Bruno Cardoso",
        "Kleomenis Katevas",
        "Joan Serr√†",
        "Aleksandar Matic",
        "Nuria Oliver"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "43",
      "title": "Heart rate variability is associated with emotion recognition: Direct evidence for a relationship between the autonomic nervous system and social cognition",
      "authors": [
        "Adam Daniel S Quintana",
        "Tim Guastella",
        "Ian Outhred",
        "Andrew Hickie",
        "Kemp"
      ],
      "year": "2012",
      "venue": "International journal of psychophysiology"
    },
    {
      "citation_id": "44",
      "title": "Predicting subjective measures of social anxiety from sparsely collected mobile sensor data",
      "authors": [
        "Haroon Rashid",
        "Sanjana Mendu",
        "Katharine Daniel",
        "Miranda Beltzer",
        "Bethany Teachman",
        "Mehdi Boukhechba",
        "Laura Barnes"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "45",
      "title": "Momentary predictors of compliance in studies using the experience sampling method",
      "authors": [
        "Aki Rintala",
        "Martien Wampers",
        "Inez Myin-Germeys",
        "Wolfgang Viechtbauer"
      ],
      "year": "2020",
      "venue": "Psychiatry research"
    },
    {
      "citation_id": "46",
      "title": "What is the clinical value of mHealth for patients?",
      "authors": [
        "Edward Simon P Rowland",
        "Thomas Fitzgerald",
        "John Holme",
        "Alison Powell",
        "Mcgregor"
      ],
      "year": "2020",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "47",
      "title": "Stress recognition using wearable sensors and mobile phones",
      "authors": [
        "Akane Sano",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "2013 Humaine association conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "48",
      "title": "Assessing the availability of users to engage in just-in-time intervention in the natural environment",
      "authors": [
        "Hillol Sarker",
        "Moushumi Sharmin",
        "Ahsan Amin",
        "Md Ali",
        "Rummana Mahbubur Rahman",
        "Syed Bari",
        "Santosh Monowar Hossain",
        "Kumar"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing"
    },
    {
      "citation_id": "49",
      "title": "Strategies for analyzing ecological momentary assessment data",
      "authors": [
        "E Joseph",
        "Arthur Schwartz",
        "Stone"
      ],
      "year": "1998",
      "venue": "Health Psychology"
    },
    {
      "citation_id": "50",
      "title": "Automatic identification of artifacts in electrodermal activity data",
      "authors": [
        "Sara Taylor",
        "Natasha Jaques",
        "Weixuan Chen",
        "Szymon Fedor",
        "Akane Sano",
        "Rosalind Picard"
      ],
      "year": "2015",
      "venue": "2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "51",
      "title": "What do patients value as incentives for participation in clinical trials? A pilot discrete choice experiment",
      "authors": [
        "Akke Vellinga",
        "Colum Devine",
        "Min Ho",
        "Colin Clarke",
        "Patrick Leahy",
        "Jane Bourke",
        "Declan Devane",
        "Sinead Duane",
        "Patricia Kearney"
      ],
      "year": "2020",
      "venue": "Research Ethics"
    },
    {
      "citation_id": "52",
      "title": "Compliance with mobile ecological momentary assessment protocols in children and adolescents: a systematic review and meta-analysis",
      "authors": [
        "Fred Cheng",
        "Stefan Wen",
        "Arthur Schneider",
        "Donna Stone",
        "Spruijt-Metz"
      ],
      "year": "2017",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "53",
      "title": "Ecological momentary assessment: A meta-analysis on designs, samples, and compliance across research fields",
      "authors": [
        "Cornelia Wrzus",
        "Andreas Neubauer"
      ],
      "year": "2023",
      "venue": "Ecological momentary assessment: A meta-analysis on designs, samples, and compliance across research fields"
    },
    {
      "citation_id": "54",
      "title": "Semi-Supervised Learning for Wearable-based Momentary Stress Detection in the Wild",
      "authors": [
        "Han Yu",
        "Akane Sano"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "55",
      "title": "EmotionSense: Emotion recognition based on wearable wristband",
      "authors": [
        "Bobo Zhao",
        "Zhu Wang",
        "Zhiwen Yu",
        "Bin Guo"
      ],
      "year": "2018",
      "venue": "Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation"
    }
  ]
}