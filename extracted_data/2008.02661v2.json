{
  "paper_id": "2008.02661v2",
  "title": "Dynamic Emotion Modeling With Learnable Graphs And Graph Inception Network",
  "published": "2020-08-06T13:51:31Z",
  "authors": [
    "A. Shirian",
    "S. Tripathi",
    "T. Guha"
  ],
  "keywords": [
    "Graph learning",
    "graph neural network",
    "inception network",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotion is expressed, perceived and captured using a variety of dynamic data modalities, such as speech (verbal), videos (facial expressions) and motion sensors (body gestures). We propose a generalized approach to emotion recognition that can adapt across modalities by modeling dynamic data as structured graphs. The motivation behind the graph approach is to build compact models without compromising on performance. To alleviate the problem of optimal graph construction, we cast this as a joint graph learning and classification task. To this end, we present the Learnable Graph Inception Network (L-GrIN) that jointly learns to recognize emotion and to identify the underlying graph structure in the dynamic data. Our architecture comprises multiple novel components: a new graph convolution operation, a graph inception layer, learnable adjacency, and a learnable pooling function that yields a graph-level embedding. We evaluate the proposed architecture on five benchmark emotion recognition databases spanning three different modalities (video, audio, motion capture), where each database captures one of the following emotional cues: facial expressions, speech and body gestures. We achieve state-of-the-art performance on all five databases outperforming several competitive baselines and relevant existing methods. Our graph architecture shows superior performance with significantly fewer parameters (compared to convolutional or recurrent neural networks) promising its applicability to resource-constrained devices. Our code is available at /github.com/AmirSh15/graph_emotion_recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Human emotion is expressed, perceived and captured using a variety of dynamic data modalities, such as speech (verbal), videos (facial expressions) and motion capture (body gestures). Modeling and analysis of these cues are critical for many human-centric systems with applications ranging from driver's safety to mental healthcare to human-robot conversational systems. In recent years, significant progress has been made towards the recognition and analysis of emotion using dynamic facial expressions  [1] ,  [2] , speech  [3] ,  [4]  and body gestures  [5] . Since human emotion is inherently multimodal, research efforts that combine information from multiple modalities are also on the rise  [6] . Besides expressed emotion, work has also been done to analyze emotion evoked by natural images  [7] , videos  [8]  and music  [9] . Fig.  1 . A generalized graph approach to modeling emotion dynamics. Data samples are transformed to a learnable graph structure, where each node corresponds to a short temporal segment or frame. A novel graph architecture (L-GRIN) produces an embedding for the entire graph facilitating emotion recognition.\n\nIn the literature of dynamic emotion recognition, recurrent models, such as Long Short Term Memory networks (LSTM) are common  [4] ,  [10] . These networks often have complex architecture with millions of trainable parameters requiring large amounts of training data. This makes many emotion recognition models incompatible for use in resource-constrained devices. A compact, efficient and scalable way to represent data is in the form of graphs. We thus adopt a graph approach to building a compact model for dynamic emotion recognition. Furthermore, existing emotion recognition models assume a prior knowledge of the input modality. Since emotion can be sensed through a variety of modalities, a generalized model that can handle disparate modalities efficiently is important. We show that our modality-agnostic graph approach is able to achieve state-of-the-art accuracy across various modalities with significantly fewer trainable parameters.\n\nTraditionally, sequences are modeled using Recurrent Neural Networks (RNNs). However, recent literature has successfully used the idea of defining a sequence over a graph  [11] ,  [12] ,  [13] . Considering a video frame sequence as a 'structured' graph,  Mao et al.  showed that graph models can outperform RNNs  [11] . Motivated by these recent successes and in the pursuit of a compact model, we propose to adopt a graph approach to model emotion dynamics. Subsequently, we cast emotion recognition as a joint graph learning and classification problem (see Fig.  1  for an overview). In our approach, each dynamic data sample is represented as a graph, where each node corresponds to a short temporal segment in the data. Each node is associated with the features extracted from the short temporal segment (frame) as its node attributes. This frame-to-node graph construction approach focuses on modeling the temporal dynamics in data; note that spatiotemporal structure (e.g., facial keypoints structure) within the graph resists the idea of a generic, modality-agnostic model and also increases model size significantly. Our graph structure (and hence the model) does not change with the choice of modality or node attributes. Modeling as a graph offers compactness and convenience to handle missing data (particularly common in mocap).\n\nThe graph structure i.e., the edge weights connecting the nodes is not naturally defined here. When a graph structure is not known apriori, a common practice is to manually construct the graph. This, however, results into sub-optimal graphs. We thus propose to learn the graph structure itself during the training stage. This is a generalized formulation, where the temporal dependencies between the nodes are automatically discovered. The only assumption we make is that the graph structure remains the same for all samples in a given database. To this end, we propose a novel Graph Convolution Network (GCN) architecture, the Learnable Graph Inception Network (L-GrIN), with several novel components: a new definition of graph convolution that uses a non-linear layer-wise projection technique, introduction of an inception module in graph domain, learnable graph structure and a learnable graph-to-vector pooling function. Our architecture produces superior results on five benchmark emotion recognition databases spanning three different modalities (video, audio, mocap). Each database captures one of the following emotional cues: facial expressions, speech and body gestures. In summary, the main contributions of this paper are as follows:\n\n• A generalized, modality-agnostic graph approach to classify dynamic signals that combines graph learning with graph classification. • A novel graph architecture, termed L-GrIN, with a new graph convolution layer, a graph inception module, learnable graph structure and learnable graph-to-vector pooling.\n\n• State-of-the-art performance on dynamic emotion recognition tasks spanning three sensory modalities (video, audio, motion sensors) on five benchmark databases.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we review the related work in the areas of GCNs and emotion recognition using various modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Graph Neural Network.",
      "text": "Deep learning on graph data has emerged as a major topic in the past few years. This is because graphs provide a natural and convenient way to deal with large data. Among the different graph neural networks, GCNs have received the most attention  [14] ,  [15] ,  [12] . GCNs have been successfully applied to various image and video-based tasks, such as face clustering  [16] , object detection  [17] , and video representation learning  [11] . GCNs have been used to address skeleton-based action recognition recorded using motion capture  [13] . The application of graph networks has also started emerging in automatic speech recognition  [18] .\n\nGCNs can be broadly classified into two categories: spatial and spectral. Spatial GCNs imitate the convolution operation of the Convolutional Neural Networks (CNN) by aggregating the information from neighboring nodes  [14] ,  [19] . The problem of different graph nodes having different number of neighbours is usually addressed by using a fixed size neighborhood  [19]  or by converting graph structures to a regular grid and subsequently applying traditional CNNs  [20] . A recent work proposed to develop the graph structure considering the Weisfeiler-Lehman graph isomorphism test  [21] , and achieved state-of-the-art performance in node classification task in citation networks. On the other hand, spectral GCNs formulate the convolution operation as a frequency domain filtering operation following the theory of signal processing  [22] , where convolution filters are seen as a set of learnable parameters. The ChebNet  [23]  is proposed to reduce the computational cost of spectral GCNs that redefined the convolution filter in terms of Chebyshev polynomials bypassing the need for eigen decomposition of the graph Laplacian. In a follow-up work  [15] , a first order approximation of the Chebyshev polynomials was introduced. This further simplified the spectral GCN computation as the convolution operation reduces to a linear projection.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Emotion Recognition.",
      "text": "Facial emotion recognition.: Recognizing facial expressions is the most common way of analyzing emotion. The majority of work rely on identifying an individual's facial expression from images or videos (fewer work on videos), and associating them to one of the basic emotion classes. Recent efforts in image-based recognition are focused on using CNNs and its variants  [24] ,  [25] , and on using adversarial learning  [26] . A few works have proposed to use attention networks to account for the context  [27] ,  [28] ,  [29] . RNNs and 3D CNNs have been used for video-based emotion recognition due to their ability to capture the temporal information  [30] ,  [31] . Another line of work focuses on the dynamics of landmark points in faces extracted from videos. In this context, a deep temporal appearance geometry network has been proposed  [32]  that uses the landmark point geometry and a CNN for emotion recognition. Another recent work constructed a trajectory matrix from the landmark points and used them as inputs to a CNN  [33] .\n\nSpeech emotion recognition: Speech emotion recognition, especially using categorical labels, has been studied widely in the past years. Many speech emotion recognition systems still rely on low-level acoustic, prosodic and lexical features, that are then fed to deep models for classification. Other approaches use spectrograms (usually used as inputs to CNN models)  [34]  and even raw speech  [35] . Recurrent models are prevalent due to their ability to capture the temporal dynamics of emotion  [36] ,  [35] . A 3D RNN model has been recently proposed for end-to-end modeling  [37] . Attentionbased techniques have been widely explored  [36] ,  [38] ,  [39] , while transformer-based architectures are gaining momentum in this field  [40] . Body emotion recognition.: Body expressions are relatively less studied in emotion recognition. The existing literature is focused on using motion information in terms of lowlevel descriptors, such as joint angles, 3D positions, distance between joints, velocity and acceleration  [5] ,  [41] ,  [42] . A trajectory learning approach  [5]  proposed to identify 'neutral' motion from input data, and used the deviation of a given input from the neutral motion as a feature for classifying emotions. Another recent work combined deep features with psychological attributes to detect emotion from 3D body pose using a random Forest classifier  [41] . Gait information has also been considered for recognizing emotion, where a spatial GCN is used to detect the emotional state  [42] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Approach",
      "text": "In this section, we describe our deep graph approach to emotion recognition. First, we construct a graph from dynamic input data following a generalized frame-to-node approach. Next, we propose a novel architecture that jointly performs graph learning and graph classification. This is achieved by optimizing over a new loss function that combines classification loss and a graph structure loss. The proposed architecture, L-GRIN, is illustrated in Fig.  2 . Below, we describe each component of this network in detail.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Graph Construction",
      "text": "Given a dynamic input sequence, our first task is to construct an undirected graph G = (V, E) that can efficiently capture the emotion-related dynamics in the data, where V is the set of nodes with cardinality |V| = M and E is the set of all edges between the connected nodes. A representative description of G is typically given by an adjacency matrix A ∈ R M ×M which is symmetric for an undirected graph.\n\nOur graph construction approach follows a frame-to-node transformation, where M frames in the data form the M graph nodes {v i } M i=1 ∈ V (see Fig.  3 ). A frame refers to a small temporal segment of the data, e.g., an audio segment of length 40ms. In order to encode the temporal information, a frame (node) should be connected with weights to a series of past and future nodes. An element (A) ij ∈ A contains the weight corresponding to the edge e ij ∈ E connecting v i and v j . Note that the graph structure is not naturally defined here, i.e., the elements in A are unknown. A common way to define the elements in A is through constructing a distance function manually  [13] . However, this may result into a suboptimal graph representation. Hence, we propose to learn the elements in A by jointly optimizing a structural loss combined with a classification loss. This loss function will be discussed in Section III-B.\n\nIn order to capture the emotion content at node level, we rely on modality-specific features or even, raw data. Each node v i is thus associated with a node feature vector n i ∈ R P . A feature matrix N ∈ R M ×P consisting all the node feature vectors is defined as\n\nFeatures for individual modalities is discussed in Section IV.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Learnable Graph Inception Network",
      "text": "Given a set of (dynamic inputs transformed to) graphs {G 1 , ..., G N } and their true labels {y 1 , ..., y N }, our task is to develop a deep graph architecture that is able to recognize the emotional content in the data. Since the graph structure is not naturally defined here, we also learn an optimal A from the training data with the underlying assumption that each graph has different node features but the same edge weights. We formulate this as a joint graph learning and graph classification problem.\n\nA common GCN architecture takes the node feature matrix N ∈ R M ×P and the graph adjacency matrix A as inputs and produces a node-level representation matrix Z ∈ R M ×Q , where Q is the dimension of the output feature vector at each node. A GCN layer H (k+1) can be defined as a non-linear function of its previous layer as follows\n\nwhere W (k) is the weight matrix for the k th layer of the neural network, σ is a non-linear activation function, such as a ReLU, and k is the layer number (k = 0, • • • K). Note that\n\nAn effective improvement on this propagation rule has been recently proposed  [15] .\n\nwhere D is the degree matrix of A, and I is an M × M identity matrix. Note that the terms within the parenthesis in Eq. (  2 ) is simply a linear projection, and can be re-written as\n\nwhere\n\n. We present a new GCN architecture, called L-GrIN (see Fig.  2 ), for joint graph learning and classification. It has the following four new components:\n\n• Non-linear spectral graph convolution (G * conv). Motivated by a recent work on spatial graph neural network  [43] , we replace the linear projection in (3) by a multi-layer perceptron (MLP) layer, and replace Â by a learnable A. Thus, instead of the linear layer in (3), we define a new spectral graph convolution layer G * (•) as follows:\n\nwhere MLP(.) has two hidden layers with η neurons each, A is the learnable adjacency matrix and σ is a nonlinear activation function. A is learned through a joint optimization process described later in this section. The ReLU(•) in Eq. (  4 ) ensures the non-negativity of A. We refer to the convolution operation defined above as G * conv in the rest of the paper.\n\n• Graph inception. We extend the idea of inception layer in traditional CNNs  [44]  to the graph domain, and introduce a graph inception module in our architecture (see Fig.  2 ). Our graph inception layer consists of two graph convolution layers and one maxpool layer operating on directly connected (1-hop) neighbours only.\n\nGiven an input H (k) , the proposed graph inception layer is defined as follows:\n\nwhere | denotes concatenation of the node features, and G * 1 and G * 2 are two G * conv layers (see Eq. (  4 )) with different size of their MLP layers (η = 128 for G * 1 and η = 64 for G * 2 ). Hence, for an input of H (k) ∈ R M ×P , the inception layer produces an output\n\nThe motivation behind the inception layer is to be able to capture the emotion dynamics at multiple temporal scales. The two G * conv layers that yield embeddings of different dimensions can be interpreted as a multiscale analysis on graphs in spectral domain. Like a traditional inception layer in CNN, our graph inception layer also combines features from multiple scales allowing the network to have both width and depth. Our graph inception layer has fewer parameters (compared to inception networks in CNNs) enabling us to feed the input directly to the inception layer.\n\nThe maxpool function in Eq. (  5 ) operates on every node separately. For each node v i , we only consider its directly connected neighbors (1-hop), and maxpool over the embeddings along feature dimension. Note that as we start with a  3 . Graph construction: Given a dynamic input sequence of M segments, a fully-connected graph with M nodes is constructed without making any assumption. The edge weights are learned during the training phase. Each node is associated with a node attribute vector n i . fully-connected graph, initially this operation is the same as maxpooling over all nodes, but this changes quickly as we start learning the graph structure.\n\n• Learnable pooling for graph-level representation. Our objective is to classify entire graphs, as opposed to the more common task of classifying each node. Hence, we seek a graph-level representation h G ∈ R Q as the output of our network. This can be obtained by pooling the node-level representations H (k) at the K-th layer before passing them to the classification layer (see Fig.  2 ). Common choices for pooling functions in graph domain are mean, max and sum pooling. Max and mean pooling often can not preserve the underlying information about the graph structure while sum pooling is shown to be a better alternative  [43] . However, all these pooling functions treat every neighboring node with equal importance, which may not be optimal. To this end, we propose to learn a pooling function Ψ that combines the node embeddings from the K-th layer to produce an embedding for the entire graph. Additionally, we also use maxpool and meanpool and combine all the graph-level embeddings together. The pooling layer is thus defined as follows:\n\nwhere p has learnable weights to combine node-level embeddings to obtain a graph-level embedding.\n\n• Learnable adjacency (A). Recall that in our task the graph structure is not known. Although we can define such structure manually, results are sub-optimal. An effective approach would be to learn the graph structure (adjacency matrix) itself by jointly optimizing over a classification loss and graph learning loss. We assume that all videos have the same underlying graph structure containing the same number of nodes and edges. This largely simplifies our task of graph structure learning. The overall loss L for joint graph learning and classification is composed of two components: (i) L GC : a graph classification loss, and (ii) L GL : a graph learning loss. The classification loss L GC is defined as the cross-entropy loss:\n\nwhere ŷn is the predicted label for the n th sample. The graph learning loss, L GL , is designed to facilitate learning the pooling vector p and the adjacency matrix A. This is defined as follows:\n\nwhere denotes element-wise multiplication, e is an all-ones vector, • F denotes Frobenious norm, λ 1 , λ 2 , and λ 3 control the relative weights of the three terms, and A d is a structure matrix defined as follows:\n\nThe structure matrix A d forces the nodes that are temporally close to each other to have stronger relationship, i.e. higher weights in the A. The larger the squared distance between two nodes v i and v j (frames), the smaller will be the weights in (A) ij . The ReLU operation (see Eq. (  4 )) ensures the nonnegativity of the elements in A. The overall optimization is thus as follows: min\n\nwhere, Θ denotes all other learnable network parameters across all graph convolution layers including its constituent MLP layers. Every term in the overall loss function L is differentiable, thereby allowing an end-to-end optimization.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments",
      "text": "We now present extensive experimental results and analysis to evaluate the performance of the proposed architecture for facial, speech and body emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Facial Emotion Recognition",
      "text": "Video databases: We use three large video emotion recognition databases for our experiments. The databases are chosen based on their popularity in emotion recognition literature. The RML database  [45]  contains 720 videos of 6 basic emotions: anger, disgust, fear, joy, sadness, surprise collected when the subjects speak. The subjects are from various ethnic groups and speak different languages. The eNTERFACE  [46]  is contains 1170 videos of 42 subjects with six basic emotion classes as RML. These emotions are the reactions after listening to six different short stories, where each person reads out 5 phrases based on their emotional reaction. The RAVDESS database  [47]  contains 4904 videos labeled with 8 classes: anger, calmness, disgust, fear, joy, neutral, sadness and surprise. This is the largest video emotion database currently available.  Node features: The databases we use provide only raw video clips. We choose to use facial landmark points extracted from the video frames as node attributes. This is because landmark points are known to effectively capture the facial dynamics  [52] . We extract 68 landmark points at every video frame using a state-of-the-art landmark detection method  [53] , resulting into node feature vectors of dimension P = 136.\n\nImplementation details: We use a 10-fold cross-validation for all three databases, and report the average recognition accuracy in Table  I . We fix the length of each input video to 90 frames yielding a graph with M = 90 nodes. The shorter videos are simply padded by duplicating frames from the beginning of the video (cyclic padding). Our network weights are initialized following the Xavier initialization. We set λ 1 = λ 2 = 0.1 and λ 3 = 1 × 10 -4 (see Eq. 8). We used Adam optimizer with a learning rate of 0.01 and decay rate of 0.5 after each 50 epochs for all experiments. To initialize the learnable adjacency matrix A, we generate a random matrix whose elements are drawn from a Normal distribution with zero mean and unit variance. We used Pytorch for implementing our model and the baselines, and an NVIDIA RTX-2080Ti GPU for all experiments.\n\nBaselines, state-of-the-art: We compare our model against two competitive and relevant baselines as follows: BLSTM. The first baseline is a Bidirectional LSTM (BLSTM), an extension of the traditional LSTMs  [54] ,  [55] . LSTM and its variants have been successfully used in sentiment analysis in language and speech. This BLSTM comprises 1-layered bidirectional cells with embedding size 300 followed by a fully connected layer. GCN  [15] . A natural baseline to compare with our model is a spectral GCN in its standard form (as in Eq. (  3 )). The original network  [15]  is designed for node classification and only yields node-level embeddings. To obtain a graph-level embedding, we used max and mean pooling at the end of convolution layers. The GCN uses a binary adjacency matrix constructed following the method used in graph-based action recognition  [13] .\n\nIn addition to the baselines, we compare with two state-ofthe-art graph classification architectures: PATCHY-SAN  [19]  is a recent architecture that learns CNNs for arbitrary graphs. This architecture is originally developed for graph classification. PATCHY-Diff  [48]  is referred to an architecture where PATCHY-SAN is used in combination with the differentiable pooling layer between graph convolution layers proposed recently  [48] . SENet  [25] , Squeeze and Excitation net is a state-of-theart CNN architecture recently proposed for facial emotion recognition in videos.\n\nComparisons are also made with other existing works on the respective databases: AudioVisual Emotion Fusion (AVEF)  [6] , Kernel Crossmodal Factor Analysis (KCFA)  [49] , Optimized Kernel-Laplacian (OKL)  [50]  and Temporal Joint Embeddings (TJE)  [51] .\n\nResults: Table  I  compares the performance of L-GrIN with all the methods mentioned above. Clearly, the proposed model outperforms all the existing methods by a significant margin, including the graph-based state-of-the-art architectures, such as PATCHY-SAN and PATCHY-Diff. Our model performs better than BLSTM -a class of models most commonly used in video-based emotion recognition. SENet is a very recent CNN architecture developed for emotion recognition, which also trails our model in terms of performance. When compared to the GCN baseline  [15] , L-GrIN improves the recognition accuracy by more than 10% on RML and eNTERFACE, and more than 5% on RAVDESS.\n\nAlso note KCFA, OKL and TJE use both audio and visual information for recognition. Our model, even though uses only visual information, shows significant improvement over the audiovisual methods.\n\nFig.  4  shows the learned adjacency matrix for the RAVDESS database. The learned graph structure shows higher values closer to the diagonal i.e., the weights shared among the neighboring nodes. This indicates higher temporal dependencies locally and weaker dependency as we go further from a node.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Speech Emotion Recognition",
      "text": "Databases: We use the popular IEMOCAP database  [57]  for evaluating the performance of our model on speech emo- Node features: We extract a set of low-level descriptors (LLDs) from the raw speech utterances as proposed for In-terspeech2009 emotion challenge  [58]  using the OpenSMILE toolkit  [59] . The feature set includes Mel-Frequency Cepstral Coefficients (MFCCs), zero-crossing rate, voice probability, fundamental frequency (F0) and frame energy. For each sample, we use a sliding window of length 25ms with a stride length of 10ms to extract the LLDs locally. Each feature is then smoothed using a moving average filter, and the smoothed version is used to compute their respective first order delta coefficients. In addition, motivated by a recent work on speech emotion recognition  [60] , we also add spontaneity as a binary feature. The spontaneity information comes with the database. Altogether this produces node feature vectors of dimension P = 35.\n\nImplementation details: Each audio sample produces a graph of M = 120 nodes, where each node corresponds to a (overlapping) speech segment of length 25ms. Cyclic padding is used to make the samples of equal length, as before. We perform a 5-fold cross-validation and report the average unweighted accuracy in Table  II . The unweighted accuracy, a standard evaluation strategy for IEMOCAP, does not take into account the class imbalances. It simply computes the total number of correct classifications across all classes. All other parameters and settings remain the same as before to show the generalizability of our model.\n\nBaselines, state-of-the-art: Our model is compared with two baselines (BLSTM and GCN), two state-of-the-art graphbased architectures (PATCHY-SAN and PATCHY-Diff) as before. In addition, we also compare our model with four state-of-art methods in speech emotion recognition: CNN  [35] , CNN-LSTM  [35] , representation learning  [56]  and LSTM with Connectionist Temporal Modeling (LSTM-CTC)  [4] .\n\nResults: Table  II  shows that our model performs better than the baselines and state-of-the-art methods on IEMOCAP. Our",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Body Emotion Recognition",
      "text": "Databases: We use the MPI emotional body expression database  [61]  for our experiments. This database contains 1447 body motion samples of actors narrating coherent stories labeled with 11 emotions: amusement, anger, disgust, fear, joy, neutral, pride, relief, sadness, shame, and surprise. During their performance, a mocap system (device model: Xsens MVN) recorded the human motion using miniature inertial sensors. The system recorded dynamic 3D postures from 22 joints with a sampling rate of 120Hz.\n\nNode features: For this database, we use the raw information provided by the mocap system. Each node contains the 3D positions and orientations (measure in terms of the Euler angles, pitch, yaw and roll) at a given time-step. These measurements come with the database. The feature consists of Euler angles from 22 joints and additional location information of the reference point. We use all the information (without any preprocessing) as node features, resulting into a vector of dimension P = 72.\n\nImplementation details: Each input sample produces a graph of M = 120 nodes, where each node corresponds to a temporal segment of 120 th of a second. Cyclic padding is used as before. We perform a 5-fold cross-validation and report the  average accuracy in Table  III . All other network parameters remain the same as before.\n\nBaselines, state-of-the-art: Our model is compared with the baselines (BLSTM and GCN), the state-of-the-art graphbased architectures (PATCHY-SAN and PATCHY-Diff), and a recent work on this database, i.e., trajectory learning  [5] . The trajectory learning system  [5]  models neural motion and analyzes the spectral difference between an expressive motion and a neutral motion in order to recognize the body expressions.\n\nResults: Table  III  shows that L-GrIN outperforms the baselines and state-of-the-art methods on the MPI body expression database. Graph-based methods continue to perform well, indicating the effectiveness of graph-based methods for such tasks. Fig.  4  shows the learned adjacency A for the MPI database. As before, the learned graph structure exhibit higher temporal dependencies among the neighboring nodes.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Network Analysis",
      "text": "Network size: Tables I, II and III list the number of learnable network parameters for the baselines, state-of-theart graph-based architectures and the proposed L-GrIN. As mentioned earlier, a graph network largely reduces the number of learnable parameters as compared to the BLSTM or CNN architectures such as SENet (see Table  I ) without compromising the recognition accuracy. Our model has more parameters than the baseline GCN due to the inception layers and other learnable parameters, but also improves the recognition accuracy significantly. PATCHY-SAN and PATCHY-Diff have smaller network size compared to L-GRIN, but both trail L-GrIN in terms of performance on all databases. In case of facial emotion recognition, we discount the model size of the landmark detector in the comparison as it is common to all except SENet. For speech and body emotion recognition, no additional network was required as we used hand-crafted features and raw data. Learnable vs. fixed pooling: Recall that to obtain a graphlevel embedding from node-level embeddings, L-GrIN learns a pooling function (see Fig.  2 ). To show if learnable pooling indeed improves the recognition performance, we compare its performance with various fixed pooling strategies: max pooling, mean pooling and sort pooling (sortpool)  [62] . Table  IV  presents the comparisons on the RML database in terms of facial emotion recognition accuracy, which clearly shows the advantage of learnable pooling over fixed pooling strategies. Similar trend is observed for other databases.\n\nLearnable vs. manually constructed adjacency: An adjacency matrix represents the pairwise relationship between the graph nodes. When this information is not available naturally, a common practice is to manually construct an adjacency matrix. We argued earlier that this may result in sub-optimal graph structures which in turn affects the classification performance. We now compare the performance of leranable adjacency with two fixed adjacency matrices: (i) Binary adjacency: a natural choice is a binary adjacency matrix as used for graph-based action recognition  [13] . This is defined as (A b ) ij = 1 if |i -j| = 1 and 0 otherwise, i.e., a node (frame) is connected only to its subsequent and preceding node in the temporal direction. (ii) Weighted adjacency: Another adjacency matrix is formed by using the squared 2 distance between two node attributes as their edge weight. This is defined as\n\nTable V compares the performance of the proposed learnable adjacency with the two fixed adjacency matrices described above on the RML, IEMOCAP and the MPI databases. We chose one database from every modality. For this set of experiments we used only maxpooling to obtain the graphlevel embeddings for fair comparison. Clearly, the learnable adjacency matrix shows consistent improvement in accuracy across all databases for a relatively small increase in model complexity (only 6% additional parameters). The results show that a learnable adjacency has better at generalizing across databases and modalities.\n\nAblation study: We performed exhaustive ablation experiments to investigate the contribution of each component we proposed to build L-GrIN.   component brings significant improvement (row 2 to row 5) over the performance of standard GCN  [15]  which has 76.57% recognition accuracy (the top row in Table  VI ).\n\nThe introduction of the graph inception layer increases the recognition rate by 11%; when combined with our new graph convolution layer G * conv (Eq. (  4 )), the accuracy increases to 90.65%. Adding the learnable graph structure (learned A) and learnable pooling bring the accuracy up to 94.11% both contributing to the accuracy. Removing either of the leanrable components reduces the accuracy by 2.61%. The ablation results show that each of the proposed components in our architecture is important, and contributes positively towards its superior performance. Similar ablation trend was observed for other databases.\n\nInception layer settings: We also investigate the effects of the graph inception layer hyperparameters: (i) the parameter η corresponding to the size of the graph convolution filters G * 1 and G * 2 in Eq. (  5 ), and (ii) the number of graph inception layers in L-GrIN. First, we vary the filter dimensions (can be interpreted as scales) in the two inception layers and note how this correspond to the model's performance. Results for or increase filter sizes within the layers. We notice a small drop in performance with larger filter sizes and with higher number of inception layers. This could be possibly due to oversmoothing and over-mixing of the node features. However, the over-smoothing effect is not as prominent as in many node classification tasks. Analysis of the control weights: We also examine the impact of the weights controlling the various components of the loss function in Eq. (  8 ), i.e., λ 1 , λ 2 and λ 3 . Fig.  6  shows that highest performance is achieved for λ 1 = λ 2 = 0.1 and λ 2 = 0.0001 (marked red in the plots) on the RML database. We use these λ values in our experiments.\n\nCross-corpus performance: Methods exhibiting superior performance on one corpus, often fall short when tested on another corpus having different statistical distributions. We investigated the ability of our model to generalize across databases by evaluating its cross-corpus performance. To this end, we trained L-GrIN on one database, followed by finetuning a fully-connected layer on the target database, without changing the graph structure (or other parameters) learned from the training database.\n\nResults in Table  VIII  shows that our model can generalize well producing consistent results under cross-corpus evaluation. Our cross-corpus results higher accuracy compared to the same-corpus GCN accuracy. Cross-corpus results are comparable with the same-corpus performance of PATCHY-SAN. This shows the strength of the proposed architecture. It is worth noticing that the RML database (when used for training) does not have neutral and calmness emotion classes, but our model still recognizes those emotions on RAVDESS with 67.2% and 73.4% accuracy.\n\nNetwork visualization: To get an insight into the learning process of our model, we visualized how it attends to different nodes. The video data are the most suitable for the visualization. We use our trained model, and then feed-forward each test video sample through the network, and identify the node (each node corresponds to a video frame) that responded most strongly towards the maxpooling layer. This yields a salient node corresponding to each input. We present the corresponding video frames -one example per emotion class for RML, eNTERFACE and RAVDESS databases in Fig.  7 . The results show that the proposed model is able to learn the salient information from the input graphs such that it is representative of each emotion.\n\nV. CONCLUSION We proposed a novel, generalized graph architecture that can recognize emotion in a variety of dynamic input sequence. Our proposed architecture, L-GrIN, learns to detect emotion while jointly learning the underlying graph structure (adjacency matrix) and a pooling function to yield graph-level representation from node-level embeddings. We proposed a new spectral graph convolution operation and introduced the idea of inception in the graph domain. The advantage of our model lies in its state-of-the-art performance spanning three different modalities (video, audio and motion capture), with significantly fewer parameters compared to the CNNs and RNNs. This indicates that our model is suitable for applications in resource-constrained devices, such as smartphones.\n\nWe used both modality-specific features and even raw data as node features in this work. Our approach is not tied to any particular feature. In fact, our model can be trained end-to-end by combining it with modality-specific networks (e.g., a CNN) for feature extraction. The architecture we developed, although focuses on emotion recognition, is fairly generic. It will be applicable to a variety of classification tasks involving dynamic data, such as pose estimation, action recognition and visual speech recognition. Since our model makes no assumption about the graph structure, this is also applicable to common unstructured graphs. Future work will be directed towards building multimodal graph architectures taking advantage of the modality-agnostic architecture.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A generalized graph approach to modeling emotion dynamics. Data",
      "page": 1
    },
    {
      "caption": "Figure 1: for an overview). In our",
      "page": 1
    },
    {
      "caption": "Figure 2: Our proposed architecture, L-GrIN, consists of two graph inception layers (with a new spectral graph convolution layer) and a pooling layer (two ﬁxed",
      "page": 3
    },
    {
      "caption": "Figure 2: Below, we describe each",
      "page": 3
    },
    {
      "caption": "Figure 3: ). A frame refers to a",
      "page": 3
    },
    {
      "caption": "Figure 2: ), for joint graph learning and classiﬁcation. It has the",
      "page": 4
    },
    {
      "caption": "Figure 3: Graph construction: Given a dynamic input sequence of M segments,",
      "page": 4
    },
    {
      "caption": "Figure 4: Learned adjacency matrices for facial and body emotion recognition",
      "page": 5
    },
    {
      "caption": "Figure 4: shows the learned adjacency matrix for the RAVDESS",
      "page": 6
    },
    {
      "caption": "Figure 5: Motion capture recording set-up for the MPI database showing an",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the learned adjacency A for the MPI",
      "page": 7
    },
    {
      "caption": "Figure 2: ). To show if learnable pooling",
      "page": 8
    },
    {
      "caption": "Figure 6: Effect of the weight parameters in the loss function; experiments on",
      "page": 8
    },
    {
      "caption": "Figure 7: The results show that the proposed model is able to learn",
      "page": 9
    },
    {
      "caption": "Figure 7: Qualitative results showing the node (frame) for a graph input that generated the strongest response in our network. One result is displayed per class",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy (%)\nRML\nIEMOCAP\nMPI": "89.5\n61.4\n53.6\n62.4\n54.3\n49.0\n91.5\n65.5\n58.9",
          "Params\nRML\nIEMOCAP\nMPI": "113K\n78K\n96K\n113K\n78K\n96K\n120K\n92K\n110K"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RAVDESS": "RML",
          "RML\neNTERFACE": "RAVDESS\neNTERFACE",
          "81.94\n75.80": "75.42\n61.71"
        },
        {
          "RAVDESS": "eNTERFACE",
          "RML\neNTERFACE": "RML\nRAVDESS",
          "81.94\n75.80": "79.86\n77.51"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal 2d+ 3d facial expression recognition with deep fusion convolutional neural network",
      "authors": [
        "H Li",
        "J Sun",
        "Z Xu",
        "L Chen"
      ],
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "2",
      "title": "A deep spatial and temporal aggregation framework for video-based facial expression recognition",
      "authors": [
        "X Pan",
        "G Ying",
        "G Chen",
        "H Li",
        "W Li"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Towards temporal modelling of categorical speech emotion recognition",
      "authors": [
        "W Han",
        "H Ruan",
        "X Chen",
        "Z Wang",
        "H Li",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Toward an efficient body expression recognition based on the synthesis of a neutral movement",
      "authors": [
        "A Crenn",
        "A Meyer",
        "R Khan",
        "H Konik",
        "S Bouakaz"
      ],
      "year": "2017",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "6",
      "title": "Audio-visual emotion fusion (avef): A deep efficient weighted approach",
      "authors": [
        "Y Ma",
        "Y Hao",
        "M Chen",
        "J Chen",
        "P Lu",
        "A Košir"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "Exploring discriminative representations for image emotion recognition with cnns",
      "authors": [
        "W Zhang",
        "X He",
        "W Lu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Recognition of emotions in user-generated videos with kernelized features",
      "authors": [
        "H Zhang",
        "M Xu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Learning affective correspondence between music and image",
      "authors": [
        "G Verma",
        "E Dhekane",
        "T Guha"
      ],
      "year": "2019",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Visual scene-aware hybrid neural network architecture for video-based facial expression recognition",
      "authors": [
        "M Lee",
        "D Choi",
        "D Kim",
        "B Song"
      ],
      "year": "2019",
      "venue": "International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "11",
      "title": "Hierarchical video frame sequence representation with deep convolutional graph network",
      "authors": [
        "F Mao",
        "X Wu",
        "H Xue",
        "R Zhang"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "12",
      "title": "Structured sequence modeling with graph convolutional recurrent networks",
      "authors": [
        "Y Seo",
        "M Defferrard",
        "P Vandergheynst",
        "X Bresson"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "13",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Neural message passing for quantum chemistry",
      "authors": [
        "J Gilmer",
        "S Schoenholz",
        "P Riley",
        "O Vinyals",
        "G Dahl"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "15",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "16",
      "title": "Linkage based face clustering via graph convolution network",
      "authors": [
        "Z Wang",
        "L Zheng",
        "Y Li",
        "S Wang"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "17",
      "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
      "authors": [
        "C Qi",
        "H Su",
        "K Mo",
        "L Guibas"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Graph-based semisupervised learning for acoustic modeling in automatic speech recognition",
      "authors": [
        "Y Liu",
        "K Kirchhoff"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Learning convolutional neural networks for graphs",
      "authors": [
        "M Niepert",
        "M Ahmed",
        "K Kutzkov"
      ],
      "year": "2016",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "20",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "W Hamilton",
        "Z Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "21",
      "title": "Graph wavelet neural network",
      "authors": [
        "B Xu",
        "H Shen",
        "Q Cao",
        "Y Qiu",
        "X Cheng"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "22",
      "title": "Spectral networks and locally connected networks on graphs",
      "authors": [
        "J Bruna",
        "W Zaremba",
        "A Szlam",
        "Y Lecun"
      ],
      "year": "2014",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "23",
      "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "authors": [
        "M Defferrard",
        "X Bresson",
        "P Vandergheynst"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "24",
      "title": "A compact embedding for facial expression similarity",
      "authors": [
        "R Vemulapalli",
        "A Agarwala"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "26",
      "title": "Occluded facial expression recognition enhanced through privileged information",
      "authors": [
        "B Pan",
        "S Wang",
        "B Xia"
      ],
      "year": "2019",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "27",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "28",
      "title": "Feratt: Facial expression recognition with attention net",
      "authors": [
        "P Marrero Fernandez",
        "F Guerrero Pena",
        "T Ren",
        "A Cunha"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "29",
      "title": "Emotion-aware human attention prediction",
      "authors": [
        "M Cordel",
        "S Fan",
        "Z Shen",
        "M Kankanhalli"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "30",
      "title": "Deep learning the dynamic appearance and shape of facial action units",
      "authors": [
        "S Jaiswal",
        "M Valstar"
      ],
      "year": "2016",
      "venue": "Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "Video-based emotion recognition using cnn-rnn and c3d hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "32",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "33",
      "title": "Multi-cue fusion for emotion recognition in the wild",
      "authors": [
        "J Yan",
        "W Zheng",
        "Z Cui",
        "C Tang",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct modelling of speech emotion from raw speech"
    },
    {
      "citation_id": "36",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "Auditoryinspired end-to-end speech emotion recognition using 3d convolutional recurrent neural networks based on spectral-temporal representation",
      "authors": [
        "Z Peng",
        "Z Zhu",
        "M Unoki",
        "J Dang",
        "M Akagi"
      ],
      "year": "2018",
      "venue": "International Conference on Multimedia & Expo (ICME)"
    },
    {
      "citation_id": "38",
      "title": "Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "International Conference on Multimedia and Expo (ICME"
    },
    {
      "citation_id": "39",
      "title": "Mutual correlation attentive factors in dyadic fusion networks for speech emotion recognition",
      "authors": [
        "Y Gu",
        "X Lyu",
        "W Sun",
        "W Li",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2019",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "41",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "T Randhavane",
        "A Bera",
        "K Kapsaskis",
        "U Bhattacharya",
        "K Gray",
        "D Manocha"
      ],
      "year": "1906",
      "venue": "CoRR"
    },
    {
      "citation_id": "42",
      "title": "STEP: spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "1910",
      "venue": "CoRR"
    },
    {
      "citation_id": "43",
      "title": "How powerful are graph neural networks?",
      "authors": [
        "K Xu",
        "W Hu",
        "J Leskovec",
        "S Jegelka"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "44",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "45",
      "title": "Recognizing human emotional state from audiovisual signals",
      "authors": [
        "Y Wang",
        "L Guan"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "The enterface'05 audiovisual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "International Conference on Data Engineering Workshops (ICDEW)"
    },
    {
      "citation_id": "47",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "48",
      "title": "Hierarchical graph representation learning with differentiable pooling",
      "authors": [
        "Z Ying",
        "J You",
        "C Morris",
        "X Ren",
        "W Hamilton",
        "J Leskovec"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "49",
      "title": "Kernel cross-modal factor analysis for information fusion with application to bimodal emotion recognition",
      "authors": [
        "Y Wang",
        "L Guan",
        "A Venetsanopoulos"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "50",
      "title": "A combined rule-based & machine learning audio-visual emotion recognition approach",
      "authors": [
        "K Seng",
        "L.-M Ang",
        "C Ooi"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "International Conference on Affective Computing & Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "52",
      "title": "Dynamic facial analysis: From bayesian filtering to recurrent neural network",
      "authors": [
        "J Gu",
        "X Yang",
        "S Mello",
        "J Kautz"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "53",
      "title": "How far are we from solving the 2d & 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "54",
      "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "authors": [
        "A Graves",
        "J Schmidhuber"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "55",
      "title": "Lstm-based deep learning models for non-factoid answer selection",
      "authors": [
        "M Tan",
        "C Santos",
        "B Xiang",
        "B Zhou"
      ],
      "year": "2016",
      "venue": "International Conference on Learning Representations workshop"
    },
    {
      "citation_id": "56",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "57",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "58",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "The interspeech 2009 emotion challenge"
    },
    {
      "citation_id": "59",
      "title": "Openear-introducing the munich open-source emotion and affect recognition toolkit",
      "authors": [
        "F Eyben",
        "M Wollmer",
        "B Schuller"
      ],
      "year": "2009",
      "venue": "International Conference on Affective Computing and Intelligent Interactions"
    },
    {
      "citation_id": "60",
      "title": "Learning spontaneity to improve emotion recognition in speech",
      "authors": [
        "K Mangalam",
        "T Guha"
      ],
      "year": "2018",
      "venue": "Learning spontaneity to improve emotion recognition in speech"
    },
    {
      "citation_id": "61",
      "title": "The mpi emotional body expressions database for narrative scenarios",
      "authors": [
        "E Volkova",
        "S De La Rosa",
        "H Bulthoff",
        "B Mohler"
      ],
      "year": "2014",
      "venue": "PloS One"
    },
    {
      "citation_id": "62",
      "title": "An end-to-end deep learning architecture for graph classification",
      "authors": [
        "M Zhang",
        "Z Cui",
        "M Neumann",
        "Y Chen"
      ],
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence"
    }
  ]
}