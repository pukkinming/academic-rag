{
  "paper_id": "2310.19995v2",
  "title": "Emotional Theory Of Mind: Bridging Fast Visual Processing With Slow Linguistic Reasoning",
  "published": "2023-10-30T20:26:12Z",
  "authors": [
    "Yasaman Etesam",
    "Özge Nilay Yalçın",
    "Chuxuan Zhang",
    "Angelica Lim"
  ],
  "keywords": [
    "emotion recognition",
    "emotional theory of mind",
    "emotional reasoning",
    "context",
    "language models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The emotional theory of mind problem requires facial expressions, body pose, contextual information and implicit commonsense knowledge to reason about the person's emotion and its causes, making it currently one of the most difficult problems in affective computing. In this work, we propose multiple methods to incorporate the emotional reasoning capabilities by constructing \"narrative captions\" relevant to emotion perception, that includes contextual and physical signal descriptors that focuses on \"Who\", \"What\", \"Where\" and \"How\" questions related to the image and emotions of the individual. We propose two distinct ways to construct these captions using zeroshot classifiers (CLIP) and fine-tuning visual-language models (LLaVA) over human generated descriptors. We further utilize these captions to guide the reasoning of language (GPT-4) and vision-language models (LLaVa, GPT-Vision). We evaluate the use of the resulting models in an image-to-language-toemotion task. Our experiments showed that combining the \"Fast\" narrative descriptors and \"Slow\" reasoning of language models is a promising way to achieve emotional theory of mind.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Imagine a photo depicting a mother looking up from a swimming pool at her 4-year old daughter perched at the edge of a high diving board. How does this person feel? Traditional computer vision approaches might detect the swimming activity and infer a positive emotion. However, considering her perspective on the invisible consequences of the diving activity, including the possibility of her daughter falling off the diving board, a more plausible emotion would be fear. Humans can adeptly combine fast visual processing (e.g. detecting the swimming activity) and slow reasoning (e.g. possibility of falling off) capabilities  [1]  to perform emotional theory of mind.\n\nIndeed, our ability to recognize emotions and their causes allows us to understand one another, foster enduring social bonds, and engage in socially acceptable interactions. Integrating emotion recognition capabilities into our technologies holds promise for enhancing and streamlining human-machine interactions  [2] . Emotion recognition in humans requires much more than detecting patterns. Understanding of the causal relations, contextual information, social relationships as well as using theory of mind, all contribute to the complexity of this task, which are unresolved problems in affective computing research. However, emotion recognition systems today still Happiness suffer from poor performance  [3]  due to the complexity of the task. Many image-based emotion recognition systems focus solely on using facial or body features  [4] ,  [5] , which can have low accuracy in the absence of contextual information  [6] ,  [7] .\n\nDue to the importance of contextual information for emotional reasoning and theory of mind tasks, the affective computing research community has created datasets and built models that include or make use of context. As a recent example, the EMOTIC dataset incorporates contextual and environmental factors for emotion recognition in still images  [8] . The inclusion of contextual information in addition to facial features is found to significantly improve the accuracy of the emotion recognition in a multi-labelling task  [9] -  [11] .\n\nIn this paper, we examine different approaches to incorporate the \"Slow\" reasoning processes into the \"Fast\" emotion recognition pipelines. We focus on a multi-label, contextual emotional theory of mind task by creating a pipeline that includes caption generation and LLMs that combines \"Fast\" and \"Slow\" processing (see Fig.  1 ). Towards visual grounding of text, we use a zero-shot classifier (CLIP) to generate explainable linguistic descriptions of a target person in images using physical signals, action descriptors and location information. We then use an LLM (e.g.  to reason about the generated narrative text and predict the possible emotions that person might be feeling. We further use a chain-of-thought method on vision-language models (i.e., LLAVA  [12] ) to guide their reasoning capabilities. We compared our results with oneshot (i.e., fast processing) vision models and vision language models, which can combine both fast visual processing and slower, cognitive and linguistic processing.\n\nThe contributions of this paper are as follows:\n\n• Comparing the \"Fast\" (CLIP) visual framework to the \"Fast and Slow\" visual and language reasoning frameworks (NarraCaps + LLM, ExpansionNet + LLM, Nar-raCapsXL) for contextual emotional understanding. • Introducing two ways of achieving \"Fast & Slow\" Framework through generating zero-shot and fine-tuned narrative descriptors of the images. • Investigating the use of \"Chain-of-thought\" prompting methods to mimic emotional reasoning steps in large vision language models (LLaVA).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Cognitive processes can be categorized into two groups, referred to as intuition and reason, or System 1 and System 2  [1] . In this dual-process model, one system rapidly generates intuitive answers to problems as they arise, while the other system monitors the quality of these proposals using reasoning  [13] -  [15] . Thinking Fast (System 1) operates quickly and intuitively, based on heuristics and shortcuts, and is the kind of thinking we use for everyday decisions. This process resembles traditional recognition and categorization models. In comparison, Thinking Slow (System 2) involves more logical and effortful thinking. This system is engaged when we face complex problems that require reasoning, causal analysis and decision making. These processes are suggested to be related to linguistic reasoning processes in humans. Human visual emotion recognition, similar to many complex tasks, rely on both fast judgements related to facial and bodily expression detection which are automated and intuitive (System 1), as well as understanding contextual and causal relationships between the actors and their surroundings (System 2).\n\nCurrent emotion recognition models suffer from relying only on fast recognition models that resemble System 1, which recently ignited discussions in the Affective Computing community on the importance of context and reasoning  [3] . Most of the prior work on visual emotion recognition tasks (see  [16] ,  [17]  for recent reviews) focus on facial expressions  [4]  or body posture  [5]  to predict emotions in usually a few number of emotion categories such as happy, sad, angry  [18] . However, facial and posture information alone would not be sufficient to understand the emotional state of a person, as humans rely on contextual information to reason about the social and environmental causes of one's emotion  [6] . There are recent attempts to incorporate contextual information to visual emotion recognition datasets and models (i.e., EMOTIC  [8] .\n\nThe EMOTIC baseline uses one CNN to extract features from the target human, and another to obtain global features over the entire image, and incorporates the two sources using a fusion network  [8] . In PERI  [19] , the modulation of attention at various levels within their feature extraction network using feature masks. To the extent of our knowledge, the current best approach was Emoticon proposed by Mittal and colleagues  [10] , which explored the use of graph convolutional networks and image depth maps, they found that using depth maps could improve the results. However, the emotional theory of mind and reasoning mechanisms are poorly understood within the affective science and social psychology community  [20]  and has not been examined or incorporated in contextual emotion recognition models.\n\nNatural language and emotional theory of mind. Language is a fundamental element in reasoning and it plays a crucial role in emotion perception  [21] -  [25] . Recent investigations into large language models (LLMs) have uncovered some latent capabilities for reasoning  [26] , including some sub-tasks on emotion inference  [27] . Emotional theory of mind tasks using language tend to focus on appraisal based reasoning on emotions, inferring based on a sequence of events. For instance, among other social intelligence tasks, further research  [28]  explored how a language model could respond to an emotional event, e.g. \"Although Taylor was older and stronger, they lost to Alex in the wrestling match. How does Alex feel?\" Their findings suggested some level of social and emotional intelligence in LLMs. Further works used chain-of-thought prompting methods on complex reasoning tasks, by generating intermediate guiding prompts to assist the reasoning process  [29] . Incorporating explicit linguistic knowledge through these type of prompting mechanisms is suggested to be a promising way to assist the reasoning and implicit knowledge in LLMs  [30] ,  [31] .\n\nVision language models. One natural approach to link the visual emotional theory of mind task to the capabilities of large language models is to explore the large body of work in image captioning  [32]  and visual question and answering  [33] . Early work related to emotional captions included Senticap  [34]  and Semstyle  [35]  which focused on the emotional style of the resulting caption, rather than an emotional theory of mind task. For example, a caption with sentiment generated by Senticap changed the neutral phrase, \"A motorcycle parked behind a truck on a green field.\" to \"A beat up, rusty motorcycle on unmowed grass by a truck and trailer.\" More recent selfsupervised vision language models such as Visual-GPT  [36]  for image captioning and CLIP  [37] ,  [38]  provide zero-shot captioning  [39]  on datasets of images and their captions of everyday activities and objects. Amidst the success of visual language models in tasks like image classification  [40] -  [42]  and object detection  [43] -  [46] , the opportunity still remains to generate captions that describe details relevant to understanding the emotional state of the person in the image.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this paper, we propose and evaluate ways to model Emotional Theory of Mind by combining the \"Fast\" visual processing capabilities using visual classification models and the \"Slow\" reasoning capabilities using language models. The \"Fast\" System 1 provides visual information related to human affect that can be directly captured from the images, e.g. \"His veins were popping out from his skin. His cheeks were red and eyebrows narrowed.\" We then employ language models to provide cognitive reasoning on the observation, akin to System 2 \"Slow\" processing, e.g. \"Adam is running in a marathon.\"\n\nTo combine these two processing capabilities, we create linguistic descriptors of the images (Fast processing), to guide the reasoning processes (Slow processing) in Large Language or Vision-Language models (see Fig.  1 ), similar to \"Chain-of-Thought\" prompting methods  [29] . We examine two different ways in constructing these linguistic descriptors, which can be considered as image captions: 1) Zero-shot NarraCaps and 2) Fine-tuning VLMs using Human-generated captions NarraCapsXL.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Narrative Captioning",
      "text": "The first captioning method which is called Narrative Captioning (NarraCap)  [47]  makes use of templates and the vision language model CLIP  [37] . We also test a baseline captioning method, ExpansionNet  [48] , and fine-tune a visionlanguage model LLaVA  [12]  that was not specifically trained for emotion understanding.\n\nFor generating NarraCaps, following previous work  [47] , we extract the cropped bounding box of a person from an image and provide gender/age information to CLIP for identification. Then, we analyze the entire image to understand the depicted actions using a list of 848 actions sourced from Kinetics-700  [49] , UCF-101  [50] , and HMDB datasets  [51] . Narrative captions enrich the image description by incorporating over 850 social signals gathered from a writer's guide that provides suggestions for sets of visual cues or actions that support emotional evocation  [52] . These signals, along with the cropped bounding box, are fed to CLIP to generate descriptive captions for the person in the image. To provide further context, we utilize 224 environmental descriptors from guides to urban  [53]  and rural  [54]  settings to describe the scene's location. Examples of narrative captions (NarraCaps) can be found in Fig.  2 , as well as failure cases in Fig.  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Human-Generated Captions",
      "text": "We then employ fine-tuning and a small set of humangenerated captions to generate human-like captions for images. To do so, we use emotional descriptions generated by human participants for 387 samples of the EMOTIC database  [55] . The descriptions include the age, gender, profession, physical signals, human interactions, and environmental features for each person. Age and gender information is captured via using one of the categories among \"adult\", \"child\" or \"adolescent\", and name of the person (e.g., \"Sean\" to indicate male, \"Jane\" for female).\n\nUsing LoRA  [56] , we fine-tuned LLaVA 1  on these human-generated captions  [55]  and ground truth labels from EMOTIC  [8] . Since human-generated captions are available for only a limited set of images, we increased the dataset size by augmenting the data and using each image ten times with random permutations of labels. The prompt we used for finetuning and inference is: \"Write a caption describing the person in the red bounding box. Then from suffering, pain, aversion, disapproval, anger, fear, annoyance, fatigue, disquietment, doubt/confusion, embarrassment, disconnection, affection, confidence, engagement, happiness, peace, pleasure, esteem, excitement, anticipation, yearning, sensitivity, surprise, sadness, and sympathy, pick the top labels that this person is feeling at the same time.\"\n\nAnd for the answer format for fine-tuning, we experimented with both <caption> + This person is feeling <labels> and <caption> + <name of the target is feeling <labels>. Note that the name of the target is the first word in the human captions. Using this fine-tuned model, we then generate similar captions given each image. We refer to this fine-tuned captioning method as NarraCapsXL (NarraCaps X LLaVA).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Emotion Inference: Fast And Slow",
      "text": "Using the captions generated either using the zero-shot NarraCaps method, or the fine-tuned NarraCapsXL method, as described above, we incorporate explicit linguistic knowledge to the Large Language Models. By doing so, we guide the reasoning process of LLMs, similar \"Chain-of-thought\" prompting methods used in prior work  [29] -  [31] .\n\nThe generated caption is sent to the LLM to obtain a set of emotions that is experienced by the individual in the bounding box. To do this, we provide the caption, along with a prompt, to GPT-4 with the temperature set to zero. The prompt asks for the top emotion labels understood from the narrative caption: \"<caption> From suffering, pain, [...], and sympathy, pick the top labels that this person is feeling at the same time.\"",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiments",
      "text": "We use the EMOTIC dataset which covers 26 different labels. The related emotion recognition task is to provide a list of emotion labels that matches those chosen by annotators. The training set (70%) was annotated by 1 annotator, where Validation (10%) and Test (20%) sets were annotated by 5 and 3 annotators, respectively. For evaluation metrics we used precision, recall, F1 score, implemented using the scikit-learn library  [57] . As our methods output labels rather than probabilities, we cannot utilize mAP as an evaluation metric. We use varying state-of-the-art model families offered by OpenAI 2  , including a zero-shot classifier (CLIP), a large language model (GPT-4, hereafter also referred to as GPT), and an large vision language model (GPT4-vision). We also use an open-source VLM (LLaVA  [12] ). We compare the following methods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Trained Baseline Model",
      "text": "EMOTIC Along with the dataset, Kosti and colleagues  [8]  introduced a two-branched CNN-based network baseline. The first branch extracts body related features and the second branch extracts scene-context features. Then a fusion network combines these features and estimates the output, which is resulted in a mAP of 28.33. Further work such as  [10]  using pose and face features (context1), background information (context2), and interactions/social dynamics (context3), reported higher mAP results (mAP = 35.48, F1 not reported), however, could not be included in our analysis as we were not able to reproduce their results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Fast Framework",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Clip-Only",
      "text": "We evaluate the capabilities of the zero-shot classifier CLIP to predict the emotion labels. In this study we pass the image with the emotion labels in the format of: \"The person in the red bounding box is feeling {emotion label}\" to produce the probabilities that CLIP gives to each of these sentences. We then select the 6 labels (average number of ground truth labels in validation set) with the highest probabilities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Fast And Slow Framework",
      "text": "ExpansionNet (ExpNet) and LLM ExpansionNet v2  [48]  is a fast end-to-end training model for Image Captioning. Its characteristic is expanding the input sequence into another length with a different length and retrieving the original length back again. The model achieves the state of art performance over the MS-COCO 2014 captioning challenge, and was used as a backbone for a recent approach trained on EMOTIC  [58] . We fed the captions generated by ExpansionNet v2 into GPT-4 to obtain top emotion labels.\n\nNarrative captioning (NarraCaps) and LLM Using narrative captions, we pass them to Large Language Models and ask for the related emotion labels using the prompt described in Sec. III-C For LLMs, we used GPT-4 with temperature parameter set to 0 and the maximum token count set to 256.\n\nFine-tuned Narrative Captions (NarraCapXL) Using a small set of human captions obtained over EMOTIC dataset  [55]  and corresponding ground truth labels, we fine-tuned LLaVA  [12]  language and vision model to generate captions, and then use these captions and the same prompt used for NarraCaps (see Section III-C) to generate top emotions, either directly in LLaVA or by using GPT-4  [59] . For finetuning LLaVA, answer format <caption> + This person is feeling <labels>, resulted in an F1 score of 30.02. After changing it to <caption> + <name of the target is feeling <labels>, the F1 score increased to 31.05. LLAVA fine-tuning was performed on four A40 48GB GPUs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Vision-Language Model (Vlm) Baselines",
      "text": "We further evaluate three baseline models using Vision-Language Models (i.e., LLaVA and GPT4-vision) to compare the accuracy of these models without the reasoning guidance from the proposed captioning mechanisms. The reasoning capabilities of LLMs and VLMs has been a topic of controversy, where many experiments point towards the lack of commonsense or causal reasoning capabilities in these models  [60] . Therefore, we cannot claim whether VLMs already should be considered as a Fast & Slow process, and evaluate these models as a separate baseline category. We use LLaVA Large Language and Vision Assistant (LLaVA)  [12]  and Open-AI's GPT4-Vision model as two baselines on large visual-language models. LLaVA is a multi-purpose multimodal model designed by combining CLIP's visual encoder  [37]  and LLAMA's language decoder  [61] . The model is fine-tuned end-to-end on the language-image instruction-following data generated using GPT-4  [59] . We used the same prompt in Section III-C without the NarraCaps caption. The experiments were run on a single A40 48GB GPU.\n\nFor Fine-tuned LLaVA without language guidance, we employ the same dataset as NarraCapXL but without captions. Additionally, we apply the augmentation approach outlined in Sec. III-B. This experiment was conducted using a single A40 48GB GPU.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "The results for our experiments are shown in Table  I , and example images with captions in Fig.  2 .\n\nOur results showed that, overall, the models that utilized the \"Fast & Slow\" Framework achieved the highest score within their respective trained and non-trained categories, compared to \"Fast\"-only or Baseline models. Fig.  2 . Qualitative results of EMOTIC images, truth (GT) labels, captions and inferred labels from example models from \"Fast\" and \"Fast & Slow\" framework.\n\nAmong the trained (i.e., EMOTIC) and fine-tuned (e.g., Nar-raCapsXL, LLaVa) models, we found that the \"Fast&Slow\" Framework performed the best. Guiding the reasoning capabilities with NarraCapsXL captions on both LLaVA and GPT4 proved to be the most successful method. Fast & Slow framework using NarraCapsXL also surpassed the VLM methods that were not using any reasoning guidance, including the fine-tuned LLaVa, suggesting that VLMs alone may not be enough for emotional reasoning. Apart from achieving the highest F1 score with a small number of fine-tuning examples, the NarraCapsXL method also is very powerful due to its explainability.\n\nAmong the zero-shot methods, as shown in Table  I , our results showed that the methods combining \"Fast and Slow\" 3a: This person is a baby boy who is skateboarding in a skate park. He has hands laying in the lap. He is rubbing one's hands on pant legs. 3b: Terry is a male adult. Terry is or has smiling, sitting. Alan is Terry's friend and he is smiling and sitting beside Terry. Terry's physical environment is on a bench in a street. 2a: This person is a girl who is huddling in a morgue. She is crying, wailing, begging for help. 2b: Chloe is a Female Adult. Chloe is or has Frowning, Crying, Covering face with hands. Chloe's physical environment is on the street, surrounded by women crying.\n\n4a: This person is a woman who is having a pillow fight in a wake. She is retreating into the fetal position or curling up. She is tossing and turning in bed, an inability to sleep. 4b: Sophia is a female adult. Sophia is or has frowning, mouth wide open, hands covering ears. Sophia's physical environment is on a bed with pillows.\n\n1a: This person is a girl who is sign language interpreting in a therapist office. She is choosing the middle, not the sides (be it a couch or a room). She is exchanging knowing looks with others. She is speed-talking with others, heads close together, gossipy. She is straining to hear, shushing others to be quiet. 1b: Chloe is a female adult. Chloe is or has taking off eyeglasses. Mia is Chloe's daughter and she is being comforted by Chloe. Chloe's physical environment is on a couch. frameworks (NarraCaps+GPT4, ExpNet+GPT4) outperform the methods relying solely on the \"Fast\" framework (CLIP) or VLM methods (i.e., LLaVA,GPT4-vision). For instance, in Fig.  2 .1, it appears that CLIP catgorizes the person with an open mouth as 'Surprise' 'Doubt/Confusion' and 'Embarrassment'. On the other hand, the 'Fast and Slow' frameworks, given the context of a ceromony, or giving a toast, may reason that this body pose is more indicative of 'engagement' and 'confidence'.\n\nImproving Narrative Captions. Within the \"Fast & Slow\" Framework models, we found that ExpansionNet (Ex-pNet+GPT4) performed the worst, pointing towards the need of providing emotionally relevant captions while guiding the emotion recognition models. However, the zero-shot Narra-Caps method was also not as successful as its fine-tuned counterpart (NarraCapsXL), suggesting that the captions themselves need improvement. Upon closer inspection, we found instances where NarraCaps was not as successful in detecting certain actions (how), or characteristics of the humans (who) in the picture. Fig.  3  shows example images where the gender and age (3.a), the location (2.a, 4.a) and the action (1.a, 4.a) was detected incorrectly by NarraCaps, while the fine-tuned NarraCapsXL was successful in correctly detecting them.\n\nTo further examine the degree in which the quality of the captions can affect the results, we also directly use the human-generated captions 3 were to guide the reasoning of GPT-4. This resulted in an F1 score of 34.17 over the small sample of 387 images  [55] , compared to 26.50 F1 score in the EMOTIC and an 26.19 F1 score with NarraCaps+GPT4. These results points toward the promise of guiding emotional reasoning via providing descriptions of the image that focuses on emotionally-salient content. However, NarraCaps method does not reach to human-level performance. We cannot compare these results with NarraCapsXL as it was fine-tuned on this same set.\n\nThe Grounding Problem. One of the important shortcomings of the guided reasoning method has been the inability of 3 https://github.com/Tandon-A/emotic/ providing the bounding boxes as input to the system. Previous work showed that CLIP, which we use for generating the oneshot NarraCaps captions and is utilized by multiple VLMs, is unable to capture bounding box information  [62] . This problem also examined in previous work that showed the inability of VLMs to understand the bounding boxes  [63] . This grounding problem seems to be the biggest challenge in our currently proposed \"Fast & Slow\" Framework, compared to the baseline model where the bounding box information could be provided during training.\n\nIn NarraCaps, we try to overcome this issue by using only the cropped image in the bounding box while asking the \"Who\" and \"How\" questions, and using the whole image while capturing the environment and actions. However, Nar-raCapsXL method generated captions using the whole image once. Examples of this grounding problem can be seen in Fig.  4 , where the individuals in each image can have different emotions depending on context and social relationship. The saliency map in Fig.  5  shows that in some cases, where the bounding box is not coinciding with the most salient information, the narration is describing the wrong individual in the image (Fig.  5 ), and resulting in incorrect predictions. This is the case even when the actions and interactions between all individuals in the scene is actually correctly predicted Fig.  5  (right). The resulting emotion categories can be incorrect in these types of examples. In images where multiple individuals have similar emotions, e.g., Fig.  4  (left), this issue does not result in wrong emotion categorization.\n\nFor our work, we tried to utilize prompt engineering and including the phrase \"Person in the red bounding box is feeling\", \"This person is feeling\" or \"<Name> is feeling\", the results of which described in Section IV-C.\n\nAt the time of writing, we are only aware of one method that incorporated the bounding box information to the VLMs  [64] . Future work will focus on using one such method while fine-tuning over human captions.\n\nTerry is a male adult. Terry is or has goggles, red jacket, black pants, gray gloves, ski poles. Alan is Terry's friend and he is skiing next to Terry. Terry's physical environment is on a ski slope.\n\nTerry is a male adult. Terry is or has smiling, goggles, blue snowsuit, ski poles, skiis. Alan is Terry's friend and he is skiing with Terry. Terry's physical environment is on a ski slope.\n\nJane is a Female Adult. Jane is or has Gaze clouding, going distant, Empty stare, Sitting, Keeping one's back to a wall. Karl is a doctor and pushing a hospital bed. Alec is a nurse and pushing a hospital bed with Alec. Jane's physical environment is in a hospital hallway.\n\nJane is a female adult. Jane is or has looking away, sitting, crossed arms. Karl is a doctor and pushing a hospital bed. Alec is a nurse and taking down notes. Jane's physical environment is in a hospital hallway.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Limitations",
      "text": "The present study assessing emotional theory of mind on the EMOTIC image dataset using the notion of combining fast visual descriptors with slow reasoning processes. Albeit achieving improvements from the baseline EMOTIC model and being first of its kind, our method is not without limitations. Firstly, we only focused on assessing a few number of Large Language and Vision Language models to test the proposed \"Fast & Slow\" framework capabilities. These included the varying model families offered by OpenAI, including a zero-shot classifier (CLIP), a large language model (GPT), and a large vision language model (GPT4-vision). As GPT-4's vision capability is closed source and has limited access, we use LLaVA as the state-of-the-art open-source alternative for the fine-tuning task. As our paper focuses on examination of guiding the emotional reasoning task inspired by human information processing, we only chose the models that were considered as the top performers at the time of writing and did not focus on a wider breadth of other models for comparison. It is possible that other models in the family of VLMs and LLM could outperform the investigated models.\n\nIn addition, this study focused on studying the as-is capabilities of these models, without a full fine-tuning or training effort, given the EMOTIC dataset. The fine-tuned NarraCap-sXL model only used 300 human-generated captions, most of which were only negative examples. Albeit achieving better accuracy than the baseline, future work needs to include more examples to fine-tune for a better comparison with the baseline models. Finally, we did not test our results in other non-contextual emotion recognition datasets in which the environment information or interaction information could not be gathered. However, EMOTIC dataset includes some examples with no visible environment.\n\nLastly, in attempting to guide LLMs and VLMs reasoning capability for this task, we used two methods to generate narrative captions for each person-image pair. One limitation is that we did not perform any quantitative human evalation on the quality of the resulting captions themselves, rather focusing on end-to-end performance against the original human annotated baseline and comparing captions qualitatively. Future work will focus on quantitative and qualitative examinations of inclusion of the interaction information.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we explore the capabilities of VLMs and LLMs for the visual emotional theory of mind task by guiding the emotional reasoning capabilities of these models using Narrative Captions. We found that \"Fast and Slow\" methods (vision and language) outperformed the \"Fast\" only (vision) frameworks. However, the zero-shot \"Fast & Slow\" methods still do not perform better than the models trained specifically for this task on the EMOTIC dataset. The grounding problem seems to be the biggest issue in our method using fine-tuning VLMs. Future work is needed in solving this issue before understanding the full capabilities and shortcomings of the proposed \"Fast & Slow\" Framework. However, our results are promising in both improving the performance and providing explainable captions for the emotional recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Viii. Ethical Considerations",
      "text": "The research team conducted all the codings and experiments in this study. The images were sourced from the EMOTIC Dataset, which consists images obtained from internet and public datasets like MSCOCO and Ade20k. To access the EMOTIC dataset, permission must be obtained from the owners. Research team has no affiliation with the individuals depicted in the images. The images show individuals experiencing a range of emotions, such as grief at a funeral, participation in protests, or being in war-related scenarios. However, the dataset does not contain images that surpass the explicitness or intensity typically encountered in news media.\n\nIt is crucial to note that there might be biases affecting this work, such as cultural biases from annotators and biases caused by the languages that GPT and CLIP are trained on. Another limitation of this method is the English language and culture based thesaurus of social signals, written by an North American author. Non-verbal social signals are culturedependent  [65]  and translation may not suffice. An interesting direction for future work could investigate the distribution of social signals across emotions and between cultures.\n\nThe implementation of this work utilized pre-trained models, like GPT, CLIP, and LLaVA. Although no additional training was conducted in this study, it is crucial to acknowledge the significant carbon cost associated with training such models, which should not be underestimated.\n\nAs a final note, it is worth mentioning that the authors strongly oppose the utilization of any software that employs emotion estimation approaches with the potential to violate privacy, personal autonomy, and human rights.\n\nExamining possible age or gender bias. One of the advantages of the NarraCaps approach is that it provides a way to explicitly select image details to include for inference and perform ablations using the text representation. To investigate the impact of age and gender, we excluded those components from the captions. The results of each study on a set of 1000 images. Our results indicated that including gender and age in the captions resulted in a slight reduction in F1 scores (a difference of -0.42 and -1.26, respectively), although based on calculations of standard error (SE 0.78 and 0.73, respectively), the difference may be attributed to noise. As a result of this study we initially did not use gender in our experiments, however, to be able to compare with NarraCapsXL that was fine-tuned on human generated captions that included age and gender information, we kept this information in our generated captions. Overall, we found that the impact of gender in our results are negligible and could be omitted from Narrative Captions to guide emotional reasoning. However, age might be a slightly important factor, possibly due to the type of interactions and vulnerability of certain age groups. More evaluations are required to see the extend of which this might be important in emotional reasoning tasks.\n\nMoreover, we acknowledge that the captions make a statistical categorization of the apparent gender of the individuals and should not be used to infer or make decisions using this information.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Fast frameworks might perceive the body pose with raised arms and",
      "page": 1
    },
    {
      "caption": "Figure 1: ). Towards visual grounding",
      "page": 1
    },
    {
      "caption": "Figure 1: ), similar to “Chain-of-",
      "page": 3
    },
    {
      "caption": "Figure 2: , as well as failure cases in Fig. 3.",
      "page": 3
    },
    {
      "caption": "Figure 2: Our results showed that, overall, the models that utilized the",
      "page": 4
    },
    {
      "caption": "Figure 2: Qualitative results of EMOTIC images, ground truth (GT) labels, captions and inferred labels from example models from ”Fast” and ”Fast & Slow”",
      "page": 5
    },
    {
      "caption": "Figure 3: Example failure cases of NarraCap (X.a) that was correctly captioned by trained LLaVA captions (X.b).",
      "page": 6
    },
    {
      "caption": "Figure 2: 1, it appears that CLIP catgorizes the person with an",
      "page": 6
    },
    {
      "caption": "Figure 3: shows example images where the gender",
      "page": 6
    },
    {
      "caption": "Figure 4: , where the individuals in each image can have different",
      "page": 6
    },
    {
      "caption": "Figure 5: shows that in some cases, where",
      "page": 6
    },
    {
      "caption": "Figure 5: ), and resulting in incorrect predictions. This",
      "page": 6
    },
    {
      "caption": "Figure 5: (right). The resulting emotion categories can be incorrect in",
      "page": 6
    },
    {
      "caption": "Figure 4: (left), this issue does not",
      "page": 6
    },
    {
      "caption": "Figure 4: Examples for correct (Left) and incorrect (Right) grounding of NarraCapsXL, where captions change depending on the red bounding box.",
      "page": 7
    },
    {
      "caption": "Figure 5: CLIP saliency maps of example images and their resulting captions",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1. NarraCapXL: Sean is a Male Adult. Sean is 2.NarraCapXL: Sean is a male adult. Sean is\na(n) President. Sean is or has Frowning, Raising a(n) soccer player. Sean is or has mouth wide\na glass of wine. Sean's physical environment is at open, squeezing eyes shut, arms reaching out, 3.NarraCapXL: Greg is a male adult. Greg is or\na ceremony. Sean is feeling Engagement, legs moving fast. Sean's physical environment is has closed eyes, lips that flatten. Alec is a pet\nConfidence, Peace. in a soccer game and has just scored a goal. owner and he is sleeping with his dog on a couch.\nNarraCap: This person is a man who is using an Sean is feeling Excitement, Happiness, Greg's physical environment is on a couch. Greg\ninhaler in a black-tie event. He is drinking. He is Confidence, Surprise, Anger , Sensitivity, is feeling Affection, Peace, Happiness, Fatigue.\ngiving a toast or praise. Anticipation. NarraCap: This person is a man who is grooming\nGT: Anticipation, Confidence, Engagement NarraCap: This person is a man who is doing a dog in a group foster home. He is curling up to\nEMOTIC: Annoyance, Anticipation, Confidence, hurling (sport) in a sporting event stands. He is take up less space. He is falling asleep while\nDisquietment, Doubt/Confusion, Engagement, tugging down the sleeves. watching TV.\nEsteem, Happiness, Sympathy GT: Confidence, Excitement, Happiness, Pain, GT: Affection, Fatigue, Happiness, Peace,\nNarraCap+gpt4: Confidence, Engagement, Pleasure Pleasure\nAnticipation, Excitement, Happiness, Peace, EMOTIC: Anticipation, Confidence, EMOTIC: Affection, Anticipation, Disquietment,\nEsteem Embarrassment, Engagement, Excitement Doubt/Confusion, Engagement, Esteem, Fatigue,\nCLIP: Sympathy, Disapproval, Surprise, NarraCap+gpt4: Engagement, Anticipation, Happiness, Pain, Peace, Pleasure, Sadness,\nEmbarrassment, Doubt/Confusion, Suffering Excitement, Confidence, Fatigue Sensitivity, Sympathy\nCLIP: Confidence, Sympathy, Excitement, NarraCap+gpt4: Fatigue, Engagement, Affection,\nSuffering, Anticipation, Embarrassment Peace, Pleasure, Doubt/Confusion\nCLIP: Fatigue, Sympathy, Engagement,\nSensitivity, Pleasure, Affection\n4.NarraCapXL: Chloe is a female adult. Chloe is 5.NarraCapXL: Sean is a Male Adult. Sean is\nor has smiling, open mouth, leaning forward, a(n) Runner. Sean is or has Staring down at the\nlooking up. Karl is an adult and playing volleyball ground, Breathing excessively, Bent spine,\nwith Chloe. Chloe's physical environment is on a Sweeping hand across the forehead to get rid of 6.NarraCapXL: Sean is a male adult. Sean is\nbeach. Chloe is feeling Confidence, Excitement, sweat. Sean's physical environment is in a park. a(n) business professional. Sean is or has closed\nHappiness, Engagement, Pleasure. Sean is feeling Fatigue, Disquietment, eyes, tilting head downward, sleeping. Sean's\nNarraCap: This person is a girl who is playing Doubt/Confusion, Suffering. physical environment is on a table with a plate of\nvolleyball in a beach party. She has jumpiness. NarraCap: This person is a boy who is drinking in food. Sean is feeling Fatigue, Peace.\nShe is participating in relaxing activities. a water park. He is pulling down glasses and NarraCap: This person is a man who is dining in\nGT: Anticipation, Confidence, Engagement, looking over the rims. a casual dining restaurant. He is laying the head\nExcitement, Happiness, Pleasure GT: Annoyance, Disquietment, Doubt/Confusion, down on the table.\nEMOTIC: Affection, Anticipation, Confidence, Fatigue, Pain GT: Disconnection, Disquietment, Fatigue\nEngagement, Excitement, Happiness, Pleasure, EMOTIC: Affection, Anticipation, Disquietment, EMOTIC: Affection, Anticipation, Engagement,\nSensitivity, Surprise, Sympathy Engagement, Esteem, Excitement, Fatigue, Excitement, Happiness, Peace, Pleasure\nNarraCap+gpt4: Excitement, Happiness, Happiness, Pleasure, Sensitivity, Sympathy NarraCap+gpt4: Suffering, Pain, Fatigue,\nAnticipation, Engagement, Pleasure, Confidence, NarraCap+gpt4: Excitement, Anticipation, Disquietment, Doubt/Confusion, Embarrassment,\nSurprise Pleasure, Happiness, Engagement Disconnection, Sadness\nCLIP: Engagement, Surprise, Confidence, CLIP: Engagement, Disconnection, Confidence, CLIP: Sympathy, Aversion, Suffering, Sensitivity,\nSensitivity, Disapproval, Esteem Esteem, Sensitivity, Fatigue Disquietment, Fatigue": "",
          "Column_2": "3.NarraCapXL: Greg is a male adult. Greg is or\nhas closed eyes, lips that flatten. Alec is a pet\nowner and he is sleeping with his dog on a couch.\nGreg's physical environment is on a couch. Greg\nis feeling Affection, Peace, Happiness, Fatigue.\nNarraCap: This person is a man who is grooming\na dog in a group foster home. He is curling up to\ntake up less space. He is falling asleep while\nwatching TV.\nGT: Affection, Fatigue, Happiness, Peace,\nPleasure\nEMOTIC: Affection, Anticipation, Disquietment,\nDoubt/Confusion, Engagement, Esteem, Fatigue,\nHappiness, Pain, Peace, Pleasure, Sadness,\nSensitivity, Sympathy\nNarraCap+gpt4: Fatigue, Engagement, Affection,\nPeace, Pleasure, Doubt/Confusion\nCLIP: Fatigue, Sympathy, Engagement,\nSensitivity, Pleasure, Affection"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "1. NarraCapXL: Sean is a Male Adult. Sean is\na(n) President. Sean is or has Frowning, Raising\na glass of wine. Sean's physical environment is at\na ceremony. Sean is feeling Engagement,\nConfidence, Peace.\nNarraCap: This person is a man who is using an\ninhaler in a black-tie event. He is drinking. He is\ngiving a toast or praise.\nGT: Anticipation, Confidence, Engagement\nEMOTIC: Annoyance, Anticipation, Confidence,\nDisquietment, Doubt/Confusion, Engagement,\nEsteem, Happiness, Sympathy\nNarraCap+gpt4: Confidence, Engagement,\nAnticipation, Excitement, Happiness, Peace,\nEsteem\nCLIP: Sympathy, Disapproval, Surprise,\nEmbarrassment, Doubt/Confusion, Suffering"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "2.NarraCapXL: Sean is a male adult. Sean is\na(n) soccer player. Sean is or has mouth wide\nopen, squeezing eyes shut, arms reaching out,\nlegs moving fast. Sean's physical environment is\nin a soccer game and has just scored a goal.\nSean is feeling Excitement, Happiness,\nConfidence, Surprise, Anger , Sensitivity,\nAnticipation.\nNarraCap: This person is a man who is doing\nhurling (sport) in a sporting event stands. He is\ntugging down the sleeves.\nGT: Confidence, Excitement, Happiness, Pain,\nPleasure\nEMOTIC: Anticipation, Confidence,\nEmbarrassment, Engagement, Excitement\nNarraCap+gpt4: Engagement, Anticipation,\nExcitement, Confidence, Fatigue\nCLIP: Confidence, Sympathy, Excitement,\nSuffering, Anticipation, Embarrassment"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "4.NarraCapXL: Chloe is a female adult. Chloe is\nor has smiling, open mouth, leaning forward,\nlooking up. Karl is an adult and playing volleyball\nwith Chloe. Chloe's physical environment is on a\nbeach. Chloe is feeling Confidence, Excitement,\nHappiness, Engagement, Pleasure.\nNarraCap: This person is a girl who is playing\nvolleyball in a beach party. She has jumpiness.\nShe is participating in relaxing activities.\nGT: Anticipation, Confidence, Engagement,\nExcitement, Happiness, Pleasure\nEMOTIC: Affection, Anticipation, Confidence,\nEngagement, Excitement, Happiness, Pleasure,\nSensitivity, Surprise, Sympathy\nNarraCap+gpt4: Excitement, Happiness,\nAnticipation, Engagement, Pleasure, Confidence,\nSurprise\nCLIP: Engagement, Surprise, Confidence,\nSensitivity, Disapproval, Esteem"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "5.NarraCapXL: Sean is a Male Adult. Sean is\na(n) Runner. Sean is or has Staring down at the\nground, Breathing excessively, Bent spine,\nSweeping hand across the forehead to get rid of\nsweat. Sean's physical environment is in a park.\nSean is feeling Fatigue, Disquietment,\nDoubt/Confusion, Suffering.\nNarraCap: This person is a boy who is drinking in\na water park. He is pulling down glasses and\nlooking over the rims.\nGT: Annoyance, Disquietment, Doubt/Confusion,\nFatigue, Pain\nEMOTIC: Affection, Anticipation, Disquietment,\nEngagement, Esteem, Excitement, Fatigue,\nHappiness, Pleasure, Sensitivity, Sympathy\nNarraCap+gpt4: Excitement, Anticipation,\nPleasure, Happiness, Engagement\nCLIP: Engagement, Disconnection, Confidence,\nEsteem, Sensitivity, Fatigue"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "6.NarraCapXL: Sean is a male adult. Sean is\na(n) business professional. Sean is or has closed\neyes, tilting head downward, sleeping. Sean's\nphysical environment is on a table with a plate of\nfood. Sean is feeling Fatigue, Peace.\nNarraCap: This person is a man who is dining in\na casual dining restaurant. He is laying the head\ndown on the table.\nGT: Disconnection, Disquietment, Fatigue\nEMOTIC: Affection, Anticipation, Engagement,\nExcitement, Happiness, Peace, Pleasure\nNarraCap+gpt4: Suffering, Pain, Fatigue,\nDisquietment, Doubt/Confusion, Embarrassment,\nDisconnection, Sadness\nCLIP: Sympathy, Aversion, Suffering, Sensitivity,\nDisquietment, Fatigue"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "1a: This person is a girl who is sign language\ninterpreting in a therapist office. She is choosing the\nmiddle, not the sides (be it a couch or a room). She\nis exchanging knowing looks with others. She is\nspeed-talking with others, heads close together,\ngossipy. She is straining to hear, shushing others to\nbe quiet.\n1b: Chloe is a female adult. Chloe is or has taking\noff eyeglasses. Mia is Chloe's daughter and she is\nbeing comforted by Chloe. Chloe's physical\nenvironment is on a couch."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "2a: This person is a girl who is\nhuddling in a morgue. She is\ncrying, wailing, begging for help.\n2b: Chloe is a Female Adult. Chloe\nis or has Frowning, Crying,\nCovering face with hands. Chloe's\nphysical environment is on the\nstreet, surrounded by women\ncrying."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "3a: This person is a baby boy who is\nskateboarding in a skate park. He has\nhands laying in the lap. He is rubbing\none's hands on pant legs.\n3b: Terry is a male adult. Terry is or has\nsmiling, sitting. Alan is Terry's friend and\nhe is smiling and sitting beside Terry.\nTerry's physical environment is on a bench\nin a street."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "4a: This person is a woman who is\nhaving a pillow fight in a wake. She\nis retreating into the fetal position or\ncurling up. She is tossing and\nturning in bed, an inability to sleep.\n4b: Sophia is a female adult. Sophia\nis or has frowning, mouth wide open,\nhands covering ears. Sophia's\nphysical environment is on a bed\nwith pillows."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Terry is a male adult. Terry is or has\ngoggles, red jacket, black pants,\ngray gloves, ski poles. Alan is\nTerry's friend and he is skiing next to\nTerry. Terry's physical environment\nis on a ski slope."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Terry is a male adult. Terry is or has\nsmiling, goggles, blue snowsuit, ski\npoles, skiis. Alan is Terry's friend\nand he is skiing with Terry. Terry's\nphysical environment is on a ski\nslope."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Jane is a Female Adult. Jane is or\nhas Gaze clouding, going distant,\nEmpty stare, Sitting, Keeping one's\nback to a wall. Karl is a doctor and\npushing a hospital bed. Alec is a\nnurse and pushing a hospital bed\nwith Alec. Jane's physical\nenvironment is in a hospital hallway."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Jane is a female adult. Jane is or\nhas looking away, sitting, crossed\narms. Karl is a doctor and pushing a\nhospital bed. Alec is a nurse and\ntaking down notes. Jane's physical\nenvironment is in a hospital hallway."
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Thinking, fast and slow",
      "authors": [
        "K Daniel"
      ],
      "year": "2017",
      "venue": "Thinking, fast and slow"
    },
    {
      "citation_id": "2",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "3",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological science in the public interest"
    },
    {
      "citation_id": "4",
      "title": "Expert system for automatic analysis of facial expressions",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2000",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "5",
      "title": "Recognizing emotions expressed by body pose: A biologically inspired neural model",
      "authors": [
        "K Schindler",
        "L Van Gool",
        "B Gelder"
      ],
      "year": "2008",
      "venue": "Neural networks"
    },
    {
      "citation_id": "6",
      "title": "Context in emotion perception",
      "authors": [
        "L Barrett",
        "B Mesquita",
        "M Gendron"
      ],
      "year": "2011",
      "venue": "Current directions in psychological science"
    },
    {
      "citation_id": "7",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "8",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "9",
      "title": "Global-local attention for emotion recognition",
      "authors": [
        "N Le",
        "K Nguyen",
        "A Nguyen",
        "B Le"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "10",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Contextdependent emotion recognition",
      "authors": [
        "Z Wang",
        "L Lao",
        "X Zhang",
        "Y Li",
        "T Zhang",
        "Z Cui"
      ],
      "year": "2022",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "12",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning",
      "arxiv": "arXiv:2304.08485"
    },
    {
      "citation_id": "13",
      "title": "Social cognitive neuroscience: a review of core processes",
      "authors": [
        "M Lieberman"
      ],
      "year": "2007",
      "venue": "Annu. Rev. Psychol"
    },
    {
      "citation_id": "14",
      "title": "Dual-processing accounts of reasoning, judgment, and social cognition",
      "authors": [
        "J Evans"
      ],
      "year": "2008",
      "venue": "Annu. Rev. Psychol"
    },
    {
      "citation_id": "15",
      "title": "Representativeness revisited: Attribute substitution in intuitive judgment",
      "authors": [
        "D Kahneman",
        "S Frederick"
      ],
      "year": "2002",
      "venue": "Heuristics and biases: The psychology of intuitive judgment"
    },
    {
      "citation_id": "16",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "17",
      "title": "Facial emotion recognition using deep learning: review and insights",
      "authors": [
        "W Mellouk",
        "W Handouzi"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "18",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "19",
      "title": "Peri: Part aware emotion recognition in the wild",
      "authors": [
        "A Mittel",
        "S Tripathi"
      ],
      "year": "2022",
      "venue": "Computer Vision-ECCV 2022 Workshops: Tel"
    },
    {
      "citation_id": "20",
      "title": "Computational models of emotion inference in theory of mind: A review and roadmap",
      "authors": [
        "D Ong",
        "J Zaki",
        "N Goodman"
      ],
      "year": "2019",
      "venue": "Topics in cognitive science"
    },
    {
      "citation_id": "21",
      "title": "The role of language in emotion: Predictions from psychological constructionism",
      "authors": [
        "K Lindquist",
        "J Maccormack",
        "H Shablack"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "22",
      "title": "Putting feelings into words",
      "authors": [
        "M Lieberman",
        "N Eisenberger",
        "M Crockett",
        "S Tom",
        "J Pfeifer",
        "B Way"
      ],
      "year": "2007",
      "venue": "Psychological science"
    },
    {
      "citation_id": "23",
      "title": "Language and the perception of emotion",
      "authors": [
        "K Lindquist",
        "L Barrett",
        "E Bliss-Moreau",
        "J Russell"
      ],
      "year": "2006",
      "venue": "Emotion"
    },
    {
      "citation_id": "24",
      "title": "What's in a word? language constructs emotion perception",
      "authors": [
        "K Lindquist",
        "M Gendron"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "25",
      "title": "Emotion words shape emotion percepts",
      "authors": [
        "M Gendron",
        "K Lindquist",
        "L Barsalou",
        "L Barrett"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "26",
      "title": "Towards reasoning in large language models: A survey",
      "authors": [
        "J Huang",
        "-C Chang"
      ],
      "year": "2023",
      "venue": "61st Annual Meeting of the Association for Computational Linguistics, ACL 2023"
    },
    {
      "citation_id": "27",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Neural theory-of-mind? on the limits of social intelligence in large lms",
      "authors": [
        "M Sap",
        "R Lebras",
        "D Fried",
        "Y Choi"
      ],
      "year": "2022",
      "venue": "Neural theory-of-mind? on the limits of social intelligence in large lms",
      "arxiv": "arXiv:2210.13312"
    },
    {
      "citation_id": "29",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Human language understanding & reasoning",
      "authors": [
        "C Manning"
      ],
      "year": "2022",
      "venue": "Daedalus"
    },
    {
      "citation_id": "31",
      "title": "Reasoning with language model prompting: A survey",
      "authors": [
        "S Qiao",
        "Y Ou",
        "N Zhang",
        "X Chen",
        "Y Yao",
        "S Deng",
        "C Tan",
        "F Huang",
        "H Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "32",
      "title": "From show to tell: A survey on deep learning-based image captioning",
      "authors": [
        "M Stefanini",
        "M Cornia",
        "L Baraldi",
        "S Cascianelli",
        "G Fiameni",
        "R Cucchiara"
      ],
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "33",
      "title": "Visual7w: Grounded question answering in images",
      "authors": [
        "Y Zhu",
        "O Groth",
        "M Bernstein",
        "L Fei-Fei"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "Senticap: Generating image descriptions with sentiments",
      "authors": [
        "A Mathews",
        "L Xie",
        "X He"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "35",
      "title": "Semstyle: Learning to generate stylised image captions using unaligned text",
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "36",
      "title": "Visualgpt: Dataefficient adaptation of pretrained language models for image captioning",
      "authors": [
        "J Chen",
        "H Guo",
        "K Yi",
        "B Li",
        "M Elhoseiny"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "37",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "38",
      "title": "Universal captioner: Inducing content-style separation in vision-and-language model training",
      "authors": [
        "M Cornia",
        "L Baraldi",
        "G Fiameni",
        "R Cucchiara"
      ],
      "year": "2021",
      "venue": "Universal captioner: Inducing content-style separation in vision-and-language model training",
      "arxiv": "arXiv:2111.12727"
    },
    {
      "citation_id": "39",
      "title": "The unreasonable effectiveness of clip features for image captioning: an experimental analysis",
      "authors": [
        "M Barraco",
        "M Cornia",
        "S Cascianelli",
        "L Baraldi",
        "R Cucchiara"
      ],
      "year": "2022",
      "venue": "The unreasonable effectiveness of clip features for image captioning: an experimental analysis"
    },
    {
      "citation_id": "40",
      "title": "What does a platypus look like? generating customized prompts for zero-shot image classification",
      "authors": [
        "S Pratt"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "41",
      "title": "Test-time prompt tuning for zero-shot generalization in vision-language models",
      "authors": [
        "M Shu"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "42",
      "title": "Vt-clip: Enhancing vision-language models with visualguided texts",
      "authors": [
        "L Qiu"
      ],
      "year": "2021",
      "venue": "Vt-clip: Enhancing vision-language models with visualguided texts",
      "arxiv": "arXiv:2112.02399"
    },
    {
      "citation_id": "43",
      "title": "Open-vocabulary object detection via vision and language knowledge distillation",
      "authors": [
        "X Gu"
      ],
      "year": "2021",
      "venue": "Open-vocabulary object detection via vision and language knowledge distillation",
      "arxiv": "arXiv:2104.13921"
    },
    {
      "citation_id": "44",
      "title": "F-vlm: Open-vocabulary object detection upon frozen vision and language models",
      "authors": [
        "W Kuo"
      ],
      "year": "2022",
      "venue": "F-vlm: Open-vocabulary object detection upon frozen vision and language models",
      "arxiv": "arXiv:2209.15639"
    },
    {
      "citation_id": "45",
      "title": "Region-aware pretraining for open-vocabulary object detection with vision transformers",
      "authors": [
        "D Kim"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "46",
      "title": "Fine-grained visual-text prompt-driven self-training for open-vocabulary object detection",
      "authors": [
        "Y Long"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "47",
      "title": "Contextual emotion recognition using large vision language models",
      "authors": [
        "Y Etesam",
        "Ö Yalc ¸ın",
        "C Zhang",
        "A Lim"
      ],
      "year": "2024",
      "venue": "Contextual emotion recognition using large vision language models",
      "arxiv": "arXiv:2405.08992"
    },
    {
      "citation_id": "48",
      "title": "Expansionnet v2: Block static expansion in fast end to end training for image captioning",
      "authors": [
        "J Hu",
        "R Cavicchioli",
        "A Capotondi"
      ],
      "year": "2022",
      "venue": "Expansionnet v2: Block static expansion in fast end to end training for image captioning",
      "arxiv": "arXiv:2208.06551"
    },
    {
      "citation_id": "49",
      "title": "A short note on the kinetics-700-2020 human action dataset",
      "authors": [
        "L Smaira",
        "J Carreira",
        "E Noland",
        "E Clancy",
        "A Wu",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "A short note on the kinetics-700-2020 human action dataset",
      "arxiv": "arXiv:2010.10864"
    },
    {
      "citation_id": "50",
      "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "authors": [
        "K Soomro",
        "A Zamir",
        "M Shah"
      ],
      "year": "2012",
      "venue": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "arxiv": "arXiv:1212.0402"
    },
    {
      "citation_id": "51",
      "title": "Hmdb: a large video database for human motion recognition",
      "authors": [
        "H Kuehne",
        "H Jhuang",
        "E Garrote",
        "T Poggio",
        "T Serre"
      ],
      "year": "2011",
      "venue": "Hmdb: a large video database for human motion recognition"
    },
    {
      "citation_id": "52",
      "title": "The emotion thesaurus: A writer's guide to character expression",
      "authors": [
        "B Puglisi",
        "A Ackerman"
      ],
      "year": "2019",
      "venue": "The emotion thesaurus: A writer's guide to character expression"
    },
    {
      "citation_id": "53",
      "title": "The Urban Setting Thesaurus: A Writer's Guide to City Spaces",
      "year": "2016",
      "venue": "The Urban Setting Thesaurus: A Writer's Guide to City Spaces"
    },
    {
      "citation_id": "54",
      "title": "The Rural Setting Thesaurus: A Writer's Guide to Personal and Natural Places",
      "year": "2016",
      "venue": "The Rural Setting Thesaurus: A Writer's Guide to Personal and Natural Places"
    },
    {
      "citation_id": "55",
      "title": "Contextual emotion estimation from image captions",
      "authors": [
        "V Yang",
        "A Srivastava",
        "Y Etesam",
        "C Zhang",
        "A Lim"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "56",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "57",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "58",
      "title": "High-level context representation for emotion recognition in images",
      "authors": [
        "W De Lima Costa",
        "E Talavera",
        "L Figueiredo",
        "V Teichrieb"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "59",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report"
    },
    {
      "citation_id": "60",
      "title": "Towards reasoning in large language models: A survey",
      "authors": [
        "J Huang",
        "-C Chang"
      ],
      "year": "2022",
      "venue": "Towards reasoning in large language models: A survey",
      "arxiv": "arXiv:2212.10403"
    },
    {
      "citation_id": "61",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "62",
      "title": "Regionclip: Region-based language-image pretraining",
      "authors": [
        "Y Zhong",
        "J Yang",
        "P Zhang",
        "C Li",
        "N Codella",
        "L Li",
        "L Zhou",
        "X Dai",
        "L Yuan",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "Contrastive region guidance: Improving grounding in vision-language models without training",
      "authors": [
        "D Wan",
        "J Cho",
        "E Stengel-Eskin",
        "M Bansal"
      ],
      "year": "2024",
      "venue": "Contrastive region guidance: Improving grounding in vision-language models without training",
      "arxiv": "arXiv:2403.02325"
    },
    {
      "citation_id": "64",
      "title": "Llava-grounding: Grounded visual chat with large multimodal models",
      "authors": [
        "H Zhang",
        "H Li",
        "F Li",
        "T Ren",
        "X Zou",
        "S Liu",
        "S Huang",
        "J Gao",
        "L Zhang",
        "C Li"
      ],
      "year": "2023",
      "venue": "Llava-grounding: Grounded visual chat with large multimodal models",
      "arxiv": "arXiv:2312.02949"
    },
    {
      "citation_id": "65",
      "title": "I feel your voice: Cultural differences in the multisensory perception of emotion",
      "authors": [
        "A Tanaka",
        "A Koizumi",
        "H Imai",
        "S Hiramatsu",
        "E Hiramoto",
        "B Gelder"
      ],
      "year": "2010",
      "venue": "Psychological science"
    }
  ]
}