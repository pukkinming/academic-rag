{
  "paper_id": "2407.03418v1",
  "title": "Hemm: Holistic Evaluation Of Multimodal Foundation Models",
  "published": "2024-07-03T18:00:48Z",
  "authors": [
    "Paul Pu Liang",
    "Akshay Goindani",
    "Talha Chafekar",
    "Leena Mathur",
    "Haofei Yu",
    "Ruslan Salakhutdinov",
    "Louis-Philippe Morency"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of realworld applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models. Preprint. Under review.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Building upon rapid progress in large-scale language and vision pretraining  [24, 69, 106] , the new generation of multimodal foundation models is increasing adept at learning interactions between modalities  [83] , enables both static prediction and dynamic interaction  [55] , and even shows emergent properties never seen before in pretraining corpora  [60] . Previous standards for benchmarking multimodal models based on collections of modality and task-specific datasets  [8, 57, 29, 66]  are increasingly insufficient in light of these general capabilities. In order to study fundamental questions regarding why multimodal foundation models exhibit certain behaviors, when they perform well in the real world, and which modeling paradigms are most effective, there is a need for a holistic evaluation scheme beyond individual datasets or contexts.\n\nTo address this need, we contribute Holistic Evaluation of Multimodal Models (HEMM), visualized in Figure  1 . HEMM, as an evaluation framework, goes beyond conventional lists of datasets to emphasize holistic benchmarking at three levels. The first level benchmarks basic multimodal skills: fundamental internal abilities required to address multimodal problems, such as interactions between redundant, unique, and synergistic features  [26, 68] , alignment of fine-grained and coarse-grained information  [104] , reasoning across compositional features  [115] , and integration of external knowledge  [90] . The second level benchmarks information flow: how multimodal information transforms Figure  1 : HEMM is an evaluation framework that characterizes multimodal models along several dimensions (size, architecture, pretraining objective, fine-tuning objective, training data) and emphasizes holistic benchmarking of these models at three disentangled levels: basic skills, information flow, and use cases.\n\nduring tasks such as querying  [98] , translation  [109] , editing  [108] , and fusion  [60] . The third level benchmarks multimodal use cases: how models perform in real-world challenges across domains, including multimedia, affective computing, natural sciences, healthcare, and human-computer interaction (HCI). Together, these three levels taxonomize a wide spectrum of 30 image-text datasets, enabling HEMM to serve as a holistic framework to evaluate multimodal models.\n\nTo aid in HEMM evaluation, we also present a new categorization of models spanning key modeling decisions, such as model size and modality processing (e.g., interleaved inputs), and training decisions, such as pretraining and fine-tuning objectives. We  (1)  identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling and training decisions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence downstream task performance. Our analysis yields tangible directions for future work, including challenging multimodal skills, tasks, and use cases, impacts of diversity and scale, and guidelines on modeling architectures and training objectives. HEMM is publicly available at anon, and encourages community involvement in its expansion of datasets, annotations, models, and evaluation metrics.\n\n2 Key Benchmarking Principles and Datasets in HEMM HEMM includes 30 datasets summarized in Table  1 . These datasets require different multimodal skills to solve, display different types of multimodal information flow, and belong to different real-world use cases with domain-specific challenges.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Basic Multimodal Skills",
      "text": "Multimodal skills are internal abilities required to solve multimodal tasks, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and using external knowledge.\n\nMultimodal interactions study how modality information is integrated for a multimodal task  [69, 77, 52, 9] , which can be redundant: shared between modalities, such as smiling while telling a humorous joke  [43, 89] , unique: present in only one of the modalities  [35, 54] , and synergistic: emergence of new information from both modalities, such as conveying sarcasm through conflicting verbal and nonverbal cues  [15, 68] . Datasets with high referential information between modalities test for redundancy, such as in VQA, and translation on NOCAPS. Tasks with uniqueness or synergy include understanding movie posters (MM-IMDB), memes (MEMECAP), figurative language (IRFL), facial expressions (FER-2013), and cartoons (NEW YORKER CARTOON).\n\nGranularity of multimodal alignment involves identifying alignment across elements in different modalities. For example, answering a question might require a model to perform fine-grained alignment to reference one specific object out of many possible objects in an image. Tasks that explicitly test for fine-grained alignment include localized reasoning on VISUAL GENOME, WINOGROUND, while tasks that emphasize coarse-grained alignment (e.g., making a prediction relevant to a whole image) include interpreting cartoon images  [37] , movie posters  [5] , and memes  [46, 89, 43] .\n\nReasoning and external knowledge involve the combination of local pieces of information to form increasingly rich and complex multimodal representations. For example, being able to perform multi-hop inference from Wikipedia text and images  [76]  or solving science questions given visual diagrams and executing multiple logical steps  [75] . Tasks like WINOGROUND explicitly test for reasoning and tasks like OK-VQA are designed to assess external knowledge.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Information Flow",
      "text": "Multimodal information flow studies how information transforms across tasks, including cross-modal translation, editing, querying, and fusion.\n\nCross-modal translation exploits shared information by mapping data in one modality to another. Examples include translating from text to image for image generation (e.g., LNCOCO) and translating from image to text for image captioning (e.g., NOCAPS, SCREEN2WORDS).\n\nCross-modal editing involves semantically editing data in one modality according to another modality (e.g., given an image, following a natural language instruction to \"change the background from day to night\"). The model takes in the original image (with potentially more reference images), along with a task description specifying the edit, and outputs the edited image. We use the MAGIC BRUSH dataset to test cross-modal editing.\n\nCross-modal querying involves a model's ability to answer natural language questions that query specific information about an input. The model takes in the original image, a text description, the query, and must output the desired answer (typically in natural language). Querying can be done for visual scenes (GQA), environmental indicators (RESISC45), and medical data (VQARAD).\n\nMultimodal fusion aims to learn interactions to combine information from different modalities, such as classifying diseases given x-ray images and medical tests, or detecting humor from cartoon images and captions. Multimodal fusion takes in the image, text, and a description of the task, and then outputs a prediction, which can include affective states like humor in NEW YORKER CARTOON, hate speech detection in HATEFUL MEMES, or in science problems (SCIENCEQA).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Real-World Use Cases",
      "text": "Each use case is drawn from a real-world application with their own specific challenges.\n\nMultimedia includes efficient search, retrieval, indexing, and generation of digital content. Multimedia tasks in HEMM include question answering about images and videos (VQA, VCR), multimedia captioning (FLICKR30K, NOCAPS), compositional visual reasoning (WINOGROUND, NLVR), understanding cartoons, movie posters (MM-IMDB), memes (MEMECAP and MEMOTION), and figurative language (IRFL), and editing images (MAGIC BRUSH).\n\nAffective computing aims to perceive human affective states (emotions, sentiment, personalities, humor, sarcasm, social interactions)  [86] , and is important for building emotionally and sociallyintelligent AI  [56, 78]  and human-AI interaction  [55] . HEMM includes NEW YORKER CARTOON (cartoon images and captions), HATEFUL MEMES (hateful content in memes), FER-2013 for facial expressions, MEMECAP for meme captioning, and MEMOTION for emotions in memes.\n\nNatural sciences aims to deepen our knowledge of physical, chemical, biological, and environmental sciences. These can involve satellite images, chemical bonds, land and agriculture use, wildlife, and specific scientific terminologye  [101] . Tasks in HEMM include SCIENCEQA testing different science topics and RESISC45 for land scene classification.\n\nHealthcare involves integrating multimodal signals such as lab tests, imaging reports, and doctorpatient interactions to help doctors interpret high-dimensional data and assist them in diagnosis  [48, 51] . We include processing text reports and medical images in the form of PATHVQA for pathology, VQARAD for radiology images, and SLAKE for medical visual question answering.\n\nHCI involves user design, usability, user experience, and other challenges related to humans interacting with computers  [81] . HCI tasks can involve visual information such as screen layouts, user actions, and feedback mechanisms. HCI tasks in HEMM include ENRICO for classifying mobile UI designs and SCREEN2WORDS for UI screen content summarization.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Key Modeling Principles And Models In Hemm",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modeling Decisions",
      "text": "Model parameters Parameters can vary greatly across different multimodal models, from 100M params to approximately 1000B params. We consider models with total number of parameters less than or equal to 4B (e.g., INSTRUCT-BLIP) as small, whereas those having more than 4B parameters (e.g., FUYU-8B) are considered medium. GPT-4V and GEMINI are considered large.\n\nModality processing Some multimodal models (e.g., FUYU-8B) support interleaved inputs like \"<dog_img> This is a very cute dog.<cat_img> This is a very cute cat.\", unlike models that only support separate image and text queries (e.g., BLIP-2, MINI-GPT-4).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Characteristics",
      "text": "Training type End-to-end training involves fine-tuning unimodal encoders, pretrained LLMs, and a multimodal model jointly, as seen in EMU, FUYU-8B, etc. Another category operates by freezing",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we discuss extensive experiments conducted to holistically evaluate the performance of multimodal foundation models based on HEMM.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "Individual metrics For all text generation tasks, we use the established natural language generation evaluation metric BARTScore  [121] , which was found to have the highest correlation with human judgement  [121] . We compute BARTScore(r, c), where r is the reference and c is the candidate. It can be interpreted as the probability of generating the candidate sentence from the reference. For example, a model might caption an image with the following generated candidate: A row of violins hanging on a wall.. The reference (ground truth) of A painting of 5 cello's with a green background would be used to compute the BARTScore with respect to c.\n\nAggregating metrics To aggregate scores across multiple tasks or models, we normalize scores using min-max scaling. Following Chang et al.  [16] , min represents the score of the worst multimodal model and max represents the identity score BARTScore(r, r), where r is the ground truth. Subsequently, these normalized scores in a 0 to 1 range can be interpreted as a percentage of model performance relative to the ground truth. shows that the models struggle on tasks requiring complex reasoning, failing to comprehend the relation between the force and the size of the magnets. In (c), all models except GPT-4V are unable to capture the fine-grained details and misclassify the image as an airport instead of a runway.\n\nComputation Since GPT-4V and GEMINI have query limits, we evaluate their performance on 100 random samples for each dataset (2800 total data points). For a fair comparison with other models, we present the results and findings below based on the performance of those 100 samples per dataset.\n\nIn Appendix C we present the results of the other models on the full evaluation sets. We evaluate all the models on a single NVIDIA A100 80GB GPU with the inference time for a single image-text pair ranging from 0.1 seconds to 63.7 seconds. We report the average inference times for the models across all datasets and include additional details on the evaluation protocol in Appendix B.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Main Results",
      "text": "We summarize our main results here and include full details in Appendix C. We first explain performance trends across the datasets in HEMM, before explaining performance differences across different multimodal foundation models and their design decisions. Overall comparisons We summarize overall trends in Figure  3     2 ). Other challenging datasets include INATURAL-IST due to fine-grained visual differences between 5000 species of plants and animals, and healthcare datasets that require intricate analysis of pathology images to identify organs, tissues, and anomalies (see Figure  8 ). Datasets related to memes were also challenging (0.32 and 0.38 on MEMECAP  [43]  and MEMOTION  [89] ), requiring knowledge about current events, pop culture, and metaphors beyond literal meanings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance Across Dataset Dimensions",
      "text": "Multimodal skills 1: Interactions The average scores for redundant, unique, and synergistic interactions are 0.29, 0.20, and 0.33. One reason for lower uniqueness scores is the presence of highly challenging visual datasets like DECIMER and ENRICO. On average, the easiest tasks in redundancy are NLVR (0.  Multimodal skills 2: Granularity We do not find that fine-grained datasets are significantly harder than those with coarse-grained alignment. Tasks requiring fine-grained alignment between image and text like GQA and WINOGROUND achieve a score of 0.26, while those only needing coarse-grained alignment (e.g., ENRICO, SCIENCEQA) are still quite challenging (score: 0.27).\n\nMultimodal skills 3: Reasoning We do not find a significant difference between the performance of the models on tasks requiring more (average score = 0.275) or less reasoning (average score = 0.268). The most challenging datasets requiring less reasoning include INATURALIST (0.08) and ENRICO (0.12) due to challenges in fine-grained visual perception and external knowledge, while there are also several challenging datasets requiring more complex reasoning like VCR (0.34) and MEMECAP (0.14), where the models encounter difficulties with samples requiring commonsense and compositional reasoning (See Figure  4  for examples).\n\nMultimodal skills 4: External knowledge The average performance on tasks requiring external knowledge is 0.23, compared to 0.30 for those not requiring external knowledge. For example, INSTRUCT-BLIP performs well on WINOGROUND and VCR that do not require external knowledge but struggles more on knowledge-intensive tasks e.g., INATURALIST, which requires knowledge about characteristics of vast number of species, and SLAKE, where medical knowledge is needed to identify the abnormalities in organs.\n\nMultimodal Skills 5: Information flow Translation has the lowest average score amongst all types of information flow (0.19), whereas the average scores on querying and fusion are 0.26 and 0.33 respectively. The low performance on translation is due to the presence of challenging datasets like DECIMER and SCREEN2WORDS requiring mapping images of chemicals and screenshots into text.\n\nAlthough the average score for fusion is high, the performance on some datasets is still quite low, such as INSTRUCT-BLIP achieving a score of only 0.04 on MEMECAP and 0.15 on MM-IMDB.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance Across Modeling Dimensions",
      "text": "We now compare different modeling decisions and training objectives in Table  4 .\n\nOverall comparisons across models GEMINI  [97]  (0.44), INSTRUCT-BLIP  [22]  (0.41), BLIP-2  [62]  (0.41), and GPT-4V  [1]  (0.40) achieve the best average performance across all tasks. The low scores of GPT-4V as compared to GEMINI and INSTRUCT-BLIP are due to its generation of keywords like \"Indeterminate\", \"Uncertain\", and \"Unknown\" on datasets like VQA and GQA, perhaps due to its alignment process. Further, on some datasets related to Memes (e.g., HATEFUL MEMES) and Health (e.g., SLAKE), GPT-4V refrains from answering the questions and instead generates a response saying Cannot assist with the request. OPENFLAMINGO  [6]  (0.06), EMU  [95]  (0.11) have the lowest average scores. From their generations, we find that these models struggle to follow the instructions for challenging datasets like DECIMER and ENRICO, and generate hallucinated responses. Moreover, with relatively easier datasets such as FLICKR30K, the captions produced by EMU and OPENFLAMINGO tend to fixate on specific objects rather than providing a comprehensive description of the scene, often leading to instances of hallucination related to these objects. As a result, these models rank lowest on many datasets, receiving a normalized score of 0. Model scale We find that the performance of larger models (both total and trainable parameters) is significantly better than the models with a medium or small number of parameters (Figure  5 ). When grouped based on the total number of parameters, the average scores achieved by large, medium, and small models are 0.42, 0.24, and 0.23 respectively. The difference between the performance of large and medium models is significant (p-value for paired t-Test < 0.001). In particular, large models showed the most improvement on MM-IMDB, MEME-CAP, and HATEFUL MEMES datasets, which fall into the category of tasks requiring synergistic interactions. On average, the large models perform the best on synergistic tasks with a score of 0.53 compared to 0.30 for medium and 0.23 for small models. For instance, on the MM-IMDB dataset, we observed significant gains in performance when increasing model size: from 0.15 for INSTRUCT-BLIP (small) to 0.36 for BLIP-2 (medium) and 0.48 for GEMINI (large).\n\nPretraining data scale Average scores of the models in large and medium data size categories are 0.31 and 0.30 respectively, whereas models with small pretraining data achieve a significantly lower score of 0.17. We also find that for all datasets, the average score of models with medium pretraining data is higher than the models with small pretraining data. For instance, on the WINOGROUND dataset which requires fine-grained alignment between the modalities, the maximum scores achieved by the models with medium and small pretraining data are 0.45 and 0.80. We also find a significant gap between the maximum scores achieved by the models in the medium (maximum score -0.18) and small categories (maximum score -0.70), on the NLVR2 dataset for visual reasoning.\n\nDiversity of pre-training data On average, models trained on diverse datasets perform better (score: 0.30) than models trained only on image captioning datasets (score: 0.21). Diverse training data allows the models to share learned knowledge and generalize across different tasks. For example, models pretrained with diverse datasets perform significantly better on the knowledge-intensive INATURALIST task, such as BLIP-2 (non-diverse) scoring 0.08 and GEMINI scoring 0.24. For the MEMECAP dataset which requires external knowledge and complex reasoning, we observe that BLIP-2 (non-diverse) scores 0.06 and MPLUG-OWL (diverse) scores 0.21.\n\nInstruction tuning vs supervised fine-tuning On average, instruction-tuned models (average score of 0.30) performed better than the models trained using only supervised fine-tuning (average score of 0.22). The top 3 tasks with the largest performance gap between instruction-tuned and non-instruction-tuned models are DECIMER, MEMECAP, and SCREEN2WORDS, with improvements of 0.15, 0.09, and 0.09 respectively. We also observe that translation tasks (image-to-text) (e.g., FLICKR30K, NOCAPS) benefit from instruction tuning, where the models generate more accurate and detailed captions after human instruction.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Human Evaluation",
      "text": "To assess how well HEMM aligns with human preferences, we performed human preference-based evaluation, following Chiang et al.  [19] , where annotators are shown the outputs of two different models for the same inputs and choose the better output or a tie option. Across 1000 pairwise comparisons by 5 annotators, the pairwise rankings are used to calculate each model's average win rate and Elo rating (see Appendix B.5 for calculation details).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal machine learning brings unique challenges for ML research due to the heterogeneity between modalities and the interconnections found between them  [69] . It has inspired many theoretical studies in data heterogeneity and interactions  [25] , as well as diverse applications in multimedia  [44, 14, 88] , affective computing  [86] , robotics  [47] , finance  [39] , HCI  [25, 82] , education  [12]  and healthcare  [80, 110] .\n\nEvaluation frameworks for multimodal models have significantly shaped the multimodal research landscape, through holistic  [57, 66]  and domain-specific benchmarks  [31, 28] . Recent benchmarks have focused on testing the capabilities of multimodal foundation models, such as MME  [29] , MMBench  [73] , LVLM-ehub  [111] , SEED-Bench  [59] , Touchstone  [7] , Mm-vet  [120] , ReForm-Eval  [65] , VisIT-Bench  [11] , FLAVA  [45] . Other benchmarks focus on evaluating hallucination  [21]  and applications in medicine  [113]  and autonomous driving  [107] . These benchmarks contain many tasks, but without the systematic taxonomy and comprehensiveness that HEMM provides.\n\nMultimodal foundation models are promising foundations for the future of AI, with impressive reasoning  [75] , interactive dialogue  [49] , and few-shot generalization abilities  [100] . These models can be pre-trained (typically with image-text self-supervised learning) and fine-tuned for downstream tasks  [63, 74, 91, 67] , or based on adapting language models with vision to enable text generation conditioned on images  [61, 105] . Cross-modal transformer architectures have emerged as a popular backbone due to their suitability for both language and image data  [17, 99] . Additionally, composable diffusion models  [96]  can be used to further generate combinations of output modalities.\n\nAdapting language models for multimodality is another promising approach where frozen models are aligned on both vision and language to generate text from multimodal inputs  [127, 62, 118, 109] . These approaches typically use parameter-efficient modules like LLaMA-Adapter V2  [30]  and MAGMA  [27]  for efficient finetuning. Vision-language instruction tuning has also emerged as a useful technique, as it allows the models to better follow human instructions  [112, 127] . Our goal is to make HEMM the most comprehensive benchmark to study the current and future generation of multimodal foundation models, and for the community to continuously contribute to its expansion.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "Holistic Evaluation of Multimodal Models (HEMM) is a framework for benchmarking multimodal foundation models. Through a new taxonomy of multimodal skills, information flow, and real-world use cases, HEMM enables comprehensive analysis of multimodal models. HEMM is publicly available, will be regularly updated, and encourages community involvement in its expansion.\n\nLimitations and social impact The evaluation of multimodal models is done only on a subset of all possible skills, information, and use cases in the world. Future work can improve the categorization of datasets into skills, information, and use cases, and discover new dimensions that pose challenges to multimodal models. Such evaluation is critical to ensure that models are sufficiently robust when deployed in real-world scenarios, to prevent unexpected and unintended consequences. Future work should also add new metrics to HEMM measuring real-world societal concerns such as fairness, robustness, social biases, privacy, and efficiency of multimodal models. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\n\nWe have included full links to the dataset, models, and code in the supplementary material.\n\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] We provide access information of the datasets in Appendix A.1.\n\nTo the best of our knowledge, all of these datasets are collected with consent from participating users, especially in the healthcare domain where user data is sensitive.\n\nBest practices for de-identification of user data were followed by these datasets. The dataset for facial expression recognition (FER-2013) contains human faces collected through Google image search queries, so user consent was not directly obtained, but the authors of FER-2013 have ensured that their dataset follows fair use guidelines and there is no personally identifiable information released.\n\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] Yes, we have included all dataset details in Appendix A.1 including if the individual datasets in HEMM contain personally identifiable information or offensive content. To the best of our knowledge, all potentially identifiable information in all datasets (especially in those from medical settings or human social data) has been removed and completely de-identified. The dataset for facial expression recognition (FER-2013) contains human faces collected through Google image search queries, but does not contain any identifying information about user identities and backgrounds. Finally, the Hateful Memes dataset contains offensive content, since that is the goal of the research.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The dataset is available to download from https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Licenses:",
      "text": "The dataset is available under CC0: Public Domain License https://creativecommons.org/publicdomain/zero/1.0/deed.en Ethical considerations: No personally identifiable information or offensive content is present in the dataset.\n\n14. FER-2013 is a classic dataset for facial expression recognition, where each image has to be classified into 7 labels. Images for this dataset were obtained from Google images, by searching them using Google Search API. OpenCV was used to get bounding boxes for faces in each of the images.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Split:",
      "text": "We evaluate on the test dataset present in https://www.kaggle.com/datasets/msambare/fer2013\n\nPrompt used: Given the photo of a face, determine the face expression, choose from the following choices: angry, disgust, fear, happy, neutral, sad, surprise. Answer in a single word.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The dataset is available to download from https://www.kaggle.com/datasets/msambare/fer2013\n\nLicenses: No license is provided with the dataset Ethical considerations: This dataset contains human faces collected through Google image search queries but does not contain any identifying information about user identities and backgrounds. No offensive content is present in the dataset.  15 . NY CARTOON is collected from the weekly New Yorker magazine cartoon captioning contest  3  , where readers are tasked to give a humorous caption for a cartoon image and the funniest captions are selected based on public votes. The dataset is formulated based on taking in the image and caption to predict how funny the pair is based on the normalized number of votes. Given an image and its caption, we ask the model if the caption is humorous or not. Each image has multiple caption choices with votes for the caption being not funny, somewhat funny, funny. We select the funniest caption to have a ground truth answer as 'yes' when prompted for evaluation. The next four funniest captions are selected to have ground truth answers as 'no' when prompted for evaluation. 17. MAGIC BRUSH is an instruction-based image editing dataset consisting of manually annotated images consisting of single-turn and multi-turn instruction-guided editing. Images are sampled from MS COCO  [71]  dataset and are annotated using DALL-E 2  5  with the help of crowdworkers from Amazon Mechanical Turk (AMT)  6  . For our evaluation, we follow a single-turn instruction editing.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Split",
      "text": "Split 18. MEMECAP is a meme captioning dataset, whose images have been taken from the subreddit r/memes  7  . The captions for these images are generated in a two-round process by human annotators using Amazon Mechanical Turk. For our evaluation process, we provide the model with the image description and title of the meme and ask what the meme is trying to convey.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Split:",
      "text": "We evaluate on the test set from https://github.com/eujhwang/meme-cap/tree/main\n\nPrompt used: This is a meme with the title <title>. The image description is <im-age_description>. What is the meme poster trying to convey? Answer:",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The dataset can be downloaded from https://github.com/eujhwang/meme-cap/tree/main Licenses: No license is available for the dataset.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Ethical Considerations:",
      "text": "No personally identifiable information is present. However, offensive content may be present in the images due to the dataset containing meme data.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Hateful Memes Was A Challenge Hosted By Meta To Classify If A Meme Image Along",
      "text": "with its text caption describes hateful intentions. Images were obtained from Getty images 8  annotated by a third-party annotation platform. Here, an image and text are provided to the model to ask if the image promotes hateful sentiments.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Splits:",
      "text": "We use the 'dev' split from https://www.kaggle.com/datasets/parthplc/facebookhateful-meme-dataset/data\n\nPrompt used: You are given an image. In the image, the text phrase that you will be given and the image are innocuous when considered by themselves. The semantic content of the meme becomes mean only when the text phrase and image are considered together. Text phrase: <text_phrase> You have to judge if the combination of image and text is hateful or not. Always begin your answer with either 'yes' or 'no' with 'yes' indicating that the meme is hateful and 'no' if it is not hateful. Answer:\n\nEthical considerations: No personally identifiable information or offensive content is present in the dataset.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "23.",
      "text": "VCR tests commonsense reasoning skills in question answering over images. Still images are extracted from movie clips, and annotations are crowdsourced using Amazon Mechanical Turk where each worker is provided an image along with detailed video captions to collect questions, answers, and rationales for an image Split: 'val' split is used from https://visualcommonsense.com/download/ Prompt used: Question: <question> Choose from the below choices: <choices>",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The dataset is available to download from https://visualcommonsense.com/download/ Licenses: The dataset is provided in license as https://visualcommonsense.com/license/ Ethical considerations: No personally identifiable information or offensive content is present in the dataset.  24 . WINOGROUND is a dataset for visual linguistic compositional reasoning involving images from Getty Images and annotations given by four expert annotators. The original task consists of matching images and captions for a pair of two images and captions. We transform this task by creating a total of four data points for each pair by pairing each caption, with each image which leads to two correct and two wrong pairs per data point. We then ask the model to see if the caption matches the pair or not.\n\nSplit: Test set from https://huggingface.co/datasets/facebook/winoground is used.\n\nPrompt used: You are given an image and a text. Answer yes if the text matches the image and no if the text does not match the image. Text: <text> Answer:",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The dataset is downloaded from https://huggingface.co/datasets/facebook/winoground Licenses: Authors of the dataset have Getty image license https://www.gettyimages.in/eula.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Ethical Considerations:",
      "text": "No personally identifiable information or offensive content is present in the dataset.  25 . RESISC45 is a land use dataset that involves land scene classification of images over 45 classes. The images for this dataset have been taken from Google Earth by experts in remote sensing image interpretation. We add all 45 classes to the prompt and let the model choose the class from the prompt itself.\n\nSplit: We use the dataset from https://www.kaggle.com/datasets/happyyang/nwpu-data-set\n\nPrompt used: Image is given to you. Classify if the image belongs to one of the following classes: 'basketball_court', 'overpass', 'ground_track_field', 'church', 'chaparral', 'forest', 'parking_lot', 'golf_course', 'baseball_diamond', 'meadow', 'beach','sparse_residential', 'desert', 'terrace', 'palace', 'bridge', 'commercial_area', 'stadium', 'runway', 'lake', 'railway', 'tennis_court', 'ship', 'intersection', 'river', 'freeway', 'airplane', 'industrial_area', 'mountain', 'storage_tank', 'cloud', 'roundabout', 'wetland', 'mobile_home_park', 'island', 'harbor', 'railway_station', 'medium_residential', 'sea_ice', 'thermal_power_station', 'snowberg', 'circular_farmland', 'airport', 'dense_residential', 'rectangular_farmland'. Choose a class from the above classes.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The dataset is available to downloaded from https://www.kaggle.com/datasets/happyyang/nwpu-data-set Licenses: No license is provided with the dataset.\n\nEthical considerations: No personally identifiable information or offensive content is present in the dataset.  26 . GQA builds up on Visual Genome scene graph structures for reasoning questions. It consists of real-world reasoning, scene understanding, and compositional question answering. Questions are generated using a robust engine which makes sure that the questions are grounded in the image. Each question is associated with a series of steps that need to be followed to get the answer as well as a scene graph that captures objects, attributes, and relations in the image Split: We use the test split available from https://cs.stanford.edu/people/dorarad/gqa/download.html\n\nPrompt used: You are given an image and a question. Answer the question in a single word. Question: <question>",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The dataset is available to download from https://cs.stanford.edu/people/dorarad/gqa/download.html Licenses: The images in the dataset come with a CC BY 4.0 DEED license https://creativecommons.org/licenses/by/4.0/deed.en 27. OPENPATH is a dataset created from Twitter and other public sources. Each image has a natural language description, and the dataset is sourced from tweets across 32 hashtag sub-specialty categories in pathology.\n\nSplit: We use the test split for evaluation.\n\nPrompt used: Choose from the below choices, Given image is a hematoxylin and eosin image of: cancer-associated stroma, adipose tissue, debris, lymphocytes, mucus, background, normal colon mucosa, colorectal adenocarcinoma epithelium, smooth muscle Access restrictions: The dataset is available to download from huggingface datasets https://huggingface.co/datasets/akshayg08/OpenPath",
      "page_start": 19,
      "page_end": 27
    },
    {
      "section_name": "Licenses:",
      "text": "The dataset is available under CC BY-NC 4.0 license. https://creativecommons.org/licenses/by-nc/4.0/ Ethical considerations: No personally identifiable information or offensive content is present in the dataset.  28 . IRFL is an image-text dataset for figurative language. The dataset consists of three broad categories: idioms, similes, and metaphors. Metaphors and similes were collected from online lists whereas idioms were collected from MAGPIE corpus  [33] . Since the MAGPIE corpus did not contain definitions for idioms, definitions were crawled from online dictionaries to search for figurative images. Google images were used for searching the images for idioms using these definitions. For similes and metaphors, annotators were used for definitions, and images were searched on the internet. For our evaluation, we use simile categorization. For each data point, one simile and four images are given. We modify this task to evaluate one image at a time, so a pair of an image and similes are passed to the model to see if they match or not.\n\nSplit: We use the Simile understanding task for evaluation.\n\nPrompt used: You are given a simile and a picture along with the simile. You have to say if the simile matches the given picture. Answer the following question in a single word with a yes or no. Simile: <simile> Answer:\n\nAccess restrictions: Dataset is available for download from https://github.com/irfldataset/IRFL\n\nLicenses: No license is provided with the dataset.\n\nEthical considerations: No personally identifiable information or offensive content is present in the dataset.\n\n29. SCREEN2WORDS is a mobile UI summarization dataset consisting of images from Rico-SCA  [64]  dataset. A total of 85 annotators were used to describe the image.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Split:",
      "text": "We use the test split from https://github.com/google-researchdatasets/screen2words/tree/main Prompt used: You are given a phone UI screen. Describe the screen in one sentence.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The dataset is available to download from https://github.com/googleresearch-datasets/screen2words   We present our results on BARTScore  [121]  as models under our evaluation generate noisy free from text, however, we also support other text generation metrics under our evaluation suite listed in the Table  11  below.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "B.2 Evaluation Protocol",
      "text": "HEMM supports image generation tasks, models and metrics. However, currently there are only 2 image generation tasks (LNCOCO and MAGIC BRUSH) and 1 model (EMU) that supports image generation. Hence, we perform all our evaluation on the remaining 28 text generation tasks and report the results on the image generation tasks in Appendix C.\n\nNote: Since HEMM contains models that are unable to process multiple images in the same input, we modify the WINOGROUND and IRFL tasks (as per A.1) in order to have a single image-text pair as input for each sample.\n\nFor each dataset, we use the same prompts across all models as shown in Section C, for standardization, however, there can be a scenario where these models perform better with other prompts or scenarios and may perform poorly under our scenarios or prompts in our evaluation.\n\nFor each dataset, the computed metrics for the models are normalized on a scale of 0 to 1, 0 corresponds to the model achieving the lowest score on that dataset, and 1 corresponds to the performance achieve by exactly generating the ground truth. For BERTScore  [125] , ROUGE  [70] ,\n\nand RefCLIPScore  [36]  the maximum value is set to 1. BARTScore  [121]  uses the log of probabilities. Following  [16] , we calculate the maximum value for each dataset separately as BARTScore(r, r) where r is the ground truth sentence.\n\nSince details regarding training type for GEMINI and GPT-4V, and modality processing for GPT-4V are not revealed, we do not use the scores from these models while evaluating the performance for the training type and modality processing dimensions. Further, for HATEFUL MEMES, OPENPATH, and MEMOTION datasets, GPT-4V did not respond and generated can't provide assistance and \"indeterminate\" for many samples. Hence, we exclude the results of GPT-4V on these datasets during evaluation.",
      "page_start": 35,
      "page_end": 36
    },
    {
      "section_name": "B.3 Significance Tests",
      "text": "While comparing performance across categories in each dimension, we perform paired t-tests to determine the significance of the results. For datasets, specifically, for each category, we calculate the average performance of each of the 11 models on all the datasets in a category (c i ) to create a vector v i ∈ R 11 . Next, we performed pairwise t-tests between these vectors to determine the significance of the results. The p-values obtained through the t-tests are presented in Table  13 . We find that the difference between the performance on different categories is statistically significant (p-value < 0.05) for real-world use cases, multimodal interaction, external knowledge, and information flow dimensions, which explains that these are particularly difficult dimensions for today's multimodal model. We also conducted t-tests for various categories in each of the modeling dimensions. For all models in a category (c i ), we use their average performance on each of the 28 datasets to construct a vector w i ∈ R 28 . We then perform pair-wise t-tests across all the categories for all dimensions. As mentioned in Section B.2, we do not use the scores of GPT-4V and GEMINI for the dimensions where their training/modeling decisions aren't revealed. We find that for all the dimensions, the best-performing category achieves significantly better scores with p-values < 0.05 (Table  14 ).",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "B.4 Model Hyperparameters And Inference Time",
      "text": "In Table  12 , we list the values of important text-generation hyperparameters used to evaluate different models. For each model, we also report the inference time for a single image-text pair averaged across all the datasets.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "B.5 Human Evaluation",
      "text": "We perform human preference-based pair-wise comparison (battles) of model responses across 1000 datapoints and use the following metrics to rank the models.\n\nAverage win rate: Similar to Chiang et al.  [19] , for each pair of models, considering only the battles between them, we determine the win rate w ab = Na Na+N b , where N a and N b are the number of battles won by model a and model b respectively. We then take the average of the win rates across all the models to calculate the average win rate for each model i.e., awr a = 1 M ∑ M b=1 w ab . The top 4 models based on the average win rate are GEMINI (0.73), GPT-4V (0.68), INSTRUCT-BLIP (0.60) and BLIP-2 (0.52).\n\nElo Rating: Using the initial rating of each model as 1000, we sequentially process the battles and update the rating of the models as per the below equations. R a and R b denote the current ratings of model a and model b in the battle. S a = 1 if model a wins the battle and 0 if it loses. S b = 1 -S a and in case of ties, S a = S b = 0.5. For more stable Elo ratings, we use K = 4.\n\nThe above update rule is sensitive to battle orders. In order to get more stable and less biased Elo ratings, we run the above computation 1000 times by shuffling the battle order each time, and report the median Elo rating over the 1000 runs for each model.\n\nThe 1000 battles were split across 5 authors randomly (200 battles each) for annotation. Using a web interface, the model outputs were presented to the annotators. For each sample, the annotators were instructed to select the output that better answers the query. For cases where both outputs were equally good/bad, or performing the task required domain knowledge (e.g., healthcare datasets), the Model comparisons: Overall, INSTRUCT-BLIP and BLIP-2 achieve the highest average scores of 0.38 and 0.37, followed by FUYU-8B (0.29). OPENFLAMINGO and EMU rank lowest on many datasets (receiving a 0 score as per our normalization) and achieve the lowest average scores of 0.05 and 0.11.",
      "page_start": 36,
      "page_end": 37
    },
    {
      "section_name": "C.2 Dataset Trends",
      "text": "In Table  15 , we summarize the average performance of models on various categories in each data dimension. We now closely compare the performance between different categories of individual dimensions.\n\nMultimodal Skills 1: Interactions The average scores on datasets having redundant, unique and synergistic interactions are 0.25, 0.14, and 0.28. The p-values obtained using paired t-test for Redundancy vs Uniqueness, Uniqueness vs Synergy, and Redundancy vs Synergy are 0.01, 0.0008, and 0.22, indicating that average scores on datasets with unique interactions is significantly lower as compared to datasets with Redundant and Synergistic interactions. Reasons for lower uniqueness scores can be attributed to the presence of highly challenging datasets such as DECIMER, INATURALIST, ENRICO.\n\nMultimodal Skills 2: Granularity The average scores of the models on datasets with fine-grained (0.23) and coarse-grained alignment (0.22) are not significantly different, indicating that both categories are challenging for the models, with the former containing tasks like GQA, WINOGROUND and NLVR and the latter having tasks such as FLICKR30K, HATEFUL MEMES, and SCIENCEQA.\n\nMultimodal Skills 3: Reasoning The average scores achieved by models on tasks requiring less or more reasoning are 0.22 and 0.23 respectively, and we find that the difference is not statistically indicating that the difference is significant. On average, we find that models pretrained with diverse data achieve better scores on knowledge-intensive tasks such as INATURALIST and OK-VQA with improvements in average scores up to 0.21.\n\nInstruction tuning vs supervised fine-tuning: Instruction-tuned models achieve a higher average score (0.23) as compared to models with only supervised fine-tuning (0.21). We observe the highest improvements in translation tasks such as DECIMER, FLICKR30K, and SCREEN2WORDS. We also observe that instruction-tuned models receive a higher average score as compared to supervised fine-tuned models (improvement of 0.12).\n\nModality processing: Models that process the modalities separately perform significantly better than the models that operate on interleaved inputs. The average scores for the former and latter models are 0.17 and 0.26 respectively (p-value ≈ 0). We find high improvements of 0.26, 0.24, 0.22, and 0.2 in the average scores for the datasets SCIENCEQA, NY CARTOON, MM-IMDB, and UCMERCED LAND USE.\n\nTraining type: We do not find a significant difference between the models that are fine-tuned in a single phase end-to-end manner (0.21) as compared to the models where only specific modules are fine-tuned in a single phase (0.23).",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "C.4 Summary Of Takeaway Messages",
      "text": "Finally, we summarize the main findings regarding the performance and evaluation of multimodal foundation models that can be important directions for future work:\n\n1. Challenging datasets: Health, HCI, and Science are all relatively difficult use cases for today's multimodal foundation models, which are statistically significantly harder than Multimedia and Affective Computing use cases. In particular, images of scientific diagrams, satellite images, medical images, memes, and rich social interactions pose challenges. It is therefore important to evaluate multimodal models on a diverse range of input modalities and output tasks to get a better measure of generalization performance. 2. Multimodal interactions: Models perform better on redundant interactions but struggle when visual information is not directly referenced by text (i.e., uniqueness or synergy). Future benchmarks should contain richer multimodal interactions beyond redundancy, such as in analyzing sarcasm, humor, memes, science, environment, and education. These can serve as better test beds for multimodal models and enable their applications towards real-world multimodal interactions.\n\n3. Reasoning, fine-grained, and knowledge: We need better datasets that test for complex reasoning and fine-grained alignment -current ones do not pose enough challenges to today's models, with no significant performance differences with or without reasoning and fine-grained alignment.\n\nWe do find that tasks requiring external knowledge are significantly harder than no knowledge; bridging this gap can be a promising direction for multimodal research. 4. Model and data size: Perhaps unsurprisingly, larger scales of data and models improve the average score across the board, with significant improvements of up to 75% as compared to medium-sized models. Training on diverse data sources also improves over models that only pretrain on images and captions. The tasks that show the most improvement are INATURALIST and MEMECAP which are knowledge-intensive and require complex reasoning. 5. Model training: Instruction-tuned models performed better than those with only supervised fine-tuning. Cross-modal translation (image-to-text) tasks show the most improvements (e.g., DECIMER, MEMECAP, and SCREEN2WORDS). However, some instruction-tuned models still struggle to follow the instructions (e.g., generating a caption when asked to classify an image, or generating long responses when asked to answer in a few words). Instruction tuning using larger datasets with diverse instructions can help alleviate this problem.",
      "page_start": 40,
      "page_end": 41
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: HEMM, as an evaluation framework, goes beyond conventional lists of datasets to",
      "page": 1
    },
    {
      "caption": "Figure 1: HEMM is an evaluation framework that characterizes multimodal models along several dimensions",
      "page": 2
    },
    {
      "caption": "Figure 2: Responses of GPT-4V and GEMINI on samples from the science category. These failure cases show",
      "page": 6
    },
    {
      "caption": "Figure 3: Average scores are higher for multimedia",
      "page": 6
    },
    {
      "caption": "Figure 3: and Table 3. On average, mod-",
      "page": 6
    },
    {
      "caption": "Figure 8: ). Datasets related",
      "page": 6
    },
    {
      "caption": "Figure 4: Tasks requiring commonsense and compositional reasoning are challenging. In (a), GPT-4V and",
      "page": 7
    },
    {
      "caption": "Figure 4: for examples).",
      "page": 7
    },
    {
      "caption": "Figure 5: On average, large models are better than small and",
      "page": 8
    },
    {
      "caption": "Figure 6: Multimodal skills are the basic building blocks central to solving problems, spanning information",
      "page": 28
    },
    {
      "caption": "Figure 7: Multimodal information flow studies how the content changes across the two modalities for the task,",
      "page": 28
    },
    {
      "caption": "Figure 8: Model outputs on samples from ENRICO, VQARAD, INATURALIST, and SCIENCEQA. In (a), all",
      "page": 40
    }
  ],
  "tables": [
    {
      "caption": "Table 1: These datasets require different multimodal",
      "page": 2
    },
    {
      "caption": "Table 1: HEMM includes a comprehensive suite of 30 datasets to benchmark multimodal foundation models.",
      "page": 3
    },
    {
      "caption": "Table 2: Models used in HEMM, ranked from small to large, and categorized by #Param (model size), Data",
      "page": 4
    },
    {
      "caption": "Table 2: summarizes the 11 models we evaluate in HEMM, which span different numbers of parame-",
      "page": 4
    },
    {
      "caption": "Table 3: Performance on different dataset dimensions,",
      "page": 5
    },
    {
      "caption": "Table 4: Performance on different modeling decisions,",
      "page": 5
    },
    {
      "caption": "Table 3: On average, mod-",
      "page": 6
    },
    {
      "caption": "Table 4: Overall comparisons across models",
      "page": 7
    },
    {
      "caption": "Table 5: Average win rate and Elo Rating of 11 mod-",
      "page": 9
    },
    {
      "caption": "Table 6: Inter-Annotator agreement scores for stage 1 annotation.",
      "page": 29
    },
    {
      "caption": "Table 7: Categorization after aggregating human annotations.",
      "page": 29
    },
    {
      "caption": "Table 7: We also consider ‘Neutral Reasoning’ and ‘Less Reasoning’ to be the same",
      "page": 29
    },
    {
      "caption": "Table 8: Categorization after aggregating GPT-4V annotations. Cases where ’-’ is present are due to the model",
      "page": 30
    },
    {
      "caption": "Table 9: Inter-annotator agreement scores for stage 2 annotations.",
      "page": 30
    },
    {
      "caption": "Table 8: For each set, we ask two annotators to label the annotation by",
      "page": 30
    },
    {
      "caption": "Table 9: We see improvements over the previous",
      "page": 30
    },
    {
      "caption": "Table 10: with the source for each categorization in the table. (1) indicates",
      "page": 30
    },
    {
      "caption": "Table 10: Final dataset categorization.",
      "page": 31
    },
    {
      "caption": "Table 10: , the majority of categories are agreed upon both by human annotators and GPT-4V,",
      "page": 31
    },
    {
      "caption": "Table 11: Evaluation metrics supported in HEMM",
      "page": 35
    },
    {
      "caption": "Table 13: We find that",
      "page": 35
    },
    {
      "caption": "Table 12: Hyperparameters used for running inference for various models. Temperature for GPT-4V and Beam",
      "page": 36
    },
    {
      "caption": "Table 12: , we list the values of important text-generation hyperparameters used to evaluate different",
      "page": 36
    },
    {
      "caption": "Table 13: Standard deviation and p-values (from paired t-tests) across categories for each dataset dimension. On",
      "page": 37
    },
    {
      "caption": "Table 14: Standard deviation and p-values for categories in various modeling dimensions. Models in the",
      "page": 38
    },
    {
      "caption": "Table 15: , we summarize the average performance of models on various categories in each data",
      "page": 38
    },
    {
      "caption": "Table 15: Comparisons on different dataset categories.",
      "page": 39
    },
    {
      "caption": "Table 16: Comparisons on different modeling deci-",
      "page": 39
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "nocaps: novel object captioning at scale",
      "authors": [
        "Harsh Agrawal",
        "Karan Desai",
        "Yufei Wang",
        "Xinlei Chen",
        "Rishabh Jain",
        "Mark Johnson",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee",
        "Peter Anderson"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "3",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "Vqa: Visual question answering",
      "authors": [
        "Stanislaw Antol",
        "Aishwarya Agrawal",
        "Jiasen Lu",
        "Margaret Mitchell",
        "Dhruv Batra",
        "C Lawrence Zitnick",
        "Devi Parikh"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "5",
      "title": "Gated multimodal units for information fusion",
      "authors": [
        "John Arevalo",
        "Thamar Solorio",
        "Manuel Montes-Y Gómez",
        "Fabio González"
      ],
      "year": "2017",
      "venue": "Gated multimodal units for information fusion",
      "arxiv": "arXiv:1702.01992"
    },
    {
      "citation_id": "6",
      "title": "Openflamingo: An open-source framework for training large autoregressive vision-language models",
      "authors": [
        "Anas Awadalla",
        "Irena Gao",
        "Josh Gardner",
        "Jack Hessel",
        "Yusuf Hanafy",
        "Wanrong Zhu",
        "Yonatan Kalyani Marathe",
        "Samir Bitton",
        "Shiori Gadre",
        "Sagawa"
      ],
      "year": "2023",
      "venue": "Openflamingo: An open-source framework for training large autoregressive vision-language models",
      "arxiv": "arXiv:2308.01390"
    },
    {
      "citation_id": "7",
      "title": "Evaluating vision-language models by language models",
      "authors": [
        "Shuai Bai",
        "Shusheng Yang",
        "Jinze Bai",
        "Peng Wang",
        "Xingxuan Zhang",
        "Junyang Lin",
        "Xinggang Wang",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Evaluating vision-language models by language models",
      "arxiv": "arXiv:2308.16890"
    },
    {
      "citation_id": "8",
      "title": "Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models",
      "authors": [
        "Mohamed Eslam",
        "Pengzhan Bakr",
        "Xiaogian Sun",
        "Faizan Shen",
        "Li Farooq Khan",
        "Mohamed Li",
        "Elhoseiny"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Text and image: A critical introduction to the visual/verbal divide",
      "authors": [
        "John Bateman"
      ],
      "year": "2014",
      "venue": "Text and image: A critical introduction to the visual/verbal divide"
    },
    {
      "citation_id": "10",
      "title": "Introducing our multimodal models",
      "authors": [
        "Rohan Bavishi",
        "Erich Elsen",
        "Curtis Hawthorne",
        "Maxwell Nye",
        "Augustus Odena",
        "Arushi Somani",
        "Sagnak Taşırlar"
      ],
      "year": "2023",
      "venue": "Introducing our multimodal models"
    },
    {
      "citation_id": "11",
      "title": "Visit-bench: A benchmark for vision-language instruction following inspired by real-world use",
      "authors": [
        "Yonatan Bitton",
        "Hritik Bansal",
        "Jack Hessel",
        "Rulin Shao",
        "Wanrong Zhu",
        "Anas Awadalla",
        "Josh Gardner",
        "Rohan Taori",
        "Ludwig Schimdt"
      ],
      "year": "2023",
      "venue": "Visit-bench: A benchmark for vision-language instruction following inspired by real-world use",
      "arxiv": "arXiv:2308.06595"
    },
    {
      "citation_id": "12",
      "title": "Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks",
      "authors": [
        "Paulo Blikstein",
        "Marcelo Worsley"
      ],
      "year": "2016",
      "venue": "Journal of Learning Analytics"
    },
    {
      "citation_id": "13",
      "title": "Decimer-handdrawn molecule images dataset",
      "authors": [
        "Otto Henning",
        "Achim Brinkhaus",
        "Christoph Zielesny",
        "Kohulan Steinbeck",
        "Rajan"
      ],
      "year": "2022",
      "venue": "Journal of Cheminformatics"
    },
    {
      "citation_id": "14",
      "title": "Revisiting the\" video\" in video-language understanding",
      "authors": [
        "Shyamal Buch",
        "Cristóbal Eyzaguirre",
        "Adrien Gaidon",
        "Jiajun Wu",
        "Li Fei-Fei",
        "Juan Niebles"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Multi-modal sarcasm detection in Twitter with hierarchical fusion model",
      "authors": [
        "Yitao Cai",
        "Huiyu Cai",
        "Xiaojun Wan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1239"
    },
    {
      "citation_id": "16",
      "title": "Webqa: Multihop and multimodal qa",
      "authors": [
        "Yingshan Chang",
        "Mridu Narang",
        "Hisami Suzuki",
        "Guihong Cao",
        "Jianfeng Gao",
        "Yonatan Bisk"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Uniter: Universal image-text representation learning",
      "authors": [
        "Yen-Chun Chen",
        "Linjie Li",
        "Licheng Yu",
        "Ahmed Kholy",
        "Faisal Ahmed",
        "Zhe Gan",
        "Yu Cheng",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "18",
      "title": "Remote sensing image scene classification: Benchmark and state of the art",
      "authors": [
        "Gong Cheng",
        "Junwei Han",
        "Xiaoqiang Lu"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "19",
      "title": "Chatbot arena: An open platform for evaluating llms by human preference",
      "authors": [
        "Wei-Lin Chiang",
        "Lianmin Zheng",
        "Ying Sheng",
        "Anastasios Nikolas Angelopoulos",
        "Tianle Li",
        "Dacheng Li",
        "Hao Zhang",
        "Banghua Zhu",
        "Michael Jordan",
        "Joseph Gonzalez"
      ],
      "year": "2024",
      "venue": "Chatbot arena: An open platform for evaluating llms by human preference",
      "arxiv": "arXiv:2403.04132"
    },
    {
      "citation_id": "20",
      "title": "Deep reinforcement learning from human preferences",
      "authors": [
        "Jan Paul F Christiano",
        "Tom Leike",
        "Miljan Brown",
        "Shane Martic",
        "Dario Legg",
        "Amodei"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges",
      "authors": [
        "Chenhang Cui",
        "Yiyang Zhou",
        "Xinyu Yang",
        "Shirley Wu",
        "Linjun Zhang",
        "James Zou",
        "Huaxiu Yao"
      ],
      "year": "2023",
      "venue": "Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges",
      "arxiv": "arXiv:2311.03287"
    },
    {
      "citation_id": "22",
      "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "authors": [
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Meng",
        "Huat Tiong",
        "Junqi Zhao",
        "Weisheng Wang",
        "Boyang Li",
        "Pascale Fung",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "arxiv": "arXiv:2305.06500"
    },
    {
      "citation_id": "23",
      "title": "Rico: A mobile app dataset for building data-driven design applications",
      "authors": [
        "Biplab Deka",
        "Zifeng Huang",
        "Chad Franzen",
        "Joshua Hibschman",
        "Daniel Afergan",
        "Yang Li",
        "Jeffrey Nichols",
        "Ranjitha Kumar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 30th annual ACM symposium on user interface software and technology"
    },
    {
      "citation_id": "24",
      "title": "A survey of vision-language pre-trained models",
      "authors": [
        "Yifan Du",
        "Zikang Liu",
        "Junyi Li",
        "Wayne Zhao"
      ],
      "year": "2022",
      "venue": "A survey of vision-language pre-trained models",
      "arxiv": "arXiv:2202.10936"
    },
    {
      "citation_id": "25",
      "title": "Multimodal interfaces: A survey of principles, models and frameworks",
      "authors": [
        "Bruno Dumas",
        "Denis Lalanne",
        "Sharon Oviatt"
      ],
      "year": "2009",
      "venue": "Human machine interaction: Research results of the mmi program"
    },
    {
      "citation_id": "26",
      "title": "Modelling fusion of modalities in multimodal interactive systems with mmmm",
      "authors": [
        "Bruno Dumas",
        "Jonathan Pirau",
        "Denis Lalanne"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "27",
      "title": "Magma-multimodal augmentation of generative models through adapter-based finetuning",
      "authors": [
        "Constantin Eichenberg",
        "Sidney Black",
        "Samuel Weinbach",
        "Letitia Parcalabescu",
        "Anette Frank"
      ],
      "year": "2021",
      "venue": "Magma-multimodal augmentation of generative models through adapter-based finetuning",
      "arxiv": "arXiv:2112.05253"
    },
    {
      "citation_id": "28",
      "title": "A survey of current datasets for vision and language research",
      "authors": [
        "Francis Ferraro",
        "Nasrin Mostafazadeh",
        "Ting-Hao Huang",
        "Lucy Vanderwende",
        "Jacob Devlin",
        "Michel Galley",
        "Margaret Mitchell"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D15-1021"
    },
    {
      "citation_id": "29",
      "title": "A comprehensive evaluation benchmark for multimodal large language models",
      "authors": [
        "Chaoyou Fu",
        "Peixian Chen",
        "Yunhang Shen",
        "Yulei Qin",
        "Mengdan Zhang",
        "Xu Lin",
        "Zhenyu Qiu",
        "Wei Lin",
        "Jinrui Yang",
        "Xiawu Zheng"
      ],
      "year": "2023",
      "venue": "A comprehensive evaluation benchmark for multimodal large language models",
      "arxiv": "arXiv:2306.13394"
    },
    {
      "citation_id": "30",
      "title": "Parameter-efficient visual instruction model",
      "authors": [
        "Peng Gao",
        "Jiaming Han",
        "Renrui Zhang",
        "Ziyi Lin",
        "Shijie Geng",
        "Aojun Zhou",
        "Wei Zhang",
        "Pan Lu",
        "Conghui He",
        "Xiangyu Yue"
      ],
      "year": "2023",
      "venue": "Parameter-efficient visual instruction model",
      "arxiv": "arXiv:2304.15010"
    },
    {
      "citation_id": "31",
      "title": "What makes the difference? an empirical comparison of fusion strategies for multimodal language analysis",
      "authors": [
        "Dimitris Gkoumas",
        "Qiuchi Li",
        "Christina Lioma",
        "Yijun Yu",
        "Dawei Song"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "32",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "33",
      "title": "Magpie: A large corpus of potentially idiomatic expressions",
      "authors": [
        "Hessel Haagsma",
        "Johan Bos",
        "Malvina Nissim"
      ],
      "year": "2020",
      "venue": "12th Language Resources and Evaluation Conference: LREC 2020"
    },
    {
      "citation_id": "34",
      "title": "The movielens datasets: History and context",
      "authors": [
        "Maxwell Harper",
        "Joseph Konstan"
      ],
      "year": "2015",
      "venue": "Acm transactions on interactive intelligent systems (tiis)"
    },
    {
      "citation_id": "35",
      "title": "Pathvqa: 30000+ questions for medical visual question answering",
      "authors": [
        "Xuehai He",
        "Yichen Zhang",
        "Luntian Mou",
        "Eric Xing",
        "Pengtao Xie"
      ],
      "year": "2020",
      "venue": "Pathvqa: 30000+ questions for medical visual question answering",
      "arxiv": "arXiv:2003.10286"
    },
    {
      "citation_id": "36",
      "title": "Clipscore: A reference-free evaluation metric for image captioning",
      "authors": [
        "Jack Hessel",
        "Ari Holtzman",
        "Maxwell Forbes",
        "Ronan Le Bras",
        "Yejin Choi"
      ],
      "year": "2021",
      "venue": "Clipscore: A reference-free evaluation metric for image captioning",
      "arxiv": "arXiv:2104.08718"
    },
    {
      "citation_id": "37",
      "title": "Do androids laugh at electric sheep? humor\" understanding\" benchmarks from the new yorker caption contest",
      "authors": [
        "Jack Hessel",
        "Ana Marasović",
        "Jena Hwang",
        "Lillian Lee",
        "Jeff Da",
        "Rowan Zellers",
        "Robert Mankoff",
        "Yejin Choi"
      ],
      "year": "2022",
      "venue": "Do androids laugh at electric sheep? humor\" understanding\" benchmarks from the new yorker caption contest",
      "arxiv": "arXiv:2209.06293"
    },
    {
      "citation_id": "38",
      "title": "Framing image description as a ranking task: Data, models and evaluation metrics",
      "authors": [
        "Micah Hodosh",
        "Peter Young",
        "Julia Hockenmaier"
      ],
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "39",
      "title": "'a picture is worth a thousand words': Multimodal sensemaking of the global financial crisis",
      "authors": [
        "Markus Höllerer",
        "Dennis Jancsary",
        "Maria Grafström"
      ],
      "year": "2018",
      "venue": "Organization Studies"
    },
    {
      "citation_id": "40",
      "title": "Language is not all you need: Aligning perception with language models",
      "authors": [
        "Shaohan Huang",
        "Li Dong",
        "Wenhui Wang",
        "Yaru Hao",
        "Saksham Singhal",
        "Shuming Ma",
        "Tengchao Lv",
        "Lei Cui",
        "Owais Khan Mohammed",
        "Barun Patra"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "41",
      "title": "Leveraging medical twitter to build a visual-language foundation model for pathology ai",
      "authors": [
        "Zhi Huang",
        "Federico Bianchi",
        "Mert Yuksekgonul",
        "Thomas Montine",
        "James Zou"
      ],
      "year": "2023",
      "venue": "bioRxiv"
    },
    {
      "citation_id": "42",
      "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "authors": [
        "A Drew",
        "Christopher Hudson",
        "Manning"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "43",
      "title": "Memecap: A dataset for captioning and interpreting memes",
      "authors": [
        "Eunjeong Hwang",
        "Vered Shwartz"
      ],
      "year": "2023",
      "venue": "Memecap: A dataset for captioning and interpreting memes",
      "arxiv": "arXiv:2305.13703"
    },
    {
      "citation_id": "44",
      "title": "Prompting visual-language models for efficient video understanding",
      "authors": [
        "Chen Ju",
        "Tengda Han",
        "Kunhao Zheng",
        "Ya Zhang",
        "Weidi Xie"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "45",
      "title": "Grounding, meaning and foundation models: Adventures in multimodal machine learning",
      "authors": [
        "Douwe Kiela"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "authors": [
        "Douwe Kiela",
        "Hamed Firooz",
        "Aravind Mohan",
        "Vedanuj Goswami",
        "Amanpreet Singh",
        "Pratik Ringshia",
        "Davide Testuggine"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Embedded multimodal interfaces in robotics: applications, future trends, and societal implications",
      "authors": [
        "Elsa Kirchner",
        "Stephen Fairclough",
        "Frank Kirchner"
      ],
      "year": "2019",
      "venue": "The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions"
    },
    {
      "citation_id": "48",
      "title": "Multimodal machine learning in precision health: A scoping review",
      "authors": [
        "Adrienne Kline",
        "Hanyin Wang",
        "Yikuan Li",
        "Saya Dennis",
        "Meghan Hutch",
        "Zhenxing Xu",
        "Fei Wang",
        "Feixiong Cheng",
        "Yuan Luo"
      ],
      "year": "2022",
      "venue": "Digital Medicine"
    },
    {
      "citation_id": "49",
      "title": "Grounding language models to images for multimodal inputs and outputs",
      "authors": [
        "Jing Yu Koh",
        "Ruslan Salakhutdinov",
        "Daniel Fried"
      ],
      "year": "2023",
      "venue": "Grounding language models to images for multimodal inputs and outputs"
    },
    {
      "citation_id": "50",
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "authors": [
        "Ranjay Krishna",
        "Yuke Zhu",
        "Oliver Groth",
        "Justin Johnson",
        "Kenji Hata",
        "Joshua Kravitz",
        "Stephanie Chen",
        "Yannis Kalantidis",
        "Li-Jia Li",
        "David Shamma"
      ],
      "year": "2017",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "51",
      "title": "Review of multimodal machine learning approaches in healthcare",
      "authors": [
        "Felix Krones",
        "Umar Marikkar",
        "Guy Parsons",
        "Adam Szmul",
        "Adam Mahdi"
      ],
      "year": "2024",
      "venue": "Review of multimodal machine learning approaches in healthcare",
      "arxiv": "arXiv:2402.02460"
    },
    {
      "citation_id": "52",
      "title": "Integrating text and image: Determining multimodal document intent in instagram posts",
      "authors": [
        "Julia Kruk",
        "Jonah Lubin",
        "Karan Sikka",
        "Xiao Lin",
        "Dan Jurafsky",
        "Ajay Divakaran"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "53",
      "title": "A dataset of clinically generated visual questions and answers about radiology images",
      "authors": [
        "Jason Lau",
        "Soumya Gayen",
        "Asma Ben Abacha",
        "Dina Demner-Fushman"
      ],
      "year": "2018",
      "venue": "Scientific data"
    },
    {
      "citation_id": "54",
      "title": "Dina Demner, and Asma Ben Abacha. Visual question answering in radiology (vqa-rad)",
      "authors": [
        "Jason Lau",
        "Soumya Gayen"
      ],
      "year": "2019",
      "venue": "Dina Demner, and Asma Ben Abacha. Visual question answering in radiology (vqa-rad)"
    },
    {
      "citation_id": "55",
      "title": "Evaluating human-language model interaction",
      "authors": [
        "Mina Lee",
        "Megha Srivastava",
        "Amelia Hardy",
        "John Thickstun",
        "Esin Durmus",
        "Ashwin Paranjape",
        "Ines Gerard-Ursin",
        "Lisa Xiang",
        "Faisal Li",
        "Frieda Ladhak",
        "Rong"
      ],
      "year": "2022",
      "venue": "Evaluating human-language model interaction",
      "arxiv": "arXiv:2212.09746"
    },
    {
      "citation_id": "56",
      "title": "Modeling multimodal social interactions: New challenges and baselines with densely aligned representations",
      "authors": [
        "Sangmin Lee",
        "Bolin Lai",
        "Fiona Ryan",
        "Bikram Boote",
        "James Rehg"
      ],
      "year": "2024",
      "venue": "Modeling multimodal social interactions: New challenges and baselines with densely aligned representations",
      "arxiv": "arXiv:2403.02090"
    },
    {
      "citation_id": "57",
      "title": "Holistic evaluation of text-to-image models",
      "authors": [
        "Tony Lee",
        "Michihiro Yasunaga",
        "Chenlin Meng",
        "Yifan Mai",
        "Sung Park",
        "Agrim Gupta",
        "Yunzhi Zhang",
        "Deepak Narayanan",
        "Benita Hannah",
        "Marco Teufel",
        "Bellagente"
      ],
      "year": "2023",
      "venue": "Holistic evaluation of text-to-image models",
      "arxiv": "arXiv:2311.04287"
    },
    {
      "citation_id": "58",
      "title": "Enrico: A high-quality dataset for topic modeling of mobile ui designs",
      "authors": [
        "Asutosh Luis A Leiva",
        "Antti Hota",
        "Oulasvirta"
      ],
      "year": "2020",
      "venue": "Proc. MobileHCI extended abstracts"
    },
    {
      "citation_id": "59",
      "title": "Seed-bench: Benchmarking multimodal llms with generative comprehension",
      "authors": [
        "Bohao Li",
        "Rui Wang",
        "Guangzhi Wang",
        "Yuying Ge",
        "Yixiao Ge",
        "Ying Shan"
      ],
      "year": "2023",
      "venue": "Seed-bench: Benchmarking multimodal llms with generative comprehension",
      "arxiv": "arXiv:2307.16125"
    },
    {
      "citation_id": "60",
      "title": "Multimodal foundation models: From specialists to general-purpose assistants",
      "authors": [
        "Chunyuan Li",
        "Zhe Gan",
        "Zhengyuan Yang",
        "Jianwei Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Multimodal foundation models: From specialists to general-purpose assistants",
      "arxiv": "arXiv:2309.10020"
    },
    {
      "citation_id": "61",
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Caiming Xiong",
        "Steven Hoi"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "62",
      "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
      "arxiv": "arXiv:2301.12597"
    },
    {
      "citation_id": "63",
      "title": "Visualbert: A simple and performant baseline for vision and language",
      "authors": [
        "Liunian Harold",
        "Mark Yatskar",
        "Cho-Jui Da Yin",
        "Kai-Wei Hsieh",
        "Chang"
      ],
      "year": "2019",
      "venue": "Visualbert: A simple and performant baseline for vision and language",
      "arxiv": "arXiv:1908.03557"
    },
    {
      "citation_id": "64",
      "title": "Mapping natural language instructions to mobile ui action sequences",
      "authors": [
        "Yang Li",
        "Jiacong He",
        "Xin Zhou",
        "Yuan Zhang",
        "Jason Baldridge"
      ],
      "year": "2020",
      "venue": "Mapping natural language instructions to mobile ui action sequences",
      "arxiv": "arXiv:2005.03776"
    },
    {
      "citation_id": "65",
      "title": "Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks",
      "authors": [
        "Zejun Li",
        "Ye Wang",
        "Mengfei Du",
        "Qingwen Liu",
        "Binhao Wu",
        "Jiwen Zhang",
        "Chengxing Zhou",
        "Zhihao Fan",
        "Jie Fu",
        "Jingjing Chen"
      ],
      "year": "2023",
      "venue": "Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks",
      "arxiv": "arXiv:2310.02569"
    },
    {
      "citation_id": "66",
      "title": "Multibench: Multiscale benchmarks for multimodal representation learning",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Xiang Fan",
        "Zetian Wu",
        "Yun Cheng",
        "Jason Wu",
        "Leslie Yufan Chen",
        "Peter Wu",
        "Michelle Lee",
        "Yuke Zhu"
      ],
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "67",
      "title": "High-modality multimodal transformer: Quantifying modality & interaction heterogeneity for high-modality representation learning",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Xiang Fan",
        "Jeffrey Tsaw",
        "Yudong Liu",
        "Shentong Mo",
        "Dani Yogatama",
        "Louis-Philippe Morency",
        "Russ Salakhutdinov"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "68",
      "title": "Quantifying & modeling multimodal interactions: An information decomposition framework",
      "authors": [
        "Paul Pu Liang",
        "Yun Cheng",
        "Xiang Fan",
        "Kai Chun",
        "Suzanne Ling",
        "Richard Nie",
        "Zihao Chen",
        "Faisal Deng",
        "Ruslan Mahmood",
        "Louis-Philippe Salakhutdinov",
        "Morency"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "69",
      "title": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2023",
      "venue": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions"
    },
    {
      "citation_id": "70",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "Chin-Yew Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out"
    },
    {
      "citation_id": "71",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Dollár",
        "C Lawrence"
      ],
      "year": "2014",
      "venue": "Computer Vision-ECCV 2014: 13th European Conference"
    },
    {
      "citation_id": "72",
      "title": "Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering",
      "authors": [
        "Bo Liu",
        "Li-Ming Zhan",
        "Li Xu",
        "Lin Ma",
        "Yan Yang",
        "Xiao-Ming Wu"
      ],
      "year": "2021",
      "venue": "2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)"
    },
    {
      "citation_id": "73",
      "title": "Is your multi-modal model an all-around player? arXiv preprint",
      "authors": [
        "Yuan Liu",
        "Haodong Duan",
        "Yuanhan Zhang",
        "Bo Li",
        "Songyang Zhang",
        "Wangbo Zhao",
        "Yike Yuan",
        "Jiaqi Wang",
        "Conghui He",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Is your multi-modal model an all-around player? arXiv preprint",
      "arxiv": "arXiv:2307.06281"
    },
    {
      "citation_id": "74",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "75",
      "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
      "authors": [
        "Pan Lu",
        "Swaroop Mishra",
        "Tony Xia",
        "Liang Qiu",
        "Kai-Wei Chang",
        "Song-Chun Zhu",
        "Oyvind Tafjord",
        "Peter Clark",
        "Ashwin Kalyan"
      ],
      "venue": "The 36th Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "76",
      "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge",
      "authors": [
        "Kenneth Marino",
        "Mohammad Rastegari",
        "Ali Farhadi",
        "Roozbeh Mottaghi"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "77",
      "title": "A taxonomy of relationships between images and text",
      "authors": [
        "Emily Marsh",
        "Marilyn White"
      ],
      "year": "2003",
      "venue": "Journal of documentation"
    },
    {
      "citation_id": "78",
      "title": "Advancing social intelligence in ai agents: Technical challenges and open questions",
      "authors": [
        "Leena Mathur",
        "Paul Liang",
        "Louis-Philippe Morency"
      ],
      "year": "2024",
      "venue": "Advancing social intelligence in ai agents: Technical challenges and open questions",
      "arxiv": "arXiv:2404.11023"
    },
    {
      "citation_id": "79",
      "title": "Wordnet: a lexical database for english",
      "authors": [
        "George Miller"
      ],
      "year": "1995",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "80",
      "title": "Multimodal learning in health sciences and medicine: Merging technologies to enhance student learning and communication",
      "authors": [
        "Christian Moro",
        "Jessica Smith",
        "Zane Stromberga"
      ],
      "year": "2019",
      "venue": "Biomedical Visualisation"
    },
    {
      "citation_id": "81",
      "title": "A brief history of human-computer interaction technology. interactions",
      "authors": [
        "Brad Myers"
      ],
      "year": "1998",
      "venue": "A brief history of human-computer interaction technology. interactions"
    },
    {
      "citation_id": "82",
      "title": "Modeling multimodal human-computer interaction",
      "authors": [
        "Zeljko Obrenovic",
        "Dusan Starcevic"
      ],
      "year": "2004",
      "venue": "Computer"
    },
    {
      "citation_id": "83",
      "title": "Understanding, categorizing and predicting semantic image-text relations",
      "authors": [
        "Christian Otto",
        "Matthias Springstein",
        "Avishek Anand",
        "Ralph Ewerth"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "84",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "85",
      "title": "Kosmos-2: Grounding multimodal large language models to the world",
      "authors": [
        "Zhiliang Peng",
        "Wenhui Wang",
        "Li Dong",
        "Yaru Hao",
        "Shaohan Huang",
        "Shuming Ma",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Kosmos-2: Grounding multimodal large language models to the world",
      "arxiv": "arXiv:2306.14824"
    },
    {
      "citation_id": "86",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "87",
      "title": "Connecting vision and language with localized narratives",
      "authors": [
        "Jordi Pont-Tuset",
        "Jasper Uijlings",
        "Soravit Changpinyo",
        "Radu Soricut",
        "Vittorio Ferrari"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "88",
      "title": "Video event understanding using natural language descriptions",
      "authors": [
        "Percy Vignesh Ramanathan",
        "Li Liang",
        "Fei-Fei"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "89",
      "title": "Task Report: Memotion Analysis 1.0 @SemEval 2020: The Visuo-Lingual Metaphor! In Proceedings of the 14th International Workshop on Semantic Evaluation",
      "authors": [
        "Chhavi Sharma",
        "William Paka",
        "Deepesh Scott",
        "Amitava Bhageria",
        "Soujanya Das",
        "Tanmoy Poria",
        "Björn Chakraborty",
        "Gambäck"
      ],
      "year": "2020",
      "venue": "Task Report: Memotion Analysis 1.0 @SemEval 2020: The Visuo-Lingual Metaphor! In Proceedings of the 14th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "90",
      "title": "K-lite: Learning transferable visual models with external knowledge",
      "authors": [
        "Sheng Shen",
        "Chunyuan Li",
        "Xiaowei Hu",
        "Yujia Xie",
        "Jianwei Yang",
        "Pengchuan Zhang",
        "Zhe Gan",
        "Lijuan Wang",
        "Lu Yuan",
        "Ce Liu"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "91",
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "authors": [
        "Weijie Su",
        "Xizhou Zhu",
        "Yue Cao",
        "Bin Li",
        "Lewei Lu",
        "Furu Wei",
        "Jifeng Dai"
      ],
      "year": "2019",
      "venue": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "arxiv": "arXiv:1908.08530"
    },
    {
      "citation_id": "92",
      "title": "One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai",
        "Pandagpt"
      ],
      "year": "2023",
      "venue": "One model to instruction-follow them all",
      "arxiv": "arXiv:2305.16355"
    },
    {
      "citation_id": "93",
      "title": "A corpus of natural language for visual reasoning",
      "authors": [
        "Alane Suhr",
        "Mike Lewis",
        "James Yeh",
        "Yoav Artzi"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "94",
      "title": "A corpus for reasoning about natural language grounded in photographs",
      "authors": [
        "Alane Suhr",
        "Stephanie Zhou",
        "Ally Zhang",
        "Iris Zhang",
        "Huajun Bai",
        "Yoav Artzi"
      ],
      "year": "2018",
      "venue": "A corpus for reasoning about natural language grounded in photographs",
      "arxiv": "arXiv:1811.00491"
    },
    {
      "citation_id": "95",
      "title": "Generative pretraining in multimodality",
      "authors": [
        "Quan Sun",
        "Qiying Yu",
        "Yufeng Cui",
        "Fan Zhang",
        "Xiaosong Zhang",
        "Yueze Wang",
        "Hongcheng Gao",
        "Jingjing Liu",
        "Tiejun Huang",
        "Xinlong Wang"
      ],
      "year": "2023",
      "venue": "Generative pretraining in multimodality",
      "arxiv": "arXiv:2307.05222"
    },
    {
      "citation_id": "96",
      "title": "Any-to-any generation via composable diffusion",
      "authors": [
        "Zineng Tang",
        "Ziyi Yang",
        "Chenguang Zhu",
        "Michael Zeng",
        "Mohit Bansal"
      ],
      "year": "2023",
      "venue": "Any-to-any generation via composable diffusion",
      "arxiv": "arXiv:2305.11846"
    },
    {
      "citation_id": "97",
      "title": "a family of highly capable multimodal models",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Yonghui Wu",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew Dai",
        "Anja Hauth"
      ],
      "year": "2023",
      "venue": "a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "98",
      "title": "Winoground: Probing vision and language models for visio-linguistic compositionality",
      "authors": [
        "Tristan Thrush",
        "Ryan Jiang",
        "Max Bartolo",
        "Amanpreet Singh",
        "Adina Williams",
        "Douwe Kiela",
        "Candace Ross"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "99",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "100",
      "title": "Multimodal few-shot learning with frozen language models",
      "authors": [
        "Maria Tsimpoukelli",
        "Jacob Menick",
        "Serkan Cabi",
        "Oriol Eslami",
        "Felix Vinyals",
        "Hill"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "101",
      "title": "Perspectives in machine learning for wildlife conservation",
      "authors": [
        "Devis Tuia",
        "Benjamin Kellenberger",
        "Sara Beery",
        "Silvia Blair R Costelloe",
        "Benjamin Zuffi",
        "Alexander Risse",
        "Mackenzie Mathis",
        "Frank Mathis",
        "Tilo Van Langevelde",
        "Burghardt"
      ],
      "year": "2022",
      "venue": "Nature communications"
    },
    {
      "citation_id": "102",
      "title": "The inaturalist species classification and detection dataset-supplementary material",
      "authors": [
        "Grant Van Horn",
        "Oisin Mac Aodha",
        "Yang Song",
        "Yin Cui",
        "Chen Sun",
        "Alex Shepard",
        "Hartwig Adam",
        "Pietro Perona",
        "Serge Belongie"
      ],
      "year": "2017",
      "venue": "Reptilia"
    },
    {
      "citation_id": "103",
      "title": "Screen2words: Automatic mobile ui summarization with multimodal learning",
      "authors": [
        "Bryan Wang",
        "Gang Li",
        "Xin Zhou",
        "Zhourong Chen",
        "Tovi Grossman",
        "Yang Li"
      ],
      "year": "2021",
      "venue": "The 34th Annual ACM Symposium on User Interface Software and Technology"
    },
    {
      "citation_id": "104",
      "title": "Can linguistic knowledge improve multimodal alignment in vision",
      "authors": [
        "Fei Wang",
        "Liang Ding",
        "Jun Rao",
        "Ye Liu",
        "Li Shen",
        "Changxing Ding"
      ],
      "year": "2023",
      "venue": "Can linguistic knowledge improve multimodal alignment in vision",
      "arxiv": "arXiv:2308.12898"
    },
    {
      "citation_id": "105",
      "title": "Git: A generative image-to-text transformer for vision and language",
      "authors": [
        "Jianfeng Wang",
        "Zhengyuan Yang",
        "Xiaowei Hu",
        "Linjie Li",
        "Kevin Lin",
        "Zhe Gan",
        "Zicheng Liu",
        "Ce Liu",
        "Lijuan Wang"
      ],
      "year": "2022",
      "venue": "Git: A generative image-to-text transformer for vision and language",
      "arxiv": "arXiv:2205.14100"
    },
    {
      "citation_id": "106",
      "title": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
      "authors": [
        "Wenhui Wang",
        "Hangbo Bao",
        "Li Dong",
        "Johan Bjorck",
        "Zhiliang Peng",
        "Qiang Liu",
        "Kriti Aggarwal",
        "Owais Khan Mohammed",
        "Saksham Singhal",
        "Subhojit Som"
      ],
      "year": "2022",
      "venue": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
      "arxiv": "arXiv:2208.10442"
    },
    {
      "citation_id": "107",
      "title": "On the road with gpt-4v(ision): Early explorations of visual-language model on autonomous driving",
      "authors": [
        "Licheng Wen",
        "Xuemeng Yang",
        "Daocheng Fu",
        "Xiaofeng Wang",
        "Pinlong Cai",
        "Xin Li",
        "Tao Ma",
        "Yingxuan Li",
        "Linran Xu",
        "Dengke Shang",
        "Zheng Zhu",
        "Shaoyan Sun",
        "Yeqi Bai",
        "Xinyu Cai",
        "Min Dou",
        "Shuanglu Hu",
        "Botian Shi"
      ],
      "year": "2023",
      "venue": "On the road with gpt-4v(ision): Early explorations of visual-language model on autonomous driving"
    },
    {
      "citation_id": "108",
      "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "authors": [
        "Chenfei Wu",
        "Shengming Yin",
        "Weizhen Qi",
        "Xiaodong Wang",
        "Zecheng Tang",
        "Nan Duan"
      ],
      "year": "2023",
      "venue": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "arxiv": "arXiv:2303.04671"
    },
    {
      "citation_id": "109",
      "title": "Next-gpt: Any-to-any multimodal llm",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Leigang Qu",
        "Wei Ji",
        "Tat-Seng Chua"
      ],
      "year": "2023",
      "venue": "Next-gpt: Any-to-any multimodal llm",
      "arxiv": "arXiv:2309.05519"
    },
    {
      "citation_id": "110",
      "title": "Multimodal machine learning for automated icd coding",
      "authors": [
        "Keyang Xu",
        "Mike Lam",
        "Jingzhi Pang",
        "Xin Gao",
        "Charlotte Band",
        "Piyush Mathur",
        "Frank Papay",
        "Ashish Khanna",
        "Jacek Cywinski",
        "Kamal Maheshwari"
      ],
      "year": "2019",
      "venue": "Machine learning for healthcare conference"
    },
    {
      "citation_id": "111",
      "title": "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models",
      "authors": [
        "Peng Xu",
        "Wenqi Shao",
        "Kaipeng Zhang",
        "Peng Gao",
        "Shuo Liu",
        "Meng Lei",
        "Fanqing Meng",
        "Siyuan Huang",
        "Yu Qiao",
        "Ping Luo"
      ],
      "year": "2023",
      "venue": "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models",
      "arxiv": "arXiv:2306.09265"
    },
    {
      "citation_id": "112",
      "title": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning",
      "authors": [
        "Zhiyang Xu",
        "Ying Shen",
        "Lifu Huang"
      ],
      "year": "2022",
      "venue": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning",
      "arxiv": "arXiv:2212.10773"
    },
    {
      "citation_id": "113",
      "title": "Multimodal chatgpt for medical applications: an experimental study of gpt-4v",
      "authors": [
        "Zhiling Yan",
        "Kai Zhang",
        "Rong Zhou",
        "Lifang He",
        "Xiang Li",
        "Lichao Sun"
      ],
      "year": "2023",
      "venue": "Multimodal chatgpt for medical applications: an experimental study of gpt-4v",
      "arxiv": "arXiv:2310.19061"
    },
    {
      "citation_id": "114",
      "title": "Bag-of-visual-words and spatial extensions for land-use classification",
      "authors": [
        "Yi Yang",
        "Shawn Newsam"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems"
    },
    {
      "citation_id": "115",
      "title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "authors": [
        "Zhengyuan Yang",
        "Linjie Li",
        "Jianfeng Wang",
        "Kevin Lin",
        "Ehsan Azarnasab",
        "Faisal Ahmed",
        "Zicheng Liu",
        "Ce Liu",
        "Michael Zeng",
        "Lijuan Wang"
      ],
      "year": "2023",
      "venue": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
      "arxiv": "arXiv:2303.11381"
    },
    {
      "citation_id": "116",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "117",
      "title": "Irfl: Image recognition of figurative language",
      "authors": [
        "Ron Yosef",
        "Yonatan Bitton",
        "Dafna Shahaf"
      ],
      "year": "2023",
      "venue": "Irfl: Image recognition of figurative language",
      "arxiv": "arXiv:2303.15445"
    },
    {
      "citation_id": "118",
      "title": "Ferret: Refer and ground anything anywhere at any granularity",
      "authors": [
        "Haoxuan You",
        "Haotian Zhang",
        "Zhe Gan",
        "Xianzhi Du",
        "Bowen Zhang",
        "Zirui Wang",
        "Liangliang Cao",
        "Shih-Fu Chang",
        "Yinfei Yang"
      ],
      "year": "2023",
      "venue": "Ferret: Refer and ground anything anywhere at any granularity",
      "arxiv": "arXiv:2310.07704"
    },
    {
      "citation_id": "119",
      "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "authors": [
        "Peter Young",
        "Alice Lai",
        "Micah Hodosh",
        "Julia Hockenmaier"
      ],
      "year": "2014",
      "venue": "TACL"
    },
    {
      "citation_id": "120",
      "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities",
      "authors": [
        "Weihao Yu",
        "Zhengyuan Yang",
        "Linjie Li",
        "Jianfeng Wang",
        "Kevin Lin",
        "Zicheng Liu",
        "Xinchao Wang",
        "Lijuan Wang"
      ],
      "year": "2023",
      "venue": "Mm-vet: Evaluating large multimodal models for integrated capabilities",
      "arxiv": "arXiv:2308.02490"
    },
    {
      "citation_id": "121",
      "title": "Bartscore: Evaluating generated text as text generation",
      "authors": [
        "Weizhe Yuan",
        "Graham Neubig",
        "Pengfei Liu"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "122",
      "title": "From recognition to cognition: Visual commonsense reasoning",
      "authors": [
        "Rowan Zellers",
        "Yonatan Bisk",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "123",
      "title": "Magicbrush: A manually annotated dataset for instruction-guided image editing",
      "authors": [
        "Kai Zhang",
        "Lingbo Mo",
        "Wenhu Chen",
        "Huan Sun",
        "Yu Su"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "124",
      "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "authors": [
        "Renrui Zhang",
        "Jiaming Han",
        "Chris Liu",
        "Peng Gao",
        "Aojun Zhou",
        "Xiangfei Hu",
        "Shilin Yan",
        "Pan Lu",
        "Hongsheng Li",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "arxiv": "arXiv:2303.16199"
    },
    {
      "citation_id": "125",
      "title": "Evaluating text generation with bert",
      "authors": [
        "Tianyi Zhang",
        "Varsha Kishore",
        "Felix Wu",
        "Kilian Weinberger",
        "Yoav Artzi",
        "Bertscore"
      ],
      "year": "2019",
      "venue": "Evaluating text generation with bert",
      "arxiv": "arXiv:1904.09675"
    },
    {
      "citation_id": "126",
      "title": "Semantic understanding of scenes through the ade20k dataset",
      "authors": [
        "Bolei Zhou",
        "Hang Zhao",
        "Xavier Puig",
        "Tete Xiao",
        "Sanja Fidler",
        "Adela Barriuso",
        "Antonio Torralba"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "127",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "128",
      "title": "For this dataset, each data point has two images and a sentence that talks about the images. We concatenate the two images so that we pass a single image in the model",
      "venue": "For this dataset, each data point has two images and a sentence that talks about the images. We concatenate the two images so that we pass a single image in the model"
    },
    {
      "citation_id": "129",
      "title": "Licenses: Images have licenses CC BY-SA 2",
      "venue": "You are given an image and a related text, use the image as context and reply with true or false only Text: <text> Answer: Access restrictions: The dataset is downloadable from"
    },
    {
      "citation_id": "130",
      "title": "KOSMOS-2 is based on a causal Transformer Language Model, and has the architecture similar to Kosmos1 [40]. It is trained on the next-token prediction task. In addition to the pre-training data used to train Kosmos1, grounded image-text pairs are added to the dataset to train Kosmos2. Overall, Kosmos2 is trained using interleaved image-text data and later instruction-tuned using both multimodal and language-only instructions. We evaluate the ydshieh/kosmos-2-patch14-224 model from HuggingFace 18 which has a total of 1.6B parameters",
      "venue": "Access restrictions: The model is available to use from"
    },
    {
      "citation_id": "131",
      "title": "OWL uses a vision foundation model to encode input image and uses a visual abstractor model to summarize the input from the encoder. The abstractor output along with the text queries are then passed to a pre-trained language foundation model that generates the response. The model is first pre-trained using supervised fine-tuning of all the parameters except for the language models",
      "authors": [
        "Mplug"
      ],
      "venue": "LICENSE Access restrictions: The model is available to use from"
    },
    {
      "citation_id": "132",
      "title": "GPT-4V is a multimodal extension to GPT-4 which has been trained on the next word prediction task using image and text data from the internet and licensed data sources and fine tuned using RLHF[84],[20]. We use 'gpt-4-vision-preview' as a chosen model for our evaluation. As of evaluating the models, 'gpt-4-vision-preview' points to 'gpt-4-1106-vision-preview' in the OpenAI API interface which has been trained up to",
      "venue": "None Access restrictions: The model is available via OpenAI's API"
    },
    {
      "citation_id": "133",
      "title": "These models have been trained on multimodal and multilingual data comprising of data from web documents, books, and code, and includes image, audio, and video data",
      "venue": "GEMINI is a series of multimodal large language models which support interleaved inputs"
    }
  ]
}