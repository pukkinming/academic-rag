{
  "paper_id": "2405.19373v1",
  "title": "Multi-Modal Mood Reader: Pre-Trained Model Empowers Cross-Subject Emotion Recognition",
  "published": "2024-05-28T14:31:11Z",
  "authors": [
    "Yihang Dong",
    "Xuhang Chen",
    "Yanyan Shen",
    "Michael Kwok-Po Ng",
    "Tao Qian",
    "Shuqiang Wang"
  ],
  "keywords": [
    "EEG-based emotion recognition",
    "Pre-trained Model",
    "spatial-temporal attention",
    "masked brain signal modeling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition based on Electroencephalography (EEG) has gained significant attention and diversified development in fields such as neural signal processing and affective computing. However, the unique brain anatomy of individuals leads to non-negligible natural differences in EEG signals across subjects, posing challenges for cross-subject emotion recognition. While recent studies have attempted to address these issues, they still face limitations in practical effectiveness and model framework unity. Current methods often struggle to capture the complex spatialtemporal dynamics of EEG signals and fail to effectively integrate multimodal information, resulting in suboptimal performance and limited generalizability across subjects. To overcome these limitations, we develop a Pre-trained model based Multimodal Mood Reader for crosssubject emotion recognition that utilizes masked brain signal modeling and interlinked spatial-temporal attention mechanism. The model learns universal latent representations of EEG signals through pre-training on large scale dataset, and employs Interlinked spatial-temporal attention mechanism to process Differential Entropy(DE) features extracted from EEG data. Subsequently, a multi-level fusion layer is proposed to integrate the discriminative features, maximizing the advantages of features across different dimensions and modalities. Extensive experiments on public datasets demonstrate Mood Reader's superior performance in cross-subject emotion recognition tasks, outperforming state-of-the-art methods. Additionally, the model is dissected from attention perspective, providing qualitative analysis of emotion-related brain areas, offering valuable insights for affective research in neural signal processing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Brain-computer interface (BCI) systems have long been an aspirational goal for researchers in the fields of computer science, neuroscience, and psychology.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Arxiv:2405.19373V1 [Eess.Sp] 28 May 2024",
      "text": "The envisioned maturation of BCI technology is expected to significantly expand human sensory, cognitive, and operational capabilities, offering unprecedented depth and breadth in human-machine interaction. However, truly efficient human-machine interaction relies not solely on the machine's ability to interpret and execute human commands, but more critically, on the sensitive detection and accurate recognition of users' implicit emotional states. Consequently, the task of emotion recognition has naturally emerged as a key research area.\n\nAlthough emotion recognition methods based on various physiological signals each have their unique characteristics, they predominantly face challenges related to the complexity of signal collection, difficulties in data processing, and high costs. Therefore, non-invasive EEG, with its relatively low cost, convenient signal collection, superior signal representation capability, and non-harmful nature to subjects, has rapidly become a primary research focus in the field of emotion recognition. The array of electrodes placed on the scalp effectively collects signals reflecting brain electrical activity. Through precise analysis and processing of these signals, an individual's emotional state can be effectively revealed.\n\nNon-invasive EEG signals are not without flaws. The unique natural physiological and anatomical structures of each individual introduce various degrees and aspects of noise interference into the measured EEG, imparting nonstationary characteristics to it. Additionally, issues such as the non-Euclidean distribution of multi-channel EEG electrodes based on biological topography collectively impact the accuracy of cross-subject emotion recognition tasks. Researchers have attempted to tackle these challenges from different directions. Transfer learning, as an indirect approach, has been utilized to migrate emotion recognition models, originally trained and adapted for existing subjects, to new individuals, aiming to minimize the EEG differences between the source and target domains  [1, 2] . Although this approach has indeed achieved certain recognition effects, it does not fundamentally solve the problem of cross-subject emotion recognition. Considering the graph-like topological structure of EEG channels and the rapid development of Graph Neural Networks (GNN), a surge of cross-subject emotion recognition methods based on GNN has emerged  [3, 4] , attempting to capture the local and global relationships among EEG channels. Similarly, other methods have also achieved certain yet limited improvements in recognition accuracy  [5, 6] .\n\nMotivated by the recent emergence of pre-trained models and their outstanding performance in downstream tasks  [7, 8, 9, 10, 11] , we recognized that the high-dimensional semantic information of EEG extracted by encoders trained on large-scale subject-independent datasets may contain global generic representations beneficial for emotion recognition tasks. Simultaneously, considering the DE feature, which has been proven to be the most effective individual computational characteristic for EEG-based emotion recognition tasks  [12, 13, 14] , as well as eye movement features often used as an additional modality to aid emotion recognition tasks and confirmed to improve recognition accuracy  [15] , we propose Mood Reader, a novel multi-modal and cross-scale fusion model for cross-subject emotion recognition that integrates global generic representations of EEG and the spatio-temporal interaction information in specific DE features. Specifically, our contributions are as follows:\n\n1. We propose an emotion recognition model architecture that integrates multimodal and cross-scale information, demonstrating exceptional performance in cross-subject recognition tasks. This architecture also proves that encoders pre-trained on large-scale EEG data possess the ability to learn emotionrelated features to a certain extent. 2. We have designed an attention-based interlinked spatio-temporal module for learning the compensatory relationships between spatio-temporal information, which aids in the fusion of spatio-temporal features.    [18, 19, 20, 21, 22, 23] . The ablation studies of these researches powerfully validate the correctness of the direction of multimodal data fusion in the field of emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Masked Brain Signal Modeling",
      "text": "With the rapid development of self-supervised pre-training models in the fields of computer vision and natural language processing, researchers have migrated this technology, which can learn generic knowledge representations for target tasks, to multiple fields including brain signal decoding. MBSM, proposed by Chen et al.  [7] ., is a self-supervised learning model for large-scale fMRI datasets, which helps its encoder learn the general representation in fMRI signals through the learning process of repeatedly reconstructing complete data from unmasked fMRI signals, further adapting the encoder to different downstream tasks using simple fine-tuning techniques  [7, 24] . Meanwhile, Bai et al. successfully overcame the inherent variability and noise of EEG data by deeply mining the semantics of EEG signals over time, migrated this technology to EEG, and applied it to downstream tasks such as decoding high-resolution images from brain activity  [25] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spatial-Temporal Attention Mechanism",
      "text": "Complex neural activities are often effectively achieved by the synergistic collaborative processing of multiple sets of continuous neural signals across various distinct neural regions.  [26, 27, 28, 29, 30, 31, 32, 33] . Therefore, for research fields including brain decoding and neural information processing, it is crucial to simultaneously capture the temporal and spatial information in brain signal data, for instance, fMRI and EEG. Since the attention mechanism was proposed, various application fields of deep learning, including but not limited to the field of emotion recognition based on EEG signals, have made great progress, and many excellent spatiotemporal information extraction modules have been deduced  [34, 35, 36] . In the realm of EEG-based emotion recognition, the Spatial-Temporal Attention (STA) mechanism emerges as a notable innovation for enhancing the interpretability and performance of deep learning models. This type of architecture ingeniously integrates spatial and temporal dimensions of EEG signals through parallel attention pathways, enabling the model to concurrently learn spatial correlations across EEG channels and temporal dependencies within signal sequences. Li et al. employed the spatio-temporal combination network R2G-STNN, which contains local-global feature combing, to extract the intrinsic information of EEG signals  [37] . In order to mine the spatiotemporal information related to emotional judgment, Gong et al. designed a stacked parallel spatial and temporal attention streams to respectively extract the spatial features and temporal features of the specially processed EEG signals  [38] . Although previous researchers have obtained satisfactory results, most of them have ignored the interaction and mutual compensation between spatial information and temporal information in EEG signals. In these network streamlines, the two kinds of information are often ignored. Parallel offload processing, which is contrary to the processing flow of neural signals in complex neural activities, may not be enough to handle more complex neural signal processing tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overview",
      "text": "Mood Reader is an emotion recognition model that accommodates multi-modal, cross-scale information and successfully integrates these features, its overall architecture is illustrated in Figure  1 . The model encompasses three distinct types of input. EEG monitoring data, subjected to simple preprocessing, are encoded by an encoder pre-trained on a large-scale dataset, resulting in outputs that contain rich semantic representations. An attention-based interlinked spatialtemporal mechanism captures the intrinsic spatio-temporal information from DE features extracted from EEG data. Additionally, a set of similar temporal attention blocks analyzes corresponding eye movement features, aiming to complement the shortcomings of EEG data. The acquired features are progressively fused in a sequential manner, ensuring that the model genuinely learns the comprehensive complementarity between different modal and scale information, and utilizes it for emotion recognition tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mbsm Based Eeg General Representation Learning",
      "text": "Due to the inherent brain differences among subjects and external noise affecting the signals collected by non-invasive EEG, we adopted a pre-training technique known as masked brain signal modeling, which has been proven effective multiple times, to learn meaningful and contextually rich general knowledge representations from cross-subject, noisy, large-scale EEG data  [25, 7, 24] . Specifically, we completed this task by training an autoencoder-decoder with an asymmetric architecture similar to that in  [25] on the EEG Motor Movement/Imagery Dataset  [39] . In this model, the temporal signals of EEG data were divided into tokens of a specific size, where a larger ratio of tokens would be randomly masked, and the architecture-simple decoder had to reconstruct the EEG data using the remaining unmasked tokens arrangement, combined with semantically rich embeddings outputted by the encoder after processing the original EEG data.The performance of masked reconstruction improves and reaches a peak when the mask ratio hits 75%  [25, 7, 24] . Consequently, by removing the decoder from this trained model, we obtained an encoder with excellent capability in extracting general EEG representations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Attention Based Interlinked Spatial-Temporal Mechanism",
      "text": "Deep learning analyses of neural processes typically begin with various types of feature extraction. For complex neural activities like EEG-based emotion recognition, which exhibit high spatiotemporal continuity, it's crucial to accurately unearth intrinsic spatial and temporal features and their interrelations to obtain more valuable information. To address this challenge, this section introduces the interlinked spatial-temporal attention module for processing DE features, comprised of multiple parallel spatiotemporal blocks and interactive spatiotemporal blocks (as depicted in Figure .), these blocks collectively facilitate the extraction of spatio-temporal information and enable the communication and complementation between the extracted spatial and temporal features.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Spatial And Temporal Representation Of De Features",
      "text": "In the domain of EEG-based emotion recognition, the DE feature, which quantifies the variability of EEG signals, has been proven to be the most effective feature, capturing brain activities related to emotions. It itself has channel-related explicit spatial information and implicit temporal information compressed within a single sliding window.In order to make the spatiotemporal information in the DE feature more balanced, we expand the number of sliding windows in the DE feature to the explicit temporal dimension, obtaining X ‚àà R N √óF √óC , where N denotes the number of sliding windows involved in the DE feature computation. For further dimension transformation, we get XS ‚àà R C√ó(N ‚Ä¢F ) for spatial information representation and XT ‚àà R N √ó(C‚Ä¢F ) for temporal information representation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Parallel Spatiotemporal Feature Extraction",
      "text": "To capture the dynamically varying key information, we apply dedicated spatial and temporal attention blocks to the differentiated DE feature spatial representation XS and temporal representation XT , respectively. Specifically, for the spatial representation XS ‚àà R C√ó(N ‚Ä¢F ) , an initial layer normalization is employed to to yield XS , which effectively mitigates the internal covariate shift within the spatial representation data, thereby maintaining the stability of its distribution, as follows,\n\nAfter the normalization, multi-head attention (MHA) computation is implemented on X ‚Ä≤ S . Within head i, X ‚Ä≤ S is processed by three separate linear networks, transforming the input into different representational spaces to obtain the corresponding query Q i S , key K i S , and value V i S , denoted as,\n\nWhere W qi S , W ki S , W vi S are the learnable network parameters respectively. Based on the attention calculation method, the attention output for each head i is obtained as\n\n. With some processing, the DE feature space representation's MHA output A S can be obtained as\n\n, where h is the number of attention heads, and W MHA S is a linear mapping weight. The attention calculation is described as,\n\nwhere d k represents the dimension of the key. At the conclusion of the spatial attention block, a residual connection is introduced by adding the dropout-processed MHA output A S to X ‚Ä≤ S . This combined result is then subjected to another layer normalization to produce the final spatial representation of the DE feature X S , which is within the space R C√ó(N ‚Ä¢F ) . The calculation process of this part is as follows,\n\nSimilarly, we process XT ‚àà R N √ó(C‚Ä¢F ) with a structurally analogous temporal attention block to obtain the temporal representation of the DE feature",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "ùêπùëÜùëá",
      "text": "Fig.  2 : Overview of the spatial interlink block, the temporal feature X T undergos multiple transformations to align with the spatial feature X S , and after concatenation, the interlink process is completed through multi-head attention computation.\n\nInterlink Between Spatio-Temporal Block For the spatial features X S and temporal features X T , which are processed by the spatio-temporal attention block, they are fed into the corresponding interlink block to facilitate the intersection of primary and secondary dimensions. Specifically, the interlinked spatial block will receive both X S and X T . However, for the primary information X S , which has already achieved a high degree of spatial information integration after being processed by the spatial attention block, it only requires a single linear transformation to become X S ‚Ä≤ , in order to preserve its existing information.\n\nAs for X T , it undergoes multiple transformations in an attempt to align with the spatial feature dimensions, resulting in X T ‚Ä≤ . Subsequently, we concatenate these two feature parts and perform MHA calculation to obtain the spatial features F S T that are interlinked with the temporal dimension, as shown in Figure  2 .\n\nSimilarly, we can also obtain the temporal features F T S that are interlinked with the spatial dimension.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multi-Level Score Filtering For Feature Fusion",
      "text": "Previous sections presented outputs from various modules, each designed for emotion recognition through different pathways, yielding high-dimensional features. To address potential redundancy and the needs of multimodal cross-scale fusion, we introduce a multi-level fusion layer based on attention mechanisms. This module synergizes and refines features by highlighting relevant information and filtering out redundancy, thus improving multimodal emotion recognition efficacy.\n\nPrior to the commencement of the fusion process, the MBSM latent representation, spatial-temporal representation, and eye movement representation, are projected onto a unified dimensional space through a series of transformations, starting with layer normalization, followed by flattening of the feature vectors, and culminating in a linear transformation. As a result of these operations, a unified feature representation F ‚àà R D unified is obtained, where D unified denotes the dimensionality of the unified feature space.\n\nIn the first level of fusion, specifically for the interlinked spatial-temporal features F S T and F T S , preprocessing is executed utilizing a linear layer and layer normalization, which preserves their dimensional attributes. Subsequently, inspired by  [18, 40, 41] , a simplified cross-attention mechanism is employed to delineate the intrinsic spatial-temporal relationships between the two, thereby augmenting the interactivity of the internal information. Specifically, the attention scores Score s and Score t , which represent the spatial features taking into account temporal features and time features taking into account spatial features respectively, are transformed using the softmax function to ascertain the corresponding fusion weights c s and c t ,\n\nwhere W S ,W T ‚àà R D unified √ñD unified is a learnable weight matrix, the process of obtaining c t is analogous to this. The spatial-temporal fusion features F ST are then calculated by,\n\nThe processing of the MBSM latent representation and eye movement representation, corresponding to F EEG and F EY E respectively, is conducted in a manner akin to the aforementioned methodology. This approach yields an additional fused feature F EE , computed as a weighted sum:\n\nwhere c eeg and c eye represent the fusion weights derived from the respective attention scores. In the terminal fusion layer, an integration is requisite for the synthesized features F ST and F EE , which respectively represent the interlinked spatio-temporal information emanating from the DE feature and the comprehensive information spanning multiple modalities and scales.The features that have undergone layer normalization are concatenated and subsequently processed through self-attention computation, yielding the final integrated feature representation M ,\n\nFor the feature M , an initial batch normalization is employed, followed by the deployment of a classifier comprised of three linear layers and a softmax function, which outputs predictions for the emotional labels y. The discrepancy between the predicted emotional labels and the true emotional labels ≈∑ is quantified using the cross-entropy loss function,\n\nwherein N represents the batch size, and C designates the count of label categories.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Interpretability",
      "text": "In order to provide a biologically plausible interpretation and inference for the research on EEG-based emotion recognition, as well as to substantiate the efficacy of the proposed model, we have conducted multiple visualizations of the attention outputs during the encoding process of EEG signals from test samples, projecting them back onto the electrode location map. This approach has allowed for a more intuitive validation of the brain regions associated with the task of emotion recognition, thereby enabling more insightful deductions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiment Result And Analysis",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Datasets And Pre-Processing",
      "text": "Extensive experiments have been conducted on two public datasets, SEED and SEED-V. The SEED dataset includes EEG monitoring data and corresponding emotional labels (sad, happy, and neutral) from 15 subjects. Each subject completed 3 sessions, with each session comprising 15 trials, resulting in a total of 45 trials. Similarly, SEED-V was completed by 16 subjects, each participating in 45 trials, and includes EEG data along with corresponding labels for five emotions (disgust, fear, sad, happy, and neutral).\n\nIn the preprocessing of raw EEG data, a sequence involving a bandpass filter with cutoff frequencies of 0.1Hz and 70Hz, followed by a notch filter at 50Hz, was implemented. Subsequently, the sampling frequency was reduced to 200Hz from its original rate. For EEG segments corresponding to trials of varied lengths, a 4s non-overlapping Hanning window was utilized for segmentation in reverse order. The Short Time Fourier Transform (STFT) was then applied to calculate the DE feature across five frequency domains. Every four sliding window calculations were grouped together to extract EEG signals requisite for the pretrained model.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Baseline Model And Settings",
      "text": "We conducted subject-independent experiments on the SEED and SEED-V datasets using baseline models including DGCNN  [42] , RGNN  [4] , SOGNN  [43]  and BFE-Net  [44] . Notably, baseline models that were trained using a single modality were explicitly annotated. The experimental framework utilized was PyTorch, with the GPU being NVIDIA A800 80GB PCIe. Furthermore, for each experiment, the data were randomly divided into training and testing sets in an 8:2 ratio. The model performance was evaluated based on the average accuracy and variance on the testing set.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "Experiment Result Table  1  presents the experimental results of the baseline model and our method on the SEED and SEED-V datasets, with annotations regarding the categories of data utilized. The results demonstrate the consistently superior classification performance of Mood Reader across different datasets. Furthermore, they validate the efficacy of the sequentially combined DE features through a sliding window approach in the task of emotion recognition, as well as the correctness of the multimodal cross-scale information fusion strategy.\n\nInterpretation The results of attention visualization are summarized in Figure  3 . It can be observed that as the emotional recognition capability improves, the network's attention on EEG signals gradually shifts from a scattered global distribution to concentrated attention on specific regions. These brain regions include the frontal lobe area, areas of the left and right temporal lobes, and a small portion of the parietal lobe, which has been proven to be closely related to the generation and processing of emotions  [45] . Additionally, we noticed that there are also small areas within the occipital lobe, primarily responsible for visual information processing, that exhibit significant attention. Given that a substantial part of the stimuli in the experimental datasets SEED and SEED-V comes from visual stimuli in videos, we have reasonable grounds to propose the hypothesis that \"visually encoded information in the human brain is directly involved in emotion generation\" to a certain extent. This hypothesis also represents the holistic view that various parts of the brain participate in different functions and collectively process complex information  [46] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Studies",
      "text": "To substantiate the effectiveness of the employed modules, we conducted ablation experiments on the SEED-V dataset using a stepwise stacking approach for the modules, with the specific experimental details as follows, and the results are depicted in Figure  4",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present the Mood Reader, a novel multimodal cross-scale fusion model for cross-subject emotion recognition based on EEG signals.Our model effectively integrates masked brain signal modeling for learning universal latent representations and an interlinked spatial-temporal attention mechanism to capture the complex dynamics of EEG signals. The multi-level fusion layer maximizes the advantages of features across different dimensions and modalities, leading to superior performance in cross-subject emotion recognition tasks. Furthermore, the model's interpretability, achieved through attention visualization, provides valuable insights into emotion-related brain areas, contributing to the understanding of neural processes underlying emotions. In conclusion, Mood Reader represents a significant step forward in cross-subject EEG-based emotion recognition, leveraging multimodal cross-scale fusion and advanced attention mechanisms.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The model encompasses three distinct types",
      "page": 4
    },
    {
      "caption": "Figure 1: The overall architecture of our proposed model and the way related data",
      "page": 5
    },
    {
      "caption": "Figure 2: Overview of the spatial interlink block, the temporal feature XT under-",
      "page": 7
    },
    {
      "caption": "Figure 2: Similarly, we can also obtain the temporal features FTS that are interlinked with",
      "page": 8
    },
    {
      "caption": "Figure 3: Attention visualization. We visualized the model‚Äôs attention weights at",
      "page": 9
    },
    {
      "caption": "Figure 3: It can be observed that as the emotional recognition capability improves, the",
      "page": 11
    },
    {
      "caption": "Figure 4: 1. STB+CF: SWC DE with spatial-temporal block + concatenation fusion",
      "page": 11
    },
    {
      "caption": "Figure 4: The results of the ablation studies, conducted through progressive stack-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "sq.wang@siat.ac.cn"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "Abstract. Emotion recognition based on Electroencephalography (EEG)"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "has gained significant attention and diversified development in fields such"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "as neural signal processing and affective computing. However, the unique"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "brain anatomy of individuals leads to non-negligible natural differences in"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "EEG signals across subjects, posing challenges for cross-subject emotion"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "recognition. While recent studies have attempted to address these issues,"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "they still face limitations in practical effectiveness and model framework"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "unity. Current methods often struggle to capture the complex spatial-"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "temporal dynamics of EEG signals and fail to effectively integrate mul-"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "timodal\ninformation,\nresulting in suboptimal performance and limited"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "generalizability across\nsubjects. To overcome these limitations, we de-"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "velop a Pre-trained model based Multimodal Mood Reader\nfor\ncross-"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "subject emotion recognition that utilizes masked brain signal modeling"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "and interlinked spatial-temporal attention mechanism. The model\nlearns"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "universal\nlatent representations of EEG signals through pre-training on"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "large scale dataset, and employs\nInterlinked spatial-temporal attention"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "mechanism to process Differential Entropy(DE) features extracted from"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "EEG data. Subsequently, a multi-level\nfusion layer\nis proposed to in-"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "tegrate the discriminative features, maximizing the advantages of\nfea-"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "tures across different dimensions and modalities. Extensive experiments"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "on public datasets demonstrate Mood Reader‚Äôs superior performance in"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "cross-subject emotion recognition tasks, outperforming state-of-the-art"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "methods. Additionally,\nthe model\nis dissected from attention perspec-"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "tive, providing qualitative analysis of emotion-related brain areas, offer-"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "ing valuable insights for affective research in neural signal processing."
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "Keywords: EEG-based emotion recognition ¬∑ Pre-trained Model ¬∑"
        },
        {
          "4 Faculty of Innovation Engineering, Macau University of Science and Technology": "spatial-temporal attention ¬∑ masked brain signal modeling"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nY. Dong et al.": "The envisioned maturation of BCI\ntechnology is expected to significantly ex-"
        },
        {
          "2\nY. Dong et al.": "pand human sensory, cognitive, and operational capabilities, offering unprece-"
        },
        {
          "2\nY. Dong et al.": "dented depth and breadth in human-machine interaction. However, truly efficient"
        },
        {
          "2\nY. Dong et al.": "human-machine interaction relies not solely on the machine‚Äôs ability to interpret"
        },
        {
          "2\nY. Dong et al.": "and execute human commands, but more critically, on the sensitive detection"
        },
        {
          "2\nY. Dong et al.": "and accurate recognition of users‚Äô\nimplicit emotional states. Consequently, the"
        },
        {
          "2\nY. Dong et al.": "task of emotion recognition has naturally emerged as a key research area."
        },
        {
          "2\nY. Dong et al.": "Although emotion recognition methods based on various physiological signals"
        },
        {
          "2\nY. Dong et al.": "each have their unique characteristics, they predominantly face challenges related"
        },
        {
          "2\nY. Dong et al.": "to the complexity of\nsignal collection, difficulties\nin data processing, and high"
        },
        {
          "2\nY. Dong et al.": "costs. Therefore, non-invasive EEG, with its relatively low cost, convenient signal"
        },
        {
          "2\nY. Dong et al.": "collection, superior signal representation capability, and non-harmful nature to"
        },
        {
          "2\nY. Dong et al.": "subjects, has\nrapidly become a primary research focus\nin the field of emotion"
        },
        {
          "2\nY. Dong et al.": "recognition. The array of electrodes placed on the scalp effectively collects signals"
        },
        {
          "2\nY. Dong et al.": "reflecting brain electrical activity. Through precise analysis and processing of"
        },
        {
          "2\nY. Dong et al.": "these signals, an individual‚Äôs emotional state can be effectively revealed."
        },
        {
          "2\nY. Dong et al.": "Non-invasive EEG signals are not without flaws. The unique natural phys-"
        },
        {
          "2\nY. Dong et al.": "iological\nand anatomical\nstructures\nof\neach individual\nintroduce\nvarious de-"
        },
        {
          "2\nY. Dong et al.": "grees and aspects of noise interference into the measured EEG,\nimparting non-"
        },
        {
          "2\nY. Dong et al.": "stationary characteristics\nto it. Additionally,\nissues\nsuch as\nthe non-Euclidean"
        },
        {
          "2\nY. Dong et al.": "distribution of multi-channel EEG electrodes based on biological\ntopography"
        },
        {
          "2\nY. Dong et al.": "collectively impact the accuracy of cross-subject emotion recognition tasks. Re-"
        },
        {
          "2\nY. Dong et al.": "searchers have attempted to tackle\nthese\nchallenges\nfrom different directions."
        },
        {
          "2\nY. Dong et al.": "Transfer\nlearning, as an indirect approach, has been utilized to migrate emo-"
        },
        {
          "2\nY. Dong et al.": "tion recognition models, originally trained and adapted for existing subjects, to"
        },
        {
          "2\nY. Dong et al.": "new individuals, aiming to minimize\nthe EEG differences between the\nsource"
        },
        {
          "2\nY. Dong et al.": "and target domains\n[1,2]. Although this approach has\nindeed achieved certain"
        },
        {
          "2\nY. Dong et al.": "recognition effects,\nit does not fundamentally solve the problem of cross-subject"
        },
        {
          "2\nY. Dong et al.": "emotion recognition. Considering the graph-like topological\nstructure of EEG"
        },
        {
          "2\nY. Dong et al.": "channels and the rapid development of Graph Neural Networks (GNN), a surge"
        },
        {
          "2\nY. Dong et al.": "of cross-subject emotion recognition methods based on GNN has emerged [3,4],"
        },
        {
          "2\nY. Dong et al.": "attempting to capture the local and global relationships among EEG channels."
        },
        {
          "2\nY. Dong et al.": "Similarly, other methods have also achieved certain yet limited improvements in"
        },
        {
          "2\nY. Dong et al.": "recognition accuracy [5,6]."
        },
        {
          "2\nY. Dong et al.": "Motivated by the\nrecent\nemergence\nof pre-trained models\nand their\nout-"
        },
        {
          "2\nY. Dong et al.": "standing performance in downstream tasks [7,8,9,10,11], we recognized that the"
        },
        {
          "2\nY. Dong et al.": "high-dimensional semantic information of EEG extracted by encoders trained on"
        },
        {
          "2\nY. Dong et al.": "large-scale subject-independent datasets may contain global generic representa-"
        },
        {
          "2\nY. Dong et al.": "tions beneficial\nfor emotion recognition tasks. Simultaneously, considering the"
        },
        {
          "2\nY. Dong et al.": "DE feature, which has been proven to be the most effective individual computa-"
        },
        {
          "2\nY. Dong et al.": "tional characteristic for EEG-based emotion recognition tasks [12,13,14], as well"
        },
        {
          "2\nY. Dong et al.": "as eye movement features often used as an additional modality to aid emotion"
        },
        {
          "2\nY. Dong et al.": "recognition tasks and confirmed to improve recognition accuracy [15], we propose"
        },
        {
          "2\nY. Dong et al.": "Mood Reader, a novel multi-modal and cross-scale fusion model for cross-subject"
        },
        {
          "2\nY. Dong et al.": "emotion recognition that integrates global generic representations of EEG and"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-modal Mood Reader\n3": "the spatio-temporal\ninteraction information in specific DE features. Specifically,"
        },
        {
          "Multi-modal Mood Reader\n3": "our contributions are as follows:"
        },
        {
          "Multi-modal Mood Reader\n3": "1. We propose an emotion recognition model architecture that integrates multi-"
        },
        {
          "Multi-modal Mood Reader\n3": "modal and cross-scale information, demonstrating exceptional performance"
        },
        {
          "Multi-modal Mood Reader\n3": "in cross-subject recognition tasks. This architecture also proves that encoders"
        },
        {
          "Multi-modal Mood Reader\n3": "pre-trained on large-scale EEG data possess\nthe ability to learn emotion-"
        },
        {
          "Multi-modal Mood Reader\n3": "related features to a certain extent."
        },
        {
          "Multi-modal Mood Reader\n3": "2. We have designed an attention-based interlinked spatio-temporal module for"
        },
        {
          "Multi-modal Mood Reader\n3": "learning the compensatory relationships between spatio-temporal\ninforma-"
        },
        {
          "Multi-modal Mood Reader\n3": "tion, which aids in the fusion of spatio-temporal\nfeatures."
        },
        {
          "Multi-modal Mood Reader\n3": "3. Supported by extensive experiments, we provide a biologically plausible in-"
        },
        {
          "Multi-modal Mood Reader\n3": "terpretation of emotion recognition research based on EEG and make rea-"
        },
        {
          "Multi-modal Mood Reader\n3": "sonable hypotheses."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nY. Dong et al.": "Chen et al.\n[7].,\nis a self-supervised learning model for large-scale fMRI datasets,"
        },
        {
          "4\nY. Dong et al.": "which helps its encoder learn the general representation in fMRI signals through"
        },
        {
          "4\nY. Dong et al.": "the learning process of repeatedly reconstructing complete data from unmasked"
        },
        {
          "4\nY. Dong et al.": "fMRI signals,\nfurther adapting the encoder to different downstream tasks using"
        },
        {
          "4\nY. Dong et al.": "simple fine-tuning techniques [7,24]. Meanwhile, Bai et al. successfully overcame"
        },
        {
          "4\nY. Dong et al.": "the inherent variability and noise of EEG data by deeply mining the seman-"
        },
        {
          "4\nY. Dong et al.": "tics of EEG signals over time, migrated this technology to EEG, and applied it"
        },
        {
          "4\nY. Dong et al.": "to downstream tasks such as decoding high-resolution images from brain activ-"
        },
        {
          "4\nY. Dong et al.": "ity [25]."
        },
        {
          "4\nY. Dong et al.": "2.3\nSpatial-Temporal Attention Mechanism"
        },
        {
          "4\nY. Dong et al.": "Complex neural activities are often effectively achieved by the synergistic col-"
        },
        {
          "4\nY. Dong et al.": "laborative processing of multiple sets of continuous neural signals across various"
        },
        {
          "4\nY. Dong et al.": "distinct neural\nregions.\n[26,27,28,29,30,31,32,33]. Therefore,\nfor\nresearch fields"
        },
        {
          "4\nY. Dong et al.": "including brain decoding and neural\ninformation processing,\nit is crucial to si-"
        },
        {
          "4\nY. Dong et al.": "multaneously capture the temporal and spatial\ninformation in brain signal data,"
        },
        {
          "4\nY. Dong et al.": "for instance,\nfMRI and EEG. Since the attention mechanism was proposed, var-"
        },
        {
          "4\nY. Dong et al.": "ious application fields of deep learning,\nincluding but not\nlimited to the field"
        },
        {
          "4\nY. Dong et al.": "of emotion recognition based on EEG signals, have made great progress, and"
        },
        {
          "4\nY. Dong et al.": "many excellent\nspatiotemporal\ninformation extraction modules have been de-"
        },
        {
          "4\nY. Dong et al.": "duced [34,35,36].\nIn the realm of EEG-based emotion recognition,\nthe Spatial-"
        },
        {
          "4\nY. Dong et al.": "Temporal Attention (STA) mechanism emerges as a notable innovation for en-"
        },
        {
          "4\nY. Dong et al.": "hancing the interpretability and performance of deep learning models. This type"
        },
        {
          "4\nY. Dong et al.": "of architecture ingeniously integrates spatial and temporal dimensions of EEG"
        },
        {
          "4\nY. Dong et al.": "signals through parallel attention pathways, enabling the model to concurrently"
        },
        {
          "4\nY. Dong et al.": "learn spatial correlations across EEG channels and temporal dependencies within"
        },
        {
          "4\nY. Dong et al.": "signal\nsequences. Li et al. employed the spatio-temporal combination network"
        },
        {
          "4\nY. Dong et al.": "R2G-STNN, which contains local-global feature combing, to extract the intrinsic"
        },
        {
          "4\nY. Dong et al.": "information of EEG signals [37]. In order to mine the spatiotemporal information"
        },
        {
          "4\nY. Dong et al.": "related to emotional\njudgment, Gong et al. designed a stacked parallel\nspatial"
        },
        {
          "4\nY. Dong et al.": "and temporal attention streams to respectively extract the spatial\nfeatures and"
        },
        {
          "4\nY. Dong et al.": "temporal\nfeatures of the specially processed EEG signals [38]. Although previ-"
        },
        {
          "4\nY. Dong et al.": "ous\nresearchers have obtained satisfactory results, most of\nthem have ignored"
        },
        {
          "4\nY. Dong et al.": "the interaction and mutual compensation between spatial\ninformation and tem-"
        },
        {
          "4\nY. Dong et al.": "poral\ninformation in EEG signals.\nIn these network streamlines, the two kinds"
        },
        {
          "4\nY. Dong et al.": "of\ninformation are often ignored. Parallel offload processing, which is contrary"
        },
        {
          "4\nY. Dong et al.": "to the processing flow of neural signals in complex neural activities, may not be"
        },
        {
          "4\nY. Dong et al.": "enough to handle more complex neural signal processing tasks."
        },
        {
          "4\nY. Dong et al.": "3\nMethodology"
        },
        {
          "4\nY. Dong et al.": "3.1\nOverview"
        },
        {
          "4\nY. Dong et al.": "Mood Reader is an emotion recognition model that accommodates multi-modal,"
        },
        {
          "4\nY. Dong et al.": "cross-scale information and successfully integrates these features,\nits overall ar-"
        },
        {
          "4\nY. Dong et al.": "chitecture is illustrated in Figure 1. The model encompasses three distinct types"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùëãùëá\nspatial\nTemporal": "Temporal"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "Interlink"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "Block"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "ùêπùëáùëÜ\nùëãùëá\nBlock"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "Fig. 1: The overall architecture of our proposed model and the way related data"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "flows in it."
        },
        {
          "ùëãùëá\nspatial\nTemporal": "of\ninput. EEG monitoring data, subjected to simple preprocessing, are encoded"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "by an encoder pre-trained on a large-scale dataset,\nresulting in outputs\nthat"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "contain rich semantic\nrepresentations. An attention-based interlinked spatial-"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "temporal mechanism captures\nthe\nintrinsic\nspatio-temporal\ninformation from"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "DE features extracted from EEG data. Additionally, a set of\nsimilar\ntemporal"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "attention blocks analyzes corresponding eye movement features, aiming to com-"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "plement the shortcomings of EEG data. The acquired features are progressively"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "fused in a sequential manner, ensuring that the model genuinely learns the com-"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "prehensive complementarity between different modal and scale information, and"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "utilizes it for emotion recognition tasks."
        },
        {
          "ùëãùëá\nspatial\nTemporal": "3.2\nMBSM Based EEG General Representation Learning"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "Due to the inherent brain differences among subjects and external noise affecting"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "the signals collected by non-invasive EEG, we adopted a pre-training technique"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "known as masked brain signal modeling, which has been proven effective mul-"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "tiple times, to learn meaningful and contextually rich general knowledge repre-"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "sentations from cross-subject, noisy,\nlarge-scale EEG data [25,7,24]. Specifically,"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "we completed this task by training an autoencoder-decoder with an asymmet-"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "ric architecture\nsimilar\nto that\nin [25]on the EEG Motor Movement/Imagery"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "Dataset\n[39].\nIn this model,\nthe\ntemporal\nsignals of EEG data were divided"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "into tokens of a specific size, where a larger ratio of tokens would be randomly"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "masked, and the architecture-simple decoder had to reconstruct the EEG data"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "using the remaining unmasked tokens arrangement, combined with semantically"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "rich embeddings outputted by the\nencoder after processing the original EEG"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "data.The performance of masked reconstruction improves and reaches a peak"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "when the mask ratio hits 75% [25,7,24]. Consequently, by removing the decoder"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "from this\ntrained model, we obtained an encoder with excellent capability in"
        },
        {
          "ùëãùëá\nspatial\nTemporal": "extracting general EEG representations."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nY. Dong et al.": "3.3\nAttention Based Interlinked Spatial-Temporal Mechanism"
        },
        {
          "6\nY. Dong et al.": "Deep learning analyses of neural processes typically begin with various types of"
        },
        {
          "6\nY. Dong et al.": "feature extraction. For complex neural activities like EEG-based emotion recog-"
        },
        {
          "6\nY. Dong et al.": "nition, which exhibit high spatiotemporal continuity,\nit‚Äôs crucial\nto accurately"
        },
        {
          "6\nY. Dong et al.": "unearth intrinsic spatial and temporal features and their interrelations to obtain"
        },
        {
          "6\nY. Dong et al.": "more valuable information. To address this challenge, this section introduces the"
        },
        {
          "6\nY. Dong et al.": "interlinked spatial-temporal attention module for processing DE features, com-"
        },
        {
          "6\nY. Dong et al.": "prised of multiple parallel spatiotemporal blocks and interactive spatiotemporal"
        },
        {
          "6\nY. Dong et al.": "blocks (as depicted in Figure.), these blocks collectively facilitate the extraction"
        },
        {
          "6\nY. Dong et al.": "of spatio-temporal\ninformation and enable the communication and complemen-"
        },
        {
          "6\nY. Dong et al.": "tation between the extracted spatial and temporal\nfeatures."
        },
        {
          "6\nY. Dong et al.": "Spatial and Temporal Representation of DE features\nIn the domain of"
        },
        {
          "6\nY. Dong et al.": "EEG-based emotion recognition, the DE feature, which quantifies the variabil-"
        },
        {
          "6\nY. Dong et al.": "ity of EEG signals, has been proven to be the most effective feature, capturing"
        },
        {
          "6\nY. Dong et al.": "brain activities related to emotions. It itself has channel-related explicit spatial"
        },
        {
          "6\nY. Dong et al.": "information and implicit temporal\ninformation compressed within a single slid-"
        },
        {
          "6\nY. Dong et al.": "ing window.In order to make the spatiotemporal\ninformation in the DE feature"
        },
        {
          "6\nY. Dong et al.": "more balanced, we\nexpand the number of\nsliding windows\nin the DE feature"
        },
        {
          "6\nY. Dong et al.": "to the explicit\ntemporal dimension, obtaining ÀÜX ‚àà RN √óF √óC, where N denotes"
        },
        {
          "6\nY. Dong et al.": "the number of\nsliding windows\ninvolved in the DE feature\ncomputation. For"
        },
        {
          "6\nY. Dong et al.": "information\nfurther dimension transformation, we get ÀÜXS ‚àà RC√ó(N ¬∑F )for spatial"
        },
        {
          "6\nY. Dong et al.": "representation and\nfor temporal\ninformation representation.\nXT ‚àà RN √ó(C¬∑F )"
        },
        {
          "6\nY. Dong et al.": "Parallel Spatiotemporal Feature Extraction To capture the dynamically"
        },
        {
          "6\nY. Dong et al.": "varying key information, we\napply dedicated spatial\nand temporal\nattention"
        },
        {
          "6\nY. Dong et al.": "blocks to the differentiated DE feature spatial representation ÀÜXS and temporal"
        },
        {
          "6\nY. Dong et al.": "representation ÀÜXT , respectively. Specifically,\nfor the spatial representation ÀÜXS ‚àà"
        },
        {
          "6\nY. Dong et al.": "ÀÜ"
        },
        {
          "6\nY. Dong et al.": "RC√ó(N ¬∑F ), an initial\nlayer normalization is employed to to yield\nXS, which ef-"
        },
        {
          "6\nY. Dong et al.": "fectively mitigates the internal covariate shift within the spatial representation"
        },
        {
          "6\nY. Dong et al.": "data, thereby maintaining the stability of\nits distribution, as follows,"
        },
        {
          "6\nY. Dong et al.": "X ‚Ä≤\n(1)\nS = LayerNorm( ÀÜXS)"
        },
        {
          "6\nY. Dong et al.": "After\nthe normalization, multi-head attention (MHA)\ncomputation is\nimple-"
        },
        {
          "6\nY. Dong et al.": "is processed by three separate linear net-\nS. Within head i, X ‚Ä≤\nS"
        },
        {
          "6\nY. Dong et al.": "works,\ntransforming the input\ninto different\nrepresentational\nspaces\nto obtain"
        },
        {
          "6\nY. Dong et al.": "S, key K i\nS, and value V i\nS, denoted as,"
        },
        {
          "6\nY. Dong et al.": "Qi\nK i\nV i\n(2)\nS = X ‚Ä≤\nSW qi\nS = X ‚Ä≤\nSW ki\nS = X ‚Ä≤\nSW vi"
        },
        {
          "6\nY. Dong et al.": "S ,\nS ,"
        },
        {
          "6\nY. Dong et al.": "Where W qi"
        },
        {
          "6\nY. Dong et al.": "are the learnable network parameters respectively.\nS , W ki\nS , W vi"
        },
        {
          "6\nY. Dong et al.": "Based on the attention calculation method,\nthe attention output\nfor\neach"
        },
        {
          "6\nY. Dong et al.": "head i\nS = Attention(Qi\nS, K i\nS, V i\nS). With some processing, the"
        },
        {
          "6\nY. Dong et al.": "DE feature space representation‚Äôs MHA output AS\ncan be obtained as AS ="
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-modal Mood Reader\n7": ", where h is the number of attention heads, and\nS, A2\nS, . . . , Ah\nS)W MHA"
        },
        {
          "Multi-modal Mood Reader\n7": "W MHA\nis a linear mapping weight. The attention calculation is described as,"
        },
        {
          "Multi-modal Mood Reader\n7": "S"
        },
        {
          "Multi-modal Mood Reader\n7": "T\n \n!"
        },
        {
          "Multi-modal Mood Reader\n7": "SK i"
        },
        {
          "Multi-modal Mood Reader\n7": "S"
        },
        {
          "Multi-modal Mood Reader\n7": "‚àö\n(3)\nS, K i\nS, V i\nS) = softmax\nS"
        },
        {
          "Multi-modal Mood Reader\n7": "dk"
        },
        {
          "Multi-modal Mood Reader\n7": "where dk represents the dimension of the key."
        },
        {
          "Multi-modal Mood Reader\n7": "At the conclusion of the spatial attention block, a residual connection is intro-"
        },
        {
          "Multi-modal Mood Reader\n7": "duced by adding the dropout-processed MHA output AS to X ‚Ä≤\nS. This combined"
        },
        {
          "Multi-modal Mood Reader\n7": "result is then subjected to another layer normalization to produce the final spa-"
        },
        {
          "Multi-modal Mood Reader\n7": "tial representation of the DE feature XS , which is within the space RC√ó(N ¬∑F )."
        },
        {
          "Multi-modal Mood Reader\n7": "The calculation process of this part is as follows,"
        },
        {
          "Multi-modal Mood Reader\n7": "(4)\nXS = LayerNorm(Dropout(AS) + X ‚Ä≤\nS)"
        },
        {
          "Multi-modal Mood Reader\n7": "Similarly, we process\nXT ‚àà RN √ó(C¬∑F ) with a structurally analogous temporal"
        },
        {
          "Multi-modal Mood Reader\n7": "attention block to obtain the temporal representation of the DE feature XT ‚àà"
        },
        {
          "Multi-modal Mood Reader\n7": "RN √ó(C¬∑F )"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nY. Dong et al.": "As for XT , it undergoes multiple transformations in an attempt to align with the"
        },
        {
          "8\nY. Dong et al.": "spatial feature dimensions, resulting in XT ‚Ä≤. Subsequently, we concatenate these"
        },
        {
          "8\nY. Dong et al.": "two feature parts and perform MHA calculation to obtain the spatial\nfeatures"
        },
        {
          "8\nY. Dong et al.": "that are interlinked with the temporal dimension, as\nshown in Figure 2.\nFST"
        },
        {
          "8\nY. Dong et al.": "Similarly, we can also obtain the temporal features FTS that are interlinked with"
        },
        {
          "8\nY. Dong et al.": "the spatial dimension."
        },
        {
          "8\nY. Dong et al.": "3.4\nMulti-Level Score Filtering for Feature Fusion"
        },
        {
          "8\nY. Dong et al.": "Previous\nsections presented outputs\nfrom various modules,\neach designed for"
        },
        {
          "8\nY. Dong et al.": "emotion recognition through different pathways, yielding high-dimensional\nfea-"
        },
        {
          "8\nY. Dong et al.": "tures. To address potential redundancy and the needs of multimodal cross-scale"
        },
        {
          "8\nY. Dong et al.": "fusion, we introduce a multi-level\nfusion layer based on attention mechanisms."
        },
        {
          "8\nY. Dong et al.": "This module synergizes and refines features by highlighting relevant information"
        },
        {
          "8\nY. Dong et al.": "and filtering out\nredundancy,\nthus\nimproving multimodal emotion recognition"
        },
        {
          "8\nY. Dong et al.": "efficacy."
        },
        {
          "8\nY. Dong et al.": "Prior\nto the\ncommencement of\nthe\nfusion process,\nthe MBSM latent\nrep-"
        },
        {
          "8\nY. Dong et al.": "resentation, spatial-temporal representation, and eye movement representation,"
        },
        {
          "8\nY. Dong et al.": "are projected onto a unified dimensional space through a series of transforma-"
        },
        {
          "8\nY. Dong et al.": "tions,\nstarting with layer normalization,\nfollowed by flattening of\nthe\nfeature"
        },
        {
          "8\nY. Dong et al.": "vectors, and culminating in a linear transformation. As a result of these opera-"
        },
        {
          "8\nY. Dong et al.": "tions, a unified feature representation F ‚àà RDunified\nis obtained, where Dunified"
        },
        {
          "8\nY. Dong et al.": "denotes the dimensionality of the unified feature space."
        },
        {
          "8\nY. Dong et al.": "In the first\nlevel of\nfusion,\nspecifically for\nthe\ninterlinked spatial-temporal"
        },
        {
          "8\nY. Dong et al.": "executed utilizing a linear\nlayer and\nfeatures FST\nand FTS , preprocessing is"
        },
        {
          "8\nY. Dong et al.": "layer normalization, which preserves their dimensional attributes. Subsequently,"
        },
        {
          "8\nY. Dong et al.": "inspired by [18,40,41], a simplified cross-attention mechanism is\nemployed to"
        },
        {
          "8\nY. Dong et al.": "delineate the intrinsic spatial-temporal\nrelationships between the two,\nthereby"
        },
        {
          "8\nY. Dong et al.": "augmenting the interactivity of the internal\ninformation. Specifically, the atten-"
        },
        {
          "8\nY. Dong et al.": "the spatial\nfeatures\ntaking into\ntion scores Scores and Scoret, which represent"
        },
        {
          "8\nY. Dong et al.": "account\ntemporal\nfeatures and time\nfeatures\ntaking into account\nspatial\nfea-"
        },
        {
          "8\nY. Dong et al.": "tures respectively, are transformed using the softmax function to ascertain the"
        },
        {
          "8\nY. Dong et al.": "corresponding fusion weights cs and ct,"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-modal Mood Reader\n9": "where ceeg and ceye represent the fusion weights derived from the respective at-"
        },
        {
          "Multi-modal Mood Reader\n9": "tention scores.\nIn the terminal\nfusion layer, an integration is\nrequisite for\nthe"
        },
        {
          "Multi-modal Mood Reader\n9": "synthesized features FST and FEE, which respectively represent the interlinked"
        },
        {
          "Multi-modal Mood Reader\n9": "spatio-temporal\ninformation emanating from the DE feature and the compre-"
        },
        {
          "Multi-modal Mood Reader\n9": "hensive information spanning multiple modalities and scales.The features\nthat"
        },
        {
          "Multi-modal Mood Reader\n9": "have undergone\nlayer normalization are\nconcatenated and subsequently pro-"
        },
        {
          "Multi-modal Mood Reader\n9": "cessed through self-attention computation, yielding the final\nintegrated feature"
        },
        {
          "Multi-modal Mood Reader\n9": "representation M ,"
        },
        {
          "Multi-modal Mood Reader\n9": "(8)\nM = Attention (Concat (FST , FEE))"
        },
        {
          "Multi-modal Mood Reader\n9": "For the feature M , an initial batch normalization is employed,\nfollowed by the"
        },
        {
          "Multi-modal Mood Reader\n9": "deployment of a classifier comprised of three linear layers and a softmax function,"
        },
        {
          "Multi-modal Mood Reader\n9": "which outputs predictions for the emotional\nlabels y. The discrepancy between"
        },
        {
          "Multi-modal Mood Reader\n9": "the predicted emotional labels and the true emotional labels ÀÜy is quantified using"
        },
        {
          "Multi-modal Mood Reader\n9": "the cross-entropy loss function,"
        },
        {
          "Multi-modal Mood Reader\n9": "1 N\nNX i\nCX c\n(9)\nL = ‚àí\nyic log(yic)"
        },
        {
          "Multi-modal Mood Reader\n9": "=1\n=1"
        },
        {
          "Multi-modal Mood Reader\n9": "wherein N represents the batch size, and C designates the count of\nlabel cate-"
        },
        {
          "Multi-modal Mood Reader\n9": "gories."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: Subject-independent classification performance (Acc/Std%) on SEED",
      "data": [
        {
          "10\nY. Dong et al.": "4\nExperiment Result and Analysis"
        },
        {
          "10\nY. Dong et al.": "4.1\nDatasets and Pre-processing"
        },
        {
          "10\nY. Dong et al.": "Extensive experiments have been conducted on two public datasets, SEED and"
        },
        {
          "10\nY. Dong et al.": "SEED-V. The SEED dataset includes EEG monitoring data and corresponding"
        },
        {
          "10\nY. Dong et al.": "emotional\nlabels (sad, happy, and neutral) from 15 subjects. Each subject com-"
        },
        {
          "10\nY. Dong et al.": "pleted 3 sessions, with each session comprising 15 trials, resulting in a total of 45"
        },
        {
          "10\nY. Dong et al.": "trials. Similarly, SEED-V was completed by 16 subjects, each participating in 45"
        },
        {
          "10\nY. Dong et al.": "trials, and includes EEG data along with corresponding labels for five emotions"
        },
        {
          "10\nY. Dong et al.": "(disgust,\nfear, sad, happy, and neutral)."
        },
        {
          "10\nY. Dong et al.": "In the preprocessing of raw EEG data, a sequence involving a bandpass filter"
        },
        {
          "10\nY. Dong et al.": "with cutoff frequencies of 0.1Hz and 70Hz, followed by a notch filter at 50Hz, was"
        },
        {
          "10\nY. Dong et al.": "implemented. Subsequently, the sampling frequency was reduced to 200Hz from"
        },
        {
          "10\nY. Dong et al.": "its original rate. For EEG segments corresponding to trials of varied lengths, a 4s"
        },
        {
          "10\nY. Dong et al.": "non-overlapping Hanning window was utilized for segmentation in reverse order."
        },
        {
          "10\nY. Dong et al.": "The Short Time Fourier Transform (STFT) was\nthen applied to calculate the"
        },
        {
          "10\nY. Dong et al.": "DE feature across five frequency domains. Every four sliding window calculations"
        },
        {
          "10\nY. Dong et al.": "were grouped together to extract EEG signals requisite for the pretrained model."
        },
        {
          "10\nY. Dong et al.": "4.2\nBaseline Model and Settings"
        },
        {
          "10\nY. Dong et al.": "We\nconducted subject-independent\nexperiments\non the SEED and SEED-V"
        },
        {
          "10\nY. Dong et al.": "datasets using baseline models including DGCNN [42], RGNN [4], SOGNN [43]"
        },
        {
          "10\nY. Dong et al.": "and BFE-Net\n[44]. Notably, baseline models\nthat were\ntrained using a single"
        },
        {
          "10\nY. Dong et al.": "modality were explicitly annotated. The experimental\nframework utilized was"
        },
        {
          "10\nY. Dong et al.": "PyTorch, with the GPU being NVIDIA A800 80GB PCIe. Furthermore, for each"
        },
        {
          "10\nY. Dong et al.": "experiment, the data were randomly divided into training and testing sets in an"
        },
        {
          "10\nY. Dong et al.": "8:2 ratio. The model performance was evaluated based on the average accuracy"
        },
        {
          "10\nY. Dong et al.": "and variance on the testing set."
        },
        {
          "10\nY. Dong et al.": "Table 1: Subject-independent classification performance (Acc/Std%) on SEED"
        },
        {
          "10\nY. Dong et al.": "and SEED-V, where SWC represents whether to combine DE features in sliding"
        },
        {
          "10\nY. Dong et al.": "window order."
        },
        {
          "10\nY. Dong et al.": "Modality\nSEED\nSEED-V"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 1: Subject-independent classification performance (Acc/Std%) on SEED",
      "data": [
        {
          "window order.": ""
        },
        {
          "window order.": "Method"
        },
        {
          "window order.": ""
        },
        {
          "window order.": ""
        },
        {
          "window order.": "SVM"
        },
        {
          "window order.": "DGCNN"
        },
        {
          "window order.": "RGNN"
        },
        {
          "window order.": "SOGNN"
        },
        {
          "window order.": "BFE-Net"
        },
        {
          "window order.": "SVM"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 1: presents the experimental results of the baseline",
      "data": [
        {
          "Multi-modal Mood Reader\n11": "4.3\nResults"
        },
        {
          "Multi-modal Mood Reader\n11": "Experiment Result Table 1 presents the experimental results of the baseline"
        },
        {
          "Multi-modal Mood Reader\n11": "model and our method on the SEED and SEED-V datasets, with annotations re-"
        },
        {
          "Multi-modal Mood Reader\n11": "garding the categories of data utilized. The results demonstrate the consistently"
        },
        {
          "Multi-modal Mood Reader\n11": "superior\nclassification performance of Mood Reader across different datasets."
        },
        {
          "Multi-modal Mood Reader\n11": "Furthermore,\nthey validate\nthe\nefficacy of\nthe\nsequentially combined DE fea-"
        },
        {
          "Multi-modal Mood Reader\n11": "tures through a sliding window approach in the task of emotion recognition, as"
        },
        {
          "Multi-modal Mood Reader\n11": "well as the correctness of the multimodal cross-scale information fusion strategy."
        },
        {
          "Multi-modal Mood Reader\n11": "Interpretation The results of attention visualization are summarized in Figure"
        },
        {
          "Multi-modal Mood Reader\n11": "3. It can be observed that as the emotional recognition capability improves, the"
        },
        {
          "Multi-modal Mood Reader\n11": "network‚Äôs\nattention on EEG signals gradually shifts\nfrom a scattered global"
        },
        {
          "Multi-modal Mood Reader\n11": "distribution to concentrated attention on specific regions. These brain regions"
        },
        {
          "Multi-modal Mood Reader\n11": "include the frontal\nlobe area, areas of the left and right temporal\nlobes, and a"
        },
        {
          "Multi-modal Mood Reader\n11": "small portion of the parietal\nlobe, which has been proven to be closely related"
        },
        {
          "Multi-modal Mood Reader\n11": "to the generation and processing of emotions [45]."
        },
        {
          "Multi-modal Mood Reader\n11": "Additionally, we noticed that there are also small areas within the occipital"
        },
        {
          "Multi-modal Mood Reader\n11": "lobe, primarily responsible for visual\ninformation processing, that exhibit signif-"
        },
        {
          "Multi-modal Mood Reader\n11": "icant attention. Given that a substantial part of the stimuli\nin the experimental"
        },
        {
          "Multi-modal Mood Reader\n11": "datasets SEED and SEED-V comes from visual stimuli\nin videos, we have rea-"
        },
        {
          "Multi-modal Mood Reader\n11": "sonable grounds to propose the hypothesis that ‚Äúvisually encoded information"
        },
        {
          "Multi-modal Mood Reader\n11": "in the human brain is directly involved in emotion generation‚Äù to a certain ex-"
        },
        {
          "Multi-modal Mood Reader\n11": "tent. This hypothesis also represents the holistic view that various parts of the"
        },
        {
          "Multi-modal Mood Reader\n11": "brain participate in different\nfunctions and collectively process complex infor-"
        },
        {
          "Multi-modal Mood Reader\n11": "mation [46]."
        },
        {
          "Multi-modal Mood Reader\n11": "4.4\nAblation Studies"
        },
        {
          "Multi-modal Mood Reader\n11": "To substantiate the effectiveness of the employed modules, we conducted ablation"
        },
        {
          "Multi-modal Mood Reader\n11": "experiments on the SEED-V dataset using a stepwise stacking approach for the"
        },
        {
          "Multi-modal Mood Reader\n11": "modules, with the specific experimental details as\nfollows, and the results are"
        },
        {
          "Multi-modal Mood Reader\n11": "depicted in Figure 4."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 1: presents the experimental results of the baseline",
      "data": [
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "modules, with the specific experimental details as\nfollows, and the results are"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "depicted in Figure 4."
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "1. STB+CF: SWC DE with spatial-temporal block + concatenation fusion"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "2. STIB+CF: SWC DE with spatial-temporal\ninterlinked block + concatena-"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "tion fusion"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "3. STIB+Encoder+CF: SWC DE with spatial-temporal\ninterlinked block +"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "pre-trained encoder + concatenation fusion"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "4. STIB+Encoder+MLF: SWC DE with spatial-temporal\ninterlinked block +"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "pre-trained encoder + multi-level\nfusion"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "5. STIB+Eye+CF: SWC DE with spatial-temporal\ninterlinked block + eye"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "movement + concatenation fusion"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "6. STIB+Eye+MLF: SWC DE with spatial-temporal\ninterlinked block + eye"
        },
        {
          "experiments on the SEED-V dataset using a stepwise stacking approach for the": "movement + multi-level\nfusion"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "STB+CF": "STIB+CF"
        },
        {
          "STB+CF": "STIB+Encoder+CF"
        },
        {
          "STB+CF": "STIB+Encoder+MLF"
        },
        {
          "STB+CF": "STIB+Eye +CF"
        },
        {
          "STB+CF": "STIB+Eye +MLF"
        },
        {
          "STB+CF": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "vignetting removal via a dual aggregated fusion transformer with adaptive channel"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "expansion.\nIn: AAAI Conference on Artificial\nIntelligence. pp. 4000‚Äì4008 (March"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "2024)"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "10. Li, Z., Chen, X., Pun, C.M., Cun, X.: High-resolution document shadow removal"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "via a large-scale real-world dataset and a frequency-aware shadow erasing net. In:"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "International Conference on Computer Vision (ICCV). pp. 12449‚Äì12458 (October"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "2023)"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "11. Li, Z., Chen, X., Wang, S., Pun, C.M.: A large-scale film style dataset for learning"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "multi-frequency driven film enhancement.\nIn:\nInternational Joint Conference on"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "Artificial Intelligence (IJCAI). pp. 1160‚Äì1168 (August 2023)"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "12. Du, X., Ma, C., Zhang, G., Li, J., Lai, Y.K., Zhao, G., Deng, X., Liu, Y.J., Wang,"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "H.: An efficient lstm network for emotion recognition from multichannel eeg signals."
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "IEEE Transactions on Affective Computing 13(3), 1528‚Äì1540 (2020)"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "13. Tao, W., Li, C., Song, R., Cheng, J., Liu, Y., Wan, F., Chen, X.: Eeg-based emotion"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "recognition via channel-wise attention and self attention.\nIEEE Transactions on"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "Affective Computing 14(1), 382‚Äì393 (2020)"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "14. Shen, X., Liu, X., Hu, X., Zhang, D., Song, S.: Contrastive learning of\nsubject-"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "invariant eeg representations for cross-subject emotion recognition. IEEE Transac-"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "tions on Affective Computing (2022)"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "15. Liu, W., Qiu, J.L., Zheng, W.L., Lu, B.L.: Comparing recognition performance and"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "robustness of multimodal deep learning models\nfor multimodal emotion recogni-"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "tion. IEEE Transactions on Cognitive and Developmental Systems 14(2), 715‚Äì729"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "(2021)"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "16. Li, C., Bao, Z., Li, L., Zhao, Z.: Exploring temporal\nrepresentations by leverag-"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "ing attention-based bidirectional\nlstm-rnns\nfor multi-modal emotion recognition."
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "Information Processing & Management 57(3), 102185 (2020)"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "17. LeDoux, J.E.: Cognitive-emotional\ninteractions in the brain. Cognition & Emotion"
        },
        {
          "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution": "3(4), 267‚Äì289 (1989)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-modal Mood Reader\n13": "2. Yan, H., Zhang, H., Shi, J., Ma, J., Xu, X.:\nInspiration transfer\nfor\nintelligent"
        },
        {
          "Multi-modal Mood Reader\n13": "design: A generative adversarial network with fashion attributes disentanglement."
        },
        {
          "Multi-modal Mood Reader\n13": "IEEE Transactions on Consumer Electronics (2023)"
        },
        {
          "Multi-modal Mood Reader\n13": "3. Li, Y., Chen, J., Li, F., Fu, B., Wu, H., Ji, Y., Zhou, Y., Niu, Y., Shi, G., Zheng, W.:"
        },
        {
          "Multi-modal Mood Reader\n13": "Gmss: Graph-based multi-task self-supervised learning for eeg emotion recognition."
        },
        {
          "Multi-modal Mood Reader\n13": "IEEE Transactions on Affective Computing (2022)"
        },
        {
          "Multi-modal Mood Reader\n13": "4. Zhong, P., Wang, D., Miao, C.: Eeg-based emotion recognition using regularized"
        },
        {
          "Multi-modal Mood Reader\n13": "graph neural networks.\nIEEE Transactions on Affective Computing 13(3), 1290‚Äì"
        },
        {
          "Multi-modal Mood Reader\n13": "1301 (2020)"
        },
        {
          "Multi-modal Mood Reader\n13": "5. Li, J., Hua, H., Xu, Z., Shu, L., Xu, X., Kuang, F., Wu, S.: Cross-subject eeg emo-"
        },
        {
          "Multi-modal Mood Reader\n13": "tion recognition combined with connectivity features and meta-transfer learning."
        },
        {
          "Multi-modal Mood Reader\n13": "Computers in biology and medicine 145, 105519 (2022)"
        },
        {
          "Multi-modal Mood Reader\n13": "6. Wang, S., Shen, Y., Zeng, D., Hu, Y.: Bone age assessment using convolutional"
        },
        {
          "Multi-modal Mood Reader\n13": "neural networks. In: 2018 International conference on artificial\nintelligence and big"
        },
        {
          "Multi-modal Mood Reader\n13": "data (ICAIBD). pp. 175‚Äì178. IEEE (2018)"
        },
        {
          "Multi-modal Mood Reader\n13": "7. Chen, Z., Qing, J., Xiang, T., Yue, W.L., Zhou, J.H.: Seeing beyond the brain:"
        },
        {
          "Multi-modal Mood Reader\n13": "Conditional diffusion model with sparse masked modeling for vision decoding. In:"
        },
        {
          "Multi-modal Mood Reader\n13": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-"
        },
        {
          "Multi-modal Mood Reader\n13": "nition. pp. 22710‚Äì22720 (2023)"
        },
        {
          "Multi-modal Mood Reader\n13": "8. Ortega Caro, J., Oliveira Fonseca, A.H., Averill, C., Rizvi, S.A., Rosati, M., Cross,"
        },
        {
          "Multi-modal Mood Reader\n13": "J.L., Mittal, P., Zappala, E., Levine, D., Dhodapkar, R.M.,\net al.: Brainlm: A"
        },
        {
          "Multi-modal Mood Reader\n13": "foundation model\nfor brain activity recordings. bioRxiv pp. 2023‚Äì09 (2023)"
        },
        {
          "Multi-modal Mood Reader\n13": "9. Luo, S., Chen, X., Chen, W., Li, Z., Wang, S., Pun, C.M.: Devignet: High-resolution"
        },
        {
          "Multi-modal Mood Reader\n13": "vignetting removal via a dual aggregated fusion transformer with adaptive channel"
        },
        {
          "Multi-modal Mood Reader\n13": "expansion.\nIn: AAAI Conference on Artificial\nIntelligence. pp. 4000‚Äì4008 (March"
        },
        {
          "Multi-modal Mood Reader\n13": "2024)"
        },
        {
          "Multi-modal Mood Reader\n13": "10. Li, Z., Chen, X., Pun, C.M., Cun, X.: High-resolution document shadow removal"
        },
        {
          "Multi-modal Mood Reader\n13": "via a large-scale real-world dataset and a frequency-aware shadow erasing net. In:"
        },
        {
          "Multi-modal Mood Reader\n13": "International Conference on Computer Vision (ICCV). pp. 12449‚Äì12458 (October"
        },
        {
          "Multi-modal Mood Reader\n13": "2023)"
        },
        {
          "Multi-modal Mood Reader\n13": "11. Li, Z., Chen, X., Wang, S., Pun, C.M.: A large-scale film style dataset for learning"
        },
        {
          "Multi-modal Mood Reader\n13": "multi-frequency driven film enhancement.\nIn:\nInternational Joint Conference on"
        },
        {
          "Multi-modal Mood Reader\n13": "Artificial Intelligence (IJCAI). pp. 1160‚Äì1168 (August 2023)"
        },
        {
          "Multi-modal Mood Reader\n13": "12. Du, X., Ma, C., Zhang, G., Li, J., Lai, Y.K., Zhao, G., Deng, X., Liu, Y.J., Wang,"
        },
        {
          "Multi-modal Mood Reader\n13": "H.: An efficient lstm network for emotion recognition from multichannel eeg signals."
        },
        {
          "Multi-modal Mood Reader\n13": "IEEE Transactions on Affective Computing 13(3), 1528‚Äì1540 (2020)"
        },
        {
          "Multi-modal Mood Reader\n13": "13. Tao, W., Li, C., Song, R., Cheng, J., Liu, Y., Wan, F., Chen, X.: Eeg-based emotion"
        },
        {
          "Multi-modal Mood Reader\n13": "recognition via channel-wise attention and self attention.\nIEEE Transactions on"
        },
        {
          "Multi-modal Mood Reader\n13": "Affective Computing 14(1), 382‚Äì393 (2020)"
        },
        {
          "Multi-modal Mood Reader\n13": "14. Shen, X., Liu, X., Hu, X., Zhang, D., Song, S.: Contrastive learning of\nsubject-"
        },
        {
          "Multi-modal Mood Reader\n13": "invariant eeg representations for cross-subject emotion recognition. IEEE Transac-"
        },
        {
          "Multi-modal Mood Reader\n13": "tions on Affective Computing (2022)"
        },
        {
          "Multi-modal Mood Reader\n13": "15. Liu, W., Qiu, J.L., Zheng, W.L., Lu, B.L.: Comparing recognition performance and"
        },
        {
          "Multi-modal Mood Reader\n13": "robustness of multimodal deep learning models\nfor multimodal emotion recogni-"
        },
        {
          "Multi-modal Mood Reader\n13": "tion. IEEE Transactions on Cognitive and Developmental Systems 14(2), 715‚Äì729"
        },
        {
          "Multi-modal Mood Reader\n13": "(2021)"
        },
        {
          "Multi-modal Mood Reader\n13": "16. Li, C., Bao, Z., Li, L., Zhao, Z.: Exploring temporal\nrepresentations by leverag-"
        },
        {
          "Multi-modal Mood Reader\n13": "ing attention-based bidirectional\nlstm-rnns\nfor multi-modal emotion recognition."
        },
        {
          "Multi-modal Mood Reader\n13": "Information Processing & Management 57(3), 102185 (2020)"
        },
        {
          "Multi-modal Mood Reader\n13": "17. LeDoux, J.E.: Cognitive-emotional\ninteractions in the brain. Cognition & Emotion"
        },
        {
          "Multi-modal Mood Reader\n13": "3(4), 267‚Äì289 (1989)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14\nY. Dong et al.": "18.\nJiang, W.B., Liu, X.H., Zheng, W.L., Lu, B.L.: Multimodal adaptive emotion trans-"
        },
        {
          "14\nY. Dong et al.": "former with flexible modality inputs on a novel dataset with continuous labels. In:"
        },
        {
          "14\nY. Dong et al.": "Proceedings of the 31st ACM International Conference on Multimedia. pp. 5975‚Äì"
        },
        {
          "14\nY. Dong et al.": "5984 (2023)"
        },
        {
          "14\nY. Dong et al.": "19. Vazquez-Rodriguez, J., Lefebvre, G., Cumin, J., Crowley, J.L.: Emotion recognition"
        },
        {
          "14\nY. Dong et al.": "with pre-trained transformers using multimodal signals. In: 2022 10th International"
        },
        {
          "14\nY. Dong et al.": "Conference on Affective Computing and Intelligent\nInteraction (ACII). pp. 1‚Äì8."
        },
        {
          "14\nY. Dong et al.": "IEEE (2022)"
        },
        {
          "14\nY. Dong et al.": "20.\nJia, Z., Lin, Y., Wang, J., Feng, Z., Xie, X., Chen, C.: Hetemotionnet: two-stream"
        },
        {
          "14\nY. Dong et al.": "heterogeneous graph recurrent neural network for multi-modal emotion recogni-"
        },
        {
          "14\nY. Dong et al.": "tion.\nIn: Proceedings of\nthe 29th ACM International Conference on Multimedia."
        },
        {
          "14\nY. Dong et al.": "pp. 1047‚Äì1056 (2021)"
        },
        {
          "14\nY. Dong et al.": "21. Ma, J., Tang, H., Zheng, W.L., Lu, B.L.: Emotion recognition using multimodal"
        },
        {
          "14\nY. Dong et al.": "residual\nlstm network.\nIn: Proceedings of the 27th ACM international conference"
        },
        {
          "14\nY. Dong et al.": "on multimedia. pp. 176‚Äì183 (2019)"
        },
        {
          "14\nY. Dong et al.": "22. Chaparro, V., Gomez, A., Salgado, A., Quintero, O.L., Lopez, N., Villa, L.F.:"
        },
        {
          "14\nY. Dong et al.": "Emotion recognition from eeg and facial expressions: a multimodal approach.\nIn:"
        },
        {
          "14\nY. Dong et al.": "2018 40th Annual International Conference of the IEEE Engineering in Medicine"
        },
        {
          "14\nY. Dong et al.": "and Biology Society (EMBC). pp. 530‚Äì533. IEEE (2018)"
        },
        {
          "14\nY. Dong et al.": "23. Zheng, W.L., Liu, W., Lu, Y., Lu, B.L., Cichocki, A.: Emotionmeter: A multi-"
        },
        {
          "14\nY. Dong et al.": "modal\nframework for\nrecognizing human emotions.\nIEEE transactions on cyber-"
        },
        {
          "14\nY. Dong et al.": "netics 49(3), 1110‚Äì1122 (2018)"
        },
        {
          "14\nY. Dong et al.": "24. Chen, Z., Qing, J., Zhou, J.H.: Cinematic mindscapes: High-quality video recon-"
        },
        {
          "14\nY. Dong et al.": "struction from brain activity. Advances in Neural Information Processing Systems"
        },
        {
          "14\nY. Dong et al.": "36 (2024)"
        },
        {
          "14\nY. Dong et al.": "25. Bai, Y., Wang, X., Cao, Y.p., Ge, Y., Yuan, C., Shan, Y.: Dreamdiffusion: Gener-"
        },
        {
          "14\nY. Dong et al.": "ating high-quality images from brain eeg signals. arXiv preprint arXiv:2306.16934"
        },
        {
          "14\nY. Dong et al.": "(2023)"
        },
        {
          "14\nY. Dong et al.": "26. Yang, E., Milisav, F., Kopal, J., Holmes, A.J., Mitsis, G.D., Misic, B., Finn, E.S.,"
        },
        {
          "14\nY. Dong et al.": "Bzdok, D.: The default network dominates neural\nresponses\nto evolving movie"
        },
        {
          "14\nY. Dong et al.": "4197 (2023)\nstories. Nature communications 14(1),"
        },
        {
          "14\nY. Dong et al.": "27. Rollo, J., Crawford, J., Hardy, J.: A dynamical\nsystems approach for multiscale"
        },
        {
          "14\nY. Dong et al.": "synthesis of alzheimer‚Äôs pathogenesis. Neuron 111(14), 2126‚Äì2139 (2023)"
        },
        {
          "14\nY. Dong et al.": "28. You, S., Lei, B., Wang, S., Chui, C.K., Cheung, A.C., Liu, Y., Gan, M., Wu,"
        },
        {
          "14\nY. Dong et al.": "G., Shen, Y.: Fine perceptive gans for brain mr image super-resolution in wavelet"
        },
        {
          "14\nY. Dong et al.": "domain. IEEE transactions on neural networks and learning systems (2022)"
        },
        {
          "14\nY. Dong et al.": "29. Gong, C., Jing, C., Chen, X., Pun, C.M., Huang, G., Saha, A., Nieuwoudt, M.,"
        },
        {
          "14\nY. Dong et al.": "Li, H.X., Hu, Y., Wang, S.: Generative ai\nfor brain image computing and brain"
        },
        {
          "14\nY. Dong et al.": "network computing: a review. Frontiers in Neuroscience 17, 1203104 (2023)"
        },
        {
          "14\nY. Dong et al.": "30. Wang, S., Wang, H., Cheung, A.C., Shen, Y., Gan, M.: Ensemble of 3d densely"
        },
        {
          "14\nY. Dong et al.": "connected convolutional network for diagnosis of mild cognitive impairment and"
        },
        {
          "14\nY. Dong et al.": "alzheimer‚Äôs disease. Deep learning applications pp. 53‚Äì73 (2020)"
        },
        {
          "14\nY. Dong et al.": "31. Hu, B., Zhan, C., Tang, B., Wang, B., Lei, B., Wang, S.Q.: 3-d brain reconstruction"
        },
        {
          "14\nY. Dong et al.": "by hierarchical\nshape-perception network from a single incomplete image.\nIEEE"
        },
        {
          "14\nY. Dong et al.": "Transactions on Neural Networks and Learning Systems (2023)"
        },
        {
          "14\nY. Dong et al.": "32. Pan, J., Lei, B., Shen, Y., Liu, Y., Feng, Z., Wang, S.: Characterization multimodal"
        },
        {
          "14\nY. Dong et al.": "connectivity of brain network by hypergraph gan for alzheimer‚Äôs disease analysis."
        },
        {
          "14\nY. Dong et al.": "In: Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV"
        },
        {
          "14\nY. Dong et al.": "2021, Beijing, China, October 29‚ÄìNovember 1, 2021, Proceedings, Part III 4. pp."
        },
        {
          "14\nY. Dong et al.": "467‚Äì478. Springer (2021)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-modal Mood Reader\n15": "33. Wang, S.Q.: A variational approach to nonlinear two-point boundary value prob-"
        },
        {
          "Multi-modal Mood Reader\n15": "lems. Computers & Mathematics with Applications 58(11-12), 2452‚Äì2455 (2009)"
        },
        {
          "Multi-modal Mood Reader\n15": "34. Cherian, A., Wang, J., Hori, C., Marks, T.: Spatio-temporal ranked-attention net-"
        },
        {
          "Multi-modal Mood Reader\n15": "works\nfor video captioning.\nIn: Proceedings of\nthe IEEE/CVF winter conference"
        },
        {
          "Multi-modal Mood Reader\n15": "on applications of computer vision. pp. 1617‚Äì1626 (2020)"
        },
        {
          "Multi-modal Mood Reader\n15": "35. Ahn, D., Kim,\nS., Hong, H., Ko, B.C.:\nStar-transformer: A spatio-temporal"
        },
        {
          "Multi-modal Mood Reader\n15": "cross attention transformer\nfor human action recognition.\nIn: Proceedings of\nthe"
        },
        {
          "Multi-modal Mood Reader\n15": "IEEE/CVF winter conference on applications of computer vision. pp. 3330‚Äì3339"
        },
        {
          "Multi-modal Mood Reader\n15": "(2023)"
        },
        {
          "Multi-modal Mood Reader\n15": "36. Zhou, Q., Li, X., He, L., Yang, Y., Cheng, G., Tong, Y., Ma, L., Tao, D.: Transvod:"
        },
        {
          "Multi-modal Mood Reader\n15": "end-to-end video object detection with spatial-temporal transformers. IEEE Trans-"
        },
        {
          "Multi-modal Mood Reader\n15": "actions on Pattern Analysis and Machine Intelligence (2022)"
        },
        {
          "Multi-modal Mood Reader\n15": "37. Li, Y., Zheng, W., Wang, L., Zong, Y., Cui, Z.: From regional to global brain: A"
        },
        {
          "Multi-modal Mood Reader\n15": "novel hierarchical spatial-temporal neural network model\nfor eeg emotion recogni-"
        },
        {
          "Multi-modal Mood Reader\n15": "tion. IEEE Transactions on Affective Computing 13(2), 568‚Äì578 (2019)"
        },
        {
          "Multi-modal Mood Reader\n15": "38. Gong, P.,\nJia, Z., Wang, P., Zhou, Y., Zhang, D.: Astdf-net: Attention-based"
        },
        {
          "Multi-modal Mood Reader\n15": "spatial-temporal dual-stream fusion network for\neeg-based emotion recognition."
        },
        {
          "Multi-modal Mood Reader\n15": "In: Proceedings of\nthe 31st ACM International Conference on Multimedia. pp."
        },
        {
          "Multi-modal Mood Reader\n15": "883‚Äì892 (2023)"
        },
        {
          "Multi-modal Mood Reader\n15": "39. Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw,\nJ.R.:"
        },
        {
          "Multi-modal Mood Reader\n15": "Bci2000: a general-purpose brain-computer\ninterface\n(bci)\nsystem.\nIEEE Trans-"
        },
        {
          "Multi-modal Mood Reader\n15": "actions on biomedical engineering 51(6), 1034‚Äì1043 (2004)"
        },
        {
          "Multi-modal Mood Reader\n15": "40. Zuo, Q., Wu, H., Chen, C.P., Lei, B., Wang, S.: Prior-guided adversarial\nlearning"
        },
        {
          "Multi-modal Mood Reader\n15": "with hypergraph for predicting abnormal connections in alzheimer‚Äôs disease. IEEE"
        },
        {
          "Multi-modal Mood Reader\n15": "Transactions on Cybernetics (2024)"
        },
        {
          "Multi-modal Mood Reader\n15": "41. Zuo, Q., Lei, B., Shen, Y., Liu, Y., Feng, Z., Wang, S.: Multimodal representations"
        },
        {
          "Multi-modal Mood Reader\n15": "learning and adversarial hypergraph fusion for early alzheimer‚Äôs disease prediction."
        },
        {
          "Multi-modal Mood Reader\n15": "In: Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV"
        },
        {
          "Multi-modal Mood Reader\n15": "2021, Beijing, China, October 29‚ÄìNovember 1, 2021, Proceedings, Part III 4. pp."
        },
        {
          "Multi-modal Mood Reader\n15": "479‚Äì490. Springer (2021)"
        },
        {
          "Multi-modal Mood Reader\n15": "42. Song, T., Zheng, W., Song, P., Cui, Z.: Eeg emotion recognition using dynamical"
        },
        {
          "Multi-modal Mood Reader\n15": "graph convolutional neural networks. IEEE Transactions on Affective Computing"
        },
        {
          "Multi-modal Mood Reader\n15": "11(3), 532‚Äì541 (2018)"
        },
        {
          "Multi-modal Mood Reader\n15": "43. Li, J., Li, S., Pan, J., Wang, F.: Cross-subject eeg emotion recognition with self-"
        },
        {
          "Multi-modal Mood Reader\n15": "organized graph neural network. Frontiers in Neuroscience 15, 611653 (2021)"
        },
        {
          "Multi-modal Mood Reader\n15": "44. Zhang, J., Hao, Y., Wen, X., Zhang, C., Deng, H., Zhao, J., Cao, R.: Subject-"
        },
        {
          "Multi-modal Mood Reader\n15": "independent emotion recognition based on eeg frequency band features and self-"
        },
        {
          "Multi-modal Mood Reader\n15": "271 (2024)\nadaptive graph construction. Brain Sciences 14(3),"
        },
        {
          "Multi-modal Mood Reader\n15": "45. Dolcos, F., LaBar, K.S., Cabeza, R.:\nInteraction between the amygdala and the"
        },
        {
          "Multi-modal Mood Reader\n15": "medial temporal lobe memory system predicts better memory for emotional events."
        },
        {
          "Multi-modal Mood Reader\n15": "Neuron 42(5), 855‚Äì863 (2004)"
        },
        {
          "Multi-modal Mood Reader\n15": "46. Sporns, O.: Structure and function of complex brain networks. Dialogues in clinical"
        },
        {
          "Multi-modal Mood Reader\n15": "neuroscience 15(3), 247‚Äì262 (2013)"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y Shen",
        "C Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "2",
      "title": "Inspiration transfer for intelligent design: A generative adversarial network with fashion attributes disentanglement",
      "authors": [
        "H Yan",
        "H Zhang",
        "J Shi",
        "J Ma",
        "X Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "3",
      "title": "Gmss: Graph-based multi-task self-supervised learning for eeg emotion recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "F Li",
        "B Fu",
        "H Wu",
        "Y Ji",
        "Y Zhou",
        "Y Niu",
        "G Shi",
        "W Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Cross-subject eeg emotion recognition combined with connectivity features and meta-transfer learning",
      "authors": [
        "J Li",
        "H Hua",
        "Z Xu",
        "L Shu",
        "X Xu",
        "F Kuang",
        "S Wu"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "6",
      "title": "Bone age assessment using convolutional neural networks",
      "authors": [
        "S Wang",
        "Y Shen",
        "D Zeng",
        "Y Hu"
      ],
      "year": "2018",
      "venue": "2018 International conference on artificial intelligence and big data (ICAIBD)"
    },
    {
      "citation_id": "7",
      "title": "Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding",
      "authors": [
        "Z Chen",
        "J Qing",
        "T Xiang",
        "W Yue",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Brainlm: A foundation model for brain activity recordings",
      "authors": [
        "J Ortega Caro",
        "A Oliveira Fonseca",
        "C Averill",
        "S Rizvi",
        "M Rosati",
        "J Cross",
        "P Mittal",
        "E Zappala",
        "D Levine",
        "R Dhodapkar"
      ],
      "year": "2023",
      "venue": "Brainlm: A foundation model for brain activity recordings"
    },
    {
      "citation_id": "9",
      "title": "Devignet: High-resolution vignetting removal via a dual aggregated fusion transformer with adaptive channel expansion",
      "authors": [
        "S Luo",
        "X Chen",
        "W Chen",
        "Z Li",
        "S Wang",
        "C Pun"
      ],
      "year": "2024",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "High-resolution document shadow removal via a large-scale real-world dataset and a frequency-aware shadow erasing net",
      "authors": [
        "Z Li",
        "X Chen",
        "C Pun",
        "X Cun"
      ],
      "year": "2023",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "11",
      "title": "A large-scale film style dataset for learning multi-frequency driven film enhancement",
      "authors": [
        "Z Li",
        "X Chen",
        "S Wang",
        "C Pun"
      ],
      "year": "2023",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "12",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y Lai",
        "G Zhao",
        "X Deng",
        "Y Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Eeg-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Contrastive learning of subjectinvariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J Qiu",
        "W Zheng",
        "B Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "16",
      "title": "Exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "17",
      "title": "Cognitive-emotional interactions in the brain",
      "authors": [
        "J Ledoux"
      ],
      "year": "1989",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "18",
      "title": "Multimodal adaptive emotion transformer with flexible modality inputs on a novel dataset with continuous labels",
      "authors": [
        "W Jiang",
        "X Liu",
        "W Zheng",
        "B Lu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition with pre-trained transformers using multimodal signals",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "20",
      "title": "Hetemotionnet: two-stream heterogeneous graph recurrent neural network for multi-modal emotion recognition",
      "authors": [
        "Z Jia",
        "Y Lin",
        "J Wang",
        "Z Feng",
        "X Xie",
        "C Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W Zheng",
        "B Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from eeg and facial expressions: a multimodal approach",
      "authors": [
        "V Chaparro",
        "A Gomez",
        "A Salgado",
        "O Quintero",
        "N Lopez",
        "L Villa"
      ],
      "year": "2018",
      "venue": "2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "23",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "Emotionmeter: A multimodal framework for recognizing human emotions"
    },
    {
      "citation_id": "24",
      "title": "Cinematic mindscapes: High-quality video reconstruction from brain activity",
      "authors": [
        "Z Chen",
        "J Qing",
        "J Zhou"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Dreamdiffusion: Generating high-quality images from brain eeg signals",
      "authors": [
        "Y Bai",
        "X Wang",
        "Y Cao",
        "Y Ge",
        "C Yuan",
        "Y Shan"
      ],
      "year": "2023",
      "venue": "Dreamdiffusion: Generating high-quality images from brain eeg signals",
      "arxiv": "arXiv:2306.16934"
    },
    {
      "citation_id": "26",
      "title": "The default network dominates neural responses to evolving movie stories",
      "authors": [
        "E Yang",
        "F Milisav",
        "J Kopal",
        "A Holmes",
        "G Mitsis",
        "B Misic",
        "E Finn",
        "D Bzdok"
      ],
      "year": "2023",
      "venue": "Nature communications"
    },
    {
      "citation_id": "27",
      "title": "A dynamical systems approach for multiscale synthesis of alzheimer's pathogenesis",
      "authors": [
        "J Rollo",
        "J Crawford",
        "J Hardy"
      ],
      "year": "2023",
      "venue": "Neuron"
    },
    {
      "citation_id": "28",
      "title": "Fine perceptive gans for brain mr image super-resolution in wavelet domain",
      "authors": [
        "S You",
        "B Lei",
        "S Wang",
        "C Chui",
        "A Cheung",
        "Y Liu",
        "M Gan",
        "G Wu",
        "Y Shen"
      ],
      "year": "2022",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "29",
      "title": "Generative ai for brain image computing and brain network computing: a review",
      "authors": [
        "C Gong",
        "C Jing",
        "X Chen",
        "C Pun",
        "G Huang",
        "A Saha",
        "M Nieuwoudt",
        "H Li",
        "Y Hu",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "30",
      "title": "Ensemble of 3d densely connected convolutional network for diagnosis of mild cognitive impairment and alzheimer's disease",
      "authors": [
        "S Wang",
        "H Wang",
        "A Cheung",
        "Y Shen",
        "M Gan"
      ],
      "year": "2020",
      "venue": "Ensemble of 3d densely connected convolutional network for diagnosis of mild cognitive impairment and alzheimer's disease"
    },
    {
      "citation_id": "31",
      "title": "3-d brain reconstruction by hierarchical shape-perception network from a single incomplete image",
      "authors": [
        "B Hu",
        "C Zhan",
        "B Tang",
        "B Wang",
        "B Lei",
        "S Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "32",
      "title": "Characterization multimodal connectivity of brain network by hypergraph gan for alzheimer's disease analysis",
      "authors": [
        "J Pan",
        "B Lei",
        "Y Shen",
        "Y Liu",
        "Z Feng",
        "S Wang"
      ],
      "year": "2021",
      "venue": "Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021"
    },
    {
      "citation_id": "33",
      "title": "A variational approach to nonlinear two-point boundary value problems",
      "authors": [
        "S Wang"
      ],
      "year": "2009",
      "venue": "Computers & Mathematics with Applications"
    },
    {
      "citation_id": "34",
      "title": "Spatio-temporal ranked-attention networks for video captioning",
      "authors": [
        "A Cherian",
        "J Wang",
        "C Hori",
        "T Marks"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "35",
      "title": "Star-transformer: A spatio-temporal cross attention transformer for human action recognition",
      "authors": [
        "D Ahn",
        "S Kim",
        "H Hong",
        "B Ko"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "36",
      "title": "Transvod: end-to-end video object detection with spatial-temporal transformers",
      "authors": [
        "Q Zhou",
        "X Li",
        "L He",
        "Y Yang",
        "G Cheng",
        "Y Tong",
        "L Ma",
        "D Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "37",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Astdf-net: Attention-based spatial-temporal dual-stream fusion network for eeg-based emotion recognition",
      "authors": [
        "P Gong",
        "Z Jia",
        "P Wang",
        "Y Zhou",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "39",
      "title": "Bci2000: a general-purpose brain-computer interface (bci) system",
      "authors": [
        "G Schalk",
        "D Mcfarland",
        "T Hinterberger",
        "N Birbaumer",
        "J Wolpaw"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on biomedical engineering"
    },
    {
      "citation_id": "40",
      "title": "Prior-guided adversarial learning with hypergraph for predicting abnormal connections in alzheimer's disease",
      "authors": [
        "Q Zuo",
        "H Wu",
        "C Chen",
        "B Lei",
        "S Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "41",
      "title": "Multimodal representations learning and adversarial hypergraph fusion for early alzheimer's disease prediction",
      "authors": [
        "Q Zuo",
        "B Lei",
        "Y Shen",
        "Y Liu",
        "Z Feng",
        "S Wang"
      ],
      "year": "2021",
      "venue": "Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021"
    },
    {
      "citation_id": "42",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Cross-subject eeg emotion recognition with selforganized graph neural network",
      "authors": [
        "J Li",
        "S Li",
        "J Pan",
        "F Wang"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "44",
      "title": "Subjectindependent emotion recognition based on eeg frequency band features and selfadaptive graph construction",
      "authors": [
        "J Zhang",
        "Y Hao",
        "X Wen",
        "C Zhang",
        "H Deng",
        "J Zhao",
        "R Cao"
      ],
      "year": "2024",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "45",
      "title": "Interaction between the amygdala and the medial temporal lobe memory system predicts better memory for emotional events",
      "authors": [
        "F Dolcos",
        "K Labar",
        "R Cabeza"
      ],
      "year": "2004",
      "venue": "Neuron"
    },
    {
      "citation_id": "46",
      "title": "Structure and function of complex brain networks",
      "authors": [
        "O Sporns"
      ],
      "year": "2013",
      "venue": "Dialogues in clinical neuroscience"
    }
  ]
}