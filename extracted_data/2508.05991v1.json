{
  "paper_id": "2508.05991v1",
  "title": "Ecmf: Enhanced Cross-Modal Fusion For Multimodal Emotion Recognition In Mer-Semi Challenge",
  "published": "2025-08-08T03:55:25Z",
  "authors": [
    "Juewen Hu",
    "Yexin Li",
    "Jiulin Li",
    "Shuo Chen",
    "Pring Wong"
  ],
  "keywords": [
    "and Qwen-omni"
  ],
  "sections": [
    {
      "section_name": "1\nINTRODUCTION",
      "text": "N\nArtificial intelligence (AI) has revolutionized numerous industries,\nwith growing emphasis on enhancing its anthropomorphic capabili-\nties. A fundamental aspect of this endeavor is equipping AI systems\nwith the ability to understand human emotions, which is critical\nfor effective human-computer interaction (HCI). Accurate emotion\nrecognition can greatly improve user experience and elevate the\nquality of interaction [17].\nAs a subtask of the MER2025 competition [4], the MER-SEMI\nchallenge seeks to advance the field of emotion recognition by\nproviding a semi-supervised learning setting that includes both\nlabeled and unlabeled video data[12][13]. Its objective is to classify\neach video sample into one of six predefined emotion categories,\ni.e., worry, happiness, neutral, anger, surprise, and sadness.\nHowever, emotion recognition poses several significant chal-\nlenges. Recognizing emotions from video involves multiple modali-\nties, including text, visual, and audio. The core difficulties lie not\nonly in effectively encoding and extracting informative features\nfrom each modality but also in integrating these heterogeneous\nsignals for accurate classification. Moreover, the scarcity of labeled\ndata further complicates the task. For instance, MER2025 provides\nonly 7,369 labeled samples[11], which limits the ability to train fully\nsupervised models and increases the reliance on semi-supervised\nor pre-trained approaches [18].\nTo address the issue of data scarcity, we leverage pre-trained mod-\nels as feature extractors, which have demonstrated strong general-\nization capabilities in data-scarce scenarios. These models, trained\narXiv:2508.05991v1  [cs.CV]  8 Aug 2025\n\n\nMRAC ’25, October 27–31, 2025, Dublin, Ireland.\nJuewen Hu, Yexin Li, JiuLin Li, Shuo Chen, Pring Wong\non large-scale corpora, provide robust and transferable represen-\ntations for each modality. For the textual modality, BERT [6] and\nRoBERTa [14] capture rich semantic and syntactic information\nthrough contextualized embeddings. In the visual domain, I3D [3]\nand SlowFast [7] extract both spatial and temporal features to ef-\nfectively represent dynamic expressions and motion cues. For the\nauditory modality, Wav2Vec [1] and HuBERT [8] produce expres-\nsive speech representations capable of capturing variations in tone,\npitch, and prosody. Furthermore, to further enhance performance,\nwe propose a dual-branch visual encoder and a context-enriched\nmethod for the visual and textual modalities, respectively, both\nbuilt upon the corresponding pre-trained models.\nIn addition, we design a fusion strategy to effectively integrate\nthe rich features extracted from multiple modalities. Prior studies\nhave shown that conflicting or redundant signals across modalities\ncan degrade performance, highlighting the importance of balancing\neach modality’s contribution in multimodal emotion recognition. To\naddress this, rather than directly concatenating features, we employ\nattention mechanisms to dynamically weight the importance of\neach modality. This approach enhances the quality of the joint\nrepresentation and promotes more robust and accurate emotion\nclassification [21].\nBeyond the proposed model architecture, we further refine noisy\nlabels in the training set through a multi-source labeling strategy.\nSpecifically, we train weak classifiers on each individual modality\nusing the original training data. For each sample, we then collect\nemotion labels from the weak classifiers as well as from a large\nlanguage model (LLM). The final refined label is determined via\nmajority voting across these sources. To ensure label reliability, a\nsmall subset of samples exhibiting highly inconsistent predictions\nis manually reviewed and corrected as necessary.\nIn summary, this study proposes a multimodal emotion recogni-\ntion framework to address the MER2025 challenge. Our contribu-\ntions are threefold.\n• To address the issue of data scarcity, we leverage appropriate\npre-trained models as multimodal feature extractors. Specif-\nically, for visual modality, we design a dual-branch visual\nencoder that captures both global frame-level features and lo-\ncalized facial representations. For textual modality, we propose\na context-enriched method using LLMs to enrich emotional\ncues in the text inputs.\n• To handle modality competition, we design a fusion strategy\nthat dynamically weights different modalities to ensure robust\nperformance.\n• Extensive experiments conducted on the official dataset demon-\nstrate a significant improvement over the baseline, achieving\na weighted F-score of 87.49 % compared to 78.63 %.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "2\nRELATED WORKS",
      "text": "S\nModality competition in multimodal fusion. Studies have\nshown that different modalities, such as audio, video, and text, may\ncompete during fusion, adversely affecting emotion recognition\nperformance. Huang et al. [9] investigated the reasons for failures in\njoint training of multimodal networks, emphasizing the importance\nof balancing contributions from each modality. Katak et al. [10]\nproposed the Maple method, which adaptively focuses on relevant\nmodalities through prompt learning to mitigate competition. Lian\net al. [4] addressed noise and open-vocabulary scenarios in semi-\nsupervised learning, making their approach suitable for real-world\napplications.\nSpatiotemporal features of video. Video-based emotion recog-\nnition requires capturing both spatial and temporal dynamics. Ruan\net al. [22] utilized 3D CNNs to extract spatiotemporal features from\naudio and video, improving recognition accuracy. Another study\n[5] applied 3D CNNs to model spatiotemporal representations in\nEEG signals, achieving significant results. Deep learning methods,\nsuch as 3D CNNs and LSTMs, excel at learning complex patterns,\nmaking them well-suited for dynamic emotion analysis.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "3\nMETHODOLOGY",
      "text": "Y\nThis section elaborates on the proposed method, which is organized\ninto three subsections. First, we present the overall framework and\nprovide a high-level description of its architecture. Second, we\ndetail the feature extraction process for each modality. Finally, we\ndescribe the multimodal fusion strategy employed to integrate the\nextracted features.\n3.1\nModel Architecture\nWe propose a multimodal emotion recognition framework, whose\noverall architecture is illustrated in Figure 1 (a). The framework con-\nsists of three main components, i.e., data input, feature extraction,\nand feature fusion.\nAt the data input stage, we first extract each modality from the\nraw video data. Modality-specific features are then obtained using\nthree pre-trained models, i.e., HuBERT-Large for audio, Chinese-\nRoBERTa-wwm-ext-large for text, and CLIP-ViT-Large for visual\ninformation. The extracted feature vectors are subsequently stan-\ndardized. Finally, each modality’s representation is fed into a ded-\nicated Feature Fusion Module to generate the final multimodal\nrepresentation for subsequent emotion classification.\n3.2\nFeature Extraction\n3.2.1\nAudio. Speech plays a crucial role in emotion recognition, as\nidentical content conveyed with different intonations can express\ndistinct emotions. Therefore, extracting audio features such as pitch,\nvolume, and tone is essential. Various pre-trained encoders differ\nin their capability to capture such features.\nInspired by prior work [24], we employ HuBERT-Large to extract\nemotional features from audio signals. Specifically, we utilize the\noutputs from layers 16 to 21 of the HuBERT-Large model, as these\nlayers have been shown to capture richer prosodic and spectral pat-\nterns [24]. Their representations exhibit enhanced adaptability to\nacoustic variations, making them particularly effective for emotion\nrecognition. As illustrated in Figure 1 (e), the audio data is fed into\nthe HuBERT-Large model, from which the selected layer outputs\nare standardized and passed to the feature fusion module.\n3.2.2\nText. Textual content plays a pivotal role in conveying emo-\ntional expressions, as it captures both the contextual background\nand causal relationships of events depicted in videos. Emotion-\nrelated lexical elements within the text, such as sentiment-bearing\n\n\nECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge\nMRAC ’25, October 27–31, 2025, Dublin, Ireland.\nFigure 1: Enhanced Cross-Modal Fusion Architecture for Multimodal Emotion Recognition. (a) Model Pipeline: integrated\nworkflow comprising data input, feature extraction, feature fusion module, and classification. (b) Text Feature Extraction:\ncontext-enriched encoding via Chinese-RoBERTa-wwm-ext-large, enhanced with GPT4-generated keywords and Qwen-omni\nemotion clues. (c) Video Feature Extraction: dual-branch encoding with OpenFace for facial detection, and CLIP-ViT-Large\nfor spatial encoding of full frames and facial regions. (d) Feature Fusion Module: multimodal integration through residual\nconnections, Modal_Token incorporated, and two-layer self-attention. (e) Audio Feature Extraction: prosodic feature extraction\nusing layers 16-21 of HuBERT-Large. Note: F_x denotes feature streams from respective modalities.\nwords or specific emotional particles, greatly contribute to distin-\nguishing between different emotional states. However, the emotion\nrecognition accuracy of the text modality often lags behind that of\nthe audio and visual modalities.\nTo address this limitation, we propose a context-enriched method\nthat leverages LLMs to enhance emotional cues within textual in-\nputs. Specifically, we augment the original text using GPT-4 and\nQwen-Omni [19]. GPT-4 is employed to generate pseudo-labels and\nemotion-related keywords for each text sample, thereby enriching\nthe emotional context [16]. In parallel, Qwen-Omni processes au-\ndio and visual content using carefully designed prompts, as shown\nin Figure 1 (b), generating pseudo-labels, detailed video descrip-\ntions, and auxiliary emotional cues [23]. These enriched outputs,\nalong with the original text inputs, are subsequently encoded by\nthe Chinese-RoBERTa-wwm-ext-large model to produce enhanced\ntextual features. The resulting embeddings are standardized and\nintegrated into the feature fusion layer for downstream analysis.\n3.2.3\nVideo. Human expressions and body language serve as key\nindicators of emotion. The MER2025 baseline [4] extracts and en-\ncodes facial regions from each video frame, achieving moderate\nperformance. Recognizing that body movements also contribute\nsignificantly to emotional expression, we develop a dual-branch\nvisual encoder that integrates both global frame-level features and\nfacial representations.\nAs illustrated in Figure 1 (c), for each video frame, facial infor-\nmation is detected and extracted using OpenFace [2]. Both the\nextracted facial patches and the full video frames are then fed into\nthe CLIP-ViT-Large encoder [20]. The resulting dual-scale visual\nfeatures are standardized and subsequently fed into the feature\nfusion layer.\n3.3\nFeature Fusion\nMultimodal feature fusion plays a pivotal role in emotion recogni-\ntion, as it enables the effective integration of emotional cues from\ndifferent modalities. Although modality-specific features are ex-\ntracted using appropriate pre-trained models—HuBERT-Large for\n\n\nMRAC ’25, October 27–31, 2025, Dublin, Ireland.\nJuewen Hu, Yexin Li, JiuLin Li, Shuo Chen, Pring Wong\naudio, Chinese-RoBERTa-wwm-ext-large for text, and CLIP-ViT-\nLarge for video—some emotion-irrelevant information may still be\nretained in the feature representations. Therefore, a robust fusion\nstrategy is essential to refine these representations and emphasize\nemotionally salient information.\nWe propose a fusion method based on self-attention mechanisms\nwith residual connections. As illustrated in Figure 1 (d), the outputs\nfrom each feature extractor are first processed through an encoder\nincorporating a residual module, which preserves original infor-\nmation while capturing additional emotional cues and projecting\nfeatures into a unified space. Standardization and dropout layers\nare applied to accelerate model convergence.\nFor each modality, a learnable Modal_Token is prepended to the\nfeature sequence to encode modality-specific information—such as\ndistinguishing between audio, text, or video features—analogous\nto the use of positional encodings in Transformers. The resulting\nsequence is then fed into two self-attention layers, which produce\nthe final emotion prediction [21].\n3.4\nImplementation Details\nBeyond the architectural design, we further enhance practical per-\nformance by refining noisy labels in the training set. In addition,\nwe employ ensemble learning to determine the final emotion label\nfor each sample, thereby improving label reliability.\n3.4.1\nRefining Noisy Labels. During data preprocessing, we ob-\nserved inconsistencies between certain training labels and their\ncorresponding video content in the MER2025-SEMI dataset. To\naddress this issue, we refine the noisy labels using a multi-source\nlabeling strategy. Specifically, we trained weak classifiers on each in-\ndividual modality using the original training data. For each sample,\nwe collected emotion label predictions from these weak classifiers.\nAdditionally, we leveraged Qwen-Omni to generate auxiliary emo-\ntion labels. We then applied a majority voting scheme across all\nsources to derive refined labels. For samples where all predicted\nlabels disagreed with the original annotation, we manually verified\nand corrected the labels to ensure quality. This relabeling improved\nmodel performance, consistent with findings in prior work [15].\n3.4.2\nEnsemble Learning. Based on the architecture illustrated in\nFigure 1 (a), we construct several model variants to enhance label\nquality through ensemble learning. Specifically, we either randomly\nremove certain modules from the original framework or train the\nsame model using different random seeds to introduce diversity.\nThese variant models are then used to predict emotion labels for\neach video sample. Finally, we apply a majority voting scheme\nacross the predictions from all variants to obtain the final ensemble-\nbased emotion labels.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "4\nEXPERIMENTS",
      "text": "S\nThis section presents the experimental setup and results of the pro-\nposed framework, including details on the dataset, hyperparameter\nconfigurations, and performance evaluation.\nTable 1: WAF of Different Methods.\nMethods\nWAF / val\nWAF / test\nBaseline\n82.05%\n76.80%\n+ Multi-source labeling strategy\n82.31%\n78.67%\n+ Dual-branch visual encoder\n82.80%\n78.68%\n+ Modal_Token\n83.27%\n78.84%\n+ Norm\n83.20%\n84.40%\n+ Roberta\n83.50%\n85.30%\n+ Fold-6\n83.60%\n85.60%\n+ GPT4-label\n84.09%\n86.08%\n+ GPT4-keywords\n84.15%\n86.49%\n+ MLP\n84.29%\n86.94%\n+ Selective Hubert_Layer\n84.84%\n87.14%\n+ Ensemble learning\n-\n87.49%\n4.1\nDataset\nWe utilized the MER2025-SEMI dataset, comprising 7,369 labeled\nsamples and 20,000 unlabeled samples. The official baseline em-\nploys five-fold cross-validation to split the training set into training\nand validation subsets, averaging the best results across the five\nvalidation sets to obtain the final weighted F-score (WAF).\n4.2\nSettings\nTo ensure stable training, we set the hidden dimension to 128, the\ndropout rate to 0.6, and use two self-attention heads, with gradient\nclipping at 1.0, a learning rate of 5e-5, and up to 200 training epochs.\nIn addition to comparing our method with the official baseline,\nwe conduct studies to evaluate the contribution of each module\nin our framework. While several components have been clearly\nexplained in previous sections, we provide further clarification for\nthe less intuitive ones as follows.\n• Norm standardizes features using the mean and standard\ndeviation to ensure consistent distributions across modalities.\n• Fold-6 applies 6-fold cross-validation to improve generaliza-\ntion by training and validating on six different data splits.\n• GPT4-label leverages GPT-4 to analyze video content and\ngenerate emotion labels, thereby enhancing the quality of\ntext-based feature representations.\n• GPT4-keywords utilizes GPT-4 to extract semantic keywords\nfrom textual data, enriching the text inputs.\n• MLP refines the multilayer perceptron architecture to re-\nencode features into a unified space.\n4.3\nResults\nOur results, as shown in Table 1, demonstrate that the proposed\nmethod substantially outperforms the baseline. Although the base-\nline achieves comparable performance on the validation set, its test\nperformance drops to 76.8 %, which is even lower than the 78.63 %\nreported in the official benchmark paper[11].\nAt the data level, our multi-source labeling strategy leads to\nmore accurate labels and improved model generalization compared\nto the baseline. At the feature level, the dual-branch visual encoder\nenables complementary integration of global scene information\n\n\nECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge\nMRAC ’25, October 27–31, 2025, Dublin, Ireland.\nand fine-grained facial cues, effectively boosting visual represen-\ntation quality. For the text modality, the incorporation of LLMs\nenriches emotional context, thereby mitigating its relative under-\nperformance. In the audio modality, selectively utilizing emotion-\nsensitive layers from HuBERT-Large further strengthens emotional\nfeature extraction. Finally, ensemble learning consistently boosts\nperformance, yielding gains of 0.5–1.3 percentage points.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "5\nCONCLUSION",
      "text": "N\nThis study presents a multimodal emotion recognition framework\nfor the MER2025-SEMI challenge, leveraging pre-trained models\nand advanced fusion techniques to enhance performance under\nlimited labeled data. Our contributions include: a context-enriched\nmethod using LLMs to improve the emotional expressiveness of\ntext features; a dual-branch visual encoder integrating global frame-\nlevel features and localized facial representations to enhance vi-\nsual modality analysis; a fusion strategy based on self-attention\nwith residual connections to effectively integrate multimodal fea-\ntures; and a multi-source labeling strategy to correct noisy labels in\nthe training set. Experimental results demonstrate superior perfor-\nmance on the MER2025-SEMI dataset, significantly outperforming\nthe baseline. Future work will explore additional data augmenta-\ntion and fusion strategies to further enhance the accuracy and\nrobustness of the proposed emotion recognition framework.\nACKNOWLEDGMENTS\nThis work was supported by the State Key Laboratory of General\nArtificial Intelligence, BIGAI. We thank our colleagues for their\nvaluable feedback during the development of this project.\nREFERENCES\n[1] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.\nwav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representa-\ntions. In Proceedings of the 34th International Conference on Neural Information Pro-\ncessing Systems (NeurIPS’20). NeurIPS, Virtual. https://arxiv.org/abs/2006.11477\n[2] Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe Morency. 2016. OpenFace:\nAn open source facial behavior analysis toolkit. In 2016 IEEE Winter Conference\non Applications of Computer Vision. 1–10. doi:10.1109/WACV.2016.7477553\n[3] Joao Carreira and Andrew Zisserman. 2017. Quo Vadis, Action Recognition? A\nNew Model and the Kinetics Dataset. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR’17). IEEE, Honolulu, HI, 4724–\n4733. doi:10.1109/CVPR.2017.502\n[4] MER2025 Challenge. 2025. MER2025: Multimodal Emotion Recognition Challenge.\nRetrieved July 22, 2025 from https://zeroqiaoba.github.io/MER2025-website/\n[5] Junghyun Cho and Hyungjoo Hwang. 2020. Spatio-Temporal Representation of\nan Electroencephalogram for Emotion Recognition Using a Three-Dimensional\nConvolutional Neural Network. Sensors 20, 12 (jun 2020), 3491. doi:10.3390/\ns20123491\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (NAACL-HLT’19).\nAssociation for Computational Linguistics, Minneapolis, MN, 4171–4186. doi:10.\n18653/v1/N19-1423\n[7] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow-\nFast Networks for Video Recognition. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV’19). IEEE, Seoul, South Korea, 6202–6211.\ndoi:10.1109/ICCV.2019.00630\n[8] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,\nRuslan Salakhutdinov, and Abdelrahman Mohamed. 2021.\nHuBERT: Self-\nSupervised Speech Representation Learning by Masked Prediction of Hidden\nUnits. IEEE/ACM Transactions on Audio, Speech, and Language Processing 29\n(2021), 3451–3460. doi:10.1109/TASLP.2021.3122291\n[9] Yu Huang, Junyang Lin, Chang Zhou, Hongxia Yang, and Longbo Huang. 2022.\nModality Competition: What Makes Joint Training of Multi-modal Network Fail\nin Deep Learning? (Provably). In Proceedings of the 39th International Conference\non Machine Learning (ICML’22, Vol. 162). PMLR, Virtual, 9226–9259.\nhttps:\n//arxiv.org/abs/2203.01389\n[10] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan,\nand Fahad Shahbaz Khan. 2023. MaPLe: Multi-modal Prompt Learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR’23). IEEE, Vancouver, Canada. https://arxiv.org/abs/2210.03117\n[11] Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang\nCheng, Bin Liu, Rui Liu, Xiaojiang Peng, Jiangyan Yi, Jianhua Tao, et al. 2025.\nMER 2025: When Affective Computing Meets Large Language Models. arXiv\npreprint arXiv:2504.19423 (2025). https://arxiv.org/abs/2504.19423 Accessed: July\n22, 2025.\n[12] Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mngyu Xu, Kexin Wang, Ke\nXu, et al. 2023. Mer 2023: Multi-label learning, modality robustness, and semi-\nsupervised learning. In Proceedings of the 31st ACM International Conference on\nMultimedia. ACM, 9610–9614.\n[13] Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen,\nHao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, Jiangyan Yi, Rui Liu, Kele Xu,\nBin Liu, Erik Cambria, Guoying Zhao, Björn W. Schuller, and Jianhua Tao. 2024.\nMER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary\nMultimodal Emotion Recognition. arXiv:2404.17113 [cs.CV] https://arxiv.org/\nabs/2404.17113\n[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa:\nA Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\nhttps://arxiv.org/abs/1907.11692\n[15] Usman Malik, Simon Bernard, Alexandre Pauchet, Clément Chatelain, Romain\nPicot-Clémente, and Jérôme Cortinovis. 2024. Pseudo-Labeling With Large\nLanguage Models for Multi-Label Emotion Classification of French Tweets. IEEE\nAccess 12 (2024), 15902–15916. doi:10.1109/ACCESS.2024.3354705\n[16] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv.\norg/abs/2303.08774\n[17] Rosalind W. Picard. 1997. Affective Computing. MIT Press, Cambridge, MA.\n[18] Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. 2017. A review of\naffective computing: From unimodal analysis to multimodal fusion. Information\nFusion 37 (sep 2017), 98–125. doi:10.1016/j.inffus.2017.02.004\n[19] Anbin Qi. 2024. Multimodal Emotion Recognition with Vision-language Prompt-\ning and Modality Dropout. arXiv:2409.07078 [cs.CV] https://arxiv.org/abs/2409.\n07078\n[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models\nFrom Natural Language Supervision. In Proceedings of the 38th International\nConference on Machine Learning (ICML’21). PMLR, 8748–8763. https://arxiv.org/\nabs/2103.00020\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Proceedings of the 31st International Conference on Neural Infor-\nmation Processing Systems (NeurIPS’17). NeurIPS, Long Beach, CA, 5998–6008.\nhttps://arxiv.org/abs/1706.03762\n[22] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai\nYin, and Xiang Ruan. 2017. Learning to Detect Salient Objects with Image-Level\nSupervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR’17). IEEE, Honolulu, HI, 136–145. doi:10.1109/CVPR.2017.23\n[23] Jin Xu et al. 2025. Qwen2.5-Omni Technical Report. arXiv:2503.20215 [cs.CL]\nhttps://arxiv.org/abs/2503.20215\n[24] Zhixian Zhao. 2024. Improving Multimodal Emotion Recognition by Leveraging\nAcoustic Adaptation and Visual Alignment. arXiv:2409.05015 [cs.CV] https:\n//arxiv.org/abs/2409.05015",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a). The framework con-",
      "page": 2
    },
    {
      "caption": "Figure 1: (e), the audio data is fed into",
      "page": 2
    },
    {
      "caption": "Figure 1: Enhanced Cross-Modal Fusion Architecture for Multimodal Emotion Recognition. (a) Model Pipeline: integrated",
      "page": 3
    },
    {
      "caption": "Figure 1: (b), generating pseudo-labels, detailed video descrip-",
      "page": 3
    },
    {
      "caption": "Figure 1: (c), for each video frame, facial infor-",
      "page": 3
    },
    {
      "caption": "Figure 1: (d), the outputs",
      "page": 4
    },
    {
      "caption": "Figure 1: (a), we construct several model variants to enhance label",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: WAF of Different Methods.",
      "page": 4
    },
    {
      "caption": "Table 1: , demonstrate that the proposed",
      "page": 4
    }
  ],
  "citations": []
}