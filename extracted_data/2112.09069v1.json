{
  "paper_id": "2112.09069v1",
  "title": "Progressive Graph Convolution Network For Eeg Emotion Recognition",
  "published": "2021-12-14T03:30:13Z",
  "authors": [
    "Yijin Zhou",
    "Fu Li",
    "Yang Li",
    "Youshuo Ji",
    "Guangming Shi",
    "Wenming Zheng",
    "Lijian Zhang",
    "Yuanfang Chen",
    "Rui Cheng"
  ],
  "keywords": [
    "Progressive graph convolution network (PGCN)",
    "EEG emotion recognition",
    "emotional hierarchical categories",
    "brain region"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Studies in the area of neuroscience have revealed the relationship between emotional patterns and brain functional regions, demonstrating that dynamic relationships between different brain regions are an essential factor affecting emotion recognition determined through electroencephalography (EEG). Moreover, in EEG emotion recognition, we can observe that clearer boundaries exist between coarse-grained emotions than those between fine-grained emotions, based on the same EEG data; this indicates the concurrence of large coarse-and small fine-grained emotion variations. Thus, the progressive classification process from coarse-to fine-grained categories may be helpful for EEG emotion recognition. Consequently, in this study, we propose a progressive graph convolution network (PGCN) for capturing this inherent characteristic in EEG emotional signals and progressively learning the discriminative EEG features. To fit different EEG patterns, we constructed a dual-graph module to characterize the intrinsic relationship between different EEG channels, containing the dynamic functional connections and static spatial proximity information of brain regions from neuroscience research. Moreover, motivated by the observation of the relationship between coarse-and fine-grained emotions, we adopt a dual-head module that enables the PGCN to progressively learn more discriminative EEG features, from coarse-grained (easy) to fine-grained categories (difficult), referring to the hierarchical characteristic of emotion. To verify the performance of our model, extensive experiments were conducted on two public datasets: SEED-IV and multi-modal physiological emotion database (MPED). The experiment results show that the PGCN achieves a state-of-art performance. Furthermore, we explored the effect of different frequency bands based on our model, and visualized the activated brain regions. The experiment results reveal the relationship between human emotion and highfrequency EEG signals, as well as the importance of the frontal and temporal lobes for emotion expression.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions play an extremely important role in human-human interaction activities, such as negotiation and work  [1] ,  [2] . Accordingly, in a human-machine interaction (HMI), we hope that machines can make adaptive changes according to the variation of human emotions to communicate with us, thus making emotion recognition an important topic in HMI  [3] . Human beings can easily realize the emotional states of others through their behavioral signals. Therefore, many studies on emotion recognition have focused on distinguishing emotional states according to behavioral signals, such as changes in voice signals  [4] ,  [5]  and facial expressions  [6] . Compared with these behavioral signals, physiological signals can also reflect human emotional changes, but are more reliable and difficult to camouflage  [7] . In various physiological signals, electroencephalography (EEG) signals are of particular concern because of their non-invasiveness, portability, and affordability, and have received substantial attention recently  [8] -  [10] .\n\nTwo issues should be addressed for EEG emotion recognition. The first issue is how to utilize the inherent characteristics of emotions to extract more discriminative features. Several popular methods for EEG emotion recognition mainly utilize the general properties of EEG signals, rather than the characteristics of emotions in such signals. For example, Alhagry et al. proposed a long short-term memory (LSTM) network with a dense layer for extracting temporal relations and achieving the classification of EEG emotion recognition  [11] . In addition, Jia et al. proposed SST-EmotionNet for EEG emotion recognition, which is a novel dense 3D spatial-spectral-temporalbased attention network  [12] . These methods have not sufficiently exploited the specific characteristics of emotions for EEG emotion recognition. Further, it would be more promising and fruitful if we could integrate the unique characteristics of emotion into deep learning methods to extract more emotionrelated features. As an intrinsic characteristic of emotion, hierarchical characteristics are intuitively reflected in daily life.\n\nFor example, when we communicate with others, it is easy to understand negative emotional states of other people (coarsegrained category); however, it is relatively difficult to further distinguish which negative emotion (fine-grained category), such as sadness, fear, or disgust, a person is experiencing. Some studies have also revealed this hierarchical characteristic for both coarse-and fine-grained categories. For example, Zhang et al. believe that positive emotions can be regarded as a class of emotions, including pride, contentment, amusement, love, and so on  [13] . Belinda et al. believe these lowerlevel positive emotion categories are under the umbrella of happiness, sharing the same feeling of happiness together  [14] . In addition, Philip et al. discovered that the Euclidean distances of disgust, fear, and anger are extremely similar, which shows the hierarchical characteristic of negative emotions from the other side  [15] . To clearly demonstrate this hierarchical characteristic of emotion, we show the data distribution of coarse-and fine-grained emotions, based on the same EEG data, as indicated in Fig.  1 . Notably, the boundaries between coarse-grained emotions (Fig.  1 (a) ) are clearer than those between fine-grained emotions (Fig.  1 (b) ), which reflects the existence of the hierarchical characteristic of emotion. Therefore, this hierarchical characteristic is a type of emotionspecific information that can be exploited to obtain more discriminative information for EEG emotion recognition.\n\nThe second issue for EEG emotion recognition is how to make use of the topological information of brain regions to model EEG emotional signals. Traditional methods are mainly based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which model EEG signals as image-or speech-like data  [16] ,  [17] . However, owing to their characteristics, CNNs and RNNs are limited in that the model input must be grid data, ignoring the connection among brain regions. Recently, graph-based methods have also been proposed to handle EEG emotion recognition tasks, wherein the data are distributed within a non-Euclidean space  [18] ,  [19] . However, these methods only adopt a single or fixed graph structure to model the EEG signals, which is unsuitable for reflecting the internal relationships between brain regions and human emotions. Neuroimaging studies have shown that emotions have a unique relationship with the activity of distributed neural systems that span the cortical and subcortical regions  [15] . Neuroscience studies have revealed the relationship between emotional patterns and the functional connections of the brain regions  [20] ,  [21] , and have indicated that the physical proximity of the brain in space is equally important  [22] . Meanwhile, EEG signals vary among individuals, and the relationship between EEG electrodes is dynamic  [7] . Therefore, it will be helpful to model EEG emotional signals according to the dynamic functional connections of the brain regions and prior knowledge in the area of neuroscience.\n\nTo address the aforementioned issues, in this study, we propose a progressive graph convolution network (PGCN) to progressively learn the discriminative information among EEG emotion signals concerning EEG emotion recognition. To achieve the first goal, we need to solve the first issue, this is, how to utilize the hierarchical characteristic of emotion to extract more discriminative features. Specifically, the hierarchical characteristic of emotion contains information on the commonness and individuality between fine-and coarsegrained emotions. Motivated by this, we construct a dualhead fine-and coarse-grained module, which enables progressive emotion recognition from coarse-grained (easy) to fine-grained categories (difficult). The coarse-grained head extracts the common features of polarization emotions, this is, positive, negative, and neutral emotions, aiming to obtain coarse-grained emotion patterns. The fine-grained head captures particular features from the subtler emotions to encode fine-grained emotion patterns. The key to solving the second issue lies in encoding the functional connections of the brain regions corresponding to emotional patterns. In the procedure used to build the graph structure for modeling EEG signals, we construct a dual-graph module, comprising two brain view graphs based on the spatial proximity and functional connectivity of the brain. The functional connectivity-based dynamic graph originates entirely from the input instance and adaptively changes along with it, which regards the dependency of each pair of electrodes as directed. The spatial distance-based static graph considers the interaction relationship between adjacent brain regions at physical distances. By integrating the above two graphs, the PGCN can learn the complementary interactive information of all electrodes from a global perspective, which enhances the robustness and discrimination of the model.\n\nOur contributions are three-fold.\n\n• We propose a dual-head model called a PGCN based on the hierarchical characteristic of emotion. The PGCN progressively learns discriminative EEG features from coarseto fine-grained categories.\n\n• We constructed a dual-graph module based on the functional connectivity and spatial proximity of the brain regions. The complementarity of the two brain view graphs provides rich spatial topology information for capturing the internal relationship between different EEG channels.\n\n• We compared the PGCN with several advanced models on two public datasets, namely, SEED-IV and MPED. The results show that the proposed PGCN model achieves a state-of-art performance. The comparative experiment showed that human emotions are related more to high-frequency EEG signals.\n\nThe rest of this paper is organized as follows. Section II summarizes related studies on EEG emotion recognition. Section III introduces the proposed PGCN in detail. Section IV discusses the results of the extensive experiments conducted. Finally, Section V provides the concluding remarks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Existing Methods For Eeg Emotion Recognition",
      "text": "In the past years, numerous studies have attempted to solve EEG emotion recognition using machine learning methods. For instance, Nie et al. used a linear dynamic system to smooth the features extracted from EEG data followed by a support vector machine (SVM) to classify positive and negative emotions. They demonstrated that emotion is closely related to different brain regions and frequency bands  [23] . In addition, Lin et al. extracted five-band power spectrum features from EEG data collected during the music wake-up paradigm, and then, classified these features using a multi-layer perceptron (MLP) and an SVM to achieve classification  [24] . Moreover, Gupta et al. comprehensively investigated a flexible analytic wavelet transform (FAWT) based feature extraction method for an emotional EEG, leading to the channel-specific nature of EEG signals  [25] . Recently, with the outstanding performance of deep learning models for various tasks, such as target detection, target recognition, and speech recognition  [26] -  [28] , an increasing number of researchers have adopted deep learning methods in the field of EEG emotion recognition. Alhagry et al. proposed an LSTM network with a dense layer to extract temporal relations and achieve classification  [11] . Xing et al. applied a stack autoencoder (SAE) and an LSTM network to build a linear hybrid model and an emotional time series model, respectively. These two models were then combined to classify EEG data from the DEAP dataset  [29] . Chao et al. attempted to extract several EEG features using a multiband feature matrix (MFM), and a capsule network(CapsNet) was then employed for classification  [30] . In addition, Li et al. took advantage of the emotional brain asymmetries between the left and right hemispheres, and adopted a bi-hemisphere domain adversarial neural network (BiDANN) for EEG emotion recognition. They mapped the EEG data of the left and right hemispheres into discriminative feature spaces separately; employed a global and two local domain discriminators; and achieved high-quality performance on the SEED  [31] . Based on the discrepancy in emotion expression between the left and right hemispheres of the human brain, Li et al. also proposed a novel bi-hemispheric discrepancy model (BiHDM) for EEG emotion recognition, and achieved excellent performance on three public EEG emotional datasets  [32] . Nevertheless, although the aforementioned methods have achieved relatively acceptable results, how to model the relationship between EEG electrodes needs to be studied further.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Gcn For Eeg Emotion Recognition",
      "text": "Because different brain regions are in a non-Euclidean space, a graph becomes the most appropriate data structure for EEG signals. Hence, an increasing amount of recent research has focused on the provisioning of graph theory-based methods for EEG emotion recognition. Wang et al. developed a methodology called the broad dynamical graph learning system (BDGLS) for dynamically extracting features  [33] . In different study, Wang et al. used a phase-locking value (PLV) based graph convolutional neural network (P-GCNN) model to acquire spatiotemporal characteristics and internal information  [19] . Song et al. proposed a dynamical graph convolutional neural network (DGCNN) to recognize EEG emotion, which can dynamically learn the information transfer relationship between the nodes in a graph. A DGCNN was evaluated on the SEED and DREAMER dataset, and achieved good results  [34] . Song et al. also introduced the instance-adaptive graph (IAG) method. The graph of the IAG was generated from the input samples to retain the spatial features of the EEG data. IAG successfully solved individual differences, and dynamically described the relationships among different EEG regions  [35] . Yin et al. investigated a combination of a GCNN and an LSTM. The GCNN was adopted to extract graph domain features, while the LSTM was used to obtain temporal correlation  [36] . The above studies demonstrate the importance of properly modeling EEG signals. Some researchers have noticed the significance of the dynamic functional connectivity of the brain regions. These aforementioned methods were compared with our model in the experimental study.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Proposed Method For Emotion Recognition",
      "text": "To specify the proposed method clearly, we depict the PGCN framework in Fig.  2 . The network aims to capture the inherent characteristics of EEG emotional signals and learn the discriminative EEG features progressively. We adopted three steps to achieve this goal. The first step was to obtain a dynamic and static graph representation. Subsequently, the dualhead network was adopted to obtain deep emotion features, to capture the emotion patterns. Finally, we applied a dualgranular emotion classification progressively from coarse-to fine-grained categories. In the following section, we introduce the details of the proposed PGCN model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Obtaining Dynamic And Static Graph Connections",
      "text": "To model the connections between different brain regions, we constructed two brain view graphs based on the spatial proximity and functional connectivity of the brain regions, to represent the dependency of each pair of nodes. Specifically, inspired by the attention mechanism  [37] , the PGCN learns a dynamic mapping relation to adaptively weigh the EEG channels according to the different instances, which constructs a dynamic graph that can select more useful electrodes in the brain regions. This graph is more powerful in representing the relationships among different EEG channels across different instances. Meanwhile, the physical proximity of the brain in space based on neuroscience research is an important reference that should be considered  [22] . Inspired by this, we constructed a static graph that fully considers the spatial proximity, based on prior knowledge in the field of neuroscience.\n\nSpecifically, we first decomposed the EEG signal into five frequency bands: δ band (1-5 Hz), θ band (4-8 Hz), α band (8-14 Hz), β band (14-30 Hz) and γ band (30-50 Hz). Subsequently, we extracted the energy feature from each frequency band to form the input EEG feature matrix X ∈ R n×d , where n is the number of EEG channels, and d is the number of frequency bands. We constructed a directed graph, represented as G = (V, E), using the standard symbol representation in graph theory, where V = {v i } n i=1 represents the set of n nodes in the graph, and (v i , v j ) ∈ E denotes the edge connected by nodes v i and v j in the graph. To characterize the relationship between the nodes in the graph, an adjacency matrix G ∈ R n×n is utilized in the PGCN.\n\nAccording to the spatial proximity based on prior knowledge of neuroscience research, we encode the adjacent nodes in each brain region into a static graph G s ∈ R n×n with an undirected attribute. Meanwhile, based on the functional connectivity of the brain regions, we extract the spatial information of the EEG signals by multiplying X left by a trainable Fig.  2 : Framework of PGCN. The coarse-grained head is employed to extract the commonalities of emotional patterns, and the fine-grained head is employed to extract the subtle differences of the emotional patterns.\n\nweight matrix P ∈ R n×n . Then, a bias matrix B ∈ R n×d is added to the above operation to increase the flexibility. To extract the frequency information of the EEG samples, a trainable weight matrix Q ∈ R d×nd is right multiplied with the above, to represent graphs in the d frequency bands. The overall process of this dynamic graph after reshaping can be expressed as follows:\n\nwhere Relu is applied to the output to guarantee non-negative elements.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Extracting Deep Emotion Features",
      "text": "After obtaining the dynamic and static graph representations, we constructed a dual-head module to extract the hierarchical characteristics for classification, which enabled the PGCN to progressively learn the discriminative features from coarse-to fine-grained categories. Motivated by the hierarchical characteristic of emotion, a coarse-grained head is proposed to extract the common features of coarse-grained emotion patterns, which has a significant effect on improving the performance of EEG emotion recognition. Meanwhile, a fine-grained head is proposed to extract the information of fine-grained emotions, which is responsible for the target task. Next, we introduce the dual-head module in detail.\n\nTo extract high-level emotion features, we employ the graph Fourier transform on the above dual-graph structure based on graph filtering theories  [38] . Owing to the computational complexity of a direct calculation, we adopt Chebyshev polynomials to approximate the graph convolution operation  [38] . Let ϕ k (G) = G k denote the k-order polynomial of the adjacency matrix G. A multilevel graph convolution can be represented as follows:\n\nwhere ϕ k (G) is the k-th level graph, and X is the output. In addition, g(•) denotes the convolution operation.\n\nIn particular, because the PGCN adopts a dual-head module to model the hierarchical characteristic of emotion, we obtain two static (G s c and G s f ) and two dynamic (G d c and G d f ) graphs for the coarse-and fine-grained head, respectively. For the coarse-grained head, we modeled the spatial proximity of the EEG data as\n\nwhere K is the output dimension for the graph convolution operation, and X is the input sample. Meanwhile, considering the relationship between emotion and the EEG signals from different frequency bands, the functional connectivity of the brain regions in the coarse-grained head can be captured from multiple graphs, expressed as\n\nwhere Cat[•] denotes the concatenation operation. Note that, for the dynamic graph, we consider the different relationships between human emotion and the EEG signals from different frequency bands, leading to differences in the dynamic graph of each frequency band. Similarly, we apply the above operations for the fine-grained head, and obtain the deep features\n\nThrough the above operation, we can obtain the deep data representations for the coarse-and fine-grained heads, which are denoted as\n\n, respectively. The overall data representation can then be denoted as\n\n, which progressively learn the discriminative features from coarse-to fine-grained categories. In summary, the proposed dual-head module extracts more discriminative features, enabling the PGCN to achieve advanced results in dual-granular emotion classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Dual-Granular Discriminative Prediction",
      "text": "As in most supervised models, we add a supervision term to the network by applying the softmax function to the output features, to predict the class label. Specifically, we re-arrange\n\nand ĥ ∈ R 1×(n * K) , respectively. The output of the FC layer can be expressed as follows:\n\nwhere\n\n]×Pc and b c ∈ R 1×Pc are the transform matrices of the coarse-grained head; W ∈ R (n * K)×P and b ∈ R 1×P are the transform matrices of the fine-grained head; P c and P represent the numbers of coarse-and fine-grained emotion categories, respectively.\n\nThen, the discriminative prediction from the softmax layer for emotion recognition can be expressed as follows:\n\nwhere Y c (p|X t ) and Y (p|X t ) denote the predicted probability that the input sample X t belongs to the pth class in the coarse-and fine-grained heads, respectively. Consequently, labels l t and lt of sample X t are predicted as follows:\n\nConsequently, the loss functions of the dual-granular L c and L can be expressed as\n\nHere, l g and lg represent the ground-truth labels of sample X t , corresponding to coarse-and fine-grained heads, respectively; M 1 is the number of training samples. The PGCN was optimized by iteratively minimizing L c and L. The procedure used to train the PGCN is presented in Algorithm 1. Note that the coarse-grained head only plays an auxiliary role, which is not trained in the same proportion as the fine-grained head. In addition, the coarse-grained head does not affect the weight the update of the fine-grained head. Therefore, the optimization processes of the two heads were isolated from each other.\n\nAlgorithm 1 Procedure of the PGCN model. Conduct fine-grained dynamic graph convolution and static graph convolution; then concatenate them:\n\nConduct coarse-grained dynamic graph convolution and static graph convolution; then concatenate them:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "8:",
      "text": "Make prediction of coarse-grained head from H c , and calculate the loss of coarse-grained head;\n\n9:\n\nConcatenate the features from coarse-grained head and fine-grained head,\n\n10:\n\nMake prediction of fine-grained head from Ĥ, and calculate the loss of fine-grained head with gradient descent;\n\n11:\n\nUpdate parameters in the fine-grained head;\n\n12:\n\nif i % steps == 0 do then 13:\n\nUpdate parameters in the coarse-grained head with gradient descent;\n\n14:\n\nend if 15: end for IV. EXPERIMENTS A. Experimental Ssettings 1) Datasets: SEED-IV  [39]  contains 15 subjects in total. These subjects were asked to view 72 movie clips in a comfortable environment. These movie clips easily trigger four emotions: happiness, sadness, fear, and neutrality. Each person watched 24 short movie clips during 3 different sessions. The dataset contains EEG and eye movement data, among which the EEG data are collected using a 62-channel ESI neuroscan system 1 . The locations of the EEG electrodes are based on the international 10-20 system. We divide each session into 1-s segments, and each segment is a training sample. We extract the differential entropy (DE) features at five frequency bands of all training samples.\n\nThe MPED  [40]  collects four modal physiological signals: EEG, galvanic skin response, respiration, and electrocardiogram (ECG). The dataset comprises 30 subjects, each of whom watched 28 videos. The 28 videos were divided into 7 categories: joy, funny, anger, fear, disgust, sadness, and neutral emotion. Herein, we only used the EEG data for emotion recognition. Similarly, we extracted the short-time Fourier transform (STFT) features at five frequency bands from the EEG data.\n\nFor the PGCN model, because the coarse-grained head extracts the commonness of the EEG data distribution characteristics from positive, negative, and neutral emotions, we need to relabel each EEG sample by referring to the coarse-grained category. Herein, we consider the coarse-grained correlation of emotions, the characteristic of discrete emotions, and the response of the emotion expression activity in the brain, to relabel the coarse-grained labels according to  [13] -  [15] . Specifically, for SEED-IV, sadness and fear are regarded as negative emotions; happiness is regarded as a positive emotion; and neutral emotion remains unchanged. For the MPED, joy and funny are regarded as positive emotions; anger, fear, disgust, and sadness are regarded as negative emotions; and neutral emotion is unchanged.\n\n2) Experiment protocol: There are two common experimental protocols in EEG emotion recognition tasks: subjectdependent and subject-independent. For the subject-dependent experiment, we adopted the same experimental protocol as in  [39]  and  [40] . Specifically, for SEED-IV, the first 16 trials per session of each subject were used as the training data, and the remaining 8 trials were regarded as the test data. For the MPED, the first 21 trials of EEG data were adopted as the training data, and the remaining 7 trails were used as testing data for each subject. For the subject-independent experiment, we adopted the leave-one-subject-out (LOSO) cross-validation strategy  [41]  to evaluate the proposed PGCN model. In the LOSO strategy, the EEG signals of one subject are regarded as testing data, and those of the remaining subjects are allocated as training data. We repeated this procedure for each subject until all EEG signals were treated as the test data once.\n\n3) Implementation details: For the PGCN model, the number of EEG channels n in the datasets was 62. The number of frequency bands d was 5. The order of the graph convolution K was set to 5. The output dimensions of the dynamic and static graph convolution were set to 512 and 256, respectively. In addition, the learning rate was set to 10 -4 during the training process. Finally, the entire model was implemented on TensorFlow 2 .\n\n1 https://compumedicsneuroscan.com/ 2 https://tensorflow.google.cn/",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Experiment Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "1) Experimental Results For Eeg Emotion Recognition:",
      "text": "To evaluate the classification superiority of the PGCN, we conducted extensive experiments on SEED-IV and the MPED. To compare the PGCN with other representative methods in previous studies on emotion recognition, we also conducted the same experiments using seven other methods: linear SVM  [42] , A-LSTM  [40] , DANN  [43] , DGCNN  [44] , IAG  [35] , BiDANN  [31] , and BiHDM  [32] . We directly quote (or reproduce) their results from the literature to ensure a convincing comparison with the proposed method. We used mean accuracy (ACC) and standard deviation (STD) as the evaluation criteria for all subjects in the dataset. The experiment results are presented in Table  I . Table  I  demonstrates that the proposed PGCN model achieves the best performance across all tasks, both subjectdependent and subject-independent, thus verifying the effectiveness of the PGCN. In particular, for the results of the subject-dependent experiments, the proposed method improves the performance of the state-of-art methods BiHDM and IAG by 3%. Note that the BiHDM and BiDANN use unlabeled test data for training, which improves EEG emotion recognition by narrowing the domain gap between the training and test data. However, in real applications, it is inconvenient to collect test data, even without emotional labels. For the PGCN, it was unnecessary to access any information on the test data during the training process. Despite this, the PGCN achieves better performance than the BiHDM, which further verifies the effectiveness and applicability of our model. Additionally, we can see that the compared IAG method, which also considers the dynamic connection relationships between the EEG electrodes, achieves the overall closest performance to the proposed PGCN. As the main difference between the IAG and PGCN methods, the former considers presenting different graphic representations determined by different input instances, and employs an additional branch to characterize the intrinsic dynamic relationships between different EEG channels. By contrast, the latter (PGCN) focuses on the dynamic functional connectivity of the brain regions from the input instances and the static graphic representation based on the spatial proximity of the EEG electrode distribution, and employs a progressive learning strategy to extract the hierarchical characteristic of emotion. The results of both the IAG and PGCN approaches indicate the importance of considering the dynamic functional connectivity of the brain regions for EEG emotion recognition. Meanwhile, the better performance of the PGCN is attributed to the spatial proximity of the EEG electrode distribution when modeling the EEG emotional signals and exploiting the hierarchical characteristic of emotion when extracting the deep discriminative features.\n\nFor subject-independent experiments, the distribution gap is much larger than that in the subject-dependent experiments. In this case, some methods employ a domain adaptation strategy to achieve a more promising performance. The state-of-art Bi-HDM method adopts the domain adaptation technique to train the model with labeled training data and unlabeled test data. Nevertheless, PGCN still achieves the highest classification results, without using the domain adaptation strategy, and is more suitable for real applications.\n\nSpecifically, the SVM only linearly classifies the input features. The graph-based DGCNN and IAG methods consider the dynamic functional connection relationship of the brain regions, but ignore the importance of the physical proximity of the brain in space. The PGCN progressively learns discriminative EEG features, from coarse-to fine-grained features, and then, considers the functional connectivity information and spatial distance information of the brain. Compared with the domain adaptation methods, BiHDM and BiDANN, the PGCN achieves better performance through its reasonable graph representation and model construction, without using unlabeled test data. In summary, the PGCN achieves the best performance for all tasks. These results verify the effectiveness of the dual-head structure and dual-graph construction.\n\n2) Confusion matrix based on PGCN results on two datasets: To explore which emotion is more easily recognized by the proposed model, we depict confusion matrices based on the results of the PGCN on SEED-IV and the MPED, which are shown in Figs.  3  and 4 , respectively. We made the following two observations. (1) For SEED-IV in Fig.  3 , happiness is easier to distinguish between subject-dependent and subject-independent experiments, indicating that happiness is more easily induced. Moreover, compared with the results of Fig.  3  (a), we can observe that the recognition rate of fear decreases the least in Fig.  3  (b), indicating that fear is associated with a more similar brain reflection across different people than other emotions. In the subjectindependent experiment in Fig.  3  (b), fear and sadness were more likely to be confused. The rate of sadness being mistakenly recognized as fear or fear being mistaken as sadness is as high as 20%. This indicates that fear and sadness, which are both negative emotions, have a certain degree of commonality, which is consistent with our research perspective. (2) For the MPED, there are two types of positive emotions and four types of negative emotions, which is much more complicated than SEED-IV. From the subject-dependent result in Fig.  4  (a), we can see that neutrality, funny, anger, and sadness are easier to distinguish, which has a large gap in the appearance of emotions. Meanwhile, we can observe that joy is more easily confused with funny, which indicates that these two types of emotions have a greater degree of similarity. In addition, funny and happiness may present progressive emotional relation- ships. When funny is generated, people cannot help but generate joy, making it extremely easy to confuse these two emotions. Among the four negative emotions, anger and fear are more likely to be mistaken for each other.\n\nNegative emotions, except anger, will more or less trigger anger in the subjects. The above observations verify the necessity of studying the hierarchical characteristics of emotions. In the subject-independent experiment on the MPED (Fig.  4  (b)), neutrality, funny, and fear with large gaps in the emotional response have a higher prediction reliability, which reflects the small individual differences corresponding to these three types of emotions. The recognition of fear achieves the best performance, which is the same as that observed in SEED-IV. Fear is associated with a more similar brain reflection in different people than other emotions.   II . The ACC of the PGCN-f decreases significantly compared with that of the PGCN, demonstrating that the dual-head structure of the PGCN is helpful for improving EEG emotion recognition. Furthermore, we can observe that the performance degradation is more noticeable on the MPED with seven emotions, confirming that the proposed dual-head PGCN model is more suitable for fine-grained classification problems with more apparent hierarchical characteristics. Meanwhile, the PGCN-f achieves a performance similar to that of the stateof-art BiHDM method for subject-dependent experiments on SEED-IV, indicating that the construction of graphs is highly effective in the modeling of EEG signals.\n\n2) Effect of graph constructions: In PGCN, we construct a dual-graph module to model the functional and spatial relationships of the brain regions. To measure the contribution of the different graph constructions, we modify the PGCN by ablating its static and dynamic graph convolution, which are abbreviated as PGCN-d and PGCN-s, respectively. Table  III  shows the subject-dependent and subject-independent experiment results of the two decomposed models on SEED-IV and the MPED.\n\nTable  III  shows that the PGCN-d and PGCN-s have different degrees of decline compared with the complete PGCN. This shows the importance of the spatial proximity of the brain and functional connections of the brain regions. The combination of static and dynamic graphs further enhances the PGCN model for EEG emotion recognition. Additionally, the PGCN-d achieves better performance than the PGCN-s, which validates the importance of the functional connectivity-based dynamic graph. We also found that the recognition rate of the PGCN-s decreased more than that of the PGCN-d; however, the STD was much smaller, indicating that the static graph is less sensitive to the differences between individuals.\n\n3) Influence of different frequency bands on EEG emotion recognition: Some studies have proven that there is a correlation between emotion and EEG frequency  [45] . To explore the influence of frequency bands on EEG emotion recognition, we conducted additional experiments based on the PGCN with different frequency bands. The classification results are shown in Table  IV . The experiment results indicate that the classification ACC of high-frequency EEG data, e.g., for the β and γ bands, is higher. This result is consistent with the conclusion in  [45] , thereby confirming that changes in human emotion are related to the high-frequency band. We can see that the recognition rate of the δ band in the subject-independent experiment on SEED-IV was the highest among the five bands considered. We speculate that the human brain contains more basic information within the low-frequency band; it is necessary in a subject-independent experiment to explore the commonalities between individuals.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "4) T-Sne Visualization Of Pgcn:",
      "text": "To further exploit the training process of the PGCN, we employ T-SNE technology  [46]  to visualize the training process of the proposed model. As shown in Fig.  5 , we visualized the EEG data of the first five subjects, from the initial to trained state, based on the MPED. Fig.  5  (1) shows that the seven types of emotions were mixed and difficult to distinguish during the initial state. After training, the same types of emotions gather, as shown in Fig.  5  (2). We found that the points representing the emotional EEG signals are easier to distinguish visually than before, which validates the effectiveness of our model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "5) Functional Connections For Emotion Expression In The Pgcn:",
      "text": "To clearly illustrate the contribution of the dynamic graph in extracting the functional connections of the brain regions, we depict the electrode activity maps from five frequency bands in Fig.  6 . Their contribution is evaluated by visualizing the fine-grained dynamic graph G d f , which is predicted from the input samples and distributed on the scalp in the form of electrode nodes. As shown in Fig.  6 , the distributions of the dynamic graphs within different frequency bands are extremely different. The more significant part concentrates on the high-frequency bands, which is confirmed by the comparative experiment discussed in Section IV-C3. Moreover, Fig.  6  (e) shows that EEG data from the frontal and temporal lobes provide more significant contributions. This observation is consistent with the cognition observation in biological psychology  [47] ,  [48] ; namely, the frontal and temporal lobes are the main areas of the brain that are correlated with emotional activity  [49] . Moreover, a part of the parietal lobe plays an important role in emotion recognition, which may arise from thalamus activity  [50] . These observations further confirm that the PGCN concentrates more on the brain regions that have a high correlation with emotions, which can help us explore the relationships between the functional connectivity of the brain regions and different emotions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this study, we propose a PGCN to capture the hierarchical characteristics of EEG emotional signals and progressively learn discriminative EEG features. The designed dual graphs used in the PGCN characterize the intrinsic relationships between different EEG channels, which contain the dynamic functional connections and static spatial proximity information of brain regions from neuroscience research. The followed dual-head module enables the PGCN to progressively learn more discriminative EEG features, from coarse-(easy) to finegrained categories (difficult), by referring to the hierarchical characteristic of the emotion. Extensive experiments on two public EEG emotion datasets demonstrate that the proposed PGCN method achieves state-of-art performance. The better recognition performance of the PGCN is attributed to the fact that it can model the EEG signals based on the dynamic functional connections and static spatial proximity of the brain regions, while using the hierarchical characteristic of emotion to progressively capture the discriminative features. Using the PGCN, we also investigate the contribution of the dualhead module and dual-graph for EEG emotion recognition, and explore the relationships between frequency bands and emotion. In a future study, we will further investigate the hierarchical information of emotion and the dynamic functional connectivity of the brain regions.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of raw EEG data on multi-modal phys-",
      "page": 1
    },
    {
      "caption": "Figure 1: Notably, the boundaries between",
      "page": 2
    },
    {
      "caption": "Figure 1: (a)) are clearer than those",
      "page": 2
    },
    {
      "caption": "Figure 1: (b)), which reﬂects",
      "page": 2
    },
    {
      "caption": "Figure 2: The network aims to capture the",
      "page": 3
    },
    {
      "caption": "Figure 2: Framework of PGCN. The coarse-grained head is employed to extract the commonalities of emotional patterns, and",
      "page": 4
    },
    {
      "caption": "Figure 3: , happiness is easier to distinguish",
      "page": 7
    },
    {
      "caption": "Figure 3: (a), we can observe that the recognition rate of fear",
      "page": 7
    },
    {
      "caption": "Figure 3: (b), indicating that fear is",
      "page": 7
    },
    {
      "caption": "Figure 3: (b), fear and sadness",
      "page": 7
    },
    {
      "caption": "Figure 4: (a), we can see that neutrality, funny,",
      "page": 7
    },
    {
      "caption": "Figure 3: Confusion matrix of PGCN results on SEED-IV.",
      "page": 7
    },
    {
      "caption": "Figure 4: (b)), neutrality, funny, and fear with large",
      "page": 7
    },
    {
      "caption": "Figure 4: Confusion matrix of PGCN results on MPED.",
      "page": 8
    },
    {
      "caption": "Figure 5: T-SNE visualization of features from PGCN in subject-dependent experiment on MPED. The points marked as 0∼6",
      "page": 9
    },
    {
      "caption": "Figure 5: , we visualized the EEG data of the",
      "page": 9
    },
    {
      "caption": "Figure 5: (1) shows that the seven types of emotions were",
      "page": 9
    },
    {
      "caption": "Figure 5: (2). We found that the points representing the emotional EEG",
      "page": 9
    },
    {
      "caption": "Figure 6: Their contribution is evaluated",
      "page": 9
    },
    {
      "caption": "Figure 6: (e) shows that EEG data from the frontal and",
      "page": 9
    },
    {
      "caption": "Figure 6: Visualization of Gd",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "ACC / STD (%)": "SEED-IV"
        },
        {
          "Method": "",
          "ACC / STD (%)": "subject-dependent"
        },
        {
          "Method": "SVM [42]",
          "ACC / STD (%)": "56.61/20.05"
        },
        {
          "Method": "A-LSTM [40]",
          "ACC / STD (%)": "69.50/15.65"
        },
        {
          "Method": "DANN [43]",
          "ACC / STD (%)": "63.07/12.66"
        },
        {
          "Method": "DGCNN [44]",
          "ACC / STD (%)": "69.88/16.29"
        },
        {
          "Method": "IAG [35]",
          "ACC / STD (%)": "73.27/16.15"
        },
        {
          "Method": "BiDANN* [31]",
          "ACC / STD (%)": "70.29/12.63"
        },
        {
          "Method": "BiHDM* [32]",
          "ACC / STD (%)": "74.35/14.09"
        },
        {
          "Method": "PGCN",
          "ACC / STD (%)": "77.08/12.43"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "ACC / STD (%)": "SEED-IV\nMPED"
        },
        {
          "Method": "",
          "ACC / STD (%)": "subject-dependent\nsubject-independent\nsubject-dependent\nsubject-independent"
        },
        {
          "Method": "PGCN",
          "ACC / STD (%)": "77.08/12.43\n69.44/10.16\n43.56/08.21\n27.88/04.79"
        },
        {
          "Method": "PGCN-d",
          "ACC / STD (%)": "68.51/15.36\n55.90/12.28\n34.74/09.26\n26.05/04.81"
        },
        {
          "Method": "PGCN-s",
          "ACC / STD (%)": "60.28/03.82\n48.84/02.80\n30.88/02.86\n22.345/00.86"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Frequency bands (Hz)": "",
          "ACC / STD (%)": "SEED-IV"
        },
        {
          "Frequency bands (Hz)": "",
          "ACC / STD (%)": "subject-dependent\nsubject-independent"
        },
        {
          "Frequency bands (Hz)": "δ (1–5)",
          "ACC / STD (%)": "52.54/12.05\n56.78/12.85"
        },
        {
          "Frequency bands (Hz)": "θ (4–8)",
          "ACC / STD (%)": "55.17/16.91\n46.27/08.54"
        },
        {
          "Frequency bands (Hz)": "α (8–14)",
          "ACC / STD (%)": "60.18/16.33\n45.93/10.12"
        },
        {
          "Frequency bands (Hz)": "β (14–30)",
          "ACC / STD (%)": "68.17/16.21\n48.44/10.10"
        },
        {
          "Frequency bands (Hz)": "γ (30–50)",
          "ACC / STD (%)": "62.47/17.67\n48.21/07.25"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion as an argumentation engine: Modeling the role of emotion in negotiation",
      "authors": [
        "B Martinovski",
        "W Mao"
      ],
      "year": "2009",
      "venue": "Group Decision & Negotiation"
    },
    {
      "citation_id": "2",
      "title": "Once more, with feeling: Reconsidering the role of emotion in work",
      "authors": [
        "V Waldron"
      ],
      "year": "1994",
      "venue": "Annals of the International Communication Association",
      "doi": "10.1080/23808985.1994.11678894"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O Kwon"
      ],
      "year": "2003",
      "venue": "Proc European Conference on Speech Communication & Technology"
    },
    {
      "citation_id": "5",
      "title": "Hidden markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Intelligent facial emotion recognition based on stationary wavelet entropy and jaya algorithm",
      "authors": [
        "S.-H Wang",
        "P Phillips",
        "Z.-C Dong",
        "Y.-D Zhang"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "7",
      "title": "Variational instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "Y Li",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "9",
      "title": "A review on eeg signals based emotion recognition",
      "authors": [
        "M Soroush",
        "K Maghooli",
        "S Setarehdan",
        "A Nasrabadi"
      ],
      "year": "2017",
      "venue": "International Clinical Neuroscience Journal"
    },
    {
      "citation_id": "10",
      "title": "Early cortical processing of vection-inducing visual stimulation as measured by eventrelated brain potentials (erp)",
      "authors": [
        "S Berti",
        "B Haycock",
        "J Adler",
        "B Keshavarz"
      ],
      "year": "2019",
      "venue": "Displays"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition based on eeg using lstm recurrent neural network",
      "authors": [
        "S Alhagry",
        "A Fahmy",
        "R El-Khoribi"
      ],
      "year": "2017",
      "venue": "Emotion"
    },
    {
      "citation_id": "12",
      "title": "Sst-emotionnet: Spatial-spectral-temporal based attention 3d dense network for eeg emotion recognition",
      "authors": [
        "Z Jia",
        "Y Lin",
        "X Cai",
        "H Chen",
        "H Gou",
        "J Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Cped: A chinese positive emotion database for emotion elicitation and analysis",
      "authors": [
        "Y Zhang",
        "G Zhao",
        "Y Ge",
        "Y Shu",
        "X Sun"
      ],
      "year": "2020",
      "venue": "Cped: A chinese positive emotion database for emotion elicitation and analysis"
    },
    {
      "citation_id": "14",
      "title": "What is shared, what is different? core relational themes and expressive displays of eight positive emotions",
      "authors": [
        "B Campos",
        "M Shiota",
        "D Keltner",
        "G Gonzaga",
        "J Goetz"
      ],
      "year": "2013",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699931.2012.683852"
    },
    {
      "citation_id": "15",
      "title": "Decoding the nature of emotion in the brain",
      "authors": [
        "P Kragel",
        "K Labar"
      ],
      "year": "2016",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "16",
      "title": "A novel convolutional neural networks for emotion recognition based on eeg signal",
      "authors": [
        "Z Wen",
        "R Xu",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)"
    },
    {
      "citation_id": "17",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "18",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Phase-locking value based graph convolutional neural networks for emotion recognition",
      "authors": [
        "Z Wang",
        "Y Tong",
        "X Heng"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Functional grouping and cortical-subcortical interactions in emotion: A meta-analysis of neuroimaging studies",
      "authors": [
        "H Kober",
        "L Barrett",
        "J Joseph",
        "E Bliss-Moreau",
        "K Lindquist",
        "T Wager"
      ],
      "year": "2008",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "21",
      "title": "The structural and functional connectivity of the amygdala: From normal emotion to pathological anxiety",
      "authors": [
        "M Kim",
        "R Loucks",
        "A Palmer",
        "A Brown",
        "K Solomon",
        "A Marchante",
        "P Whalen"
      ],
      "year": "2011",
      "venue": "Behavioural Brain Research"
    },
    {
      "citation_id": "22",
      "title": "Neurophysiological Architecture of Functional Magnetic Resonance Images of Human Brain",
      "authors": [
        "R Salvador",
        "J Suckling",
        "M Coleman",
        "J Pickard",
        "D Menon",
        "E Bullmore"
      ],
      "year": "2005",
      "venue": "Cerebral Cortex",
      "doi": "10.1093/cercor/bhi016"
    },
    {
      "citation_id": "23",
      "title": "Eeg-based emotion recognition during watching movies",
      "authors": [
        "D Nie",
        "X Wang",
        "L Shi",
        "B Lu"
      ],
      "year": "2011",
      "venue": "International IEEE/EMBS Conference on Neural Engineering"
    },
    {
      "citation_id": "24",
      "title": "Eegbased emotion recognition in music listening",
      "authors": [
        "Y Lin",
        "C Wang",
        "T Jung",
        "T Wu",
        "S Jeng",
        "J Duann",
        "J Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "25",
      "title": "Cross-subject emotion recognition using flexible analytic wavelet transform from eeg signals",
      "authors": [
        "V Gupta",
        "M Chopda",
        "R Pachori"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "26",
      "title": "A hierarchical graph network for 3d object detection on point clouds",
      "authors": [
        "J Chen",
        "B Lei",
        "Q Song",
        "H Ying",
        "D Chen",
        "J Wu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "27",
      "title": "Learning the redundancy-free features for generalized zero-shot object recognition",
      "authors": [
        "Z Han",
        "Z Fu",
        "J Yang"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "28",
      "title": "Conformer: Convolutionaugmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu",
        "R Pang"
      ],
      "year": "2020",
      "venue": "Conformer: Convolutionaugmented transformer for speech recognition"
    },
    {
      "citation_id": "29",
      "title": "Sae+lstm: A new framework for emotion recognition from multi-channel eeg",
      "authors": [
        "X Xing",
        "Z Li",
        "T Xu",
        "L Shu",
        "B Hu",
        "X Xu"
      ],
      "year": "2019",
      "venue": "Frontiers in Neurorobotics",
      "doi": "10.3389/fnbot.2019.00037"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition from multiband eeg signals using capsnet",
      "authors": [
        "H Chao",
        "L Dong",
        "Y Liu",
        "B Lu"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "31",
      "title": "A bihemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "33",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks and broad learning system",
      "authors": [
        "X Wang",
        "T Zhang",
        "X Xu",
        "L Chen",
        "X Xing",
        "C Chen"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "34",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Instance-adaptive graph for eeg emotion recognition"
    },
    {
      "citation_id": "36",
      "title": "Eeg emotion recognition using fusion model of graph convolutional neural networks and lstm",
      "authors": [
        "Y Yin",
        "X Zheng",
        "B Hu",
        "Y Zhang",
        "X Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "37",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2017",
      "venue": "Semi-supervised classification with graph convolutional networks"
    },
    {
      "citation_id": "39",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "40",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "41",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "IJCAI"
    },
    {
      "citation_id": "42",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "43",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "44",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W Zheng",
        "B Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "46",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "47",
      "title": "Frontal eeg asymmetry as a moderator and mediator of emotion",
      "authors": [
        "J Coan",
        "J Allen"
      ],
      "year": "2004",
      "venue": "frontal EEG Asymmetry, Emotion, and Psychopathology"
    },
    {
      "citation_id": "48",
      "title": "The amygdala: vigilance and emotion",
      "authors": [
        "M Davis",
        "P Whalen"
      ],
      "year": "2001",
      "venue": "Molecular psychiatry"
    },
    {
      "citation_id": "49",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcão",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Neural correlates of social and nonsocial emotions: An fmri study",
      "authors": [
        "J Britton",
        "K Phan",
        "S Taylor",
        "R Welsh",
        "K Berridge",
        "I Liberzon"
      ],
      "year": "2006",
      "venue": "NeuroImage"
    }
  ]
}