{
  "paper_id": "2207.01113v2",
  "title": "Are 3D Face Shapes Expressive Enough For Recognising Continuous Emotions And Action Unit Intensities?",
  "published": "2022-07-03T20:19:06Z",
  "authors": [
    "Mani Kumar Tellamekala",
    "Ömer Sümer",
    "Björn W. Schuller",
    "Elisabeth André",
    "Timo Giesbrecht",
    "Michel Valstar"
  ],
  "keywords": [
    "Facial Expression Analysis",
    "Dimensional Affect Recognition",
    "Action Unit Intensity Estimation",
    "3D Morphable Models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognising continuous emotions and action unit (AU) intensities from face videos, requires a spatial and temporal understanding of expression dynamics. Existing works primarily rely on 2D face appearance features to extract such dynamics. This work focuses on a promising alternative based on parametric 3D face alignment models, which disentangle different factors of variation, including expression-induced shape variations. We aim to understand how expressive 3D face shapes are in estimating valence-arousal and AU intensities compared to the state-of-the-art 2D appearance-based models. We benchmark five recent 3D face models: ExpNet, 3DDFA-V2, RingNet, DECA, and EMOCA. In valence-arousal estimation, expression features of 3D face models consistently surpassed previous works and yielded an average concordance correlation of .745 and .574 on SEWA and AVEC 2019 CES corpora, respectively. We also study how 3D face shapes performed on AU intensity estimation on BP4D and DISFA datasets, and report that 3D face features were on par with 2D appearance features in recognising AUs 4, 6, 10, 12, and 25, but not the entire set of AUs. To understand this discrepancy, we conduct a correspondence analysis between valence-arousal and AUs, which points out that accurate prediction of valence-arousal may require the knowledge of only a few AUs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "F ACIAL expressions are important social signals pro- duced through coordinated movements of facial muscles along spatio-temporal dimensions. Automatic recognition of facial expressions from video data is a fundamental task in Affective Computing with a wide range of applications, including but not limited to psychotherapy and well-being  [1] , educational analytics  [2] , naturalistic human-computer  [3] , and human-robot interactions  [4] . The problem of automated facial expressive behaviour analysis has been extensively studied in the last two decades  [5] ,  [6] ,  [7] ,  [8] . The two most common video-based facial expression analysis approaches are based on Russell's circumplex model of dimensional emotions  [9]  and Facial Action Coding System (FACS)  [10] . The circumplex model represents emotions in a continuous space composed of two orthogonal axes, namely valence and arousal dimensions. In contrast, FACS encodes the movements of different facial muscle groups by defining the occurrence and intensity values of their corresponding Action Units (AUs).\n\nA common challenge encountered in video-based facial Fig.  1 : Recognising continuous dimensional emotions and facial action unit intensities from the temporal dynamics of of 3D Morphable Models' expression coefficients expression analysis in naturalistic conditions is to disentangle expression-induced facial variations from a wide range of other factors of variation in a given 2D face image sequence. Expression-irrelevant facial variations typically include head pose changes, facial geometry that contains identity information, or other fine-scale details such as identity-specific face wrinkles, etc. In the era of deep representation learning, most state-of-the-art methods depend on end-to-end learning from 2D face appearances. Such 2D appearance-based expression features achieved impressive performance on both valence-arousal and AU intensity estimation tasks  [11] ,  [12] ,  [13] . However, they heavily rely on the manual annotations of emotions or AU intensities for vast amounts of visual data to extract facial expression features and their temporal dynamics. In contrast, analysis-by-synthesis methods such as 3D Morphable Models (3DMM)  [14]  of faces offer an interesting alternative to distil expression-induced facial shape variations in a more principled approach. Such analysis-by-synthesis methods, most importantly, do not need the labels of emotions or AUs to extract expression features from 2D face images. Several parametric 3D face alignment models  [15] ,  [16] ,  [17] ,  [18] ,  [19]  based on 3DMM formulation achieved significant improvements in recent years by leveraging advancements in data-driven representation learning. Some recent works on 3D face alignment even attempted to reconstruct facial expressions with high fidelity  [18] ,  [19] ,  [20] . Despite such improvements in 3D face alignment methods, the idea of utilising 3DMM expression information for video-based facial expression analysis received limited attention compared to 2D appearance-based approaches. On that regard, we pose the following questions in this work:\n\n• Are 3D face shapes expressive enough to estimate AU intensities as well as dimensional emotions (valence-arousal) from face video data?\n\n• Where do 3D face shape expression features stand w.r.t. 2D face appearance features that are directly learned for estimating AU intensities and emotions in an end-to-end fashion?\n\nTo answer these questions, as Fig.  1  illustrates, we train AU intensity estimation and dimensional emotion recognition models based on the temporal dynamics of 3D facial expressions. We extensively evaluate the quality of 3DMM based expression features on the datasets of valence-arousal estimation (SEWA  [21] , AVEC 2019 CES  [7] ) and AU intensity estimation (BP4D  [22]  and DISFA  [23] ). We apply a simple bi-directional Gated Recurrent Unit (GRU) network to model the temporal dynamics of 3DMM expression features extracted from five dense 3D face alignment models: ExpNet  [20] , 3DDFA-V2  [16] , RingNet  [17] , DECA  [18] , and EMOCA  [19] . We compare the recognition performance of different 3D face shape models with the 2D face appearance baselines and models that currently have state-of-the-art performance on both tasks.\n\nOur experimental analysis shows that in the case of continuous emotion recognition 3D face expression features outperform the existing benchmarks as well as the 2D appearance baselines evaluated in this work. However, on the task of AU intensity prediction 3D face shape models perform poorly compared to the existing state-of-the-art benchmarks based on 2D appearance features. Further, we conduct a correspondence analysis between different AUs and valence-arousal dimensions to explain the performance discrepancy of 3D face models between emotion recognition and AU intensity prediction tasks. Thus, this work comprehensively illustrates the current state of the 3D face shape expression features in terms of their ability to model continuous facial expression dynamics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Related Work",
      "text": "As the main focus of this work is the analysis of 3D face shape models for facial expression analysis, we review the literature on 3D morphable models of faces and expression analysis tasks tackled by using 3D face features. Here, our particular interest is facial geometry-aware approaches in two video-based facial expression analysis tasks: valencearousal estimation and AU intensity estimation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "3D Morphable Models Of Faces",
      "text": "Estimating 3D shape models from 2D measurements is a fundamental problem in computer vision. Mainly focusing on face analysis, Blanz and Vetter  [14]  initially addressed this problem and proposed a 3D Morphable Model (3DMM) to generate 3D face shape and appearance. 3DMM can be considered a representation of facial shape and texture, disentangling them from other factors of variation. Fundamentally, 3DMM is a statistical model learned from dense one-to-one correspondences of representative 3D shapes and 2D appearance data. The original work performed face registration from unregistered 3D scenes using gradientbased optical flow and created a 3D face model, learning an optimisation problem to synthesise 2D appearance from a linear combination of 3D shapes using PCA decomposition. We refer the interested readers to  [24]  for a detailed review of 3D morphable face models.\n\nThe main factors of variation in a 3DMM are shape and texture. The original formulation can be given as follows:\n\nwhere S is a 3D face, S is the mean 3D shape, A shape , A texture are shape and texture bases, and α shape , α texture are their parameters. After the 3D face is reconstructed with this model, it is projected back to the image plane using a scale orthographic model:\n\nwhere V 2d (p) generates the 2D locations of 3D model vertices. The head pose information comes from the scale factor f , orthographic projection matrix, Pr and rotation matrix, R. α s contains the identity-related and expression-induced shape information, whereas texture details captured by α t correspond to variations that may also contain expression information. However, the original and many following 3DMM approaches focused on reconstructing 3D face shapes per identity and neglected expression variations specifically in their optimisation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modelling Facial Expressions In 3Dmms.",
      "text": "Looking into the methods that incorporate facial expression information in 3DMMs, 3DFFA  [15]  optimised the bases on 3D models of faces with various expressions in the FaceWarehouse database  [25]  containing 3D scans of 150 people of diverse ages and ethnic backgrounds. Subsequently, Guo et al.  [16]  improved the optimisation by computing Vertex Distance Cost (VDC) and Weighted Parameter Distance Cost (WPDC) and by using more compact backbone regressors; however, their expression modelling approach remained the same. Similar to 3DDFA  [15] , Chang et al.  [20]  proposed a landmark-free approach. They estimated 3DMM shape and pose parameters using CNN-based models. By leveraging the identity information and assuming that the shape parameters of a person's different images will remain the same, they acquired the expression deformation using Gauss-Newton optimisation. Subsequently, they extracted expression codes on large-scale face datasets and regressed them with ResNet-101 deep network architecture.\n\nRecently, Li et al. (FLAME model  [26] ) used a large amount of training data to capture intrinsic shape deformations and the deformations related to pose changes, to some extent, modelling facial muscles' activation and respective expressions. In order to decouple expressions from pose variations, they estimated pose coefficients, applied an inverse transform, and normalised pose to reduce its effect on expression parameters. Sanyal et al. (RingNet,  [17] ) learned a mapping from RGB images to 3D FLAME model parameters by adding a geometric constraint. They incorporated a shape consistency loss that encourages the same shape for the same persons' face images and different shapes for different persons and a 2D feature loss that predicts 2D facial landmarks by projecting corresponding 3D points from the FLAME template. RingNet used more face scans and additional geometric losses but lacks any explicit emotion terms in the optimization. Feng et al. (DECA,  [18] ), by building on top of the FLAME model, disentangled static and dynamic facial details in unconstrained images. After reconstructing the coarse shape, they swap person-specific details and jaw pose parameters between different images of the same person and disentangle them from expression.\n\nWe should note the initial line of work in 3DMM fitting is based on analysis-by-synthesis optimisation  [27] ,  [28] ,  [29] ,  [30] ,  [31] . Most of these approaches are sensitive to the initial conditions in their optimisation. They performed well in constrained face capture settings but are susceptible to operating conditions and image quality. Another significant difference is deep learning-based regression methods can capture shape and texture better by learning an encoder representation from large-scale labelled and unlabeled facial data. Furthermore, optimisation-based methods proposed additive and multiplicative methods (similar to Eq. 1). However, expressions are highly complex and entangled with other facial traits. Thus, nonlinear expression modelling enhances the quality of captured expressions, as in the FLAME model that combines articulated jaw and eyeballs and linear expression blend shapes. Furthermore, unlike the optimisation-based approaches, deep learningbased methods can also easily incorporate auxiliary tasks, for instance, emotion recognition in EMOCA  [19]  to improve the expression features of 3D faces.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "3D Face Models In Expression Analysis",
      "text": "This work mainly focuses on using 3D face models in videobased expression analysis tasks, namely, valence-arousal and facial action unit intensity estimation. As alternatives to the standard 2D appearance-based features used in expression analysis models (e.g.  [32] ,  [33] ,  [34] ,  [35] ), several previous works explored the use of shape features derived from 3D face models. Such commonly used shape features include landmark locations predicted in 3D face alignment models (e.g.  [36] ), their displacement vectors (e.g.  [37] ), and different parametric representations of 3D face shapes (e.g.,  [38] ,  [39] ), etc. This section presents a brief overview of the notable works that explored the application of 3D face features in recognising discrete emotions, continuous emotions, and facial action units.\n\nMost existing methods using 3D facial features for expression analysis focus on discrete emotion recognition. By tracking non-rigid deformations in 3D face surfaces, early works such as Wen and Huang  [40]  attempted to classify four discrete emotions (anger, disgust, fear, and sadness) on the selected images of CMU Cohn-Kanade expression database that is limited in terms of subjects and also environmental conditions. In later works, the focus shifted to recognising discrete emotions from 3D face scans through 3DMM parameters. 3DMM methods were initially based on optimisation and inverse graphics, and they aimed at an analysis-by-synthesis mapping between 2D and 3D observations. They mostly used expressive faces and discrete facial expression recognition to validate their 3D model fitting. However, most state-of-the-art 3DMM works use regression methods and apply deep learning on a large number of unconstrained images to gain robustness. Learning facial expressions is either a separate learning task using 3DMM representations or a loss term when optimising deep learning models to regress 3DMM parameters.\n\nBejaoui et al.  [41]  used the face mesh generated by a 3DMM model and combined it with an appearance feature, Local Binary Patterns (LBP), to represent both geometry and appearance. Subsequently, by training an SVM classifier on those features, they reported discrete emotion recognition results on the Bosphorus database  [42] . Ferrari et al.  [30]  proposed a dictionary learning-based method for 3DMM fitting and also validated the expressiveness of their approach on Extended Cohn-Kanade (CK+)  [43]  and the Facial Expression Recognition and Analysis (FERA)  [44]  for discrete emotion recognition and facial action unit detection tasks, respectively. Even though these were the first works that classified discrete emotions from the 3DMM expression parameters, they were based on analysis-by-synthesis 3DMM methods. Their performance was limited, and they were not evaluated in continuous expression tasks.\n\nThere are more recent, deep learning-based examples that used 3DMM features. For instance, Shi et al.  [45]  trained an encoder-decoder architecture to reconstruct 3DMM expression parameters (not ground truths, predictions of another 3DMM estimator) and perform facial expression classification simultaneously. Their approach achieved facial expression accuracy of 75.63% and 70.20% on CK+ and OULU-CASIA databases. Among the learning-based 3DMM methods, ExpNet  [20]  regressed 3DMM shape parameters using a deep neural network, projected 3D shape to 2D points of the image and estimated expression coefficients using standard Gauss-Newton optimisation. They subsequently used the expression coefficients and a simple kNN classifier to classify discrete facial expressions on the CK+ and the Emotion Recognition in the Wild Challenge (EmotiW-17) dataset. EMOCA  [19]  is the current state-of-the-art approach, and it also uses emotion information during training. Building on DECA  [18] , EMOCA learns to minimise a perceptual emotion consistency loss between the emotion features of input images and those of rendered ones.\n\nDeepExp3D  [39]  is another recent approach that regresses 3DMM expression parameters independent of a person's identity. They reported discrete emotion recogni-tion performance on static face image datasets like CK+, CFEE, etc, where all of them are posed and captured under controlled environments. This line of work showed enough evidence about emotion information learned by 3DMM parameters; however, a comprehensive analysis of 3D face models on real-world tasks, especially on continuous emotion estimation, is largely lacking.\n\nPei et al.  [37]  and Chen et al.  [38]  are two notable approaches among continuous emotion recognition methods based on 3D face features. Pei et al.  [37]  proposed to learn an extended 3DMM model that provides spatiotemporal features for valence-arousal estimation. Though this method showed better results than different baseline models using CNN-based appearance features, its evaluation did not include any state-of-the-art 3DMM models for comparison.\n\nIn Chen et al.  [38] , a novel random forest-based joint regression model was proposed for recovering 3D face shapes and estimating valence-arousal values from video data. On a relatively small-scale dataset (AVEC 2012  [46] ) composed of videos recorded in controlled settings, this method demonstrated promising emotion recognition performance (Pearson's correlation coefficients of 0.45 and 0.56 for valence and arousal, respectively). However, its performance benchmarking, similar to Pei et al.  [37] , did not consider other 3DMM models as baselines. Thus, it is not clear how well this method performs when compared with different 3DMM models and 2D appearance-based features on videobased valence-arousal estimation, particularly on recently developed large-scale in-the-wild emotion datasets. Though EMOCA partially addressed this problem by benchmarking different 3DMM models on continuous emotion estimation, its evaluation was done on only static face image datasets such as AffectNet  [47] . Our work aims to fill this gap by extensively benchmarking five state-of-the-art 3DMM models' valence-arousal estimation performance on two inthe-wild video datasets, SEWA  [21]  and AVEC 2019  [7] .\n\nUnlike the emotion recognition tasks, not much attention has been paid so far to AU intensity estimation from videos using 3D face models. Ferrari et al.  [30]  and Ariano et al.  [48]  studied the use of 3DMM coefficients in detecting AU occurrences in static face images. These works showed that 3DMM coefficients coupled with SVM classifiers can achieve comparable AU detection results to the models using appearance features. However, it is important to note that in the case of AU intensity estimation facial features need to be more fine-grained than the features required for AU detection. Furthermore, it is not known how well the temporal dynamics of 3DMM coefficients capture continuous-valued AU intensities in videos. Aiming to answer these questions, in this work we evaluate the AU intensity estimation performance of state-of-the-art 3DMM models on two benchmark video datasets (BP4D and DISFA).\n\nIn summary, while all the aforementioned existing works partially demonstrated the efficacy of 3D facial features in expression analysis tasks in general, it is not comprehensively understood how well different 3D facial features perform on in-the-wild video data, in comparison with 2D appearance features based on deep representation learning. To this end, we extensively evaluate five state-of-the-art 3D face models on different benchmark datasets of facial emotion and action unit analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "3D Shape Vs. 2D Appearance Features For Continuous Facial Expression Analysis",
      "text": "The face is essentially a 3D volumetric surface that undergoes rigid (e.g., head pose changes) and non-rigid (e.g., talking and raising eyebrows) deformations. Capturing such non-rigid deformations that correspond to emotional expressions along spatio-temporal dimensions is at the core of video-based facial expressive behaviour analysis. Here, our goal is to comprehensively compare and analyse the performance of standard 2D CNN-based facial appearance features learned using task-specific target labels and expression-related facial features derived from dense 3D face alignment models. To this end, we model the temporal dynamics of 3D shape-based features and 2D appearance-based features extracted from face image sequences for learning expression analysis tasks. In particular, we consider time-continuous dimensional emotion (valence-arousal) recognition and action unit (AU) intensity estimation as representative tasks for video-based facial expression analysis. In this comparison, it is worth noting that expression features in 3D face models are learned with the objective of accurate shape reconstruction, whereas 2D face appearance features are directly optimised to predict taskspecific target labels (valence-arousal or AU intensities).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Expression Embeddings From 3D Face Shapes",
      "text": "For extracting expression-specific 3D face shape features, we consider 3D Morphable Models (3DMM)  [14]  of faces, for they offer a principled approach to factorise the facial expression information. In the standard linear representation of 3DMM used in face alignment, as shown in Eq. 1, the shape component can be further decomposed as follows:\n\nwhere A id and A ex are the basis matrices of shape identity and expressions respectively, and α id and α ex are their corresponding coefficient vectors. Here, we refer to the coefficient vectors α ex as expression embeddings. Given a 2D face image as input, to extract its expression embedding from its 3D face shape, we consider five different approaches that learn 3DMM parameters: ExpNet  [20] , 3DDFA-V2  [16] , RingNet  [17] , DECA  [18] , and EMOCA  [19] . The criteria for selecting these five models are as follows: EMOCA  [19]  is the current state-of-the-art in capturing 3D facial expressions, building on the ability of DECA  [18]  formulation in modelling detailed facial expressions. DECA develops this ability by adopting a consistency loss to effectively disentangle details specific to a person from wrinkles induced by expressions. Next, we choose 3DDFA-V2  [16]  since its global shape reconstruction error is very close to that of DECA  [18] . In contrast to the aforementioned models, which use 2D facial geometry only as a regularisation constraint in minimising 3D reconstruction loss, RingNet  [17]  heavily relies on 2D landmarks for supervision by minimising a novel shape consistency loss. The last model that we evaluate here is ExpNet  [20] , which directly regresses expression coefficients inferred using 3DDFA  [49] , a predecessor to 3DDFA-V2.\n\nThough all these five models output similar expression embedding vectors (α ex ), their dimensionality varies from",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Parametric 3D Face Alignment",
      "text": "Expression Feature Embeddings",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Regression Of Valence (V) -Arousal (A) Or Au Intensity Vectors",
      "text": "Input Video",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Expression Embeddings From 2D Face Images",
      "text": "We use end-to-end learning models based on the standard CNN + RNN architectures as 2D appearance baselines. For this purpose, we adopt two strong CNN backbones that are extensively used for end-to-end facial feature learning in several recent works  [11] ,  [33] ,  [34] ,  [50] .\n\nResNet-50  [51] , particularly a version of it pre-trained on the VGG-Face database  [52] , is a commonly used CNN backbone for feature extraction from face images for emotion recognition  [32] ,  [33] ,  [53] . In implementing this backbone, we flatten the output feature maps of its last convolutional layer into 2056-dimensional vectors, which we further map to 512-dimensional features using a fully connected layer. Considering the relatively small-scale training datasets available for AU intensity estimation tasks, we use ResNet-18 model, following existing works  [12] ,  [50] ,  [54] .\n\nEmoFAN  [11] ,  [54]  is designed for facial feature extraction using only convolution layers to make the model more efficient in terms of the number of trainable parameters. A pre-trained variant of this backbone on 2D face alignment tasks is found to be very effective for transfer learning  [11] ,  [12] . To extract facial features with better generalisation capacity, we use a variant of this CNN backbone pre-trained on image-based emotion recognition using the AffectNet dataset  [47] , in addition to the 2D face alignment task. Following prior works  [11] ,  [54] , we use this backbone to extract 512-dimensional facial embedding vectors.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Temporal Dynamics Of Expression Embeddings",
      "text": "Fig.  2  illustrates the steps that we follow for video-based expression analysis tasks. Modelling temporal dynamics of frame-wise expression features in a video is critical for dimensional emotion recognition and AU intensity estimation tasks. For this purpose, we use a simple bidirectional GRU network with two hidden layers of 128 dimensions. For a fair comparison, we use the same temporal network for the expression embeddings from the 3D shape and 2D appearance models. Note that the dimensionality of the input embeddings varies across the different models. On top of the last layer of GRU block output, there is a single fully connected layer to map the per-frame hidden state vector to the final output vector of dimensional emotions or AU intensities. The output is two-dimensional in valence-arousal prediction models, whereas it differs in the number of action units in AU intensity estimation models (5-dimensional in the BP4D dataset and 12-dimensional in the DISFA dataset). Action Unit Intensity Estimation. We use two video-based AU intensity labelled datasets: DISFA  [23]  and BP4D  [22] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dimensional Emotion",
      "text": "DISFA has 27 videos of 27 subjects; each video contains approximately 4844 frames annotated with the intensity values of 12 AUs. As there are no predefined training, validation and test partitions, a subject-independent 3-fold cross-validation is a commonly used evaluation protocol for this dataset. To compare with the state-of-the-art results on DISFA, following the existing works (e.g.  [12] ,  [50] ,  [54] ), we also perform the 3-fold cross-validation; each fold containing 18 videos for training and 9 videos for evaluation.\n\nBP4D has 487 videos of 41 subjects, containing approximately 140,000 frames annotated with the intensity values of 5 AUs. It was the main corpus of the FERA 2015 challenge  [6] . We use the same training (168 videos), validation",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Dimensional Emotion Recognition performance is measured using Lin's Concordance Correlation Coefficient (CCC)  [55] , which computes the agreement between target emotion labels e * and their predicted values e o as,\n\nwhere ρ e * e o denotes the Pearson's coefficient of correlation between e * and e o , and (µ e * , µ e o ), (σ e * , σ e o ) denote their mean and standard deviation values, respectively. AU Intensity Estimation is evaluated using two standard metrics: Intra-class Correlation Coefficient (ICC) and Mean Square Error (MSE), computed for each AU individually.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Details",
      "text": "Loss Functions. To train the dimensional emotion recognition models, we use inverse-CCC + MSE loss, following the objective function originally proposed in  [56] . Whereas for the AU intensity estimation, we use the MSE alone as the loss function, similar to the existing methods  [12] ,  [54] . In both cases, the per-frame loss is accumulated over an input image sequence in computing the total loss per mini-batch. Optimisation. We use the Adam optimiser  [57]  to train all the models evaluated in this work. Note that in the 2D appearance baselines, unlike in the case of 3D face models, CNN backbones and GRU blocks are trained end-to-end.\n\nDuring training, dropout values of GRUs and FC layers are set to 0.5 and 0.25 respectively. Each mini-batch is composed of 4 sequences, with each sequence containing 100 frames. The initial learning rate value is 1e-4 and it is tuned using a cosine annealing based scheduler with warm restarts enabled  [58] . Also, L 2 regularisation is applied during model training by setting the weight decay value to 1e-4.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Task-Wise Performance Analysis",
      "text": "Dimensional Emotion Recognition on SEWA.   EMOCA even outperforms all the existing benchmarks on SEWA with improved mean CCC values in the range of +.315  [61]  to +.050  [54] . Interestingly, RingNet has a lower mean CCC score compared to ExpNet, although its 3D reconstruction error is higher than that of ExpNet (see Table  1 ). DECA and 3DDFA-V2 improve mean CCC values by +.077 and +.037 compared to the mean CCC of EmoFAN. Further, we observe that 3DDFA-V2 is on par with the SOTA method (APs  [54] ), which benefits from a more advanced temporal learning method based on stochastic context modelling.\n\nTo summarise, the above-discussed results on SEWA show that the 3D face features are expressive enough to recognise continuous emotions and they perform better than 2D appearance features used in existing state-of-the-art models. Note that in contrast to all 2D appearance-based approaches that applied transfer learning to CNN backbones, the training procedure of 3D face models does not rely on any labelled facial expression data. One exception is EMOCA, which used AffectNet  [47]  pretraining and an additional emotion recognition module in its training. Nevertheless, the remaining 3D face models that do not use any emotion labels in learning their expression parameters, perform on par or better than 2D appearance baselines.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dimensional Emotion Recognition On Avec'19 Ces.",
      "text": "Results on the AVEC'19 CES database, as shown in Table  3 , exhibit similar trends. All five 3D face models perform far above the ResNet-50+GRU baseline, except for Exp-Net; the rest also outperform a stronger 2D CNN baseline, EmoFAN+GRU. The AVEC'19 CES challenge winners, Zhao et al.  [62]  achieve a mean CCC score of +.012 above the best performing 3D face model, i.e. EMOCA. In valence estimation, unlike in the case of SEWA, the best performing method is 3DDFA-V2. Also, CCC values of ExpNet and RingNet models are in line with their 3D reconstruction errors shown in Table  1 .\n\nFigure  3  illustrates the qualitative results of EMOCA in valence-arousal estimation on some of the validation examples from the AVEC'19 corpus. Interestingly, the emotion recognition performance is slightly worse in negative valence and arousal cases compared to their positive counterparts. This could be due to the availability of fewer training examples for negative valence and low arousal quadrants  [7] ,  [21] .  4  presents the results of existing SOTA benchmarks, our in-house evaluated 2D CNNs and 3D face models, on the test set of BP4D. By comparing average ICC and MSE values achieved by the models listed in Table  4 , we can clearly notice that 3D face models have inferior performance to 2D appearancebased baselines. In contrast to the results of valence-arousal models, in estimating AU intensities all 3D face features, including EMOCA, fall behind 2D appearance features.  In this illustration, we can notice a clear correspondence between less accurate predictions made for AUs such as dimpler (AU 14) and chin raiser (AU 17) and somewhat poor 3D reconstructions of their corresponding facial regions (enclosed in yellow coloured ellipses in Figure  4 ). For instance, in the case of AU 17, the details of the chin region are poorly reconstructed in its 3D face, which could explain the poor performance of EMOCA expression features in predicting AU 17 in this example.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Au Intensity Estimation On Bp4D. Table",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cheek Raiser",
      "text": "AU Intensity Estimation on DISFA. In Table  5 , we compare the aggregated results of 3-fold cross-validation of different SOTA methods, 2D CNN baselines, and 3D face models. Similar to BP4D results, here also 3D face models perform inferior to 2D appearance-based models. Among 3D face models, ExpNet has the worst performance, and EMOCA has the best performance. Unlike in the case of BP4D, EMOCA achieves significantly better performance (ICC score of +.14 w.r.t. DECA) than the remaining 3D face models. Compared to the best performing 2D appearancebased model, APs  [54] , EMOCA has lower performance by a margin of -.15 ICC score.\n\nBased on the combined results on BP4D and DISFA datasets, we segregate all the AUs into three groups based on their best ICC values in the case of 3D face models. We observe that only five AUs  (12, 10, 25, 6)  listed in group 1 are captured well in the 3D face expressions. It seems, the subtler the AUs (e.g. AU 17 -chin raiser) are, the worse their ICC scores are with 3D face models. One possible explanation is that such subtle expression-specific 3D reconstruction errors are likely to get suppressed by the global reconstruction loss functions that are commonly used in training 3D face alignment models. Further, the size of the facial region corresponding to each AU varies widely. For example, as shown in Figure  4 , facial regions corresponding to AU10 (upper lip raiser) are wider than the areas corresponding to AU17 (chin raiser).\n\nOverall, the trends in AU estimation results clearly show that the 3D faces are still far from capturing the fine-grained facial expressions that are critical to fully understanding expressive facial behaviour. While the emotion recognition performance of all 3D shape features is considerably better than 2D appearance features, it is interesting to note their inability to recognise a wide range of action units. Towards explaining this discrepancy, we performed a correspondence analysis between AUs and emotion labels.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Correspondence Analysis: Dimensional Emotions And Action Units",
      "text": "We investigate the significance of different AUs to recognising dimensional emotions, for reconciling the above observations regarding 3D face models' results on emotion and AU estimation tasks. To this end, we perform a simple linear regression analysis -in which AU intensities are used  as input features to predict their corresponding emotion labels. By comparing the coefficient values of different AUs, we interpret the importance of each AU in predicting the target emotion labels. For this purpose, we use a recently released in-the-wild emotion recognition corpus, Aff-wild-2  [8] , in which video data is annotated with both valencearousal values and their corresponding AU occurrences.\n\nAlthough the Aff-Wild-2 dataset seems to be a more suitable candidate for the 3D face expression evaluation, it is composed of highly challenging, in-the-wild videos. As depicted in Figure  8 , the top 3 best performing 3D face models show poor reconstruction results on the Aff-wild-2 face images, except for EMOCA, which shows slightly better performance w.r.t. capturing facial expressions. Rather than pushing the limits of 3D face models to perform well in such in-the-wild conditions, our focus here is to investigate the current status of existing 3D face alignment models where they could attain acceptable shape-fitting performance. Figure  5  and Figure  6  illustrate AU-wise regression coefficients for valence and arousal respectively. From these results, we can infer that the presence of AU 2 or AU 12 or the absence of AU 4 seems to be highly critical for predicting valence. Whereas for arousal prediction, the presence of AU 1 or AU 4 or AU 25 looks important. All five 3D face models perform well (see Group 1 in Table  6 ) on at least one of the aforementioned AUs that seem critical for valence and arousal prediction. Thus, the superior emotion recognition performance of 3D face features is clearly explainable based on their relatively high ICC values for AU 4 (Table  4 ) and AU 25 (Table  5 ).\n\nTo further validate the efficacy of 3D face expression    features in recognising apparent emotions, we extend our experimental analysis to categorical emotion recognition. Below, we present a correspondence analysis between different AUs and discrete emotion classes. Refer to Appendix A for discrete emotion recognition results of 3D face features.\n\nCorrespondence Analysis between Discrete Emotions and AUs. Figure  7  presents a comparison of emotion-wise regression coefficients of different AUs labelled in the Affwild-2 dataset. Similar to the case of continuous emotions, the performance of 3D face models on discrete emotion recognition can be clearly explained by their performance on some specific AUs. For instance, the emotion class 'happy' is strongly correlated with the presence of AU 6 or AU 12, or the absence of AU 4. For this class, 3D face models show the best recognition accuracy (see Figure  10  and Figure  11 in  Appendix A ). Explaining their superior performance w.r.t. predicting the class 'happy', some of the 3D face models show good performance on AU 4 and AU 12 (see Group 1 in Table  6 ). Similarly, the class 'fear' is correlated with the presence of AU 1 or AU 2, for which the 3D face models have very poor performance (see Group 3 in Table  6 ).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Summary And Discussion",
      "text": "Based on all the above-discussed results on dimensional emotion recognition and AU intensity estimation tasks, and the correspondence analysis between apparent emotions and AU intensities, we draw the following conclusions: Except for the results of ExpNet and RingNet on SEWA, the overall performance of 3D face features on emotion and AU intensity estimation is in line with their corresponding 3D shape reconstruction errors (see Table  1 ), as reported in the NoW evaluation repository leaderboard 2 . ExpNet, RingNet, 3DDFA-V2, and DECA do not use any emotion labels, and they are not trained on transferred representations from affect-related tasks. Deviating from these models, EMOCA, the best performing 3D face model, builds on DECA and it uses a valence-arousal estimation model pretrained on AffectNet  [47] . EMOCA additionally optimises an additional loss component, perceptual emotion consistency loss between the emotion features of RGB input and another valence-arousal estimator on the DECA model's expression and detail coefficients. It is important to note that our results show even through the use of AffectNet pretrain- ing and emotion consistency, EMOCA demonstrates poor performance in capturing the facial actions corresponding to several AUs, as listed in Table  6 .\n\nFor the poor AU intensity estimation performance of the 3D face models evaluated in this work, using a global basis vector for expression modelling could also be a reason. Alternative 3DMM formulations based on sparse and localised shape models, such as the ones proposed in  [69]  and  [31] , could mitigate this problem to some extent. However, when applied to in-the-wild face image data, the generalisation performance of such sparse and locally constrained 3D face models has yet to be demonstrated in the literature. On the other hand, the benefits of current deep learning-based approaches used in this work are that they incorporated different auxiliary terms (i.e., face recognition, consistency losses, texture modelling, etc) in their optimisation and used large-scale in-the-wild 2D datasets for training.\n\nTo make 3D face shape models expressive enough to capture the complete set of facial actions, discrete or continuous emotion labels alone as additional supervision signals do not suffice. It is important to focus on collecting 3D face scans captured in conditions eliciting individual AUs and their combinations. Towards addressing this challenge, it is also important to leverage naturally available supervision cues, such as temporal coherency of facial actions in a video  [70] ,  [71] , to learn more expressive 3D face shapes in a label-efficient manner.\n\nAnother important consideration is to increase the expressiveness of 3D face models by enhancing the representation capacity of 3DMMs. To this end, increasing the dimensionality of expression coefficient vector is one possible solution. However, as mentioned in several prior 3D face alignment works (e.g.  [16] ), higher dimensional expression vectors may negatively impact the shape reconstruction loss optimisation, hence slowing down the convergence of model training.\n\nEthical Considerations and Limitations. Automated facial expression analysis, particularly in affective computing, has valuable use cases for society. For example, humancomputer interaction, learning analytics, mental health and well-being and teleconferencing are only a few of these beneficial applications for facial expression analysis. However, there exist potential use cases raising ethical questions such as surveillance and military applications.\n\nFrom the algorithmic fairness point of view, building emotion recognition based on 3D face models has advantages over CNN models that learn emotions directly from RGB images and videos. Most datasets are imbalanced in gender, ethnicity, age, and other appearance-relevant traits. Even though algorithmic bias is still a significant and open issue, learning from emotion coefficients of 3D face models discards all additional information that appearance-based CNN models jointly learn and condition on. However, 3D face models require good quality images for alignment (for instance, see the qualitative performance of all compared 3D face models in Fig.  8 ), which may limit their use cases.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "We systematically investigated the ability of 3D face models to capture expression-induced shape deformations. By evaluating the 3D face expressions on the standard emotion recognition and AU intensity corpora, we presented a detailed exposition of their current strengths and limitations compared to state-of-the-art models based on 2D face image sequences. Our key findings in this study pointed out that expression features from 3D face models can achieve stateof-the-art results on time-continuous dimensional emotion recognition by outperforming most previous works and strong 2D face appearance baselines. However, the poor performance of 3D face models in AU intensity estimation indicates that their expression features are far from describing the complete set of facial actions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Appendix A Discrete Emotion Recognition",
      "text": "In discrete emotion recognition tasks, we evaluate the top 3 best performing 3D face alignment models considered in this work: 3DDFA-v2, DECA, and EMOCA, on the CK+  [43]  and CFEE  [72]  datasets. Both these datasets are acquired in controlled lab settings. Note that here our objective is not to aim for a state-of-the-art performance but to compare the expression representations derived from the 3D face models, as an ablation study.\n\nCK+  [43]  contains 327 video clips starting from a neutral state and ending at the apex point of anger (An), contempt (Co), disgust (Di), fear (Fe), happy (Ha), sadness (Sa), and surprise (Su). We use the apex frames in our evaluation.\n\nCFEE  [73]  contains still images of 230 subjects from diverse ethnic backgrounds with 22 basic and compound emotions categories. We us all samples (1375 images) labelled with basic emotions: anger (An), disgust (Di), fear (Fe), happy (Ha), sadness (Sa), and surprise (Su).\n\nAs an ablation study to compare the expression coefficients of 3D face models, we normalise the expression coefficients according to the quantile range of the values and used a simple kNN classifier (k=5) with leave-one-out crossvalidation and report the confusion matrices and emotion recognition accuracies. Discrete Emotion Recognition. Figure  10  and Figure  11  compares class-wise performance of all four 3D face models on CFEE and CK+ datasets respectively. While all the 3D models achieved reasonably good classification accuracy, EMOCA demonstrates the best performance in terms of mean accuracy on both the datasets. The remaining 3D face models perform consistently well on the positive emotions, i.e., the happy and surprise classes, but for negative emotions (angry, sad, fear, disgust), they have relatively poor performance in most of the cases. t-SNE distributions of the 3D face expressions features illustrated in Figure  9  show similar trends.",
      "page_start": 15,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Recognising continuous dimensional emotions and",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates, we train",
      "page": 2
    },
    {
      "caption": "Figure 2: Modelling temporal dynamics of 3DMM expression",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates the steps that we follow for video-based",
      "page": 5
    },
    {
      "caption": "Figure 3: Dimensional emotion recognition results on AVEC’19 validation examples using EMOCA [19] (Ygt and Ypr denote",
      "page": 7
    },
    {
      "caption": "Figure 3: illustrates the qualitative results of EMOCA",
      "page": 7
    },
    {
      "caption": "Figure 4: Action Unit intensity estimation results on BP4D validation examples using EMOCA [19] (Ygt and Ypr denote the",
      "page": 8
    },
    {
      "caption": "Figure 4: qualitatively illustrates the",
      "page": 8
    },
    {
      "caption": "Figure 4: , facial regions",
      "page": 8
    },
    {
      "caption": "Figure 8: , the top 3 best performing 3D face",
      "page": 9
    },
    {
      "caption": "Figure 5: and Figure 6 illustrate AU-wise regression co-",
      "page": 9
    },
    {
      "caption": "Figure 5: Coefficients of a linear regression model predicting",
      "page": 9
    },
    {
      "caption": "Figure 6: Coefficients of a linear regression model predicting",
      "page": 9
    },
    {
      "caption": "Figure 7: presents a comparison of emotion-wise",
      "page": 10
    },
    {
      "caption": "Figure 10: and Figure 11 in",
      "page": 10
    },
    {
      "caption": "Figure 7: Coefficients of linear regression model predicting discrete emotions from AUs.",
      "page": 11
    },
    {
      "caption": "Figure 8: Shape fitting results of the top 3 best performing 3D",
      "page": 12
    },
    {
      "caption": "Figure 8: ), which may limit their use cases.",
      "page": 12
    },
    {
      "caption": "Figure 10: and Figure 11",
      "page": 15
    },
    {
      "caption": "Figure 9: t-SNE distributions of the samples with basic",
      "page": 15
    },
    {
      "caption": "Figure 10: Discrete Emotion Recognition Results on CFEE",
      "page": 16
    },
    {
      "caption": "Figure 11: Discrete Emotion Recognition Results on CK+",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Monocular 3D face reconstruction error (scan-",
      "page": 6
    },
    {
      "caption": "Table 2: presents the results of valence and arousal estimation on the",
      "page": 6
    },
    {
      "caption": "Table 2: Dimensional emotion recognition results on the",
      "page": 6
    },
    {
      "caption": "Table 3: Dimensional emotion recognition results on the",
      "page": 6
    },
    {
      "caption": "Table 1: Figure 3 illustrates the qualitative results of EMOCA",
      "page": 7
    },
    {
      "caption": "Table 4: presents the",
      "page": 7
    },
    {
      "caption": "Table 4: , we can clearly notice that 3D",
      "page": 7
    },
    {
      "caption": "Table 4: BP4D test set results (†denotes in-house evaluation).",
      "page": 9
    },
    {
      "caption": "Table 6: ) on at least one of",
      "page": 9
    },
    {
      "caption": "Table 5: Aggregated 3-fold cross validation results on DISFA dataset († denotes in-house evaluation).",
      "page": 10
    },
    {
      "caption": "Table 6: Segregation of AUs, from both BP4D and DISFA,",
      "page": 10
    },
    {
      "caption": "Table 6: ). Similarly, the class ’fear’ is correlated with the",
      "page": 10
    },
    {
      "caption": "Table 1: ), as reported",
      "page": 11
    },
    {
      "caption": "Table 6: For the poor AU intensity estimation performance of the",
      "page": 11
    },
    {
      "caption": "Table 3: D dense face alignment,” in ECCV, 2020, pp.",
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
      "authors": [
        "J Girard",
        "J Cohn",
        "M Mahoor",
        "S Mavadati",
        "D Rosenwald"
      ],
      "year": "2013",
      "venue": "IEEE FG Worksh"
    },
    {
      "citation_id": "2",
      "title": "Review of affective computing in education/learning: Trends and challenges",
      "authors": [
        "C.-H Wu",
        "Y.-M Huang",
        "J.-P Hwang"
      ],
      "year": "2016",
      "venue": "British Journal of Educational Technology"
    },
    {
      "citation_id": "3",
      "title": "Facial expression recognition for HCI applications",
      "authors": [
        "F Dornaika",
        "B Raducanu"
      ],
      "year": "2009",
      "venue": "Encyclopedia of Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Affective facial expressions recognition for human-robot interaction",
      "authors": [
        "D Faria",
        "M Vieira",
        "F Faria",
        "C Premebida"
      ],
      "year": "2017",
      "venue": "Affective facial expressions recognition for human-robot interaction"
    },
    {
      "citation_id": "5",
      "title": "Fully automatic facial action unit detection and temporal analysis",
      "authors": [
        "M Valstar",
        "M Pantic"
      ],
      "year": "2006",
      "venue": "CVPR Worksh"
    },
    {
      "citation_id": "6",
      "title": "FERA 2015-second facial expression recognition and analysis challenge",
      "authors": [
        "M Valstar",
        "T Almaev",
        "J Girard",
        "G Mckeown",
        "M Mehu",
        "L Yin",
        "M Pantic",
        "J Cohn"
      ],
      "year": "2015",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "7",
      "title": "AVEC 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner"
      ],
      "year": "2019",
      "venue": "AVEC"
    },
    {
      "citation_id": "8",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2001",
      "venue": "Analysing affective behavior in the first abaw 2020 competition"
    },
    {
      "citation_id": "9",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "10",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "11",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "A transfer learning approach to heatmap regression for action unit intensity estimation",
      "authors": [
        "I Ntinou",
        "E Sanchez",
        "A Bulat",
        "M Valstar",
        "Y Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "13",
      "title": "Exploiting multi-cnn features in cnnrnn based dimensional emotion recognition on the omg in-thewild dataset",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "14",
      "title": "A morphable model for the synthesis of 3D faces",
      "authors": [
        "V Blanz",
        "T Vetter"
      ],
      "year": "1999",
      "venue": "SIGGRAPH"
    },
    {
      "citation_id": "15",
      "title": "Face alignment in full pose range: A 3D total solution",
      "authors": [
        "X Zhu",
        "X Liu",
        "Z Lei",
        "S Li"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "16",
      "title": "Towards fast, accurate and stable 3D dense face alignment",
      "authors": [
        "J Guo",
        "X Zhu",
        "Y Yang",
        "F Yang",
        "Z Lei",
        "S Li"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "17",
      "title": "Learning to regress 3D face shape and expression from an image without 3D supervision",
      "authors": [
        "S Sanyal",
        "T Bolkart",
        "H Feng",
        "M Black"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "Learning an animatable detailed 3D face model from in-the-wild images",
      "authors": [
        "Y Feng",
        "H Feng",
        "M Black",
        "T Bolkart"
      ],
      "year": "2021",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "19",
      "title": "EMOCA: Emotion driven monocular face capture and animation",
      "authors": [
        "R Danecek",
        "M Black",
        "T Bolkart"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "ExpNet: Landmark-free, deep, 3D facial expressions",
      "authors": [
        "F.-J Chang",
        "A Tuan Tran",
        "T Hassner",
        "I Masi",
        "R Nevatia",
        "G Medioni"
      ],
      "year": "2018",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "21",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "22",
      "title": "BP4D-spontaneous: a high-resolution spontaneous 3D dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "23",
      "title": "DISFA: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "24",
      "title": "3D morphable face models-past, present, and future",
      "authors": [
        "B Egger",
        "W Smith",
        "A Tewari",
        "S Wuhrer",
        "M Zollhoefer",
        "T Beeler",
        "F Bernard",
        "T Bolkart",
        "A Kortylewski",
        "S Romdhani",
        "C Theobalt",
        "V Blanz",
        "T Vetter"
      ],
      "year": "2020",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "25",
      "title": "FaceWarehouse: a 3D facial expression database for visual computing",
      "authors": [
        "C Cao",
        "Y Weng",
        "S Zhou",
        "Y Tong",
        "K Zhou"
      ],
      "year": "2014",
      "venue": "IEEE Trans. on Vis. and Comput. Graphics"
    },
    {
      "citation_id": "26",
      "title": "Learning a model of facial shape and expression from 4D scans",
      "authors": [
        "T Li",
        "T Bolkart",
        "M Black",
        "H Li",
        "J Romero"
      ],
      "year": "2017",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "27",
      "title": "Face recognition based on fitting a 3D morphable model",
      "authors": [
        "V Blanz",
        "T Vetter"
      ],
      "year": "2003",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "28",
      "title": "Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior",
      "authors": [
        "S Romdhani",
        "T Vetter"
      ],
      "year": "2005",
      "venue": "CVPR"
    },
    {
      "citation_id": "29",
      "title": "Reconstructing detailed dynamic face geometry from monocular video",
      "authors": [
        "P Garrido",
        "L Valgaerts",
        "C Wu",
        "C Theobalt"
      ],
      "year": "2013",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "30",
      "title": "A dictionary learning-based 3D morphable shape model",
      "authors": [
        "C Ferrari",
        "G Lisanti",
        "S Berretti",
        "A Bimbo"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "31",
      "title": "A sparse and locally coherent morphable face model for dense semantic correspondence across heterogeneous 3D faces",
      "authors": [
        "C Ferrari",
        "S Berretti",
        "P Pala",
        "A Del Bimbo"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "32",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "CVPR Worksh"
    },
    {
      "citation_id": "33",
      "title": "Consistent generative query networks",
      "authors": [
        "A Kumar",
        "S Eslami",
        "D Rezende",
        "M Garnelo",
        "F Viola",
        "E Lockhart",
        "M Shanahan"
      ],
      "year": "2018",
      "venue": "Consistent generative query networks"
    },
    {
      "citation_id": "34",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition"
    },
    {
      "citation_id": "35",
      "title": "Joint action unit localisation and intensity estimation through heatmap regression",
      "authors": [
        "E Sánchez-Lozano",
        "G Tzimiropoulos",
        "M Valstar"
      ],
      "year": "2018",
      "venue": "Joint action unit localisation and intensity estimation through heatmap regression",
      "arxiv": "arXiv:1805.03487"
    },
    {
      "citation_id": "36",
      "title": "Deformable synthesis model for emotion recognition",
      "authors": [
        "D Fabiano",
        "S Canavan"
      ],
      "year": "2019",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "37",
      "title": "Monocular 3D facial expression features for continuous affect recognition",
      "authors": [
        "E Pei",
        "M Oveneke",
        "Y Zhao",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "38",
      "title": "3D model-based continuous emotion recognition",
      "authors": [
        "H Chen",
        "J Li",
        "F Zhang",
        "Y Li",
        "H Wang"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "39",
      "title": "Real-time facial expression recognition \"in the wild\" by disentangling 3D expression from identity",
      "authors": [
        "M Koujan",
        "L Alharbawee",
        "G Giannakakis",
        "N Pugeault",
        "A Roussos"
      ],
      "year": "2020",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "40",
      "title": "Capturing subtle facial motions in 3D face tracking",
      "authors": [
        "Z Wen",
        "T Huang"
      ],
      "year": "2003",
      "venue": "ICCV"
    },
    {
      "citation_id": "41",
      "title": "Fully automated facial expression recognition using 3D morphable model and mesh-local binary pattern",
      "authors": [
        "H Bejaoui",
        "H Ghazouani",
        "W Barhoumi"
      ],
      "year": "2017",
      "venue": "ACIVS"
    },
    {
      "citation_id": "42",
      "title": "Bosphorus database for 3d face analysis",
      "authors": [
        "A Savran",
        "N Aly Üz",
        "H Dibeklio Glu",
        "O ¸eliktutan",
        "B Ökberk",
        "B Sankur",
        "L Akarun"
      ],
      "year": "2008",
      "venue": "Bosphorus database for 3d face analysis"
    },
    {
      "citation_id": "43",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "CVPR Worksh"
    },
    {
      "citation_id": "44",
      "title": "Metaanalysis of the first facial expression recognition challenge",
      "authors": [
        "M Valstar",
        "M Mehu",
        "B Jiang",
        "M Pantic",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Syst., Man, and Cyber"
    },
    {
      "citation_id": "45",
      "title": "Pose-robust facial expression recognition by 3D morphable model learning",
      "authors": [
        "Y Shi",
        "Q Zou",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "ICCC"
    },
    {
      "citation_id": "46",
      "title": "Avec 2012: the continuous audio/visual emotion challenge",
      "authors": [
        "B Schuller",
        "M Valster",
        "F Eyben",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "ICMI"
    },
    {
      "citation_id": "47",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "48",
      "title": "Action unit detection by learning the deformation coefficients of a 3D morphable model",
      "authors": [
        "L Ariano",
        "C Ferrari",
        "S Berretti",
        "A Del Bimbo"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "49",
      "title": "Face alignment across large poses: A 3D solution",
      "authors": [
        "X Zhu",
        "Z Lei",
        "X Liu",
        "H Shi",
        "S Li"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "50",
      "title": "Modelling stochastic context of audio-visual expressive behaviour with affective processes",
      "authors": [
        "M Tellamekala",
        "T Giesbrecht",
        "M Valstar"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "51",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "52",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "BMVC"
    },
    {
      "citation_id": "53",
      "title": "Mimamo net: Integrating micro-and macro-motion for video emotion recognition",
      "authors": [
        "D Deng",
        "Z Chen",
        "Y Zhou",
        "B Shi"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "54",
      "title": "Affective processes: stochastic modelling of temporal context for emotion and facial expression recognition",
      "authors": [
        "E Sanchez",
        "M Tellamekala",
        "M Valstar",
        "G Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "55",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "56",
      "title": "Factorized higher-order cnns with an application to spatio-temporal emotion estimation",
      "authors": [
        "J Kossaifi",
        "A Toisoul",
        "A Bulat",
        "Y Panagakis",
        "T Hospedales",
        "M Pantic"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "57",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "58",
      "title": "SGDR: Stochastic gradient descent with warm restarts",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2016",
      "venue": "SGDR: Stochastic gradient descent with warm restarts"
    },
    {
      "citation_id": "59",
      "title": "Regressing robust and discriminative 3D morphable models with a very deep neural network",
      "authors": [
        "A Tuan Tran",
        "T Hassner",
        "I Masi",
        "G Medioni"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "60",
      "title": "Towards metrical reconstruction of human faces",
      "authors": [
        "W Zielonka",
        "T Bolkart",
        "J Thies"
      ],
      "year": "2022",
      "venue": "Towards metrical reconstruction of human faces",
      "arxiv": "arXiv:2204.06607"
    },
    {
      "citation_id": "61",
      "title": "Valence and arousal estimation in-the-wild with tensor methods",
      "authors": [
        "A Mitenkova",
        "J Kossaifi",
        "Y Panagakis",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "62",
      "title": "Adversarial domain adaption for multi-cultural dimensional emotion recognition in dyadic interactions",
      "authors": [
        "J Zhao",
        "R Li",
        "J Liang",
        "S Chen",
        "Q Jin"
      ],
      "year": "2019",
      "venue": "AVEC"
    },
    {
      "citation_id": "63",
      "title": "Cross-dataset learning and person-specific normalisation for automatic action unit detection",
      "authors": [
        "T Baltrušaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "64",
      "title": "Facial action unit intensity prediction via hard multi-task metric learning for kernel regression",
      "authors": [
        "J Nicolle",
        "K Bailly",
        "M Chetouani"
      ],
      "year": "2015",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "65",
      "title": "G2RL: Geometry-guided representation learning for facial action unit intensity estimation",
      "authors": [
        "Y Fan",
        "Z Lin"
      ],
      "year": "2020",
      "venue": "IJCAI"
    },
    {
      "citation_id": "66",
      "title": "Re-net: A relation embedded deep model for au occurrence and intensity estimation",
      "authors": [
        "H Yang",
        "L Yin"
      ],
      "year": "2020",
      "venue": "ACCV"
    },
    {
      "citation_id": "67",
      "title": "Variational gaussian process auto-encoder for ordinal prediction of facial action units",
      "authors": [
        "S Eleftheriadis",
        "O Rudovic",
        "M Deisenroth",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "ACCV"
    },
    {
      "citation_id": "68",
      "title": "Deepcoder: Semi-parametric variational autoencoders for automatic facial action coding",
      "authors": [
        "D Linh Tran",
        "R Walecki",
        "S Eleftheriadis",
        "B Schuller",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "69",
      "title": "Sparse localized deformation components",
      "authors": [
        "T Neumann",
        "K Varanasi",
        "S Wenger",
        "M Wacker",
        "M Magnor",
        "C Theobalt"
      ],
      "year": "2013",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "70",
      "title": "Temporally coherent visual representations for dimensional affect recognition",
      "authors": [
        "M Tellamekala",
        "M Valstar"
      ],
      "year": "2019",
      "venue": "ACII"
    },
    {
      "citation_id": "71",
      "title": "Self-supervised learning for facial action unit recognition through temporal consistency",
      "authors": [
        "L Lu",
        "L Tavabi",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "Self-supervised learning for facial action unit recognition through temporal consistency"
    },
    {
      "citation_id": "72",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "PNAS"
    },
    {
      "citation_id": "73",
      "title": "Compound facial expressions of emotion",
      "year": "2014",
      "venue": "PNAS"
    }
  ]
}