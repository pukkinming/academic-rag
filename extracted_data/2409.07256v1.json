{
  "paper_id": "2409.07256v1",
  "title": "Mrac Track 1: 2Nd Workshop On Multimodal, Generative And Responsible Affective Computing",
  "published": "2024-09-11T13:25:42Z",
  "authors": [
    "Shreya Ghosh",
    "Zhixi Cai",
    "Abhinav Dhall",
    "Dimitrios Kollias",
    "Roland Goecke",
    "Tom Gedeon"
  ],
  "keywords": [
    "Affective Computing",
    "Human Computer Interaction",
    "Generative AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the rapid advancements in multimodal generative technology, Affective Computing research has provoked discussion about the potential consequences of AI systems equipped with emotional intelligence. Affective Computing involves the design, evaluation, and implementation of Emotion AI and related technologies aimed at improving people's lives. Designing a computational model in affective computing requires vast amounts of multimodal data, including RGB images, video, audio, text, and physiological signals. Moreover, Affective Computing research is deeply engaged with ethical considerations at various stages-from training emotionally intelligent models on large-scale human data to deploying these models in specific applications. Fundamentally, the development of any AI system must prioritize its impact on humans, aiming to augment and enhance human abilities rather than replace them, while drawing inspiration from human intelligence in a safe and responsible manner. The MRAC 2024 Track 1 workshop seeks to extend these principles from controlled, small-scale lab environments to real-world, large-scale contexts, emphasizing responsible development. The workshop also aims to highlight the potential implications of generative technology, along with the ethical consequences of its use, to researchers and industry professionals. To the best of our knowledge, this is the first workshop series to comprehensively address the full spectrum of multimodal, generative affective computing from a responsible AI perspective, and this is the second iteration of this workshop. Webpage: https://react-ws.github.io/2024/ \n CCS Concepts • Computing methodologies → Neural networks; Supervised learning; • Human-centered computing → Collaborative interaction.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing is a multidisciplinary field that involves the sensing, computational modelling, evaluation, and deployment of AI systems capable of recognizing, interpreting, and responding to human emotions  [38] . These systems rely on large-scale multimodal data encompassing RGB images, video, audio, text, and physiological signals to develop accurate and effective emotion-AI models  [8, 16, 28, 29] . The goal is not just to create machines that can mimic human emotions, but to develop systems that can genuinely enhance human communication, interactions  [17, 21, 24, 42]  by being more emotionally intelligent.\n\nThe computational modelling begins with collecting and annotating multimodal datasets, developing algorithms that can process and interpret this data, and training models to recognize and respond to emotional cues. This phase is heavily reliant on data that captures human non-verbal cues, including facial expressions, eyegaze, voice intonations, body language, and even physiological signals like heart rate or skin conductivity  [12, 18, 23, 31, 43, 51, 52] . The challenge here is not just in the technical development but also in ensuring that the data used in the training process is representative, diverse, and ethically sourced  [11] . After computational modelling, the evaluation phase is critical in ensuring that the Emotion AI models perform as intended across different scenarios and populations. This involves rigorous testing with diverse datasets to ensure that the models can accurately recognize and respond to a wide range of empathetic responses. The evaluation process must also consider the ethical implications of these technologies, such as potential biases in emotion recognition, the impact on users' privacy, and the consequences of incorrect or inappropriate emotional responses by the AI  [25, 26] . Once the models have been created and evaluated, the final stage is deployment, where these systems are integrated into real-world applications  [2, 35] . This could range from customer service bots that can detect and respond to customer frustration  [22] , to healthcare applications that monitor patients' emotional states to provide better care  [32, 34] . However, deployment brings its own set of ethical challenges, such as ensuring that these systems are used in ways that are beneficial to users and do not infringe on their rights or well-being  [10, 34] . Basically, the field of Affective Computing mainly aim to integrate emotional intelligence to computers or AI systems. As the development and deployment of Emotion AI and related technologies progress, they offer the potential to significantly enhance human life by making machines more responsive and empathetic. The MRAC 2024 workshop aims to address these by focusing on the responsible creation, evaluation, and deployment of Emotion AI and assistive technologies with applications in education, entertainment and healthcare.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Responsible Affective Computing",
      "text": "Ethics is the building block of any responsible AI systems, and Affective Computing is no exception. The development of AI systems that can recognize and respond to human emotions must consider ethical aspects. Data Privacy and Consent. The collection and use of multimodal data raise concerns about privacy and consent. For instance, the use of facial recognition technology to detect emotions can be invasive, especially if users are not fully aware of how their data is being used. Ensuring that data is collected with informed consent and users have control over their data is crucial in the responsible development of Emotional AI  [33, 47] . Bias and Fairness. Emotion AI systems learns on the basis of input training data. If the training data is biased whether in terms of demographic representation or cultural context; the resulting computational models can propagate these biases  [49] . This could lead to scenarios where certain groups are unfairly treated or misinterpreted by the AI, leading to negative consequences  [40] . Addressing these biases is critical to developing fair and inclusive Emotion AI systems for different applications  [44, 45] . Also, there is an increasing risk of generated data being used 'without knowing it is generated' in training models. There is always a potential risk associated with it. Transparency and Interpretability. Another key consideration while developing Emotion AI models is the transparency of the system. Users should be aware of how the system work, what data is used, and how decisions are made. This transparency is crucial for building trust in the technology  [46] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Role Of Generative Technology",
      "text": "Generative AI, that involves the creation of new data or multimodal digital content by AI systems/models, is becoming increasingly important in Affective Computing  [48] . For example, generative models can create realistic avatars that can interact with users in a more human-like way. AI can even generate synthetic data to train emotion-AI models. However, the use of generative technology also raises additional concerns. Synthetic Data and Deepfakes. The ability to generate synthetic data can be beneficial for training Emotion AI models, especially when real-world data is scarce or difficult to obtain. However, this same technology can also be used to create deepfakes manipulated videos or images that can be used to deceive or harm others. Ensuring that generative technology is used responsibly to prevent potential misuse is a critical challenge  [6, 7, 9, 36] . Human-Machine Interaction. It is a growing field in the area of affective computing that aims to encode human non verbal as well as verbal behavior  [19, 20, 41]  followed by incorporating it in human AI interaction systems  [1]   Impact. In recent years, the AI revolution has already influenced our daily life through virtual assistants across various sectors such as healthcare, banking, transportation, and education. As we look toward the future, it's clear that humans will increasingly interact with AI-powered systems, potentially even more frequently than with other humans.\n\nTo this end, Affective computing holds numerous promising robust future applications, such as forecasting and preventing anxiety, stress, depression, and other mental health issues  [15] ; enhancing robotic empathy  [23] ; supporting individuals with communication, behavioral, and emotional regulation challenges; and promoting overall health and well-being. However, these applications often involve handling sensitive, private, and personal data, necessitating strong controls and protections. Therefore, it is essential to consider emotionally intelligent systems that are 'Responsive' and 'Responsible'. Relevance. Affective Computing often involves unimodal or multimodal data such as images, video, audio, text, physiological signals. Moreover, for the adaptation process to a new deployment scenario requires environment, culture specific data. Developing AI models with multimodal human data is highly co-related with the themes of ACM-MM 2024. The five major themes of ACM-MM 2024 are Engaging Users with Multimedia, Experience, Multimedia systems, Multimedia Content Understanding and Multimedia in the Generative AI Era. All of the themes are co-related with this workshop explicitly or implicitly. Engaging users with multimedia aims to encode emotional and social signal which is highly aligned with the workshop theme. Similar aims with understanding multimedia context (where the ground truth is human-level perception), experience (interaction and experience), generative AI for affective face generation and multimedia system. Expert Talks. We invited Prof. Julian Epps (University of New South Wales)  [14]  and Prof. Mohammed Bennamoun (University of Western Australia)  [4]  to deliver keynotes. Their keynote details are mentioned in our website.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Contribution Papers",
      "text": "This is the second iteration of MRAC workshop, where the accepted papers mainly focus on potential threats of generative AI and deepfakes  [13, 50] , long video based affect understanding  [3]  and affect prediction in video conversation  [39] . In the remainder of this section, we briefly introduce each accepted paper.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Are You Paying Attention? Multimodal Linear Attention Transformers For Affect Prediction In Video Conversations",
      "text": "The post COVID-19 era has witnessed a significant increase in the adoption of video-based communication, highlighting the importance of unintrusive affect recognition method during digital interactions. This paper presents a multimodal emotion recognition in video conversational settings by leveraging linear attention-based Transformer framework from audio-visual cues. This paper  [39]  involves exploring various linear attention mechanisms and comparing them with traditional self-attention techniques. By utilizing the K-EmoCon dataset  [37] , the method demonstrates competitive performance in predicting affective states while significantly enhancing memory efficiency. Ablation studies reveal that carefully optimized simple fusion methods can match or even surpass the effectiveness of more complex approaches. The research contributes to the development of more accessible and efficient multimodal emotion recognition systems tailored for video-based conversations.\n\nThese advancements have practical applications in enhancing remote communication and monitoring digital well-being, particularly in the context of the post-pandemic era where digital interactions have become increasingly prevalent. The findings emphasize the potential for linear attention-based models to offer robust solutions for emotion recognition, providing a balance between performance and computational efficiency. This work underscores the importance of efficient and accessible tools for emotion recognition in the evolving landscape of digital communication. By addressing the challenges of memory efficiency and model complexity, the proposed approach offers a viable pathway for integrating affect recognition into everyday video interactions, thereby contributing to improved remote communication experiences and better digital well-being monitoring.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Can Expression Sensitivity Improve Macroand Micro-Expression Spotting In Long Videos?",
      "text": "Spotting facial expressions is crucial as it directly reflects emotions, particularly those underlying feelings and intentions that might not be verbally expressed. Detecting both macro-and microexpressions is especially important in psychological analysis, as it provides insight into nuanced emotional states. The detection process involves identifying specific intervals within a long video that contain either macro-or micro-expressions, distinguishing these critical moments from other facial movements occurring at different time scales. Building on successful outcomes in micro-expression recognition, this paper proposes an algorithm designed to enhance the performance of spotting both macro-and micro-expressions using an expression-sensitive model. This study  [3]  introduces a novel and effective approach that emphasizes the integration of recognition and detection tasks in the analysis of facial expressions. By utilizing datasets initially intended for micro-expression recognition, this approach broadens the scope of these datasets, expanding their utility beyond their original purpose. The proposed algorithm leverages a pre-trained CNN model and introduces a unique confidence value-based mechanism to label training samples. This method not only achieves superior performance compared to other advanced techniques but also demonstrates an exceptional balance across various databases, encompassing both macro-and micro-expressions, as well as between precision and recall. This balanced performance is crucial for accurately detecting and analyzing expressions across different contexts.\n\nThe benchmarks and evaluation standards used in this study adhere to the challenge policies and evaluation metrics established by MEGC 2020  [27]  and MEGC 2021  [30] . By aligning with these standards, the proposed approach ensures that its results are both reliable and comparable with other leading methods in the field. Overall, this study contributes to the advancement of facial expression analysis by offering a robust solution for the detection of both macro-and micro-expressions, enhancing the understanding of subtle emotional cues in psychological and behavioral analysis.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "The-Fd: Task Hierarchical Emotion-Aware For Fake Detection",
      "text": "The rapid advancement of deepfake generation technology has led to the emergence of more naturally fine-grained modifications within short segments of content. Despite this, most research has primarily focused on classifying entire videos as either real or fake, with only a limited number of studies addressing the precise localization of forgeries within both audio and visual modalities of a video. This paper introduces a comprehensive solution that not only addresses whole-video fake classification but also emphasizes segment-level forgery temporal localization. The proposed approach  [50] , Task Hierarchical Emotion-aware for Fake Detection (THE-FD) architecture, is designed to initially process video-level data and seamlessly adapt to segment-level analysis. This hierarchical structure provides a general framework that allows for the inheritance and sharing of features across different levels, optimizing the detection process. Key innovations of the THE-FD architecture include the introduction of an emotional feature space to represent sentiment perturbations, which are often indicative of manipulations. Additionally, a fake-frame heatmap module is proposed to capture hidden fake patterns across multimodal data, enhancing the system's ability to detect subtle alterations. To further refine the detection process, a multiscale pyramid transformer is employed to learn features at various levels and time scales, ensuring comprehensive coverage of potential forgeries. The combination of these advancements results in a significant improvement in performance compared to existing methods.\n\nThe proposed approach not only enhances the accuracy of fake detection but also ranks among the top-performing methods in the 2024 1M-Deepfakes Detection Challenge  [5] . By addressing the challenges of fine-grained temporal localization and leveraging the emotional feature space, this research contributes to the development of more robust and precise tools for detecting deepfake content, offering a valuable resource in the ongoing battle against digital manipulation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "W-Tdl: Window-Based Temporal Deepfake Localization",
      "text": "Nowadays distinguishing highly realistic synthetic data from genuine samples has become increasingly challenging. This growing sophistication is particularly concerning with the rise of deepfake content, including images, videos, and audio, which are often used for malicious purposes, making effective detection methods more critical than ever. Despite significant advancements through various competitions, a substantial gap remains in accurately identifying the temporal boundaries of these manipulations.\n\nTo address this challenge, a novel approach for temporal deepfake localization (TDL)  [13]  is proposed, utilizing a window-based method for audio (W-TDL) alongside a complementary visual frame-based model. The contributions of this approach are twofold: firstly,  [13]  introduces an effective method for detecting and localizing manipulated segments in both video and audio; secondly, it tackles the issue of unbalanced training labels within spoofed audio datasets, which often hinder the performance of detection models. Leveraging the EVA visual transformer for precise frame-level analysis enhances the ability to identify deepfake manipulations in video content. In addition, the modified TDL method for audio improves the detection of temporal boundaries, ensuring more accurate identification of fake segments. This dual-model strategy addresses current limitations in deepfake detection, offering a comprehensive solution that integrates both visual and auditory cues.\n\nComprehensive experiments on the AV-Deepfake1M dataset have demonstrated the effectiveness of this approach, achieving competitive results in the 1M-DeepFakes Detection Challenge  [5] . The success of this method underscores its potential as a robust solution for detecting and localizing deepfake manipulations, marking a significant advancement in the fight against deepfake threats. This work aims to contribute to the development of more secure and reliable systems for identifying synthetic content across various media types.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Summary",
      "text": "The MRAC 2024 workshop aims to pave the way for the responsible use of multimodal and generative technologies in Affective Computing. Affective Computing represents a powerful tool for enhancing human interactions with AI systems by adding the \"empathy\" component. As multimodal and generative technologies continue to advance, it is crucial to ensure that these systems are developed responsibly, with a focus on augmenting human capabilities. The development of Emotion AI relies on vast amounts of multimodal data, including images, video, audio, text, and physiological signals, to create models capable of recognizing and interpreting human emotions. While this data is crucial for the accuracy and effectiveness of these systems, it also raises concerns about privacy, consent, and potential biases. To ensure that these models are trained on diverse, representative datasets is essential to avoid propagating biases and to create fair, inclusive AI systems.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Future Directions",
      "text": "The future of multimodal, generative and responsible AI holds several exciting possibilities and challenges as affective computing and assistive technology continues to advance. Here are some key aspects to consider while thinking about the future of emotion AI:\n\n( A new approach is the generation of synthetic data which matches important distributional properties of private data, and can be used to train models which are then tested on the private data which therefore can solve the purpose of keeping the real data truly private. (8) Ethical AI Research: There will be a growing emphasis on conducting research specifically focused on the ethical aspects of AI, including the exploration of novel ethical theories, frameworks, and guidelines tailored to AI systems. Companies and organizations will be expected to demonstrate a strong commitment to responsible AI practices. Ethical AI development and deployment will become a competitive advantage, and consumers may prioritize products and services that align with their values. As AI intersects with emerging technologies like quantum computing, biotechnology, and autonomous vehicles, ethical considerations will become even more critical. Responsible AI frameworks and guardrails will need to adapt to these evolving technologies. (9) Generative AI addressing inclusion: A major challenge in generative AI for human centric computing comes from the variability due to different cultures and languages. By focusing on universal features and building language-agnostic systems, generative AI can become more effective in empowering ML models, even in scenarios where the language or cultural context is unfamiliar. From a generative AI perspective, generating audio-visual content in non-native languages presents a unique challenge. Typically, when foreign language videos are accessed, they are often accompanied by voice-overs or native language subtitles. This creates a complex scenario where the subtleties of the original language, cultural information, and context may be missed, making it difficult for both humans and AI models to accurately interpret the video. Thus, it is crucial to develop and use diverse datasets containing generative content in multiple languages. Additionally, the models should be trained to prioritize features that are invariant to language, such as discrepancies in audio-visual synchronization or unnatural facial movements, allowing them to generalize across different linguistic and cultural scenarios.\n\nThe future of emotion AI holds the promise of more ethical, transparent, and accountable AI systems. However, it also poses challenges related to the evolving nature of technology, the need for adaptive ethical frameworks, and the importance of global collaboration. Emotion AI will continue to be a dynamic field, evolving to meet the ethical and societal demands of AI in an ever-changing world.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Queen Mary University of London\nUNSW": "London, United Kingdom\nCanberra, Australia",
          "Curtin University": "Perth, Australia"
        },
        {
          "Queen Mary University of London\nUNSW": "d.kollias@qmul.ac.uk\nr.goecke@unsw.edu.au",
          "Curtin University": "tom.gedeon@curtin.edu.au"
        },
        {
          "Queen Mary University of London\nUNSW": "Abstract",
          "Curtin University": "ACM Reference Format:"
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "Shreya Ghosh, Zhixi Cai, Abhinav Dhall, Dimitrios Kollias, Roland Goecke,"
        },
        {
          "Queen Mary University of London\nUNSW": "With the rapid advancements in multimodal generative technology,",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "and Tom Gedeon. 2024. MRAC Track 1: 2nd Workshop on Multimodal,"
        },
        {
          "Queen Mary University of London\nUNSW": "Affective Computing research has provoked discussion about the",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "Generative and Responsible Affective Computing. In Proceedings of the 2nd"
        },
        {
          "Queen Mary University of London\nUNSW": "potential consequences of AI systems equipped with emotional in-",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "International Workshop on Multimodal and Responsible Affective Computing"
        },
        {
          "Queen Mary University of London\nUNSW": "telligence. Affective Computing involves the design, evaluation, and",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "(MRAC ’24), November 1, 2024, Melbourne, VIC, Australia. ACM, New York,"
        },
        {
          "Queen Mary University of London\nUNSW": "implementation of Emotion AI and related technologies aimed at",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "NY, USA, 6 pages. https://doi.org/10.1145/3689092.3690042"
        },
        {
          "Queen Mary University of London\nUNSW": "improving people’s lives. Designing a computational model in affec-",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "tive computing requires vast amounts of multimodal data, including",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "RGB images, video, audio, text, and physiological signals. Moreover,",
          "Curtin University": "1\nIntroduction"
        },
        {
          "Queen Mary University of London\nUNSW": "Affective Computing research is deeply engaged with ethical con-",
          "Curtin University": "Affective Computing is a multidisciplinary field that involves the"
        },
        {
          "Queen Mary University of London\nUNSW": "siderations at various stages-from training emotionally intelligent",
          "Curtin University": "sensing, computational modelling, evaluation, and deployment of"
        },
        {
          "Queen Mary University of London\nUNSW": "models on large-scale human data to deploying these models in",
          "Curtin University": "AI systems capable of recognizing,\ninterpreting, and responding"
        },
        {
          "Queen Mary University of London\nUNSW": "specific applications. Fundamentally, the development of any AI sys-",
          "Curtin University": "to human emotions [38]. These systems rely on large-scale mul-"
        },
        {
          "Queen Mary University of London\nUNSW": "tem must prioritize its impact on humans, aiming to augment and",
          "Curtin University": "timodal data encompassing RGB images, video, audio, text, and"
        },
        {
          "Queen Mary University of London\nUNSW": "enhance human abilities rather than replace them, while drawing in-",
          "Curtin University": "physiological signals to develop accurate and effective emotion-AI"
        },
        {
          "Queen Mary University of London\nUNSW": "spiration from human intelligence in a safe and responsible manner.",
          "Curtin University": "models [8, 16, 28, 29]. The goal is not just to create machines that"
        },
        {
          "Queen Mary University of London\nUNSW": "The MRAC 2024 Track 1 workshop seeks to extend these princi-",
          "Curtin University": "can mimic human emotions, but to develop systems that can gen-"
        },
        {
          "Queen Mary University of London\nUNSW": "ples from controlled, small-scale lab environments to real-world,",
          "Curtin University": "uinely enhance human communication, interactions [17, 21, 24, 42]"
        },
        {
          "Queen Mary University of London\nUNSW": "large-scale contexts, emphasizing responsible development. The",
          "Curtin University": "by being more emotionally intelligent."
        },
        {
          "Queen Mary University of London\nUNSW": "workshop also aims to highlight the potential implications of gen-",
          "Curtin University": "The computational modelling begins with collecting and anno-"
        },
        {
          "Queen Mary University of London\nUNSW": "erative technology, along with the ethical consequences of its use,",
          "Curtin University": "tating multimodal datasets, developing algorithms that can process"
        },
        {
          "Queen Mary University of London\nUNSW": "to researchers and industry professionals. To the best of our knowl-",
          "Curtin University": "and interpret this data, and training models to recognize and re-"
        },
        {
          "Queen Mary University of London\nUNSW": "edge, this is the first workshop series to comprehensively address",
          "Curtin University": "spond to emotional cues. This phase is heavily reliant on data that"
        },
        {
          "Queen Mary University of London\nUNSW": "the full spectrum of multimodal, generative affective computing",
          "Curtin University": "captures human non-verbal cues, including facial expressions, eye-"
        },
        {
          "Queen Mary University of London\nUNSW": "from a responsible AI perspective, and this is the second iteration",
          "Curtin University": "gaze, voice intonations, body language, and even physiological"
        },
        {
          "Queen Mary University of London\nUNSW": "of this workshop. Webpage: https://react-ws.github.io/2024/",
          "Curtin University": "signals like heart rate or skin conductivity [12, 18, 23, 31, 43, 51, 52]."
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "The challenge here is not just in the technical development but"
        },
        {
          "Queen Mary University of London\nUNSW": "CCS Concepts",
          "Curtin University": "also in ensuring that the data used in the training process is repre-"
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "sentative, diverse, and ethically sourced [11]. After computational"
        },
        {
          "Queen Mary University of London\nUNSW": "• Computing methodologies → Neural networks; Supervised",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "modelling, the evaluation phase is critical in ensuring that the Emo-"
        },
        {
          "Queen Mary University of London\nUNSW": "learning; • Human-centered computing → Collaborative inter-",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "tion AI models perform as intended across different scenarios and"
        },
        {
          "Queen Mary University of London\nUNSW": "action.",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "populations. This involves rigorous testing with diverse datasets to"
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "ensure that the models can accurately recognize and respond to a"
        },
        {
          "Queen Mary University of London\nUNSW": "Keywords",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "wide range of empathetic responses. The evaluation process must"
        },
        {
          "Queen Mary University of London\nUNSW": "Affective Computing, Human Computer Interaction, Generative AI",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "also consider the ethical implications of these technologies, such"
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "as potential biases in emotion recognition, the impact on users’ pri-"
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "vacy, and the consequences of incorrect or inappropriate emotional"
        },
        {
          "Queen Mary University of London\nUNSW": "This work is licensed under a Creative Commons Attribution",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "International 4.0 License.",
          "Curtin University": "responses by the AI [25, 26]. Once the models have been created"
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "and evaluated, the final stage is deployment, where these systems"
        },
        {
          "Queen Mary University of London\nUNSW": "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.",
          "Curtin University": "are integrated into real-world applications [2, 35]. This could range"
        },
        {
          "Queen Mary University of London\nUNSW": "© 2024 Copyright held by the owner/author(s).",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "",
          "Curtin University": "from customer service bots that can detect and respond to customer"
        },
        {
          "Queen Mary University of London\nUNSW": "ACM ISBN 979-8-4007-1203-6/24/10",
          "Curtin University": ""
        },
        {
          "Queen Mary University of London\nUNSW": "https://doi.org/10.1145/3689092.3690042",
          "Curtin University": "frustration [22], to healthcare applications that monitor patients’"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "emotional states to provide better care [32, 34]. However, deploy-",
          "Shreya Ghosh et al.": "videos or images that can be used to deceive or harm others. En-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "ment brings its own set of ethical challenges, such as ensuring that",
          "Shreya Ghosh et al.": "suring that generative technology is used responsibly to prevent"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "these systems are used in ways that are beneficial to users and do",
          "Shreya Ghosh et al.": "potential misuse is a critical challenge [6, 7, 9, 36]."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "Human-Machine Interaction. It is a growing field in the area"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "field of Affective Computing mainly aim to integrate emotional",
          "Shreya Ghosh et al.": "of affective computing that aims to encode human non verbal as"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "intelligence to computers or AI systems. As the development and",
          "Shreya Ghosh et al.": "well as verbal behavior [19, 20, 41] followed by incorporating it in"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "deployment of Emotion AI and related technologies progress, they",
          "Shreya Ghosh et al.": "human AI interaction systems [1] using generative technologies."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "offer the potential\nto significantly enhance human life by mak-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "ing machines more responsive and empathetic. The MRAC 2024",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "1.3\nMRAC 2024 Workshop: A Focus on"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "workshop aims to address these by focusing on the responsible",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "Multimodal, Responsibile and Generative AI"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "creation, evaluation, and deployment of Emotion AI and assistive",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "The MRAC 2024 workshop aims to address these challenges by"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "technologies with applications in education, entertainment and",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "focusing on responsible, multimodal and generative Affective Com-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "healthcare.",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "puting. The workshop explore how to transfer the principles of"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "responsive and responsible AI from small-scale, lab-based environ-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "ments to real-world, large-scale applications. This includes not only"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "1.1\nResponsible Affective Computing",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "the technical aspects of creating and deploying Emotion AI systems"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Ethics is the building block of any responsible AI systems, and Af-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "but also the broader implications of the human centric technologies."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "fective Computing is no exception. The development of AI systems",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "that can recognize and respond to human emotions must consider",
          "Shreya Ghosh et al.": "Real-World Applications. The workshop showcase real-world"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "applications of Affective Computing in healthcare, education, enter-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "ethical aspects.",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "tainment etc, highlighting the potential benefits and the challenges"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Data Privacy and Consent. The collection and use of multimodal",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "involved. We believe that participants have the opportunity to dis-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "data raise concerns about privacy and consent. For instance, the",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "cuss how to design Emotion AI systems that are both effective and"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "use of\nfacial recognition technology to detect emotions can be",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "ethically sound, with a focus on transparency, explainability, and"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "invasive, especially if users are not fully aware of how their data is",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "user empowerment."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "being used. Ensuring that data is collected with informed consent",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and users have control over their data is crucial in the responsible",
          "Shreya Ghosh et al.": "Interdisciplinary Collaboration. The workshop will bring to-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "gether researchers, industry professionals to discuss the future of"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "development of Emotional AI [33, 47].",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "Affective Computing. By fostering interdisciplinary collaboration,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Bias and Fairness. Emotion AI systems learns on the basis of",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "the workshop aims to ensure that the development of Emotion AI is"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "input training data. If the training data is biased whether in terms",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "guided by diverse perspectives and the inclusiveness is integrated"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "of demographic representation or cultural context; the resulting",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "into every stage of the process."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "computational models can propagate these biases [49]. This could",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "lead to scenarios where certain groups are unfairly treated or mis-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "interpreted by the AI, leading to negative consequences [40]. Ad-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "2\nObjective, scope and topics of the workshop"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "dressing these biases is critical\nto developing fair and inclusive",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "The 2nd International Workshop on Multimodal, Generative and"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Emotion AI systems for different applications [44, 45]. Also, there is",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "Responsible Affective Computing (MRAC 2024) at ACM-MM 2024"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "an increasing risk of generated data being used ‘without knowing",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "(track for Multimodal and Responsible Affective Computing) aims"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "it is generated’ in training models. There is always a potential risk",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "to encourage and highlight novel strategies for affective phenom-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "associated with it.",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "ena estimation and prediction with a focus on robustness and ac-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Transparency and Interpretability. Another key consideration",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "curacy in extended parameter spaces, spatially, temporally, spatio-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "while developing Emotion AI models is the transparency of the",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "temporally and most importantly ‘Responsibly’. This is expected"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "system. Users should be aware of how the system work, what data",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "to be achieved by applying novel neural network architectures,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "is used, and how decisions are made. This transparency is crucial",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "generative ai, incorporating anatomical insights and constraints,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "for building trust in the technology [46].",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "introducing new and challenging datasets, and exploiting multi-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "modal training. The topics of the workshop include but not limited"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "to the following:"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "1.2\nThe Role of Generative Technology",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Generative AI, that involves the creation of new data or multimodal",
          "Shreya Ghosh et al.": "(1) Large scale data generation or Inexpensive annotation for Af-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "digital content by AI systems/models,\nis becoming increasingly",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "fective Computing"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "important in Affective Computing [48]. For example, generative",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "(2) Generative AI for Affective Computing using multimodal sig-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "models can create realistic avatars that can interact with users in a",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "nals"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "(3) Multi-modal method for emotion recognition"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "(4) Privacy preserving large scale emotion recognition in the wild"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "(5) Generative aspects of affect analysis"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Synthetic Data and Deepfakes. The ability to generate synthetic",
          "Shreya Ghosh et al.": "(6) Contextual Gesture Generation"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "(7) Deepfake generation, detection and temporal deepfake local-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "when real-world data is scarce or difficult to obtain. However, this",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "ization aka Gernerative AI"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "(8) Multimodal data analysis"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(9) Affective Computing Applications in education, entertainment\n4.1\nAre You Paying Attention? Multimodal"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "& healthcare"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Linear Attention Transformers for Affect"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(10) Explainable or Privacy Preserving AI in affective computing"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Prediction in Video Conversations"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(11) Generative and responsible personalization of affective phe-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "The post COVID-19 era has witnessed a significant increase in the"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "nomena"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "adoption of video-based communication, highlighting the impor-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(12) Zero-shot and few-shot learning strategies in Emotion AI data"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "tance of unintrusive affect recognition method during digital inter-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(13) Bias in affective computing data (e.g.\nlack of multi-cultural"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "actions. This paper presents a multimodal emotion recognition in"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "datasets)"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "video conversational settings by leveraging linear attention-based"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(14) Affective Video Understanding"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Transformer framework from audio-visual cues."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(15) Semi-\nsupervised, weakly\nsupervised,\nunsupervised,\nself-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "This paper [39] involves exploring various linear attention mech-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "supervised learning methods"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "anisms and comparing them with traditional self-attention tech-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(16) domain adaptation methods for Affective Computing"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "niques. By utilizing the K-EmoCon dataset [37], the method demon-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "strates competitive performance in predicting affective states while"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "3\nResearch Impact, Relevance and Expert Talks\nsignificantly enhancing memory efficiency. Ablation studies reveal"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "that carefully optimized simple fusion methods can match or even\nImpact. In recent years, the AI revolution has already influenced"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "our daily life through virtual assistants across various sectors such\nsurpass the effectiveness of more complex approaches. The research"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "as healthcare, banking, transportation, and education. As we look\ncontributes to the development of more accessible and efficient"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "toward the future, it’s clear that humans will increasingly interact\nmultimodal emotion recognition systems tailored for video-based"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "with AI-powered systems, potentially even more frequently than\nconversations."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "with other humans.\nThese advancements have practical applications in enhancing"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "To this end, Affective computing holds numerous promising ro-\nremote communication and monitoring digital well-being, particu-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "bust future applications, such as forecasting and preventing anxiety,\nlarly in the context of the post-pandemic era where digital interac-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "stress, depression, and other mental health issues [15]; enhancing\ntions have become increasingly prevalent. The findings emphasize"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "robotic empathy [23]; supporting individuals with communication,\nthe potential for linear attention-based models to offer robust so-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "behavioral, and emotional regulation challenges; and promoting\nlutions for emotion recognition, providing a balance between per-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "overall health and well-being. However, these applications often\nformance and computational efficiency. This work underscores the"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "involve handling sensitive, private, and personal data, necessitat-\nimportance of efficient and accessible tools for emotion recognition"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "ing strong controls and protections. Therefore,\nit\nis essential\nto\nin the evolving landscape of digital communication. By addressing"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "consider emotionally intelligent systems that are ‘Responsive’ and\nthe challenges of memory efficiency and model complexity, the"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "‘Responsible’.\nproposed approach offers a viable pathway for integrating affect"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "recognition into everyday video interactions, thereby contributing\nRelevance. Affective Computing often involves unimodal or multi-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "modal data such as images, video, audio, text, physiological signals.\nto improved remote communication experiences and better digital"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Moreover, for the adaptation process to a new deployment scenario\nwell-being monitoring."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "requires environment, culture specific data. Developing AI models"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "with multimodal human data is highly co-related with the themes"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "of ACM-MM 2024. The five major themes of ACM-MM 2024 are\n4.2\nCan Expression Sensitivity Improve Macro-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Engaging Users with Multimedia, Experience, Multimedia systems,"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and Micro-Expression Spotting in Long"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Multimedia Content Understanding and Multimedia in the Gener-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Videos?"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "ative AI Era. All of the themes are co-related with this workshop"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Spotting facial expressions is crucial as it directly reflects emo-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "explicitly or implicitly. Engaging users with multimedia aims to"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "tions, particularly those underlying feelings and intentions that"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "encode emotional and social signal which is highly aligned with"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "might not be verbally expressed. Detecting both macro- and micro-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "the workshop theme. Similar aims with understanding multimedia"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "expressions is especially important in psychological analysis, as it"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "context (where the ground truth is human-level perception), expe-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "provides insight into nuanced emotional states. The detection pro-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "rience (interaction and experience), generative AI for affective face"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "cess involves identifying specific intervals within a long video that"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "generation and multimedia system."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "contain either macro- or micro-expressions, distinguishing these"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Expert Talks. We invited Prof. Julian Epps (University of New"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "critical moments from other facial movements occurring at different"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "South Wales) [14] and Prof. Mohammed Bennamoun (University"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "time scales. Building on successful outcomes in micro-expression"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "of Western Australia) [4] to deliver keynotes. Their keynote details"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "recognition, this paper proposes an algorithm designed to enhance"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "are mentioned in our website."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "the performance of spotting both macro- and micro-expressions"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "using an expression-sensitive model."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "4\nContribution papers\nThis study [3] introduces a novel and effective approach that"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "This is the second iteration of MRAC workshop, where the ac-\nemphasizes the integration of recognition and detection tasks in"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "cepted papers mainly focus on potential threats of generative AI\nthe analysis of facial expressions. By utilizing datasets initially in-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and deepfakes [13, 50], long video based affect understanding [3]\ntended for micro-expression recognition, this approach broadens"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and affect prediction in video conversation [39]. In the remainder\nthe scope of these datasets, expanding their utility beyond their"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "of this section, we briefly introduce each accepted paper.\noriginal purpose. The proposed algorithm leverages a pre-trained"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "CNN model and introduces a unique confidence value-based mech-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "4.4 W-TDL: Window-Based Temporal Deepfake"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "anism to label training samples. This method not only achieves",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "Localization"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "superior performance compared to other advanced techniques but",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "Nowadays distinguishing highly realistic synthetic data from gen-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "also demonstrates an exceptional balance across various databases,",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "uine samples has become increasingly challenging. This growing"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "encompassing both macro- and micro-expressions, as well as be-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "sophistication is particularly concerning with the rise of deepfake"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "tween precision and recall. This balanced performance is crucial",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "content, including images, videos, and audio, which are often used"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "for accurately detecting and analyzing expressions across different",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "for malicious purposes, making effective detection methods more"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "contexts.",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "critical than ever. Despite significant advancements through various"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "The benchmarks and evaluation standards used in this study",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "competitions, a substantial gap remains in accurately identifying"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "adhere to the challenge policies and evaluation metrics established",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "the temporal boundaries of these manipulations."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "by MEGC 2020 [27] and MEGC 2021 [30]. By aligning with these",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "To address this challenge, a novel approach for temporal deep-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "standards, the proposed approach ensures that its results are both",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "fake localization (TDL) [13] is proposed, utilizing a window-based"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "reliable and comparable with other leading methods in the field.",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "method for audio (W-TDL) alongside a complementary visual"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Overall, this study contributes to the advancement of facial expres-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "frame-based model. The contributions of this approach are twofold:"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "sion analysis by offering a robust solution for the detection of both",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "firstly, [13] introduces an effective method for detecting and local-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "macro- and micro-expressions, enhancing the understanding of",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "izing manipulated segments in both video and audio; secondly, it"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "subtle emotional cues in psychological and behavioral analysis.",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "tackles the issue of unbalanced training labels within spoofed audio"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "datasets, which often hinder the performance of detection mod-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "els. Leveraging the EVA visual transformer for precise frame-level"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "analysis enhances the ability to identify deepfake manipulations"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "in video content. In addition, the modified TDL method for audio"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "4.3\nTHE-FD: Task Hierarchical Emotion-aware",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "improves the detection of temporal boundaries, ensuring more ac-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "for Fake Detection",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "curate identification of fake segments. This dual-model strategy"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "The rapid advancement of deepfake generation technology has",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "addresses current limitations in deepfake detection, offering a com-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "led to the emergence of more naturally fine-grained modifications",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "prehensive solution that integrates both visual and auditory cues."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "within short segments of content. Despite this, most research has",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "Comprehensive experiments on the AV-Deepfake1M dataset"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "primarily focused on classifying entire videos as either real or",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "have demonstrated the effectiveness of this approach, achieving"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "fake, with only a limited number of studies addressing the precise",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "competitive results in the 1M-DeepFakes Detection Challenge [5]."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "localization of forgeries within both audio and visual modalities of",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "The success of this method underscores its potential as a robust"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "a video. This paper introduces a comprehensive solution that not",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "solution for detecting and localizing deepfake manipulations, mark-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "only addresses whole-video fake classification but also emphasizes",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "ing a significant advancement in the fight against deepfake threats."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "segment-level forgery temporal localization.",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "This work aims to contribute to the development of more secure and"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "The proposed approach [50], Task Hierarchical Emotion-aware",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "reliable systems for identifying synthetic content across various"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "for Fake Detection (THE-FD) architecture, is designed to initially",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "media types."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "process video-level data and seamlessly adapt\nto segment-level",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "analysis. This hierarchical structure provides a general framework",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "that allows for the inheritance and sharing of features across dif-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "5\nSummary"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "ferent levels, optimizing the detection process. Key innovations",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "The MRAC 2024 workshop aims to pave the way for the responsible"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "of the THE-FD architecture include the introduction of an emo-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "use of multimodal and generative technologies in Affective Comput-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "tional feature space to represent sentiment perturbations, which",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "ing. Affective Computing represents a powerful tool for enhancing"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "are often indicative of manipulations. Additionally, a fake-frame",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "human interactions with AI systems by adding the “empathy” com-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "heatmap module is proposed to capture hidden fake patterns across",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "ponent. As multimodal and generative technologies continue to"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "multimodal data, enhancing the system’s ability to detect subtle",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "advance,\nit is crucial to ensure that these systems are developed"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "alterations. To further refine the detection process, a multiscale",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "responsibly, with a focus on augmenting human capabilities. The"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "pyramid transformer is employed to learn features at various lev-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "development of Emotion AI relies on vast amounts of multimodal"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "els and time scales, ensuring comprehensive coverage of poten-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "data, including images, video, audio, text, and physiological signals,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "tial forgeries. The combination of these advancements results in",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "to create models capable of recognizing and interpreting human"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "a significant improvement in performance compared to existing",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "emotions. While this data is crucial for the accuracy and effective-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "methods.",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "ness of these systems, it also raises concerns about privacy, consent,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "The proposed approach not only enhances the accuracy of fake",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "and potential biases. To ensure that these models are trained on"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "detection but also ranks among the top-performing methods in",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "diverse, representative datasets is essential to avoid propagating"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "the 2024 1M-Deepfakes Detection Challenge [5]. By addressing",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Shreya Ghosh et al.": "biases and to create fair, inclusive AI systems."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "the challenges of fine-grained temporal localization and leveraging",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "the emotional feature space, this research contributes to the devel-",
          "Shreya Ghosh et al.": ""
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "opment of more robust and precise tools for detecting deepfake",
          "Shreya Ghosh et al.": "6\nFuture Directions"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "content, offering a valuable resource in the ongoing battle against",
          "Shreya Ghosh et al.": "The future of multimodal, generative and responsible AI holds"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "digital manipulation.",
          "Shreya Ghosh et al.": "several exciting possibilities and challenges as affective computing"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "data acquisition limitations.": "(4) Bias and Fairness Mitigation: Efforts to mitigate bias in",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "AI will continue to evolve. Advanced techniques, such as",
          "audio-visual synchronization or unnatural facial movements,": "cultural scenarios."
        },
        {
          "data acquisition limitations.": "adversarial debiasing and fairness-aware machine learning,",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "",
          "audio-visual synchronization or unnatural facial movements,": "The future of emotion AI holds the promise of more ethical,"
        },
        {
          "data acquisition limitations.": "will be developed to reduce bias in AI systems and ensure",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "",
          "audio-visual synchronization or unnatural facial movements,": "transparent, and accountable AI systems. However, it also poses"
        },
        {
          "data acquisition limitations.": "fairness in models’s decision-making processes.",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "",
          "audio-visual synchronization or unnatural facial movements,": "challenges related to the evolving nature of technology, the need"
        },
        {
          "data acquisition limitations.": "(5) Explainability and Transparency: The demand for AI",
          "audio-visual synchronization or unnatural facial movements,": "for adaptive ethical frameworks, and the importance of global col-"
        },
        {
          "data acquisition limitations.": "explainability and transparency will intensify. Researchers",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "",
          "audio-visual synchronization or unnatural facial movements,": "laboration. Emotion AI will continue to be a dynamic field, evolving"
        },
        {
          "data acquisition limitations.": "will work on making AI models more interpretable and un-",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "",
          "audio-visual synchronization or unnatural facial movements,": "to meet the ethical and societal demands of AI in an ever-changing"
        },
        {
          "data acquisition limitations.": "derstandable, helping users to trust AI systems and better",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "",
          "audio-visual synchronization or unnatural facial movements,": "world."
        },
        {
          "data acquisition limitations.": "understand model’s decision-making processes.",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "(6) Human-AI Collaboration: The future of responsive AI will",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "",
          "audio-visual synchronization or unnatural facial movements,": "References"
        },
        {
          "data acquisition limitations.": "involve closer collaboration between humans and AI systems.",
          "audio-visual synchronization or unnatural facial movements,": ""
        },
        {
          "data acquisition limitations.": "",
          "audio-visual synchronization or unnatural facial movements,": "[1]\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi,"
        },
        {
          "data acquisition limitations.": "Human-AI partnerships will become more integrated into",
          "audio-visual synchronization or unnatural facial movements,": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "world."
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "References"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "[1]\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi,"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019."
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "human factors in computing systems. 1–13."
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "[2] Mostafa M Amin, Rui Mao, Erik Cambria, and Björn W Schuller. 2024. A wide"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "evaluation of ChatGPT on affective computing tasks.\nIEEE Transactions on"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "Affective Computing (2024)."
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "[3] Mengjiong Bai and Roland Goecke. 2024. Can Expression Sensitivity Improve"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "Macro- and Micro-Expression Spotting in Long Videos?. In Proceedings of the 2nd"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "International Workshop on Multimodal and Responsible Affective Computing. 0–0."
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "[4] Mohammed Bennamoun. 2024. Seeing in 3D: Assistive Robotics with Advanced"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "Computer Vision. In Proceedings of the 2nd International Workshop on Multimodal"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "and Responsible Affective Computing. 3–4."
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "[5] Zhixi Cai, Abhinav Dhall, Shreya Ghosh, Munawar Hayat, Dimitrios Kollias,"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "Kalin Stefanov, and Usman Tariq. 2024. 1M-Deepfakes Detection Challenge. In"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": ""
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "ACM Multimedia."
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "[6] Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav Dhall,"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "and Kalin Stefanov. 2024. AV-Deepfake1M: A large-scale LLM-driven audio-visual"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "deepfake dataset. ACM Multimedia (2024)."
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "[7] Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, and"
        },
        {
          "to meet the ethical and societal demands of AI in an ever-changing": "Munawar Hayat. 2023. Glitch in the matrix: A large scale benchmark for content"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and assistive technology continues to advance. Here are some key\naspects of AI, including the exploration of novel ethical theo-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "aspects to consider while thinking about the future of emotion AI:\nries, frameworks, and guidelines tailored to AI systems. Com-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "panies and organizations will be expected to demonstrate"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(1) Multi-modal/Cross-modal Analysis: Over"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "decade, head, eye and facial gesture synthesis has become\nAI development and deployment will become a competi-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "an interesting line of research. Prior works in this area have\ntive advantage, and consumers may prioritize products and"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "mainly use visual modality to generate realistic gesture. The\nservices that align with their values. As AI intersects with"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "main challenge in this domain is multimodal fine grained\nemerging technologies like quantum computing, biotechnol-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "data annotation for motion synthesis, gesture analysis which\nogy, and autonomous vehicles, ethical considerations will"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "is a noisy and error prone process. Research along this di-\nbecome even more critical. Responsible AI frameworks and"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "rection have potential to estimate assistive technology in\nguardrails will need to adapt to these evolving technologies."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(9) Generative AI addressing inclusion: A major challenge"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(2) Emotion AI in Unconstrained Settings: The most precise"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "methods for emotion and gesture estimation is performed\nthe variability due to different cultures and languages. By fo-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "via intrusive sensors,\nIR camera, and RGBD camera. The\ncusing on universal features and building language-agnostic"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "main drawback of existing systems is that their performance\nsystems, generative AI can become more effective in em-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "degrades when used in real-world settings.\nIn future, as-\npowering ML models, even in scenarios where the language"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "sistive technology models should consider these situations.\nor cultural context is unfamiliar. From a generative AI per-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Although several current efforts in this direction employ\nspective, generating audio-visual content in non-native lan-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "techniques, yet further research is needed. Moreover, most of\nguages presents a unique challenge. Typically, when foreign"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "the current affective computing benchmark datasets require\nlanguage videos are accessed, they are often accompanied by"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "the proper geometric arrangement as well as user coopera-\nvoice-overs or native language subtitles. This creates a com-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "tion. It would be an interesting direction to explore it in a\nplex scenario where the subtleties of the original language,"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "more flexible setting.\ncultural information, and context may be missed, making it"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(3) Learning with Limited Supervision and LLMs: With the"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "surge in unsupervised, self-supervised, weakly supervised\npret the video. Thus, it is crucial to develop and use diverse"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and VLM techniques in this domain, more exploration in this\ndatasets containing generative content in multiple languages."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "direction is required to reduce or eliminate the dependency\nAdditionally, the models should be trained to prioritize fea-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "on ground truth labels which could be error-prone due to\ntures that are invariant to language, such as discrepancies in"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "data acquisition limitations.\naudio-visual synchronization or unnatural facial movements,"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(4) Bias and Fairness Mitigation: Efforts to mitigate bias in"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "AI will continue to evolve. Advanced techniques, such as\ncultural scenarios."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "adversarial debiasing and fairness-aware machine learning,"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "The future of emotion AI holds the promise of more ethical,"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "will be developed to reduce bias in AI systems and ensure"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "transparent, and accountable AI systems. However, it also poses"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "fairness in models’s decision-making processes."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "challenges related to the evolving nature of technology, the need"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(5) Explainability and Transparency: The demand for AI\nfor adaptive ethical frameworks, and the importance of global col-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "explainability and transparency will intensify. Researchers"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "laboration. Emotion AI will continue to be a dynamic field, evolving"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "will work on making AI models more interpretable and un-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "to meet the ethical and societal demands of AI in an ever-changing"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "derstandable, helping users to trust AI systems and better"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "world."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "understand model’s decision-making processes."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(6) Human-AI Collaboration: The future of responsive AI will"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "References"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "involve closer collaboration between humans and AI systems."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[1]\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi,"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Human-AI partnerships will become more integrated into\nPenny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "various domains, enhancing productivity, decision-making,\nGuidelines for human-AI interaction. In Proceedings of the 2019 chi conference on"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "human factors in computing systems. 1–13."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and problem-solving capabilities."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[2] Mostafa M Amin, Rui Mao, Erik Cambria, and Björn W Schuller. 2024. A wide"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "evaluation of ChatGPT on affective computing tasks.\n(7) Privacy-Preserving AI: Techniques for privacy-preserving\nIEEE Transactions on"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Affective Computing (2024).\nAI will continue to advance, allowing AI to work with sensi-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[3] Mengjiong Bai and Roland Goecke. 2024. Can Expression Sensitivity Improve"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "tive data while protecting individuals’ privacy rights. Fed-"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Macro- and Micro-Expression Spotting in Long Videos?. In Proceedings of the 2nd"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "erated learning, differential privacy, and secure multiparty\nInternational Workshop on Multimodal and Responsible Affective Computing. 0–0."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[4] Mohammed Bennamoun. 2024. Seeing in 3D: Assistive Robotics with Advanced"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "computation are examples of privacy-preserving approaches."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Computer Vision. In Proceedings of the 2nd International Workshop on Multimodal"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "A new approach is the generation of synthetic data which"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and Responsible Affective Computing. 3–4."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[5] Zhixi Cai, Abhinav Dhall, Shreya Ghosh, Munawar Hayat, Dimitrios Kollias,\nmatches important distributional properties of private data,"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Kalin Stefanov, and Usman Tariq. 2024. 1M-Deepfakes Detection Challenge. In"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and can be used to train models which are then tested on"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "ACM Multimedia."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "the private data which therefore can solve the purpose of\n[6] Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav Dhall,"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "and Kalin Stefanov. 2024. AV-Deepfake1M: A large-scale LLM-driven audio-visual\nkeeping the real data truly private."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "deepfake dataset. ACM Multimedia (2024)."
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(8) Ethical AI Research: There will be a growing emphasis\n[7] Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, and"
        },
        {
          "MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing\nMRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "on conducting research specifically focused on the ethical\nMunawar Hayat. 2023. Glitch in the matrix: A large scale benchmark for content"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "driven audio–visual forgery detection and localization. Computer Vision and",
          "Shreya Ghosh et al.": "[31]\nShan Li and Weihong Deng. 2020. Deep facial expression recognition: A survey."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Image Understanding 236 (2023), 103818.",
          "Shreya Ghosh et al.": "IEEE transactions on affective computing 13, 3 (2020), 1195–1215."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[8] Zhixi Cai, Shreya Ghosh, Kalin Stefanov, Abhinav Dhall,\nJianfei Cai, Hamid",
          "Shreya Ghosh et al.": "[32] Yi Li, Shreya Ghosh, and Jyoti Joshi. 2021. PLAAN: pain level assessment with"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Rezatofighi, Reza Haffari, and Munawar Hayat. 2023. Marlin: Masked autoencoder",
          "Shreya Ghosh et al.": "anomaly-detection based network. Journal on Multimodal User Interfaces 15, 4"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "for facial video representation learning. In Proceedings of the IEEE/CVF conference",
          "Shreya Ghosh et al.": "(2021), 359–372."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "on computer vision and pattern recognition. 1493–1504.",
          "Shreya Ghosh et al.": "[33]\nShiya Liu, Yue Yao, Chaoyue Xing, and Tom Gedeon. 2020. Disguising Personal"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[9] Zhixi Cai, Kalin Stefanov, Abhinav Dhall, and Munawar Hayat. 2022. Do you",
          "Shreya Ghosh et al.": "Identity Information in EEG Signals. In Neural Information Processing: 27th In-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "really mean that? content driven audio-visual deepfake dataset and multimodal",
          "Shreya Ghosh et al.": "ternational Conference, ICONIP 2020, Bangkok, Thailand, November 18–22, 2020,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "method for temporal forgery localization. In 2022 International Conference on",
          "Shreya Ghosh et al.": "Proceedings, Part V 27. Springer, 87–95."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Digital Image Computing: Techniques and Applications (DICTA). IEEE, 1–10.",
          "Shreya Ghosh et al.": "[34]\nYuanyuan Liu, Ke Wang, Lin Wei, Jingying Chen, Yibing Zhan, Dapeng Tao, and"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[10] Karina Cortiñas-Lorenzo and Gerard Lacey. 2023. Toward explainable affective",
          "Shreya Ghosh et al.": "Zhe Chen. 2024. Affective Computing for Healthcare: Recent Trends, Applica-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "computing: A review.\nIEEE Transactions on Neural Networks and Learning Systems",
          "Shreya Ghosh et al.": "tions, Challenges, and Beyond. arXiv preprint arXiv:2402.13589 (2024)."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(2023).",
          "Shreya Ghosh et al.": "[35] Hao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingyong Hu, Jiaqi Tang, Yuting"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[11]\nLaurence Devillers and Roddy Cowie. 2023. Ethical considerations on affective",
          "Shreya Ghosh et al.": "Zhang, Kaishen Yuan, Bin Huang, Zitong Yu, et al. 2024. Gpt as psychologist?"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "computing: an overview. Proc. IEEE (2023).",
          "Shreya Ghosh et al.": "preliminary evaluations for gpt-4v on visual affective computing. In Proceedings"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[12] Abhinav Dhall, Roland Goecke, Tom Gedeon, and Nicu Sebe. 2016.\nEmotion",
          "Shreya Ghosh et al.": "of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 322–331."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "recognition in the wild.\n, 95–97 pages.",
          "Shreya Ghosh et al.": "[36] Kartik Narayan, Harsh Agarwal, Kartik Thakral, Surbhi Mittal, Mayank Vatsa,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Luka Dragar, Peter Rot, Peter Peer, Vitomir Struc, and Borut Batagelj. 2024. W-\n[13]",
          "Shreya Ghosh et al.": "and Richa Singh. 2023. Df-platter: Multi-face heterogeneous deepfake dataset. In"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "TDL: Window-Based Temporal Deepfake Localization. In Proceedings of the 2nd",
          "Shreya Ghosh et al.": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "International Workshop on Multimodal and Responsible Affective Computing. 0–0.",
          "Shreya Ghosh et al.": "9739–9748."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[14]\nJulien Epps. 2024. Wearable Sensing for Longitudinal Automatic Task Analysis.",
          "Shreya Ghosh et al.": "[37] Cheul Young Park, Narae Cha, Soowon Kang, Auk Kim, Ahsan Habib Khandoker,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "In Proceedings of the 2nd International Workshop on Multimodal and Responsible",
          "Shreya Ghosh et al.": "Leontios Hadjileontiadis, Alice Oh, Yong Jeong, and Uichin Lee. 2020. K-EmoCon,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Affective Computing. 1–2.",
          "Shreya Ghosh et al.": "a multimodal sensor dataset for continuous emotion recognition in naturalistic"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[15]\nShreya Ghosh and Tarique Anwar. 2021. Depression intensity estimation via",
          "Shreya Ghosh et al.": "conversations. Scientific Data 7, 1 (2020), 293."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "social media: A deep learning approach.\nIEEE Transactions on Computational",
          "Shreya Ghosh et al.": "[38] Rosalind W Picard. 2000. Affective computing. MIT press."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Social Systems 8, 6 (2021), 1465–1474.",
          "Shreya Ghosh et al.": "Jia Qing Poh, John See, Neamat El Gayar, and Lai-Kuan Wong. 2024. Are You\n[39]"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[16]\nShreya Ghosh, Zhixi Cai, Parul Gupta, Garima Sharma, Abhinav Dhall, Munawar",
          "Shreya Ghosh et al.": "Paying Attention? Multimodal Linear Attention Transformers for Affect Predic-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Hayat, and Tom Gedeon. 2023.\nEmolysis: A multimodal open-source group",
          "Shreya Ghosh et al.": "tion in Video Conversations. In Proceedings of the 2nd International Workshop on"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "emotion analysis and visualization toolkit. ACIIW (2023).",
          "Shreya Ghosh et al.": "Multimodal and Responsible Affective Computing. 0–0."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Shreya Ghosh and Abhinav Dhall. 2018. Role of group level affect to find the\n[17]",
          "Shreya Ghosh et al.": "[40] Abdallah Hussein Sham, Kadir Aktas, Davit Rizhinashvili, Danila Kuklianov, Fatih"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "most influential person in images. In Proceedings of the European Conference on",
          "Shreya Ghosh et al.": "Alisinanoglu, Ikechukwu Ofodile, Cagri Ozcinar, and Gholamreza Anbarjafari."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Computer Vision (ECCV) Workshops. 0–0.",
          "Shreya Ghosh et al.": "2023. Ethical AI in facial expression analysis: racial bias. Signal, Image and Video"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[18]\nShreya Ghosh, Abhinav Dhall, Munawar Hayat, Jarrod Knibbe, and Qiang Ji.",
          "Shreya Ghosh et al.": "Processing 17, 2 (2023), 399–406."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "2023. Automatic gaze analysis: A survey of deep learning based approaches.\nIEEE",
          "Shreya Ghosh et al.": "[41] Garima Sharma, Shreya Ghosh, and Abhinav Dhall. 2019. Automatic group level"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Transactions on Pattern Analysis and Machine Intelligence 46, 1 (2023), 61–84.",
          "Shreya Ghosh et al.": "affect and cohesion prediction in videos. In 2019 8th International Conference on"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[19]\nShreya Ghosh, Abhinav Dhall, and Nicu Sebe. 2018. Automatic group affect",
          "Shreya Ghosh et al.": "Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "analysis in images via visual attribute and feature networks. In 2018 25th IEEE",
          "Shreya Ghosh et al.": "IEEE, 161–167."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "International Conference on Image Processing (ICIP). IEEE, 1967–1971.",
          "Shreya Ghosh et al.": "[42] Garima Sharma, Shreya Ghosh, Abhinav Dhall, Munawar Hayat, Jianfei Cai, and"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Shreya Ghosh, Abhinav Dhall, Nicu Sebe, and Tom Gedeon. 2019. Predicting\n[20]",
          "Shreya Ghosh et al.": "Tom Gedeon. 2023. GraphITTI: Attributed Graph-based Dominance Ranking"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "group cohesiveness in images. In 2019 International Joint Conference on Neural",
          "Shreya Ghosh et al.": "in Social Interaction Videos. In Companion Publication of the 25th International"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Networks (IJCNN). IEEE, 1–8.",
          "Shreya Ghosh et al.": "Conference on Multimodal Interaction. 323–329."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[21]\nShreya Ghosh, Abhinav Dhall, Nicu Sebe, and Tom Gedeon. 2020. Automatic",
          "Shreya Ghosh et al.": "[43] Nandita Sharma, Abhinav Dhall, Tom Gedeon, and Roland Goecke. 2013. Mod-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "prediction of group cohesiveness in images.\nIEEE Transactions on Affective",
          "Shreya Ghosh et al.": "eling stress using thermal facial patterns: A spatio-temporal approach. In 2013"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Computing 13, 3 (2020), 1677–1690.",
          "Shreya Ghosh et al.": "Humaine Association Conference on Affective Computing and Intelligent Interaction."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[22]\nShaswat Gupta, Srishti Saxena, Vandana Verma, and DK Nishad. 2024. Facial",
          "Shreya Ghosh et al.": "IEEE, 387–392."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Emotion Recognition for Virtual Customer Service Agents. In 2024 International",
          "Shreya Ghosh et al.": "[44] Gizem Sogancioglu, Heysem Kaya, and Albert Ali Salah. 2023. The effects of"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Conference on Communication, Computer Sciences and Engineering (IC3SE). IEEE,",
          "Shreya Ghosh et al.": "gender bias in word embeddings on patient phenotyping in the mental health do-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "321–326.",
          "Shreya Ghosh et al.": "main. In 2023 11th International Conference on Affective Computing and Intelligent"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[23] Md Rakibul Hasan, Md Zakir Hossain, Shreya Ghosh, Susannah Soon, and Tom",
          "Shreya Ghosh et al.": "Interaction (ACII). IEEE, 1–8."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Gedeon. 2023. Empathy detection using machine learning on text, audiovisual,",
          "Shreya Ghosh et al.": "[45] Gizem Sogancioglu, Heysem Kaya, and Albert Ali Salah. 2023. Using Explain-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "audio or physiological signals. arXiv preprint arXiv:2311.00721 (2023).",
          "Shreya Ghosh et al.": "ability for Bias Mitigation: A Case Study for Fair Recruitment Assessment. In"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[24]\nSimindokht Jahangard, Zhixi Cai, Shiki Wen, and Hamid Rezatofighi. 2024. JRDB-",
          "Shreya Ghosh et al.": "Proceedings of the 25th International Conference on Multimodal Interaction. 631–"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynam-",
          "Shreya Ghosh et al.": "639."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "ics of Human Interactions Within Social Groups. In Proceedings of the IEEE/CVF",
          "Shreya Ghosh et al.": "Joana Sousa, Salvador Santos, Luis André, and João Ferreira. 2024. Converging\n[46]"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Conference on Computer Vision and Pattern Recognition. 22087–22097.",
          "Shreya Ghosh et al.": "Affective Computing and Ethical Challenges: The Quest for Universal Access in"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Jian Jiang, Viswonathan Manoranjan, Hanan Salam, and Oya Celiktutan. 2023.\n[25]",
          "Shreya Ghosh et al.": "Human-Machine Cooperation. In International Conference on Human-Computer"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Generalised bias mitigation for personality computing.\nIn Proceedings of\nthe",
          "Shreya Ghosh et al.": "Interaction. Springer, 106–121."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "1st International Workshop on Multimodal and Responsible Affective Computing.",
          "Shreya Ghosh et al.": "[47]\nFlorian van der Steen, Fré Vink, and Heysem Kaya. 2023. Privacy Constrained"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "39–50.",
          "Shreya Ghosh et al.": "Fairness Estimation for Decision Trees. arXiv preprint arXiv:2312.08413 (2023)."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[26]\nJian Jiang, Viswonathan Manoranjan, Hanan Salam, and Oya Celiktutan. 2024.",
          "Shreya Ghosh et al.": "[48] Heting Wang, Vidya Gaddy, James Ross Beveridge, and Francisco R Ortega. 2021."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Towards Generalised and Incremental Bias Mitigation in Personality Computing.",
          "Shreya Ghosh et al.": "Building an emotionally responsive avatar with dynamic facial expressions in"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "IEEE Transactions on Affective Computing (2024).",
          "Shreya Ghosh et al.": "human—computer interactions. Multimodal Technologies and Interaction 5, 3"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[27]\nLI Jingting, Su-Jing Wang, Moi Hoon Yap, John See, Xiaopeng Hong, and Xiaobai",
          "Shreya Ghosh et al.": "(2021), 13."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Li. 2020. Megc2020-the third facial micro-expression grand challenge. In 2020",
          "Shreya Ghosh et al.": "[49] Yu Yang, Aayush Gupta, Jianwei Feng, Prateek Singhal, Vivek Yadav, Yue Wu,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "15th IEEE International Conference on Automatic Face and Gesture Recognition (FG",
          "Shreya Ghosh et al.": "Pradeep Natarajan, Varsha Hedau, and Jungseock Joo. 2022. Enhancing fairness"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "2020). IEEE, 777–780.",
          "Shreya Ghosh et al.": "in face detection in computer vision systems by demographic bias mitigation. In"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[28] Dimitrios Kollias and Stefanos Zafeiriou. 2019.\nExpression, Affect, Action",
          "Shreya Ghosh et al.": "Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. 813–822."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace. arXiv preprint",
          "Shreya Ghosh et al.": "[50] Wu yang Chen, Yanjie Sun, Kele Xu, and Yong Dou. 2024. THE-FD: Task Hierar-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "arXiv:1910.04855 (2019).",
          "Shreya Ghosh et al.": "chical Emotion-aware for Fake Detection. In Proceedings of the 2nd International"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[29] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav Dhall, Shreya Ghosh,",
          "Shreya Ghosh et al.": "Workshop on Multimodal and Responsible Affective Computing. 0–0."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Chunchang Shao, and Guanyu Hu. 2024.\n7th abaw competition: Multi-task",
          "Shreya Ghosh et al.": "[51]\nYue Yao, Jo Plested, and Tom Gedeon. 2018. Deep feature learning and visualiza-"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "learning and compound expression recognition. arXiv preprint arXiv:2407.03835",
          "Shreya Ghosh et al.": "tion for EEG recording using autoencoders. In Neural Information Processing: 25th"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "(2024).",
          "Shreya Ghosh et al.": "International Conference, ICONIP 2018, Siem Reap, Cambodia, December 13–16,"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "[30]\nJingting Li, Moi Hoon Yap, Wen-Huang Cheng, John See, Xiaopeng Hong, Xiaobai",
          "Shreya Ghosh et al.": "2018, Proceedings, Part VII 25. Springer, 554–566."
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Li, Su-Jing Wang, Adrian K Davison, Yante Li, and Zizhao Dong. 2022. Megc2022:",
          "Shreya Ghosh et al.": "[52] Xuanying Zhu, Tom Gedeon, Sabrina Caldwell, and Richard Jones. 2019. Detecting"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "Acm multimedia 2022 micro-expression grand challenge. In Proceedings of the",
          "Shreya Ghosh et al.": "emotional reactions to videos of depression.\nIn 2019 IEEE 23rd International"
        },
        {
          "MRAC ’24, November 1, 2024, Melbourne, VIC, Australia.": "30th ACM International Conference on Multimedia. 7170–7174.",
          "Shreya Ghosh et al.": "Conference on Intelligent Engineering Systems (INES). IEEE, 000147–000152."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Guidelines for human-AI interaction",
      "authors": [
        "Saleema Amershi",
        "Dan Weld",
        "Mihaela Vorvoreanu",
        "Adam Fourney",
        "Besmira Nushi",
        "Penny Collisson",
        "Jina Suh",
        "Shamsi Iqbal",
        "Paul Bennett",
        "Kori Inkpen"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 chi conference on human factors in computing systems"
    },
    {
      "citation_id": "2",
      "title": "A wide evaluation of ChatGPT on affective computing tasks",
      "authors": [
        "Rui Mostafa M Amin",
        "Erik Mao",
        "Björn Cambria",
        "Schuller"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Can Expression Sensitivity Improve Macro-and Micro-Expression Spotting in Long Videos",
      "authors": [
        "Mengjiong Bai",
        "Roland Goecke"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Seeing in 3D: Assistive Robotics with Advanced Computer Vision",
      "authors": [
        "Mohammed Bennamoun"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "2024. 1M-Deepfakes Detection Challenge",
      "authors": [
        "Zhixi Cai",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Munawar Hayat",
        "Dimitrios Kollias",
        "Kalin Stefanov",
        "Usman Tariq"
      ],
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "6",
      "title": "AV-Deepfake1M: A large-scale LLM-driven audio-visual deepfake dataset",
      "authors": [
        "Zhixi Cai",
        "Shreya Ghosh",
        "Aman Pankaj Adatia",
        "Munawar Hayat",
        "Abhinav Dhall",
        "Kalin Stefanov"
      ],
      "year": "2024",
      "venue": "AV-Deepfake1M: A large-scale LLM-driven audio-visual deepfake dataset"
    },
    {
      "citation_id": "7",
      "title": "Glitch in the matrix: A large scale benchmark for content driven audio-visual forgery detection and localization",
      "authors": [
        "Zhixi Cai",
        "Shreya Ghosh",
        "Abhinav Dhall",
        "Tom Gedeon",
        "Kalin Stefanov",
        "Munawar Hayat"
      ],
      "year": "2023",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "8",
      "title": "Marlin: Masked autoencoder for facial video representation learning",
      "authors": [
        "Zhixi Cai",
        "Shreya Ghosh",
        "Kalin Stefanov",
        "Abhinav Dhall",
        "Jianfei Cai",
        "Hamid Rezatofighi",
        "Reza Haffari",
        "Munawar Hayat"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Do you really mean that? content driven audio-visual deepfake dataset and multimodal method for temporal forgery localization",
      "authors": [
        "Zhixi Cai",
        "Kalin Stefanov",
        "Abhinav Dhall",
        "Munawar Hayat"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA)"
    },
    {
      "citation_id": "10",
      "title": "Toward explainable affective computing: A review",
      "authors": [
        "Karina Cortiñas",
        "- Lorenzo",
        "Gerard Lacey"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "11",
      "title": "Ethical considerations on affective computing: an overview",
      "authors": [
        "Laurence Devillers",
        "Roddy Cowie"
      ],
      "year": "2023",
      "venue": "Proc. IEEE"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition in the wild",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Tom Gedeon",
        "Nicu Sebe"
      ],
      "year": "2016",
      "venue": "Emotion recognition in the wild"
    },
    {
      "citation_id": "13",
      "title": "W-TDL: Window-Based Temporal Deepfake Localization",
      "authors": [
        "Luka Dragar",
        "Peter Rot",
        "Peter Peer",
        "Vitomir Struc",
        "Borut Batagelj"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Wearable Sensing for Longitudinal Automatic Task Analysis",
      "authors": [
        "Julien Epps"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Depression intensity estimation via social media: A deep learning approach",
      "authors": [
        "Shreya Ghosh",
        "Tarique Anwar"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "16",
      "title": "Emolysis: A multimodal open-source group emotion analysis and visualization toolkit",
      "authors": [
        "Shreya Ghosh",
        "Zhixi Cai",
        "Parul Gupta",
        "Garima Sharma",
        "Abhinav Dhall",
        "Munawar Hayat",
        "Tom Gedeon"
      ],
      "year": "2023",
      "venue": "ACIIW"
    },
    {
      "citation_id": "17",
      "title": "Role of group level affect to find the most influential person in images",
      "authors": [
        "Shreya Ghosh",
        "Abhinav Dhall"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "18",
      "title": "Automatic gaze analysis: A survey of deep learning based approaches",
      "authors": [
        "Shreya Ghosh",
        "Abhinav Dhall",
        "Munawar Hayat",
        "Jarrod Knibbe",
        "Qiang Ji"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Automatic group affect analysis in images via visual attribute and feature networks",
      "authors": [
        "Shreya Ghosh",
        "Abhinav Dhall",
        "Nicu Sebe"
      ],
      "year": "2018",
      "venue": "2018 25th IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "20",
      "title": "Predicting group cohesiveness in images",
      "authors": [
        "Shreya Ghosh",
        "Abhinav Dhall",
        "Nicu Sebe",
        "Tom Gedeon"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "21",
      "title": "Automatic prediction of group cohesiveness in images",
      "authors": [
        "Shreya Ghosh",
        "Abhinav Dhall",
        "Nicu Sebe",
        "Tom Gedeon"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Facial Emotion Recognition for Virtual Customer Service Agents",
      "authors": [
        "Shaswat Gupta",
        "Srishti Saxena",
        "Vandana Verma",
        "Nishad"
      ],
      "year": "2024",
      "venue": "2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)"
    },
    {
      "citation_id": "23",
      "title": "Empathy detection using machine learning on text, audiovisual, audio or physiological signals",
      "authors": [
        "Md Md Rakibul Hasan",
        "Shreya Zakir Hossain",
        "Susannah Ghosh",
        "Tom Soon",
        "Gedeon"
      ],
      "year": "2023",
      "venue": "Empathy detection using machine learning on text, audiovisual, audio or physiological signals",
      "arxiv": "arXiv:2311.00721"
    },
    {
      "citation_id": "24",
      "title": "JRDB-Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Interactions Within Social Groups",
      "authors": [
        "Simindokht Jahangard",
        "Zhixi Cai",
        "Shiki Wen",
        "Hamid Rezatofighi"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Generalised bias mitigation for personality computing",
      "authors": [
        "Jian Jiang",
        "Viswonathan Manoranjan",
        "Hanan Salam",
        "Oya Celiktutan"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Towards Generalised and Incremental Bias Mitigation in Personality Computing",
      "authors": [
        "Jian Jiang",
        "Viswonathan Manoranjan",
        "Hanan Salam",
        "Oya Celiktutan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Megc2020-the third facial micro-expression grand challenge",
      "authors": [
        "Su-Jing Li Jingting",
        "Moi Wang",
        "John Hoon Yap",
        "Xiaopeng See",
        "Xiaobai Hong",
        "Li"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "28",
      "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "29",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "30",
      "title": "Megc2022: Acm multimedia 2022 micro-expression grand challenge",
      "authors": [
        "Jingting Li",
        "Moi Hoon Yap",
        "Wen-Huang Cheng",
        "John See",
        "Xiaopeng Hong",
        "Xiaobai Li",
        "Su-Jing Wang",
        "Adrian Davison",
        "Yante Li",
        "Zizhao Dong"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "32",
      "title": "PLAAN: pain level assessment with anomaly-detection based network",
      "authors": [
        "Yi Li",
        "Shreya Ghosh",
        "Jyoti Joshi"
      ],
      "year": "2021",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "33",
      "title": "Disguising Personal Identity Information in EEG Signals",
      "authors": [
        "Shiya Liu",
        "Yue Yao",
        "Chaoyue Xing",
        "Tom Gedeon"
      ],
      "year": "2020",
      "venue": "Neural Information Processing: 27th International Conference"
    },
    {
      "citation_id": "34",
      "title": "Affective Computing for Healthcare: Recent Trends, Applications, Challenges, and Beyond",
      "authors": [
        "Yuanyuan Liu",
        "Ke Wang",
        "Lin Wei",
        "Jingying Chen",
        "Yibing Zhan",
        "Dapeng Tao",
        "Zhe Chen"
      ],
      "year": "2024",
      "venue": "Affective Computing for Healthcare: Recent Trends, Applications, Challenges, and Beyond",
      "arxiv": "arXiv:2402.13589"
    },
    {
      "citation_id": "35",
      "title": "Gpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing",
      "authors": [
        "Hao Lu",
        "Xuesong Niu",
        "Jiyao Wang",
        "Yin Wang",
        "Qingyong Hu",
        "Jiaqi Tang",
        "Yuting Zhang",
        "Kaishen Yuan",
        "Bin Huang",
        "Zitong Yu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Df-platter: Multi-face heterogeneous deepfake dataset",
      "authors": [
        "Kartik Narayan",
        "Harsh Agarwal",
        "Kartik Thakral",
        "Surbhi Mittal",
        "Mayank Vatsa",
        "Richa Singh"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "Young Cheul",
        "Narae Park",
        "Soowon Cha",
        "Auk Kang",
        "Ahsan Kim",
        "Leontios Habib Khandoker",
        "Alice Hadjileontiadis",
        "Yong Oh",
        "Uichin Jeong",
        "Lee"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "38",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "39",
      "title": "Are You Paying Attention? Multimodal Linear Attention Transformers for Affect Prediction in Video Conversations",
      "authors": [
        "Qing Jia",
        "John Poh",
        "Neamat See",
        "Lai-Kuan Gayar",
        "Wong"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Ethical AI in facial expression analysis: racial bias. Signal, Image and Video Processing",
      "authors": [
        "Abdallah Hussein Sham",
        "Kadir Aktas",
        "Davit Rizhinashvili",
        "Danila Kuklianov",
        "Fatih Alisinanoglu",
        "Ikechukwu Ofodile",
        "Cagri Ozcinar",
        "Gholamreza Anbarjafari"
      ],
      "year": "2023",
      "venue": "Ethical AI in facial expression analysis: racial bias. Signal, Image and Video Processing"
    },
    {
      "citation_id": "41",
      "title": "Automatic group level affect and cohesion prediction in videos",
      "authors": [
        "Garima Sharma",
        "Shreya Ghosh",
        "Abhinav Dhall"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "42",
      "title": "GraphITTI: Attributed Graph-based Dominance Ranking in Social Interaction Videos",
      "authors": [
        "Garima Sharma",
        "Shreya Ghosh",
        "Abhinav Dhall",
        "Munawar Hayat",
        "Jianfei Cai",
        "Tom Gedeon"
      ],
      "year": "2023",
      "venue": "Companion Publication of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "43",
      "title": "Modeling stress using thermal facial patterns: A spatio-temporal approach",
      "authors": [
        "Nandita Sharma",
        "Abhinav Dhall",
        "Tom Gedeon",
        "Roland Goecke"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "44",
      "title": "The effects of gender bias in word embeddings on patient phenotyping in the mental health domain",
      "authors": [
        "Gizem Sogancioglu",
        "Heysem Kaya",
        "Albert Salah"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "45",
      "title": "Using Explainability for Bias Mitigation: A Case Study for Fair Recruitment Assessment",
      "authors": [
        "Gizem Sogancioglu",
        "Heysem Kaya",
        "Albert Salah"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "46",
      "title": "Converging Affective Computing and Ethical Challenges: The Quest for Universal Access in Human-Machine Cooperation",
      "authors": [
        "Joana Sousa",
        "Salvador Santos",
        "Luis André",
        "João Ferreira"
      ],
      "year": "2024",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "47",
      "title": "Privacy Constrained Fairness Estimation for Decision Trees",
      "authors": [
        "Florian Van Der Steen",
        "Fré Vink",
        "Heysem Kaya"
      ],
      "year": "2023",
      "venue": "Privacy Constrained Fairness Estimation for Decision Trees",
      "arxiv": "arXiv:2312.08413"
    },
    {
      "citation_id": "48",
      "title": "Building an emotionally responsive avatar with dynamic facial expressions in human-computer interactions",
      "authors": [
        "Heting Wang",
        "Vidya Gaddy",
        "James Beveridge",
        "Francisco Ortega"
      ],
      "year": "2021",
      "venue": "Multimodal Technologies and Interaction"
    },
    {
      "citation_id": "49",
      "title": "Enhancing fairness in face detection in computer vision systems by demographic bias mitigation",
      "authors": [
        "Yu Yang",
        "Aayush Gupta",
        "Jianwei Feng",
        "Prateek Singhal",
        "Vivek Yadav",
        "Yue Wu",
        "Pradeep Natarajan",
        "Varsha Hedau",
        "Jungseock Joo"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "50",
      "title": "THE-FD: Task Hierarchical Emotion-aware for Fake Detection",
      "authors": [
        "Yanjie Wu Yang Chen",
        "Kele Sun",
        "Yong Xu",
        "Dou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "Deep feature learning and visualization for EEG recording using autoencoders",
      "authors": [
        "Yue Yao",
        "Jo Plested",
        "Tom Gedeon"
      ],
      "year": "2018",
      "venue": "Neural Information Processing: 25th International Conference"
    },
    {
      "citation_id": "52",
      "title": "Detecting emotional reactions to videos of depression",
      "authors": [
        "Xuanying Zhu",
        "Tom Gedeon",
        "Sabrina Caldwell",
        "Richard Jones"
      ],
      "year": "2019",
      "venue": "IEEE 23rd International Conference on Intelligent Engineering Systems (INES)"
    }
  ]
}