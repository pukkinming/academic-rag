{
  "paper_id": "2507.21778v1",
  "title": "Au-Llm: Micro-Expression Action Unit Detection Via Enhanced Llm-Based Feature Fusion",
  "published": "2025-07-29T13:01:59Z",
  "authors": [
    "Zhishu Liu",
    "Kaishen Yuan",
    "Bo Zhao",
    "Yong Xu",
    "Zitong Yu"
  ],
  "keywords": [
    "Micro-Expression",
    "Action Unit Detection",
    "Large Language Model",
    "Feature Fusion",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The detection of micro-expression Action Units (AUs) is a formidable challenge in affective computing, pivotal for decoding subtle, involuntary human emotions. While Large Language Models (LLMs) demonstrate profound reasoning abilities, their application to the finegrained, low-intensity domain of micro-expression AU detection remains unexplored. This paper pioneers this direction by introducing AU-LLM, a novel framework that for the first time uses LLM to detect AUs in micro-expression datasets with subtle intensities and the scarcity of data. We specifically address the critical vision-language semantic gap, the Enhanced Fusion Projector (EFP). The EFP employs a Multi-Layer Perceptron (MLP) to intelligently fuse mid-level (local texture) and highlevel (global semantics) visual features from a specialized 3D-CNN backbone into a single, information-dense token. This compact representation effectively empowers the LLM to perform nuanced reasoning over subtle facial muscle movements.Through extensive evaluations on the benchmark CASME II and SAMM datasets, including stringent Leave-One-Subject-Out (LOSO) and cross-domain protocols, AU-LLM establishes a new state-of-the-art, validating the significant potential and robustness of LLM-based reasoning for micro-expression analysis. The codes are available at Link.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial Action Units (AUs), defined by the Facial Action Coding System (FACS)  [5] , are the atomic components of facial expressions, making their detection fundamental to affective computing. Unlike overt macro-expressions, micro-expressions are brief, low-intensity, involuntary muscle twitches that betray concealed emotions. Their fleeting nature creates an extremely low signal-to-noise ratio, demanding exceptional model sensitivity to capture faint visual cues. While traditional deep learning models like CNNs and Transformers have advanced AU detection  [14, 25, 28] , they often lack the capacity to reason about the complex interplay of AUs in such subtle conditions.\n\nThe ascendancy of Large Language Models (LLMs) has established a new paradigm for complex reasoning  [1, 2] , offering a promising avenue for interpreting the 'grammar' of facial movements. However, applying this potential to microexpression AU detection is challenged by the semantic gap between continuous visual features and the LLM's discrete token space  [10] . A naive projection risks losing the critical low-intensity signals, raising the question: how can we distill the essence of a fleeting facial movement into a representation an LLM can effectively reason about?\n\nTo bridge this gap, we introduce AU-LLM, the first framework to leverage LLM reasoning for micro-expression AU detection. Our approach features the Enhanced Fusion Projector (EFP), an MLP-based module designed to fuse crucial mid-level (local texture) and high-level (global context) visual features into a single, information-dense token. This provides the LLM with a rich, distilled representation for nuanced reasoning, and we adapt the model efficiently using Low-Rank Adaptation (LoRA)  [7] . Extensive experiments from both within-domain (LOSO) and cross-domain perspectives demonstrate that AU-LLM significantly outperforms state-of-the-art methods, showcasing its powerful generalization and reasoning capabilities. Our contributions can be summarized as follows:\n\n-We propose AU-LLM, the first framework to successfully apply LLM-based reasoning to the challenging task of micro-expression AU detection. -We introduce the EFP module, an effective method for fusing multi-level visual features into a compact token to combine crucial local textural details with global semantic context, thereby bridging the vision-language gap more effectively. -Our model outperforms state-of-the-art methods on the CASME II and SAMM datasets, validated through comprehensive experiments from both withindomain (LOSO) and cross-domain perspectives.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Micro-Expression AU Detection. Automated Facial Action Unit (AU) detection is a core task in affective computing  [21] . While detecting AUs in macroexpressions is well-established, micro-expression AU detection is significantly more challenging due to their fleeting, low-intensity, and involuntary nature  [23] .\n\nTo overcome this, recent works enhance feature learning by either improving the visual signal itself (e.g., LED  [20] , InfuseNet  [9] ) or learning more discriminative representations via attention (e.g., SCA  [11] ) and advanced training strategies like knowledge distillation (e.g., DVASP  [12] ) and contrastive learning (e.g., IICL  [13] ). Despite these advances, their primary focus is on refining the visual representation for a simple final classifier, limiting high-level semantic reasoning. Fig.  1 : The overall framework of our proposed AU-LLM. A video sequence is first processed by the visual backbone, which uses a LED matrix  [20]  and 3D-CNNs to extract multi-level features (F mid and F high ). The Enhanced Fusion Projector (EFP) then fuses these features via concatenation and an MLP into a single, information-dense visual token (T v ). This visual token is combined with a text prompt and fed to a LoRA-tuned LLM for reasoning and final AU classification.\n\nVision-Language Fusion for LLMs. Large Language Models (LLMs) are massive, transformer-based networks pre-trained on text corpora, demonstrating emergent capabilities in complex reasoning that form the foundation for modern AI  [2] . A significant research frontier is extending their reasoning to multi-modal contexts like vision. This adaptation is made feasible by parameter-efficient finetuning (PEFT) techniques such as Low-Rank Adaptation (LoRA)  [7] , which we employ. A primary bottleneck remains the effective translation of rich visual information into a format LLMs can process, as simply projecting a final global feature often discards critical details. Our proposed enhanced fusion projector (EFP) carves a distinct niche. By using an MLP to learn a non-linear mapping from concatenated multi-level visual features to a single, informationdense token, the EFP creates the high-quality representation necessary for robust LLM-based reasoning.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we present our proposed framework, AU-LLM, designed for enhancing micro-expression facial Action Unit detection by leveraging the reasoning capabilities of Large Language Models (LLMs). As illustrated in Figure  1 , the architecture comprises a visual backbone for multi-level feature extraction, our novel Enhanced Fusion Projector (EFP), an LLM reasoning module, and a final classification head.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual Backbone",
      "text": "The visual backbone is engineered to extract a rich set of spatio-temporal features from an input video sequence V ∈ R T ×H×W , where T = 6 is the number of frames, and H = W = 64 are the spatial dimensions. Temporal Filtering. To amplify subtle motion cues crucial for microexpression AUs, we first apply a custom temporal filtering operation, termed the Laplacian of Exponential of Difference (LED) module  [20] . This module reweights the temporal sequence by applying a learnable filter matrix W ∈ R T ×T , computed as:\n\nwhere α, r 1 , r 2 are learnable parameters initialized based on prior work to capture onset-apex-offset patterns  [20] . The filtered video representation V ′ is obtained via a normalized matrix multiplication, enhancing transient changes.\n\nSpatio-Temporal Feature Extraction. The temporally enhanced sequence V ′ is processed by a 3D-CNN. The network consists of 3D convolutional layers, batch normalization, and dropout. A Squeeze-and-Excitation (SE) Layer is integrated after the second convolution to perform channel-wise feature recalibration, allowing the network to focus on more informative channels for AU detection  [8] . We extract features from two distinct stages of this backbone:\n\n-Mid-level Features (F mid ): Extracted after the second 3D convolutional block and pooling layer, these features retain significant spatial information and describe local textures and shapes  [22] . -High-level Features (F high ): Extracted after a fully-connected layer that processes the flattened mid-level features, these features are more abstract and capture global semantic information about the facial region.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Enhanced Fusion Projector (Efp)",
      "text": "A critical challenge in leveraging LLMs for visual tasks is effectively translating rich, high-dimensional visual data into the compact, discrete token space that LLMs operate on. Simply projecting final-layer features can discard vital midlevel details crucial for fine-grained tasks like AU detection. To address this, we designed the Enhanced Fusion Projector (EFP). It intelligently fuses the multilevel visual features into a single, information-dense token for the LLM. The mid-level and high-level feature vectors are first flattened and concatenated:\n\nThis vector f cat , containing a comprehensive visual summary, is then passed through a dedicated fusion MLP. The MLP learns a powerful non-linear transformation to both fuse the features and project them into the LLM's embedding space:\n\nwhere W 1 , b 1 , W 2 , b 2 are the weights and biases of the MLP, and σ is a nonlinear activation. The resulting visual token, T v , is a highly distilled vector that encapsulates both the local textural details (from F mid ) and the global semantic context (from F high ) of the facial movement. In the next step, this single token acts as a soft prompt or a visual instruction for the LLM. It is prepended to a task-specific text prompt, and the combined sequence of embeddings is fed into the LLM to perform the core reasoning task. This learned fusion is superior to a simple linear projection, as the MLP can model complex interactions between mid-and high-level features, distilling the most salient AU-related information for the final classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Llm Reasoning Module",
      "text": "To steer the LLM's powerful reasoning capabilities towards our specific task, we must provide it with both the visual evidence and the analytical context. This is achieved by prepending the visual token T v , which serves as a soft prompt encapsulating the visual information, to a task-specific text prompt (e.g. Analyze the facial features to classify action units:). This fusion of visual evidence and textual instruction creates a multi-modal input sequence that is subsequently fed into a pre-trained LLM. We efficiently fine-tune the LLM using Low-Rank Adaptation (LoRA), which introduces low-rank matrices into the query and value projections of the self-attention layers for efficient adaptation. Finally, the hidden state of the last token from the LLM, which encapsulates the model's unified reasoning over both visual and textual cues, is passed to a linear classifier to produce the final logits z ∈ R N AU s for each AU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Loss Function And Optimization",
      "text": "To address the severe class imbalance inherent in multi-label AU detection, we employ the Asymmetric Loss (ASL)  [15] . ASL is designed to mitigate the dominance of negative samples by applying different focusing parameters to positive and negative examples. The loss is formulated as:\n\nwhere p ij is the predicted probability and y ij is the ground truth label. The focusing parameters γ + and γ -allow for flexible weighting of samples  [25] . Based on our implementation, we set γ + = 0 and γ -= 4.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets And Protocol",
      "text": "We validate our approach on two publicly available spontaneous micro-expression datasets, both of which are standard benchmarks in the field. CASME II  [24]  contains 247 micro-expression samples from 26 subjects. Captured with a highspeed camera (200 fps) under controlled laboratory conditions, it is one of the most widely used datasets for micro-expression analysis. Following standard protocol, we detect 8 AUs: AU1, AU2, AU4, AU7, AU12, AU14, AU15, and AU17. SAMM  [3]  consists of 159 micro-expression samples from 32 subjects with diverse ethnic backgrounds. This dataset also utilizes a high-speed camera and provides a challenging testbed for evaluating model generalization. We conduct cross-dataset validation on SAMM, detecting 4 AUs common to its annotation: AU2, AU4, AU7, and AU12.\n\nFor fair and robust evaluation, we employ the stringent Leave-One-Subject-Out (LOSO) cross-validation protocol  [20]  to ensure subject-independent evaluation. The primary metric is the macro F1-score, which is well-suited for AU detection tasks with significant class imbalance as it treats each AU class equally  [19] . The F1-score is the harmonic mean of precision and recall, calculated for each action unit class c as:\n\nThe final reported score is the unweighted average of these individual F1-scores across all AU classes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "Our framework, AU-LLM, is implemented in PyTorch. The visual backbone is trained from scratch with random initialization for each fold of the crossvalidation. We use the Adam optimizer with a learning rate of 3 × 10 -5 and a weight decay of 0.005. The model is trained with a batchsize of 256. We utilize several 1.5B-parameter LLMs, including Qwen2-1.5B, Qwen2.5-1.5B, and DeepSeek-R1-Distill-Qwen-1.5B, loaded from Hugging Face  [4] [17] .\n\nParameter-Efficient Fine-Tuning is performed using Low-Rank Adaptation (LoRA) with a rank r = 16 and alpha α = 32 applied to the query and value matrices of the attention blocks. All experiments are run on a single NVIDIA H100 GPU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "We compare AU-LLM with several state-of-the-art (SOTA) methods on both datasets. As shown in Table  1 , our model variants achieve superior performance on CASME II. Notably, AU-deepseek R1(1.5B) achieves a mean F1-score of 81.4%, surpassing the previous best method SSSNet LED (78.4%) by a significant margin of 3.0%. Our method shows particularly strong performance on key AUs like AU2 (87.0%), AU4 (89.5%), and AU7 (68.1%), demonstrating its effectiveness in capturing diverse facial muscle movements. LBP-TOP  [27]  58.8 47.9 44.5 49.5 50.2 ResNet18  [6]  49.7 49.1 46.1 40.5 46.4 ResNet34  [6]  44.0 55.2 38.0 40.5 44.4 Fit-18  [16]  54.1 51.2 44.5 48.3 49.5 SP-18  [18]  42.8 64.2 38.1 49.5 48.7 AT-18  [26]  47.2 60.5 43.5 38.0 47.3 SCA  [11]  45.7 59.2 43.9 53.2 50.5 DVASP-18  [12]  47.8 67.5 48.1 44.7 52.0 Resnet18 LED  [20]  57.  4   The results on the SAMM dataset, shown in Table  2 , further validate the generalization capability of our framework. In this cross-dataset setting, AUdeepseek R1(1.5B) again achieves the highest average F1-score of 61.9%, outperforming all previous methods. This robust performance highlights the benefit of our EFP module in creating a rich, dataset-agnostic visual representation that enables the LLM to reason effectively even on unseen data distributions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "Ablation on Modules. To validate our key design choices, we conducted ablation studies on the CASME II and SAMM datasets. We designed three variants to isolate the contributions of our core components: (1) using only high-level features (F high ) or (  2 ) only mid-level features (F mid ) to test the necessity of fusing multi-level information, and (3) EFP-, which replaces the EFP's MLP with a simple linear layer, to verify the benefit of non-linear fusion. As shown in Table  3 , the full model (all) consistently outperforms all ablated versions across both datasets. The performance drop when using single-level features confirms that both local and global cues are vital for capturing the full spectrum of AU-related movements. The superiority over the EFP-variant highlights the importance of the MLP's non-linear fusion capability. These results are complemented by the   visualizations in Figure  2 , which provide a graphical representation of the performance differences.\n\nAblation on Prompting Strategy. To further validate our dynamic visual prompting strategy, we introduce an ablation study that replaces our EFP module with a strong baseline: Learnable Text Prompts  [29] . This variant uses a set of static, learnable embedding vectors optimized to act as a general textual instruction for the AU detection task, instead of generating a dynamic visual token for each sample. This experiment directly contrasts our \"early fusion\" approach, where rich visual information is integrated before reaching the LLM, with a \"late fusion\" approach. The expected superior performance of our EFP-based model would strongly demonstrate that for fine-grained visual tasks like microexpression detection, dynamically generating an instance-specific visual prompt is a more effective strategy than using a static, task-level textual prompt, thus validating the design of our EFP module.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Cross-Domain Evaluation",
      "text": "To further assess the model's generalization capability, we conducted bidirectional cross-domain evaluations between the CASME II and SAMM datasets on their shared AUs. As presented in",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Visualization And Analysis",
      "text": "To intuitively understand how our model's visual backbone processes facial information, we visualize the feature heatmaps and compare them against a baseline 3D-CNN. As shown in Figure  3 , our model demonstrates a superior ability to focus on the correct AU-related facial regions. The qualitative analysis in Figure  3  provides strong evidence for our model's effectiveness. For both a simple case (AU12, top row) and a complex combination (AU4+15+17, bottom row), our model's heatmaps (c) accurately localize the corresponding facial regions. In contrast, the baseline model (b) shows diffuse or incorrect attention. Furthermore, the SE-Layer's channel attention maps (d) illustrate the model's ability to dynamically re-weight feature channels to amplify the most informative signals. This confirms that our visual backbone learns a more precise and interpretable feature representation, providing high-quality input for the subsequent LLM reasoning.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced AU-LLM, a framework that, for the first time, successfully leverages Large Language Models for the challenging task of microexpression Action Unit detection. Our innovation, the Enhanced Fusion Projector (EFP), effectively bridges the vision-language semantic gap by fusing multi-level visual features into a compact, information-rich token, empowering the LLM to perform nuanced reasoning over subtle facial cues. This work validates the significant potential of applying advanced reasoning engines to this fine-grained domain. For future work, our focus will shift towards fully harnessing the capabilities of LLMs. We aim to build upon this foundation to develop systems capable of more complex micro-expression reasoning and interactive, context-aware question-answering, moving beyond simple classification to a deeper understanding of concealed emotions.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall framework of our proposed AU-LLM. A video sequence is first",
      "page": 3
    },
    {
      "caption": "Figure 2: Visualization of ablation studies on both datasets, showing the perfor-",
      "page": 8
    },
    {
      "caption": "Figure 2: , which provide a graphical representation of the per-",
      "page": 8
    },
    {
      "caption": "Figure 3: Visualization of model attention. (a) Original micro-expression samples.",
      "page": 9
    },
    {
      "caption": "Figure 3: , our model demonstrates a superior ability to",
      "page": 9
    },
    {
      "caption": "Figure 3: provides strong evidence for our model’s",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Great Bay University": "2Harbin Institute of Technology , Shenzhen"
        },
        {
          "1Great Bay University": "Abstract. The detection of micro-expression Action Units\n(AUs)\nis a"
        },
        {
          "1Great Bay University": "formidable challenge in affective computing, pivotal\nfor decoding sub-"
        },
        {
          "1Great Bay University": "tle, involuntary human emotions. While Large Language Models (LLMs)"
        },
        {
          "1Great Bay University": "demonstrate profound reasoning abilities,\ntheir application to the fine-"
        },
        {
          "1Great Bay University": "grained,\nlow-intensity domain of micro-expression AU detection remains"
        },
        {
          "1Great Bay University": "unexplored. This paper pioneers this direction by introducing AU-LLM,"
        },
        {
          "1Great Bay University": "a novel\nframework that\nfor\nthe first\ntime uses LLM to detect AUs\nin"
        },
        {
          "1Great Bay University": "micro-expression datasets with subtle intensities and the scarcity of data."
        },
        {
          "1Great Bay University": "We specifically address the critical vision-language semantic gap, the En-"
        },
        {
          "1Great Bay University": "hanced Fusion Projector (EFP). The EFP employs a Multi-Layer"
        },
        {
          "1Great Bay University": "Perceptron (MLP) to intelligently fuse mid-level (local texture) and high-"
        },
        {
          "1Great Bay University": "level (global semantics) visual features from a specialized 3D-CNN back-"
        },
        {
          "1Great Bay University": "bone into a single, information-dense token. This compact representation"
        },
        {
          "1Great Bay University": "effectively empowers the LLM to perform nuanced reasoning over subtle"
        },
        {
          "1Great Bay University": "facial muscle movements.Through extensive evaluations on the bench-"
        },
        {
          "1Great Bay University": "mark CASME II and SAMM datasets,\nincluding stringent Leave-One-"
        },
        {
          "1Great Bay University": "Subject-Out\n(LOSO) and cross-domain protocols, AU-LLM establishes"
        },
        {
          "1Great Bay University": "a new state-of-the-art, validating the significant potential and robust-"
        },
        {
          "1Great Bay University": "ness of LLM-based reasoning for micro-expression analysis. The codes"
        },
        {
          "1Great Bay University": "are available at Link."
        },
        {
          "1Great Bay University": "Keywords: Micro-Expression, Action Unit Detection, Large Language"
        },
        {
          "1Great Bay University": "Model, Feature Fusion, Affective Computing"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": "facial expressions, making their detection funda-"
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": "involuntary muscle twitches that betray concealed emo-"
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        },
        {
          "Keywords: Micro-Expression, Action Unit Detection, Large Language": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nZhishu Liu et al.": "The ascendancy of Large Language Models\n(LLMs) has established a new"
        },
        {
          "2\nZhishu Liu et al.": "paradigm for complex reasoning [1,2], offering a promising avenue for interpreting"
        },
        {
          "2\nZhishu Liu et al.": "the ’grammar’ of\nfacial movements. However, applying this potential to micro-"
        },
        {
          "2\nZhishu Liu et al.": "expression AU detection is challenged by the semantic gap between continuous"
        },
        {
          "2\nZhishu Liu et al.": "visual features and the LLM’s discrete token space [10]. A naive projection risks"
        },
        {
          "2\nZhishu Liu et al.": "losing the critical\nlow-intensity signals, raising the question: how can we distill"
        },
        {
          "2\nZhishu Liu et al.": "the\nessence of a fleeting facial movement\ninto a representation an LLM can"
        },
        {
          "2\nZhishu Liu et al.": "effectively reason about?"
        },
        {
          "2\nZhishu Liu et al.": "To bridge this gap, we introduce AU-LLM,\nthe first\nframework to lever-"
        },
        {
          "2\nZhishu Liu et al.": "age LLM reasoning for micro-expression AU detection. Our approach features"
        },
        {
          "2\nZhishu Liu et al.": "the Enhanced Fusion Projector\n(EFP), an MLP-based module designed"
        },
        {
          "2\nZhishu Liu et al.": "to fuse crucial mid-level\n(local\ntexture) and high-level\n(global context) visual"
        },
        {
          "2\nZhishu Liu et al.": "features into a single,\ninformation-dense token. This provides the LLM with a"
        },
        {
          "2\nZhishu Liu et al.": "rich, distilled representation for nuanced reasoning, and we adapt the model ef-"
        },
        {
          "2\nZhishu Liu et al.": "ficiently using Low-Rank Adaptation (LoRA)\n[7]. Extensive experiments\nfrom"
        },
        {
          "2\nZhishu Liu et al.": "both within-domain (LOSO) and cross-domain perspectives demonstrate that"
        },
        {
          "2\nZhishu Liu et al.": "AU-LLM significantly outperforms state-of-the-art methods, showcasing its pow-"
        },
        {
          "2\nZhishu Liu et al.": "erful generalization and reasoning capabilities. Our contributions can be sum-"
        },
        {
          "2\nZhishu Liu et al.": "marized as follows:"
        },
        {
          "2\nZhishu Liu et al.": "– We propose AU-LLM,\nthe first\nframework to successfully apply LLM-based"
        },
        {
          "2\nZhishu Liu et al.": "reasoning to the challenging task of micro-expression AU detection."
        },
        {
          "2\nZhishu Liu et al.": "– We\nintroduce\nthe EFP module,\nan effective method for\nfusing multi-level"
        },
        {
          "2\nZhishu Liu et al.": "visual\nfeatures into a compact token to combine crucial\nlocal textural details"
        },
        {
          "2\nZhishu Liu et al.": "with global semantic context, thereby bridging the vision-language gap more"
        },
        {
          "2\nZhishu Liu et al.": "effectively."
        },
        {
          "2\nZhishu Liu et al.": "– Our model outperforms state-of-the-art methods on the CASME II and SAMM"
        },
        {
          "2\nZhishu Liu et al.": "datasets, validated through comprehensive\nexperiments\nfrom both within-"
        },
        {
          "2\nZhishu Liu et al.": "domain (LOSO) and cross-domain perspectives."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Visual backbone": "LLM Reasoning \nEnhanced Fusion Projector"
        },
        {
          "Visual backbone": "Module"
        },
        {
          "Visual backbone": "(EFP)"
        },
        {
          "Visual backbone": "Loss"
        },
        {
          "Visual backbone": "Ground Truth\nFC\nGlobal pool\nReLU\nSigmoid\nFC\nAU Predictions (Logits)"
        },
        {
          "Visual backbone": "Fig. 1: The overall framework of our proposed AU-LLM. A video sequence is first"
        },
        {
          "Visual backbone": "processed by the visual backbone, which uses a LED matrix [20] and 3D-CNNs to"
        },
        {
          "Visual backbone": "extract multi-level\nfeatures\n(Fmid and Fhigh). The Enhanced Fusion Projector"
        },
        {
          "Visual backbone": "(EFP)\nthen fuses\nthese features via concatenation and an MLP into a single,"
        },
        {
          "Visual backbone": "information-dense visual token (Tv). This visual token is combined with a text"
        },
        {
          "Visual backbone": "prompt and fed to a LoRA-tuned LLM for reasoning and final AU classification."
        },
        {
          "Visual backbone": "Vision-Language Fusion for LLMs. Large Language Models\n(LLMs) are"
        },
        {
          "Visual backbone": "massive, transformer-based networks pre-trained on text corpora, demonstrating"
        },
        {
          "Visual backbone": "emergent capabilities in complex reasoning that form the foundation for modern"
        },
        {
          "Visual backbone": "AI [2]. A significant research frontier is extending their reasoning to multi-modal"
        },
        {
          "Visual backbone": "contexts like vision. This adaptation is made feasible by parameter-efficient fine-"
        },
        {
          "Visual backbone": "tuning (PEFT) techniques such as Low-Rank Adaptation (LoRA) [7], which we"
        },
        {
          "Visual backbone": "employ. A primary bottleneck remains the effective translation of rich visual\nin-"
        },
        {
          "Visual backbone": "formation into a format LLMs can process, as simply projecting a final global"
        },
        {
          "Visual backbone": "feature often discards critical details. Our proposed enhanced fusion projec-"
        },
        {
          "Visual backbone": "tor\n(EFP)\ncarves a distinct niche. By using an MLP to learn a non-linear"
        },
        {
          "Visual backbone": "mapping from concatenated multi-level visual\nfeatures to a single,\ninformation-"
        },
        {
          "Visual backbone": "dense token, the EFP creates the high-quality representation necessary for robust"
        },
        {
          "Visual backbone": "LLM-based reasoning."
        },
        {
          "Visual backbone": "3\nMethodology"
        },
        {
          "Visual backbone": "In this\nsection, we present our proposed framework, AU-LLM, designed for"
        },
        {
          "Visual backbone": "enhancing micro-expression facial Action Unit detection by leveraging the rea-"
        },
        {
          "Visual backbone": "soning capabilities of Large Language Models (LLMs). As illustrated in Figure 1,"
        },
        {
          "Visual backbone": "the architecture comprises a visual backbone for multi-level\nfeature extraction,"
        },
        {
          "Visual backbone": "our novel Enhanced Fusion Projector (EFP), an LLM reasoning module, and a"
        },
        {
          "Visual backbone": "final classification head."
        },
        {
          "Visual backbone": "3.1\nVisual Backbone"
        },
        {
          "Visual backbone": "The visual backbone is engineered to extract a rich set of spatio-temporal\nfea-"
        },
        {
          "Visual backbone": "tures from an input video sequence V ∈ RT ×H×W , where T = 6 is the number"
        },
        {
          "Visual backbone": "of\nframes, and H = W = 64 are the spatial dimensions."
        },
        {
          "Visual backbone": "Temporal Filtering. To\namplify\nsubtle motion cues\ncrucial\nfor micro-"
        },
        {
          "Visual backbone": "expression AUs, we first apply a custom temporal filtering operation,\ntermed"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nZhishu Liu et al.": "the Laplacian of Exponential of Difference (LED) module [20]. This module re-"
        },
        {
          "4\nZhishu Liu et al.": "weights the temporal sequence by applying a learnable filter matrix W ∈ RT ×T ,"
        },
        {
          "4\nZhishu Liu et al.": "computed as:"
        },
        {
          "4\nZhishu Liu et al.": "(cid:16)\n(cid:17)"
        },
        {
          "4\nZhishu Liu et al.": "α\nif j > i,\n(1 − r1)j−irmin(1,i)\n− (1 − r2)j−irmin(1,i)"
        },
        {
          "4\nZhishu Liu et al.": " \n(1)\nWi,j ="
        },
        {
          "4\nZhishu Liu et al.": "if j = i,\nα(r1 − r2)"
        },
        {
          "4\nZhishu Liu et al.": "0\nif j < i,"
        },
        {
          "4\nZhishu Liu et al.": "where α, r1, r2 are learnable parameters initialized based on prior work to capture"
        },
        {
          "4\nZhishu Liu et al.": "onset-apex-offset patterns [20]. The filtered video representation V′\nis obtained"
        },
        {
          "4\nZhishu Liu et al.": "via a normalized matrix multiplication, enhancing transient changes."
        },
        {
          "4\nZhishu Liu et al.": "Spatio-Temporal Feature Extraction. The temporally enhanced sequence"
        },
        {
          "4\nZhishu Liu et al.": "V′\nis processed by a 3D-CNN. The network consists of 3D convolutional\nlay-"
        },
        {
          "4\nZhishu Liu et al.": "ers, batch normalization, and dropout. A Squeeze-and-Excitation (SE) Layer is"
        },
        {
          "4\nZhishu Liu et al.": "integrated after\nthe second convolution to perform channel-wise feature recal-"
        },
        {
          "4\nZhishu Liu et al.": "ibration, allowing the network to focus on more informative channels\nfor AU"
        },
        {
          "4\nZhishu Liu et al.": "detection [8]. We extract features from two distinct stages of this backbone:"
        },
        {
          "4\nZhishu Liu et al.": "– Mid-level Features\nthe second 3D convolutional\n(Fmid): Extracted after"
        },
        {
          "4\nZhishu Liu et al.": "block and pooling layer,\nthese features retain significant spatial\ninformation"
        },
        {
          "4\nZhishu Liu et al.": "and describe local textures and shapes [22]."
        },
        {
          "4\nZhishu Liu et al.": "that\n– High-level Features (Fhigh): Extracted after a fully-connected layer"
        },
        {
          "4\nZhishu Liu et al.": "processes\nthe flattened mid-level\nfeatures,\nthese\nfeatures are more abstract"
        },
        {
          "4\nZhishu Liu et al.": "and capture global semantic information about the facial region."
        },
        {
          "4\nZhishu Liu et al.": "3.2\nEnhanced Fusion Projector (EFP)"
        },
        {
          "4\nZhishu Liu et al.": "A critical challenge in leveraging LLMs for visual tasks is effectively translating"
        },
        {
          "4\nZhishu Liu et al.": "rich, high-dimensional visual data into the compact, discrete token space that"
        },
        {
          "4\nZhishu Liu et al.": "LLMs operate on. Simply projecting final-layer features can discard vital mid-"
        },
        {
          "4\nZhishu Liu et al.": "level details crucial\nfor fine-grained tasks like AU detection. To address this, we"
        },
        {
          "4\nZhishu Liu et al.": "designed the Enhanced Fusion Projector (EFP). It intelligently fuses the multi-"
        },
        {
          "4\nZhishu Liu et al.": "level visual\nfeatures\ninto a single,\ninformation-dense\ntoken for\nthe LLM. The"
        },
        {
          "4\nZhishu Liu et al.": "mid-level and high-level\nfeature vectors are first flattened and concatenated:"
        },
        {
          "4\nZhishu Liu et al.": "(2)\nfcat = Concat(Flatten(Fmid), Fhigh)."
        },
        {
          "4\nZhishu Liu et al.": "is then passed\nThis vector fcat, containing a comprehensive visual summary,"
        },
        {
          "4\nZhishu Liu et al.": "through a dedicated fusion MLP. The MLP learns a powerful non-linear trans-"
        },
        {
          "4\nZhishu Liu et al.": "formation to both fuse the features and project them into the LLM’s embedding"
        },
        {
          "4\nZhishu Liu et al.": "space:"
        },
        {
          "4\nZhishu Liu et al.": "(3)\nTv = σ(W2(ReLU(W1fcat + b1)) + b2),"
        },
        {
          "4\nZhishu Liu et al.": "where W1, b1, W2, b2 are the weights and biases of the MLP, and σ is a non-"
        },
        {
          "4\nZhishu Liu et al.": "is a highly distilled vector that\nlinear activation. The resulting visual token, Tv,"
        },
        {
          "4\nZhishu Liu et al.": "encapsulates both the local textural details (from Fmid) and the global semantic"
        },
        {
          "4\nZhishu Liu et al.": "context (from Fhigh) of the facial movement. In the next step, this single token"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "acts as a soft prompt or a visual\ninstruction for the LLM. It is prepended to a"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "task-specific text prompt, and the combined sequence of embeddings is fed into"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "the LLM to perform the core reasoning task. This learned fusion is superior to"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "a simple linear projection, as the MLP can model complex interactions between"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "mid- and high-level\nfeatures, distilling the most salient AU-related information"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "for the final classification."
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "3.3\nLLM Reasoning Module"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "To steer the LLM’s powerful reasoning capabilities towards our specific task, we"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "must provide it with both the visual evidence and the analytical context. This"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "is achieved by prepending the visual\ntoken Tv, which serves as a soft prompt"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "encapsulating the visual information, to a task-specific text prompt (e.g. Analyze"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "the facial\nfeatures to classify action units:). This fusion of visual evidence and"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "textual\ninstruction creates a multi-modal\ninput\nsequence that\nis\nsubsequently"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "fed into a pre-trained LLM. We efficiently fine-tune the LLM using Low-Rank"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "Adaptation (LoRA), which introduces\nlow-rank matrices\ninto the query and"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "value projections of the self-attention layers for efficient adaptation. Finally, the"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "hidden state of\nthe last\ntoken from the LLM, which encapsulates\nthe model’s"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "unified reasoning over both visual and textual cues, is passed to a linear classifier"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "for each AU.\nto produce the final\nlogits z ∈ RNAU s"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "3.4\nLoss Function and Optimization"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "To address the severe class imbalance inherent in multi-label AU detection, we"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "employ the Asymmetric Loss (ASL) [15]. ASL is designed to mitigate the domi-"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "nance of negative samples by applying different focusing parameters to positive"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n5": "and negative examples. The loss is formulated as:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: , our model variants achieve superior performance",
      "data": [
        {
          "Zhishu Liu et al.": "Table 1: Micro-Expression AU Detection Performance Comparison (CASME II)."
        },
        {
          "Zhishu Liu et al.": "F1-scores (%) are reported. Best results are in bold."
        },
        {
          "Zhishu Liu et al.": "Method"
        },
        {
          "Zhishu Liu et al.": "LBPTOP [27]"
        },
        {
          "Zhishu Liu et al.": "Resnet18 [6]"
        },
        {
          "Zhishu Liu et al.": "Resnet34 [6]"
        },
        {
          "Zhishu Liu et al.": "Fitnet [16]"
        },
        {
          "Zhishu Liu et al.": "SP [18]"
        },
        {
          "Zhishu Liu et al.": "AT [26]"
        },
        {
          "Zhishu Liu et al.": "SCA [11]"
        },
        {
          "Zhishu Liu et al.": "DVASP [12]"
        },
        {
          "Zhishu Liu et al.": "Resnet18 LED [20]"
        },
        {
          "Zhishu Liu et al.": "SSSNet LED [20]"
        },
        {
          "Zhishu Liu et al.": "AU-DeepSeek R1(1.5B)"
        },
        {
          "Zhishu Liu et al.": "AU-Qwen2(1.5B)"
        },
        {
          "Zhishu Liu et al.": "AU-Qwen2.5(1.5B)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Micro-Expression AU Detection Performance Comparison (SAMM).",
      "data": [
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": ""
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": ""
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": ""
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "58.8"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "49.7"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "44.0"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "54.1"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "42.8"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "47.2"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "45.7"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "47.8"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "57.4"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "61.5"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "AU-DeepSeek R1(1.5B) 66.9 71.9 55.8"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "63.6"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs": "63.2"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Micro-Expression AU Detection Performance Comparison (SAMM).",
      "data": [
        {
          "Table 3: Ablation study on CASME II and SAMM datasets. Average F1-scores": "(%) are reported. Model names are abbreviated. Best results are in bold."
        },
        {
          "Table 3: Ablation study on CASME II and SAMM datasets. Average F1-scores": ""
        },
        {
          "Table 3: Ablation study on CASME II and SAMM datasets. Average F1-scores": "Variant"
        },
        {
          "Table 3: Ablation study on CASME II and SAMM datasets. Average F1-scores": "Fhigh"
        },
        {
          "Table 3: Ablation study on CASME II and SAMM datasets. Average F1-scores": "Fmid"
        },
        {
          "Table 3: Ablation study on CASME II and SAMM datasets. Average F1-scores": "EFP–"
        },
        {
          "Table 3: Ablation study on CASME II and SAMM datasets. Average F1-scores": "Prompt Learning"
        },
        {
          "Table 3: Ablation study on CASME II and SAMM datasets. Average F1-scores": "all"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: F1-scores (in %) for cross-domain evaluations between CASME II and",
      "data": [
        {
          "Table 4: F1-scores (in %) for cross-domain evaluations between CASME II and": "Datasets"
        },
        {
          "Table 4: F1-scores (in %) for cross-domain evaluations between CASME II and": "AU"
        },
        {
          "Table 4: F1-scores (in %) for cross-domain evaluations between CASME II and": "ResNet-18 [6]"
        },
        {
          "Table 4: F1-scores (in %) for cross-domain evaluations between CASME II and": "LED SSSNet [20]"
        },
        {
          "Table 4: F1-scores (in %) for cross-domain evaluations between CASME II and": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "(b) Heatmaps from a baseline 3D-CNN. (c) Heatmaps from our visual backbone,"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "showing more precise focus. (d) SE-Layer channel attention weights over time."
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "Top row: The ground truth is AU12 (Lip Corner Puller). Bottom row: The"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "ground truth is a combination of AU4 (Brow Lowerer), AU15 (Lip Corner De-"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "pressor), and AU17 (Chin Raiser)."
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "4.6\nVisualization and Analysis"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "To intuitively understand how our model’s visual backbone processes facial infor-"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "mation, we visualize the feature heatmaps and compare them against a baseline"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "3D-CNN. As\nshown in Figure 3, our model demonstrates a superior ability to"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "focus on the correct AU-related facial regions."
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "The qualitative analysis in Figure 3 provides strong evidence for our model’s"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "effectiveness. For both a simple case (AU12, top row) and a complex combination"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "(AU4+15+17, bottom row), our model’s heatmaps\n(c) accurately localize the"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "corresponding facial\nregions.\nIn contrast,\nthe baseline model\n(b)\nshows diffuse"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "or incorrect attention. Furthermore, the SE-Layer’s channel attention maps (d)"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "illustrate the model’s ability to dynamically re-weight feature channels to amplify"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "the most\ninformative\nsignals. This\nconfirms\nthat our visual backbone\nlearns"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "a more precise and interpretable feature representation, providing high-quality"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "input for the subsequent LLM reasoning."
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "5\nConclusion"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "In this paper, we introduced AU-LLM, a framework that,\nfor\nthe first\ntime,"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "successfully leverages Large Language Models for the challenging task of micro-"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "expression Action Unit detection. Our innovation, the Enhanced Fusion Pro-"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "jector\n(EFP), effectively bridges\nthe vision-language semantic gap by fusing"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "multi-level visual\nfeatures into a compact,\ninformation-rich token, empowering"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "the LLM to perform nuanced reasoning over subtle facial cues. This work val-"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "idates\nthe significant potential of applying advanced reasoning engines\nto this"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "fine-grained domain. For\nfuture work, our\nfocus will\nshift\ntowards\nfully har-"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "nessing the capabilities of LLMs. We aim to build upon this foundation to de-"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "velop systems capable of more complex micro-expression reasoning and interac-"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "tive, context-aware question-answering, moving beyond simple classification to"
        },
        {
          "Fig. 3: Visualization of model attention. (a) Original micro-expression samples.": "a deeper understanding of concealed emotions."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10\nZhishu Liu et al.": "References"
        },
        {
          "10\nZhishu Liu et al.": "1. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr,\nI., Hasson, Y., Lenc, K.,"
        },
        {
          "10\nZhishu Liu et al.": "Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual\nlanguage model"
        },
        {
          "10\nZhishu Liu et al.": "for few-shot learning. NeurIPS (2022)"
        },
        {
          "10\nZhishu Liu et al.": "2. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee-"
        },
        {
          "10\nZhishu Liu et al.": "lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot"
        },
        {
          "10\nZhishu Liu et al.": "learners. NeurIPS (2020)"
        },
        {
          "10\nZhishu Liu et al.": "3. Davison, A.K., Lansley, C., Costen, N., Tan, K., Yap, M.H.: Samm: A spontaneous"
        },
        {
          "10\nZhishu Liu et al.": "micro-facial movement dataset. In: IEEE TAFFC (2018)"
        },
        {
          "10\nZhishu Liu et al.": "4. DeepSeek-AI: Deepseek-r1: A 671b moe model with fine-grained sparsity (2024)"
        },
        {
          "10\nZhishu Liu et al.": "5. Ekman, P., Friesen, W.V.: Facial action coding system. Consulting Psychologists"
        },
        {
          "10\nZhishu Liu et al.": "Press (1978)"
        },
        {
          "10\nZhishu Liu et al.": "6. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual\nlearning for image recognition."
        },
        {
          "10\nZhishu Liu et al.": "In: CVPR (2016)"
        },
        {
          "10\nZhishu Liu et al.": "7. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,"
        },
        {
          "10\nZhishu Liu et al.": "W., et al.: Lora: Low-rank adaptation of\nlarge language models. In: ICLR (2022)"
        },
        {
          "10\nZhishu Liu et al.": "8. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: CVPR (2018)"
        },
        {
          "10\nZhishu Liu et al.": "9. Khor, H.Q., Li, Y., Jiang, X., Zhao, G.: Infused suppression of magnification arte-"
        },
        {
          "10\nZhishu Liu et al.": "facts for micro-au detection. In: SCIA (2025)"
        },
        {
          "10\nZhishu Liu et al.": "10. Li, G., Zhu, X., Zeng, Y., Wang, Q., Lin, L.: Semantic relationships guided repre-"
        },
        {
          "10\nZhishu Liu et al.": "sentation learning for facial action unit recognition. In: AAAI (2019)"
        },
        {
          "10\nZhishu Liu et al.": "11. Li, Y., Huang, X., Zhao, G.: Micro-expression action unit detection with spatial"
        },
        {
          "10\nZhishu Liu et al.": "and channel attention. Neurocomputing (2021)"
        },
        {
          "10\nZhishu Liu et al.": "12. Li, Y., Peng, W., Zhao, G.: Micro-expression action unit detection with dual-view"
        },
        {
          "10\nZhishu Liu et al.": "attentive similarity-preserving knowledge distillation. In: FG (2021)"
        },
        {
          "10\nZhishu Liu et al.": "13. Li, Y., Zhao, G.:\nIntra-and inter-contrastive learning for micro-expression action"
        },
        {
          "10\nZhishu Liu et al.": "unit detection. In: ICMI (2021)"
        },
        {
          "10\nZhishu Liu et al.": "14. Miriam Jacob, G., Stenger, B.: Facial action unit detection with transformers. In:"
        },
        {
          "10\nZhishu Liu et al.": "CVPR (2021)"
        },
        {
          "10\nZhishu Liu et al.": "15. Ridnik, T., Ben-Baruch, E., Zamir, N., Noy, A., Friedman, I., Protter, M., Zelnik-"
        },
        {
          "10\nZhishu Liu et al.": "Manor, L.: Asymmetric loss for multi-label classification. In: ICCV (2021)"
        },
        {
          "10\nZhishu Liu et al.": "16. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gkioxari, G., Bengio, Y.: Fit-"
        },
        {
          "10\nZhishu Liu et al.": "nets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014)"
        },
        {
          "10\nZhishu Liu et al.": "17. Team, Q.: Qwen2 technical report (2024)"
        },
        {
          "10\nZhishu Liu et al.": "18. Tung, F., Mori, G.: Similarity-preserving knowledge distillation. In: ICCV (2017)"
        },
        {
          "10\nZhishu Liu et al.": "19. Varanka, T., Li, Y., Peng, W., Zhao, G.: Data leakage and evaluation issues\nin"
        },
        {
          "10\nZhishu Liu et al.": "micro-expression analysis. IEEE TAFFC (2023)"
        },
        {
          "10\nZhishu Liu et al.": "20. Varanka, T., Peng, W., Zhao, G.: Learnable eulerian dynamics for micro-expression"
        },
        {
          "10\nZhishu Liu et al.": "action unit detection. In: Image Analysis (2024)"
        },
        {
          "10\nZhishu Liu et al.": "21. Xie, H.X., Lo, L., Shuai, H.H., Cheng, W.H.: Au-assisted graph attention convo-"
        },
        {
          "10\nZhishu Liu et al.": "lutional network for micro-expression recognition. In: ACM MM (2020)"
        },
        {
          "10\nZhishu Liu et al.": "22. Xie, Y., Zhao, B., Dai, M., Zhou, J.P., Sun, Y., Tan, T., Xie, W., Shen, L., Yu, Z.:"
        },
        {
          "10\nZhishu Liu et al.": "Physllm: Harnessing large language models\nfor cross-modal\nremote physiological"
        },
        {
          "10\nZhishu Liu et al.": "sensing. arXiv preprint arXiv:2505.03621 (2025)"
        },
        {
          "10\nZhishu Liu et al.": "23. Xing, B., Yuan, K., Yu, Z., Liu, X., K¨alvi¨ainen, H.: Au-ttt: Vision test-time training"
        },
        {
          "10\nZhishu Liu et al.": "model\nfor facial action unit detection. arXiv preprint arXiv:2503.23450 (2025)"
        },
        {
          "10\nZhishu Liu et al.": "24. Yan, W.J., Li, X., Wang, S.J., Zhao, G., Liu, Y.J., Chen, Y.H., Fu, X.: CASME II:"
        },
        {
          "10\nZhishu Liu et al.": "An improved spontaneous micro-expression database and the baseline evaluation."
        },
        {
          "10\nZhishu Liu et al.": "In: PLOS ONE (2014)"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "25. Yuan, K., Yu, Z., Liu, X., Xie, W., Yue, H., Yang, J.: Auformer: Vision transformers"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "are parameter-efficient facial action unit detectors. In: ECCV (2024)"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "26. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "performance of convolutional neural networks via attention transfer. arXiv preprint"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "arXiv:1612.03928 (2016)"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "27. Zhao, G., Pietikainen, M.: Dynamic texture recognition using local binary patterns"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "with an application to facial expressions. IEEE TPAMI (2007)"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "28. Zhao, K., Chu, W.S., De\nla Torre, F., Cohn, J.F., Zhang, H.: Joint patch and"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "multi-label\nlearning for facial action unit detection. In: CVPR (2015)"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "29. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt\nfor vision-language"
        },
        {
          "AU-LLM: Enhancing Micro-Expression AU Detection with LLMs\n11": "models. IJCV (2022)"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "J Alayrac",
        "J Donahue",
        "P Luc",
        "A Miech",
        "I Barr",
        "Y Hasson",
        "K Lenc",
        "A Mensch",
        "K Millican",
        "M Reynolds"
      ],
      "year": "2022",
      "venue": "Flamingo: a visual language model for few-shot learning"
    },
    {
      "citation_id": "2",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners"
    },
    {
      "citation_id": "3",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2018",
      "venue": "IEEE TAFFC"
    },
    {
      "citation_id": "4",
      "title": "Deepseek-r1: A 671b moe model with fine-grained sparsity",
      "year": "2024",
      "venue": "Deepseek-r1: A 671b moe model with fine-grained sparsity"
    },
    {
      "citation_id": "5",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system"
    },
    {
      "citation_id": "6",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "7",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "Lora: Low-rank adaptation of large language models"
    },
    {
      "citation_id": "8",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "9",
      "title": "Infused suppression of magnification artefacts for micro-au detection",
      "authors": [
        "H Khor",
        "Y Li",
        "X Jiang",
        "G Zhao"
      ],
      "year": "2025",
      "venue": "SCIA"
    },
    {
      "citation_id": "10",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li",
        "X Zhu",
        "Y Zeng",
        "Q Wang",
        "L Lin"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "11",
      "title": "Micro-expression action unit detection with spatial and channel attention",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "12",
      "title": "Micro-expression action unit detection with dual-view attentive similarity-preserving knowledge distillation",
      "authors": [
        "Y Li",
        "W Peng",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "FG"
    },
    {
      "citation_id": "13",
      "title": "Intra-and inter-contrastive learning for micro-expression action unit detection",
      "authors": [
        "Y Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "ICMI"
    },
    {
      "citation_id": "14",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "Miriam Jacob",
        "G Stenger"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "15",
      "title": "Asymmetric loss for multi-label classification",
      "authors": [
        "T Ridnik",
        "E Ben-Baruch",
        "N Zamir",
        "A Noy",
        "I Friedman",
        "M Protter",
        "L Zelnik-Manor"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "16",
      "title": "Fitnets: Hints for thin deep nets",
      "authors": [
        "A Romero",
        "N Ballas",
        "S Kahou",
        "A Chassang",
        "G Gkioxari",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Fitnets: Hints for thin deep nets",
      "arxiv": "arXiv:1412.6550"
    },
    {
      "citation_id": "17",
      "title": "Qwen2 technical report",
      "authors": [
        "Q Team"
      ],
      "year": "2024",
      "venue": "Qwen2 technical report"
    },
    {
      "citation_id": "18",
      "title": "Similarity-preserving knowledge distillation",
      "authors": [
        "F Tung",
        "G Mori"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "19",
      "title": "Data leakage and evaluation issues in micro-expression analysis",
      "authors": [
        "T Varanka",
        "Y Li",
        "W Peng",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "IEEE TAFFC"
    },
    {
      "citation_id": "20",
      "title": "Learnable eulerian dynamics for micro-expression action unit detection",
      "authors": [
        "T Varanka",
        "W Peng",
        "G Zhao"
      ],
      "year": "2024",
      "venue": "Image Analysis"
    },
    {
      "citation_id": "21",
      "title": "Au-assisted graph attention convolutional network for micro-expression recognition",
      "authors": [
        "H Xie",
        "L Lo",
        "H Shuai",
        "W Cheng"
      ],
      "year": "2020",
      "venue": "Au-assisted graph attention convolutional network for micro-expression recognition"
    },
    {
      "citation_id": "22",
      "title": "Physllm: Harnessing large language models for cross-modal remote physiological sensing",
      "authors": [
        "Y Xie",
        "B Zhao",
        "M Dai",
        "J Zhou",
        "Y Sun",
        "T Tan",
        "W Xie",
        "L Shen",
        "Z Yu"
      ],
      "year": "2025",
      "venue": "Physllm: Harnessing large language models for cross-modal remote physiological sensing",
      "arxiv": "arXiv:2505.03621"
    },
    {
      "citation_id": "23",
      "title": "Au-ttt: Vision test-time training model for facial action unit detection",
      "authors": [
        "B Xing",
        "K Yuan",
        "Z Yu",
        "X Liu",
        "H Kälviäinen"
      ],
      "year": "2025",
      "venue": "Au-ttt: Vision test-time training model for facial action unit detection",
      "arxiv": "arXiv:2503.23450"
    },
    {
      "citation_id": "24",
      "title": "CASME II: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W Yan",
        "X Li",
        "S Wang",
        "G Zhao",
        "Y Liu",
        "Y Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "25",
      "title": "Auformer: Vision transformers are parameter-efficient facial action unit detectors",
      "authors": [
        "K Yuan",
        "Z Yu",
        "X Liu",
        "W Xie",
        "H Yue",
        "J Yang"
      ],
      "year": "2024",
      "venue": "Auformer: Vision transformers are parameter-efficient facial action unit detectors"
    },
    {
      "citation_id": "26",
      "title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
      "authors": [
        "S Zagoruyko",
        "N Komodakis"
      ],
      "year": "2016",
      "venue": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
      "arxiv": "arXiv:1612.03928"
    },
    {
      "citation_id": "27",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "28",
      "title": "Joint patch and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W Chu",
        "F De La Torre",
        "J Cohn",
        "H Zhang"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "29",
      "title": "Learning to prompt for vision-language models",
      "authors": [
        "K Zhou",
        "J Yang",
        "C Loy",
        "Z Liu"
      ],
      "year": "2022",
      "venue": "IJCV"
    }
  ]
}