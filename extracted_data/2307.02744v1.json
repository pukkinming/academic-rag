{
  "paper_id": "2307.02744v1",
  "title": "Active Learning With Contrastive Pre-Training For Facial Expression Recognition",
  "published": "2023-07-06T03:08:03Z",
  "authors": [
    "Shuvendu Roy",
    "Ali Etemad"
  ],
  "keywords": [
    "Facial Expression Recognition",
    "Semi-supervised Learning",
    "Contrastive Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning has played a significant role in the success of facial expression recognition (FER), thanks to large models and vast amounts of labelled data. However, obtaining labelled data requires a tremendous amount of human effort, time, and financial resources. Even though some prior works have focused on reducing the need for large amounts of labelled data using different unsupervised methods, another promising approach called active learning is barely explored in the context of FER. This approach involves selecting and labelling the most representative samples from an unlabelled set to make the best use of a limited 'labelling budget'. In this paper, we implement and study 8 recent active learning methods on three public FER datasets, FER13, RAF-DB, and KDEF. Our findings show that existing active learning methods do not perform well in the context of FER, likely suffering from a phenomenon called 'Cold Start', which occurs when the initial set of labelled samples is not well representative of the entire dataset. To address this issue, we propose contrastive self-supervised pre-training, which first learns the underlying representations based on the entire unlabelled dataset. We then follow this with the active learning methods and observe that our 2-step approach shows up to 9.2% improvement over random sampling and up to 6.7% improvement over the best existing active learning baseline without the pre-training. We will make the code for this study public upon publication at: github.com/ShuvenduRoy/ActiveFER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial expression recognition (FER) has seen growing interest in the deep learning community  [1] -  [5]  mainly due to its practical applications ranging from smart devices and medical care assistants to smart vehicles. However, the size of the labelled FER datasets is generally one of the concerns prohibiting further progress in the area. Many of the recently developed deep learning models, such as Transformer  [6] , inherently require very large amounts of data. As a result, many recent works on FER have focused on developing methods that learn better representations from small amounts of labelled data  [7] -  [9] .\n\nAlthough labelled datasets are hard to collect and annotate, unlabelled images are widely available on the Internet. Given a pre-defined labelling budget, the annotation process involves resolving the choice of which samples of the unlabelled dataset to annotate  [10] . Recently, active learning has emerged as a viable solution for identifying key samples from the unlabelled set  [11] . The basic idea of active learning is to start the training process with a few randomly selected samples and their corresponding labels. As the training progresses, a selection criterion is used to find more samples from the unlabelled set that are the best candidates for annotation. The model is then trained with this new labelled set along with the previously available labelled set. This cycle continues as long as the labelling budget is not exhausted.\n\nA variety of new active learning methods have been proposed in recent years in different areas  [11] . Although a few works have explored active learning specifically for FER  [12] ,  [13] , more recent approaches in active learning  [14] ,  [15]  have not yet been studied in this context. Moreover, to our knowledge, a comprehensive study to benchmark the performance of different active learning methods for FER under the same training protocol has not been conducted.\n\nAnother well-known fact about active learning training with a small labelling budget is the 'cold start' problem. The cold start problem occurs when the initial labelled set is either too small or not a good representative for the entire dataset. In such scenarios, the model fails to learn effective representations from the initial labelled set, and thus informative samples are not selected in later cycles of the active learning process. This may result in a final accuracy that is even worse than not using active sampling altogether.\n\nIn this work, we address the two problems mentioned above. First, we present a comprehensive study of different active learning methods for FER. To this end, we compare 8 active learning methods, namely: Entropy  [16] , Margin  [17] , Least Confidence  [17] , BADGE  [15] , GLISTER  [18] , Coreset  [14] , BALD  [19] , and Adversarial Deepfool  [20] . We conduct our study on three FER datasets (FER13, RAF-DB, and KDEF) and show that surprisingly, simpler methods like Least Confidence and Margin obtain better results than recently proposed methods like Coresets and GLISTER. On average, the Least Confidence method shows the best performance across all three datasets. Additionally, we find that active learning on FER does indeed suffer from the cold start problem. To address this issue, we propose a simple yet effective solution: self-supervised pre-training using the unlabelled data. We select the best-performing active learning method, Least Confidence, and show that by adding a self-supervised pretraining step, the cold start problem is reduced. Specifically, for self-supervised pre-training, we explore BYOL, MOCO, Barlow Twins, SwAV, and SimCLR and observe that while all of them are effective in reducing the negative impact of  the cold start issue, SimCLR is the most effective. The selfsupervised pre-training step helps the method select more representative samples at the first cycle and effectively enables better learning at later cycles. Overall, our proposed solution shows up to 9% improvement over random sampling and up to 6% improvements over the scenario where the cold start issue is not addressed. Further ablation studies confirm that the improvements in performance are not simply due to a better encoder (pre-trained), but rather because of the fact that better samples are selected from the unlabelled set due to the added pre-training step.\n\nOur contributions in this work are summarized as follows:\n\n• We study active learning in the context of FER by exploring eight different active learning methods on three FER datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we discuss the related literature in two areas relevant to this work: (a) active learning and (b) self-supervised learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Active Learning",
      "text": "The objective of active learning is to utilize a selection criterion for selecting the most representative samples from an unlabelled set for annotation. Although active learning is not a new concept, the rise of deep learning has resulted in a surge in active learning methods since deep learning methods require large datasets to train, which are not always available for many domains. Some selection methods in earlier forms of active learning utilized the concept of uncertainty in the model's prediction as an indicator for selecting new samples for labelling. For example, in  [16] , the entropy in the model's prediction on an unlabelled sample was used as the selection criterion. Two other variants of uncertainty-based sampling techniques were proposed in  [17] . The first criterion (Margin) uses the difference between the top two predictions as the indicator for selection. A confident prediction will have a large difference between the highest and second-highest predictions over the number of classes. The second criterion (Least Confidence) simply takes the maximum over the class probability as the indicator. In this criterion, a less confident prediction will have lower confidence for the predicted class.\n\nMore recent methods focus more on the learning progress of the model and utilize more specific signals as the selection criteria. For instance,  [15]  inspects the loss gradient and selects a set of samples with diverse loss gradients. GLISTER  [18]  is another method focusing on diverse sampling over the entire dataset using bi-level optimization. The concept of Coresets  [14]  has also been utilized as a selection criterion. BALD  [19]  uses Bayesian deep learning to maximize the information between the prediction and model posterior as an indicator for sampling. The concept of adversarial attacks has also been utilized to estimate the decision boundary of classes and select samples that are close to the boundaries  [20] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Self-Supervised Learning",
      "text": "Self-supervised learning (SSL) is one of the most popular unsupervised representation learning methods that has shown remarkable progress in various areas. SSL can learn important representations of the data without any supervision. Most of the earlier SSL methods utilized the concept of pre-text tasks, where an auxiliary task was defined on the unlabelled data. One example of such a pre-text task is rotation prediction, where an input unlabelled image is rotated at a certain degree and the model is tasked with predicting the angle of rotation. A more recent form of SSL utilizes the concept of contrastive learning. Contrastive learning in computer vision was popularized by SimCLR, which utilizes a contrastive loss on two augmentations of an unlabelled image to maximize their agreement in the embedding space. SimCLR also utilizes the concept of projection heads, hard augmentations, and  carefully designed training protocols to perform effectively in many scenarios. Many variants of SimCLR have been since proposed. For instance, MoCo  [21]  utilizes a momentum encoder to encode one augmented image, while the other image is encoded by an online encoder. Later BYOL  [22]  proposed a slightly different objective function with respect to MoCO, which predicts the embedding of one augmented image from the other. SwAV  [23]  also has a similar idea with the distinction of predicting a prototype rather than the actual embedding. Barlow Twins  [24]  proposed a different loss calculated on the cross-correlation between the predicted embedding of the two images. This method explicitly avoided the mode collapse problem of self-supervised learning while achieving strong performance on downstream tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Preliminaries",
      "text": "Let X U = (x i ) N i=1 be a set of unlabelled samples, where N is the total number of samples in the unlabelled set, and n be the total labelling budget, where n ≪ N . An active learning method first randomly samples a small subset of s samples from X U and annotates them to form X L S = (x i , y i ) s i=1 . The model is trained with X L S for certain epochs. These two steps are together called a cycle. Over the next (c -1) cycles, the active learning method samples (n -s)/(c -1) samples per cycle using a selection criterion, and trains the model with the current and previously sampled data. In this study, we explore the following active learning methods.\n\nEntropy. In Entropy, the uncertainty in the prediction of the model is used as the selection criterion  [16] . Let p(x) = sof tmax(M j (x)) be the prediction of the model, where M j is the trained model at cycle j. The entropy selection criterion (H(x)) is represented as:\n\nwhere i is the index over the vector dimension of the model's predictions. Here, the method chooses the sample with the highest entropy.\n\nMargin. This method considers the difference between the highest two predictions as the selection criterion (F (x))  [17] , which is defined as:\n\nHere, m 1 and m 2 are the largest and second-largest predictions, and the active learning method selects the sample with the lowest margin. Least Confidence. Least Confidence is another simple approach where the prediction confidence is used as the selection criterion  [17] . This criterion (C(x)) is defined as:\n\nHere, the active learning method selects the minimum C(x) over all the unlabeled samples. BADGE. This approach is one of the more recent active learning selection methods that take the model's learning progress into consideration  [15] . It first computes the gradient of the last layer with some pre-defined loss function. Then, a K-Means++ algorithm is utilized to find the desired number of centers with diverse loss gradients.\n\nGLISTER. In this recent method, the active learning solution aims to select samples that are representative of the entire domain  [18] . For this, GLISTER utilizes a bi-level optimization problem where the inner optimization learns model parameters, and the outer optimization selects a set of unlabeled samples.\n\nCoreset. In this approach, the aim of the method is to find the samples that represent or capture the structure of the entire unlabelled dataset  [14] . Here, a k-center algorithm is utilized to solve a pre-defined objective function to find the coresets.\n\nBALD. In this method, the concept of Bayesian deep learning is utilized for active learning  [19] . BALD uses the concept of information maximization between the prediction and model posterior to select a pool of samples.\n\nAdversarial DeepFool. This active learning method operates by selecting points that are closer to the decision boundaries  [20] . Since calculating the actual decision boundary in the embedding space is difficult, this method proposes to use the concept of adversarial attacks to estimate it. For each sample, the number of adversarial perturbations required to flip the prediction is considered as the indication of how close it is to the decision boundary.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Proposed Solution For The Cold Start Problem",
      "text": "In this section, we present a simple solution to address the cold start problem in active learning for improved performance in the context of FER. The proposed solution involves a two-step training protocol. In the first step, we pre-train the model with the entire unlabeled set X U in a self-supervised setting to learn the underlying representation of the data. In the second step, we follow conventional active learning training, where the learned representation from the first step helps the model to learn a discriminative representation at the first cycle from the small labelled subset X LS , without overfitting. This approach enables a better selection of representative samples in later cycles of the active learning training process, effectively reducing the cold start problem. We explore some recently proposed self-supervised methods, including SimCLR  [25] , MoCo v2  [21] , BYOL  [22] , SwAV  [23] , and Barlow Twins  [24] , for the first step. For the second step, we use the Least Confidence as the active learning component. Below, we provide a brief overview of each of the self-supervised methods that we explore to evaluate our proposed solution.\n\nSimCLR  [25]  is a popular contrastive self-supervised technique responsible for popularizing contrastive learning in the field of computer vision. The basic idea behind this method is to learn from positive and negative samples, where positive samples are variations or transformations of an input sample, typically generated through augmentations. All other samples are considered negative with respect to the input sample. By bringing together positive samples and moving them away from negatives in the embedding space, contrastive learning allows for effective learning of the underlying representation of the data. The contrastive loss function for two positive samples, denoted as i and j, is defined by:\n\nwhere, z i is the embedding of the encoder, cos() is the cosine similarity function, and τ is a temperature parameter. A visual illustration of the SimCLR method is depicted in Figure  2a .\n\nMoCo  [21]  is another popular self-supervised learning technique. Similar to SimCLR, the basic idea behind this technique is to learn representations that can differentiate between positive and negative samples. The positive pairs are again generated by data augmentation, while negative pairs are taken from a queue of samples that were stored at previous iterations of training. MoCo maintains two encoders, one 'online' and the other 'momentum'. The online encoder is updated after processing each minibatch, while the momentum encoder is updated using a moving average of the online encoder parameters. This ensures that the momentum encoder is always slightly behind the online encoder, enabling it to capture information from a larger amount of samples. The momentum encoder is updated with the following equation:\n\nwhere θ q and θ k are the parameters of the online and momentum encoder and m is the momentum coefficient. MoCo v2 is a simple extension of MoCo that utilizes the projection head and hard augmentation concepts introduced in SimCLR. Figure  2b  depicts the MoCo framework. BYOL  [22]  also utilizes two encoders called online and target encoder. Like MoCo, the target encoder is a moving average of the online encoder. The objective of BYOL is to train an online encoder to predict the target encoder's represen-tation of the same image under different augmentations. The online encoder is updated with the following loss function:\n\nwhere Z j is the embedding generated by the target encoder, and P is the prediction generated from the online encoder's representation. BYOL framework is depicted in Figure  2c  SwAV  [23]  is another popular self-supervised learning technique, which is a clustering-based approach. It first generates multiple views on a sample image by applying augmentations and then predicts its cluster assignment. Considering each view as a query and other views as keys, SwAV  [23]  utilizes contrastive learning on its cluster assignment. This clusteringbased approach is able to learn high-quality representations even when the dataset is highly diverse or has many classes. The SwAV method is visualized in Figure  2d .\n\nBarlow Twins  [24]  proposed a loss function that explicitly avoids the collapse in self-supervised representation learning. It does so by calculating the cross-correlation matrix between two augmented images and making this matrix close to identity. The loss function of Barlow Twins is represented as follows:\n\nwhere C ij is the cross-correlation between ith and jth images in a batch. Here, the first term is called the invariance term, and the second term is called the redundancy reduction term. The Barlow Twins method is shown in Figure  2e .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "In this section, we describe the experimental setup and the results. First, we present the implementation details and dataset description. Then we discuss the results of different active learning methods for FER. Finally, we present the results of our proposed method and a details study of different aspects of the method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets And Implementation Details",
      "text": "The experiments in the paper are conducted on 3 popular expression recognition datasets: FER13, RAF-DB, and KDEF. These datasets were selected to cover various aspects of FER datasets, including dataset size (from small to very large), spatial resolutions (from low to high), and sources (lab condition vs. in-the-wild). FER13  [26]  is an in-the-wild dataset that was collected by the Google search API. All images in this dataset have an input resolution of 48×48, and it contains seven expression classes with 28K and 7K images in the training and validation splits, respectively. RAF-DB  [27]  is another in-the-wild dataset containing 12K and 3K images for training and validation, respectively. The images in this dataset are re-scaled to a resolution of 96×96. KDEF  [28]  is a smaller dataset that was collected in a lab environment with comparatively higher-resolution images.\n\nTo ensure a fair comparison, all the active learning methods in the experiment were trained under the same training settings. Specifically, a ResNet-18 model is trained for seven cycles (c) with 40% of the total labelled samples of the original dataset. The models were trained with an SGD optimizer with a learning rate of 0.01 and a batch size of 20. The pre-training was done for 400 epochs following the implementation details of SimCLR  [25] . The code is implemented in PyTorch and trained with Nvidia V100 GPU.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Performance Of Existing Active Learning Methods On Fer",
      "text": "In this section, we evaluate the performance of various active learning methods for FER on the three aforementioned datasets. The results of this study are presented in Table  I . We observe three important findings that summarize the results from this study as follows:\n\n(1) Existing methods perform poorly on FER. In general, active learning on FER does not provide a reasonable improvement over random sampling. Only Least confidence and GLISTER show improvements over random sampling for all datasets. The average across the active learning methods shows some improvement over random sampling for FER13 and RAF-DB datasets (2.30% and 2.74%) but gets reduced performance for KDEF. This signifies the fact that FER requires more specialized active learning methods to get a reasonable improvement over random sampling.\n\n(2) Simpler methods outperform more complicated and specialized methods. For FER, simpler approaches such as Entropy, Margin, or Least Confidence show improvements over the latest methods like BADGE, GLISTER, and Core-Set. Among the two methods that show improvements for all datasets (Least Confidence, GLISTER), Least confidence shows 2.48%, 2.89%, and 2.46% improvements on FER, RAF-DB, and KDEF, while GLISTER shows 1.98%, 3.28%, and 0.14% improvements. Thus we can conclude that the Least Confidence method is the best default choice for FER datasets for active learning.\n\n(3) Active learning does not work well on small datasets. In our experiment on the smallest dataset, KDEF, we find no improvement for most of the active learning methods. Apart from Least Confident and GLISTER, all other methods perform below random sampling. We argue that this poor performance is caused by the cold start problem. Since KFEF is a small dataset, training starts with a very small-sized labelled set, thus overfitting without learning generalizable representations. In the next section, we present the results of our proposed solution that alleviates the cold start problem and improves the accuracy of all datasets, including the very small KDEF dataset.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Performance Of The Proposed Solution",
      "text": "In Table  II , we present the performance of the best active learning method for FER, i.e., Least Confidence, with and without our proposed solution for self-supervised pre-training. In general, we observe noticeable improvements with the proposed SSL pre-training in comparison to the baseline, with the highest improvement being 6.74% on the KDEF dataset. Recall that most existing methods showed performance degradation on KDEF due to the cold start problem. Furthermore, the proposed method shows a 9.2% improvement over random sampling on this dataset. This shows the effectiveness of the proposed approach for solving the cold start problem and improving performance. On the RAF-DB dataset, we see a 2.93% improvement with pre-training and a 5.82% improvement over random sampling. Similarly, for FER13, we observe a 1.48% improvement with pre-training and 3.96% over random sampling.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Ablation Study",
      "text": "In this section, we further analyze the impact of SSL pre-training on active learning. While Table II demonstrated the positive impact of pre-training on the Least Confidence method, to investigate whether this improvement is a result of the pre-training alone or the fact that pre-training combined with active learning alleviates the cold start problem, we perform the same pre-training followed by random sampling. This result is presented in Table  II , where we observe a 2.45%, 2.64%, and 6.81% drop in performance on the three datasets, respectively. This finding enforces that a pre-trained encoder (in a self-supervised setting) on its own does not provide much improvement. Rather, selecting more representative samples in active learning boosts performance.\n\nWe also investigate different choices for self-supervised pretraining, which we discussed in Section III-B. We summarize these results in Table  III",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Sensitivity Study",
      "text": "In this section, we present a detailed sensitivity study on different hyper-parameters involved in the entire pipeline, including pre-training and active learning. More specifically, we conduct experiments on the following factors: (1) initial labelled set size of active learning, (2) number of active learning cycles, (3) number of epochs per cycle, (4) total labelling budget, and (5) optimizer. We show the results for the best performing active learning method for FER, i.e., Least Confidence, on all three datasets.\n\n1) Sensitivity toward the initial labelled set size: The 'initial labelled set size (s)' is an important hyper-parameter for active learning. Since the initial labelled set is selected randomly, selecting a large value for s reduces the number of samples that can be selected with active learning at later cycles. On the other hand, choosing very small values of s can contribute to the cold start problem. Therefore, we investigate different values of s for FER using the two-step training solution. In Figure  3a , we illustrate the sensitivity for different initial labelled set sizes. The figure shows that choosing small values of s leads to better performance. For example, both FER13 and RAF-DB show the best performance when only 5% of samples are selected as the initial labelled set. For KDEF, the best accuracy is achieved with an initial labelled set size of 15%. We argue that the underlying representations learned by the self-supervised pre-training are responsible for this phenomenon. Due to the pre-training step, the model can learn from a small initial labelled set and select most of the samples in later cycles. Another important trend is the increased standard deviation when more samples are selected as the initial labelled set, especially for the KDEF dataset.\n\n2) Sensitivity toward the number of active learning cycles: Another important parameter for active learning training is the total number of cycles. A large number of cycles provides    the opportunity for selecting more representative samples at later cycles. Nevertheless, increasing the number of cycles also increases the total training cost, and it is, therefore, important to find an optimal number for this parameter. The sensitivity toward different numbers of training cycles is presented in Figure  3b . We find the best performance is obtained for FER and RAF-DB when the model is trained for seven cycles and KDEF for nine cycles.\n\n3) Sensitivity toward training epochs per cycle: We also investigate the optimal number of training epochs per cycle. This is another important parameter since training for an excessive number of epochs can cause the model to overfit the data available in that cycle, whereas using too few can hamper learning for the set of data in that cycle. As a result, we present a sensitivity study on the number of active learning epochs in Figure  3c . The observations show that the best accuracies are observed for 150 epochs for both KDEF and RAF-DB datasets. The FER13 shows better performance for longer training as it obtains the best accuracy with 250 epochs, with 200 also showing a close accuracy.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "4) Performance Versus Different Labelling Budgets:",
      "text": "In Figure  3d , we show the performance for different amounts of total labelling budget. In general, more labelled samples results in better accuracy for almost all settings. However, the change in accuracy is sharper in low data regimes (e.g. increase from 20% to 25%) compared to the higher ones.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "5) Sensitivity Toward The Optimizer:",
      "text": "We also investigate the impact of the choice of optimizer on the final performance in Table  IV . The results in this table show that SGD is considerably better than Adam optimizer for all the datasets. SGD shows 7.29%, 4.31%, and 3.44% higher accuracy compared to Adam on KDEF, RAF-DB and FER13, respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this study, we explored active learning as a solution for reducing the reliance on large amounts of labelled data to train deep learning models for FER. First, we implemented and evaluated various active learning methods for FER and confirmed the presence of a cold start problem. To overcome this issue and further enhance FER performance, we employed a two-step training protocol. In the first step, we conducted contrastive self-supervised pre-training using the entire set of unlabeled data. Our extensive studies showed that the two-step training protocol alleviates the cold start problem and improves performance by considerable margins. An extensive ablation study showed the effectiveness of the proposed pre-training step, and a comprehensive sensitivity study identified the optimal parameters for each dataset. We hope that this research will bring attention to this important direction for reducing the labelling cost, which can help facilitate the development of improved FER methods.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This study did not include the collection of any new datasets as it relied on three popular public FER datasets used by most existing works in this domain. These datasets feature diverse demographic groups and contain no personal identification information (besides what has been already public or known to the original authors of these datasets) or offensive material, thereby avoiding any privacy concerns. Since our work relies on datasets collected from the internet, we believe good generalizability towards different demographic groups of people is achievable. However, a detailed study on bias is required to further analyze this notion. We acknowledge that, like any FER method, the system developed in this paper has the potential to be used to analyze the facial expressions of individuals without their consent. As a result, we find it absolutely imperative for such systems to be used ethically and responsibly with full compliance with ethical, moral, and legal guidelines. The training process of each experiment took under 12 hours on a single Nvidia V100 GPU, a reasonable time-frame that does not pose a significant carbon footprint.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the 2-step training protocol. First, we pre-train the model with a contrastive self-supervised learning",
      "page": 2
    },
    {
      "caption": "Figure 2: Architecture of different self-supervised training frameworks explored for the pre-training.",
      "page": 3
    },
    {
      "caption": "Figure 2: b depicts the MoCo framework.",
      "page": 4
    },
    {
      "caption": "Figure 3: a, we illustrate the sensitivity for different",
      "page": 6
    },
    {
      "caption": "Figure 3: Sensitivity study on various parameters of the 2-step training protocol on all three datasets. The light-shadowed area",
      "page": 7
    },
    {
      "caption": "Figure 3: b. We find the best performance is obtained for FER",
      "page": 7
    },
    {
      "caption": "Figure 3: c. The observations show that the best accuracies are",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "Random Sampling",
          "FER13\nRAF-DB\nKDEF": "63.15±0.33\n73.74±1.52\n83.57±2.85"
        },
        {
          "Methods": "Entropy [16]\nMargin [17]\nLeast Confidence [17]\nBADGE [15]\nGLISTER [18]\nCoreSet\n[14]\nBALD [19]\nAdv. DeepFool\n[20]",
          "FER13\nRAF-DB\nKDEF": "65.86±0.33\n76.81±1.51\n80.78±1.76\n65.87±0.62\n77.64±0.93\n83.30±0.92\n86.03±1.44\n65.63±0.28\n76.63±0.95\n64.91±0.21\n77.29±0.59\n82.00±5.11\n65.13±0.2\n77.02±0.76\n83.71±1.67\n64.99±0.09\n75.76±0.96\n81.94±1.75\n65.58±0.17\n74.79±1.06\n79.69±1.42\n78.56±0.48\n64.81±0.38\n79.96±5.21"
        },
        {
          "Methods": "Average",
          "FER13\nRAF-DB\nKDEF": "65.35\n76.48\n82.20"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FER13\nRAF-DB": "KDEF"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Clifer: Continual learning with imagination for facial expression recognition",
      "authors": [
        "Nikhil Churamani",
        "Hatice Gunes"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "2",
      "title": "Face trees for expression recognition",
      "authors": [
        "Mojtaba Kolahdouzi",
        "Alireza Sepas-Moghaddam",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "3",
      "title": "Facetoponet: Facial expression recognition using face topology learning",
      "authors": [
        "Mojtaba Kolahdouzi",
        "Alireza Sepas-Moghaddam",
        "Ali Etemad"
      ],
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Learning bases of activity for facial expression recognition",
      "authors": [
        "Evangelos Sariyanidi",
        "Hatice Gunes",
        "Andrea Cavallaro"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "5",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "6",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Analysis of semi-supervised methods for facial expression recognition",
      "authors": [
        "Shuvendu Roy",
        "Ali Etemad"
      ],
      "year": "2022",
      "venue": "10th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "8",
      "title": "Self-supervised contrastive learning of multi-view facial expressions",
      "authors": [
        "Shuvendu Roy",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "9",
      "title": "Spatiotemporal contrastive learning of facial expressions in videos",
      "authors": [
        "Shuvendu Roy",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "Spatiotemporal contrastive learning of facial expressions in videos"
    },
    {
      "citation_id": "10",
      "title": "Impact of labelled set selection and supervision policies on semi-supervised learning",
      "authors": [
        "Shuvendu Roy",
        "Ali Etemad"
      ],
      "year": "2022",
      "venue": "Impact of labelled set selection and supervision policies on semi-supervised learning",
      "arxiv": "arXiv:2211.14912"
    },
    {
      "citation_id": "11",
      "title": "A survey of deep active learning",
      "authors": [
        "Pengzhen Ren",
        "Yun Xiao",
        "Xiaojun Chang",
        "Po-Yao Huang",
        "Zhihui Li",
        "B Brij",
        "Xiaojiang Gupta",
        "Xin Chen",
        "Wang"
      ],
      "year": "2021",
      "venue": "ACM computing surveys"
    },
    {
      "citation_id": "12",
      "title": "Action unit classification for facial expression recognition using active learning and svm",
      "authors": [
        "Li Yao",
        "Yan Wan",
        "Hongjie Ni",
        "Bugao Xu"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "13",
      "title": "Wild facial expression recognition based on incremental active learning",
      "authors": [
        "Ahmed Minhaz Uddin",
        "Jin Woo",
        "Kim Yeong Hyeon",
        "Md Rezaul Bashar",
        "Phill Kyu"
      ],
      "year": "2018",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "14",
      "title": "Active learning for convolutional neural networks: A core-set approach",
      "authors": [
        "Ozan Sener",
        "Silvio Savarese"
      ],
      "year": "2005",
      "venue": "Active learning for convolutional neural networks: A core-set approach",
      "arxiv": "arXiv:1708.00489"
    },
    {
      "citation_id": "15",
      "title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
      "authors": [
        "Chicheng Jordan T Ash",
        "Akshay Zhang",
        "John Krishnamurthy",
        "Alekh Langford",
        "Agarwal"
      ],
      "year": "2005",
      "venue": "Deep batch active learning by diverse, uncertain gradient lower bounds",
      "arxiv": "arXiv:1906.03671"
    },
    {
      "citation_id": "16",
      "title": "Active learning literature survey",
      "authors": [
        "Burr Settles"
      ],
      "year": "2005",
      "venue": "Active learning literature survey"
    },
    {
      "citation_id": "17",
      "title": "A new active labeling method for deep learning",
      "authors": [
        "Dan Wang",
        "Yi Shang"
      ],
      "year": "2014",
      "venue": "International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "18",
      "title": "Glister: Generalization based data subset selection for efficient and robust learning",
      "authors": [
        "Krishnateja Killamsetty",
        "Durga Sivasubramanian",
        "Ganesh Ramakrishnan",
        "Rishabh Iyer"
      ],
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Deep bayesian active learning with image data",
      "authors": [
        "Yarin Gal",
        "Riashat Islam",
        "Zoubin Ghahramani"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "20",
      "title": "Adversarial active learning for deep networks: a margin based approach",
      "authors": [
        "Melanie Ducoffe",
        "Frederic Precioso"
      ],
      "year": "2005",
      "venue": "Adversarial active learning for deep networks: a margin based approach",
      "arxiv": "arXiv:1802.09841"
    },
    {
      "citation_id": "21",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Bootstrap your own latent-a new approach to self-supervised learning",
      "authors": [
        "Jean-Bastien Grill",
        "Florian Strub",
        "Florent Altché",
        "Corentin Tallec",
        "Pierre Richemond",
        "Elena Buchatskaya",
        "Carl Doersch",
        "Bernardo Avila Pires",
        "Zhaohan Guo",
        "Mohammad Gheshlaghi Azar"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "authors": [
        "Mathilde Caron",
        "Ishan Misra",
        "Julien Mairal",
        "Priya Goyal",
        "Piotr Bojanowski",
        "Armand Joulin"
      ],
      "year": "2005",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Barlow twins: Self-supervised learning via redundancy reduction",
      "authors": [
        "Jure Zbontar",
        "Li Jing",
        "Ishan Misra",
        "Yann Lecun",
        "Stéphane Deny"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "25",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "27",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "CD ROM from Department of Clinical Neuroscience",
      "authors": [
        "Daniel Lundqvist",
        "Anders Flykt",
        "Arne Öhman"
      ],
      "year": "1998",
      "venue": "Psychology Section, Karolinska Institutet"
    }
  ]
}