{
  "paper_id": "2501.03103v1",
  "title": "Mvp: Multimodal Emotion Recognition Based On Video And Physiological Signals",
  "published": "2025-01-06T16:09:22Z",
  "authors": [
    "Valeriya Strizhkova",
    "Hadi Kachmar",
    "Hava Chaptoukaev",
    "Raphael Kalandadze",
    "Natia Kukhilava",
    "Tatia Tsmindashvili",
    "Nibras Abo-Alzahab",
    "Maria A. Zuluaga",
    "Michal Balazia",
    "Antitza Dantcheva",
    "François Brémond",
    "Laura Ferrari"
  ],
  "keywords": [
    "Emotion recognition",
    "Physiological signals",
    "Multimodal transformer",
    "Feature fusion",
    "Cross-attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotions entail a complex set of behavioral, physiological and cognitive changes. Current state-of-the-art models fuse the behavioral and physiological components using classic machine learning, rather than recent deep learning techniques. We propose to fill this gap, designing the Multimodal for Video and Physio (MVP) architecture, streamlined to fuse video and physiological signals. Differently then others approaches, MVP exploits the benefits of attention to enable the use of long input sequences (1-2 minutes). We have studied video and physiological backbones for inputting long sequences and evaluated our method with respect to the state-of-the-art. Our results show that MVP outperforms former methods for emotion recognition based on facial videos, EDA, and ECG/PPG. The code is available on GitHub 5 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The recognition of human behavior from video data has been deeply investigated in the affective computing area  [7] . What is limiting the performance of computer vision algorithms is the recognizability and the meaning of visual patterns. Facial expressions visible from cameras do not always correlate with the experienced emotions, which can lead to incorrect emotion recognition, e.g., a person may smile without being happy. In order to improve emotion recognition performance, multimodal learning has been introduced. Most of the current research on multimodal emotion recognition focuses on the combination of video, audio, and language cues  [6, 11] . Such cues represent voluntary expressions of emotions, i.e., the behavioral component. On the other hand, the physiological component is largely involuntary and is directly activated when a person is emotionally triggered  [23] . When an emotional trigger appears, a change in the physiological pattern is inevitable and detectable  [12] ; e.g., the heart rate increases if a person sees a snake. Thanks to their involuntary nature and to the ease of recording, peripheral signals as the electrocardiography (ECG) or photoplethysmogram (PPG), for the heart activity, and the electrodermal activity (EDA), for to the skin conductivity, have been exploited for the purpose of recognizing human emotions  [14, 22, 28] .\n\nWhile unimodal video and physiological signals have been used to recognize emotions, very few works combine the two kind of data  [5, 18, 24] . A recent work from 2023, proposes to fuse traditional features, classifying them by SVM and k-NN  [18] . Similarly, in a recent dataset for stress identification  [5] , the baselines for both unimodal and multimodal experiments are conducted based on SVM/MLP using hand-crafted features for both video and physiological signals. The stateof-the-art approaches utilize old-fashioned machine learning pipelines. This is probably due to the small number and small size (max 60 subjects) datasets available. We propose to fill this gap, designing a model that exploits the benefits of the transformer architecture attention  [29] . The idea is to enable the use of long input sequences (1-2 minutes), in the context of emotion recognition, with video and physiological data, in order to extract complementary features.\n\nFirst, we study how to efficiently input the video data. We compare two approaches, the DL features extracted through VideoMAE  [27]  and the action units (AUs), extracted through OpenFace  [3] . VideoMAE and its modification  [4]  have just recently showed to achieve state-of-the-art results on multiple tasks, comprehending facial expression recognition. The main limitation of the available approaches is related to the input length. Typically short videos of few seconds are used. An open question is if and how these models can scale on longer videos. Here we adapt the Hugging Face implementation of VideoMAE to long (1-2 minutes) videos, and we compare these results with the traditional approach of AUs extraction. AUs are fine-grained facial muscle movements  [9] , typically extracted through neural networks pre-trained on large sets of facial data. Each AU is associated with a specific set of facial muscles. As this is the first study on scaling VideoMAE for long emotion recognition videos, we assessed multiple configurations, on two datasets (AMIGOS and DEAP).\n\nSecond, we study how to efficiently input physiological data. The state-of-theart is based on 1D-CNN and unimodal transformer for the recognition of emotions from raw physiological data  [30, 31] . One main limitation of these methods is related to the input signals, which are cut into 10-seconds segments. Here we improve the physiological backbone permitting the input of the full sequences, thus enabling attention to find correlation in both short and long range dependencies. Finally, we propose a multimodal transformer-based architecture, the Multimodal for Video and Physio (MVP), to efficiently couple the behavioral and physiological components for the emotion recognition task (Figure  1 ). The model combines facial video features with ECG/PPG and EDA. Deviating from other multimodal transformer-based architectures for video and language data  [6, 11, 34] , which use as input short subsequences (4-10 seconds), the MVP architecture is developed to handle long input sequences (1-2 minutes) of complementary behavioral and physiological data. Summarized contributions:\n\n1. We comprehensively study the video and physiological backbones for inputting long sequences. 2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Table  1  shows the state-of-the-art emotion recognition methods. In the case of video, audio and language, transformer-based architectures have been proposed. The data are typically input into transformer as features  [31, 33, 34] : Action Units for the video (e.g., from OpenFace), audio features (e.g., Mel spectrogram from Librosa or Covarep), and language features (e.g., from GloVe or BERT). Each modality is used as input into self-attention layers, which allow the model to attend to different parts of the input sequence and capture long-range dependencies. Then the multimodal part takes place, performing cross-attention across modalities. The cross-attention can be done in multiple ways. The cross-modal fusion has been developed on pairwise representations  [11] . Here two couples of the modalities (i.e., language + audio and language + video, with language as the primary modality) enter a complementary module where bimodal fusion is done at the Multi-Head Attention level. Here, each modality has its own representation to preserve modality-specific features. A joint-encoding has been also used at the level of the Multi-Head Attention, to encode information from all the modalities, producing a unified representation  [6] . The best result is achieved with a bimodal model using the linguistic + acoustic cues. Notably, in both cases  [6, 11] , the language cue is the most representative. This can be due to the fact that annotators mainly worked on the transcript data. One of the major limitation of these works is related to the unique use of the behavioral component, i.e., video, audio and language data.\n\nFew attempts have been made to build a multimodal framework that can exploit both the behavioral and physiological components. These works rely on late fusion of classical features. In the UDLBV  [24]  a CNN architecture (VGG-16 pretrained on ImageNet) and hand-crafted (HC) features are concatenated in a first step, after which PCA is applied, for then passing the features to an LSTM (only for the DEAP dataset) and finally classify with extreme learning machines (ELM). In EHCI  [18]  traditional features as spectral, HC, and local binary patterns for facial images are late fused and classified through SVM and k-NN.\n\nOur aim is to propose a transformer-based architecture that fuses video and physiological data through cross-attention. The idea is to improve the emotions recognition by combining these two complementary signals, which represent the behavioral and the physiological components of emotions.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Video Cues. State-Of-The-Art Video-Based Emotion Recognition Methods Use Two Main Types Of Facial Video Features: (I) Classical Features Like Action Units (Aus)",
      "text": "and facial landmarks  [11, 16]  and (ii) general features extracted through DL techniques  [25, 33, 34] . We compare these two approaches: (i) the use of AUs, extracted through OpenFace  [3] , and (ii) general features extracted through Video-MAE  [27] . AUs are fine-grained facial muscle movements  [9] , typically extracted through neural networks pre-trained on large sets of facial data. These methods have been largely used showing stable and reliable results. VideoMAE and its modifications  [4, 32]  are new pre-training methods, providing the best results on many tasks, including video-based facial expression recognition. Masked autoencoders (MAE) are self-supervised learners (SSL) trained to reconstruct the missing masked random patches of the input video. The use of VideoMAE in the emotion recognition field is very recent, therefore we report our adaptation to the challenging set-up of emotion recognition with long videos.\n\nPhysiological cues. Peripheral physiological data are largely involuntary  [23] , mediated by the autonomous nervous system, which is directly activated when a person is emotionally triggered. This means that when an emotional activation is present, a change in physiological patterns is inevitable and detectable  [12] . Peripheral physiological data, as ECG/PPG and EDA, have been exploited to derive reliable information for emotion recognition  [8] . The use of machine learning on physiological data is quite recent, up to 5-6 years ago computational models were the most used. In the last years, classical machine learning pipelines, with models as Multilayer Perceptron (MLP), Support Vector Machine (SVM) and random forest trained on HC features  [20]  became more popular. The first approach exploiting deep learning architectures has seen the translation of physiological time-series data into 2D images, representing spectrogram or power spectral density, which are then input into CNN architectures (as ResNet or VGG) pretrained on different datasets (as ImageNet)  [2, 7, 10] . GNNs have also been used  [13]  and two stacked convolutional autoencoders has been proposed to combine ECG and EDA  [20] . Now, the state-of-the-art method in the field is based on 1D-CNN and self-attention transformers for the recognition of emotions from raw physiological data  [30, 31] . The results have been proved only for the AMIGOS dataset. In ERMS  [30]  the two signals are used as time-series and the prediction is done after the concatenation of the last hidden layers of unimodal transformers. One main limitation of all these methods is related to the input signals, which are cut segments of 10-seconds. Then, or random 10s are taken for each sample, or each sample is used and the output of the features extractor is averaged. The advantage of using 10-seconds subsequences is that the network can work with a small and fixed input, resulting in a higher efficiency. This approach does not consider two factors. The ECG and EDA vary on different time scales, from few ms to tens of seconds/minute. The annotations are weak; a subject may express the labeled emotion for some seconds, few times in the whole recording, and being neutral the rest of the time. We advance the state- of-the-art permitting the input of the full sequences, thus enabling attention to find correlation in both short and long range dependencies. We compare our approach with with state-of-the-art methods for the input of physiological data  [31]  and for the combination of multimodal signals  [30] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Videomae For Long Input Videos",
      "text": "We use the Hugging Face implementation of VideoMAE, with the ViT-B encoder of 12 layers, hidden size of 768, and 87M parameters. During pre-training, the input video is masked with a ratio of 90% and fed into the encoder, which outputs the latent representations. The shallow decoder then takes the latent representations from the encoder and reconstructs the input videos. During fine-tuning, the pre-trained encoder is learned to predict binarized valence and arousal from original (i.e. not masked) videos. We perform experiments on two datasets, AMI-GOS  [17]  and DEAP  [15] . We implemented and tested multiple configurations of pre-training strategy. (i) Self-supervised pre-training on Kinetics-400 and finetuning on the target dataset (i.e. AMIGOS or DEAP). (ii) Self-supervised pretraining on Kinetics-400, plus AMIGOS and DEAP, and fine-tuning on the target dataset (see Figure  2 ). (iii) Self-supervised pre-training on Kinetics-400, plus an intermediate step of supervised pre-training on Kinetics-400 and one of the two target datasets and fine-tuning on the other target dataset. Table  2  shows input video datasets used. Figure  2  represents the pipeline of VideoMAE pre-training, with masked input video and reconstruction task, and the fine-tuning step to predict valence and arousal.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Selected Backbones For Features Extraction",
      "text": "Vision backbone. Our video backbone is composed of 1D-CNN v followed by MLP v . It takes as input the AUs matrix of the full input video and outputs embeddings that are used as tokens in transformer. Physiological backbone. Our physiological backbone extracts temporal features from the entire raw physiological data using 1D-CNN p and MLP p layer. We expanded the use of the sole 1D-CNN p  [31] , adding the MLP p , to enable the handling of long sequences. The extracted features are used as tokens in transformer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Mvp Architecture",
      "text": "Figure  1  shows the MVP architecture. Here, one input trial is exemplified, consisting of a video sequence SV ij and a physiological sequence SP ij , of shapes T V max × 42 and T P max × 2, respectively. In our experiments, we use a batch size of 8, so at each training iteration we randomly choose 8 trials and perform all the calculations for this batch. The input data enters the video and physiological backbones. For the video input, the zero-padded matrix with AUs enters 1D-CNN v followed by MLP v . For the physiological data, the zero-padded matrix with concatenated ECG and EDA enters 1D After that, the outputs from 1D-CNNs are used as input into transformer with cross-attention for fusing video with physiological signals. The MVP method uses mid-fusion fusion, where video modality is input as keys (k) and values (v) and physiological one as queries (q) (Equation  1 ).\n\nwhere Q p is the query from the physiological data and K v and V v are the keys and the values from the video data. transformer has 8 attention heads and 8 attention layers and it predicts two outputs: valence and arousal, binarized, as reported in Figure  1 . We use binary cross entropy (BCE) to calculate the loss.\n\nDuring training the loss is backpropagated through the neural network layers to calculate the gradient for each weight.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Handling",
      "text": "The entire sequence of video and physiological data is used, with a length that can vary between 60s and 155s. As the adopted datasets contain multiple trials from different subjects, the input data for a trail i of a subject j is composed of two matrices: SV ij for video and SP ij for physiological signals, with dimensions [T V ij , 42] and [T P ij , 2], respectively, where T V ij and T P ij correspond to lengths of the sequences and 42 and 2 are input data dimensions of each modality.Video and physiological signals are sampled at different rates, therefore T V ij and T P ij are different (e.g., for the AMIGOS dataset T V max is 2.8k and T P max is 19.9k, corresponding to 155s sampled at 18fps and 128Hz, for the video and the physiological data, respectively). We extract 42 visual features from each video frame and use the sequence of AUs and eye gaze as video representation. AUs are fine-grained facial muscle movements  [9] , each of which relates to a subset of extracted facial landmarks  [19] . From each frame we extract the following AUs: 1, 2, 4, 5, 6, 7, 9, 10,  12, 14, 15, 17, 20, 23, 25, 26, 28, and 45 , by means of the Open-Face library  [3] . Each AU is described in two ways: presence, if AU is visible in the face, and intensity, how intense is the AU on a 5-point scale. The eye gaze corresponds to two gaze direction vectors of each eye. The physiological data has a dimension of 2 at each timestamp, corresponding to the 2D time series, made of ECG and EDA raw signals.\n\nBefore using the data as input to our model, we perform the following preprocessing. We find the longest sequence and use its T V max and T P max lengths to construct input matrices for all the trials in the dataset. Each pair of input sequences SV ij , SP ij is zero-padded so that all input matrices with AUs and physiological signals have sizes T V max × 42 and T P max × 2, respectively.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotion Recognition Labels",
      "text": "Emotion classification is typically approached by measuring arousal and valence based on the emotion circumplex model  [21] . Valence refers to the degree of positive or negative affect associated with an emotion. Arousal refers to the degree of activation, engagement associated with an emotion. The valence and arousal dimensions have been used to categorize emotions into quadrants (see Figure  1 ): high valence/high arousal (e.g., excitement, happiness), high valence/low arousal (e.g., relaxation), low valence/high arousal (e.g., anger), and low valence/low arousal (e.g., boredom, tiredness). In the classification task these labels are typically extracted by the Self-Assessment Manikin (SAM) scale reported by each subject. SAM is a pictorial tool for rating valence and arousal on a scale of 1 to 9. A subject selects the pictogram that best corresponds to its emotional state at the end of each elicitation video. Therefore, such labels are more subjective and referred to the whole session, that typically last between 1 and 2 minutes. In Figure  3 , the label distribution of the used datasets is reported.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Datasets And Pre-Processing",
      "text": "The AMIGOS  [17]  and the DEAP  [15]  datasets are available online and comprise both video and physiological data. The recordings are performed in a laboratory setting and the participants are instructed to watch emotional videos and then respond to the Self-Assessment Manikin scale. The AMIGOS dataset includes the recording of 40 subjects, who rated 16 movie video clips for valence, arousal, and other measures (i.e., dominance, liking, familiarity and basic emotions). The DEAP dataset includes the recording of 32 subjects, who rated 40 music video clips for valence, arousal and other measures. We use the facial videos and EDA + ECG for AMIGOS, and the facial videos and EDA + PPG for DEAP, preprocessed as follow.\n\nFor VideoMAE, the input videos are cut into smaller clips of 4 seconds, from which 16 frames are taken and cropped using facial landmarks generated by OpenFace  [26] . Two different cropping strategies were evaluated (Figure  4 ). The larger one captures the entire face and a bit of background, ensuring the face remains visible even when the subject head is in motion, the smaller one captures exclusively the face, causing occasionally and partially cropping out. We experimented the two cropping strategies, achieving higher performances (7-9%) with the larger crops; thereafter this is the set-up adopted in this work.\n\nFor the AUs, we use OpenFace  [1]  to extract AUs, as it is a stable and largely adopted library in the community. 42 AUs and eye gaze direction features are extracted for each frame.\n\nFor the physiological data, we downsampled to 128Hz, then we filtered the signals with Butterworth filter plus powerline filter (to remove the 50Hz). The Butterworth filters used are: 3th order band-pass with [0.5,8]Hz band for PPG, 5th order high-pass with a 0.5Hz cutoff for ECG, 4th order low-pass with a 3Hz cutoff for EDA. In the AMIGOS dataset, we cut the first 1s in all the physiological signals, as artefacts are present in most of the recordings (probably due to the device connection). Since the signal sequences have different lengths, we pad the shorter sequences with zeros, so that all input sequences have the same length. The raw data is normalized with zero-mean and unity-variance before entering the pipeline.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experimental Set-Up",
      "text": "We use 5-fold cross validation to split data into train and test subject independently. All trials of one subject are either in the train or in the test split. We use the weighted F1-scores as metric, as the datasets are imbalanced, reporting the mean and standard deviation.\n\nWe split valence and arousal continuous labels using the thresholds of 4.5 and 5 for AMIGOS and DEAP, respectively. If the continuous label is less or equal than the threshold, the binarized label is 0 and 1 otherwise. VideoMAE for emotion recognition. The results in Tables  3a  and 3b  are related to the valence prediction for the AMIGOS and DEAP datasets using the VideoMAE self-supervised pre-training. The results show that the most effective approach is the three steps strategy, where, after the SSL pre-training on Kinetics-400, a supervised pre-training step is added before fine-tuning on the target dataset. Kinetics-400 is biggest dataset typically used to pre-train Video-MAE. This is related to the need of having data with the same distribution in the pre-training step.\n\nWhile increasing the amount of data in the self-supervised pre-training phase does not achieve the highest weighted F1-score, it is still more effective than solely relying on Kinetics-400 for supervised pre-training or not using it at all. Additionally, when the supervised pre-training step is added, the model adapts faster. For instance, with DEAP, the model requires only 3 epochs to fine-tune, as opposed to the initial 10. This further supports the intuition that leveraging datasets from the same domain can significantly benefit the learning process.\n\nThe unimodal transformer backbone uses 10s subsequences and 1D-CNN while our backbone is made of 1D-CNN + MLP and data is input as whole sequences. The experiments are run on the ECG signal of the AMIGOS dataset. Table  4a  shows that our proposed approach outperforms the state-of-the-art by around 3 percentage points in predicting valence. This means that a deeper temporal understanding is beneficial for the valence.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "The need of combining video and physiological data through a novel DL architecture is investigated. We compare the MVP method with classical machine learning approaches and more recent transformer-based architectures proposed for both unimodal and multimodal data. The classical machine learning pipeline used here exploits hand-crafted (HC) features as input to Multi-Layer Perceptron (MLP) and Support Vector Machine (SVM) to classify valence and arousal  [5] . All transformer models are used with adaptations to ensure fair comparison. In UniTransformer we combine ECG and EDA, whereas the original UniTransformer uses only ECG as input. In MultiTransformer, we add AUs as video component, using ECG+EDA as one input and AUs as the second one. The original MultiTransformer uses ECG and EEG. The main difference between MultiTransformer and MVP is the fusion. In MultiTransformer late fusion is used, while MVP uses mid-fusion. For VideoMAE we have studied and adapted the method to the case of long facial videos. Tables  4a  and 4a  show that transformers outperform classical machine learning. And the use of mid-fusion and a temporal model able to input the full sequence, as is done in MVP, gives the best results. This is due to the capability of cross-attention to find long-range dependencies between different kinds of input data.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Success Case Analysis",
      "text": "We conduct case studies to show videos and physiological sequences in which the proposed MVP method outperforms state-of-the-art competitors. We compare our MVP with MLP proposed by StressID for multimodal fusion  [5] . Table  4b  shows that MVP outperforms MLP on the DEAP dataset. Figures  5a  and 5b  demonstrate the sequences where MVP gives better results than the competitor. We show both physiological signals and video frames of our success cases. Figure  5a  shows the case where the true arousal is 1, our MVP method predicts 1, and the competitor predicts 0. The video frames above show that the participant is initially neutral, and after a few seconds they smile, while at the same time the physiological signals change. The result demonstrates that MVP better captures the dynamics of the sequence. Figure  5b  shows the case where the true valence is 0, our MVP method predicts 0, and the competitor predicts 1. The video frames above show that the participant remains neutral throughout the experiment, without any changes except blinking while the physiological signal changes. This success case shows that MVP better fuses multiple modalities than MLP.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "We propose a new method for combining complementary data, the behavioral and physiological component of emotions. The method relies on the relevance of inputting long full sequences, exploiting attention. Regarding the video input, AUs are performing better than deep learning features in extracting facial representations for emotion recognition. Nevertheless, a strategy that includes similar data distribution in the pre-training step shows improved results for deep learning features. Our experimental results showcase that fusing video and physiological signals outperforms each modality individually. MVP achieves state-ofthe-art results in the challenging field of multimodal emotion recognition with video and physiological data, where small datasets are available. With this work we aim to foster research in the field, to improve the understanding of human emotions expressed by behavioral and physiological signals.\n\nFuture works involve exploiting ECG and EDA data as two separated modalities, fused by a transformer model. We will develop a dedicated pre-training technique, in order to increase recognition capabilities of the model and to bring insight into physiological and behavioral modalities.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Multimodal Emotion Recognition based on Video and Physiological Signals",
      "page": 2
    },
    {
      "caption": "Figure 2: ). (iii) Self-supervised pre-training on Kinetics-400, plus an",
      "page": 6
    },
    {
      "caption": "Figure 2: represents the pipeline of VideoMAE pre-training,",
      "page": 6
    },
    {
      "caption": "Figure 2: Pre-training and fine-tuning steps of VideoMAE in emotion recognition. In",
      "page": 7
    },
    {
      "caption": "Figure 1: shows the MVP architecture. Here, one input trial is exemplified, con-",
      "page": 7
    },
    {
      "caption": "Figure 1: We use binary cross entropy (BCE) to calculate the loss.",
      "page": 8
    },
    {
      "caption": "Figure 3: Label distribution of AMIGOS and DEAP datasets.",
      "page": 9
    },
    {
      "caption": "Figure 3: , the label distribution of the used datasets is reported.",
      "page": 9
    },
    {
      "caption": "Figure 4: Crop comparison on the Amigos dataset. The larger crop X captures the entire",
      "page": 10
    },
    {
      "caption": "Figure 5: a shows the case where the true arousal is 1, our MVP method predicts",
      "page": 13
    },
    {
      "caption": "Figure 5: b shows the case where the true",
      "page": 13
    },
    {
      "caption": "Figure 5: The success cases for the arousal and valence prediction on the DEAP dataset.",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: State-of-the-art multimodal emotion recognition methods. V means visual,",
      "data": [
        {
          "2019": "2020",
          "UDLBV [24]": "TBJE [34]",
          "V, P": "V, A, L",
          "VGG-16 (V)": "R(2+1)D-152 outputs (V)"
        },
        {
          "2019": "2021",
          "UDLBV [24]": "BBMF [11]",
          "V, P": "V, A, L",
          "VGG-16 (V)": "Action Units (V)"
        },
        {
          "2019": "2022",
          "UDLBV [24]": "M-SENA [16]",
          "V, P": "V, A, L",
          "VGG-16 (V)": "Action Units (V)\nFacial Landmarks (V)\nEye Gaze (V)"
        },
        {
          "2019": "",
          "UDLBV [24]": "2022 MultiTransformer [30]",
          "V, P": "P (ECG, EEG)",
          "VGG-16 (V)": "1D-CNN + Transformer (P)"
        },
        {
          "2019": "2023",
          "UDLBV [24]": "EHCI [18]",
          "V, P": "V, P",
          "VGG-16 (V)": "Hand-Crafted (V, P)"
        },
        {
          "2019": "2023",
          "UDLBV [24]": "StressID [5]",
          "V, P": "V, P (ECG, EDA)",
          "VGG-16 (V)": "Hand-Crafted (V, P)"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Openface: A general-purpose face recognition library with mobile applications",
      "authors": [
        "B Amos",
        "B Ludwiczuk",
        "M Satyanarayanan"
      ],
      "year": "2016",
      "venue": "Openface: A general-purpose face recognition library with mobile applications"
    },
    {
      "citation_id": "2",
      "title": "Recognition of emotional states using frequency effective connectivity maps through transfer learning approach from electroencephalogram signals",
      "authors": [
        "S Bagherzadeh",
        "K Maghooli",
        "A Shalbaf",
        "A Maghsoudi"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "3",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "4",
      "title": "Marlin: Masked autoencoder for facial video representation learning",
      "authors": [
        "Z Cai",
        "S Ghosh",
        "K Stefanov",
        "A Dhall",
        "J Cai",
        "H Rezatofighi",
        "R Haffari",
        "M Hayat"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "5",
      "title": "StressID: a multimodal dataset for stress identification",
      "authors": [
        "H Chaptoukaev",
        "V Strizhkova",
        "M Panariello",
        "B Dalpaos",
        "A Reka",
        "V Manera",
        "S Thummler",
        "E Ismailova",
        "N Bremond",
        "F Todisco",
        "M Zuluaga",
        "M Ferrari"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "6",
      "title": "A transformer-based jointencoding for emotion recognition and sentiment analysis",
      "authors": [
        "J Delbrouck",
        "N Tits",
        "M Brousmiche",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "Annual Meeting of the Association for Computational Linguistics (ACL"
    },
    {
      "citation_id": "7",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "S Ebrahimi Kahou",
        "V Michalski",
        "K Konda",
        "R Memisevic",
        "C Pal"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition from physiological signal analysis: A review",
      "authors": [
        "M Egger",
        "M Ley",
        "S Hanke"
      ],
      "year": "2019",
      "venue": "Electronic Notes in Theoretical Computer Science"
    },
    {
      "citation_id": "9",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "10",
      "title": "Multi-modal emotion recognition using recurrence plots and transfer learning on physiological signals",
      "authors": [
        "R Elalamy",
        "M Fanourakis",
        "G Chanel"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII52823.2021.9597442"
    },
    {
      "citation_id": "11",
      "title": "Bibimodal modality fusion for correlation-controlled multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "A Gelbukh",
        "A Zadeh",
        "L Philippe Morency",
        "S Poria"
      ],
      "year": "2021",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "12",
      "title": "Physiological signals based human emotion recognition: a review",
      "authors": [
        "S Jerritta",
        "M Murugappan",
        "R Nagarajan",
        "K Wan"
      ],
      "year": "2011",
      "venue": "IEEE"
    },
    {
      "citation_id": "13",
      "title": "Hetemotionnet: two-stream heterogeneous graph recurrent neural network for multi-modal emotion recognition",
      "authors": [
        "Z Jia",
        "Y Lin",
        "J Wang",
        "Z Feng",
        "X Xie",
        "C Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Stress and heart rate variability: a meta-analysis and review of the literature",
      "authors": [
        "H Kim",
        "E Cheon",
        "D Bai",
        "Y Lee",
        "B Koo"
      ],
      "year": "2018",
      "venue": "Psychiatry investigation"
    },
    {
      "citation_id": "15",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "16",
      "title": "M-sena: An integrated platform for multimodal sentiment analysis",
      "authors": [
        "H Mao",
        "Z Yuan",
        "H Xu",
        "W Yu",
        "Y Liu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Annual Meeting of the Association for Computational Linguistics System Demonstration Track (ACL"
    },
    {
      "citation_id": "17",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition framework using multiple modalities for an effective human-computer interaction",
      "authors": [
        "A Moin",
        "F Aadil",
        "Z Ali",
        "D Kang"
      ],
      "year": "2023",
      "venue": "The Journal of Supercomputing"
    },
    {
      "citation_id": "19",
      "title": "Configural representation of facial action units for spontaneous facial expression recognition in the wild",
      "authors": [
        "N Perveen",
        "C Mohan"
      ],
      "year": "2020",
      "venue": "VISIGRAPP"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data",
      "authors": [
        "K Ross",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "21",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "22",
      "title": "Machine learning for stress detection from electrodermal activity: A scoping review",
      "authors": [
        "R Sánchez-Reolid",
        "M López",
        "A Fernández-Caballero"
      ],
      "year": "2020",
      "venue": "Machine learning for stress detection from electrodermal activity: A scoping review"
    },
    {
      "citation_id": "23",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "24",
      "title": "Utilizing deep learning towards multimodal bio-sensing and vision-based affective computing",
      "authors": [
        "S Siddharth",
        "T Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Video representation learning for conversational facial expression recognition guided by multiple angles reconstruction",
      "authors": [
        "V Strizhkova",
        "L Ferrari",
        "H Kachmar",
        "A Dantcheva",
        "F Bremond"
      ],
      "year": "2024",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshop"
    },
    {
      "citation_id": "26",
      "title": "Openface: An open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrušaitis",
        "L Peter Robinson"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Videomae: Masked autoencoders are dataefficient learners for self-supervised video pre-training",
      "authors": [
        "Z Tong",
        "Y Song",
        "J Wang",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition using a reduced set of eeg channels based on holographic feature maps",
      "authors": [
        "A Topic",
        "M Russo",
        "M Stella",
        "M Saric"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition with pre-trained transformers using multimodal signals",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "31",
      "title": "Transformer-based self-supervised learning for emotion recognition",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2022",
      "venue": "International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "32",
      "title": "Videomae v2: Scaling video masked autoencoders with dual masking",
      "authors": [
        "L Wang",
        "B Huang",
        "Z Zhao",
        "Z Tong",
        "Y He",
        "Y Wang",
        "Y Wang",
        "Y Qiao"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "33",
      "title": "Multi-task learning framework for emotion recognition in-thewild",
      "authors": [
        "T Zhang",
        "C Liu",
        "X Liu",
        "Y Liu",
        "L Meng",
        "L Sun",
        "W Jiang",
        "F Zhang",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision Workshop"
    },
    {
      "citation_id": "34",
      "title": "Transformer-based multimodal information fusion for facial expression analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "S Wang",
        "H Zeng",
        "Z Zhang",
        "R An",
        "B Ma",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshop"
    }
  ]
}