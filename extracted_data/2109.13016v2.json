{
  "paper_id": "2109.13016v2",
  "title": "Semi-Supervised Adversarial Discriminative Domain Adaptation",
  "published": "2021-09-27T12:52:50Z",
  "authors": [
    "Thai-Vu Nguyen",
    "Anh Nguyen",
    "Nghia Le",
    "Bac Le"
  ],
  "keywords": [
    "Domain adaptation",
    "Semi-supervised domain adaptation",
    "Semisupervised adversarial discriminative domain adaptation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Domain adaptation is a potential method to train a powerful deep neural network across various datasets. More precisely, domain adaptation methods train the model on training data and test that model on a completely separate dataset. The adversarial-based adaptation method became popular among other domain adaptation methods. Relying on the idea of GAN, the adversarialbased domain adaptation tries to minimize the distribution between the training and testing dataset based on the adversarial learning process. We observe that the semi-supervised learning approach can combine with the adversarial-based method to solve the domain adaptation problem. In this paper, we propose an improved adversarial domain adaptation method called Semi-Supervised Adversarial Discriminative Domain Adaptation (SADDA), which can outperform other prior domain adaptation methods. We also show that SADDA has a wide range of applications and illustrate the promise of our method for image classification and sentiment classification problems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Over the past few years, deep neural networks have achieved significant achievements in many applications. One of the major limitations of deep neural networks is the dataset bias or domain shift problems  [1] . These phenomena occur when the model obtains good results on the training dataset; however, showing poor performance on a testing dataset or a real-world sample.\n\nAs shown in Figure  1 , because of numerous reasons (illumination, image quality, background), there is always a different distribution between two datasets, which is the main factor reducing the performance of deep neural networks. Even though various research has proved that deep neural networks can learn transferable feature representation over different datasets  [2, 3] , Donahue et al.  [4]  showed that domain shift still influences the accuracy of the deep neural network when testing these networks in a different dataset.\n\nFig.  1 : Examples of images from different datasets. (a) Some digit images from MNIST  [5] , USPS  [6] , MNIST-M  [7] , and SVHN  [8]  datasets. (b) Some object images from the \"bird\" category in CALTECH  [9] , LABELME  [10] , PASCAL  [11] , and SUN  [12]  datasets.\n\nThe solution for the aforementioned problems is domain adaptation techniques  [13, 14] . The main idea of domain adaptation techniques is to learn how a deep neural network can map the source domain and target domain into a common feature space, which minimize the negative influence of domain shift or dataset bias.\n\nThe adversarial-based adaptation method  [15, 16]  has become a well-known technique among other domain adaptation methods. Adversarial adaptation includes two networks -an encoder and a discriminator, trained simultaneously with conflicting objectives. The encoder is trained to encode images from the original domain (source domain) and new domain (target domain) such that it puzzles the discriminator. In contrast, the discriminator tries to distinguish between the source and target domain. Recently, Adversarial Discriminative Domain Adaptation (ADDA) by Tzeng et al.  [16]  has shown that adversarial adaptation can handle dataset bias and domain shift problems. From there, we extend the ADDA method to the semi-supervised learning context by obliging the discriminator network to predict class labels.\n\nSemi-supervised learning  [17]  is an approach that builds a predictive model with a small labeled dataset and a large unlabeled dataset. The model must learn from the small labeled dataset and somehow exploit the larger unlabeled dataset to classify new samples. In the context of unsupervised domain adaptation tasks, the semi-supervised learning approach needs to take advantage of the labeled source dataset to map to the unlabeled target dataset, thereby correctly classifying the labels of the target dataset. The Semi-Supervised GAN  [18]  is designed to handle the semi-supervised learning tasks and inspired us to develop our model.\n\nIn this paper, we present a novel method called Semi-supervised Adversarial Discriminative Domain Adaptation (SADDA), where the discriminator is a multiclass classifier. Instead of only distinguishing between source images and target images (method like ADDA  [16] ), the discriminator learns to distinguish N + 1 classes, where N is the number of classes in the classification task, and the last one uses to distinguish between the source dataset or the target dataset. The discriminator focuses not only on the domain label between two datasets but also on the labeled images from the source dataset, which improves the generalization ability of the discriminator and the encoder as well as the classification accuracy.\n\nTo validate the effectiveness of our methodology, we experiment with domain adaptation tasks on digit datasets, including MNIST  [5] , USPS  [6] , MNIST-M  [7] , and SVHN  [8] . In addition, we also prove the robustness ability of the SADDA method by using t-SNE visualization of the digit datasets, the SADDA method keeps the t-SNE clusters as tight as possible and maximizes the separation between two clusters. We also test its potential with a more sophisticated dataset, by object recognition task with CALTECH  [9] , LABELME  [10] , PASCAL  [11] , and SUN  [12]  datasets. In addition, we evaluate our method for the natural language processing task, with three text datasets including Women's E-Commerce Clothing Reviews  [19] , Coronavirus tweets NLP -Text Classification  [20] , and Trip Advisor Hotel Reviews  [21] . The Python code of the SADDA method for object recognition tasks can be downloaded at https://github.com/NguyenThaiVu/SADDA.\n\nOur contributions can be summarized as follows:\n\n-We propose a new Semi-supervised Adversarial Discriminative Domain Adaptation method (SADDA) for addressing the unsupervised domain adaptation task. -We illustrate that SADDA improves digit classification tasks and achieves competitive performance with other adversarial adaptation methods. -We also demonstrate that the SADDA method can apply to multiple applications, including object recognition and natural language processing tasks.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Domain adaptation is an active research field, which can handle numerous problems such as imbalanced data  [22] , dataset bias  [23] , and domain shift  [24] . Recent research has focused on domain adaptation from a labeled source dataset to an unlabeled target dataset, also known as unsupervised domain adaptation  [25, 26] . The principle technique is minimizing the distinction between the source and target distribution  [27] . Some popular approaches are Maximum Mean Discrepancy (MMD)  [1] , deep reconstruction classification network (DRCN)  [28]  or Autoencoder-based domain adaptation  [29] .\n\nAdversarial-based domain adaptation. With the rise of generative adversarial networks  [15] , the adversarial-based made huge advancements in the domain adaptation task  [30, 31, 32] . Adversarial-based techniques try to achieve domain adaptation by using domain discriminators, which increases domain con-fusion through an adversarial process. A popular adversarial-based domain adaptation method is the Adversarial Discriminative Domain Adaptation (ADDA) by Tzeng et al.  [16] . ADDA approach aims to diminish the distance between the source encoder and target encoder distributions through the domain-adversarial process. However, this method only distinguishes between the source and target domain. Instead, our SADDA method not only predicts whether the source domain or the target domain, but also classifies the label of the source dataset. More concretely, we force the adversarial-based method to the semi-supervised context. We will show that this creation can produce a more efficient classification model.\n\nCombining adversarial-based domain adaptation with other auxiliary tasks. Recently, some works have focused on combining auxiliary tasks for adversarial-based adaptation to exploit more information  [33, 34] . Xavier and Bengio introduce Stacked Denoising Autoencoders  [35, 36] , reconstructing the merging data from numerous domains with the same network, such that the representations can be symbolized by both the source and target domain. Deep reconstruction classification network (DRCN)  [28]  attempts to solve two sub-problem at the same time: classification of the source data, and reconstruction of the unlabeled target data. However, these auxiliary tasks are not towards the same goal. We observe that during the adversarial process, we can classify the source or target dataset and predict the label of the source dataset simultaneously. That allows us to re-use the same output layers in the discriminator model as well as forces two discriminator models towards the same goal (sub-section 3.2 for more details). In addition, we also demonstrate that our SADDA method not only applies to computer vision tasks but also the natural language processing task.  In this section, we describe in detail our Semi-supervised Adversarial Discriminative Domain Adaptation (SADDA) method. An overview of our method can be found in Figure  2 .\n\nIn the unsupervised domain adaptation task, we already have source images X s and source labels Y s come from the source domain distribution p s (x, y). Besides that, a target dataset X t comes from a target distribution p t (x, y), where the label of the target dataset is non-exist. We desire to learn a target encoder M t and classifier C t , which can accurately predict the target image's label. In an adversarial-based adaptation approach, we aim to diminish the distance between the source mapping distribution (M s (X s )) and target mapping distributions (M t (X t )). As a result, we can straightly apply the source classifier C s to classify the target images, in other words, C = C t = C s . The summary process of SADDA includes three steps: pre-training, training target encoder, and testing.\n\nPre-training. In the pre-training phase, training source encoder (M s ) and source classifier (C s ), by using the source labeled images (X s ,Y s ). This step is a standard supervised classification task, a common form can be denoted as:\n\nwhere L cls is a supervised classification loss (categorical crossentropy loss), and N is the number of classes.\n\nTraining target encoder. In the training target encoder phase, we first present a training discriminator process and then present a procedure for training the target encoder.\n\nFirstly, training the discriminator (D) in two modes, each giving a corresponding output. (1) Supervised mode, where the supervised discriminator (D sup ) predicts N labels from the original classification task. (2) Unsupervised mode, where the unsupervised discriminator (D unsup ) classifies between X s and X t . Discriminator correlates with unconstrained optimization:\n\narg min\n\nIn equation 2, L cls is a supervised classification loss corresponding to predicting N labels from the original classification task in the source dataset (X s ), which will update the parameter in D sup . In equation (3), L adv D is an adversarial loss for unsupervised discriminator D unsup , which trains (D unsup ) to maximize the probability of predicting the correct label from the source dataset or target dataset. One thing to notice is that the unsupervised discriminator uses a custom activation function (equation 5), which returns a probability to determine whether a source image or target image (sub-section 3.2 for more details).\n\nSecondly, training the target encoder M t with the standard loss function and inverted labels  [15] . This implies that the unsupervised discriminator D unsup is fooled by the target encoder M t , in other words, D unsup is unable to determine\n\nTesting. In the testing phase, we concatenate the target encoder M t and the source classifier C s to predict the label of target images X t .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Discriminator Model",
      "text": "In this section, we describe the detail of the discriminator model and provide some arguments to prove the effectiveness of the discriminator model in the SADDA method.\n\nIn the training target encoder step, the discriminator model is trained to predict N+1 classes, where N is the number of classes in the original classification task (supervised mode) and the final class label predicts whether the sample comes from the source dataset or target dataset (unsupervised mode). The supervised discriminator and the unsupervised discriminator have different output layers but have the same feature extraction layers -via backpropagation when we train the network, updating the weights in one model will impact the other model as well.\n\nThe supervised discriminator model produces N output classes (with a softmax activation function). The unsupervised discriminator is defined such that it grabs the output layer of the supervised mode prior softmax activation and computes a normalized sum of the exponential outputs (custom activation). When training the unsupervised discriminator, the source sample will have a class label of 1.0, while the target sample will be labeled as 0.0. The explicit formula of custom activation  [37]  is:\n\nwhere\n\nThe experiment of equation (  5 ) is described in Table  1 , and the outputs are between 0.0 and 1.0. If the probability of output value prior softmax activation is a large number (meaning: low entropy) then the custom activation output value is close to 1.0. In contrast, if the output probability is a small value (meaning: high entropy), then the custom activation output value is close to 0.0. Implied that the discriminator is encouraged to output a confidence class prediction for the source sample, while it predicts a small probability for the target sample. That is an elegant method allowing re-use of the same feature extraction layers for both the supervised discriminator and the unsupervised discriminator.\n\nIt is reasonable that learning the well supervised discriminator will improve the unsupervised discriminator. Moreover, training the discriminator in unsupervised mode allows the model to learn useful feature extraction capabilities from huge unlabeled datasets. As a sequence, improving the supervised discriminator will improve the unsupervised discriminator and vice versa. Improving the discriminator will enhance the target encoder  [18] . In total, this is one kind of advantage circle, in which three elements (unsupervised discriminator, supervised discriminator, and target encoder) iteratively make each other better.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Guideline For Stable Sadda",
      "text": "In general, training SADDA is an extremely hard process, there are two losses we need to optimize: the loss for the discriminator and the loss for the target encoder. For that reason, the loss landscape of SADDA is fluctuating and dynamic (detail in sub-section 4.4). When implementing and training the SADDA, we find that is a tough process. To overcome the limitation of the adversarial process, we present a full architecture of SADDA. This designed architecture increases training stability and prevents non-convergence. In this section, we present the key ideas in designing the model for the image classification and the sentiment classification task.\n\nImage classification. The design of SADDA is inspired by Deep Convolutional GAN (DCGAN) architecture  [38] . The summary architecture of the SADDA method for digit recognition is shown in Figure  5 . On the one hand, the encoder is used to capture the content in the image, increasing the number of filters while decreasing the spatial dimension by the convolutional layer. On the other hand, the discriminator is symmetric expansion with the encoder by using fractionallystrided convolutions (transpose convolution).\n\nMoreover, our recommendation for efficient training SADDA in the image classification task:\n\n• Replace any pooling layers with convolution layers (or transposed convolution) with strides larger than 1. • Remove fully connected layers in both encoder and discriminator (except the last fully connected layers, which are used for prediction). • Use ReLU activation  [39]  in the encoder and LeakyReLU activation  [39]  (with alpha=0.2) in the discriminator.\n\nSentiment classification. The design of the SADDA method for sentiment classification is inspired by the architecture called Autoencoders LSTM  [40, 41] . The summary architecture of the SADDA method for sentiment classification is demonstrated in Figure  7 . In general, the architecture of the model used in the sentiment classification task has many similarities with the architecture used in image classification. Firstly, we remove fully connected layers in both the encoder and the discriminator. Instead, we use the Long Short Term Memory  [42]  (LSTM) to handle sequences of text data. Secondly, the network is organized into an architecture called the Encoder-Decoder LSTM, with the Encoder LSTM being the encoder block and the Decoder LSTM being the discriminator block respectively. The Encoder-Decoder LSTM was built for the NLP task where it illustrated stateof-the-art performance, such as machine translation  [43] . From the empirical, we find that the Encoder-Decoder is suitable for the unsupervised domain adaptation task.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we evaluate our SADDA method for unsupervised domain adaptation tasks in three scenarios: digit recognition, object recognition, and sentiment classification.\n\nIn the experiments, we focus on probing how the SADDA method improves the unsupervised domain adaptation task. For this purpose, we only choose shallow architecture rather than a deep network. We leave the sophisticated design for a future job.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Digit Recognition",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets And Domain Adaptation Scenarios",
      "text": "We evaluate SADDA on various unsupervised domain adaptation experiments, examining the following popular used digits datasets and settings (the visualization is in Figure  1 ):\n\nMNIST ←→ USPS: MNIST  [5]  includes 28x28 pixels, which are grayscale images of digit numbers. USPS  [6]  is a digit dataset, which contains 9298 grayscale images. The image is 16x16 pixels. In this experiment, we follow the evaluation protocol of  [44] .\n\nMNIST → MNIST-M: MNIST-M  [7]  is made by merging MNIST digits with the patches arbitrarily extracted from color images of BSDS500  [45] . In this experiment, we set the input size is 28x28x3 pixels, and we follow the evaluation protocol of  [44]  SVHN → MNIST: The Street View House Number (SVHN)  [8]  is a digit dataset, which contains 600000 32×32 RGB images. In this experiment, we convert the SVHN dataset to grayscale images and resize the MNIST images into 32x32 grayscale images. We use the evaluation protocol of  [28] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implementation Details",
      "text": "The SADDA model is trained with different learning rates in different phases. In the pre-training phases, this is a standard classification task, we use a learning rate is 0.001 in our experiment. In the training target encoder phases, we suggested a learning rate of 0.0002 as well as an Adam optimizer  [46]  and setting the β 1 equal to 0.5 to help stabilize training. In the LeakyReLU activation, we set α = 0.2 in the whole model.\n\nIn this experiment, the encoder consists of four convolutional layers with 4 x 4 kernel size, 2 x 2 strides, same padding, and ReLU activation. The number of filters for four convolution layers are 32, 64, 128, and 256, respectively. The target encoder has the same architecture as the source encoder, and the source encoder is used as an initialization for the target encoder.\n\nThe classifier takes the outputs of the encoder as input. Next, a fully connected layer with 100 feature channels, followed by ReLU activation. Finally, the fully connected layer with ten feature channels and the softmax activation.\n\nIn the training target encoder phases, the outputs of the encoder serve as the input of the discriminator. The discriminator consists of four Transpose Convolutional layers with 4 x 4 kernels size, 2 x 2 strides, the same padding, and the Leaky ReLU activation (alpha = 0.2). The number of kernels for four Transpose Convolutional is 256, 128, 64, and 32, respectively. We illustrate the overall architecture in Figure  5 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results On Digit Datasets",
      "text": "In this experiment, we compare our SADDA method against multiple state-of-theart unsupervised domain adaptation methods. Table  2 : Experimental results on unsupervised domain adaptation on digit datasets. The results are not re-implement, instead, we select based on the available result in the previous publication (some experimental results have the standard deviation because that publication has the standard deviation while others do not.)\n\nExperimental results are shown in Table  2 . In the real-world dataset SVHN → MNIST, the SADDA model showed approximately 26% improvement over the Source only model, 10% more than the ADDA method  [16] . In addition, in the first two experiments (MNIST → USPS and USPS → MNIST), the SADDA method achieves extremely high accuracy, which results in 98.1% and 97.8% respectively. However, SADDA has a little lower accuracy than other methods like SBADA-GAN  [47] , SHOT  [48] , and DFA-MCD  [49]  in some experiments.\n\nAlthough, our method did not achieve the highest accuracy in any of the experiments. Our method still has competitive accuracy when compared with the state-of-the-art methods in the last two years (SHOT  [48] , DFA-MCD  [49] ). For example, our SADDA method compared with the DFA-MCD method, in the MNIST → USPS experiment, we have lower accuracy (98.1% versus 98.6%); however, in the USPS → MNIST experiment, we achieved a higher accuracy (SADDA achieved 97.8% compared to 96.6%).\n\nFor further insight into the SADDA model effect on the digit classification tasks, we use t-SNE  [50]  to visualize the 2D point of the last encoder layer of Fig.  3 : t-SNE embedding of digit classification, using (2 x 2 x 256) dimensional representation, with Source only (on the left) and SADDA (on the right) on the target dataset. Note that SADDA minimizes intra-class distance and maximizes inter-class distance.\n\nSADDA (as described in Figure  3 ). Ten labels are from 0 to 9 corresponding, and 100 samples per label. The domain invariance is determined by the degree of overlap between features. Regarding the Source only model, the distribution and the density are messy. In contrast, the SADDA method splits different labels into different regions, and the overlap is more prominent.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Object Recognition",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Datasets And Preprocessing",
      "text": "In this subsection, we present the experiments for evaluating the SADDA method. The experiment is performed on the VLCS  [23]  dataset, including PASCAL VOC2007 (V)  [11] , LABELME (L)  [10] , CALTECH (C)  [9] , and SUN (S)  [12]  datasets. Each dataset contains five categories: bird, car, chair, dog, and person. Since the number of images per class is not equal, we use data augmentation techniques to balance the number of images. In this experiment, we use the Albumentations library  [51]  to increase to 5000 images per class. We divide the dataset into a training set (60%), validation set (20%), and test set (20%).\n\nThe detailed architecture is shown in Figure  6 . The rest of the other installation (optimization algorithm, learning rate) is the same as section 4.1.2.\n\nIn the experiments, one dataset is used as the source domain and the rest is used as the target domain, resulting in four different cases (Table  3 ). In addition, we do not compare our SADDA model with other domain adaptation methods due to different setups.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "The results on the VLCS are shown in Table  3 . Source only is a model that only trains on the source dataset without using any domain adaptation methods. Overall, the accuracy when applying the SADDA method overcomes the Source only model in all cases. In some specific cases like PASCAL → CALTECH, the classification accuracy goes from 45.10 to 55.30 (improving approximately 10%).\n\nIn case LABELME → CALTECH, the accuracy grows from 32.75% to 39.36%.\n\nExamining the results in Table  3 , the Source only model has low accuracy, which reveals that the domain shift is quite large. In other words, the Source only model does not learn any knowledge about the source dataset to predict the target dataset. In contrast, the SADDA model learns a more useful feature representation, leading to higher accuracy when performing a prediction on the target dataset.\n\nAlthough the SADDA method has certain improvements compared to the Source only method, the accuracy of the SADDA method is still low and not ideal. For further comparison, we also test the hypothesis situation where the target labels are present (the train on target model). There is still a big gap between the accuracy of the SADDA method and the train on target method.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Sentiment Classification",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Datasets And Preprocessing",
      "text": "In this subsection, we evaluate the SADDA method for the sentiment classification task. We use three sentimental datasets, including Women's E-Commerce Clothing Reviews  [19] , Coronavirus tweets NLP -Text Classification  [20] , and Trip Advisor Hotel Reviews  [21] :\n\nWomen's E-Commerce Clothing Reviews  [19] . This is real commercial data, where the reviews are written by customers. In this task, we only use two features called Review Text (the raw text review) and Rating (the positive integer for the product, provided by the customer from 1 Worst to 5 Best). Regarding the Rating, we relabel into the sets {positive, neutral, negative} with the following rule: if a review is greater than 3, it is considered a positive comment; if a review is equal to 3, it is considered a neutral comment; if a review is less than 3, it is considered a negative comment.\n\nCoronavirus tweets NLP -Text Classification  [20] . The tweets were downloaded from Twitter and tagged manually. Although there are four columns in total, we only use the Original Tweet feature and Label in our experiment. In the case of Label, the original label includes Extremely Negative, Negative, Neutral, Positive, and Extremely Positive. However, we convert to the sets {positive, neutral, negative} respectively.\n\nTrip Advisor Hotel Reviews  [21] . This contains reviews crawled from the travel company called Tripadvisor. The dataset contains two features, including Review Text and Rating (the positive integer from 1 Worst to 5 Best). Regarding Rating, we process the same as the case Women's E-Commerce Clothing Reviews above.\n\nIn all three datasets, we perform the following text preprocessing steps: removing the punctuation, URL, hashtags, mentions, and stop words (with the support of the NLTK [52] library). We limit the input sentence to a max length equal to 50. The GloVe  [53]  word embedding is applied to map the word in the text review to the vector space.\n\nBecause the number of samples per class is not balanced, we use the data augmentation techniques to balance the number of samples -with the support of the TextAugment  [54]  library. Particularly, we perform data augmentation such that each class has up to 20 000 samples. The dataset is divided into a training set (60%), validation set (20%), and test set (20%).",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Experiments And Results",
      "text": "For this experiment, our architecture is illustrated in Figure  5 . Elements in that architecture such as the LSTM layer and the Repeat Vector layer are implemented by the TensorFlow library with default settings. The optimization algorithms and learning rates are set up as in section 4.1.2. In addition, we do not attempt to fine-tune the architecture and leave it for future work.\n\nThe results of our experiment are provided in Table  4 . Compared with the Source only model, the SADDA method shows a little improvement in the accuracy of this sentiment analysis task. For a certain experiment, like T → W, the classification accuracy goes from 41.07% to 49.28%. However, not all experiments improve, such as experiment T → C, the accuracy even dropped a bit (from 38.46% down to 38.05%). Additionally, a comparison with the \"Train on target\" model exposes that the SADDA model is far from the ideal model. We hope that is the motivation for future development.   When training the target encoder, we do not try to find a minimum value for either discriminator loss or adversarial loss. Instead, we are looking for a Nash equilibrium state for both losses  [37] . In practice, we observe the discriminator loss and adversarial loss after each epoch, when both loss values no longer change, we trigger an early stopping. Early stopping is the technique to stop the training process at a certain point before the model overfits the training dataset and has poor performance on the test set. In that case, we consider that our model has converged. Keep in mind that the loss of 0.0 in the discriminator loss or the adversarial loss during the training process is a failure mode.\n\nRegarding figure  4 , the convergence point is in the epoch 20. At that point, we will stop the training process because the discriminator loss and the adversarial loss are saturated at around 0.33 and 2.30 respectively. In that experiment (and other object recognition tasks), it took around 1 hour on a single Tesla T4 GPU to complete the training procedure.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a more stable and high-accuracy architecture for training adversarialbased domain adaptation methods. The key idea of this approach is to train discriminators in two modes: supervised mode and unsupervised mode. Moreover, utilize this to create a more efficient target encoder, which will help improve the classification accuracy.\n\nWhile the SADDA method has demonstrated an improvement in many tasks like image classification or sentiment classification, there are still open challenges. Particularly, the SADDA model in object recognition and sentiment classification is far from the desired accuracy model. We hope that the intuition of this research will facilitate further advances in domain adaptation tasks. Fig.  5 : The overview of the SADDA method for the digit recognition task. We found that Global Average Pooling (GAP)  [55]  increased model stability and reduce the number of parameter. Fig.  6 : The overview of the SADDA method for the object recognition task on the VLCS  [23]  dataset. With the input image's shape is 64 x 64 x 3 Fig.  7 : The overview of the SADDA method for the sentiment classification task. The input sentence has a max length equal to 50. In the design above, to prevent overfitting, the LSTM layer is always followed by the dropout layer with 0.2 rates. The numbers under the particular layer are the output shape of that layer.",
      "page_start": 14,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , because of numerous reasons (illumination, image qual-",
      "page": 2
    },
    {
      "caption": "Figure 1: Examples of images from diﬀerent datasets. (a) Some digit images from",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview of the SADDA. Firstly, training the source encoder (Ms)",
      "page": 4
    },
    {
      "caption": "Figure 2: In the unsupervised domain adaptation task, we already have source images",
      "page": 5
    },
    {
      "caption": "Figure 5: On the one hand, the encoder",
      "page": 7
    },
    {
      "caption": "Figure 7: In general, the architecture of the model used in the",
      "page": 7
    },
    {
      "caption": "Figure 5: 4.1.3 Results on digit datasets",
      "page": 9
    },
    {
      "caption": "Figure 3: t-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional",
      "page": 10
    },
    {
      "caption": "Figure 3: ). Ten labels are from 0 to 9 corresponding,",
      "page": 10
    },
    {
      "caption": "Figure 6: The rest of the other installation",
      "page": 10
    },
    {
      "caption": "Figure 5: Elements in that",
      "page": 12
    },
    {
      "caption": "Figure 4: The discriminator loss and adversarial loss in the LABEL →CALTECH",
      "page": 13
    },
    {
      "caption": "Figure 5: The overview of the SADDA method for the digit recognition task. We found",
      "page": 17
    },
    {
      "caption": "Figure 6: The overview of the SADDA method for the object recognition task on the",
      "page": 18
    },
    {
      "caption": "Figure 7: The overview of the SADDA method for the sentiment classiﬁcation task.",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the date of receipt and acceptance should be inserted later": "Abstract Domain adaptation is a potential method to train a powerful deep"
        },
        {
          "the date of receipt and acceptance should be inserted later": "neural network across various datasets. More precisely, domain adaptation meth-"
        },
        {
          "the date of receipt and acceptance should be inserted later": "ods\ntrain the model on training data and test\nthat model on a completely sep-"
        },
        {
          "the date of receipt and acceptance should be inserted later": "arate dataset. The adversarial-based adaptation method became popular among"
        },
        {
          "the date of receipt and acceptance should be inserted later": "other domain adaptation methods. Relying on the idea of GAN, the adversarial-"
        },
        {
          "the date of receipt and acceptance should be inserted later": "based domain adaptation tries to minimize the distribution between the training"
        },
        {
          "the date of receipt and acceptance should be inserted later": "and testing dataset based on the adversarial\nlearning process. We observe that"
        },
        {
          "the date of receipt and acceptance should be inserted later": "the\nsemi-supervised learning approach can combine with the adversarial-based"
        },
        {
          "the date of receipt and acceptance should be inserted later": "method to solve the domain adaptation problem.\nIn this paper, we propose an"
        },
        {
          "the date of receipt and acceptance should be inserted later": "improved adversarial domain adaptation method called Semi-Supervised Adver-"
        },
        {
          "the date of receipt and acceptance should be inserted later": "sarial Discriminative Domain Adaptation (SADDA), which can outperform other"
        },
        {
          "the date of receipt and acceptance should be inserted later": "prior domain adaptation methods. We also show that SADDA has a wide range of"
        },
        {
          "the date of receipt and acceptance should be inserted later": "applications and illustrate the promise of our method for image classiﬁcation and"
        },
        {
          "the date of receipt and acceptance should be inserted later": "sentiment classiﬁcation problems."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "As shown in Figure 1, because of numerous reasons (illumination,\nimage qual-"
        },
        {
          "2": "ity, background),\nthere\nis always a diﬀerent distribution between two datasets,"
        },
        {
          "2": "which is the main factor reducing the performance of deep neural networks. Even"
        },
        {
          "2": "though various research has proved that deep neural networks can learn transfer-"
        },
        {
          "2": "able feature representation over diﬀerent datasets [2, 3], Donahue et al.\n[4] showed"
        },
        {
          "2": "that domain shift\nstill\ninﬂuences\nthe accuracy of\nthe deep neural network when"
        },
        {
          "2": "testing these networks in a diﬀerent dataset."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "dataset to map to the unlabeled target dataset, thereby correctly classifying the"
        },
        {
          "3": "labels of the target dataset. The Semi-Supervised GAN [18]\nis designed to handle"
        },
        {
          "3": "the semi-supervised learning tasks and inspired us to develop our model."
        },
        {
          "3": "In this paper, we present a novel method called Semi-supervised Adversarial"
        },
        {
          "3": "Discriminative Domain Adaptation (SADDA), where the discriminator is a multi-"
        },
        {
          "3": "class classiﬁer.\nInstead of only distinguishing between source images and target"
        },
        {
          "3": "images\n(method like ADDA [16]),\nthe discriminator\nlearns\nto distinguish N + 1"
        },
        {
          "3": "classes, where N is\nthe number of classes\nin the classiﬁcation task, and the last"
        },
        {
          "3": "one uses\nto distinguish between the\nsource dataset or\nthe\ntarget dataset. The"
        },
        {
          "3": "discriminator focuses not only on the domain label between two datasets but also"
        },
        {
          "3": "on the labeled images from the source dataset, which improves the generalization"
        },
        {
          "3": "ability of the discriminator and the encoder as well as the classiﬁcation accuracy."
        },
        {
          "3": "To validate the eﬀectiveness of our methodology, we experiment with domain"
        },
        {
          "3": "adaptation tasks on digit datasets,\nincluding MNIST [5], USPS [6], MNIST-M [7],"
        },
        {
          "3": "and SVHN [8].\nIn addition, we also prove the robustness ability of\nthe SADDA"
        },
        {
          "3": "method by using t-SNE visualization of\nthe digit datasets,\nthe SADDA method"
        },
        {
          "3": "keeps the t-SNE clusters as tight as possible and maximizes the separation between"
        },
        {
          "3": "two clusters. We also test its potential with a more sophisticated dataset, by object"
        },
        {
          "3": "recognition task with CALTECH [9], LABELME [10], PASCAL [11], and SUN [12]"
        },
        {
          "3": "datasets. In addition, we evaluate our method for the natural\nlanguage processing"
        },
        {
          "3": "task, with three text datasets including Women’s E-Commerce Clothing Reviews"
        },
        {
          "3": "[19], Coronavirus\ntweets NLP - Text Classiﬁcation [20], and Trip Advisor Hotel"
        },
        {
          "3": "Reviews [21]. The Python code of the SADDA method for object recognition tasks"
        },
        {
          "3": "can be downloaded at https://github.com/NguyenThaiVu/SADDA."
        },
        {
          "3": "Our contributions can be summarized as follows:"
        },
        {
          "3": "– We propose a new Semi-supervised Adversarial Discriminative Domain Adap-"
        },
        {
          "3": "tation method (SADDA) for addressing the unsupervised domain adaptation"
        },
        {
          "3": "task."
        },
        {
          "3": "– We illustrate that SADDA improves digit classiﬁcation tasks and achieves com-"
        },
        {
          "3": "petitive performance with other adversarial adaptation methods."
        },
        {
          "3": "– We also demonstrate that the SADDA method can apply to multiple applica-"
        },
        {
          "3": "tions,\nincluding object recognition and natural\nlanguage processing tasks."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "fusion through an adversarial process. A popular adversarial-based domain adap-"
        },
        {
          "4": "tation method is the Adversarial Discriminative Domain Adaptation (ADDA) by"
        },
        {
          "4": "Tzeng et al.\n[16]. ADDA approach aims\nto diminish the distance between the"
        },
        {
          "4": "source encoder and target encoder distributions\nthrough the domain-adversarial"
        },
        {
          "4": "process. However,\nthis method only distinguishes between the source and target"
        },
        {
          "4": "domain.\nInstead, our SADDA method not only predicts whether\nthe source do-"
        },
        {
          "4": "main or the target domain, but also classiﬁes the label of the source dataset. More"
        },
        {
          "4": "concretely, we force the adversarial-based method to the semi-supervised context."
        },
        {
          "4": "We will show that this creation can produce a more eﬃcient classiﬁcation model."
        },
        {
          "4": "Combining adversarial-based domain adaptation with other auxil-"
        },
        {
          "4": "iary tasks. Recently, some works have focused on combining auxiliary tasks for"
        },
        {
          "4": "adversarial-based adaptation to exploit more information [33, 34]. Xavier and Ben-"
        },
        {
          "4": "gio introduce Stacked Denoising Autoencoders [35, 36], reconstructing the merging"
        },
        {
          "4": "data from numerous domains with the same network,\nsuch that\nthe representa-"
        },
        {
          "4": "tions can be symbolized by both the source and target domain. Deep reconstruction"
        },
        {
          "4": "classiﬁcation network (DRCN) [28] attempts to solve two sub-problem at the same"
        },
        {
          "4": "time: classiﬁcation of the source data, and reconstruction of the unlabeled target"
        },
        {
          "4": "data. However, these auxiliary tasks are not towards the same goal. We observe"
        },
        {
          "4": "that during the adversarial process, we can classify the source or\ntarget dataset"
        },
        {
          "4": "and predict the label of the source dataset simultaneously. That allows us to re-use"
        },
        {
          "4": "the same output layers in the discriminator model as well as forces two discrimina-"
        },
        {
          "4": "tor models towards the same goal (sub-section 3.2 for more details). In addition,"
        },
        {
          "4": "we also demonstrate that our SADDA method not only applies to computer vision"
        },
        {
          "4": "tasks but also the natural\nlanguage processing task."
        },
        {
          "4": "3 Proposed method"
        },
        {
          "4": "3.1 Semi-supervised Adversarial Discriminative Domain Adaptation"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "In this section, we describe in detail our Semi-supervised Adversarial Discrim-"
        },
        {
          "5": "inative Domain Adaptation (SADDA) method. An overview of our method can be"
        },
        {
          "5": "found in Figure 2."
        },
        {
          "5": "In the unsupervised domain adaptation task, we already have source images"
        },
        {
          "5": "Xs and source labels Ys come from the source domain distribution ps(x, y). Be-"
        },
        {
          "5": "sides\ncomes\nthat, a target dataset Xt\nfrom a target distribution pt(x, y), where"
        },
        {
          "5": "the label of\nthe target dataset\nis non-exist. We desire to learn a target encoder"
        },
        {
          "5": "the\ntarget\nimage’s\nlabel.\nIn\nMt\nand classiﬁer Ct, which can accurately predict"
        },
        {
          "5": "an adversarial-based adaptation approach, we aim to diminish the distance be-"
        },
        {
          "5": "tween the source mapping distribution (Ms(Xs)) and target mapping distributions"
        },
        {
          "5": "to classify\n(Mt(Xt)). As a result, we can straightly apply the source classiﬁer Cs"
        },
        {
          "5": "the target images,\nin other words, C = Ct = Cs. The summary process of SADDA"
        },
        {
          "5": "includes three steps: pre-training, training target encoder, and testing."
        },
        {
          "5": "Pre-training.\nIn the pre-training phase,\ntraining source\nencoder\n(Ms) and"
        },
        {
          "5": "source classiﬁer (Cs), by using the source labeled images (Xs,Ys). This step is a"
        },
        {
          "5": "standard supervised classiﬁcation task, a common form can be denoted as:"
        },
        {
          "5": "N(cid:88) n\narg min\n(1)\nyn log Cs(Ms(xn ))\nLcls(Xs, Ys) = −E(x,y)∼(Xs,Ys)"
        },
        {
          "5": "Ms,Cs"
        },
        {
          "5": "=1"
        },
        {
          "5": "is a supervised classiﬁcation loss\n(categorical\ncrossentropy loss),\nwhere Lcls"
        },
        {
          "5": "and N is the number of classes."
        },
        {
          "5": "Training\ntarget\nencoder.\nIn the\ntraining\ntarget\nencoder phase, we ﬁrst"
        },
        {
          "5": "present a training discriminator process and then present a procedure for training"
        },
        {
          "5": "the target encoder."
        },
        {
          "5": "Firstly, training the discriminator (D) in two modes, each giving a correspond-"
        },
        {
          "5": "ing output. (1) Supervised mode, where the supervised discriminator (Dsup) pre-"
        },
        {
          "5": "dicts N labels from the original classiﬁcation task. (2) Unsupervised mode, where"
        },
        {
          "5": "the unsupervised discriminator (Dunsup) classiﬁes between Xs and Xt. Discrimi-"
        },
        {
          "5": "nator correlates with unconstrained optimization:"
        },
        {
          "5": "N(cid:88) n\n(2)\narg min\nyn log Dsup(Ms(xn ))\nLcls(Xs, Ys) = −E(x,y)∼(Xs,Ys)"
        },
        {
          "5": "Dsup"
        },
        {
          "5": "=1"
        },
        {
          "5": "arg min\nLadvD (Xs, Xt, Ms, Mt) = −Exs∼Xs log Dunsup(Ms(xs))"
        },
        {
          "5": "Dunsup"
        },
        {
          "5": "(3)"
        },
        {
          "5": "−Ext∼Xt log(1 − Dunsup(Mt(xt)))"
        },
        {
          "5": "In equation 2, Lcls is a supervised classiﬁcation loss corresponding to predicting"
        },
        {
          "5": "N labels\nfrom the original classiﬁcation task in the source dataset\n(Xs), which"
        },
        {
          "5": "loss\nwill update the parameter in Dsup.\nIn equation (3), LadvD is an adversarial"
        },
        {
          "5": "to maximize the\nfor unsupervised discriminator Dunsup, which trains\n(Dunsup)"
        },
        {
          "5": "probability of predicting the correct label from the source dataset or target dataset."
        },
        {
          "5": "One thing to notice is that the unsupervised discriminator uses a custom activation"
        },
        {
          "5": "function (equation 5), which returns a probability to determine whether a source"
        },
        {
          "5": "image or target image (sub-section 3.2 for more details)."
        },
        {
          "5": "Secondly, training the target encoder Mt with the standard loss function and"
        },
        {
          "5": "inverted labels\n[15]. This\nimplies\nthat\nis\nthe unsupervised discriminator Dunsup"
        },
        {
          "5": "fooled by the target encoder Mt,\nin other words, Dunsup is unable to determine"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: , and the outputs are",
      "data": [
        {
          "Output probabilities (prior softmax)": "[9.0, 1.0, 1.0]",
          "Custom activation": "0.9999",
          "Entropy": "Low"
        },
        {
          "Output probabilities (prior softmax)": "[5.0, 1.0, 1.0]",
          "Custom activation": "0.9935",
          "Entropy": "Low"
        },
        {
          "Output probabilities (prior softmax)": "[-5.0, -5.0, -5.0]",
          "Custom activation": "0.0198",
          "Entropy": "High"
        },
        {
          "Output probabilities (prior softmax)": "",
          "Custom activation": "",
          "Entropy": ""
        },
        {
          "Output probabilities (prior softmax)": "",
          "Custom activation": "",
          "Entropy": ""
        },
        {
          "Output probabilities (prior softmax)": "",
          "Custom activation": "",
          "Entropy": ""
        },
        {
          "Output probabilities (prior softmax)": "the Mt",
          "Custom activation": "to learn how to produce a more authentic encoder. The loss",
          "Entropy": ""
        },
        {
          "Output probabilities (prior softmax)": "",
          "Custom activation": "",
          "Entropy": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "discriminator is encouraged to output a conﬁdence class prediction for the source"
        },
        {
          "7": "sample, while\nit predicts a small probability for\nthe\ntarget\nsample. That\nis an"
        },
        {
          "7": "elegant method allowing re-use of the same feature extraction layers for both the"
        },
        {
          "7": "supervised discriminator and the unsupervised discriminator."
        },
        {
          "7": "It is reasonable that learning the well supervised discriminator will improve the"
        },
        {
          "7": "unsupervised discriminator. Moreover, training the discriminator in unsupervised"
        },
        {
          "7": "mode allows\nthe model\nto learn useful\nfeature extraction capabilities\nfrom huge"
        },
        {
          "7": "unlabeled datasets. As a sequence, improving the supervised discriminator will im-"
        },
        {
          "7": "prove the unsupervised discriminator and vice versa. Improving the discriminator"
        },
        {
          "7": "will enhance the target encoder [18]. In total, this is one kind of advantage circle, in"
        },
        {
          "7": "which three elements (unsupervised discriminator, supervised discriminator, and"
        },
        {
          "7": "target encoder) iteratively make each other better."
        },
        {
          "7": "3.3 Guideline for stable SADDA"
        },
        {
          "7": "In general, training SADDA is an extremely hard process, there are two losses we"
        },
        {
          "7": "need to optimize: the loss for the discriminator and the loss for the target encoder."
        },
        {
          "7": "For that reason, the loss landscape of SADDA is ﬂuctuating and dynamic (detail"
        },
        {
          "7": "in sub-section 4.4). When implementing and training the SADDA, we ﬁnd that is a"
        },
        {
          "7": "tough process. To overcome the limitation of the adversarial process, we present a"
        },
        {
          "7": "full architecture of SADDA. This designed architecture increases training stability"
        },
        {
          "7": "and prevents non-convergence. In this section, we present the key ideas in designing"
        },
        {
          "7": "the model\nfor the image classiﬁcation and the sentiment classiﬁcation task."
        },
        {
          "7": "Image classiﬁcation. The design of SADDA is\ninspired by Deep Convolu-"
        },
        {
          "7": "tional GAN (DCGAN) architecture [38]. The summary architecture of the SADDA"
        },
        {
          "7": "method for digit recognition is shown in Figure 5. On the one hand, the encoder"
        },
        {
          "7": "is used to capture the content in the image,\nincreasing the number of ﬁlters while"
        },
        {
          "7": "decreasing the spatial dimension by the convolutional\nlayer. On the other hand,"
        },
        {
          "7": "the discriminator is symmetric expansion with the encoder by using fractionally-"
        },
        {
          "7": "strided convolutions (transpose convolution)."
        },
        {
          "7": "Moreover, our recommendation for eﬃcient training SADDA in the image clas-"
        },
        {
          "7": "siﬁcation task:"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "encoder block and the Decoder LSTM being the discriminator block respectively."
        },
        {
          "8": "The Encoder-Decoder LSTM was built for the NLP task where it illustrated state-"
        },
        {
          "8": "of-the-art performance, such as machine translation [43]. From the empirical, we"
        },
        {
          "8": "ﬁnd that the Encoder-Decoder is suitable for the unsupervised domain adaptation"
        },
        {
          "8": "task."
        },
        {
          "8": "4 Experiments"
        },
        {
          "8": "In this section, we evaluate our SADDA method for unsupervised domain adapta-"
        },
        {
          "8": "tion tasks in three scenarios: digit recognition, object recognition, and sentiment"
        },
        {
          "8": "classiﬁcation."
        },
        {
          "8": "In the experiments, we focus on probing how the SADDA method improves the"
        },
        {
          "8": "unsupervised domain adaptation task. For\nthis purpose, we only choose shallow"
        },
        {
          "8": "architecture rather than a deep network. We leave the sophisticated design for a"
        },
        {
          "8": "future job."
        },
        {
          "8": "4.1 Digit recognition"
        },
        {
          "8": "4.1.1 Datasets and Domain Adaptation Scenarios"
        },
        {
          "8": "We\nevaluate SADDA on various unsupervised domain adaptation experiments,"
        },
        {
          "8": "examining the following popular used digits datasets and settings (the visualization"
        },
        {
          "8": "is in Figure 1):"
        },
        {
          "8": "MNIST ←→ USPS: MNIST [5]\nincludes 28x28 pixels, which are grayscale"
        },
        {
          "8": "images of digit numbers. USPS [6] is a digit dataset, which contains 9298 grayscale"
        },
        {
          "8": "images. The image is 16x16 pixels.\nIn this experiment, we follow the evaluation"
        },
        {
          "8": "protocol of\n[44]."
        },
        {
          "8": "MNIST → MNIST-M: MNIST-M [7]\nis made by merging MNIST digits"
        },
        {
          "8": "with the patches arbitrarily extracted from color images of BSDS500 [45]. In this"
        },
        {
          "8": "experiment, we set the input size is 28x28x3 pixels, and we follow the evaluation"
        },
        {
          "8": "protocol of\n[44]"
        },
        {
          "8": "SVHN → MNIST: The Street View House Number\n(SVHN)\n[8]\nis a digit"
        },
        {
          "8": "dataset, which contains 600000 32×32 RGB images. In this experiment, we convert"
        },
        {
          "8": "the SVHN dataset to grayscale images and resize the MNIST images into 32x32"
        },
        {
          "8": "grayscale images. We use the evaluation protocol of\n[28]."
        },
        {
          "8": "4.1.2 Implementation details"
        },
        {
          "8": "The SADDA model\nis trained with diﬀerent learning rates in diﬀerent phases. In"
        },
        {
          "8": "the pre-training phases, this is a standard classiﬁcation task, we use a learning rate"
        },
        {
          "8": "is 0.001 in our experiment. In the training target encoder phases, we suggested a"
        },
        {
          "8": "learning rate of 0.0002 as well as an Adam optimizer [46] and setting the β1 equal"
        },
        {
          "8": "to 0.5 to help stabilize training. In the LeakyReLU activation, we set α = 0.2 in"
        },
        {
          "8": "the whole model."
        },
        {
          "8": "In this experiment, the encoder consists of\nfour convolutional\nlayers with 4 x"
        },
        {
          "8": "4 kernel size, 2 x 2 strides, same padding, and ReLU activation. The number of"
        },
        {
          "8": "ﬁlters for four convolution layers are 32, 64, 128, and 256, respectively. The target"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Experimental results on unsupervised domain adaptation on digit",
      "data": [
        {
          "9": "encoder has the same architecture as the source encoder, and the source encoder"
        },
        {
          "9": "is used as an initialization for the target encoder."
        },
        {
          "9": "The classiﬁer takes the outputs of the encoder as input. Next, a fully connected"
        },
        {
          "9": "layer with 100 feature channels,\nfollowed by ReLU activation. Finally,\nthe fully"
        },
        {
          "9": "connected layer with ten feature channels and the softmax activation."
        },
        {
          "9": "In the training target encoder phases, the outputs of the encoder serve as the"
        },
        {
          "9": "input of the discriminator. The discriminator consists of\nfour Transpose Convolu-"
        },
        {
          "9": "tional layers with 4 x 4 kernels size, 2 x 2 strides, the same padding, and the Leaky"
        },
        {
          "9": "ReLU activation (alpha = 0.2). The number of kernels for four Transpose Convo-"
        },
        {
          "9": "lutional\nis 256, 128, 64, and 32, respectively. We illustrate the overall architecture"
        },
        {
          "9": "in Figure 5."
        },
        {
          "9": "4.1.3 Results on digit datasets"
        },
        {
          "9": "In this experiment, we compare our SADDA method against multiple state-of-the-"
        },
        {
          "9": "art unsupervised domain adaptation methods."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: Experimental results on unsupervised domain adaptation on digit",
      "data": [
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "art unsupervised domain adaptation methods."
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "Method"
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "Source only"
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "DANN [7]"
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "DRCN [28]"
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "ADDA [16]"
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "SBADA-GAN [47]"
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "SHOT [48]"
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "DFA-MCD [49]"
        },
        {
          "In this experiment, we compare our SADDA method against multiple state-of-the-": "SADDA (our)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: ). In addition,",
      "data": [
        {
          "Fig. 3:\nt-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional": "representation, with Source only (on the left) and SADDA (on the right) on the"
        },
        {
          "Fig. 3:\nt-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional": "target dataset. Note that SADDA minimizes intra-class distance and maximizes"
        },
        {
          "Fig. 3:\nt-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional": "inter-class distance."
        },
        {
          "Fig. 3:\nt-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional": "SADDA (as described in Figure 3). Ten labels are\nfrom 0 to 9 corresponding,"
        },
        {
          "Fig. 3:\nt-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional": "and 100 samples per label. The domain invariance is determined by the degree of"
        },
        {
          "Fig. 3:\nt-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional": "overlap between features. Regarding the Source only model, the distribution and"
        },
        {
          "Fig. 3:\nt-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional": "the density are messy. In contrast, the SADDA method splits diﬀerent labels into"
        },
        {
          "Fig. 3:\nt-SNE embedding of digit classiﬁcation, using (2 x 2 x 256) dimensional": "diﬀerent regions, and the overlap is more prominent."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: The accuracy (%) on the VLCS dataset.",
      "data": [
        {
          "Source only": "SADDA",
          "33.26": "37.73",
          "45.10": "55.30",
          "33.78": "36.21"
        },
        {
          "Source only": "Train on target",
          "33.26": "86.52",
          "45.10": "99.27",
          "33.78": "88.26"
        },
        {
          "Source only": "",
          "33.26": "(a) Source domain: PASCAL",
          "45.10": "",
          "33.78": ""
        },
        {
          "Source only": "",
          "33.26": "PASCAL",
          "45.10": "CALTECH",
          "33.78": "SUN"
        },
        {
          "Source only": "Source only",
          "33.26": "28.73",
          "45.10": "32.75",
          "33.78": "27.71"
        },
        {
          "Source only": "SADDA",
          "33.26": "32.71",
          "45.10": "39.36",
          "33.78": "31.01"
        },
        {
          "Source only": "Train on target",
          "33.26": "75.28",
          "45.10": "99.27",
          "33.78": "88.26"
        },
        {
          "Source only": "",
          "33.26": "(b) Source domain: LABELME",
          "45.10": "",
          "33.78": ""
        },
        {
          "Source only": "",
          "33.26": "PASCAL",
          "45.10": "LABELME",
          "33.78": "SUN"
        },
        {
          "Source only": "Source only",
          "33.26": "26.71",
          "45.10": "28.27",
          "33.78": "31.62"
        },
        {
          "Source only": "SADDA",
          "33.26": "31.82",
          "45.10": "31.39",
          "33.78": "37.67"
        },
        {
          "Source only": "Train on target",
          "33.26": "75.28",
          "45.10": "86.52",
          "33.78": "88.26"
        },
        {
          "Source only": "",
          "33.26": "(c) Source domain: CALTECH",
          "45.10": "",
          "33.78": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: The accuracy (%) on the VLCS dataset.",
      "data": [
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "4.2.2 Results"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "The\nresults on the VLCS are\nshown in Table 3. Source only is a model\nthat"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "only trains on the source dataset without using any domain adaptation methods."
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "Overall,\nthe accuracy when applying the SADDA method overcomes the Source"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "only model\nin all cases.\nIn some speciﬁc cases\nlike PASCAL → CALTECH,\nthe"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "classiﬁcation accuracy goes from 45.10 to 55.30 (improving approximately 10%)."
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "In case LABELME → CALTECH, the accuracy grows from 32.75% to 39.36%."
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "Examining the results\nin Table 3,\nthe Source only model has\nlow accuracy,"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "which reveals\nthat\nthe domain shift\nis quite\nlarge.\nIn other words,\nthe Source"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "only model does not\nlearn any knowledge about\nthe\nsource dataset\nto predict"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "the target dataset.\nIn contrast,\nthe SADDA model\nlearns a more useful\nfeature"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "representation,\nleading to higher accuracy when performing a prediction on the"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "target dataset."
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "Although the SADDA method has\ncertain improvements\ncompared to\nthe"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "Source only method,\nthe accuracy of\nthe SADDA method is\nstill\nlow and not"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "ideal. For further comparison, we also test the hypothesis situation where the tar-"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "get labels are present (the train on target model). There is still a big gap between"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "the accuracy of the SADDA method and the train on target method."
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "4.3 Sentiment classiﬁcation"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "4.3.1 Datasets and Preprocessing"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "In this subsection, we evaluate the SADDA method for the sentiment classiﬁcation"
        },
        {
          "Table 3: The accuracy (%) on the VLCS dataset.": "task. We use three sentimental datasets, including Women’s E-Commerce Clothing"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: Compared with the",
      "data": [
        {
          "12": "Reviews [19], Coronavirus tweets NLP - Text Classiﬁcation [20], and Trip Advisor"
        },
        {
          "12": "Hotel Reviews [21]:"
        },
        {
          "12": "Women’s E-Commerce Clothing Reviews [19]. This\nis\nreal commercial"
        },
        {
          "12": "data, where the reviews are written by customers.\nIn this task, we only use two"
        },
        {
          "12": "features called Review Text\n(the raw text review) and Rating (the positive integer"
        },
        {
          "12": "for\nthe product, provided by the customer\nfrom 1 Worst\nto 5 Best). Regarding"
        },
        {
          "12": "the Rating, we relabel\ninto the sets {positive, neutral, negative} with the following"
        },
        {
          "12": "rule:\nif a review is greater than 3,\nit is considered a positive comment;\nif a review"
        },
        {
          "12": "is equal to 3,\nit is considered a neutral comment;\nif a review is less than 3,\nit is"
        },
        {
          "12": "considered a negative comment."
        },
        {
          "12": "Coronavirus\ntweets NLP - Text Classiﬁcation [20]. The\ntweets were"
        },
        {
          "12": "downloaded from Twitter and tagged manually. Although there are four columns"
        },
        {
          "12": "in total, we only use the Original Tweet\nfeature and Label\nin our experiment. In"
        },
        {
          "12": "the case of Label, the original\nlabel\nincludes Extremely Negative, Negative, Neu-"
        },
        {
          "12": "tral, Positive, and Extremely Positive. However, we convert to the sets {positive,"
        },
        {
          "12": "neutral, negative} respectively."
        },
        {
          "12": "Trip Advisor Hotel Reviews [21]. This contains reviews crawled from the"
        },
        {
          "12": "travel company called Tripadvisor. The dataset contains\ntwo features,\nincluding"
        },
        {
          "12": "Review Text and Rating (the positive integer from 1 Worst to 5 Best). Regarding"
        },
        {
          "12": "Rating, we process the same as the case Women’s E-Commerce Clothing Reviews"
        },
        {
          "12": "above."
        },
        {
          "12": "In all three datasets, we perform the following text preprocessing steps: remov-"
        },
        {
          "12": "ing the punctuation, URL, hashtags, mentions, and stop words (with the support"
        },
        {
          "12": "of the NLTK [52]\nlibrary). We limit the input sentence to a max length equal to"
        },
        {
          "12": "50. The GloVe [53] word embedding is applied to map the word in the text review"
        },
        {
          "12": "to the vector space."
        },
        {
          "12": "Because\nthe number of\nsamples per\nclass\nis not balanced, we use\nthe data"
        },
        {
          "12": "augmentation techniques to balance the number of samples - with the support of"
        },
        {
          "12": "the TextAugment\n[54]\nlibrary. Particularly, we perform data augmentation such"
        },
        {
          "12": "that each class has up to 20 000 samples. The dataset is divided into a training"
        },
        {
          "12": "set (60%), validation set (20%), and test set (20%)."
        },
        {
          "12": "4.3.2 Experiments and Results"
        },
        {
          "12": "For this experiment, our architecture is illustrated in Figure 5. Elements in that"
        },
        {
          "12": "architecture such as the LSTM layer and the Repeat Vector layer are implemented"
        },
        {
          "12": "by the TensorFlow library with default settings. The optimization algorithms and"
        },
        {
          "12": "learning rates are set up as\nin section 4.1.2.\nIn addition, we do not attempt\nto"
        },
        {
          "12": "ﬁne-tune the architecture and leave it for future work."
        },
        {
          "12": "The results of our experiment are provided in Table 4. Compared with the"
        },
        {
          "12": "Source only model, the SADDA method shows a little improvement in the accu-"
        },
        {
          "12": "racy of\nthis sentiment analysis task. For a certain experiment,\nlike T → W,\nthe"
        },
        {
          "12": "classiﬁcation accuracy goes from 41.07% to 49.28%. However, not all experiments"
        },
        {
          "12": "improve, such as experiment T → C, the accuracy even dropped a bit (from 38.46%"
        },
        {
          "12": "down to 38.05%). Additionally, a comparison with the ”Train on target” model"
        },
        {
          "12": "exposes that the SADDA model\nis far from the ideal model. We hope that is the"
        },
        {
          "12": "motivation for future development."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 4: The accuracy (%) of unsupervised domain adaptation on the sentiment",
      "data": [
        {
          "Source only": "SADDA",
          "37.97": "43.88",
          "50.11": "55.54",
          "45.91": "48.02",
          "49.93": "56.10",
          "41.07": "49.28",
          "38.46": "38.05"
        },
        {
          "Source only": "Train on target",
          "37.97": "79.01",
          "50.11": "96.10",
          "45.91": "93.02",
          "49.93": "96.10",
          "41.07": "93.02",
          "38.46": "79.01"
        },
        {
          "Source only": "Table 4: The accuracy (%) of unsupervised domain adaptation on the sentiment",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "classiﬁcation task.",
          "37.97": "In the table,",
          "50.11": "",
          "45.91": "there are three datasets: Women’s E-Commerce",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "Clothing Reviews (W) [19], Coronavirus tweets (C) [20], Trip Advisor Hotel Re-",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "views (T) [21]",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "4.4 Challenge and convergence analysis",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "In this subsection, we will discuss the challenge of training a stable SADDA model",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "and how to\ntrigger",
          "37.97": "the",
          "50.11": "early stopping",
          "45.91": "of\nthe",
          "49.93": "",
          "41.07": "training progress",
          "38.46": "to\nachieve"
        },
        {
          "Source only": "convergent state.",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "",
          "37.97": "In the SADDA model, we have three stages: Pre-training, Training target en-",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "coder, and Testing. The diﬃculty comes from the Training target encoder process.",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "The reason is that both the discriminator and target encoder are trained simulta-",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "neously in that procedure, which can lead to updating the parameter of one model",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "will reduce the performance of the other model. More concretely, there are two loss",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "functions we need to optimize: the discriminator loss and the adversarial",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "discriminator\nloss",
          "37.97": "(equation 3)",
          "50.11": "is",
          "45.91": "the loss when the unsupervised discriminator",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        },
        {
          "Source only": "predicts the source or target samples. The adversarial",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "loss (equation 4) is the loss",
          "38.46": ""
        },
        {
          "Source only": "of the target encoder when training with the inverted labels.",
          "37.97": "",
          "50.11": "",
          "45.91": "",
          "49.93": "",
          "41.07": "",
          "38.46": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "we trigger an early stopping. Early stopping is the technique to stop the training"
        },
        {
          "14": "process at a certain point before the model overﬁts the training dataset and has"
        },
        {
          "14": "poor performance on the test\nset.\nIn that case, we consider\nthat our model has"
        },
        {
          "14": "converged. Keep in mind that\nthe\nloss of 0.0 in the discriminator\nloss or\nthe"
        },
        {
          "14": "adversarial\nloss during the training process is a failure mode."
        },
        {
          "14": "Regarding ﬁgure 4, the convergence point is in the epoch 20. At that point, we"
        },
        {
          "14": "will stop the training process because the discriminator\nloss and the adversarial"
        },
        {
          "14": "loss are saturated at around 0.33 and 2.30 respectively.\nIn that experiment (and"
        },
        {
          "14": "other object recognition tasks),\nit took around 1 hour on a single Tesla T4 GPU"
        },
        {
          "14": "to complete the training procedure."
        },
        {
          "14": "5 Conclusion"
        },
        {
          "14": "We proposed a more stable and high-accuracy architecture for training adversarial-"
        },
        {
          "14": "based domain adaptation methods. The key idea of this approach is to train dis-"
        },
        {
          "14": "criminators\nin two modes:\nsupervised mode and unsupervised mode. Moreover,"
        },
        {
          "14": "utilize this to create a more eﬃcient target encoder, which will help improve the"
        },
        {
          "14": "classiﬁcation accuracy."
        },
        {
          "14": "While the SADDA method has demonstrated an improvement in many tasks"
        },
        {
          "14": "like image classiﬁcation or sentiment classiﬁcation, there are still open challenges."
        },
        {
          "14": "Particularly, the SADDA model\nin object recognition and sentiment classiﬁcation"
        },
        {
          "14": "is far from the desired accuracy model. We hope that the intuition of this research"
        },
        {
          "14": "will\nfacilitate further advances in domain adaptation tasks."
        },
        {
          "14": "References"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "1. A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Sch¨olkopf, Covari-"
        },
        {
          "References": "ate shift and local\nlearning by distribution matching, pp. 131–160. Cambridge, MA, USA:"
        },
        {
          "References": "MIT Press, 2009."
        },
        {
          "References": "2. M. Long, Y. Cao, J. Wang, and M.\nI. Jordan, “Learning transferable features with deep"
        },
        {
          "References": "adaptation networks,” ArXiv, vol. abs/1502.02791, 2015."
        },
        {
          "References": "3. A. Nguyen, N. Nguyen, K. Tran, E. Tjiputra, and Q. D. Tran, “Autonomous naviga-"
        },
        {
          "References": "tion in complex environments with deep multimodal\nfusion network,” in 2020 IEEE/RSJ"
        },
        {
          "References": "International Conference on Intelligent Robots and Systems (IROS), 2020."
        },
        {
          "References": "4.\nJ. Donahue, Y. Jia, O. Vinyals, J. Hoﬀman, N. Zhang, E. Tzeng, and T. Darrell, “Decaf:"
        },
        {
          "References": "A deep convolutional activation feature for generic visual recognition,” 2013."
        },
        {
          "References": "5. Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner, “Gradient-based learning applied to"
        },
        {
          "References": "document recognition,” Proceedings of\nthe IEEE, vol. 86, no. 11, pp. 2278–2324, 1998."
        },
        {
          "References": "6.\nJ. J. Hull, “A database for handwritten text recognition research,” IEEE Transactions on"
        },
        {
          "References": "Pattern Analysis and Machine Intelligence, vol. 16, no. 5, pp. 550–554, 1994."
        },
        {
          "References": "7. Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand,"
        },
        {
          "References": "and V. Lempitsky, “Domain-adversarial training of neural networks,” 2015."
        },
        {
          "References": "8. Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading digits\nin"
        },
        {
          "References": "natural\nimages with unsupervised feature learning,” in NIPS Workshop on Deep Learning"
        },
        {
          "References": "and Unsupervised Feature Learning 2011, 2011."
        },
        {
          "References": "9. L. Fei-Fei, R. Fergus, and P. Perona, “One-shot\nlearning of object\ncategories,” IEEE"
        },
        {
          "References": "Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 594–611,"
        },
        {
          "References": "2006."
        },
        {
          "References": "10. B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman, “Labelme: A database and"
        },
        {
          "References": "web-based tool for image annotation,” International Journal of Computer Vision, vol. 77,"
        },
        {
          "References": "pp. 157–173, 2007."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "15": "11. M. Everingham, L. Van Gool, C. K.\nI. Williams,\nJ. Winn,\nand A. Zisserman,\n“The"
        },
        {
          "15": "PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.” http://www.pascal-"
        },
        {
          "15": "network.org/challenges/VOC/voc2007/workshop/index.html."
        },
        {
          "15": "12. M. J. Choi, J. J. Lim, A. Torralba, and A. S. Willsky, “Exploiting hierarchical context"
        },
        {
          "15": "on a large database of object categories,” in 2010 IEEE Computer Society Conference on"
        },
        {
          "15": "Computer Vision and Pattern Recognition, pp. 129–136, 2010."
        },
        {
          "15": "13. M. Gheisari and M. S. Baghshah, “Unsupervised domain adaptation via representation"
        },
        {
          "15": "learning and adaptive classiﬁer learning,” Neurocomput., vol. 165, p. 300–311, Oct. 2015."
        },
        {
          "15": "14.\nS. J. Pan,\nI. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation via transfer"
        },
        {
          "15": "component analysis,” IEEE Transactions on Neural Networks, vol. 22, no. 2, pp. 199–210,"
        },
        {
          "15": "2011."
        },
        {
          "15": "15.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,"
        },
        {
          "15": "and Y. Bengio, “Generative adversarial nets,” in Advances in Neural Information Process-"
        },
        {
          "15": "ing Systems (Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger,"
        },
        {
          "15": "eds.), vol. 27, Curran Associates, Inc., 2014."
        },
        {
          "15": "16. E. Tzeng, J. Hoﬀman, K. Saenko, and T. Darrell, “Adversarial discriminative domain"
        },
        {
          "15": "adaptation,” in 2017 IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
          "15": "(CVPR), pp. 2962–2971, 2017."
        },
        {
          "15": "17. Y. Reddy, V. Pulabaigari, and E. B, “Semi-supervised learning: a brief review,” Interna-"
        },
        {
          "15": "tional Journal of Engineering & Technology, vol. 7, p. 81, 02 2018."
        },
        {
          "15": "18. A. Odena, “Semi-supervised learning with generative adversarial networks,” 2016."
        },
        {
          "15": "19. Nicapotato, “Women’s e-commerce clothing reviews,” Feb 2018."
        },
        {
          "15": "20. A. Miglani, “Coronavirus tweets nlp - text classiﬁcation,” Sep 2020."
        },
        {
          "15": "21. M. H. Alam, W.-J. Ryu, and S. Lee, “Joint multi-grain topic sentiment: modeling semantic"
        },
        {
          "15": "aspects for online reviews,” Information Sciences, vol. 339, pp. 206–223, 2016."
        },
        {
          "15": "22. B. Krawczyk, “Learning from imbalanced data: open challenges and future directions,”"
        },
        {
          "15": "Progress in Artiﬁcial Intelligence, vol. 5, pp. 221–232, 2016."
        },
        {
          "15": "23. A. Torralba and A. A. Efros, “Unbiased look at dataset bias,” CVPR 2011, pp. 1521–1528,"
        },
        {
          "15": "2011."
        },
        {
          "15": "24. W. Chi, G. Dagnino, T. M. Kwok, A. Nguyen, D. Kundrat, M. E. Abdelaziz, C. Riga,"
        },
        {
          "15": "C. Bicknell, and G.-Z. Yang, “Collaborative robot-assisted endovascular catheterization"
        },
        {
          "15": "with generative adversarial\nimitation learning,” in 2020 IEEE International Conference"
        },
        {
          "15": "on Robotics and Automation (ICRA), pp. 2414–2420, IEEE, 2020."
        },
        {
          "15": "25. W. M. Kouw and M. Loog, “A review of domain adaptation without target labels,” IEEE"
        },
        {
          "15": "Transactions on Pattern Analysis and Machine\nIntelligence, vol. 43, p. 766–785, Mar"
        },
        {
          "15": "2021."
        },
        {
          "15": "26. A. Margolis, “A literature review of domain adaptation with unlabeled data,” Rapport"
        },
        {
          "15": "Technique, University of Washington, 01 2011."
        },
        {
          "15": "27. M. Wang and W. Deng, “Deep visual domain adaptation: A survey,” Neurocomputing,"
        },
        {
          "15": "vol. 312, p. 135–153, Oct 2018."
        },
        {
          "15": "28. M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li, “Deep reconstruction-"
        },
        {
          "15": "classiﬁcation networks for unsupervised domain adaptation,” 2016."
        },
        {
          "15": "29.\nJ. Deng, Z. Zhang, F. Eyben, and B. Schuller, “Autoencoder-based unsupervised domain"
        },
        {
          "15": "adaptation for speech emotion recognition,” IEEE Signal Processing Letters, vol. 21, no. 9,"
        },
        {
          "15": "pp. 1068–1072, 2014."
        },
        {
          "15": "30. M. Long, H. Zhu, J. Wang, and M. I. Jordan, “Deep transfer learning with joint adaptation"
        },
        {
          "15": "networks,” 2016."
        },
        {
          "15": "31. M. Long, Z. Cao, J. Wang, and M. I. Jordan, “Conditional adversarial domain adaptation,”"
        },
        {
          "15": "2017."
        },
        {
          "15": "32. K. Saito, K. Watanabe, Y. Ushiku, and T. Harada, “Maximum classiﬁer discrepancy for"
        },
        {
          "15": "unsupervised domain adaptation,” in Proceedings of\nthe IEEE Conference on Computer"
        },
        {
          "15": "Vision and Pattern Recognition (CVPR), June 2018."
        },
        {
          "15": "33. M. Ghifary, W. B. Kleijn, M. Zhang, and D. Balduzzi, “Domain generalization for object"
        },
        {
          "15": "recognition with multi-task autoencoders,” 2015."
        },
        {
          "15": "34. K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan, “Domain separa-"
        },
        {
          "15": "tion networks,” 2016."
        },
        {
          "15": "35. X. Glorot, A. Bordes, and Y. Bengio, “Domain adaptation for large-scale sentiment clas-"
        },
        {
          "15": "siﬁcation: A deep learning approach,” in ICML, pp. 513–520, 2011."
        },
        {
          "15": "36. P. Vincent, H. Larochelle,\nI. Lajoie, Y. Bengio, and P.-A. Manzagol, “Stacked denoising"
        },
        {
          "15": "autoencoders: Learning useful\nrepresentations\nin a deep network with a local denoising"
        },
        {
          "15": "criterion,” J. Mach. Learn. Res., vol. 11, pp. 3371–3408, 2010."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "16": "37. T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved"
        },
        {
          "16": "techniques for training gans,” 2016."
        },
        {
          "16": "38. A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep"
        },
        {
          "16": "convolutional generative adversarial networks,” 2015."
        },
        {
          "16": "39. A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectiﬁer nonlinearities\nimprove neural net-"
        },
        {
          "16": "work acoustic models,” in in ICML Workshop on Deep Learning for Audio, Speech and"
        },
        {
          "16": "Language Processing, 2013."
        },
        {
          "16": "40. N. Srivastava, E. Mansimov, and R. Salakhutdinov, “Unsupervised learning of video rep-"
        },
        {
          "16": "resentations using lstms,” 2015."
        },
        {
          "16": "41.\nJ. Brownlee, “A gentle introduction to lstm autoencoders,” Aug 2020."
        },
        {
          "16": "42.\nS. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9,"
        },
        {
          "16": "pp. 1735–80, 12 1997."
        },
        {
          "16": "43. K. Cho, B. van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and"
        },
        {
          "16": "Y. Bengio, “Learning phrase representations using RNN encoder–decoder\nfor\nstatistical"
        },
        {
          "16": "machine\ntranslation,” in Proceedings of\nthe 2014 Conference on Empirical Methods\nin"
        },
        {
          "16": "Natural Language Processing (EMNLP),\n(Doha, Qatar), pp. 1724–1734, Association for"
        },
        {
          "16": "Computational Linguistics, Oct. 2014."
        },
        {
          "16": "44. K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, “Unsupervised pixel-"
        },
        {
          "16": "level domain adaptation with generative adversarial networks,” 2016."
        },
        {
          "16": "45. P. Arbel´aez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and hierarchical"
        },
        {
          "16": "image segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence,"
        },
        {
          "16": "vol. 33, no. 5, pp. 898–916, 2011."
        },
        {
          "16": "46. D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” 2014."
        },
        {
          "16": "47. P. Russo, F. M. Carlucci, T. Tommasi, and B. Caputo, “From source to target and back:"
        },
        {
          "16": "symmetric bi-directional adaptive gan,” 2017."
        },
        {
          "16": "48.\nJ. Liang, D. Hu, and J. Feng, “Do we\nreally need to access\nthe\nsource data?\nsource"
        },
        {
          "16": "hypothesis transfer for unsupervised domain adaptation,” in International Conference on"
        },
        {
          "16": "Machine Learning (ICML), pp. 6028–6039, 2020."
        },
        {
          "16": "49.\nJ. Wang, J. Chen, J. Lin, L. Sigal, and C. W. de Silva, “Discriminative feature alignment:"
        },
        {
          "16": "Improving transferability of unsupervised domain adaptation by gaussian-guided latent"
        },
        {
          "16": "alignment.,” Pattern Recognition, p. 107943, 2021."
        },
        {
          "16": "50. L. van der Maaten and G. Hinton, “Visualizing data using t-sne,” Journal of Machine"
        },
        {
          "16": "Learning Research, vol. 9, no. 86, pp. 2579–2605, 2008."
        },
        {
          "16": "51. A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin, and A. A. Kalinin,"
        },
        {
          "16": "“Albumentations: Fast and ﬂexible\nimage augmentations,” Information, vol. 11, no. 2,"
        },
        {
          "16": "2020."
        },
        {
          "16": "52. E. Loper and S. Bird, “Nltk:\nthe natural\nlanguage toolkit,” CoRR, vol. cs.CL/0205028,"
        },
        {
          "16": "07 2002."
        },
        {
          "16": "53.\nJ. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors for word representa-"
        },
        {
          "16": "tion,” in Proceedings of\nthe 2014 Conference on Empirical Methods in Natural Language"
        },
        {
          "16": "Processing (EMNLP), (Doha, Qatar), pp. 1532–1543, Association for Computational Lin-"
        },
        {
          "16": "guistics, Oct. 2014."
        },
        {
          "16": "54. V. Marivate and T. Sefara, “Improving short text classiﬁcation through global augmen-"
        },
        {
          "16": "tation methods,” in International Cross-Domain Conference for Machine Learning and"
        },
        {
          "16": "Knowledge Extraction, pp. 385–399, Springer, 2020."
        },
        {
          "16": "55. M. Lin, Q. Chen, and S. Yan, “Network in network,” 2013."
        },
        {
          "16": "Appendices"
        },
        {
          "16": "In this appendix section, we present in detail the design architecture of our SADDA"
        },
        {
          "16": "method in three experiments,\nincluding digit recognition, object recognition, and"
        },
        {
          "16": "sentiment classiﬁcation."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "17": ""
        },
        {
          "17": "increased model stability and reduce the"
        },
        {
          "17": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "18": "Fig. 6: The overview of the SADDA method for the object recognition task on the"
        },
        {
          "18": "VLCS [23] dataset. With the input image’s shape is 64 x 64 x 3"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "19": "Fig. 7: The overview of the SADDA method for the sentiment classiﬁcation task."
        },
        {
          "19": "The input sentence has a max length equal to 50. In the design above, to prevent"
        },
        {
          "19": "overﬁtting, the LSTM layer is always followed by the dropout layer with 0.2 rates."
        },
        {
          "19": "The numbers under the particular layer are the output shape of that layer."
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Covariate shift and local learning by distribution matching",
      "authors": [
        "A Gretton",
        "A Smola",
        "J Huang",
        "M Schmittfull",
        "K Borgwardt",
        "B Schölkopf"
      ],
      "year": "2009",
      "venue": "Covariate shift and local learning by distribution matching"
    },
    {
      "citation_id": "2",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "ArXiv"
    },
    {
      "citation_id": "3",
      "title": "Autonomous navigation in complex environments with deep multimodal fusion network",
      "authors": [
        "A Nguyen",
        "N Nguyen",
        "K Tran",
        "E Tjiputra",
        "Q Tran"
      ],
      "year": "2020",
      "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "4",
      "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
      "authors": [
        "J Donahue",
        "Y Jia",
        "O Vinyals",
        "J Hoffman",
        "N Zhang",
        "E Tzeng",
        "T Darrell"
      ],
      "year": "2013",
      "venue": "Decaf: A deep convolutional activation feature for generic visual recognition"
    },
    {
      "citation_id": "5",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "6",
      "title": "A database for handwritten text recognition research",
      "authors": [
        "J Hull"
      ],
      "year": "1994",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "Domain-adversarial training of neural networks"
    },
    {
      "citation_id": "8",
      "title": "Reading digits in natural images with unsupervised feature learning",
      "authors": [
        "Y Netzer",
        "T Wang",
        "A Coates",
        "A Bissacco",
        "B Wu",
        "A Ng"
      ],
      "year": "2011",
      "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning"
    },
    {
      "citation_id": "9",
      "title": "One-shot learning of object categories",
      "authors": [
        "L Fei-Fei",
        "R Fergus",
        "P Perona"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Labelme: A database and web-based tool for image annotation",
      "authors": [
        "B Russell",
        "A Torralba",
        "K Murphy",
        "W Freeman"
      ],
      "year": "2007",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results",
      "authors": [
        "M Everingham",
        "L Van Gool",
        "C Williams",
        "J Winn",
        "A Zisserman"
      ],
      "venue": "The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results"
    },
    {
      "citation_id": "12",
      "title": "Exploiting hierarchical context on a large database of object categories",
      "authors": [
        "M Choi",
        "J Lim",
        "A Torralba",
        "A Willsky"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised domain adaptation via representation learning and adaptive classifier learning",
      "authors": [
        "M Gheisari",
        "M Baghshah"
      ],
      "year": "2015",
      "venue": "Neurocomput"
    },
    {
      "citation_id": "14",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "15",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Adversarial discriminative domain adaptation",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "17",
      "title": "Semi-supervised learning: a brief review",
      "authors": [
        "Y Reddy",
        "V Pulabaigari"
      ],
      "year": "2018",
      "venue": "International Journal of Engineering & Technology"
    },
    {
      "citation_id": "18",
      "title": "Semi-supervised learning with generative adversarial networks",
      "authors": [
        "A Odena"
      ],
      "year": "2016",
      "venue": "Semi-supervised learning with generative adversarial networks"
    },
    {
      "citation_id": "19",
      "title": "Women's e-commerce clothing reviews",
      "authors": [
        "Nicapotato"
      ],
      "year": "2018",
      "venue": "Women's e-commerce clothing reviews"
    },
    {
      "citation_id": "20",
      "title": "Coronavirus tweets nlp -text classification",
      "authors": [
        "A Miglani"
      ],
      "year": "2020",
      "venue": "Coronavirus tweets nlp -text classification"
    },
    {
      "citation_id": "21",
      "title": "Joint multi-grain topic sentiment: modeling semantic aspects for online reviews",
      "authors": [
        "M Alam",
        "W.-J Ryu",
        "S Lee"
      ],
      "year": "2016",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "22",
      "title": "Learning from imbalanced data: open challenges and future directions",
      "authors": [
        "B Krawczyk"
      ],
      "year": "2016",
      "venue": "Progress in Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Unbiased look at dataset bias",
      "authors": [
        "A Torralba",
        "A Efros"
      ],
      "year": "2011",
      "venue": "CVPR 2011"
    },
    {
      "citation_id": "24",
      "title": "Collaborative robot-assisted endovascular catheterization with generative adversarial imitation learning",
      "authors": [
        "W Chi",
        "G Dagnino",
        "T Kwok",
        "A Nguyen",
        "D Kundrat",
        "M Abdelaziz",
        "C Riga",
        "C Bicknell",
        "G.-Z Yang"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "25",
      "title": "A review of domain adaptation without target labels",
      "authors": [
        "W Kouw",
        "M Loog"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "26",
      "title": "A literature review of domain adaptation with unlabeled data",
      "authors": [
        "A Margolis"
      ],
      "year": "2011",
      "venue": "A literature review of domain adaptation with unlabeled data"
    },
    {
      "citation_id": "27",
      "title": "Deep visual domain adaptation: A survey",
      "authors": [
        "M Wang",
        "W Deng"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "28",
      "title": "Deep reconstructionclassification networks for unsupervised domain adaptation",
      "authors": [
        "M Ghifary",
        "W Kleijn",
        "M Zhang",
        "D Balduzzi",
        "W Li"
      ],
      "year": "2016",
      "venue": "Deep reconstructionclassification networks for unsupervised domain adaptation"
    },
    {
      "citation_id": "29",
      "title": "Autoencoder-based unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "30",
      "title": "Deep transfer learning with joint adaptation networks",
      "authors": [
        "M Long",
        "H Zhu",
        "J Wang",
        "M Jordan"
      ],
      "year": "2016",
      "venue": "Deep transfer learning with joint adaptation networks"
    },
    {
      "citation_id": "31",
      "title": "Conditional adversarial domain adaptation",
      "authors": [
        "M Long",
        "Z Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2017",
      "venue": "Conditional adversarial domain adaptation"
    },
    {
      "citation_id": "32",
      "title": "Maximum classifier discrepancy for unsupervised domain adaptation",
      "authors": [
        "K Saito",
        "K Watanabe",
        "Y Ushiku",
        "T Harada"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "33",
      "title": "Domain generalization for object recognition with multi-task autoencoders",
      "authors": [
        "M Ghifary",
        "W Kleijn",
        "M Zhang",
        "D Balduzzi"
      ],
      "year": "2015",
      "venue": "Domain generalization for object recognition with multi-task autoencoders"
    },
    {
      "citation_id": "34",
      "title": "Domain separation networks",
      "authors": [
        "K Bousmalis",
        "G Trigeorgis",
        "N Silberman",
        "D Krishnan",
        "D Erhan"
      ],
      "year": "2016",
      "venue": "Domain separation networks"
    },
    {
      "citation_id": "35",
      "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "authors": [
        "X Glorot",
        "A Bordes",
        "Y Bengio"
      ],
      "year": "2011",
      "venue": "ICML"
    },
    {
      "citation_id": "36",
      "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "authors": [
        "P Vincent",
        "H Larochelle",
        "I Lajoie",
        "Y Bengio",
        "P.-A Manzagol"
      ],
      "year": "2010",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "37",
      "title": "Improved techniques for training gans",
      "authors": [
        "T Salimans",
        "I Goodfellow",
        "W Zaremba",
        "V Cheung",
        "A Radford",
        "X Chen"
      ],
      "year": "2016",
      "venue": "Improved techniques for training gans"
    },
    {
      "citation_id": "38",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2015",
      "venue": "Unsupervised representation learning with deep convolutional generative adversarial networks"
    },
    {
      "citation_id": "39",
      "title": "Rectifier nonlinearities improve neural network acoustic models",
      "authors": [
        "A Maas",
        "A Hannun",
        "A Ng"
      ],
      "year": "2013",
      "venue": "ICML Workshop on Deep Learning for Audio, Speech and Language Processing"
    },
    {
      "citation_id": "40",
      "title": "Unsupervised learning of video representations using lstms",
      "authors": [
        "N Srivastava",
        "E Mansimov",
        "R Salakhutdinov"
      ],
      "year": "2015",
      "venue": "Unsupervised learning of video representations using lstms"
    },
    {
      "citation_id": "41",
      "title": "A gentle introduction to lstm autoencoders",
      "authors": [
        "J Brownlee"
      ],
      "year": "2020",
      "venue": "A gentle introduction to lstm autoencoders"
    },
    {
      "citation_id": "42",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "43",
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "44",
      "title": "Unsupervised pixellevel domain adaptation with generative adversarial networks",
      "authors": [
        "K Bousmalis",
        "N Silberman",
        "D Dohan",
        "D Erhan",
        "D Krishnan"
      ],
      "year": "2016",
      "venue": "Unsupervised pixellevel domain adaptation with generative adversarial networks"
    },
    {
      "citation_id": "45",
      "title": "Contour detection and hierarchical image segmentation",
      "authors": [
        "P Arbeláez",
        "M Maire",
        "C Fowlkes",
        "J Malik"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "46",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "47",
      "title": "From source to target and back: symmetric bi-directional adaptive gan",
      "authors": [
        "P Russo",
        "F Carlucci",
        "T Tommasi",
        "B Caputo"
      ],
      "year": "2017",
      "venue": "From source to target and back: symmetric bi-directional adaptive gan"
    },
    {
      "citation_id": "48",
      "title": "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
      "authors": [
        "J Liang",
        "D Hu",
        "J Feng"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "49",
      "title": "Discriminative feature alignment: Improving transferability of unsupervised domain adaptation by gaussian-guided latent alignment",
      "authors": [
        "J Wang",
        "J Chen",
        "J Lin",
        "L Sigal",
        "C Silva"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "51",
      "title": "Albumentations: Fast and flexible image augmentations",
      "authors": [
        "A Buslaev",
        "V Iglovikov",
        "E Khvedchenya",
        "A Parinov",
        "M Druzhinin",
        "A Kalinin"
      ],
      "year": "2020",
      "venue": "Information"
    },
    {
      "citation_id": "52",
      "title": "Nltk: the natural language toolkit",
      "authors": [
        "E Loper",
        "S Bird"
      ],
      "year": "2002",
      "venue": "CoRR"
    },
    {
      "citation_id": "53",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "54",
      "title": "Improving short text classification through global augmentation methods",
      "authors": [
        "V Marivate",
        "T Sefara"
      ],
      "year": "2020",
      "venue": "International Cross-Domain Conference for Machine Learning and Knowledge Extraction"
    },
    {
      "citation_id": "55",
      "title": "Network in network",
      "authors": [
        "M Lin",
        "Q Chen",
        "S Yan"
      ],
      "year": "2013",
      "venue": "Network in network"
    }
  ]
}