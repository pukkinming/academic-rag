{
  "paper_id": "2508.11663v1",
  "title": "Unsupervised Pairwise Learning Optimization Framework For Cross-Corpus Eeg-Based Emotion Recognition Based On Prototype Representation",
  "published": "2025-08-06T06:29:19Z",
  "authors": [
    "Guangli Li",
    "Canbiao Wu",
    "Zhen Liang"
  ],
  "keywords": [
    "EEG",
    "Emotion Recognition",
    "EEG Processing",
    "Optimization Strategy",
    "Domain Adversarial Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing is a rapidly developing interdisciplinary research direction in the field of brain-computer interface, the core goal of which is to achieve accurate recognition of emotional states through physiological signals. In recent years, the introduction of deep learning technology has greatly promoted the development of the field of emotion recognition. However, due to physiological differences between subjects and changes in the experimental environment, cross-corpus emotion recognition faces serious challenges, especially for samples near the decision boundary. To solve the above problems, we propose an optimization method based on domain adversarial transfer learning to fine-grained alignment of affective features. In domain adaptive optimization, we propose Local maximum mean discrepancy (Lmmd) and Contrastive domain discrepancy (Cdd) strategies based on pairwise learning, introduce the theory of Reproducing Kernel Hilbert Space (RKHS), and use feature kernel function κ to realize the alignment of sample features. In the rule domain adaptive optimization, we further propose a Maximum classifier discrepancy with Pairwise Learning (McdPL) model to maximize classification discrepancy and minimize feature distribution by designing dual adversarial classifiers (Ada and RMS classifiers) to process samples around decision boundaries, and through three-stage adversarial training. During domain adversarial training, the two classifiers also maintain an adversarial relationship, ultimately enabling precise cross-corpus feature alignment. We conducted systematic experimental evaluation of the model using publicly available SEED, SEED-IV and SEED-V databases. The results show that the McdPL model is superior to other baseline models in the cross-corpus emotion recognition task, achieving SOTA performance with average accuracy improvements of 4.76% and 3.97%, respectively. Our work provides a promising solution for emotion recognition across databases. The source code is available at https://github.com/WuCB-BCI/Mcd_PL.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "A ffective is a psychological state, usually caused by neuro- physiological changes, and is one of the basic psychological experiences of human beings  [1] . It not only affects people's feelings, thinking, and behavior, but also affects people's physical and mental health  [2] . Therefore, how to accurately assess people's emotional state and provide personalized adjustment programs is a difficult problem to be solved. Emotional computing is a rapidly developing interdisciplinary research field, and emotional state recognition is a key issue in the field of Affective computing  [3] . Traditional emotion recognition mainly relies on non-physiological signals such as facial expressions, voice signals, and body gestures  [4] , which are highly subjective and the performance is limited. As a kind of physiological signal, electroencephalography (EEG) has the advantages of not being easily camouflaged, excellent real-time performance, objectivity and etc  [5]   [6] . It can provide a more direct and objective clue for understanding and evaluating emotional states, so it has attracted more and more attention from researchers in different fields such as computer science, neuroscience, and signal processing  [7] .\n\nIn recent years, more and more researchers have focused on applying deep learning methods to mitigate individual differences in EEG signals  [8] [9] [10] [11] [12]  and improve feature invariant representation  [13] [14] [15] . Emotion recognition models based on EEG have been widely used and have shown very good performance in emotion recognition tasks in databases.Such as, Zhang et al.  [16]  introduced both cascade and parallel convolutional recurrent neural network models for precisely identifying human intended movements and instructions, effectively learning the compositional spatio-temporal representations of raw EEG streams. Li et al.  [17]  proposed the R2G-STNN model, which consists of spatial and temporal neural network models with a regional to global hierarchical feature learning process to learn discriminative spatial-temporal EEG features. Feng et al.  [18]  designed a hybrid model called ST-GCLSTM, which comprises a spatial-graph convolutional network (SGCN) module and an attention-enhanced bi-directional Long Short-Term Memory (LSTM) module, which can be used to extract representative spatial-temporal features from multiple EEG channels. Yang et al.  [19]  proposed spectral-spatial atten- tion alignment multi-source domain adaptation (S 2 A 2 -MSD), which constructs domain attention to represent affective cognition attributes in spatial and spectral domains and utilizes domain consistent loss to align them between domains. Yan et al.  [20]  proposed the bridge graph attention-based graph convolution network (BGAGCN). It bridges previous graph convolution layers to attention coefficients of the final layer by adaptively combining each graph convolution output based on the graph attention network, thereby enhancing feature distinctiveness. However, (1) Due to the huge discrepancy between different databases  [21] , traditional EEG emotion recognition mainly focuses on intra-individual or cross-task. When dealing with EEG emotion recognition tasks in different databases, in addition to dealing with individual discrepancy, it is also necessary to face numerous factors such as the discrepancy in the environment and equipment of EEG collection, resulting in a significant decline in the performance of these emotion recognition methods  [3] . (2) At present, EEG-emotion experiments are basically induced by video. Subjects may not always respond correctly to emotions due to individual physiological factors, and their emotional changes maybe not be accurately described. This brings unavoidable label noise to the emotional labeling of EEG samples  [22] . The traditional EEG based emotion recognition model is mainly based on pointwise learning and has been successfully applied, but it is highly dependent on accurately labeled EEG data. In contrast, pairwise learning can model the relative association between samples and evaluate the similarity between samples, with less dependence on labels and better robustness and generalization performance  [23] [24] [25] [26] . (3) Domain adversarial learning has become a critical important strategy in the field of emotion recognition, significantly reducing the impact of individual discrepancy on recognition performance, and effectively improving the feature representation and generalization ability of the model  [27]   [28] . However, as shown in Fig.  1 .(a), many domain adversarial learning methods attempt to fully match feature distribution between different domains, but they ignore the decision boundaries of specific tasks between categories, which makes it difficult to accurately identify samples near the boundaries.\n\nTherefore, in order to solve the aforementioned triple challenges of cross-corpus feature distribution discrepancy, label noise interference and domain confrontation decision boundary ambiguity, we innovatively propose a Cross-corpus EEG emotion recognition Transfer model based on Maximum classifier discrepancy Pairwise Learning (McdPL). In addition, we also propose two cross-corpus transfer learning optimization methods (LmmdPL and CddPL), which we will introduce in Sec.3.2 and Sec.3.3.\n\nIn the McdPL model, we use domain discriminators and feature generators to mitigate the discrepancy in feature distribution between the source domain and the target domain, making it difficult for the discriminator to distinguish whether these features come from the source domain or the target domain. Furthermore, assuming that each emotion has fundamental characteristic attributes-named prototype representation, we extract the prototype representation to explore the potential variables of emotion-category EEG signals, and learn the generalized prototype representation of emotion representation between individuals to enhance the generalization ability of the model. We reformulate emotion recognition as a pairwise learning problem to replace traditional classifiers, thereby significantly reducing the model's dependence on high-precision emotion labels. Therefore, based on the idea of pairwise learning, we propose two classifiers with the same structure but different optimization strategies, and divide the training process into three stages, which are Basic Training, Maximize Classifiers Discrepancy and Minimize Features Distribution. Ultimately, the McdPL model achieves fine-grained alignment of features in cross-corpus samples, enhancing the model's ability to handle samples near the decision boundary. Overall, the main contributions of this paper are summarized as follows :\n\n• Based on adaptive optimization of domain adversarial learning, we propose the LmmdPL and CddPL models that focus on the relationship between related subdomains, and the McdPL models that focus on decision boundary Con-troversy samples. These models effectively align in the feature space, which enhances model performance.\n\n• In the McdPL model, we adopt the unsupervised learning strategy and propose the architecture of domain adversarial learning fusing two pairwise learning classifiers. During adversarial training of sample features from the source and target domains, the classifiers also conducts adversarial learning.\n\n• We conduct rigorous cross-corpus tests using three published databases (SEED, SEED-IV, and SEED-V). All the proposed models achieved excellent performance. Among them, the McdPL achieved SOTA performance, while thorough analyses of model components and parameters, along with feature visualization, are performed to enhance our understanding of the results.\n\nThe rest of this article is arranged as follows: We briefly described the background to the model in Sec.2. Then in Sec.3, we introduce the concrete implementation of our proposed model in detail. Our experimental results was described in Sec.4. Finally, We discuss our model performance and conclusions in Sec.5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Domain Adversarial Learning",
      "text": "Domain Adaptation Learning is a hot research field in machine learning, which maps samples in the source domain and target domain with different distributions into the same feature space so that their similar features in this space are as close as possible. It can effectively mitigate the learning problem of inconsistent probability distribution of source domain and target domain samples, so more and more researchers have paid attention to it, and it has been successfully applied in many fields  [29] [30] [31] [32] [33] [34] [35] [36] .\n\nSuch as, Huang et al.  [29]  proposed a bi-hemisphere discrepancy convolutional neural network model (BiDCNN) for EEG emotion recognition, which can effectively learn the different response patterns between the left and right hemispheres. Zhu et al.  [30]  presented a Multi-Representation Adaptation Network (MRAN), which dramatically improve the classification accuracy for cross-domain task and specially aims to align the distributions of multiple representations extracted. Gideon et al.  [31]  introduced Adversarial Discriminative Domain Generalization (ADDoG), which follows an easier to train 'meet in the middle' approach. This model iteratively moves representations learned for each dataset closer to one another, improving cross-dataset generalization. Tzeng et al.  [32]  presented an Adversarial Discriminative Domain Adaptation (ADDA) framework, which combines discriminative modeling, untied weight sharing and a GAN loss, achieving excellent unsupervised adaptation results on classification tasks. Luo et al.  [33]  introduced a category-level adversarial network, aiming to enforce local semantic consistency during the trend of global alignment and align each class with an adaptive adversarial loss, improving semantic segmentation accuracy. Gokhale et al.  [34]  proposed an adversarial training approach which learns to generate new samples to maximize exposure of the classifier to the attribute-space, without having access to the data from the test domain,which enables deep neural networks to be robust against a wide range of naturally occurring perturbations. Lee et al.  [35]  connected two distinct concepts for unsupervised domain adaptation: feature distribution alignment between domains by utilizing the task-specific decision boundary and the Wasserstein metric, which enhances the effectiveness and universality of the model. Zhou et al.  [36]  proposed a transfer learning framework based on Prototypical Representation based Pairwise Learning (PR-PL) to encode semantic structures inherent in affective EEG data, aligning individual EEG features with a shared common feature space However, PRPL only performs simple global alignment of sample features in domain anti-loss training, which may lose fine-grained information per class, which is more prominent in cross-corpus data. A visual example is shown in Fig.  1 .(a), after global alignment, the distributions of the two domains are roughly the same, but the overly close feature distributions may degrade partial model performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "The methods of emotion recognition are mainly based on non-physiological signals, such as facial expressions, language, Body gesture, and physiological signals, such as electroencephalogram (EEG), electrocardiogram (ECG), electromyogram (EMG), electrodermal activity (EDA), skin temperature (SKT), photoplethysmogram (PPG), respiration (RSP), electrooculogram (EOG)  [37] [38] [39] [40] . Compared to non-physiological signals, physiological signals seem to be more reliable in analyzing human emotions. As a kind of physiological signal, EEG has the advantage of being difficult to hide and disguise, so it has been widely studied in the field of affective computing, and the development of deep learning has greatly promoted research on emotion recognition based on EEG  [41] [42] [43] [44] [45] [46] [47] [48] .\n\nSuch as, Song et al.  [41]  proposed a variational instanceadaptive graph method (V-IAG) that simultaneously captures the individual dependencies among different EEG electrodes and estimates the underlying uncertain information. Ma et al.  [42]  proposed a multimodal residual LSTM (MMResLSTM) network for emotion recognition. The MMResLSTM network shares the weights across the modalities in each LSTM layer to learn the correlation between EEG and other physiological signals, which can efficiently learn emotion-related highlevel features. Liu et al.  [43]  proposed two methods for extending the original DCCA model for multimodal fusion: weighted sum fusion and attention-based fusion, enhanceing recognition performance and robustness of emotion recognition models. Anuragi et al.  [44]  proposed an automated crosssubject emotion recognition framework based on EEG signals, which uses the Fourier-Bessel series expansion-based empirical wavelet transform (FBSE-EWT) method, and achieved excellent human emotion recognition performance. Tao et al.  [45]  proposed an attention-based convolutional recurrent neural network (ACRNN) to extract more discriminative features from EEG signals and improve the accuracy of emotion recognition. Lin et al.  [46]  proposed a novel emotion recognition method based on a novel deep learning model (ERDL). The model fuses graph convolutional neural network (GCNN) and long-short term memories neural networks (LSTM), to extract graph domain features and temporal features. Liu et al.  [48]  proposed an effective multi-level features guided capsule network (MLF-CapsNet), which incorporates multi-level feature maps learned by different layers in forming the primary capsules, for multi-channel EEG-based emotion recognition.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "Suppose the samples and labels of source domain S and target domain T are expression by (X S , Y S ) and (X T , Y T ), and (X S , Y S ) = (x s i , y s i ) n i=1 , (X T , Y T ) = (x t i , y t i ) m i=1 , where x s i and x t i are EEG data samples, y s i and y t i are corresponding emotional labels. n and m are the number of samples in the source domain and the target domain, respectively. Significantly, the label information of the target domain is completely missing in the training process. For clearer expression, frequently used notations in this paper are summarized in Tab.1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Underlying Architecture",
      "text": "Our proposed model consists of three main modules: feature discriminator method, prototype representation extraction method and pairwise learning classifier.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Discriminator",
      "text": "Based on domain adaptive neural network (DANN), We introduce domain adversarial training to characterize the sample features. This method utilizes domain adaptive and adversarial training to minimize the differences between sample feature representations and effectively extract domain invariant attributes  [49] [50] [51] [52] [53] . Thus, To distinguish whether the sample features belong to the source domain or the target domain, suppose that D(•) represents the domain discriminator and f (•) represents the feature extractor. The domain countermeasure loss function l disc adopts the binary cross-entropy loss function, which is defined as:\n\nhere, x s i and x t j represent the i th sample from the source domain and the j th sample from the target domain, respectively. The n\\m denote the number of samples in the source and target domains, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Prototype Representation Extraction",
      "text": "For each emotion, it is assumed that there is a prototype representation, which represents the essential attribute of the emotion category, and samples belonging to the same category are distributed around the prototype representation. From the perspective of probability distribution, the prototype representation can be regarded as the centroid of the features of all samples within that category. Therefore, extracting prototype representations can enhance the generalization ability of the model. Suppose that\n\n, where x sc i denotes the i th sample belonging to category c in the source domain, and [X S c ] represents the number of samples in that category. C represent the number of emotional categories. For convenience, the prototype representations for each emotion can be defined as {ψ 1 , ψ 2 , . . . , ψ C } = ψ 1:C . Thus, the prototype representation defined as :\n\noverall, the prototype representation for the emotion category c is obtained by the average vector of all sample features belonging to that category. During model training, the prototype representations are constantly updated iteratively, ultimately yielding the optimal prototype representation for each emotion category.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pairwise Learning Classifier",
      "text": "Traditional point-to-point learning only focuses on the relationship between a single sample feature (F) and a prototype representation (ψ), but in order to capture the internal relationship between multiple samples, we adopt a pairwise learning strategy. Assume that the interaction feature Γ represents the interaction between the sample feature and the prototype representation, defined as Γ = (F • ψ c • θ), where (•) represents the inner product operation, F represents the extracted sample feature, ψ c is the prototype representation with emotion class c, θ is a trainable transformation matrix. Therefore, the pairwise learning target loss function based on interaction features  ). These modules include domain adversarial loss (Eq.1), supervised and unsupervised pairwise learning losses for source and target domains (Eq.3), maximum classifier discrepancy loss between Ada classifier and Rms classifier (Eq.13), and minimum feature distribution loss (Eq.14). X S and X T denotes source and target samples, respectively; φ(•) denotes the feature mapping operation; F s and F t represent the mapped features; ψ C denotes the prototype representation with emotion label c (Eq.2); ϕ(•) denotes the similarity measurement between samples and µ(•) denotes the pseudo-labeling operation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Similarity Measurement",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Subjective Feedbacks",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Minimize Features Distribution",
      "text": "is defined as:\n\nhere, ϕ i j (•) represents the similarity measurement between samples x i and x j , indicating the probability that samples x i and x j belong to the same emotion label, ranges from 0 to 1, with values closer to 1 suggesting that samples x i and x j are more likely to belong to the same emotional category, and vice versa. µ i j indicates whether samples x i and x j belong to the same emotion category, with the value 0 or 1. µ i j = 1 indicates that the sample pair belongs to the same emotion category, and vice versa. In the unsupervised learning of the target domain, µ i j is determined by pseudo-label. We set two thresholds, if the similarity measurement is higher than the upper threshold, the sample pairs are considered to belong to the same category, while if the similarity measurement is lower than the lower threshold, the sample pairs are not considered to belong to the same category.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Local Maximum Mean Discrepancy With Pairwise Learning",
      "text": "Maximum mean discrepancy (Mmd) is a statistical measure used to quantify the difference between two distributions. In transfer learning, Mmd can be applied to evaluate the similarity between the source domain and the target domain samples extracted by the feature extractor, that is the non-parametric measure  [54] . When applied to the loss function, the optimization objective is to minimize this metric. Mmd maps the feature distribution from a low-dimensional space to a highdimensional space. The samples evaluate their similarity by calculate the distance between their distributions, thereby enabling fine-grained alignment of source and target domain samples in the feature space.The source and target domain samples are represented as X S = {x s i } n i=1 and X T = {x t j } m j=1 , respectively. The objective function for Mmd is defined as:\n\nhere, φ(•) denotes the feature mapping operation for the source domain sample x s i or the target domain sample x t j . Hκ represents the corresponding Reproducing Kernel Hilbert Space (RKHS) with feature kernel κ, which is a feature mapping space. The feature kernel κ can be understood as a distance function, which defines the distance between samples x s and x t in the RKHS. This distance measure can be obtained by calculating the inner product of the samples in the feature space, expressed as κ = φ(x s ), φ(x t ) .\n\nFurthermore, as shown in Fig.  1 .(b), we focus not only on macro-level alignment but also on aligning sample features from a micro perspective, and needs to consider the relationships between subdomains of the same class across different domains. Therefore, based on Mmd, we propose the LmmdPL, suppose that the probability weights be denoted as ω, the Lmmd is defined as :\n\nhere, n c and m c represent the number of samples belonging to category c in the source and target domains, respectively. ω sc i and ω tc j indicate the probabilities of the source domain sample x s i and the target domain sample x t j belonging to their respective domains given category c. The Lmmd helps balance the characteristic contributions of the two domains, and focusing on the relationship between subdomains of the same class across different domains. Significantly, LmmdPL also requires mapping the sample features into the RKHS with the feature kernel κ. Therefore, Eq.5 can be transformed into:\n\nIn the source domain samples, we can easily obtain the true labels, But in the target domain, the label information is completely missing in the training process. Thus, we use the interaction feature Γ to approximate the estimation of the emotional categories of the samples, as Γ effectively represents the probability of the target domain samples belonging to each emotional category. Consequently, it is appropriate to calculate the weights ω using the interaction feature Γ in the target domain. Significantly, the sum of the weights of all samples belonging to category c in the source or target domain is always equal to 1,\n\ni=1 ω sc i = 1 and m c j=1 ω tc j = 1. Thus, the weights ω sc i and ω tc j for the source and target domain samples are defined as follows:\n\nhere, y c i represents the entry at the c th position of the true label y i for the source domain sample x s i . Significantly, the true labels for samples in the source domain are encoded using one-hot encoding. Eq.6 is defined as the loss function of lmmd (l lmmd Hκ ), Eq.3 is defined as the pairwise learning loss function of supervised source domain and unsupervised target domain respectively (l s pair \\l t pair ), and Eq.1 is defined as the domain adversarial loss function (l disc ). Overall, the objective function of LmmdPL is defined as:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Contrastive Domain Discrepancy With Pairwise Learning",
      "text": "LmmdPL method achieves alignment of subdomains in the feature space by reducing the distance between subdomains of the same category across the two domains. However, the distances between feature distributions of different categories in different domains may be too close. In the source domain, guided by true labels, the classifier can calculate complex decision boundaries to fit samples of different categories. However, when this rule is transferred to the target domain, samples located near the boundary are maybe misclassified by the model.\n\nThus, we further propose Contrastive Domain Discrepancy. Suppose d cc (•) represent the intra-domain discrepancy among samples of the same category, and d cc ′ (•) represent the interdomain discrepancy between different categories. For convenience, the predicted labels of the target domain samples {p t 1 , p t 2 , . . . , p t m } ∈ Γ t are expressed as p t 1:m . Overall, the Cdd objective function is defined as：\n\nhere, φ denotes the feature mapping operation. Significantly, the objective function consists of two parts, the portion before the minus sign indicates the inter-domain discrepancy among samples of the same category, while the latter represents the inter-domain discrepancy among samples of different categories. In this strategy, intra-class and inter-class discrepancies are optimized in opposing directions. With the iterative improvement of the model, as shown in Fig.  1 .(c), the feature representation of samples of the same emotion category becomes more compact, while those of different emotional categories gradually move away from each other. which can enhance the processing ability of the model for samples near the decision boundary. Suppose γ is a hyperparameter. We take Eq.9 is defined as the loss function of Cdd (l cdd Hκ ). Thus, the objective loss function of CddPL defined as:\n\nL Cdd = l s pair + αl t pair -βl disc + γl cdd Hκ (10)",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Maximum Classifier Discrepancy With Pairwise Learning",
      "text": "Both LmmdPL and CddPL models are built from the perspective of domain adaptation. Furthermore, from the perspective of adaptive optimization of rule domain, in order to find  a finer grained matching method between source domain and target domain, we consider the relationship between classification boundary and target samples. Thus, we designed two classifiers, named the Ada classifier and the Rms classifier, as shown in Fig.  2 . The Ada classifier employs the Adaptive Moment Estimation (Adam  [55] ) gradient descent algorithm, while the Rms classifier utilizes the Root Mean Square Propagation (RMSprop  [56] ) gradient descent algorithm. Although the classifier structures are identical, they obtain different parameters from the outset of training. During training, when the sample features from the source domain and the target domain are subjected to domain adversarial learning, the two classifiers also form adversarial learning.\n\nIn the feature space, suppose that there is an implicit correlation between the distribution of target domain samples and the corresponding distribution of source domain. Specifically, when the classification boundary derived from the source domain is directly applied to the target domain, it may yield classification results by both high and low confidence. The target domain samples with high confidence are closer to the source domain samples in feature space, indicating target domain exist source domain support. In contrast, samples with lower confidence are typically positioned further away from all feature cluster centers and are often located near the decision boundary, we refer to these samples as controversial samples. Identifying these controversial samples is important to improve the performance of the model for emotion recognition across databases, and these samples usually produce different classification results between the two classifiers. Thus, we propose a method Step 1 3:  l t pair ada , l t pair rms = L t pair (F t , ψ c , θ)\n\nL 1 = l disc + αl s pair + βl t pair ada + γl t pair rms 8:\n\nStep 2\n\n9:\n\nLock f s (X S )\\ f t (X T ) 10:\n\nl t pair ada , l t pair rms = L t pair (F t , ψ c , θ)\n\n11:\n\nL 2 = l t pair ada + l t pair rmsd cla 13:\n\nUnLock f s (X S )\\ f t (X T ) 14:\n\nStep 3\n\n15:\n\nLock Ada classi f ier\\Rms classi f ier 16:\n\nfor m = 1 to M do:\n\n17:\n\nl s pair = L s pair ( f (X S ), Y S )#Source Pairwise Loss 19:\n\nend for",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "21:",
      "text": "UnLock Ada Classi f ier\\Rms Classi f ier 22: end for Evaluation: Acc\\Std = Predicts(X T ,Y T ) to maximize classification differences during training. Without this step, the two classifiers would become excessively similar. Subsequently, the feature extractor is employed to deceive the discriminator, ensuring that target domain samples are generated within the support range of the source domain, thereby minimizing the feature distribution discrepancy. Importantly, the model iterates through this adversarial learning step repeatedly. The McdPL related algorithms are presented in Alg.1. Overall, the model training is organized into the following three steps.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Step 1: Basic Model Training",
      "text": "As shown in Fig.  3 .(Step 1), In order to enable the model to learn the basic classification parameters, we conduct preliminary training on the model's feature generator (F) and classifier (C Ada , C Rms ), aiming to obtain the baseline performance of the model in emotion recognition. The step 1 loss function is defined as:\n\npair + αl t pair ada + βl t pair ams + γl dise  (11)  here, l t pair ada and l t pair ams represent the pairwise learning losses of the Ada classifier and Rms classifier, respectively. α, β, γ are hyperparameters optimized for model performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Step 2: Maximize Classifiers Discrepancy",
      "text": "As shown in Fig.  3 .(Step 2) and Fig.  4 .(Step 2), the model freezes the parameters of the feature generator (F) and focuses on training the two classifiers (C Ada \\C Rms ) during training. This step aims to maximize the discrepancy between the classifiers, ensuring accurate classification of source domain samples while simultaneously identifying a greater number of target domain controversial samples. Therefore, we introduce a metric that quantifies the discrepancy between the two classifiers, defined as:\n\nhere, x t ∈ X T denotes a target domain samples, |•| represents the absolute operation. Γ Ada (x t ) and Γ Rms (x t ) denote the predicted probabilities for sample x t by the Ada classifier and Rms classifier, respectively. ϕ(•) is a similarity measurement between samples pairs. Suppose that l di f f represents the discrepancy value, the loss function for step 2 is defined as :\n\nhere, during the model training process, the overall trend of this loss function is downward. Consequently, L 1 and l di f f (X T ) are optimized in opposite directions. It is worth noting that L 2 remains greater than 0.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Step 3: Minimize Features Distribution",
      "text": "As shown in Fig.  3 .(Step 3) and Fig.  4 .(Step 3), in the process of model training, the goal is to minimize the difference of feature distribution. Therefor, we freezes the parameters of the two classifiers (C Ada \\C Rms ), and training the feature generator (F) to minimize the feature distribution. The model adjusts the mapping of target domain samples during training, the features extracted from the target domain samples gradually converge towards the clustering centers of the source domain in the feature space, enabling the Ada classifier and Rms classifier to achieve more accurate classifications. The loss function for step 3 is defined as:\n\nhere, l disc represents the domain discriminator loss of the model, and l pair represents the pairwise learning loss of the source domain. Minimizing the feature distribution is significant for the controversy samples identified in step 2.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emotional Dataset And Data Preprocessing",
      "text": "We validated our proposed three models on three well-known public databases: SEED  [65] , SEED-IV  [52]  and SEED-V  [49] . These databases have been widely utilized in research  [66] [67] [68] [69] . In the SEED and SEED-IV databases, a total of 15 subjects were participated in the experiments, with each subject completing three sessions on different dates. Each session in SEED consisted of 15 trials and included three emotions: negative, neutral and positive. Each session in SEED-IV consisted of 24 trials and included four emotions: happiness, sadness, fear, and neutral. In the SEED-V database, a total of 16 subjects were participated, with each completing three sessions on different dates. Each session comprised 15 trials and include five emotions: happiness, neutral, sadness, disgust and fear.\n\nTo consistency and rigor in our experiments, we defined happiness as a positive emotion and sadness as a negative emotion. we retained only positive, neutral, and negative emotional states in all databases. In the SEED-IV database, samples associated with the emotion of fear were excluded, while in the SEED-V database, samples associated with he emotion of fear and disgust were excluded. During the EEG signal collection process, all databases both utilized a 62-channel ESI neuroscan system, and all data underwent uniform preprocessing procedures.We downsampled the EEG data to 200 Hz and manually removed contaminated signals, such as EMG and EOG. Subsequently, the EEG data were filtered using a band-pass filter with a range of 0.3 Hz to 50 Hz. We set a window length of 1 second, segmenting the data for each trial into non-overlapping segments of 1 second. Based on five predefined frequency bands, Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-13 Hz), Beta (14-30 Hz) and Gamma (31-50 Hz), the corresponding signals is extracted according to these five frequency bands. Then, the Differential Entropy (DE  [70] ) was computed to represent the logarithm energy spectrum of each specific frequency band. For a sequence X that follows a Gaussian distribution N(µ, δ 2 ), the differential entropy for the i th frequency band can be defined as :\n\nThus, a complete emotional data segment contains 310 features (62 channels × 5 frequency bands). We applied a Linear Dynamic System (LDS) approach to smooth all features, which effectively leverages the temporal dependencies of emotional variations and filters out EEG components of unrelated and noise  [71] . Significantly, during the preprocessing of crossvalidation data, there is no issue of any information leakage, as the source and target domains samples from the independent databases.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Implementation Results",
      "text": "The model's feature extractor f (•) and discriminator D(•) are constructed using Multilayer Perceptron (MLP) with ReLU activation functions. The parameters of the bilinear transformation matrix θ are initialized randomly by uniform distribution. In the model architecture, the feature extractor is designed as: input layer (310) -hidden layer 1 (128) -ReLU activationhidden layer 2 (64) -ReLU activation -feature output layer  (64) . The domain discriminator is designed as: feature input",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiment Protocols",
      "text": "To comprehensively evaluate the robustness and stability of the models, we randomly selected source domain data and target domain data from the three databases, denoted as (Source Domain→Target Domain). It produce six different cross-corpus training combinations: SEED→SEED-IV, SEED→SEED-V, SEED-IV→SEED, SEED-IV→SEED-V, SEED-V→SEED and SEED-V→SEED-IV, respectively. Additionally, we employed four different validation protocols.\n\n(1) Cross-corpus Cross-subjects Single-session hold-outvaluation. In single-database EEG emotion recognition tasks, utilizing the first session for model evaluation is the most widely adopted validation protocol  [75] [76] [77] . The first session of all subjects from one database was designated as the source domain data, while the first session of all subjects from another database was used as the target domain data. (2) Crosscorpus Cross-subjects Cross-session hold-out-valuation. all sessions from all subjects within one database as source domain data and the another database as target domain data. since the Table  4 : The accuracy and Standard-Deviations of the various EEG emotion recognition Methods in Cross-corpus Cross-subjects Single-session leave-one-subjectout Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain). ↑ % denotes the improvement in accuracy between the best performance and the second-best performance. Here, the model results reproduced by us are indicated by '*'. cross-corpus cross-subject cross-session, this evaluation protocol presents significant challenges in EEG emotion recognition tasks.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Seed→Seed-Iv Seed→Seed-V Seed-Iv→Seed Seed-Iv→Seed-V Seed-V→Seed Seed-V→Seed-Iv Ave",
      "text": "(3) Cross-corpus Cross-subjects Single-session leaveone-subject-out Cross-valuation. To more rigorously validate the robustness and generalization performance of our model across databases, we will use an independent validation set for evaluation. Specifically, in one database, the first session of all subjects will be used as source domain sample data. In another database, the first session of one subject will be treated as the independent validation set, which will not participate in training or testing, and will be used to validate the overall performance of the model after training, while the first session of the remaining subjects will be used as target domain sample data. This process will continue until the first session of each subject in the target domain database has been used as the independent validation set. We will perform 15-fold (SEED,SEED-IV) or 16-fold (SEED-V) cross-validation, and the results will be averaged. (4) Cross-corpus Cross-subjects Cross-session leave-one-subject-out Cross-valuation. Specifically, all sessions of all subjects from one database are treated as source domain sample data, while all sessions of one subject from another database serve as the independent validation set, and the sessions of the remaining subjects are used as target domain sample data. This process will continue until each subject in the target domain database has been used as the independent validation set. We will perform 15-fold (SEED, SEED-IV) or 16-fold (SEED-V) cross-validation, and the results will be averaged. This evaluation protocol imposes more stringent requirements on the model.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Cross-Corpus Cross-Subjects Single-Session Hold-Outvaluation",
      "text": "The",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Confusion Matrix",
      "text": "As shown in Fig.  5 , we present the confusion matrices for the models (PrPL, LmmdPL, CddPL and McdPL) in SEED→SEED-V and SEED-V→SEED. In Fig.  5 .(a)∼(d), It is shown that the McdPL model achieved the best performance in all models, with accuracy of 55.33%, 52.25% and 72.53% for correctly identifying negative, neutral and positive emotion samples, respectively. Additionally, in the negative and positive emotion categories, the CddPL model achieved the secondbest recognition performance, with accuracies of 52.42% and 56.52%. The PrPL model demonstrated the second-SOTA performance in the neutral emotion category, with an accuracy of 51.63%. In Fig.  5 .(e)∼(h), It is shown that the McdPL model achieved accuracy of 66.72% and 79.91% for correctly identifying neutral and positive emotion samples, respectively. The",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Visualization Of Learned Representation",
      "text": "To provide a more intuitive comparison of the feature representation effects of different models, we used T-distributed Stochastic Neighbor Embedding (T-SNE  [78] ) algorithm to visualize the feature samples and interactive features in the source and target domains of the PrPL, LmmdPL, CddPL and McdPL models. As shown in Fig.  6 . This visualization demonstrate each model's ability to learn discriminative features and achieve class differentiation. Specifically, we randomly selected 256 samples each from the source and target domains to visualize the learned feature representations, respectively. In cross-corpus training combinations SEED→SEED-V and SEED-V→SEED, which are significant performance differences across models. In the feature distribution of the PrPL model (Fig.  6 .(a)(e)), the distribution of each category is the most dispersed, with blurred boundaries between classes, indicating the feature differentiation of samples is weak. In contrast, the feature distributions in the LmmdPL (Fig.  6 .(b)(f)) and CddPL (Fig.  6 .(c)(g)) models show increasing Clustering ability, with relatively clear boundaries forming between categories, reflecting stronger feature alignment potential. Significantly, the feature distribution obtained with the McdPL model (Fig.  6 .(d)(h)) is the most compact, with the clearest boundaries between categories. This dense and distinct distribution demonstrate the McdPL model's strong cross-domain feature align-ment capability and excellent classification performance, giving it a significant advantage in emotion recognition tasks.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Effect Of Noisy Labels",
      "text": "We adopt a pairwise learning strategy to effectively overcome the problem of label noise. In order to verify the robustness of the model under the pairwise learning and pointwise learning strategies and its dependence on the sample labels, we train the model using the source domain samples with added noise. Specifically, we introduce noise into the real label of the source domain in a controlled manner and evaluate the model using a sample of the target domain. We generate a large number of random labels and replace 10%, 20%, 30% and 40% of the source domain sample labels with these random labels. It is worth noting that during the training process, the label information of the target domain is completely lost.\n\nWe adopt the McdPL model as the baseline model and the results are shown in Tab.6. The introduction of noise in the source domain labels in the pairwise learning strategy results in a slight degradation of the model performance. When the noise level is 10%, 20%, 30% and 40%, the average accuracy is 57.04%, 55.52%, 54.52% and 54.22%, respectively, which is only reduced by 2.82%. However, the introduction of noise in the source domain labels in the pointwise learning strategy leads to a significant degradation in the model performance. When the noise level is 10%, 20%, 30% and 40%, the average accuracy is 50.22%, 48.11%, 46.01% and 44.57%, respectively, which is reduced by 5.65%. It is worth noting that compared with the pairwise learning strategy, the performance of the model is affected more by the increase of the proportion of label noise in the pointwise learning strategy. Without adding label noise (0%), the average accuracy is 58.90% and 53.74%, with a difference of 5.16%. However, when the proportion of label noise reaches 40%, the average accuracy is 54.22% and 44.57%, with a difference of 9.65%. These results show that increasing the proportion of noise in the source domain sample has a limited effect on model performance. Overall, our proposed McdPL model based on pairwise learning has excellent robustness and reliability, and has a high tolerance for noise labels.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Ablation Experiment",
      "text": "We systematically explored the effectiveness of different components in the proposed model through ablation experiment and examined the corresponding contributions to the overall performance, and the results are shown in Tab.7. Pairwise learning strategies have a better tolerance for the inevitable label noise in emotional EEG data. Therefore, when we removed the pairwise learning strategy from the target domain, the accuracy of the proposed LmmdPL, CddPL and McdPL were 54.18%, 54.19% and 65.11%, respectively, and the performances decreased by 4.09%, 2.09% and 3.64%, respectively. Additionally, when we remove the pairwise learning strategy from both the source and target domain, the performance of the three proposed models is significantly decreased by 13.7%, 9.92%, and 10.3%, respectively. These results show that the pairwise learning strategy effectively improves the performance and effectiveness of the model. When the prototype representation module is removed, the performance of CddPL and McdPL is significantly decreased by 6.33% and 10.1%, respectively. This indicates that the extraction of prototype representation makes a significant contribution to the model. In addition, the introduction of the bilinear transformation matrix θ proposed in Sec.3.1.3 can improve the model performance, and the recognition accuracy in CddPL is improved by 9.27% (from 47.01% to 56.28%). Experiments show that the introduction of domain adversarial training can greatly enhance the emotion recognition performance on the target domain. When the model removes the discriminator loss function, the performance of all three models decreases. The recognition accuracy of LmmdPL, CddPL and McdPL is 53.21%, 51.22% and 59.74%, and the performance is decreased by 5.06%, 5.06% and 9.01%, respectively. This significant decrease indicates the significant impact of individual differences issues on model performance and highlights the great potential of transfer learning in affective brain-computer interface (aBCI) applications.\n\nIn the MCdPL model, we adopted two classifiers based on pairwise learning and used three training steps to train the model. When we removed training step.2 (Maximize classification Discrepancy), the model performance decreased by 8.94% (from 68.75% to 59.81%). When we removed training step.3 (Minimize distribution difference), the model performance decreased by 14.7% (from 68.75% to 54.02%). This further demonstrates the contribution of the three training steps to the processing ability of samples located near the decision boundary. In addition, we remove step.2 and step.3 in McdPL, and use the traditional method to train the model. The results shown that the accuracy is 58.38%, and the model performance is significantly decreased by 10.4%.\n\nFurthermore, we tested the value of the hyperparameter γ in the loss function (Eq.8) of the LmmdPL model. The results show that when the value of γ is 0.5, the model demonstrated the optimal performance, with the accuracy reaching 58.27%. Similarly, we tested the value of γ in the loss function (Eq.10) of the CddPL model. The results show that when the value of γ is 1, the model exhibits the optimal performance.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "This work was proposed three cross-corpus transfer learning Methods based on pairwise learning to enhance feature alignment between source and target domain samples, thereby improving cross-corpus emotion recognition performance. In the domain adaptation approach, we proposed the LmmdPL and CddPL Methods to achieve finer-grained alignment of samples in the feature space. For rule-domain adaptation, we proposed the McdPL framework, which designs two distinct classifiers and incorporates three specialized training steps to explore a more suitable feature space for aligning sample features. The models were comprehensively evaluated on the SEED, SEED-IV and SEED-V databases. In addition, the stability and generalization of the model under various experimental settings were thoroughly validated. The experimental results show that Lmmd, CddPL and McdPL models have achieved excellent performance. Among them, the McdPL model achieves the optimal performance and is superior in dealing with the individual differences and noisy labeling problems in aBCI systems, which provides a promising solution for cross-corpus emotion recognition. In future studies, in view of the problems of individual differences and device differences in EEG, we will continue to explore solutions with better performance and stronger generalization ability.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Sample feature alignment schematic diagram of source domain and target domain: (a) sample feature alignment using traditional methods; (b) fine-",
      "page": 2
    },
    {
      "caption": "Figure 2: The McdPL model framework.",
      "page": 5
    },
    {
      "caption": "Figure 3: McdPL model training steps. The gray samples represent the contro-",
      "page": 6
    },
    {
      "caption": "Figure 4: Schematic diagram of each module of McdPL model training. Step 2",
      "page": 7
    },
    {
      "caption": "Figure 5: The confusion matrices for PrPL, LmmdPL, CddPL and McdPL during cross-corpus training. In matrices (a)∼(d), SEED is utilized as the source domain",
      "page": 11
    },
    {
      "caption": "Figure 6: Displays the T-SNE visualization of the sample features learned by the model (PrPL,LmmdPL,CddPL,McdPL) in both the source and target",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "Abstract"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "Affective computing is a rapidly developing interdisciplinary research direction in the field of brain-computer interface, the core"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "goal of which is to achieve accurate recognition of emotional states through physiological signals. In recent years, the introduction"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "of deep learning technology has greatly promoted the development of the field of emotion recognition. However, due to physio-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "logical differences between subjects and changes in the experimental environment, cross-corpus emotion recognition faces serious"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "challenges, especially for samples near the decision boundary. To solve the above problems, we propose an optimization method"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "based on domain adversarial\ntransfer\nlearning to fine-grained alignment of affective features.\nIn domain adaptive optimization,"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "we propose Local maximum mean discrepancy (Lmmd) and Contrastive domain discrepancy (Cdd) strategies based on pairwise"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "learning, introduce the theory of Reproducing Kernel Hilbert Space (RKHS), and use feature kernel function κ to realize the align-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "ment of sample features.\nIn the rule domain adaptive optimization, we further propose a Maximum classifier discrepancy with"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "Pairwise Learning (McdPL) model\nto maximize classification discrepancy and minimize feature distribution by designing dual"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "adversarial classifiers (Ada and RMS classifiers) to process samples around decision boundaries, and through three-stage adversar-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "ial\ntraining. During domain adversarial\ntraining,\nthe two classifiers also maintain an adversarial relationship, ultimately enabling"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "precise cross-corpus feature alignment. We conducted systematic experimental evaluation of the model using publicly available"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "SEED, SEED-IV and SEED-V databases.\nThe results show that\nthe McdPL model\nis superior\nto other baseline models in the"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "cross-corpus emotion recognition task, achieving SOTA performance with average accuracy improvements of 4.76% and 3.97%,"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "respectively. Our work provides a promising solution for emotion recognition across databases. The source code is available at"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "https://github.com/WuCB-BCI/Mcd_PL."
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "Keywords: EEG, Emotion Recognition, EEG Processing, Optimization Strategy, Domain Adversarial Learning."
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "1.\nIntroduction\nsignal processing [7]."
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "In recent years, more and more researchers have focused"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "on applying deep learning methods to mitigate individual dif-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "ferences in EEG signals [8–12] and improve feature invariant\nAffective is a psychological state, usually caused by neuro-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "representation [13–15]. Emotion recognition models based on"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "cal experiences of human beings [1]. It not only affects people’s"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "EEG have been widely used and have shown very good per-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "feelings, thinking, and behavior, but also affects people’s phys-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "formance in emotion recognition tasks\nin databases.Such as,"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "ical and mental health [2]. Therefore, how to accurately assess"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "Zhang et al.\n[16] introduced both cascade and parallel convo-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "people’s emotional state and provide personalized adjustment"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "lutional\nrecurrent neural network models for precisely identi-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "programs is a difficult problem to be solved. Emotional com-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "fying human intended movements and instructions, effectively"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "puting is a rapidly developing interdisciplinary research field,"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "learning the compositional spatio-temporal\nrepresentations of"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "and emotional state recognition is a key issue in the field of Af-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "raw EEG streams.\nLi et al.\n[17] proposed the R2G-STNN"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "fective computing [3]. Traditional emotion recognition mainly"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "model, which consists of spatial and temporal neural network"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "relies on non-physiological signals such as facial expressions,"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "models with a\nregional\nto global hierarchical\nfeature\nlearn-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "voice signals, and body gestures [4], which are highly subjec-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "ing process to learn discriminative spatial-temporal EEG fea-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "tive and the performance is limited. As a kind of physiologi-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "tures.\nFeng et al.\n[18] designed a hybrid model called ST-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "cal signal, electroencephalography (EEG) has the advantages of"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "GCLSTM, which comprises a spatial-graph convolutional net-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "not being easily camouflaged, excellent real-time performance,"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "work (SGCN) module and an attention-enhanced bi-directional"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "objectivity and etc [5][6].\nIt can provide a more direct and ob-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "Long Short-Term Memory (LSTM) module, which can be used"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "jective clue for understanding and evaluating emotional states,"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "to extract representative spatial-temporal features from multiple"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "so it has attracted more and more attention from researchers"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "EEG channels. Yang et al. [19] proposed spectral-spatial atten-"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "in different fields such as computer science, neuroscience, and"
        },
        {
          "dAddress correspondence to: janezliang@szu.edu.cn": "Preprint submitted to arXiv\nAugust 19, 2025"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Target Domain": "Target Domain"
        },
        {
          "Target Domain": "(a)Previous  Method"
        },
        {
          "Target Domain": "Figure 1: Sample feature alignment schematic diagram of source domain and target domain:"
        },
        {
          "Target Domain": "grained feature alignment\nin subdomains of the same category in different domains;"
        },
        {
          "Target Domain": "sample features are close to each other and dissimilar sample features are far away from each other."
        },
        {
          "Target Domain": "tion alignment multi-source domain adaptation (S2A2-MSD),"
        },
        {
          "Target Domain": "which constructs domain attention to represent affective cogni-"
        },
        {
          "Target Domain": "tion attributes in spatial and spectral domains and utilizes do-"
        },
        {
          "Target Domain": "main consistent loss to align them between domains. Yan et al."
        },
        {
          "Target Domain": "[20] proposed the bridge graph attention-based graph convolu-"
        },
        {
          "Target Domain": "tion network (BGAGCN).\nIt bridges previous graph convolu-"
        },
        {
          "Target Domain": "tion layers to attention coefficients of\nthe final\nlayer by adap-"
        },
        {
          "Target Domain": "tively combining each graph convolution output based on the"
        },
        {
          "Target Domain": "graph attention network, thereby enhancing feature distinctive-"
        },
        {
          "Target Domain": "ness."
        },
        {
          "Target Domain": "However, (1) Due to the huge discrepancy between different"
        },
        {
          "Target Domain": "databases [21], traditional EEG emotion recognition mainly fo-"
        },
        {
          "Target Domain": "cuses on intra-individual or cross-task. When dealing with EEG"
        },
        {
          "Target Domain": "emotion recognition tasks in different databases,\nin addition to"
        },
        {
          "Target Domain": "dealing with individual discrepancy, it is also necessary to face"
        },
        {
          "Target Domain": "numerous factors such as the discrepancy in the environment"
        },
        {
          "Target Domain": "and equipment of EEG collection, resulting in a significant de-"
        },
        {
          "Target Domain": "cline in the performance of these emotion recognition methods"
        },
        {
          "Target Domain": "[3].\n(2) At present, EEG-emotion experiments are basically in-"
        },
        {
          "Target Domain": "duced by video.\nSubjects may not always\nrespond correctly"
        },
        {
          "Target Domain": "to emotions due to individual physiological\nfactors, and their"
        },
        {
          "Target Domain": "emotional changes maybe not be accurately described.\nThis"
        },
        {
          "Target Domain": "brings unavoidable label noise to the emotional labeling of EEG"
        },
        {
          "Target Domain": "samples [22]. The traditional EEG based emotion recognition"
        },
        {
          "Target Domain": "model is mainly based on pointwise learning and has been suc-"
        },
        {
          "Target Domain": "cessfully applied, but\nit\nis highly dependent on accurately la-"
        },
        {
          "Target Domain": "beled EEG data.\nIn contrast, pairwise learning can model\nthe"
        },
        {
          "Target Domain": "relative association between samples and evaluate the similarity"
        },
        {
          "Target Domain": "between samples, with less dependence on labels and better ro-"
        },
        {
          "Target Domain": "bustness and generalization performance [23–26].\n(3) Domain"
        },
        {
          "Target Domain": "adversarial\nlearning has become a critical\nimportant\nstrategy"
        },
        {
          "Target Domain": "in the field of emotion recognition, significantly reducing the"
        },
        {
          "Target Domain": "impact of individual discrepancy on recognition performance,"
        },
        {
          "Target Domain": "and effectively improving the feature representation and gener-"
        },
        {
          "Target Domain": "alization ability of the model [27][28]. However, as shown in"
        },
        {
          "Target Domain": ""
        },
        {
          "Target Domain": "Fig.1.(a), many domain adversarial learning methods attempt to"
        },
        {
          "Target Domain": ""
        },
        {
          "Target Domain": "fully match feature distribution between different domains, but"
        },
        {
          "Target Domain": ""
        },
        {
          "Target Domain": "they ignore the decision boundaries of specific tasks between"
        },
        {
          "Target Domain": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "thorough analyses of model components and parameters,"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "along with feature visualization, are performed to enhance"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "our understanding of the results."
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "The rest of\nthis article is arranged as follows: We briefly de-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "scribed the background to the model in Sec.2. Then in Sec.3, we"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "introduce the concrete implementation of our proposed model"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "in detail. Our experimental results was described in Sec.4. Fi-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "nally, We discuss our model performance and conclusions in"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "Sec.5."
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "2. Related Work"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "2.1. Domain Adversarial Learning"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": ""
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "Domain Adaptation Learning is a hot\nresearch field in ma-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "chine learning, which maps samples in the source domain and"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "target domain with different distributions into the same feature"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "space so that\ntheir similar features in this space are as close as"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "possible.\nIt can effectively mitigate the learning problem of in-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "consistent probability distribution of source domain and target"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "domain samples, so more and more researchers have paid at-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "tention to it, and it has been successfully applied in many fields"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "[29–36]."
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "Such as, Huang et al. [29] proposed a bi-hemisphere discrep-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "ancy convolutional neural network model (BiDCNN) for EEG"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "emotion recognition, which can effectively learn the different"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "response patterns between the left and right hemispheres. Zhu"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "et al.\n[30] presented a Multi-Representation Adaptation Net-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "work (MRAN), which dramatically improve the classification"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "accuracy for cross-domain task and specially aims to align the"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "distributions of multiple representations extracted. Gideon et"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "al.\n[31] introduced Adversarial Discriminative Domain Gener-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "alization (ADDoG), which follows an easier\nto train ’meet\nin"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "the middle’ approach. This model\niteratively moves represen-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "tations learned for each dataset closer to one another,\nimprov-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "ing cross-dataset generalization.\nTzeng et al.\n[32] presented"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "an Adversarial Discriminative Domain Adaptation\n(ADDA)"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "framework, which combines discriminative modeling, untied"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "weight sharing and a GAN loss, achieving excellent unsuper-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "vised adaptation results on classification tasks. Luo et al.\n[33]"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "introduced a category-level adversarial network, aiming to en-"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "force\nlocal\nsemantic\nconsistency during the\ntrend of global"
        },
        {
          "them,\nthe McdPL achieved SOTA performance, while": "alignment and align each class with an adaptive adversarial"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "troversy samples. These models effectively\nalign in the": "feature space, which enhances model performance.",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "al. [34] proposed an adversarial training approach which learns"
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "to generate new samples to maximize exposure of the classifier"
        },
        {
          "troversy samples. These models effectively\nalign in the": "•",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "In the McdPL model, we adopt the unsupervised learning",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "to the attribute-space, without having access to the data from"
        },
        {
          "troversy samples. These models effectively\nalign in the": "strategy and propose the architecture of domain adversar-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "the test domain,which enables deep neural networks to be ro-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "ial\nlearning fusing two pairwise learning classifiers. Dur-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "bust against a wide range of naturally occurring perturbations."
        },
        {
          "troversy samples. These models effectively\nalign in the": "ing adversarial training of sample features from the source",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "Lee et al. [35] connected two distinct concepts for unsupervised"
        },
        {
          "troversy samples. These models effectively\nalign in the": "and target domains, the classifiers also conducts adversar-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "domain adaptation:\nfeature distribution alignment between do-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "ial learning.",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "mains by utilizing the task-specific decision boundary and the"
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "Wasserstein metric, which enhances the effectiveness and uni-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "• We conduct\nrigorous cross-corpus tests using three pub-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "versality of\nthe model.\nZhou et al.\n[36] proposed a trans-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "lished databases (SEED, SEED-IV, and SEED-V). All the",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "fer\nlearning framework based on Prototypical Representation"
        },
        {
          "troversy samples. These models effectively\nalign in the": "proposed models achieved excellent performance. Among",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "based Pairwise Learning (PR-PL) to encode semantic structures"
        },
        {
          "troversy samples. These models effectively\nalign in the": "them,\nthe McdPL achieved SOTA performance, while",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "inherent\nin affective EEG data, aligning individual EEG fea-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "thorough analyses of model components and parameters,",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "tures with a shared common feature space However, PRPL only"
        },
        {
          "troversy samples. These models effectively\nalign in the": "along with feature visualization, are performed to enhance",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "performs simple global alignment of sample features in domain"
        },
        {
          "troversy samples. These models effectively\nalign in the": "our understanding of the results.",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "anti-loss training, which may lose fine-grained information per"
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "class, which is more prominent\nin cross-corpus data. A visual"
        },
        {
          "troversy samples. These models effectively\nalign in the": "The rest of\nthis article is arranged as follows: We briefly de-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "example is shown in Fig.1.(a), after global alignment, the distri-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "scribed the background to the model in Sec.2. Then in Sec.3, we",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "butions of the two domains are roughly the same, but the overly"
        },
        {
          "troversy samples. These models effectively\nalign in the": "introduce the concrete implementation of our proposed model",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "close feature distributions may degrade partial model perfor-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "in detail. Our experimental results was described in Sec.4. Fi-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "mance."
        },
        {
          "troversy samples. These models effectively\nalign in the": "nally, We discuss our model performance and conclusions in",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "Sec.5.",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "2.2. EEG-based emotion recognition"
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "The methods of emotion recognition are mainly based on"
        },
        {
          "troversy samples. These models effectively\nalign in the": "2. Related Work",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "non-physiological signals, such as facial expressions, language,"
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "Body gesture,\nand physiological\nsignals,\nsuch as\nelectroen-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "2.1. Domain Adversarial Learning",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": ""
        },
        {
          "troversy samples. These models effectively\nalign in the": "",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "cephalogram (EEG),\nelectrocardiogram (ECG),\nelectromyo-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "Domain Adaptation Learning is a hot\nresearch field in ma-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "gram (EMG), electrodermal activity (EDA), skin temperature"
        },
        {
          "troversy samples. These models effectively\nalign in the": "chine learning, which maps samples in the source domain and",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "(SKT), photoplethysmogram (PPG),\nrespiration (RSP),\nelec-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "target domain with different distributions into the same feature",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "trooculogram (EOG) [37–40]. Compared to non-physiological"
        },
        {
          "troversy samples. These models effectively\nalign in the": "space so that\ntheir similar features in this space are as close as",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "signals, physiological signals seem to be more reliable in ana-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "possible.\nIt can effectively mitigate the learning problem of in-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "lyzing human emotions. As a kind of physiological signal, EEG"
        },
        {
          "troversy samples. These models effectively\nalign in the": "consistent probability distribution of source domain and target",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "has the advantage of being difficult\nto hide and disguise, so it"
        },
        {
          "troversy samples. These models effectively\nalign in the": "domain samples, so more and more researchers have paid at-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "has been widely studied in the field of affective computing, and"
        },
        {
          "troversy samples. These models effectively\nalign in the": "tention to it, and it has been successfully applied in many fields",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "the development of deep learning has greatly promoted research"
        },
        {
          "troversy samples. These models effectively\nalign in the": "[29–36].",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "on emotion recognition based on EEG [41–48]."
        },
        {
          "troversy samples. These models effectively\nalign in the": "Such as, Huang et al. [29] proposed a bi-hemisphere discrep-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "Such as, Song et al.\n[41] proposed a variational\ninstance-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "ancy convolutional neural network model (BiDCNN) for EEG",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "adaptive graph method (V-IAG)\nthat simultaneously captures"
        },
        {
          "troversy samples. These models effectively\nalign in the": "emotion recognition, which can effectively learn the different",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "the individual dependencies among different EEG electrodes"
        },
        {
          "troversy samples. These models effectively\nalign in the": "response patterns between the left and right hemispheres. Zhu",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "and estimates the underlying uncertain information. Ma et al."
        },
        {
          "troversy samples. These models effectively\nalign in the": "et al.\n[30] presented a Multi-Representation Adaptation Net-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "[42] proposed a multimodal\nresidual LSTM (MMResLSTM)"
        },
        {
          "troversy samples. These models effectively\nalign in the": "work (MRAN), which dramatically improve the classification",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "network for emotion recognition. The MMResLSTM network"
        },
        {
          "troversy samples. These models effectively\nalign in the": "accuracy for cross-domain task and specially aims to align the",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "shares the weights across the modalities in each LSTM layer"
        },
        {
          "troversy samples. These models effectively\nalign in the": "distributions of multiple representations extracted. Gideon et",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "to learn the correlation between EEG and other physiologi-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "al.\n[31] introduced Adversarial Discriminative Domain Gener-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "cal signals, which can efficiently learn emotion-related high-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "alization (ADDoG), which follows an easier\nto train ’meet\nin",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "level\nfeatures.\nLiu et al.\n[43] proposed two methods\nfor"
        },
        {
          "troversy samples. These models effectively\nalign in the": "the middle’ approach. This model\niteratively moves represen-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "extending the original DCCA model\nfor multimodal\nfusion:"
        },
        {
          "troversy samples. These models effectively\nalign in the": "tations learned for each dataset closer to one another,\nimprov-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "weighted sum fusion and attention-based fusion,\nenhanceing"
        },
        {
          "troversy samples. These models effectively\nalign in the": "ing cross-dataset generalization.\nTzeng et al.\n[32] presented",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "recognition performance and robustness of emotion recogni-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "an Adversarial Discriminative Domain Adaptation\n(ADDA)",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "tion models. Anuragi et al.\n[44] proposed an automated cross-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "framework, which combines discriminative modeling, untied",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "subject emotion recognition framework based on EEG signals,"
        },
        {
          "troversy samples. These models effectively\nalign in the": "weight sharing and a GAN loss, achieving excellent unsuper-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "which uses the Fourier-Bessel series expansion-based empiri-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "vised adaptation results on classification tasks. Luo et al.\n[33]",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "cal wavelet\ntransform (FBSE-EWT) method, and achieved ex-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "introduced a category-level adversarial network, aiming to en-",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "cellent human emotion recognition performance.\nTao et al."
        },
        {
          "troversy samples. These models effectively\nalign in the": "force\nlocal\nsemantic\nconsistency during the\ntrend of global",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "[45] proposed an attention-based convolutional recurrent neu-"
        },
        {
          "troversy samples. These models effectively\nalign in the": "alignment and align each class with an adaptive adversarial",
          "loss,\nimproving semantic segmentation accuracy. Gokhale et": "ral network (ACRNN)\nto extract more discriminative features"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "from EEG signals and improve the accuracy of emotion recog-": "nition. Lin et al.\n[46] proposed a novel emotion recognition",
          "feature representations and effectively extract domain invari-": "ant attributes [49–53]. Thus, To distinguish whether the sam-"
        },
        {
          "from EEG signals and improve the accuracy of emotion recog-": "method based on a novel deep learning model\n(ERDL). The",
          "feature representations and effectively extract domain invari-": "ple features belong to the source domain or the target domain,"
        },
        {
          "from EEG signals and improve the accuracy of emotion recog-": "model fuses graph convolutional neural network (GCNN) and",
          "feature representations and effectively extract domain invari-": "suppose that D(·) represents the domain discriminator and f (·)"
        },
        {
          "from EEG signals and improve the accuracy of emotion recog-": "long-short\nterm memories neural networks (LSTM),\nto extract",
          "feature representations and effectively extract domain invari-": "represents the feature extractor.\nThe domain countermeasure"
        },
        {
          "from EEG signals and improve the accuracy of emotion recog-": "graph domain features and temporal\nfeatures. Liu et al.\n[48]",
          "feature representations and effectively extract domain invari-": "loss function ldisc adopts the binary cross-entropy loss function,"
        },
        {
          "from EEG signals and improve the accuracy of emotion recog-": "proposed an effective multi-level\nfeatures guided capsule net-",
          "feature representations and effectively extract domain invari-": "which is defined as:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "j))\ni ))"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": ""
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "i=0\nj=0"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "(1)"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": ""
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "here, xs"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "j represent the ith sample from the source domain\ni and xt"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": ""
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "and the\nsample from the target domain,\nrespectively.\nThe\njth"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "n\\m denote the number of samples in the source and target do-"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "mains, respectively."
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": ""
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": ""
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "3.1.2. Prototype Representation Extraction"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": ""
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "For each emotion, it is assumed that there is a prototype rep-"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "resentation, which represents the essential attribute of the emo-"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "tion category, and samples belonging to the same category are"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "distributed around the prototype representation. From the per-"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "spective of probability distribution,\nthe prototype representa-"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "tion can be regarded as the centroid of the features of all sam-"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "ples within that category. Therefore, extracting prototype repre-"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "sentations can enhance the generalization ability of the model."
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "c ]"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "}[XS\n= {xsc\nxsc\nSuppose that XS"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": ", where\ndenotes\nsam-\nthe ith\nc\ni\ni\ni=1"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "ple belonging to category c in the source domain,\nand [XS"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "c ]"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "represents\nthe number of\nsamples\nin that category.\nC repre-"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "sent\nthe number of emotional categories. For convenience,\nthe"
        },
        {
          "−\nlog\nD( f (xs\nlog\n1 − D( f (xt\nldisc(XS , XT ) = −": "prototype representations for each emotion can be defined as"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0  , 1  , 0": "1  , 0  , 0",
          "0  , 1  , 0  ,   , 0": "1  , 0  , 1  ,   , 1"
        },
        {
          "0  , 1  , 0": ".\n.\n.",
          "0  , 1  , 0  ,   , 0": ".\n.\n.\n."
        },
        {
          "0  , 1  , 0": "0  , 0  , 1",
          "0  , 1  , 0  ,   , 0": "0  , 0  , 1  ,   , 0"
        },
        {
          "0  , 1  , 0": "Samples Categories",
          "0  , 1  , 0  ,   , 0": "Similarity Measurement"
        },
        {
          "0  , 1  , 0": "0.95, 0  ,0.05",
          "0  , 1  , 0  ,   , 0": "0.99,0.03,0.15,   ,0.99"
        },
        {
          "0  , 1  , 0": "0.03,0.93,0.04",
          "0  , 1  , 0  ,   , 0": "0.03,1.00,0.04,   ,0.06"
        },
        {
          "0  , 1  , 0": "0.87,0.12,0.01",
          "0  , 1  , 0  ,   , 0": ""
        },
        {
          "0  , 1  , 0": "",
          "0  , 1  , 0  ,   , 0": "0.95,0.04,0.02,   ,0.05"
        },
        {
          "0  , 1  , 0": ".\n.\n.",
          "0  , 1  , 0  ,   , 0": ".\n.\n.\n."
        },
        {
          "0  , 1  , 0": ".\n.\n.",
          "0  , 1  , 0  ,   , 0": ".\n.\n.\n."
        },
        {
          "0  , 1  , 0": "0.11,0.05,0.84",
          "0  , 1  , 0  ,   , 0": "0.01,0.09,0.98,   ,1.00"
        },
        {
          "0  , 1  , 0": "",
          "0  , 1  , 0  ,   , 0": ""
        },
        {
          "0  , 1  , 0": "Interaction Feature",
          "0  , 1  , 0  ,   , 0": "Similarity Measurement"
        },
        {
          "0  , 1  , 0": "",
          "0  , 1  , 0  ,   , 0": ""
        },
        {
          "0  , 1  , 0": "Ada_Classifier",
          "0  , 1  , 0  ,   , 0": ""
        },
        {
          "0  , 1  , 0": "",
          "0  , 1  , 0  ,   , 0": ""
        },
        {
          "0  , 1  , 0": "0.22,0.65,0.13",
          "0  , 1  , 0  ,   , 0": "1.00,0.05,0.96,   ,0.99"
        },
        {
          "0  , 1  , 0": "0.43,0.33,0.24",
          "0  , 1  , 0  ,   , 0": "0.03,1.00,0.04,   ,0.16"
        },
        {
          "0  , 1  , 0": "0.17,0.32,0.57",
          "0  , 1  , 0  ,   , 0": "0.95,0.04,0.92,   ,0.12"
        },
        {
          "0  , 1  , 0": ".\n.\n.",
          "0  , 1  , 0  ,   , 0": ".\n.\n.\n."
        },
        {
          "0  , 1  , 0": "0.31,0.08,0.61",
          "0  , 1  , 0  ,   , 0": "0.22,0.15,0.89,   ,1.00"
        },
        {
          "0  , 1  , 0": "Interaction Feature",
          "0  , 1  , 0  ,   , 0": "Similarity Measurement"
        },
        {
          "0  , 1  , 0": "",
          "0  , 1  , 0  ,   , 0": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Pairwise Loss": "Rms_Classifier"
        },
        {
          "Pairwise Loss": "Figure 2:\nThe McdPL model\nframework.\nThe model\nincludes Feature Discriminator, Prototype Representation Extraction and pairwise learning Classifiers"
        },
        {
          "Pairwise Loss": "(Ada classifier, Rms classifier). These modules include domain adversarial loss (Eq.1), supervised and unsupervised pairwise learning losses for source and target"
        },
        {
          "Pairwise Loss": "domains (Eq.3), maximum classifier discrepancy loss between Ada classifier and Rms classifier (Eq.13), and minimum feature distribution loss (Eq.14). XS\nand"
        },
        {
          "Pairwise Loss": "XT denotes source and target samples, respectively; φ(·) denotes the feature mapping operation; Fs and Ft represent the mapped features; ψC denotes the prototype"
        },
        {
          "Pairwise Loss": "representation with emotion label c (Eq.2); ϕ(·) denotes the similarity measurement between samples and µ(·) denotes the pseudo-labeling operation."
        },
        {
          "Pairwise Loss": "is defined as:\nmeasure [54]. When applied to the loss\nfunction,\nthe opti-"
        },
        {
          "Pairwise Loss": "mization objective is to minimize this metric. Mmd maps the"
        },
        {
          "Pairwise Loss": "(cid:88)\n(cid:104)\n(cid:16)\n(cid:17)(cid:105)\n1"
        },
        {
          "Pairwise Loss": "feature distribution from a low-dimensional\nspace to a high-\nlpair =\n−µi j log ϕi j(Γ) − (1 − µi j) log\n1 − ϕi j(Γ)"
        },
        {
          "Pairwise Loss": "(n\\m)2"
        },
        {
          "Pairwise Loss": "dimensional\nspace.\nThe samples evaluate their\nsimilarity by\ni, j∈k"
        },
        {
          "Pairwise Loss": "calculate the distance between their distributions,\nthereby en-\n(3)"
        },
        {
          "Pairwise Loss": "abling fine-grained alignment of source and target domain sam-"
        },
        {
          "Pairwise Loss": "here, ϕi j(·) represents the similarity measurement between sam-\nples in the feature space.The source and target domain samples"
        },
        {
          "Pairwise Loss": "ples xi and x j, indicating the probability that samples xi and x j\nj}m\ni }n\ni=1 and XT = {xt\nj=1, respectively."
        },
        {
          "Pairwise Loss": "belong to the same emotion label, ranges from 0 to 1, with val-\nThe objective function for Mmd is defined as:"
        },
        {
          "Pairwise Loss": "ues closer to 1 suggesting that samples xi and x j are more likely"
        },
        {
          "Pairwise Loss": "(cid:13)\n(cid:13)\n2"
        },
        {
          "Pairwise Loss": "(cid:13)\n(cid:13)"
        },
        {
          "Pairwise Loss": "n\nm\nto belong to the same emotional category, and vice versa. µi j in-\n(cid:13)\n(cid:13)"
        },
        {
          "Pairwise Loss": "(cid:88)\n(cid:88)"
        },
        {
          "Pairwise Loss": "(cid:13)\n(cid:13)"
        },
        {
          "Pairwise Loss": "1 n\n1 m\n(4)\n(cid:13)\n(cid:13)\nφ(xs\nφ(xt\nMMDHκ(XS , XT ) ="
        },
        {
          "Pairwise Loss": "dicates whether samples xi and x j belong to the same emotion\nj)\ni ) −\n(cid:13)\n(cid:13)"
        },
        {
          "Pairwise Loss": "(cid:13)\n(cid:13)"
        },
        {
          "Pairwise Loss": "i=1\nj=1\n(cid:13)\n(cid:13)\nthe sam-\ncategory, with the value 0 or 1. µi j = 1 indicates that\nHκ"
        },
        {
          "Pairwise Loss": "ple pair belongs to the same emotion category, and vice versa."
        },
        {
          "Pairwise Loss": "here, φ(·) denotes the feature mapping operation for the source"
        },
        {
          "Pairwise Loss": "is deter-\nIn the unsupervised learning of the target domain, µi j"
        },
        {
          "Pairwise Loss": "domain sample xs\nthe target domain sample xt"
        },
        {
          "Pairwise Loss": "or\nj. Hκ rep-\ni\nmined by pseudo-label. We set\ntwo thresholds,\nif the similar-"
        },
        {
          "Pairwise Loss": "resents\nthe corresponding Reproducing Kernel Hilbert Space"
        },
        {
          "Pairwise Loss": "ity measurement is higher than the upper threshold, the sample"
        },
        {
          "Pairwise Loss": "(RKHS) with feature kernel\nκ, which is\na\nfeature mapping"
        },
        {
          "Pairwise Loss": "pairs are considered to belong to the same category, while if the"
        },
        {
          "Pairwise Loss": "space.\nThe feature kernel κ can be understood as a distance"
        },
        {
          "Pairwise Loss": "similarity measurement\nis lower\nthan the lower\nthreshold,\nthe"
        },
        {
          "Pairwise Loss": "function, which defines the distance between samples xs and xt"
        },
        {
          "Pairwise Loss": "sample pairs are not considered to belong to the same category."
        },
        {
          "Pairwise Loss": "in the RKHS. This distance measure can be obtained by cal-"
        },
        {
          "Pairwise Loss": "culating the inner product of the samples in the feature space,"
        },
        {
          "Pairwise Loss": "3.2. Local maximum mean discrepancy with Pairwise Learning"
        },
        {
          "Pairwise Loss": "expressed as κ = (cid:10)φ(xs), φ(xt)(cid:11)."
        },
        {
          "Pairwise Loss": "Furthermore, as\nshown in Fig.1.(b), we focus not only on\nMaximum mean discrepancy (Mmd) is a statistical measure"
        },
        {
          "Pairwise Loss": "macro-level\nalignment but\nalso on aligning sample\nfeatures\nused to quantify the difference between two distributions.\nIn"
        },
        {
          "Pairwise Loss": "from a micro perspective, and needs to consider\nthe relation-\ntransfer learning, Mmd can be applied to evaluate the similar-"
        },
        {
          "Pairwise Loss": "ships between subdomains of\nthe same class across different\nity between the source domain and the target domain samples"
        },
        {
          "Pairwise Loss": "domains. Therefore, based on Mmd, we propose the LmmdPL,\nextracted by the feature extractor,\nthat\nis\nthe non-parametric"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": "is defined as :"
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": ""
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": ""
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": ""
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": "LmmdHκ(XS , XT ) = 1"
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": ""
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": "C"
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": ""
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": ""
        },
        {
          "suppose that the probability weights be denoted as ω, the Lmmd": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "here, nc and mc": "category c in the source and target domains, respectively. ωsc",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": "i"
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "and ωtc",
          "represent": "indicate the probabilities of the source domain sample",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "j",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "xs",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "i and the target domain sample xt",
          "the number of samples belonging to": "j belonging to their respective"
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "domains given category c. The Lmmd helps balance the char-",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "acteristic contributions of the two domains, and focusing on the",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "relationship between subdomains of the same class across dif-",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "ferent domains. Significantly, LmmdPL also requires mapping",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "the sample features into the RKHS with the feature kernel κ.",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "Therefore, Eq.5 can be transformed into:",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": "C\nn\nn"
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": "(cid:88)\n(cid:88)\n(cid:88)"
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "LmmdHκ(XS , XT ) = 1",
          "the number of samples belonging to": "ωsc\n]\ni ωsc\nj κ(xs\ni , xs\nj)"
        },
        {
          "here, nc and mc": "",
          "represent": "C",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": "i=1\nj=1\nc=1"
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": "(6)"
        },
        {
          "here, nc and mc": "m\nm",
          "represent": "",
          "the number of samples belonging to": "n\nm"
        },
        {
          "here, nc and mc": "(cid:88)\n(cid:88)",
          "represent": "",
          "the number of samples belonging to": "(cid:88)\n(cid:88)"
        },
        {
          "here, nc and mc": "+",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "ωtc",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "i ωtc\nj κ(xt\ni, xt",
          "the number of samples belonging to": "ωsc\nj) − 2\nj κ(xs\nj)\ni ωtc\ni , xt"
        },
        {
          "here, nc and mc": "i=1\nj=1",
          "represent": "",
          "the number of samples belonging to": "i=1\nj=1"
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "In the source domain samples, we can easily obtain the true",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "labels, But",
          "represent": "in the target domain,",
          "the number of samples belonging to": "the label\ninformation is com-"
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "pletely missing in the training process. Thus, we use the inter-",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "action feature Γ to approximate the estimation of the emotional",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "",
          "the number of samples belonging to": ""
        },
        {
          "here, nc and mc": "",
          "represent": "categories of the samples, as Γ effectively represents the prob-",
          "the number of samples belonging to": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 1 McdPL model Training algorithm.": ""
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "Input: Source and Target domain samples XS \\XT ; Source and"
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": ""
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "Target Labels Y S \\Y T ; Feature Extractor\nfs(·)\\ ft(·); Prototype"
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": ""
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "Representation Extractor ψc;\nInteraction Features ΓAda\\ΓRms;"
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": ""
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "domain adversarial\nloss Ldise; Source and Target domain pair-"
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": ""
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "wise loss Ls"
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "pair; Discrepancy between classifiers D; Dis-\npair\\Lt"
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "crepancy of Samples Feature D f ; Hyperparameter M; Accu-"
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "racy and Standard-Deviation Acc\\S td."
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "Training:"
        },
        {
          "Algorithm 1 McdPL model Training algorithm.": "1:\nfor n = 1 to (E poch size) do:"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "."
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "Fix\nUpdate"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "Classifier \n(Step 3)"
        },
        {
          "Discrepancy Loss": "Classifier"
        },
        {
          "Discrepancy Loss": "Predictions"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "Feature"
        },
        {
          "Discrepancy Loss": "Target"
        },
        {
          "Discrepancy Loss": "Extractor\n."
        },
        {
          "Discrepancy Loss": "Sample"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "."
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "."
        },
        {
          "Discrepancy Loss": "Distribution Loss"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "Update\nFix"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "Figure 4: Schematic diagram of each module of McdPL model training. Step 2"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "corresponds to Fig.3.(Step 2),\nin which the parameters of the feature extractor"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "are fixed and two classifiers are trained. Step 3 corresponds to Fig.3.(Step 3),"
        },
        {
          "Discrepancy Loss": "in which the parameters of the two classifiers are fixed and the parameters of"
        },
        {
          "Discrepancy Loss": "the feature extractor are trained."
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "a finer grained matching method between source domain and"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "target domain, we consider\nthe relationship between classifi-"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "cation boundary and target samples.\nThus, we designed two"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "classifiers, named the Ada classifier and the Rms classifier, as"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "shown in Fig.2. The Ada classifier employs the Adaptive Mo-"
        },
        {
          "Discrepancy Loss": "ment Estimation (Adam[55]) gradient descent algorithm, while"
        },
        {
          "Discrepancy Loss": "the Rms classifier utilizes the Root Mean Square Propagation"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "(RMSprop[56]) gradient descent algorithm. Although the clas-"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "sifier structures are identical,\nthey obtain different parameters"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "from the outset of training. During training, when the sample"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "features from the source domain and the target domain are sub-"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "jected to domain adversarial\nlearning,\nthe two classifiers also"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "form adversarial learning."
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "In the feature space, suppose that\nthere is an implicit corre-"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "lation between the distribution of\ntarget domain samples and"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "the corresponding distribution of source domain. Specifically,"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "when the classification boundary derived from the source do-"
        },
        {
          "Discrepancy Loss": "main is directly applied to the target domain, it may yield clas-"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "sification results by both high and low confidence. The target"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "domain samples with high confidence are closer to the source"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "domain samples in feature space, indicating target domain exist"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "source domain support.\nIn contrast, samples with lower con-"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "fidence are typically positioned further away from all\nfeature"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "cluster centers and are often located near the decision boundary,"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "we refer to these samples as controversial samples.\nIdentifying"
        },
        {
          "Discrepancy Loss": "these controversial samples is important to improve the perfor-"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "mance of the model for emotion recognition across databases,"
        },
        {
          "Discrepancy Loss": "and these samples usually produce different classification re-"
        },
        {
          "Discrepancy Loss": ""
        },
        {
          "Discrepancy Loss": "sults between the two classifiers. Thus, we propose a method"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "pleting three sessions on different dates. Each session in SEED"
        },
        {
          "hyperparameters optimized for model performance.": "3.4.2. Step 2: Maximize Classifiers Discrepancy",
          "were participated in the experiments, with each subject com-": "consisted of 15 trials and included three emotions:\nnegative,"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "neutral and positive. Each session in SEED-IV consisted of 24"
        },
        {
          "hyperparameters optimized for model performance.": "As\nshown in Fig.3.(Step 2) and Fig.4.(Step 2),\nthe model",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "trials and included four emotions: happiness, sadness, fear, and"
        },
        {
          "hyperparameters optimized for model performance.": "freezes\nthe parameters of\nthe\nfeature generator\n(F)\nand fo-",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "neutral.\nIn the SEED-V database, a total of 16 subjects were"
        },
        {
          "hyperparameters optimized for model performance.": "cuses on training the two classifiers (CAda\\CRms) during train-",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "participated, with each completing three sessions on different"
        },
        {
          "hyperparameters optimized for model performance.": "ing. This step aims to maximize the discrepancy between the",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "dates. Each session comprised 15 trials and include five emo-"
        },
        {
          "hyperparameters optimized for model performance.": "classifiers,\nensuring accurate classification of\nsource domain",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "tions: happiness, neutral, sadness, disgust and fear."
        },
        {
          "hyperparameters optimized for model performance.": "samples while simultaneously identifying a greater number of",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "To consistency and rigor in our experiments, we defined hap-"
        },
        {
          "hyperparameters optimized for model performance.": "target domain controversial samples. Therefore, we introduce",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "piness as a positive emotion and sadness as a negative emotion."
        },
        {
          "hyperparameters optimized for model performance.": "a metric that quantifies the discrepancy between the two classi-",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "we retained only positive, neutral, and negative emotional states"
        },
        {
          "hyperparameters optimized for model performance.": "fiers, defined as:",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "in all databases.\nIn the SEED-IV database, samples associated"
        },
        {
          "hyperparameters optimized for model performance.": "(cid:88)\n(cid:12)\n(cid:12)",
          "were participated in the experiments, with each subject com-": "with the emotion of fear were excluded, while in the SEED-V"
        },
        {
          "hyperparameters optimized for model performance.": "\n\n(cid:12)",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "(12)\nldi f\nf (XT ) = Mean\n(cid:12)ϕ(ΓAda(xt)) − ϕ(ΓRms(xt))\n(cid:12)",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "database, samples associated with he emotion of fear and dis-"
        },
        {
          "hyperparameters optimized for model performance.": "xt∈XT",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "gust were excluded. During the EEG signal collection process,"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "all databases both utilized a 62-channel ESI neuroscan system,"
        },
        {
          "hyperparameters optimized for model performance.": "here, xt ∈ XT denotes a target domain samples, |·| represents the",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "and all data underwent uniform preprocessing procedures.We"
        },
        {
          "hyperparameters optimized for model performance.": "absolute operation. ΓAda(xt) and ΓRms(xt) denote the predicted",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "downsampled the EEG data to 200 Hz and manually removed"
        },
        {
          "hyperparameters optimized for model performance.": "probabilities for sample xt by the Ada classifier and Rms classi-",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "contaminated signals,\nsuch as EMG and EOG. Subsequently,"
        },
        {
          "hyperparameters optimized for model performance.": "fier, respectively. ϕ(·) is a similarity measurement between sam-",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "the EEG data were filtered using a band-pass filter with a range"
        },
        {
          "hyperparameters optimized for model performance.": "ples pairs. Suppose that\nrepresents the discrepancy value,\nldi f\nf",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "of 0.3 Hz to 50 Hz. We set a window length of 1 second, seg-"
        },
        {
          "hyperparameters optimized for model performance.": "the loss function for step 2 is defined as :",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "menting the data for each trial\ninto non-overlapping segments"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "of 1 second. Based on five predefined frequency bands, Delta"
        },
        {
          "hyperparameters optimized for model performance.": "(13)\nf (XT ))\nL2 = min(L1 − max{CAda,CRms}ldi f",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "(1-3 Hz), Theta (4-7 Hz), Alpha (8-13 Hz), Beta (14-30 Hz)"
        },
        {
          "hyperparameters optimized for model performance.": "here, during the model\ntraining process,\nthe overall\ntrend of",
          "were participated in the experiments, with each subject com-": "and Gamma (31-50 Hz), the corresponding signals is extracted"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "according to these five frequency bands. Then, the Differential"
        },
        {
          "hyperparameters optimized for model performance.": "f (XT )\nthis loss function is downward. Consequently, L1 and ldi f",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "Entropy (DE [70]) was computed to represent the logarithm en-"
        },
        {
          "hyperparameters optimized for model performance.": "are optimized in opposite directions.\nIt\nis worth noting that L2",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "remains greater than 0.",
          "were participated in the experiments, with each subject com-": "ergy spectrum of each specific frequency band. For a sequence"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "X that follows a Gaussian distribution N(µ, δ2),\nthe differential"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "entropy for the ith frequency band can be defined as :"
        },
        {
          "hyperparameters optimized for model performance.": "3.4.3. Step 3: Minimize Features Distribution",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "As shown in Fig.3.(Step 3) and Fig.4.(Step 3),\nin the pro-",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "(x−µ)2\n(x−µ)2\n∞(cid:90)"
        },
        {
          "hyperparameters optimized for model performance.": "cess of model\ntraining,\nthe goal\nis to minimize the difference",
          "were participated in the experiments, with each subject com-": "\n\n1\n1\n2δ2\n2δ2"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "i\ni\ndx\ne\ne\nlog\nhi(X) ="
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "(cid:113)\n(cid:113)"
        },
        {
          "hyperparameters optimized for model performance.": "of feature distribution. Therefor, we freezes the parameters of",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "2πδ2\n2πδ2\n(15)\n−∞"
        },
        {
          "hyperparameters optimized for model performance.": "the two classifiers (CAda\\CRms), and training the feature genera-",
          "were participated in the experiments, with each subject com-": "i\ni"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "(cid:16)\n(cid:17)"
        },
        {
          "hyperparameters optimized for model performance.": "tor (F) to minimize the feature distribution. The model adjusts",
          "were participated in the experiments, with each subject com-": "= 1"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "log\n2πeδ2\ni"
        },
        {
          "hyperparameters optimized for model performance.": "the mapping of target domain samples during training, the fea-",
          "were participated in the experiments, with each subject com-": "2"
        },
        {
          "hyperparameters optimized for model performance.": "tures extracted from the target domain samples gradually con-",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "Thus, a complete emotional data segment contains 310 fea-"
        },
        {
          "hyperparameters optimized for model performance.": "verge towards the clustering centers of the source domain in the",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "tures (62 channels × 5 frequency bands). We applied a Lin-"
        },
        {
          "hyperparameters optimized for model performance.": "feature space, enabling the Ada classifier and Rms classifier to",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "ear Dynamic System (LDS) approach to smooth all\nfeatures,"
        },
        {
          "hyperparameters optimized for model performance.": "achieve more accurate classifications. The loss function for step",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "which effectively leverages the temporal dependencies of emo-"
        },
        {
          "hyperparameters optimized for model performance.": "3 is defined as:",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "(cid:16)\n(cid:17)",
          "were participated in the experiments, with each subject com-": "tional variations and filters out EEG components of unrelated"
        },
        {
          "hyperparameters optimized for model performance.": "(14)\nldisc + ls\nL3 = min",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "pair",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "F",
          "were participated in the experiments, with each subject com-": "and noise [71]. Significantly, during the preprocessing of cross-"
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "validation data, there is no issue of any information leakage, as"
        },
        {
          "hyperparameters optimized for model performance.": "here, ldisc represents the domain discriminator loss of the model,",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "the source and target domains samples from the independent"
        },
        {
          "hyperparameters optimized for model performance.": "represents the pairwise learning loss of the source do-\nand lpair",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "",
          "were participated in the experiments, with each subject com-": "databases."
        },
        {
          "hyperparameters optimized for model performance.": "main. Minimizing the feature distribution is significant for the",
          "were participated in the experiments, with each subject com-": ""
        },
        {
          "hyperparameters optimized for model performance.": "controversy samples identified in step 2.",
          "were participated in the experiments, with each subject com-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "",
          "↑ %": ""
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "",
          "↑ %": ""
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "SEED→SEED-IV",
          "↑ %": "Ave Acc"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "43.14 ± 8.36",
          "↑ %": "38.45"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "48.57 ± 8.62",
          "↑ %": "46.18"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "44.13 ± 7.85",
          "↑ %": "39.89"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "41.35 ± 4.63",
          "↑ %": "34.18"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "42.30 ± 8.55",
          "↑ %": "40.03"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "51.04 ± 2.82",
          "↑ %": "53.76"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "41.48 ± 8.50",
          "↑ %": "42.12"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "48.28 ± 8.31",
          "↑ %": "47.22"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "51.08 ± 1.08",
          "↑ %": "52.72"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "52.63 ± 1.59",
          "↑ %": "54.17"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "46.24 ± 9.03",
          "↑ %": "42.63"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "53.50 ± 2.48",
          "↑ %": "58.93"
        },
        {
          "the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target Domain).": "↑ 1.13",
          "↑ %": "↑ 4.76"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "layer (64) – hidden layer 1 (128) – ReLU activation – dropout",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "4.3. Experiment Protocols"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "layer – hidden layer 2 (64) – Sigmoid activation (1) – out-",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "To\ncomprehensively\nevaluate\nthe\nrobustness\nand\nstability"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "put\nlayer (1).\nIn the LmmdPL and CddPL model, Since there",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "of\nthe models, we\nrandomly\nselected\nsource\ndomain\ndata"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "is only one classifier, We randomly initialize the parameters",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "and\ntarget\ndomain\ndata\nfrom the\nthree\ndatabases,\ndenoted"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "and utilize the Rmsprop optimizer\nto optimize the model.\nIn",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "as\n(Source Domain→Target Domain).\nIt\nproduce\nsix\ndif-"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "the McdPL model,\nthe two classifiers utilize the Adam opti-",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "ferent\ncross-corpus\ntraining\ncombinations:\nSEED→SEED-"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "mizer and RMSprop optimizer, respectively, which demonstrat-",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "IV, SEED→SEED-V, SEED-IV→SEED, SEED-IV→SEED-V,"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "ing better performance compared to other classical optimizers.",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "SEED-V→SEED and SEED-V→SEED-IV,\nrespectively. Ad-"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "The learning rate is set\nto 1e-3, with a training epoch size of",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "ditionally, we\nemployed\nfour\ndifferent\nvalidation\nprotocols."
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "300 and a batch size of 256. To avoid overfitting, L2 regular-",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "(1) Cross-corpus Cross-subjects\nSingle-session\nhold-out-"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "ization with a weight of 1 × 10−5\nis employed during model",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "valuation.\nIn single-database EEG emotion recognition tasks,"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "training. All models are executed under the following config-",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "utilizing\nthe first\nsession\nfor model\nevaluation\nis\nthe most"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "uration: NVIDIA GeForce RTX 3090, CUDA=11.6, PyTorch",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "widely adopted validation protocol\n[75–77]. The first session"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "=1.12.1.\nSignificantly,\nthe label\ninformation of\nthe target do-",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "of all subjects from one database was designated as the source"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "main is completely missing in the training process, consistent",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "domain data, while the first\nsession of all\nsubjects\nfrom an-"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "with previously employed EEG emotion recognition models",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "(2) Cross-\nother database was used as the target domain data."
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "utilizing transfer learning [72–74].",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "corpus Cross-subjects Cross-session hold-out-valuation. all"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "sessions from all subjects within one database as source domain"
        },
        {
          "↑ 1.05\n↑ 4.34\n↑ 0.88": "",
          "↑ 5.31\n↑ 5.65\n↑ 3.66\n↑ 3.97": "data and the another database as target domain data.\nsince the"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": ""
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "indicated by ’*’."
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "Methods"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "DANN*[59]"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "LeNet*[61]"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "LmmdPL"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "DCORAL*[62]"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "DDA*[63]"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "PrPL*[36]"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "CddPL"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "BLFBA*[64]"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": "McdPL"
        },
        {
          "out Cross-valuation, the results are shows as (Accuracy% ± Standard-Deviation%). The database training combination is represented as (Source Domain → Target": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "cross-corpus cross-subject cross-session,\nthis evaluation proto-",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "sample data. This process will continue until each subject\nin"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "col presents significant challenges in EEG emotion recognition",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "the target domain database has been used as the independent"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "(3) Cross-corpus Cross-subjects Single-session leave-\ntasks.",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "validation set. We will perform 15-fold (SEED, SEED-IV) or"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "one-subject-out Cross-valuation. To more rigorously validate",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "16-fold (SEED-V) cross-validation, and the results will be aver-"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "the robustness and generalization performance of our model",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "aged. This evaluation protocol imposes more stringent require-"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "across databases, we will use an independent validation set for",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "ments on the model."
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "evaluation.\nSpecifically,\nin one database,\nthe first\nsession of",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": ""
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "all subjects will be used as source domain sample data.\nIn an-",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": ""
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "4.4. Cross-corpus\nCross-subjects\nSingle-session\nhold-out-"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "other database,\nthe first session of one subject will be treated",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": ""
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "valuation"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "as the independent validation set, which will not participate in",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": ""
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "training or testing, and will be used to validate the overall per-",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "The training results are shown in Tab.2, all results are repre-"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "formance of the model after training, while the first session of",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "sented as (Accuracy% ± Standard-Deviation%), State-of-The-"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "the remaining subjects will be used as target domain sample",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "Art\n(SOTA) performances are shown in black bold,\nand the"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "data. This process will continue until\nthe first session of each",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "second-best performance\nare underlined.\nThe\nresults\nshow"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "subject in the target domain database has been used as the inde-",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "that\nthe LmmdPL, CddPL and McdPL Methods are superior"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "pendent validation set. We will perform 15-fold (SEED,SEED-",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "to the baseline models. Notably,\nthe McdPL achieves SOTA"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "IV) or 16-fold (SEED-V) cross-validation, and the results will",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "performance\nacross\nall\nsix\ncombinations\nof\ntraining\ncross-"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "(4) Cross-corpus Cross-subjects Cross-session\nbe averaged.",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "corpus. CddPL achieves the second-best accuracy in three out"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "leave-one-subject-out Cross-valuation.\nSpecifically, all ses-",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "of six training cross-corpus combinations (SEED→SEED-IV,"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "sions of all subjects from one database are treated as source",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "SEED-IV→SEED-V, SEED-V→SEED-IV), while LmmdPL"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "domain sample data, while all sessions of one subject from an-",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "alse gets\nthree (SEED→SEED-V, SEED-IV→SEED, SEED-"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "other database serve as the independent validation set, and the",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "V→SEED).\nIt\nis worth noting that\nthe\naverage\naccuracy of"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "sessions of\nthe remaining subjects are used as target domain",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "LmmdPL, CddPL and McdPL models is 53.18%, 54.17% and"
        },
        {
          "↑ 1.62\n↑ 2.34\n↑ 0.03": "",
          "↑ 2.10\n↑ 0.20\n↑ 4.92\n↑ 0.13": "58.93%,\nrespectively. Compared with the second-best model"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "Figure 5: The confusion matrices for PrPL, LmmdPL, CddPL and McdPL during cross-corpus training. In matrices (a)∼(d), SEED is utilized as the source domain",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "and SEED-V as the target domain, denoted as SEED→SEED-V. Conversely, matrices (e)∼(h) use SEED-V as the source domain and SEED as the target domain,",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "represented as SEED-V→SEED. In each confusion matrix view, the horizontal axis represents the predicted labels, while the vertical axis represents the true labels.",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "(CddPL),\nthe\naverage\naccuracy\nof McdPL is\nimproved\nby",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "generalization capabilities of the McdPL architecture compared"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "4.76%. These results demonstrate that McdPL achieve superior",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "to alternative approaches."
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "fine-grained feature alignment and EEG emotion recognition",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "performance.",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "4.7. Cross-corpus\nCross-subjects\nCross-session\nleave-one-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "subject-out Cross-valuation"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "4.5. Cross-corpus\nCross-subjects\nCross-session\nhold-out-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "The performance results of\nthe model are shown in Tab.5."
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "valuation",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "The McdPL framework demonstrates superior accuracy in the"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "This validation protocol presents\nsignificant\nchallenges\nin",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "SEED→SEED-V, SEED-IV→SEED-V and SEED-V→SEED"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "EEG emotion\nrecognition\ntasks.\nThe\nexperimental\nresults",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "cross-corpus evaluations combinations, and achieve the SOTA"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "presented in Tab.3 demonstrate that\nthe McdPL model con-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "performance in three of\nthe six cross-corpus\ntraining combi-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "sistently achieves SOTA performance\nacross\nall\nsix training",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "nations,\naccuracy are 54.50%, 55.72% and 49.80%,\nrespec-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "cross-corpus combinations.\nNotably,\nthe cross-corpus evalu-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "tively. The CddPL model achieved the superior accuracy in the"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "ation from SEED-V → SEED yields the highest accuracy of",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "SEED→SEED-IV and SEED-IV→SEED cross-corpus combi-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "68.53%.\nAmong the comparative methods, CddPL achieves",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "nations, with an accuracy of 52.89% and 56.55%, respectively."
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "second-best performance in three cross-corpus training combi-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "Meanwhile, the LmmdPL model achieved the superior accuracy"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "nations, while LmmdPL attains this position in one configura-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "in the SEED-V→SEED-IV combination, with an accuracy of"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "tion. The average accuracy analysis reveals comparable perfor-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "53.05%. Overall, the McdPL model continues to show the most"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "mance levels between PrPL (54.01%), LmmdPL (54.28%), and",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "potential, with an average accuracy of 51.73% percent across"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "CddPL (54.93%) methods.\nIt\nis worth noting that\nthe McdPL",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "all cross-corpus combinations, slightly outperforming the other"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "framework achieves a mean accuracy of 58.90%, representing",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "models."
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "a 3.97% improvement over the second-best performing method",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "(CddPL). These results demonstrate the superior generalization",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "5. Discussion and Conclusion"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "capabilities and potential of the proposed McdPL architecture.",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "5.1. Confusion Matrix"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "4.6. Cross-corpus\nCross-subjects\nSingle-session\nleave-one-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "As\nshown\nin\nFig.5, we\npresent\nthe\nconfusion matrices"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "subject-out Cross-valuation",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": ""
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "for\nthe models\n(PrPL, LmmdPL, CddPL and McdPL)\nin"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "Since the independent validation set excluded from model",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "SEED→SEED-V and SEED-V→SEED. In Fig.5.(a)∼(d), It\nis"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "training,\nthis experimental protocols eliminates potential bias",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "shown that\nthe McdPL model achieved the best performance"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "contamination while presenting enhanced challenges for model",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "in all models, with accuracy of 55.33%, 52.25% and 72.53%"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "generalization. As shown in Tab.4 (with SOTA performance",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "for correctly identifying negative, neutral and positive emotion"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "highlighted in black bold),\nthe LmmdPL model achieves opti-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "samples,\nrespectively. Additionally,\nin the negative and posi-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "mal performance on the SEED-IV→SEED cross-corpus evalu-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "tive emotion categories, the CddPL model achieved the second-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "ation combination, attaining an accuracy of 57.09%. Notably,",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "best\nrecognition performance, with accuracies of 52.42% and"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "the McdPL framework achieve SOTA performances across all",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "56.52%. The PrPL model demonstrated the second-SOTA per-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "remaining cross-corpus validation combinations,\nachieving a",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "formance in the neutral emotion category, with an accuracy of"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "mean accuracy of 56.3% that demonstrates a 5.13% improve-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "51.63%.\nIn Fig.5.(e)∼(h),\nIt\nis shown that\nthe McdPL model"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "ment over secondary methods (CddPL). Collectively,\nthese re-",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "achieved accuracy of 66.72% and 79.91% for correctly identi-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LMMD_PRPL": "sults demonstrate the superior performance and cross-corpus",
          "(g)SEED-V SEED, CDD_PRPL\n(h)SEED-V SEED,MCD_PRPL": "fying neutral and positive emotion samples,\nrespectively. The"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "Source positive\nSource neutral\nSource negative"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "Target positive\nTarget neutral\nTarget negative"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "Figure\n6:\nDisplays\nthe T-SNE visualization\nof\nthe\nsample\nfeatures\nlearned\nby\nthe model\n(PrPL,LmmdPL,CddPL,McdPL)\nin\nboth\nthe\nsource\nand\ntarget"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "domains.(a)∼(d) utilize SEED as the source domain and SEED-V as the target domain,\nrepresented as SEED→SEED-V. Conversely,\n(e)∼(h) utilize SEED-V"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "as the source domain and SEED as the target domain, denoted as SEED-V→SEED. Different colors in the visualization correspond to different categories."
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "LmmdPL model recorded an accuracy of 68.11% for negative\nment capability and excellent classification performance, giv-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "emotion samples, while the McdPL model achieved 66.34%,\ning it a significant advantage in emotion recognition tasks."
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "indicating the latter’s second-best performance. Overall, The"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "McdPL model still shows the SOTA performance of emotion"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "5.3. Effect of Noisy Labels"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "recognition, while the LmmdPL and CddPL models\nslightly"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "We adopt a pairwise learning strategy to effectively overcome"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "outperform the PrPL model."
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "the problem of\nlabel noise.\nIn order\nto verify the robustness"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "of the model under the pairwise learning and pointwise learn-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "5.2. Visualization of Learned Representation"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "ing strategies and its dependence on the sample labels, we train"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "the model using the source domain samples with added noise.\nTo provide a more intuitive comparison of\nthe feature rep-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "resentation effects of different models, we used T-distributed\nSpecifically, we introduce noise into the real label of the source"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "domain in a controlled manner and evaluate the model using\nStochastic Neighbor Embedding (T-SNE [78])\nalgorithm to"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "a sample of\nthe target domain. We generate a large number\nvisualize\nthe\nfeature\nsamples\nand interactive\nfeatures\nin the"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "of random labels and replace 10%, 20%, 30% and 40% of the\nsource\nand\ntarget\ndomains\nof\nthe PrPL, LmmdPL, CddPL"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "source domain sample labels with these random labels.\nIt\nis\nand McdPL models.\nAs\nshown in Fig.6.\nThis visualization"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "demonstrate each model’s ability to learn discriminative fea-\nworth noting that during the training process,\nthe label\ninfor-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "mation of the target domain is completely lost.\ntures and achieve class differentiation.\nSpecifically, we ran-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "We adopt\nthe McdPL model as the baseline model and the\ndomly selected 256 samples each from the source and target"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "results are shown in Tab.6.\nThe introduction of noise in the\ndomains to visualize the learned feature representations, respec-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "source domain labels in the pairwise learning strategy results\ntively.\nIn cross-corpus training combinations SEED→SEED-V"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "in a slight degradation of\nthe model performance. When the\nand SEED-V→SEED, which are significant performance dif-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "ferences across models.\nIn the feature distribution of the PrPL\nnoise level\nis 10%, 20%, 30% and 40%,\nthe average accuracy"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "is 57.04%, 55.52%, 54.52% and 54.22%,\nrespectively, which\nmodel\n(Fig.6.(a)(e)),\nthe distribution of each category is\nthe"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "is only reduced by 2.82%. However,\nthe introduction of noise\nmost dispersed, with blurred boundaries between classes,\nindi-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "in the source domain labels in the pointwise learning strategy\ncating the feature differentiation of samples is weak.\nIn con-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "leads\nto a significant degradation in the model performance.\ntrast,\nthe\nfeature distributions\nin the LmmdPL (Fig.6.(b)(f))"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "When the noise level\nis 10%, 20%, 30% and 40%,\nthe aver-\nand CddPL (Fig.6.(c)(g)) models\nshow increasing Clustering"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "ability, with relatively clear boundaries forming between cate-\nage accuracy is 50.22%, 48.11%, 46.01% and 44.57%, respec-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "tively, which is reduced by 5.65%.\nIt is worth noting that com-\ngories, reflecting stronger feature alignment potential. Signifi-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "pared with the pairwise learning strategy,\nthe performance of\ncantly, the feature distribution obtained with the McdPL model"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "the model is affected more by the increase of the proportion of\n(Fig.6.(d)(h)) is the most compact, with the clearest boundaries"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "label noise in the pointwise learning strategy. Without adding\nbetween categories. This dense and distinct distribution demon-"
        },
        {
          "(e)SEED-V SEED, PRPL\n(f)SEED-V SEED,LmmdPL\n(g)SEED-V SEED, CddPL\n(h)SEED-V SEED,McdPL": "label noise (0%),\nthe average accuracy is 58.90% and 53.74%,\nstrate the McdPL model’s strong cross-domain feature align-"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "expressed as (Accuracy%±Standard-Deviation%). ↓ % denotes the difference between the performances."
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": ""
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "SEED→SEED-IV"
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": ""
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "SEED→SEED-V"
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": ""
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "SEED-IV→SEED"
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": ""
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "SEED-IV→SEED-V"
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": ""
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "SEED-V→SEED"
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": ""
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "SEED-V→SEED-IV"
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": ""
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "Mean Acc"
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": ""
        },
        {
          "Table 6: The results of the McdPL model adding different proportions of noisy labels to the source domain in the pairwise learning and pointwise learning strategies,": "Difference in Mean Acc"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "with a difference of 5.16%. However, when the proportion of",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "to 56.28%). Experiments show that the introduction of domain"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "label noise reaches 40%,\nthe average accuracy is 54.22% and",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "adversarial\ntraining can greatly enhance the emotion recogni-"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "44.57%, with a difference of 9.65%.",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "tion performance on the target domain. When the model\nre-"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "These results show that increasing the proportion of noise in",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "moves the discriminator loss function,\nthe performance of all"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "the source domain sample has a limited effect on model perfor-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "three models decreases. The recognition accuracy of LmmdPL,"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "mance. Overall, our proposed McdPL model based on pairwise",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "CddPL and McdPL is 53.21%, 51.22% and 59.74%, and the"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "learning has excellent robustness and reliability, and has a high",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "performance is decreased by 5.06%, 5.06% and 9.01%, respec-"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "tolerance for noise labels.",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "tively.\nThis significant decrease indicates\nthe significant\nim-"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "pact of individual differences issues on model performance and"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "5.4. Ablation Experiment",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "highlights\nthe great potential of\ntransfer\nlearning in affective"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "We\nsystematically\nexplored\nthe\neffectiveness\nof\ndifferent",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "brain-computer interface (aBCI) applications."
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "components in the proposed model through ablation experiment",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "In the MCdPL model, we adopted two classifiers based on"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "and examined the corresponding contributions\nto the overall",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "pairwise\nlearning and used three\ntraining steps\nto train the"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "performance, and the results are shown in Tab.7. Pairwise learn-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "model. When we\nremoved training step.2 (Maximize\nclas-"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "ing strategies have a better\ntolerance for\nthe inevitable label",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "sification Discrepancy),\nthe model performance decreased by"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "noise in emotional EEG data. Therefore, when we removed the",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "8.94% (from 68.75% to 59.81%). When we removed train-"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "pairwise learning strategy from the target domain, the accuracy",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "ing step.3 (Minimize distribution difference), the model perfor-"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "of\nthe proposed LmmdPL, CddPL and McdPL were 54.18%,",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "mance decreased by 14.7% (from 68.75% to 54.02%).\nThis"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "54.19% and 65.11%,\nrespectively,\nand the performances de-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "further demonstrates the contribution of the three training steps"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "creased by 4.09%, 2.09% and 3.64%,\nrespectively. Addition-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "to the processing ability of samples located near\nthe decision"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "ally, when we remove the pairwise learning strategy from both",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "boundary.\nIn addition, we remove step.2 and step.3 in McdPL,"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "the source and target domain, the performance of the three pro-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "and use the traditional method to train the model. The results"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "posed models is significantly decreased by 13.7%, 9.92%, and",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "shown that the accuracy is 58.38%, and the model performance"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "10.3%, respectively. These results show that the pairwise learn-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "is significantly decreased by 10.4%."
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "ing strategy effectively improves the performance and effective-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "Furthermore, we tested the value of the hyperparameter γ in"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "ness of the model. When the prototype representation module",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "the loss function (Eq.8) of\nthe LmmdPL model.\nThe results"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "is removed,\nthe performance of CddPL and McdPL is signifi-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "show that when the value of γ is 0.5,\nthe model demonstrated"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "cantly decreased by 6.33% and 10.1%,\nrespectively.\nThis in-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "the optimal performance, with the accuracy reaching 58.27%."
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "dicates\nthat\nthe extraction of prototype representation makes",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "Similarly, we tested the value of γ in the loss function (Eq.10)"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "a significant contribution to the model.\nIn addition,\nthe in-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "of the CddPL model. The results show that when the value of γ"
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "troduction of the bilinear transformation matrix θ proposed in",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": "is 1, the model exhibits the optimal performance."
        },
        {
          "Difference in Mean Acc\n↓ 5.16\n↓ 6.82": "Sec.3.1.3 can improve the model performance, and the recog-",
          "↓ 7.41\n↓ 8.51\n↓ 9.65": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "Strategy"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "w/o pairwise learning on the target"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "w/o pairwise learning on the source and target"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "w/o prototypical representation"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "w/o the bilinear transformation matrix θ in Sec.3.1.3"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "w/o feature discriminator"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "A single pairwise learning classifier"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "w/o Maximize Classifiers Discrepancy(step.2 in McdPL)"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "w/o Minimize Features Distribution(step.3 in McdPL)"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "w/o step.2 and step.3 in McdPL"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "γ value in LmmdPL (Eq.8) and CddPL (Eq.10)"
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": ""
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": ""
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": ""
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": ""
        },
        {
          "Table 7: The ablation experiment of our proposed model, expressed as (Accuracy%±Standard-Deviation%). ↓ % is denoted as the gap from the best performance.": "Ours"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "domain adaptation approach, we proposed the LmmdPL and": "CddPL Methods to achieve finer-grained alignment of samples"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "in the feature space. For rule-domain adaptation, we proposed"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "the McdPL framework, which designs two distinct classifiers"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "and incorporates three specialized training steps to explore a"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "more suitable feature space for aligning sample features. The"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "models were comprehensively evaluated on the SEED, SEED-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "IV and SEED-V databases.\nIn addition,\nthe stability and gen-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "eralization of\nthe model under various experimental\nsettings"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "were thoroughly validated. The experimental results show that"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "Lmmd, CddPL and McdPL models have achieved excellent per-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "formance. Among them,\nthe McdPL model achieves the opti-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "mal performance and is superior\nin dealing with the individ-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "ual differences and noisy labeling problems in aBCI systems,"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "which provides a promising solution for cross-corpus emotion"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "recognition.\nIn future studies,\nin view of the problems of indi-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "vidual differences and device differences in EEG, we will con-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "tinue to explore solutions with better performance and stronger"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "generalization ability."
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "Acknowledgements"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "This work was supported in part by the National Natural Sci-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "ence Foundation of China under Grant 62176089, 62276169"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "and 62201356,\nin part by the Natural Science Foundation of"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "Hunan Province under Grant 2023JJ20024,\nin part by the Key"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "Research\nand Development Project\nof Hunan Province\nun-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "der Grant 2025QK3008,\nin part by the Key Project of Xi-"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "angjiang Laboratory\nunder Granted\n23XJ02006,\nin\npart\nby"
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": ""
        },
        {
          "domain adaptation approach, we proposed the LmmdPL and": "the STI 2030-Major Projects 2021ZD0200500,\nin part by the"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours": "5.5. Conclusion",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "Medical-Engineering Interdisciplinary Research Foundation of"
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "Shenzhen University under Grant 2024YG008,\nin part by the"
        },
        {
          "Ours": "This work was proposed three cross-corpus transfer learning",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "Shenzhen University-Lingnan University Joint Research Pro-"
        },
        {
          "Ours": "Methods based on pairwise learning to enhance feature align-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "gramme,\nand\nin\npart\nby Shenzhen-Hong Kong\nInstitute\nof"
        },
        {
          "Ours": "ment between source and target domain samples,\nthereby im-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "Brain\nScience-Shenzhen\nFundamental Research\nInstitutions"
        },
        {
          "Ours": "proving cross-corpus emotion recognition performance.\nIn the",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "(2023SHIBS0003)."
        },
        {
          "Ours": "domain adaptation approach, we proposed the LmmdPL and",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "CddPL Methods to achieve finer-grained alignment of samples",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "in the feature space. For rule-domain adaptation, we proposed",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "References"
        },
        {
          "Ours": "the McdPL framework, which designs two distinct classifiers",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "and incorporates three specialized training steps to explore a",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[1] D. Schacter, D. Gilbert, D. Wegner, Psychology (2nd ed.), Annual Review"
        },
        {
          "Ours": "more suitable feature space for aligning sample features. The",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "of Neuroscience, 2011."
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[2]\nS. PARADISO, Affective neuroscience: The foundations of human and"
        },
        {
          "Ours": "models were comprehensively evaluated on the SEED, SEED-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "animal emotions, American Journal of Psychiatry 159 (10) (2002) 1805–"
        },
        {
          "Ours": "IV and SEED-V databases.\nIn addition,\nthe stability and gen-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "1805. doi:10.1176/appi.ajp.159.10.1805."
        },
        {
          "Ours": "eralization of\nthe model under various experimental\nsettings",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[3]\nS. Rayatdoost, M. Soleymani, Cross-corpus eeg-based emotion recogni-"
        },
        {
          "Ours": "were thoroughly validated. The experimental results show that",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "tion,\nin: 2018 IEEE 28th International Workshop on Machine Learning"
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "for Signal Processing (MLSP), 2018, pp. 1–6.\ndoi:10.1109/MLSP."
        },
        {
          "Ours": "Lmmd, CddPL and McdPL models have achieved excellent per-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "2018.8517037."
        },
        {
          "Ours": "formance. Among them,\nthe McdPL model achieves the opti-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[4]\nL. Kessous, G. Castellano, G. Caridakis, Multimodal emotion recogni-"
        },
        {
          "Ours": "mal performance and is superior\nin dealing with the individ-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "tion in speech-based interaction using facial expression, body gesture and"
        },
        {
          "Ours": "ual differences and noisy labeling problems in aBCI systems,",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "acoustic analysis, Journal on Multimodal User Interfaces 3 (1–2) (2010)"
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "33–48. doi:10.1007/s12193-009-0025-5."
        },
        {
          "Ours": "which provides a promising solution for cross-corpus emotion",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[5] W. Ye, Z. Zhang, F. Teng, M. Zhang,\nJ. Wang, D. Ni, F. Li, P. Xu,"
        },
        {
          "Ours": "recognition.\nIn future studies,\nin view of the problems of indi-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "Z. Liang, Semi-supervised dual-stream self-attentive adversarial graph"
        },
        {
          "Ours": "vidual differences and device differences in EEG, we will con-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "contrastive\nlearning\nfor\ncross-subject\neeg-based\nemotion\nrecognition,"
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "IEEE Transactions on Affective Computing 16 (1) (2025) 290–305. doi:"
        },
        {
          "Ours": "tinue to explore solutions with better performance and stronger",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "10.1109/TAFFC.2024.3433470."
        },
        {
          "Ours": "generalization ability.",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[6] W. Ye,\nJ. Wang, L. Chen, L. Dai, Z. Sun, Z. Liang, Adaptive\nspa-"
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "tial–temporal\naware\ngraph\nlearning\nfor\neeg-based\nemotion\nrecogni-"
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "tion, Cyborg and Bionic Systems 6 (2025) 0088.\ndoi:10.34133/"
        },
        {
          "Ours": "Acknowledgements",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "cbsystems.0088."
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[7]\nSiddharth, T.-P. Jung, T. J. Sejnowski, Utilizing deep learning towards"
        },
        {
          "Ours": "This work was supported in part by the National Natural Sci-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "multi-modal\nbio-sensing\nand\nvision-based\naffective\ncomputing,\nIEEE"
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "Transactions on Affective Computing 13 (01)\n(2022) 96–107.\ndoi:"
        },
        {
          "Ours": "ence Foundation of China under Grant 62176089, 62276169",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "10.1109/TAFFC.2019.2916015."
        },
        {
          "Ours": "and 62201356,\nin part by the Natural Science Foundation of",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[8] V. Jayaram, M. Alamgir, Y. Altun, B. Scholkopf, M. Grosse-Wentrup,"
        },
        {
          "Ours": "Hunan Province under Grant 2023JJ20024,\nin part by the Key",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "Transfer learning in brain-computer interfaces,\nIEEE Computational In-"
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "telligence Magazine 11 (1)\n(2016) 20–31.\ndoi:10.1109/MCI.2015."
        },
        {
          "Ours": "Research\nand Development Project\nof Hunan Province\nun-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "2501545."
        },
        {
          "Ours": "der Grant 2025QK3008,\nin part by the Key Project of Xi-",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "[9]\nJ. Li, S. Qiu, Y.-Y. Shen, C.-L. Liu, H. He, Multisource transfer learning"
        },
        {
          "Ours": "angjiang Laboratory\nunder Granted\n23XJ02006,\nin\npart\nby",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": ""
        },
        {
          "Ours": "",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "for cross-subject eeg emotion recognition, IEEE Transactions on Cyber-"
        },
        {
          "Ours": "the STI 2030-Major Projects 2021ZD0200500,\nin part by the",
          "58.27 ± 4.57\n56.28 ± 2.92\n68.75 ± 9.35": "netics 50 (7) (2020) 3281–3293. doi:10.1109/TCYB.2019.2904052."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "emotion recognition using an end-to-end regional-asymmetric convolu-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[27] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "tional neural network, Knowledge-Based Systems 205 (2020) 106243.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "lette, M. Marchand, V. Lempitsky, Domain-adversarial\ntraining of neu-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "doi:10.1016/j.knosys.2020.106243.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "ral networks,\nJ. Mach. Learn. Res. 17 (1)\n(2016) 2096–2030.\ndoi:"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[11]\nP. Zhong, D. Wang, C. Miao, Eeg-based emotion recognition using regu-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "10.5555/2946645.2946704."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "larized graph neural networks, IEEE Transactions on Affective Comput-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[28]\nZ. Lian, J. Tao, B. Liu, J. Huang, Domain adversarial learning for emotion"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "ing (2022) 1290–1301doi:10.1109/taffc.2020.2994159.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "recognition (2019). arXiv:1910.13807."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[12] X. Gu, Z. Cao, A.\nJolfaei, P. Xu, D. Wu, T.-P.\nJung, C.-T. Lin, Eeg-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "URL https://arxiv.org/abs/1910.13807"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "based brain-computer\ninterfaces\n(bcis): A survey of\nrecent\nstudies on",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[29] D. Huang, S. Chen, C. Liu, L. Zheng, Z. Tian, D.\nJiang, Differences"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "signal\nsensing technologies and computational\nintelligence approaches",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "first\nin asymmetric brain: A bi-hemisphere discrepancy convolutional"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "and their applications, IEEE/ACM Transactions on Computational Biol-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "neural network for\neeg emotion recognition, Neurocomputing (2021)"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "ogy and Bioinformatics 18 (5) (2021) 1645–1666. doi:10.1109/TCBB.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "140–151doi:10.1016/j.neucom.2021.03.105."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "2021.3052811.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[30] Y. Zhu, F. Zhuang,\nJ. Wang,\nJ. Chen, Z. Shi, W. Wu, Q. He, Multi-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[13] O. Ozdenizci, Y. Wang, T. Koike-Akino, D. Erdogmus, Adversarial",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "representation adaptation network for cross-domain image classification,"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "deep learning in eeg biometrics,\nIEEE Signal Processing Letters (2019)",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "Neural Networks (2019) 214–221doi:10.1016/j.neunet.2019.07."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "710–714doi:10.1109/lsp.2019.2906826.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "010."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[14] O. Ozdenizci, Y. Wang, T. Koike-Akino, D. Erdogmus, Learning invariant",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[31]\nJ. Gideon, M. G. McInnis,\nE. M. Provost,\nImproving\ncross-corpus"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "representations from eeg via adversarial\ninference,\nIEEE Access (2020)",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "speech emotion recognition with adversarial discriminative domain gen-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "27074–27085doi:10.1109/access.2020.2971600.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "eralization (addog),\nIEEE Transactions on Affective Computing (2021)"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[15] D. Bethge, P. Hallgarten, T. A. Große-Puppendahl, M. Kari, R. Mikut,",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "1055–1068doi:10.1109/taffc.2019.2916092."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "¨\nA. Schmidt, O.\nOzdenizci, Domain-invariant representation learning from",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[32]\nE. Tzeng,\nJ. Hoffman, K. Saenko, T. Darrell, Adversarial discrimina-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "eeg with private encoders, ICASSP 2022 - 2022 IEEE International Con-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "tive domain adaptation,\nin:\n2017 IEEE Conference on Computer Vi-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "ference on Acoustics, Speech and Signal Processing (ICASSP)\n(2022)",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "sion and Pattern Recognition (CVPR), 2017,\npp. 2962–2971.\ndoi:"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "1236–1240.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "10.1109/CVPR.2017.316."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "URL https://api.semanticscholar.org/CorpusID:246294547",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[33] Y. Luo, L. Zheng, T. Guan, J. Yu, Y. Yang, Taking a closer look at do-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[16] D. Zhang, L. Yao, X. Zhang, S. Wang, W. Chen, R. Boots, Cascade and",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "main shift: Category-level adversaries for semantics consistent domain"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "parallel convolutional\nrecurrent neural networks on eeg-based intention",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "adaptation, in: 2019 IEEE/CVF Conference on Computer Vision and Pat-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "recognition for brain computer interface,\nin: 32nd AAAI Conference on",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "tern Recognition (CVPR), 2019, pp. 2502–2511. doi:10.1109/CVPR."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "Artificial Intelligence, AAAI 2018, AAAI Press, United States, 2018, pp.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "2019.00261."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "1703–1710. doi:10.1609/aaai.v32i1.11496.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[34]\nT. Gokhale, R. Anirudh, B. Kailkhura,\nJ.\nJ. Thiagarajan, C. Baral,"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[17] Y. Li, W. Zheng, L. Wang, Y. Zong, Z. Cui, From regional\nto global",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "Y\n. Yang, Attribute-guided adversarial\ntraining for\nrobustness to natural"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "brain: A novel hierarchical spatial-temporal neural network model for eeg",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "perturbations, Proceedings of the AAAI Conference on Artificial Intelli-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "emotion recognition, IEEE Transactions on Affective Computing (2022)",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "gence (2022) 7574–7582doi:10.1609/aaai.v35i9.16927."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "568–578doi:10.1109/taffc.2019.2922912.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[35] C.-Y. Lee, T. Batra, M. H. Baig, D. Ulbricht, Sliced wasserstein discrep-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[18]\nL. Feng, C. Cheng, M. Zhao, H. Deng, Y. Zhang, Eeg-based emotion",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "ancy for unsupervised domain adaptation,\nin:\n2019 IEEE/CVF Confer-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "recognition using spatial-temporal graph convolutional\nlstm with atten-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "ence on Computer Vision and Pattern Recognition (CVPR), 2019, pp."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "tion mechanism,\nIEEE Journal of Biomedical and Health Informatics",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "10277–10287. doi:10.1109/CVPR.2019.01053."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "(2022) 5406–5417doi:10.1109/jbhi.2022.3198688.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[36] R. Zhou, Z. Zhang, H. Fu, L. Zhang, L. Li, G. Huang, F. Li, X. Yang,"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[19] Y. Yang, Z. Wang, W. Tao, X. Liu, Z. Jia, B. Wang, F. Wan, Spectral-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "Y\n. Dong, Y.-T. Zhang, Z. Liang, Pr-pl: A novel prototypical representa-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "spatial attention alignment\nfor multi-source domain adaptation in eeg-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "tion based pairwise learning framework for emotion recognition using eeg"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "based emotion recognition,\nIEEE Transactions on Affective Computing",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "signals,\nIEEE Transactions on Affective Computing 15 (2) (2024) 657–"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "15 (4) (2024) 2012–2024. doi:10.1109/TAFFC.2024.3394436.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "670. doi:10.1109/TAFFC.2023.3288118."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[20] H. Yan, K. Guo, X. Xing, X. Xu, Bridge graph attention based graph",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[37]\nS. Jerritta, M. Murugappan, R. Nagarajan, K. Wan, Physiological signals"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "convolution network with multi-scale transformer for eeg emotion recog-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "based human emotion recognition: a review,\nin: 2011 IEEE 7th Interna-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "nition,\nIEEE Transactions on Affective Computing 15 (4) (2024) 2042–",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "tional Colloquium on Signal Processing and its Applications, 2011, pp."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "2054. doi:10.1109/TAFFC.2024.3394873.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "410–415. doi:10.1109/CSPA.2011.5759912."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[21] Y. Zhou, F. Li, Y. Li, Y. Ji, L. Zhang, Y. Chen, W. Zheng, G. Shi, Eeg-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[38] K. Vora, S. Shah, H. Harsoda, J. Sheth, S. Agarwal, A. Thakkar, S. H."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "based emotion style transfer network for cross-dataset emotion recogni-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "Mankad, Emotion recognition from sensory and bio-signals: A survey, in:"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "tion (2023). arXiv:2308.05767.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "Proceedings of the 2nd International Conference on Data Engineering and"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "URL https://arxiv.org/abs/2308.05767",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "Communication Technology:\nICDECT 2017, Springer, 2019, pp. 345–"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[22] Y. Jia, M. Salzmann, T. Darrell, Factorized latent spaces with structured",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "355. doi:10.1007/978-981-13-1610-4_35."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "sparsity,\nin: Proceedings of the 24th International Conference on Neural",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[39] M. Egger, M. Ley, S. Hanke, Emotion recognition from physiological sig-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "Information Processing Systems - Volume 1, NIPS’10, Curran Associates",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "nal analysis: A review, Electronic Notes in Theoretical Computer Science"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "Inc., Red Hook, NY, USA, 2010, p. 982–990.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "343 (2019) 35–55,\nthe proceedings of AmI,\nthe 2018 European Confer-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[23] H. Bao, G. Niu, M. Sugiyama, Classification from pairwise similarity",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "ence on Ambient Intelligence. doi:10.1016/j.entcs.2019.04.009."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "and unlabeled data,\nin:\nJ. Dy, A. Krause (Eds.), Proceedings of the 35th",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[40] W. Li, Z. Zhang, A. Song, Physiological-signal-based emotion recogni-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "International Conference on Machine Learning, Vol. 80 of Proceedings of",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "tion: An odyssey from methodology to philosophy, Measurement 172"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "Machine Learning Research, PMLR, 2018, pp. 452–461.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "(2021) 108747. doi:10.1016/j.measurement.2020.108747."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "URL https://proceedings.mlr.press/v80/bao18a.html",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[41]\nT. Song, S. Liu, W. Zheng, Y. Zong, Z. Cui, Y. Li, X. Zhou, Variational"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[24] H. Bao, T. Shimada, L. Xu, I. Sato, M. Sugiyama, Similarity-based clas-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "instance-adaptive graph for eeg emotion recognition, IEEE Transactions"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "sification: Connecting similarity learning to binary classification, ArXiv",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "on Affective Computing (2023) 343–356doi:10.1109/taffc.2021."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "abs/2006.06207 (2020).",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "3064940."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "URL https://api.semanticscholar.org/CorpusID:219573392",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[42]\nJ. Ma, H. Tang, W.-L. Zheng, B.-L. Lu, Emotion recognition using mul-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[25] C.-C. Hsu, Y.-X. Zhuang, C.-Y. Lee, Deep fake image detection based",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "timodal\nresidual\nlstm network,\nin: Proceedings of\nthe 27th ACM Inter-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "on pairwise learning, Applied Sciences 10 (1)\n(2020).\ndoi:10.3390/",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "national Conference on Multimedia, MM ’19, Association for Comput-"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "app10010370.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "ing Machinery, New York, USA, 2019, p. 176–183.\ndoi:10.1145/"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "[26] H. Zhao, S. Zhang, G. Wu, J. a. P. Costeira, J. M. F. Moura, G. J. Gor-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "3343031.3350871."
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "don, Adversarial multiple source domain adaptation,\nin: Proceedings of",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "[43] W. Liu,\nJ.-L. Qiu, W.-L. Zheng, B.-L. Lu, Comparing\nrecognition"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "the 32nd International Conference on Neural Information Processing Sys-",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "performance\nand robustness of multimodal deep learning models\nfor"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "tems, NIPS’18, Curran Associates Inc., Red Hook, NY, USA, 2018, p.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "multimodal emotion recognition,\nIEEE Transactions on Cognitive and"
        },
        {
          "[10] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen, Eeg-based": "8568–8579.",
          "URL https://api.semanticscholar.org/CorpusID:55701085": "Developmental\nSystems\n(2022)\n715–729doi:10.1109/tcds.2021."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3071170.": "[44] A. Anuragi, D. Singh Sisodia, R. Bilas Pachori, Eeg-based cross-subject",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[64] B. Hu, R. Liu, G. Liu, A bi-lstm frequency band attention network for"
        },
        {
          "3071170.": "emotion recognition using fourier-bessel series expansion based empirical",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "eeg- based emotion recognition,\nin:\n2024 5th International Seminar on"
        },
        {
          "3071170.": "wavelet transform and nca feature selection method, Information Sciences",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "Artificial Intelligence, Networking and Information Technology (AINIT),"
        },
        {
          "3071170.": "(2022) 508–524doi:10.1016/j.ins.2022.07.121.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "2024, pp. 1708–1711. doi:10.1109/AINIT61980.2024.10581447."
        },
        {
          "3071170.": "[45] W. Tao, C. Li, R. Song,\nJ. Cheng, Y. Liu, F. Wan, X. Chen, Eeg-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[65] W.-L. Zheng, B.-L. Lu, Investigating critical frequency bands and chan-"
        },
        {
          "3071170.": "based emotion recognition via\nchannel-wise\nattention and self\natten-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "nels for eeg-based emotion recognition with deep neural networks, IEEE"
        },
        {
          "3071170.": "tion,\nIEEE Transactions on Affective Computing (2023) 382–393doi:",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "Transactions on Autonomous Mental Development (2015) 162–175doi:"
        },
        {
          "3071170.": "10.1109/taffc.2020.3025777.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "10.1109/tamd.2015.2431497."
        },
        {
          "3071170.": "[46] Y. Yin, X. Zheng, B. Hu, Y. Zhang, X. Cui, Eeg emotion recognition using",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[66]\nT. Song, W. Zheng, P. Song, Z. Cui, Eeg emotion recognition using dy-"
        },
        {
          "3071170.": "fusion model of graph convolutional neural networks and lstm, Applied",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "namical graph convolutional neural networks, IEEE Transactions on Af-"
        },
        {
          "3071170.": "Soft Computing (2021) 106954doi:10.1016/j.asoc.2020.106954.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "fective Computing 11 (3) (2020) 532–541. doi:10.1109/TAFFC.2018."
        },
        {
          "3071170.": "[47] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang, X. Zhou, A bi-hemisphere",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "2817622."
        },
        {
          "3071170.": "domain adversarial neural network model\nfor eeg emotion recognition,",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[67] Q. Liu, Z. Zhou,\nJ. Wang, Z. Liang,\nJoint\ncontrastive\nlearning with"
        },
        {
          "3071170.": "IEEE Transactions on Affective Computing (2021) 494–504doi:10.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "feature alignment\nfor cross-corpus eeg-based emotion recognition,\nin:"
        },
        {
          "3071170.": "1109/taffc.2018.2885474.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "Proceedings of\nthe 1st\nInternational Workshop on Brain-Computer\nIn-"
        },
        {
          "3071170.": "[48] Y. Liu, Y. Ding, C. Li, J. Cheng, R. Song, F. Wan, X. Chen, Multi-channel",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "terfaces\n(BCI)\nfor Multimedia Understanding, BCIMM ’24, Associa-"
        },
        {
          "3071170.": "eeg-based emotion recognition via a multi-level features guided capsule",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "tion for Computing Machinery, New York, NY, USA, 2024, p. 9–17."
        },
        {
          "3071170.": "network, Computers in Biology and Medicine (2020) 103927doi:10.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "doi:10.1145/3688862.3689112."
        },
        {
          "3071170.": "1016/j.compbiomed.2020.103927.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[68]\nT. Zhang, W. Zheng, Z. Cui, Y. Zong, Y. Li, Spatial–temporal recurrent"
        },
        {
          "3071170.": "[49] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang, X. Zhou, A bi-hemisphere",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "neural network for emotion recognition, IEEE Transactions on Cybernet-"
        },
        {
          "3071170.": "domain adversarial neural network model\nfor eeg emotion recognition,",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "ics (2019) 839–847doi:10.1109/tcyb.2017.2788081."
        },
        {
          "3071170.": "IEEE Transactions on Affective Computing (2021) 494–504doi:10.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[69]\nL. Feng, C. Cheng, M. Zhao, H. Deng, Y. Zhang, Eeg-based emotion"
        },
        {
          "3071170.": "1109/taffc.2018.2885474.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "recognition using spatial-temporal graph convolutional\nlstm with atten-"
        },
        {
          "3071170.": "[50] Y. Li, W. Zheng, L. Wang, Y. Zong, Z. Cui, From regional\nto global",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "tion mechanism,\nIEEE Journal of Biomedical and Health Informatics"
        },
        {
          "3071170.": "brain: A novel hierarchical spatial-temporal neural network model for eeg",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "(2022) 5406–5417doi:10.1109/jbhi.2022.3198688."
        },
        {
          "3071170.": "emotion recognition, IEEE Transactions on Affective Computing (2022)",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[70] R.-N. Duan,\nJ.-Y. Zhu, B.-L. Lu, Differential entropy feature for eeg-"
        },
        {
          "3071170.": "568–578doi:10.1109/taffc.2019.2922912.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "based emotion classification,\nin:\n2013 6th International\nIEEE/EMBS"
        },
        {
          "3071170.": "[51] X. Du, C. Ma, G. Zhang, J. Li, Y.-K. Lai, G. Zhao, X. Deng, Y.-J. Liu,",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "Conference on Neural Engineering (NER), 2013,\npp. 81–84.\ndoi:"
        },
        {
          "3071170.": "H. Wang, An efficient\nlstm network for emotion recognition from multi-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "10.1109/NER.2013.6695876."
        },
        {
          "3071170.": "channel eeg signals,\nIEEE Transactions on Affective Computing (2022)",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[71]\nL.-C. Shi, B.-L. Lu, Off-line and on-line vigilance estimation based on"
        },
        {
          "3071170.": "1528–1540doi:10.1109/taffc.2020.3013711.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "linear dynamical system and manifold learning,\nin: 2010 Annual\nInter-"
        },
        {
          "3071170.": "[52]\nP. O. Pinheiro, Unsupervised domain adaptation with similarity learning,",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "national Conference of the IEEE Engineering in Medicine and Biology,"
        },
        {
          "3071170.": "in: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recog-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "2010, pp. 6587–6590. doi:10.1109/IEMBS.2010.5627125."
        },
        {
          "3071170.": "nition, 2018, pp. 8004–8013. doi:10.1109/CVPR.2018.00835.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[72] W. Li, W. Huan, B. Hou, Y. Tian, Z. Zhang, A. Song, Can emotion"
        },
        {
          "3071170.": "[53] Y. Li, W. Zheng, L. Wang, Y. Zong, Z. Cui, From regional\nto global",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "be\ntransferred?—a\nreview on transfer\nlearning for\neeg-based emotion"
        },
        {
          "3071170.": "brain: A novel hierarchical spatial-temporal neural network model for eeg",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "recognition, IEEE Transactions on Cognitive and Developmental Systems"
        },
        {
          "3071170.": "emotion recognition, IEEE Transactions on Affective Computing (2022)",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "(2022) 833–846doi:10.1109/tcds.2021.3098842."
        },
        {
          "3071170.": "568–578doi:10.1109/taffc.2019.2922912.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[73]\nP. Zhong, D. Wang, C. Miao, Eeg-based emotion recognition using regu-"
        },
        {
          "3071170.": "[54] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, A. Smola, A",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "larized graph neural networks, IEEE Transactions on Affective Comput-"
        },
        {
          "3071170.": "kernel\ntwo-sample test, Journal of Machine Learning Research 13 (25)",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "ing (2022) 1290–1301doi:10.1109/taffc.2020.2994159."
        },
        {
          "3071170.": "(2012) 723–773.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[74] D. Wu, Y. Xu, B.-L. Lu, Transfer learning for eeg-based brain-computer"
        },
        {
          "3071170.": "[55] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization (2017).",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "interfaces: A review of progress made since 2016,\nIEEE Transactions"
        },
        {
          "3071170.": "arXiv:1412.6980, doi:10.48550/arXiv.1412.6980.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "on Cognitive and Developmental Systems\n(2022) 4–19doi:10.1109/"
        },
        {
          "3071170.": "[56]\nS. Ruder, An\noverview of\ngradient\ndescent\noptimization\nalgorithms,",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "tcds.2020.3007453."
        },
        {
          "3071170.": "arXiv: Learning,arXiv: Learning (Sep 2016).",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[75] Y. Luo, S.-Y. Zhang, W.-L. Zheng, B.-L. Lu, Wgan domain adaptation for"
        },
        {
          "3071170.": "[57]\nJ. Suykens,\nJ. Vandewalle, Least squares support vector machine clas-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "eeg-based emotion recognition,\nin: L. Cheng, A. C. S. Leung, S. Ozawa"
        },
        {
          "3071170.": "sifiers, Neural\nProcessing Letters\n(1999)\n293–300doi:10.1023/a:",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "(Eds.), Neural Information Processing, Springer International Publishing,"
        },
        {
          "3071170.": "1018628609742.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "Cham, 2018, pp. 275–286. doi:10.1007/978-3-030-04221-9_25."
        },
        {
          "3071170.": "[58]\nJ. MacQueen, Some methods for classification and analysis of multivari-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[76]\nJ. Li, S. Qiu, C. Du, Y. Wang, H. He, Domain adaptation for eeg emo-"
        },
        {
          "3071170.": "ate observations,\nin: Proceedings of\nthe Fifth Berkeley Symposium on",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "tion recognition based on latent\nrepresentation similarity,\nIEEE Trans-"
        },
        {
          "3071170.": "Mathematical Statistics and Probability, Volume 1: Statistics, Vol. 5, Uni-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "actions on Cognitive and Developmental Systems (2020) 344–353doi:"
        },
        {
          "3071170.": "versity of California press, 1967, pp. 281–298.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "10.1109/tcds.2019.2949306."
        },
        {
          "3071170.": "[59]\nE. Tzeng, J. Hoffman, N. Zhang, K. Saenko, T. Darrell, Deep domain con-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[77] W.-L. Zheng, Y.-Q. Zhang, J.-Y. Zhu, B.-L. Lu, Transfer components be-"
        },
        {
          "3071170.": "fusion: Maximizing for domain invariance (2014). arXiv:1412.3474.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "tween subjects for eeg-based emotion recognition, in: 2015 International"
        },
        {
          "3071170.": "URL https://arxiv.org/abs/1412.3474",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "Conference on Affective Computing and Intelligent\nInteraction (ACII),"
        },
        {
          "3071170.": "[60] D. Coomans, D. Massart, Alternative k-nearest neighbour rules in super-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "2015, pp. 917–922. doi:10.1109/ACII.2015.7344684."
        },
        {
          "3071170.": "vised pattern recognition:\nPart 1. k-nearest neighbour classification by",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "[78]\nL. van der Maaten, G. Hinton, Visualizing data using t-sne,\nJournal of"
        },
        {
          "3071170.": "using alternative voting rules, Analytica Chimica Acta 136 (1982) 15–27.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": "Machine Learning Research 9 (86) (2008) 2579–2605."
        },
        {
          "3071170.": "doi:10.1016/S0003-2670(01)95359-0.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "[61] Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning ap-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "plied to document recognition, Proceedings of the IEEE 86 (11) (1998)",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "2278–2324. doi:10.1109/5.726791.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "[62] B. Sun, K. Saenko, Deep CORAL: correlation alignment\nfor deep do-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "main adaptation,\nin: G. Hua, H. J´egou (Eds.), Computer Vision - ECCV",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "2016 Workshops - Amsterdam, The Netherlands, October 8-10 and 15-16,",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "2016, Proceedings, Part III, Vol. 9915 of Lecture Notes in Computer Sci-",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "ence, 2016, pp. 443–450. doi:10.1007/978-3-319-49409-8\\_35.",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "[63]\nZ. Li, E. Zhu, M. Jin, C. Fan, H. He, T. Cai,\nJ. Li, Dynamic domain",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "adaptation for class-aware cross-subject and cross-session eeg emotion",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        },
        {
          "3071170.": "recognition, IEEE Journal of Biomedical and Health Informatics 26 (12)",
          "(2022) 5964–5973. doi:10.1109/JBHI.2022.3210158.": ""
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Annual Review of Neuroscience",
      "authors": [
        "D Schacter",
        "D Gilbert",
        "D Wegner"
      ],
      "year": "2011",
      "venue": "Annual Review of Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Affective neuroscience: The foundations of human and animal emotions",
      "authors": [
        "S Paradiso"
      ],
      "year": "2002",
      "venue": "American Journal of Psychiatry",
      "doi": "10.1176/appi.ajp.159.10.1805"
    },
    {
      "citation_id": "3",
      "title": "Cross-corpus eeg-based emotion recognition",
      "authors": [
        "S Rayatdoost",
        "M Soleymani"
      ],
      "year": "2018",
      "venue": "IEEE 28th International Workshop on Machine Learning for Signal Processing",
      "doi": "10.1109/MLSP.2018.8517037"
    },
    {
      "citation_id": "4",
      "title": "Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis",
      "authors": [
        "L Kessous",
        "G Castellano",
        "G Caridakis"
      ],
      "year": "2010",
      "venue": "Journal on Multimodal User Interfaces",
      "doi": "10.1007/s12193-009-0025-5"
    },
    {
      "citation_id": "5",
      "title": "Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "Z Zhang",
        "F Teng",
        "M Zhang",
        "J Wang",
        "D Ni",
        "F Li",
        "P Xu",
        "Z Liang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3433470"
    },
    {
      "citation_id": "6",
      "title": "Adaptive spatial-temporal aware graph learning for eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "J Wang",
        "L Chen",
        "L Dai",
        "Z Sun",
        "Z Liang"
      ],
      "year": "2025",
      "venue": "Cyborg and Bionic Systems",
      "doi": "10.34133/cbsystems.0088"
    },
    {
      "citation_id": "7",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "T.-P Siddharth",
        "T Jung",
        "Sejnowski"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2019.2916015"
    },
    {
      "citation_id": "8",
      "title": "Transfer learning in brain-computer interfaces",
      "authors": [
        "V Jayaram",
        "M Alamgir",
        "Y Altun",
        "B Scholkopf",
        "M Grosse-Wentrup"
      ],
      "year": "2016",
      "venue": "IEEE Computational Intelligence Magazine",
      "doi": "10.1109/MCI.2015.2501545"
    },
    {
      "citation_id": "9",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cybernetics",
      "doi": "10.1109/TCYB.2019.2904052"
    },
    {
      "citation_id": "10",
      "title": "Eeg-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2020.106243"
    },
    {
      "citation_id": "11",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2020.2994159"
    },
    {
      "citation_id": "12",
      "title": "Eegbased brain-computer interfaces (bcis): A survey of recent studies on signal sensing technologies and computational intelligence approaches and their applications",
      "authors": [
        "X Gu",
        "Z Cao",
        "A Jolfaei",
        "P Xu",
        "D Wu",
        "T.-P Jung",
        "C.-T Lin"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
      "doi": "10.1109/TCBB.2021.3052811"
    },
    {
      "citation_id": "13",
      "title": "Adversarial deep learning in eeg biometrics",
      "authors": [
        "O Ozdenizci",
        "Y Wang",
        "T Koike-Akino",
        "D Erdogmus"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/lsp.2019.2906826"
    },
    {
      "citation_id": "14",
      "title": "Learning invariant representations from eeg via adversarial inference",
      "authors": [
        "O Ozdenizci",
        "Y Wang",
        "T Koike-Akino",
        "D Erdogmus"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/access.2020.2971600"
    },
    {
      "citation_id": "15",
      "title": "Domain-invariant representation learning from eeg with private encoders",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "T Große-Puppendahl",
        "M Kari",
        "R Mikut",
        "A Schmidt",
        "O Özdenizci"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Cascade and parallel convolutional recurrent neural networks on eeg-based intention recognition for brain computer interface",
      "authors": [
        "D Zhang",
        "L Yao",
        "X Zhang",
        "S Wang",
        "W Chen",
        "R Boots"
      ],
      "year": "2018",
      "venue": "32nd AAAI Conference on Artificial Intelligence, AAAI 2018",
      "doi": "10.1609/aaai.v32i1.11496"
    },
    {
      "citation_id": "17",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2019.2922912"
    },
    {
      "citation_id": "18",
      "title": "Eeg-based emotion recognition using spatial-temporal graph convolutional lstm with attention mechanism",
      "authors": [
        "L Feng",
        "C Cheng",
        "M Zhao",
        "H Deng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/jbhi.2022.3198688"
    },
    {
      "citation_id": "19",
      "title": "Spectralspatial attention alignment for multi-source domain adaptation in eegbased emotion recognition",
      "authors": [
        "Y Yang",
        "Z Wang",
        "W Tao",
        "X Liu",
        "Z Jia",
        "B Wang",
        "F Wan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3394436"
    },
    {
      "citation_id": "20",
      "title": "Bridge graph attention based graph convolution network with multi-scale transformer for eeg emotion recognition",
      "authors": [
        "H Yan",
        "K Guo",
        "X Xing",
        "X Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3394873"
    },
    {
      "citation_id": "21",
      "title": "Eegbased emotion style transfer network for cross-dataset emotion recognition",
      "authors": [
        "Y Zhou",
        "F Li",
        "Y Li",
        "Y Ji",
        "L Zhang",
        "Y Chen",
        "W Zheng",
        "G Shi"
      ],
      "year": "2023",
      "venue": "Eegbased emotion style transfer network for cross-dataset emotion recognition",
      "arxiv": "arXiv:2308.05767"
    },
    {
      "citation_id": "22",
      "title": "Factorized latent spaces with structured sparsity",
      "authors": [
        "Y Jia",
        "M Salzmann",
        "T Darrell"
      ],
      "year": "2010",
      "venue": "Proceedings of the 24th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Classification from pairwise similarity and unlabeled data",
      "authors": [
        "H Bao",
        "G Niu",
        "M Sugiyama"
      ],
      "year": "2018",
      "venue": "Proceedings of the 35th International Conference on Machine Learning"
    },
    {
      "citation_id": "24",
      "title": "Similarity-based classification: Connecting similarity learning to binary classification",
      "authors": [
        "H Bao",
        "T Shimada",
        "L Xu",
        "I Sato",
        "M Sugiyama"
      ],
      "year": "2020",
      "venue": "Similarity-based classification: Connecting similarity learning to binary classification"
    },
    {
      "citation_id": "25",
      "title": "Deep fake image detection based on pairwise learning",
      "authors": [
        "C.-C Hsu",
        "Y.-X Zhuang",
        "C.-Y Lee"
      ],
      "year": "2020",
      "venue": "Applied Sciences",
      "doi": "10.3390/app10010370"
    },
    {
      "citation_id": "26",
      "title": "Gordon, Adversarial multiple source domain adaptation",
      "authors": [
        "H Zhao",
        "S Zhang",
        "G Wu",
        "J Costeira",
        "J Moura"
      ],
      "year": "2018",
      "venue": "Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18"
    },
    {
      "citation_id": "27",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "J. Mach. Learn. Res",
      "doi": "10.5555/2946645.2946704"
    },
    {
      "citation_id": "28",
      "title": "Domain adversarial learning for emotion recognition",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Domain adversarial learning for emotion recognition",
      "arxiv": "arXiv:1910.13807"
    },
    {
      "citation_id": "29",
      "title": "Differences first in asymmetric brain: A bi-hemisphere discrepancy convolutional neural network for eeg emotion recognition",
      "authors": [
        "D Huang",
        "S Chen",
        "C Liu",
        "L Zheng",
        "Z Tian",
        "D Jiang"
      ],
      "year": "2021",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2021.03.105"
    },
    {
      "citation_id": "30",
      "title": "Multirepresentation adaptation network for cross-domain image classification",
      "authors": [
        "Y Zhu",
        "F Zhuang",
        "J Wang",
        "J Chen",
        "Z Shi",
        "W Wu",
        "Q He"
      ],
      "year": "2019",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2019.07.010"
    },
    {
      "citation_id": "31",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2019.2916092"
    },
    {
      "citation_id": "32",
      "title": "Adversarial discriminative domain adaptation",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.316"
    },
    {
      "citation_id": "33",
      "title": "Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation",
      "authors": [
        "Y Luo",
        "L Zheng",
        "T Guan",
        "J Yu",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2019.00261"
    },
    {
      "citation_id": "34",
      "title": "Attribute-guided adversarial training for robustness to natural perturbations",
      "authors": [
        "T Gokhale",
        "R Anirudh",
        "B Kailkhura",
        "J Thiagarajan",
        "C Baral",
        "Y Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v35i9.16927"
    },
    {
      "citation_id": "35",
      "title": "Sliced wasserstein discrepancy for unsupervised domain adaptation",
      "authors": [
        "C.-Y Lee",
        "T Batra",
        "M Baig",
        "D Ulbricht"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2019.01053"
    },
    {
      "citation_id": "36",
      "title": "Pr-pl: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2023.3288118"
    },
    {
      "citation_id": "37",
      "title": "Physiological signals based human emotion recognition: a review",
      "authors": [
        "S Jerritta",
        "M Murugappan",
        "R Nagarajan",
        "K Wan"
      ],
      "year": "2011",
      "venue": "2011 IEEE 7th International Colloquium on Signal Processing and its Applications",
      "doi": "10.1109/CSPA.2011.5759912"
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition from sensory and bio-signals: A survey",
      "authors": [
        "K Vora",
        "S Shah",
        "H Harsoda",
        "J Sheth",
        "S Agarwal",
        "A Thakkar",
        "S Mankad"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2nd International Conference on Data Engineering and Communication Technology: ICDECT 2017",
      "doi": "10.1007/978-981-13-1610-4_35"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition from physiological signal analysis: A review",
      "authors": [
        "M Egger",
        "M Ley",
        "S Hanke"
      ],
      "year": "2018",
      "venue": "European Conference on Ambient Intelligence",
      "doi": "10.1016/j.entcs.2019.04.009"
    },
    {
      "citation_id": "40",
      "title": "Physiological-signal-based emotion recognition: An odyssey from methodology to philosophy",
      "authors": [
        "W Li",
        "Z Zhang",
        "A Song"
      ],
      "year": "2021",
      "venue": "Measurement",
      "doi": "10.1016/j.measurement.2020.108747"
    },
    {
      "citation_id": "41",
      "title": "Variational instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "Y Li",
        "X Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2021.3064940"
    },
    {
      "citation_id": "42",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia, MM '19, Association for Computing Machinery",
      "doi": "10.1145/3343031.3350871"
    },
    {
      "citation_id": "43",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems",
      "doi": "10.1109/tcds.2021.3071170"
    },
    {
      "citation_id": "44",
      "title": "Eeg-based cross-subject emotion recognition using fourier-bessel series expansion based empirical wavelet transform and nca feature selection method",
      "authors": [
        "A Anuragi",
        "D Singh Sisodia",
        "R Pachori"
      ],
      "year": "2022",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2022.07.121"
    },
    {
      "citation_id": "45",
      "title": "Eegbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2020.3025777"
    },
    {
      "citation_id": "46",
      "title": "Eeg emotion recognition using fusion model of graph convolutional neural networks and lstm",
      "authors": [
        "Y Yin",
        "X Zheng",
        "B Hu",
        "Y Zhang",
        "X Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing",
      "doi": "10.1016/j.asoc.2020.106954"
    },
    {
      "citation_id": "47",
      "title": "A bi-hemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2018.2885474"
    },
    {
      "citation_id": "48",
      "title": "Multi-channel eeg-based emotion recognition via a multi-level features guided capsule network",
      "authors": [
        "Y Liu",
        "Y Ding",
        "C Li",
        "J Cheng",
        "R Song",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine",
      "doi": "10.1016/j.compbiomed.2020.103927"
    },
    {
      "citation_id": "49",
      "title": "A bi-hemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2018.2885474"
    },
    {
      "citation_id": "50",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2019.2922912"
    },
    {
      "citation_id": "51",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2020.3013711"
    },
    {
      "citation_id": "52",
      "title": "Unsupervised domain adaptation with similarity learning",
      "authors": [
        "P Pinheiro"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00835"
    },
    {
      "citation_id": "53",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2019.2922912"
    },
    {
      "citation_id": "54",
      "title": "A kernel two-sample test",
      "authors": [
        "A Gretton",
        "K Borgwardt",
        "M Rasch",
        "B Schölkopf",
        "A Smola"
      ],
      "year": "2012",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "55",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization",
      "doi": "10.48550/arXiv.1412.6980",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "56",
      "title": "An overview of gradient descent optimization algorithms, arXiv: Learning,arXiv: Learning",
      "authors": [
        "S Ruder"
      ],
      "year": "2016",
      "venue": "An overview of gradient descent optimization algorithms, arXiv: Learning,arXiv: Learning"
    },
    {
      "citation_id": "57",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters",
      "doi": "10.1023/a:1018628609742"
    },
    {
      "citation_id": "58",
      "title": "Some methods for classification and analysis of multivariate observations",
      "authors": [
        "J Macqueen"
      ],
      "year": "1967",
      "venue": "Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability"
    },
    {
      "citation_id": "59",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "Deep domain confusion: Maximizing for domain invariance",
      "arxiv": "arXiv:1412.3474"
    },
    {
      "citation_id": "60",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "authors": [
        "D Coomans",
        "D Massart"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta",
      "doi": "10.1016/S0003-2670(01)95359-0"
    },
    {
      "citation_id": "61",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE",
      "doi": "10.1109/5.726791"
    },
    {
      "citation_id": "62",
      "title": "Deep CORAL: correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV"
    },
    {
      "citation_id": "63",
      "title": "Proceedings, Part III",
      "authors": [
        "Workshops -Amsterdam"
      ],
      "year": "2016",
      "venue": "Proceedings, Part III",
      "doi": "10.1007/978-3-319-49409-8_35"
    },
    {
      "citation_id": "64",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Z Li",
        "E Zhu",
        "M Jin",
        "C Fan",
        "H He",
        "T Cai",
        "J Li"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2022.3210158"
    },
    {
      "citation_id": "65",
      "title": "A bi-lstm frequency band attention network for eeg-based emotion recognition",
      "authors": [
        "B Hu",
        "R Liu",
        "G Liu"
      ],
      "year": "2024",
      "venue": "2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)",
      "doi": "10.1109/AINIT61980.2024.10581447"
    },
    {
      "citation_id": "66",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development",
      "doi": "10.1109/tamd.2015.2431497"
    },
    {
      "citation_id": "67",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2817622"
    },
    {
      "citation_id": "68",
      "title": "Joint contrastive learning with feature alignment for cross-corpus eeg-based emotion recognition",
      "authors": [
        "Q Liu",
        "Z Zhou",
        "J Wang",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 1st International Workshop on Brain-Computer Interfaces (BCI) for Multimedia Understanding, BCIMM '24, Association for Computing Machinery",
      "doi": "10.1145/3688862.3689112"
    },
    {
      "citation_id": "69",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics",
      "doi": "10.1109/tcyb.2017.2788081"
    },
    {
      "citation_id": "70",
      "title": "Eeg-based emotion recognition using spatial-temporal graph convolutional lstm with attention mechanism",
      "authors": [
        "L Feng",
        "C Cheng",
        "M Zhao",
        "H Deng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/jbhi.2022.3198688"
    },
    {
      "citation_id": "71",
      "title": "Differential entropy feature for eegbased emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)",
      "doi": "10.1109/NER.2013.6695876"
    },
    {
      "citation_id": "72",
      "title": "Off-line and on-line vigilance estimation based on linear dynamical system and manifold learning",
      "authors": [
        "L.-C Shi",
        "B.-L Lu"
      ],
      "year": "2010",
      "venue": "2010 Annual International Conference of the IEEE Engineering in Medicine and Biology",
      "doi": "10.1109/IEMBS.2010.5627125"
    },
    {
      "citation_id": "73",
      "title": "Can emotion be transferred?-a review on transfer learning for eeg-based emotion recognition",
      "authors": [
        "W Li",
        "W Huan",
        "B Hou",
        "Y Tian",
        "Z Zhang",
        "A Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems",
      "doi": "10.1109/tcds.2021.3098842"
    },
    {
      "citation_id": "74",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2020.2994159"
    },
    {
      "citation_id": "75",
      "title": "Transfer learning for eeg-based brain-computer interfaces: A review of progress made since",
      "authors": [
        "D Wu",
        "Y Xu",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems",
      "doi": "10.1109/tcds.2020.3007453"
    },
    {
      "citation_id": "76",
      "title": "Wgan domain adaptation for eeg-based emotion recognition",
      "authors": [
        "Y Luo",
        "S.-Y Zhang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing",
      "doi": "10.1007/978-3-030-04221-9_25"
    },
    {
      "citation_id": "77",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems",
      "doi": "10.1109/tcds.2019.2949306"
    },
    {
      "citation_id": "78",
      "title": "Transfer components between subjects for eeg-based emotion recognition",
      "authors": [
        "W.-L Zheng",
        "Y.-Q Zhang",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII.2015.7344684"
    },
    {
      "citation_id": "79",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}