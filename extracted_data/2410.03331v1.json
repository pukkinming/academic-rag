{
  "paper_id": "2410.03331v1",
  "title": "Emojiherovr: A Study On Facial Expression Recognition Under Partial Occlusion From Head-Mounted Displays",
  "published": "2024-10-04T11:29:04Z",
  "authors": [
    "Thorben Ortmann",
    "Qi Wang",
    "Larissa Putzar"
  ],
  "keywords": [
    "facial expressions",
    "emotion recognition",
    "virtual reality",
    "affective game"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition promotes the evaluation and enhancement of Virtual Reality (VR) experiences by providing emotional feedback and enabling advanced personalization. However, facial expressions are rarely used to recognize users' emotions, as Head-Mounted Displays (HMDs) occlude the upper half of the face. To address this issue, we conducted a study with 37 participants who played our novel affective VR game EmojiHeroVR. The collected database, EmoHeVRDB (Emoji-HeroVR Database), includes 3,556 labeled facial images of 1,778 reenacted emotions. For each labeled image, we also provide 29 additional frames recorded directly before and after the labeled image to facilitate dynamic Facial Expression Recognition (FER). Additionally, EmoHeVRDB includes data on the activations of 63 facial expressions captured via the Meta Quest Pro VR headset for each frame. Leveraging our database, we conducted a baseline evaluation on the static FER classification task with six basic emotions and neutral using the EfficientNet-B0 architecture. The best model achieved an accuracy of 69.84% on the test set, indicating that FER under HMD occlusion is feasible but significantly more challenging than conventional FER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Over the last decade, Virtual Reality (VR) has become an established technology in various applications and research areas, including therapy, training, education, entertainment, and human behavior  [1] ,  [2] . Through Head-Mounted Displays (HMDs), VR experiences deliver high levels of immersion, presence and interaction while simulating almost arbitrary environments  [3] . These capabilities render VR an ideal tool for the reliable elicitation and study of emotions  [4] . Vice versa, emotion recognition is valuable for evaluating and enhancing VR experiences. Emotions affect human perception, decision-making, behavior and overall psychological and physiological state  [5] . Their recognition promotes indepth analysis of VR experiences and enables systems to react directly to a user's emotional state, further increasing interactivity and personalization. Facial expressions are a very This study was partially funded as project C4T910 by the Authority for Science, Research, Equality and Districts of the Free and Hanseatic City of Hamburg, Germany (BWFGB).\n\nnatural and potent signal to convey emotions  [6] . Automatic Facial Expression Recognition (FER) is a well-researched task with a rich history in computer vision and affective computing  [7] . Modern approaches build upon deep learning models, such as Convolutional Neural Networks (CNNs) and transformer architectures  [8] . Typically, FER systems categorize images or image sequences into six to eight emotion categories based on Ekman's theory of basic emotions  [9]    [10] . While more works focus on static FER, processing single images, dynamic FER systems have demonstrated that including temporal features using multiple frames can be beneficial  [11] .\n\nHowever, FER is rarely applied in current VR research as HMDs occlude the upper face half, severely limiting the capabilities of conventional FER systems  [12] . While some works have reported promising results on images with artificial HMD occlusion  [13] -  [18] , these findings still need to be validated with naturally occluded data in actual VR settings. Our work addresses this gap by conducting a user study with our novel affective VR game, EmojiHeroVR. We record emotional faces under natural HMD occlusion, demonstrate the practical value of FER in VR environments and evaluate the applicability of existing findings. Moreover, we contribute our collected database EmoHeVRDB (EmojiHeroVR Database), which is suitable for static, dynamic, and multimodal FER, with baseline evaluations on the static FER task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Few works have been published regarding the particular case of FER under partial occlusion from HMDs. Only one  [25]  of the 42 studies included in the comprehensive review on emotion recognition in VR conducted by Marín-Morales et al. applied FER  [2] . However, in  [25] , Granato et al. did not apply image-based FER but measured facial muscle activity during VR racing games via Electromyography (EMG) to predict arousal and valence values. In  [12] , we analyzed 21 studies in our systematic literature review on FER in VR in detail. Most studies employed sensors attached to or embedded in HMDs, most prominently electrodes for EMG. A minority of six works  [13] -  [18]  relied on conventional FER based on   [26] , a variant of the VGG-16 architecture  [27]  extensively pre-trained for face recognition on the VGG-face dataset. Using the 8-class AffectNet database  [24] , they first trained and evaluated their model with unoccluded data, achieving an accuracy of 59.03%.\n\nTo simulate HMD occlusion, they blacked out the entire upper half of the images. When evaluated against the occluded test set, the model's accuracy dropped to 37.70%. However, when also trained with occluded data, the model's accuracy only decreased to 49.23%. For the FER+ dataset  [22] , accuracy even only decreased from 84.79% to 82.28%. Houshamand et al. performed a more sophisticated HMD simulation based on facial landmarks to accurately add a black rectangle to each image  [15] . They reported results similar to those of Georgescu et al., with VGG-face and ResNet50  [28]  architectures on the AffectNet, FER+, and RAF-DB  [23]  datasets. Large, diverse and well-designed databases are the basis for the development of robust FER models. Early FER databases, such as KDEF  [19] , JAFFE  [29] , CK  [30] , CK+  [20] , and RaFD  [21] , laid the groundwork by recording posed emotions under lab-controlled conditions. However, sample sizes were still small for deep learning, real-world conditions like varying angles, illumination, and occlusions were not represented enough, and posed emotions differed from spontaneous ones. Consequently, web-based and crowd-sourced databases like FER-2013  [31] , FER+, RAF-DB and AffectNet were constructed. Researchers acquired large amounts of image data by keyword-based image search using engines like Google, Bing, Yahoo, and Flickr. One of the largest and most widely used databases is AffectNet. It comprises about one million facial images. About 420,000 were manually annotated, resulting in 291,651 images with expression labels for eight categories and arousal and valence values. Web-based image databases are generally more diverse and natural than lab-controlled ones. Facial expressions are more spontaneous, although depending on the search results, web-based databases include posed expressions as well. While various natural occlusions, such as sunglasses or hands, are present in these databases, no database focuses on HMD occlusion. Thus, the works concerned with FER under HMD occlusion applied artificial occlusions to simulate HMDs. Our database, EmoHeVRDB, follows the traditional approach of recording posed expressions under lab-controlled conditions. However, it is the first to provide naturally HMD-occluded emotional faces. Table I provides a comprehensive comparison of key FER databases and Emo-HeVRDB.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Study Preparation",
      "text": "We prepared a user study to collect naturally HMD-occluded emotional faces. We developed the VR game EmojiHeroVR to elicit and record posed emotions, trained an FER model on artificially occluded data and arranged the experimental setup.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. The Game -Emojiherovr",
      "text": "EmojiHeroVR is a single-player high-score game. Its name and design are inspired by the popular Guitar Hero video game series. However, we chose a more neutral design, shown in Fig.  1 , with a pinball-machine-like object as the main component. Emojis spawn in its back and move toward the player in one of four lanes. When an emoji reaches the red zone directly in front of the player, the player has about one second to reenact the emotion symbolized by the emoji. The emojis used correspond to the six basic emotions of anger, disgust, fear, happiness, sadness, and surprise, plus neutral, which are commonly used in FER research. For our study, we designed four levels comprising a predetermined order of emojis. The levels become progressively more challenging to elevate players' engagement as the number of emojis per level and their movement speed increase while emojis also spawn more frequently. In total, all four levels comprise 70 emojis or 10 per emotion category. We developed EmojiHeroVR using the Unity3D game engine. The FER model, employed to determine whether a reenactment was successful, was developed in Python and made available via a minimalistic web service built with the FastAPI framework.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Facial Expression Recognition Model",
      "text": "EmojiHeroVR's game mechanism depends on an FER model to rate whether players reenact emotions successfully. Following the approach of related works, we prepared a static FER model for HMD occlusion by training with artificially occluded data. We built upon the Poster++ model architecture  [32]  due to its state-of-the-art results on RAF-DB and Affect-Net, its comparably low computational complexity and code availability. We combined data from the AffectNet, EmotioNet  [33] , ExpW  [34] , FER+, and SFEW  [35]  datasets for the seven emotion categories used in EmojiHeroVR. Subsequently, we performed face and facial keypoint detection for each image using Google's MediaPipe Python package to crop and occlude images uniformly. To accurately simulate HMD occlusion, we followed the same approach as  [15]  and  [18]  and added a black rectangle to each image based on the facial keypoint detection result. Consequently, our imbalanced training set comprised about 360,000 images that we used alongside a balanced validation set of 5,600 images to train our model. The training set was highly imbalanced due to the imbalance of the underlying datasets, especially AffectNet. We accounted for this by setting each sample's selection probability inversely proportional to its class frequency, in combination with data augmentation techniques, as Mao et al. did in  [32] .\n\nWe also adjusted the class weights of the Categorical Cross Entropy loss function to favor the anger, fear and sadness classes. Following this training approach, our model achieved a validation accuracy of 61%, with no class's F-Score being below 55%. To further validate our model, we performed cross-dataset evaluation against HMD-occluded versions of the 2,940 central and 45°side-view images of the KDEF dataset, exemplarily shown in Fig.  2 . We chose KDEF because it is balanced, and its recording conditions are similar to those of our study. Our model achieved a cross-dataset evaluation accuracy of 55%. After resuming the training process with a very low learning rate and the occluded images of 62 out of 70 subjects of the KDEF dataset, the model's accuracy on the remaining occluded KDEF data rose to 81%, providing us with confidence to apply the model in EmojiHeroVR.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Experimental Setup",
      "text": "We used two consumer Logitech webcams (models C930e and C920e) to record participants' faces from a frontal and a 45°side view at 30 frames per second (FPS) and a resolution of 1280x720 pixels. Two softboxes ensured good illumination at about 5400K color temperature. We employed the Meta Quest Pro as the VR headset for our study because it can capture facial expressions by accessing its embedded sensors via the Meta XR Core SDK's Face Tracking API  [36] . As depicted in Fig.  3 , both cameras and the headset were connected to a PC, which ran EmojiHeroVR as a Unity application and the Python web service providing the FER model. The PC had an Intel i9-12900K CPU, 64GB of DDR4 RAM, and an Nvidia GeForce RTX 3090 GPU and ran Windows 11 version 22H2.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Sample Selection",
      "text": "We primarily recruited participants in the environment of the Hamburg University of Applied Sciences (HAW Hamburg). We promoted the study via e-mail using mailing lists, in lectures, and by prominently placing posters and flyers around the HAW Hamburg's Finkenau campus. Participants were compensated with C10 in cash to value their contribution. The recruitment process, the study's conduction, and the corresponding data processing were approved by the ethics committees of the University of the West of Scotland (application 21639) and of the HAW Hamburg (application 2023-25).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Study Execution And Results",
      "text": "We conducted the study on eight days between the 15 th of November and the 1 st of December 2023 in the facilities of the HAW Hamburg's Finkenau campus. Thirty-seven subjects, primarily students and faculty members, participated. Twenty participants reported being male, fifteen reported being female, and two chose not to disclose their gender. The age ranged from 19 to 50 years (µ = 27.19 and σ = 7.66). Most subjects identified with a European heritage.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Procedure",
      "text": "First, participants were briefed on the study's procedure and objectives. Next, the research team handed out participant information sheets and consent forms detailing the study design, particularly emphasizing data processing practices. After giving informed consent, participants completed a short questionnaire focusing on demographic data. Then, they were introduced to the Facial Action Coding System (FACS)  [37]  and trained on accurately reenacting the seven emotions featured in EmojiHeroVR for about 10 minutes. Subsequently, the research team positioned participants at the recording station, adjusted the cameras, and set up the VR headset. After a brief trial session, participants played through four levels, taking about 90-second breaks between each level. The duration of levels increased from level one, with about 25 seconds and nine emojis, to level four, with about 50 seconds and 28 emojis. Upon completion, participants answered the Game Experience Questionnaire (GEQ)  [38]  and the Virtual Reality Sickness Questionnaire (VRSQ)  [39]  to assess their game experience and any feelings of VR sickness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Collected Data",
      "text": "Both cameras recorded participants' faces at 30 FPS with a resolution of 1280x720 pixels during their gameplay. We directly associated every recorded frame with its corresponding emoji or reenactment process by defining a recording window for each emoji. An emoji's recording window equals the time the emoji is present in the red reenactment zone displayed in the game's graphical user interface, plus about half a second upfront. We saved every frame inside an emojis recording window in a corresponding directory as a PNG file with a UNIX timestamp plus a camera index as its name. Each frame outside a recording window was discarded. For level one, the average length of the recording window is 2.13 seconds, resulting in 64.63 frames (σ = 0.61) on average per camera. For level four, the average length of the recording window is 1.59 seconds, resulting in 48.52 frames (σ = 0.63) on average per camera. The differences are due to increasing movement speed and decreasing spawn rate of emojis between levels one and four. In total, we recorded 37 participants reenacting 2,590 emojis, resulting in 147,490 images per camera. In addition to recording with external cameras, we also captured facial expressions using the Meta Quest Pro's sensors. The Meta XR Core SDK's Face Tracking API was accessed in each executed update loop of the game inside an emoji's recording window, averaging 62.78 times per second. In consequence, a total of 332,734 API calls were executed during the study conduction. Due to a technical malfunction, no face-tracking data were captured for the seventh participant. For each API call, we saved a list of 63 floating-point numbers ranging from 0 to 1. Each number represents the measured activation strength of one of 63 facial expressions based on FACS. Typically, these measured facial expression activations are utilized to adjust blend shapes, allowing for a detailed and accurate mapping of a user's facial movements onto a 3D model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Data Annotation",
      "text": "Our collected data have several limitations. First, emotions are not spontaneous but posed by participants who are not trained actors. Authentically reenacting emotions is a challenging task, especially under time pressure caused by the game mechanism of EmojiHeroVR. Second, the challenge during the gameplay might elicit spontaneous emotions contrary to the emotion to pose, for example, stress or frustration when a player fails to reenact an emoji correctly. Third, players might quickly learn how to please the game or the underlying FER model to get a higher score and adjust their original facial expressions accordingly, resulting in data biased towards the employed FER model. To compensate for these limitations and ensure high data quality, three student annotators additionally labeled the collected data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Procedure",
      "text": "We decided to label one central-view image per reenacted emoji. To select the image to label, we used our FER model, described in subsection III-B, to predict a 7-dimensional stochastic vector for each image. Then, per emoji, we chose the image with the highest probability for the emotion to be reenacted. We excluded the first 15 frames per emoji from the selection as those were recorded before an emoji entered the red reenactment zone in the game. Also, this ensures that at least 15 frames are available as context before each labeled image. Each of the 2,590 selected images was labeled by three students with experience in the field of FER. For labeling, we developed a web service using the Python Flask framework that draws images in random order, shows the image to label, and lets the user label the image by clicking on one of seven buttons labeled with the corresponding emotion category.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Results",
      "text": "On average, annotators recognized the emotion to be reenacted in 71.87% of cases. As visualized in Fig.  4 , the most recognizable emotions were happiness, neutral, and surprise, with F-scores of 92.44%, 77.68% and 80.37%, respectively. In contrast, with recall values between 51.44% and 57.12%, anger, fear and disgust were often not recognized. However, Cohen's Kappa scores for annotator pairs, listed in Table  II , and a Fleiss' Kappa score of 67.76% indicate a substantial agreement and provide confidence in the given labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Database Construction",
      "text": "To ensure high data quality in our database, we discarded all samples for which not at least two of three annotators agreed to recognize the emotion that was to be reenacted. As a result, 1,921 labeled images with a Fleiss' Kappa Score of 78.60% remained. Due to this selection step, the data are unbalanced in favor of emotions that were easier to reenact and recognize.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Training, Validation And Test Split",
      "text": "We split our data into a training, a validation, and a test set to establish a comparable baseline. To prevent information leakage, we divided the 37 participants into a training set of 21, a validation set of 8 and a test set of 8. We considered the available age, gender, and ethnicity data for a stratified split and aimed for the validation and test sets to be as balanced as possible. Subsequently, to balance the validation and test sets, we iteratively discarded random samples of the most represented participant for each class until the number of samples was equal to the lowest class frequency. Consequently, we discarded 64 of 449 samples from the validation set and 79 of 457 samples from the test set. As a result, EmoHeVRDB comprises an unbalanced training set of 1015 labels, a balanced validation set of 385 labels and a balanced test set of 378 labels. The final label distribution is visualized in Fig.  5 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Construction And Statistics",
      "text": "Based on the 1,778 labels for central-view images, we constructed the rest of EmoHeVRDB. First, we added the sideview recordings with identical timestamps as the labeled images, doubling the number of samples to 3,556. Applying face detection with the MediaPipe Python package, we cropped all images to 720x720 pixels centered at the face. Subsequently, we converted the PNG files to JPG with a compression quality of 95% to reduce file size. Next, we prepared our database for dynamic FER. For each labeled image, we selected 29 contextual frames from the corresponding recording window to cover about one second of facial movements for each reenactment. We added as many preceding frames as possible and increased the count to 30 with subsequent frames if necessary. Consequently, EmoHeVRDB includes 3,556 30frame sequences recorded from two perspectives. Lastly, we added the data captured via the Meta Quest Pro to Emo-HeVRDB. Using the recorded timestamps, we associated a list of 63 facial expression activations with each included frame. Notably, this was not possible for recordings of the seventh participant, as no face-tracking data were captured due to a technical malfunction. In total, we provide 1,727 30-element facial expression activation sequences as JSON files.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vii. Baseline",
      "text": "We performed several experiments with EmoHeVRDB to create a baseline for the image-based static FER task and examine the effects of natural HMD occlusion. For comparability, reproducibility, and fast training, we used the EfficientNet-B0 architecture for all experiments. The EfficientNets model family has achieved state-of-the-art results on several computer vision tasks while being highly efficient  [40] . B0 is its smallest variant. In  [41] , Savchenko reported an accuracy of 61.32% for the 8-class FER task on AffectNet using an EfficientNet-B0 (∼5.3M parameters) pre-trained on the VGGFace2 dataset  [42] . Thereby, it outperforms the accuracy of 59.03% reported by  [14]  using the much larger VGG-face model (∼138.4M parameters). We implemented our experiments with Tensor-Flow 2.15, leveraging Keras' standard implementation of EfficientNet-B0. All experiments were executed on a machine with an Intel i9-12900K CPU, 64GB of DDR4 RAM, and an Nvidia GeForce RTX 3090 GPU. It ran Ubuntu 22.04 and had CUDA 12.2 and cuDNN 8.9 installed.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Database Preparation",
      "text": "We prepared five datasets for our experiments. For face and facial keypoint detection, we used the MediaPipe Python package, version 0.10. For resizing images with Lanczos interpolation, we used the opencv-python package, version 4.9.\n\n1) AffectNet-7: We built the 7-class AffectNet dataset by removing all images of the contempt class from the 8-class AffectNet dataset, resulting in 287,401 available images. As typically done, we used the predefined validation set, comprising 500 images per class, as the test set. We randomly split 380 samples, which is 10% of the lowest class frequency, from each class to create a balanced validation set. All images have a resolution of 224x224 pixels.\n\n2) AffectNet-7-occl: We created an artificially occluded version of AffectNet-7 by adding a black rectangle to each image based on face and facial keypoint detection results, similar to the approaches of  [15]  and  [18] . If no face was detected or the detection results were not plausible, we defaulted to occluding the top 54% of the image at 90% of its width.\n\n3) KDEF-SHR: The original KDEF dataset contains 4,900 facial images of 70 subjects for seven emotion categories recorded from five angles. For our experiments, KDEF served as a reference for the results on EmoHeVRDB. So, we prepared it to resemble EmoHeVRDB as closely as possible. Thus, we selected only the 1,960 central and 45°left-sideview images, coded with S for straight and HR for half right profile, respectively, for our experiments. We cropped the images closer to the face and resized them to 224x224 pixels to make them uniform with the AffectNet images. Subsequently, we split the data into training, validation, and test sets by randomly selecting 14 out of 70 subjects, seven female and seven male, for the validation and the test set each.\n\n4) KDEF-SHR-occl: Similar to AffectNet-7-occl, we also created an occluded version of our prepared KDEF-SHR dataset using the same occlusion approach.\n\n5) EmoHeVRDB: We selected the 3,556 labeled central and 45°side-view images from EmoHeVRDB, cropped them closer to the face and downsized them to 224x224 pixels.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Training",
      "text": "Similar to  [14] ,  [15] ,  [18] ,  [41] , we applied a transfer learning approach in our experiments. We initialized an EfficientNet-B0, pre-trained on the ImageNet database  [43] , froze its weights and replaced the final Dense layer with a new one with 7 units and softmax activation. Subsequently, we started fitting the network's top for FER by training for a few epochs with a high learning rate. Next, we incrementally unfroze more parts of the network to fine-tune it with a lower learning rate. For all experiments, we used the Spar-seCategoricalCrossentropy loss function, an Adam optimizer with a learning rate between 1e-3 and 1e-6 and a batch size of 32. We weighted the loss function with the normalized inverse class frequencies to account for imbalances in the training sets. Additionally, we incorporated several image augmentation layers, namely RandomFlip, RandomTranslation, RandomRotation, RandomZoom, RandomContrast and RandomBrightness, in our network to increase the variety of the training data. Generally, we kept the hyperparameter space small and focused on optimizing the fine-tuning process by tuning the combination of layers to unfreeze, epochs to train and the learning rate.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Results",
      "text": "For the first set of experiments, we fine-tuned an EfficientNet-B0 for each of the five datasets listed in subsection VII-A following the training process described in the previous subsection. Our results on AffectNet, listed in Table  III , demonstrate the effectiveness of our approach. We achieve higher accuracies than related works on unoccluded and occluded versions of AffectNet. However, in contrast to us,  [14] ,  [15] ,  [44]  operated on the 8-class AffectNet dataset. Similar to  [14] , we find an accuracy decrease of about 10% when working with the occluded version of AffectNet.  AffectNet-7 is more affected by occlusion than KDEF-SHR because its images contain fewer distinctive features. Thus, additional occlusions are more likely to hide the only crucial information available.\n\nOur baseline accuracy on EmoHeVRDB is 69.84%. Our model performs worst in the anger class, with an F-score of 49.11%, followed by the disgust class, with 61.67%. The surprise, happiness, and neutral classes are the most recognizable, with F-Scores of 86.12%, 77.18%, and 76.42%, respectively. The overall accuracy is 16.64% lower than on KDEF-SHR, which was recorded under similar conditions, indicating that FER under natural HMD occlusion is a substantially harder task than regular FER. However, the accuracy on EmoHeVRDB is also 9.75% lower than on KDEF-SHR-occl, which we constructed to resemble EmoHeVRDB. This difference is likely due to the higher data variety in EmoHeVRDB. The subjects recorded for KDEF were trained amateur actors instructed to pose strong and clear facial expressions. They were between 20 and 30 years old and wore no beards, mustaches, earrings, eyeglasses, or visible makeup  [45] . In contrast, for EmoHeVRDB, the age ranged from 19 to 50, and subjects wore various beards, makeup, earrings, and similar jewelry. Additionally, emotions were posed under time pressure while playing a VR game, resulting in a larger variety of facial expressions and head poses.\n\nFor the second set of experiments, we performed crossdataset evaluations with the models trained on AffectNet-7 and AffectNet-7-occl against KDEF-SHR, KDEF-SHR-occl and EmoHeVRDB. Applying the AffectNet-7 model on KDEF-SHR works very well, resulting in an accuracy of 81.82%. The AffectNet-7-occl model's accuracy on KDEF-SHR-occl is significantly lower, but the model still classifies 64.10% of the samples correctly. Most notably, the AffectNet-7 and AffectNet-7-occl models, perform equally poorly on Emo-HeVRDB, with accuracies of about 35%. This implies a We conclude that the primary reason for the very low crossdataset evaluation accuracy is the absence of naturally HMDoccluded faces in AffectNet-7-occl. It seems that our artificial occlusion approach does not simulate HMDs realistically enough to make models robust against natural HMD occlusion. These findings underline the importance of EmoHeVRDB for developing reliable FER models for real VR scenarios.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Viii. Conclusion And Future Work",
      "text": "Our work presents the novel database EmoHeVRDB. It comprises 3,556 image sequences of the emotional faces of 37 participants playing our novel affective VR game, EmojiHeroVR. Additionally, EmoHeVRDB includes 1,727 facial expression activation sequences captured via the Meta Quest Pro VR headset. The results of our baseline evaluation on the static FER task demonstrate the success of our study design and subsequent data annotation process in constructing a novel, high-quality database. Our code and details on how to request access to EmoHeVRDB are available on GitHub: https:// github.com/ thorbenortmann/ emoji-hero-vr-database. A demo video showing the third level of EmojiHeroVR, used in the user study, is available on YouTube: https:// youtu.be/ TnJrGYOjKJc. In future work, we plan to leverage the facial expression activations included in EmoHeVRDB, first for static unimodal FER and, subsequently, in combination with the image data for multimodal FER. Finally, we plan to make use of EmoHeVRDB's sequential character to also experiment with dynamic FER in VR environments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "The overarching goal of our research is to enhance VR experiences through automatic emotion recognition. Particularly in application areas, such as therapy, education and entertainment, recognizing the user's emotional state can be highly beneficial. Our study contributes to this goal by exploring the feasibility of FER utilizing data collected via an affective VR game.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Conducting A User Study",
      "text": "Our research design included collecting naturally HMDoccluded faces through a user study. The study design was approved by the ethics committees of the University of the West of Scotland (application 21639) and of the HAW Hamburg (application 2023-25).\n\n1) Data Privacy and Informed Consent: All participants provided informed consent for their data to be used within the context of our research and for their pseudonymized data to be shared with other researchers under controlled conditions. We require all researchers to apply for access to EmoHeVRDB, stating their name, affiliation and intended usage. Access is only granted after confirming an applicant's identity and receiving a signed licensing agreement, which especially does not permit any redistribution of the data without our prior written approval.\n\n2) Data Diversity and Bias: While we did not target any specific group, our study lacks ethnic diversity. The vast majority of participants identified with a European heritage. However, we believe this is not due to a biased recruitment process but mirrors our university's environment in Hamburg, Germany. For the same reason, our sample's average age is significantly lower than the general population's. Concerning gender, our sample represents slightly more male than female participants. Consequently, our sample of 37 participants in EmoHeVRDB is not representative of the general population and has to be used with care. Algorithmic bias is probable when only learning from our data and applying learned models to a broader demographic group.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Emotion Modelling And Elicitation",
      "text": "1) Categorical Basic Emotions: Our study employed a categorical emotion model of six basic emotions plus neutral, which is widely utilized in FER research. Using it enabled us to rely on existing datasets and compare our results with related works. However, we acknowledge that this model carries the risk of oversimplification and may not encompass the cultural and individual variability in emotional expression.\n\n2) Posed Emotions: In our study, participants posed emotions to gain points in the EmojiHeroVR game. However, authentically reenacting emotions is challenging. Thus, posed emotions can differ significantly from spontaneous ones. To mitigate this and to improve data consistency, participants were introduced to FACS and trained on accurately reenacting emotions for about 10 minutes before playing EmojiHeroVR. Also, the data annotation process provided an additional level to secure data quality. Nevertheless, there remains a gap between the posed emotions collected in our study and real spontaneous emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Data Annotation",
      "text": "To ensure high data quality, three students who have worked in the field of FER before labeled 2,590 images of the collected data to construct the final database. Our university employed all annotators as student or research assistants. While annotators were familiar with various emotion models and FACS, a personal labeling bias is possible. Also, all annotators had a similar cultural background and age. For privacy reasons, their names are not disclosed. To mitigate personal bias, we computed several metrics to ensure substantial labeling agreement.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Conclusion",
      "text": "We acknowledge several limitations of our study, particularly regarding the demographic diversity of the collected data, the difference between posed and spontaneous emotions and the capability of the categorical emotion model to capture all facets of emotions accurately. However, we emphasize our study's role as the first user study to investigate image-based FER under natural HMD occlusion to gain a preliminary understanding of the feasibility of FER in VR settings. We expect a positive impact from reliable automatic emotion recognition in VR as many beneficial applications such as therapy and education may profit from it. Nevertheless, further developments must carefully consider the capability of highly immersive and affective VR environments to influence people's emotions and possibly opinions.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , with a pinball-machine-like object as the main",
      "page": 2
    },
    {
      "caption": "Figure 1: Captured screen of the Unity Editor during the study conduction - top left: left eye view, top right: right eye view, bottom left: FER results, bottom",
      "page": 3
    },
    {
      "caption": "Figure 2: We chose KDEF because it is",
      "page": 3
    },
    {
      "caption": "Figure 2: Artificially occluded images from KDEF [19] (image IDs from left",
      "page": 3
    },
    {
      "caption": "Figure 3: Experimental setup with two cameras, two softboxes, a PC and a",
      "page": 4
    },
    {
      "caption": "Figure 3: , both cameras and the headset were connected to a",
      "page": 4
    },
    {
      "caption": "Figure 4: , the most",
      "page": 5
    },
    {
      "caption": "Figure 4: Confusion matrix - emotion to reenact vs labeled emotion.",
      "page": 5
    },
    {
      "caption": "Figure 5: Fig. 5. Label distribution for the training, validation and test sets.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "KDEF,\n1998 [19]": "CK+,\n2010 [20]",
          "4,900 images from five angles in two\nsessions": "593 image sequences (327 with emo-\ntion labels)",
          "70": "123",
          "Lab": "Lab",
          "P": "P",
          "N": "N",
          "6\nbasic\nexpressions\n+\nneutral": "6\nbasic\nexpressions\n+\nneutral and contempt",
          "Acted by subjects": "Acted by subjects"
        },
        {
          "KDEF,\n1998 [19]": "RaFD,\n2010 [21]",
          "4,900 images from five angles in two\nsessions": "8,040\nimages\nfrom five\nangles with\nthree gaze directions",
          "70": "67",
          "Lab": "Lab",
          "P": "P",
          "N": "N",
          "6\nbasic\nexpressions\n+\nneutral": "6\nbasic\nexpressions\n+\nneutral and contempt",
          "Acted by subjects": "Acted by subjects"
        },
        {
          "KDEF,\n1998 [19]": "FER+,\n2016 [22]",
          "4,900 images from five angles in two\nsessions": "35,887\nimages\nbased\non\nkeyword\nsearch via Google image search",
          "70": "N/A",
          "Lab": "Web",
          "P": "(P&)S",
          "N": "V",
          "6\nbasic\nexpressions\n+\nneutral": "6\nbasic\nexpressions\n+\nneutral and contempt",
          "Acted by subjects": "Each image\nlabeled by 10 an-\nnotators"
        },
        {
          "KDEF,\n1998 [19]": "RAF-DB,\n2017 [23]",
          "4,900 images from five angles in two\nsessions": "29,672\nimages\nbased\non\nkeyword\nsearch via Flickr\nimage search",
          "70": "N/A",
          "Lab": "Web",
          "P": "(P&)S",
          "N": "V",
          "6\nbasic\nexpressions\n+\nneutral": "6 basic + 12 compound\nexpressions + neutral",
          "Acted by subjects": "Each\nimage\nlabeled\nby\n40\nof\n315 annotators"
        },
        {
          "KDEF,\n1998 [19]": "AffectNet,\n2019 [24]",
          "4,900 images from five angles in two\nsessions": "291,651\nmanually\nlabeled\nimages\nbased on keyword search via Google,\nBing and Yahoo image search",
          "70": "N/A",
          "Lab": "Web",
          "P": "(P&)S",
          "N": "V",
          "6\nbasic\nexpressions\n+\nneutral": "6\nbasic\nexpressions\n+\nneutral and contempt +\nvalence and arousal",
          "Acted by subjects": "Each image\nlabeled by one of\ntwelve\nannotators;\n36,000\nim-\nages labeled by two annotators"
        },
        {
          "KDEF,\n1998 [19]": "EmoHeVRDB,\n2024",
          "4,900 images from five angles in two\nsessions": "3,556 image sequences\nfrom two an-\ngles + 1,727 facial expression activa-\ntion sequences",
          "70": "37",
          "Lab": "Lab",
          "P": "P",
          "N": "HMD",
          "6\nbasic\nexpressions\n+\nneutral": "6\nbasic\nexpressions\n+\nneutral",
          "Acted by subjects": "Acted by subjects and each im-\nage labeled by three annotators"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A systematic review of physiological measurements, factors, methods, and applications in virtual reality",
      "authors": [
        "A Halbig",
        "M Latoschik"
      ],
      "year": "2021",
      "venue": "Front. Virtual Real",
      "doi": "10.3389/frvir.2021.694567"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in immersive virtual reality: From statistics to affective computing",
      "authors": [
        "J Marín-Morales",
        "C Llinares",
        "J Guixeres",
        "M Alcañiz"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20185163"
    },
    {
      "citation_id": "3",
      "title": "The past, present, and future of virtual and augmented reality research: A network and cluster analysis of the literature",
      "authors": [
        "P Cipresso",
        "I Giglioli",
        "M Raya",
        "G Riva"
      ],
      "year": "2018",
      "venue": "Front. Psychol",
      "doi": "10.3389/fpsyg.2018.02086"
    },
    {
      "citation_id": "4",
      "title": "Virtual reality for emotion elicitation: A review",
      "authors": [
        "R Somarathna",
        "T Bednarz",
        "G Mohammadi"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2022.3181053"
    },
    {
      "citation_id": "5",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective computing",
      "doi": "10.7551/mitpress/1140.001.0001"
    },
    {
      "citation_id": "6",
      "title": "Darwin and facial expression: A century of research in review",
      "authors": [
        "P Ekman"
      ],
      "year": "2006",
      "venue": "Ishk"
    },
    {
      "citation_id": "7",
      "title": "Facial expression recognition using computer vision: A systematic review",
      "authors": [
        "D Canedo",
        "A Neves"
      ],
      "year": "2019",
      "venue": "Applied Sciences",
      "doi": "10.3390/app9214678"
    },
    {
      "citation_id": "8",
      "title": "Towards facial expression recognition in immersive virtual reality with EmojiRain",
      "authors": [
        "T Ortmann"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)",
      "doi": "10.1109/ACIIW59127.2023.10388094"
    },
    {
      "citation_id": "9",
      "title": "Facial expressions of emotion",
      "authors": [
        "P Ekman",
        "H Oster"
      ],
      "year": "1979",
      "venue": "Annu. Rev. Psychol"
    },
    {
      "citation_id": "10",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2020.2981446"
    },
    {
      "citation_id": "11",
      "title": "Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition",
      "authors": [
        "D Kim",
        "W Baddar",
        "J Jang",
        "Y Ro"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2017.2695999"
    },
    {
      "citation_id": "12",
      "title": "Facial emotion recognition in immersive virtual reality: A systematic literature review",
      "authors": [
        "T Ortmann",
        "Q Wang",
        "L Putzar"
      ],
      "year": "2023",
      "venue": "Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments, ser. PETRA '23",
      "doi": "10.1145/3594806.3594861"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in gamers wearing head-mounted display",
      "authors": [
        "H Yong",
        "J Lee",
        "J Choi"
      ],
      "year": "2019",
      "venue": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)",
      "doi": "10.1109/VR.2019.8797736"
    },
    {
      "citation_id": "14",
      "title": "Recognizing facial expressions of occluded faces using convolutional neural networks",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu"
      ],
      "year": "2019",
      "venue": "Neural Information Processing",
      "doi": "10.1007/978-3-030-36808-1_70"
    },
    {
      "citation_id": "15",
      "title": "Facial expression recognition under partial occlusion from virtual reality headsets based on transfer learning",
      "authors": [
        "B Houshmand",
        "N Khan"
      ],
      "year": "2020",
      "venue": "2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM)",
      "doi": "10.1109/BigMM50055.2020.00020"
    },
    {
      "citation_id": "16",
      "title": "Teacher-student training and triplet loss to reduce the effect of drastic face occlusion",
      "authors": [
        "M.-I Georgescu",
        "G.-E Dut",
        "R Ionescu"
      ],
      "year": "2021",
      "venue": "Machine Vision and Applications",
      "doi": "10.1007/s00138-021-01270-x"
    },
    {
      "citation_id": "17",
      "title": "Teacher-student training and triplet loss for facial expression recognition under occlusion",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR48806.2021.9412493"
    },
    {
      "citation_id": "18",
      "title": "Valence/arousal estimation of occluded faces from vr headsets",
      "authors": [
        "T Gotsman",
        "N Polydorou",
        "A Edalat"
      ],
      "year": "2021",
      "venue": "2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI)",
      "doi": "10.1109/CogMI52975.2021.00021"
    },
    {
      "citation_id": "19",
      "title": "Karolinska Directed Emotional Faces",
      "authors": [
        "D Lundqvist",
        "A Flykt",
        "A Öhman"
      ],
      "year": "1998",
      "venue": "PsycTESTS Dataset"
    },
    {
      "citation_id": "20",
      "title": "The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "21",
      "title": "Presentation and validation of the Radboud Faces Database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bijlstra",
        "D Wigboldus",
        "S Hawk",
        "A Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699930903485076"
    },
    {
      "citation_id": "22",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ser. ICMI '16",
      "doi": "10.1145/2993148.2993165"
    },
    {
      "citation_id": "23",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.277"
    },
    {
      "citation_id": "24",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affective Comput",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "25",
      "title": "An empirical study of players' emotions in vr racing games based on a dataset of physiological data",
      "authors": [
        "M Granato",
        "D Gadia",
        "D Maggiorini",
        "L Ripamonti"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-019-08585-y"
    },
    {
      "citation_id": "26",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "BMVC 2015 -Proceedings of the British Machine Vision Conference"
    },
    {
      "citation_id": "27",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "29",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition",
      "doi": "10.1109/AFGR.1998.670949"
    },
    {
      "citation_id": "30",
      "title": "Comprehensive database for facial expression analysis",
      "authors": [
        "T Kanade",
        "J Cohn",
        "Y Tian"
      ],
      "year": "2000",
      "venue": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)",
      "doi": "10.1109/AFGR.2000.840611"
    },
    {
      "citation_id": "31",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2013",
      "venue": "Neural Information Processing",
      "doi": "10.1007/978-3-642-42051-1_16"
    },
    {
      "citation_id": "32",
      "title": "Poster++: A simpler and stronger facial expression recognition network",
      "authors": [
        "J Mao",
        "R Xu",
        "X Yin",
        "Y Chang",
        "B Nie",
        "A Huang"
      ],
      "year": "2023",
      "venue": "Poster++: A simpler and stronger facial expression recognition network",
      "doi": "10.48550/arXiv.2301.12149"
    },
    {
      "citation_id": "33",
      "title": "Emo-tioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "34",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-017-1055-1"
    },
    {
      "citation_id": "35",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)",
      "doi": "10.1109/ICCVW.2011.6130508"
    },
    {
      "citation_id": "36",
      "title": "Face tracking for Movement SDK for Unity",
      "year": "2024",
      "venue": "Face tracking for Movement SDK for Unity"
    },
    {
      "citation_id": "37",
      "title": "Facial action coding system: A technique for the measurement of facial movement",
      "authors": [
        "P Ekman"
      ],
      "year": "1978",
      "venue": "Facial action coding system: A technique for the measurement of facial movement"
    },
    {
      "citation_id": "38",
      "title": "The Game Experience Questionnaire",
      "authors": [
        "W Ijsselsteijn",
        "Y De Kort",
        "K Poels"
      ],
      "year": "2013",
      "venue": "The Game Experience Questionnaire"
    },
    {
      "citation_id": "39",
      "title": "Virtual Reality Sickness Questionnaire (VRSQ): Motion sickness measurement index in a virtual reality environment",
      "authors": [
        "H Kim",
        "J Park",
        "Y Choi",
        "M Choe"
      ],
      "year": "2018",
      "venue": "Applied ergonomics",
      "doi": "10.1016/j.apergo.2017.12.016"
    },
    {
      "citation_id": "40",
      "title": "EfficientNet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "41",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "2021 IEEE 19th International Symposium on Intelligent Systems and Informatics (SISY)",
      "doi": "10.1109/SISY52375.2021.9582508"
    },
    {
      "citation_id": "42",
      "title": "VGGFace2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)",
      "doi": "10.1109/FG.2018.00020"
    },
    {
      "citation_id": "43",
      "title": "ImageNet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein",
        "A Berg",
        "L Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision (IJCV)",
      "doi": "10.1007/s11263-015-0816-y"
    },
    {
      "citation_id": "44",
      "title": "Classifying emotions and engagement in online learning based on a single facial expression recognition neural network",
      "authors": [
        "A Savchenko",
        "L Savchenko",
        "I Makarov"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3188390"
    },
    {
      "citation_id": "45",
      "title": "The Karolinska Directed Emotional Faces: A validation study",
      "authors": [
        "E Goeleven",
        "R Raedt",
        "L Leyman",
        "B Verschuere"
      ],
      "year": "2008",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699930701626582"
    }
  ]
}