{
  "paper_id": "2507.19356v1",
  "title": "Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps Of Asr Transcripts And Speaker Diarization",
  "published": "2025-07-25T15:05:20Z",
  "authors": [
    "Hsuan-Yu Wang",
    "Pei-Ying Lee",
    "Berlin Chen"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Automatic Speech Recognition",
    "Speaker Diarization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we investigate the impact of incorporating timestamp-based alignment between Automatic Speech Recognition (ASR) transcripts and Speaker Diarization (SD) outputs on Speech Emotion Recognition (SER) accuracy. Misalignment between these two modalities often reduces the reliability of multimodal emotion recognition systems, particularly in conversational contexts. To address this issue, we introduce an alignment pipeline utilizing pre-trained ASR and speaker diarization models, systematically synchronizing timestamps to generate accurately labeled speaker segments. Our multimodal approach combines textual embeddings extracted via RoBERTa with audio embeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating mechanism. Experimental evaluations on the IEMOCAP benchmark dataset demonstrate that precise timestamp alignment improves SER accuracy, outperforming baseline methods that lack synchronization. The results highlight the critical importance of temporal alignment, demonstrating its effectiveness in enhancing overall emotion recognition accuracy and providing a foundation for robust multimodal emotion analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) has gained substantial research attention, particularly for its applications in humancomputer interaction. While significant advancements have been made, real-world conversational scenarios pose unique challenges. Traditional SER systems often rely on manually segmented utterances, which are impractical to obtain at scale. Consequently, recent efforts have focused on integrating components like Speaker Diarization (SD) and Automatic Speech Recognition (ASR) to enable more autonomous emotion analysis from raw audio  [1] . However, a fundamental limitation persists within these integrated approaches: the lack of precise temporal synchronization between ASR transcripts and speaker diarization outputs. This inherent misalignment can severely compromise the reliability of multimodal emotion recognition systems, especially in dynamic, turn-taking dialogues.\n\nTo overcome this critical challenge, we introduce a timestamp alignment pipeline that systematically matches ASR transcripts with speaker diarization segments. This process ensures that each transcribed sentence is accurately matched to its speaker and temporal boundaries, yielding high-quality, speaker-attributed utterances. Using this aligned data, our multimodal SER system combines RoBERTa-based textual embeddings  [2]  with wav2vec 2.0-derived acoustic features  [3] , wherein a cross-attention fusion mechanism, augmented by a gating component, integrates these modalities effectively  [4] . Evaluations on the IEMOCAP dataset  [5] demonstrate that precise alignment significantly improves SER accuracy, outperforming baseline systems lacking synchronization. These results emphasize the essential role of alignment in enhancing the robustness and reliability of multimodal emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A recent line of research has addressed the limitations of manual segmentation in Speech Emotion Recognition (SER) by integrating Automatic Speech Recognition (ASR), speaker diarization (SD), and Voice Activity Detection (VAD) into fully automatic pipelines. While such integration improves scalability, it introduces challenges-particularly due to temporal misalignment between outputs from these components.\n\nWhisper  [6] , a self-supervised ASR model with fine-grained timestamping, has facilitated more precise speaker attribution and segment alignment. Building on this,  [7]  demonstrated that even minor misalignments between ASR transcripts and speaker turns can significantly degrade SER performance. These works underscore the importance of synchronization but fall short of providing flexible, modular approaches that split alignment from end-to-end learning.\n\nMeanwhile, a number of works have improved SER through architectural and learning-based innovations.  [8]  introduced a NAS-based framework to jointly optimize CNN and sequential modules, while  [9]  and  [10]  showed that pre-trained ASR features and compact representation learning respectively enhance emotion classification.  [11]  advanced modality fusion via a multi-loss framework, and  [12]  incorporated conversational context in dyadic settings.  [13]  emphasized the need to model temporal dynamics in emotion progression. These efforts contribute to downstream accuracy. However, these works often assume well-aligned inputs and overlook the critical preprocessing stage. Our approach addresses this upstream alignment challenge-an implicit dependency in many SER models.\n\nSpecific attention has also been paid to joint modeling of ASR and SD.  [14]  proposed an end-to-end speaker-attributed Fig.  1 . Proposed Model ASR model, while another approach  [15]  uses encoderdecoder attractors to jointly infer transcripts and speaker turns.  [16]  formally introduced the Speech Emotion Diarization (SED) task and proposed the EDER metric to assess timesensitive accuracy.  [17]  demonstrated that syntactic alignment cues further improve speaker attribution in realistic meetings. While effective, these methods are often tightly coupled or application-specific. In contrast, our modular alignment pipeline can be flexibly integrated with pre-trained ASR and SD models to enable more robust and transferable multimodal SER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Proposed Model",
      "text": "This section sheds light on our proposed alignment-based multimodal SER framework, designed to robustly recognize speech emotions in conversational contexts. Our framework is structured around two core sections: a precise timestampbased alignment pipeline for ASR and SD outputs, and a sophisticated cross-attention fusion mechanism for multimodal embeddings. The overall architecture is schematically depicted in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Asr, Sd, And Alignment Approach",
      "text": "Accurate temporal synchronization between spoken words and their corresponding speakers is essential for reliable multimodal emotion recognition in conversational contexts. Our system begins by processing raw input audio through two independent, state-of-the-art models: WhisperX for ASR  [6]  and Pyannote 3.1 for SD  [18] . WhisperX generates detailed word-level transcripts with precise timestamps (e.g., [6.92s → 7.16s] \"Excuse\", [7.16s → 7.23s] \"me\"), while Pyannote identifies speaker turns by producing timestamped speakerlabeled segments (e.g., [6.92s → 7.24s] Speaker 00). These two outputs serve as the foundation for our alignment procedure, which systematically links the transcribed text to the appropriate speaker segments. The entire process is visualized in Figure  2 .\n\nTo overcome the fragility of sentence-level text matching and mitigate data loss from ASR errors, we developed a robust, time-and speaker-aware alignment pipeline. This process, illustrated in Figure  2 , transforms the word-level ASR and diarization outputs into coherent conversational turns. The procedure unfolds in three main stages: This methodology ensures that the final speaker-attributed segments are resilient to minor ASR transcription errors and accurately reflect the natural flow of a conversation, provid-ing high-quality, contextually rich inputs for the downstream emotion recognition model. Figure  3  demonstrates the impact of our timestamp alignment. The 'Transcript Without Alignment' section illustrates common challenges in systems lacking precise temporal synchronization: fragmented ASR output and potential wrong speaker identification. As seen, the sentence is broken into very short, discrete segments, often leading to incomplete utterances. This fragmentation risks losing crucial contextual information, as the full emotion conveyed might only be apparent when considering the entire thought or conversational turn. Furthermore, short utterances can be mislabeled, as exemplified by a segment incorrectly attributed to another speaker, further corrupting input for emotion analysis.\n\nIn contrast, the 'Transcript With Alignment,' generated by our proposed alignment block, showcases significant improvements. Our process merges these shorter, fragmented ASR outputs based on temporal proximity and speaker consistency, resulting in longer, coherent conversational turns. This extended context allows the model to better capture the temporal evolution of emotions. Importantly, the alignment also corrects speaker misattributions and ensures that each segment is a complete utterance from a single, correctly identified speaker, providing cleaner, more reliable input for subsequent feature extraction.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Bimodal Cross-Attention Fusion And Linear Classifier",
      "text": "Following the precise timestamp alignment, the system proceeds to extract multimodal embeddings from the refined speaker-attributed segments, forming the core of our emotion recognition pipeline, as depicted in Figure  1 . For each aligned segment, its audio waveform is processed by a wav2vec 2.0 model to generate rich, contextualized audio embeddings  [3] . The output of wav2vec 2.0, a sequence of audio features R T ×768 , undergoes mean pooling across the time dimension to yield a fixed-size segment-level audio embedding of R 1×768  [19] .\n\nMeanwhile, the corresponding aligned textual content is fed into a RoBERTa model, which produces contextualized text embeddings  [2] . Similar to wav2vec 2.0, RoBERTa's output sequence R T ′ ×768 is mean-pooled across the token dimension to derive a segment-level text embedding of R 1×768 . These segment-level embeddings are then prepared for the subsequent cross-attention operation. Both audio and text embeddings are then unsqueezed to R 1×1×768 to ensure compatibility for the subsequent cross-attention operation. These processed audio and text embeddings serve as the inputs to our multimodal fusion strategy, which aims to leverage the complementary strengths of both modalities for robust emotion recognition. The final stages involve a fusion layer and a linear classifier to predict the emotion.\n\nThe extracted audio and text embeddings are then integrated within a Cross-Attention Fusion Block to enable inter-modal information exchange. Our model explicitly adopts the crossmodality gated attention fusion approach proposed by  [4] . While their original work focused on fusing three modalities (text, audio, and video), we adapt this mechanism for our twomodality setup (text and audio). This block utilizes Multi-Head Attention (MHA) to allow each modality to selectively attend to relevant parts of the other: A symmetric process yields h(a, t).\n\nThis mechanism helps focus on the most salient multimodal features while mitigating noise. The output of the forget gate is a gated representation of dimension R 1×2×768 , combining the refined multimodal features. Subsequently, this gated representation is fed into a Fusion Layer, implemented as a Transformer-based architecture. Inside this layer, which is implemented as a Transformer-based architecture, the features are first concatenated along the sequence dimension to form a combined representation of R 1×2×768 . The output is then mean-pooled across its second dimension to condense it into a single R 1×768 vector representing the final fused embedding. Finally, this fused embedding is passed to a linear classifier. This classifier maps the high-dimensional representation to the output space of emotion categories via a linear transformation (i.e., Dim = 768 → Classes), followed by a Softmax activation function to yield a probability distribution over emotion classes. The emotion with the highest probability (determined by arg max) is selected as the final prediction.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "This section details the experimental methodology used to evaluate our proposed multimodal SER framework, including the dataset, various training strategies, and the evaluation metrics employed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset",
      "text": "Our experiments were conducted on the IEMOCAP dataset  [5] , a widely used benchmark for emotion recognition research, comprising approximately 12 hours of audio-visual recordings of dyadic conversations between actors in both improvised and scripted scenarios. A key feature of IEMOCAP is its rich annotation, with each conversational turn labeled with speaker ID and categorical emotion. For this study, we focused exclusively on the four primary emotion categories commonly recognized in the literature: happy, sad, angry, and neutral. This dataset provides a challenging yet representative environment for evaluating multimodal emotion recognition systems in conversational contexts due to its realistic interactions and the presence of multiple speakers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Training Strategies",
      "text": "To thoroughly assess the impact of our proposed timestamp alignment and multimodal fusion approach, we designed several distinct training strategies:\n\n• Full Proposed Model (with Alignment): This configuration represented our complete framework, where the input audio underwent the full timestamp alignment pipeline (as detailed in Section 3.1) to generate precisely attributed speaker segments. These aligned segments were then processed by the wav2vec 2.0 and RoBERTa feature extractors, followed by the cross-attention fusion block (with the forget gate), fusion layer, and finally the linear classifier. This setup was expected to demonstrate the benefits of accurate temporal synchronization. • Model Without Alignment (Baseline): To highlight the critical role of our alignment pipeline, we established a baseline model that omits this preprocessing step. In this configuration, ASR transcripts (e.g., from WhisperX) and raw speaker segments (from Pyannote) were used with a simpler, less precise method of association (e.g., overlap at the segment level without fine-grained word-level attribution or sentence reconstruction). The multimodal fusion architecture remained the same, allowing us to isolate the performance gains attributable solely to the timestamp alignment. • Freezing Embedding Extractors: For both the with alignment and without alignment setups, we conducted experiments where the pre-trained wav2vec 2.0 and RoBERTa embedding extractors were frozen during training. This strategy prevented fine-tuning of these large foundation models and primarily evaluated the capacity of the crossattention fusion and subsequent layers to learn effective multimodal representations from fixed, high-quality embeddings. This facilitated to understand whether the performance gains were due to the fusion mechanism itself or also involved adaptive fine-tuning of the feature extractors. • Fine-tuning Embedding Extractors Conversely, in another set of experiments, the wav2vec 2.0 and RoBERTa models were fine-tuned along with the rest of the network. This allowed the feature extractors to adapt their representations specifically for the SER task and the nuances of the IEMOCAP dataset, potentially leading to higher overall accuracy.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Evaluation Metrics",
      "text": "To rigorously evaluate the performance of our models, we employ standard classification metrics, as well as specialized metrics suited for SER in conversational contexts. Performance is primarily measured using:\n\n• Weighted Average Recall (WAR / Accuracy): Represents the overall classification accuracy across all emotion categories, weighted by the number of samples in each class.\n\nFurthermore, acknowledging the complexities of evaluating emotion recognition on automatically segmented and diarized speech, we adopt the advanced metrics proposed by  [1] :\n\n• Time-weighted Emotion Error Rate (TEER): A duration-aware metric that penalizes missed speech, false alarms, and emotion misclassifications. This provides a more ecologically valid measure of SER performance in continuous speech.\n\n• Speaker-Attributed TEER (sTEER): Extends TEER by incorporating speaker attribution errors. It penalizes both incorrect emotion labels and incorrect speaker assignments, directly assessing the quality of automatic speakeremotion alignment.\n\nBy utilizing these comprehensive metrics, we aim to provide a thorough and nuanced assessment of the effectiveness of our model, particularly in addressing the challenges of real-world, speaker-rich conversational SER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Experimental Results",
      "text": "As shown in TABLE I, our proposed model, incorporating the timestamp alignment pipeline, achieves an overall accuracy of 66.81%. This represents a notable improvement over the \"Without Alignment\" baseline, which achieves an accuracy of 56.82%. This difference, while seemingly modest in raw accuracy, signifies the benefit of precise temporal synchronization. More importantly, the Weighted F1-score for our proposed model increases substantially from 53.87% to 66.81%, and the Macro F1-score sees a significant leap from 47.10% to 66.48%. The substantial improvement in Macro F1-score is particularly encouraging, as it indicates a much better balance in performance across all emotion categories, especially for minority classes. A key finding from our per-emotion analysis is the significant improvement in recognizing the \"sad\" category. As shown in TABLE II, the F1-score for \"sadness\" increases massively from 0.26 to 0.67 after implementing our alignment pipeline. This substantial gain highlights the critical role of temporal context for certain emotions. This result aligns with existing research indicating that emotions like sadness are often characterized by subtle acoustic cues expressed over longer durations, requiring a broader temporal window for accurate recognition  [12]    [13] . Without proper alignment, conversational turns can be fragmented, breaking these essential long-term dependencies and causing the model to miss the defining features of sadness. Our alignment pipeline directly remedies this by reconstructing coherent, complete utterances. By providing the model with the full emotional expression, it is better equipped to capture the subtle and extended patterns characteristic of sadness, distinguishing it more effectively from other low-arousal states. To further assess the impact of accurate segmentation on overall system performance, we compare our standard pipeline with a VAD-Oracle configuration, in which perfect groundtruth segment boundaries are provided and the internal VAD is disabled. As shown in Table  III , removing the influence of VAD yields a massive improvement: TEER drops from 89.35 to 53.9%, and sTEER from 90.96 to 62.54%. These results demonstrate that when segmentation quality is no longer a limiting factor, our alignment and speaker-attribution components are capable of operating near their optimal capacity. Compared to the baseline established in  [1] , which reported TEER and sTEER of 66.03% and 65.17% respectively, our VAD-Oracle pipeline achieves lower errors across both metrics. This suggests that, while VAD remains a major bottleneck in practical systems, our alignment pipeline-when being free from segmentation noise-can outperform previously reported state-of-the-art results. To further analyze the contribution of different components, we investigated the effect of fine-tuning the pre-trained wav2vec 2.0 and RoBERTa embedding extractors. As shown in Table  IV , when the embedding extractors are frozen (i.e., their weights are kept fixed during training), our model achieves an accuracy of 56.82%. In contrast, allowing the embedding extractors to be fine-tuned alongside the rest of the network leads to a notable increase in overall accuracy to 66.81%. This improvement indicates that while the pre-trained wav2vec 2.0 and RoBERTa models provide strong general-purpose representations, enabling them to adapt their weights specifically to the nuances of the IEMOCAP dataset and the SER task significantly enhances overall performance. The ability of the fine-tuned extractors to learn more task-relevant features directly contributes to the improved classification accuracy. While specific F1-scores for the \"frozen\" setup are not detailed per emotion in the presented tables, the overall accuracy difference strongly suggests that fine-tuning also leads to better balanced performance across classes, implying improvements in both Weighted and Macro F1-scores akin to the gains observed with the alignment strategy. This highlights that optimized feature extraction, through fine-tuning, complements the benefits derived from our precise timestamp alignment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "In this paper, we have put forward and validated a systematic timestamp alignment pipeline designed to resolve synchronization issues and reconstruct contextual coherence from the outputs of independent ASR and speaker diarization models for multimodal SER. Our core contribution is a multistage process that segments transcripts into meaningful sentences, attributes speakers at a sentence level, and merges them into conversational turns, thereby providing a high-quality, analysis-ready input for downstream tasks.\n\nTo demonstrate the efficacy of our alignment pipeline, we have evaluated its impact on a representative cross-attention fusion architecture. The results conclusively affirm our hypothesis: the model trained on data processed by our alignment pipeline significantly outperforms the baseline that used a simpler association method. This is most evident in the substantial increase of the Macro F1-score from 47.10% to 66.48%, driven by a huge improvement in recognizing contextdependent emotions like \"sadness\" (F1-score improved from 0.26 to 0.67). This finding underscores that for complex conversational tasks like SER, meticulous front-end data processing is not merely a preliminary step but a fundamental determinant of model performance. Our work illustrates that the quality and contextual integrity of input segments are as crucial as the sophistication of the downstream fusion architecture. While this study focuses on IEMOCAP due to its detailed annotations and widespread adoption, future work will extend to multilingual and in-the-wild datasets to evaluate cross-domain generalization. We also plan to refine the turn re-segmentation logic, fine-tune the VAD component for improved boundary accuracy, and systematically assess the robustness of the proposed alignment pipeline across diverse multimodal fusion models.",
      "page_start": 5,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed Model",
      "page": 2
    },
    {
      "caption": "Figure 1: A. ASR, SD, and Alignment Approach",
      "page": 2
    },
    {
      "caption": "Figure 2: To overcome the fragility of sentence-level text matching",
      "page": 2
    },
    {
      "caption": "Figure 2: , transforms the word-level ASR and",
      "page": 2
    },
    {
      "caption": "Figure 2: Timestamp Alignment Block",
      "page": 2
    },
    {
      "caption": "Figure 3: Transcript Comparison: without vs. with Alignment",
      "page": 3
    },
    {
      "caption": "Figure 3: demonstrates the impact of our timestamp align-",
      "page": 3
    },
    {
      "caption": "Figure 1: For each aligned",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Setup": "Wu et.al\n[1]\n(on 6 emotion categories)",
          "Accuracy (%)": "49.49",
          "Weighted\nF1-Score": "-",
          "Macro\nF1-Score": "-"
        },
        {
          "Setup": "Without Alignment",
          "Accuracy (%)": "56.82",
          "Weighted\nF1-Score": "53.87",
          "Macro\nF1-Score": "47.10"
        },
        {
          "Setup": "Proposed Approach",
          "Accuracy (%)": "66.81",
          "Weighted\nF1-Score": "66.88",
          "Macro\nF1-Score": "66.48"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "happy\nangry\nsad\nneutral",
          "F1-score\n(Without Alignment)": "0.64\n0.38\n0.26\n0.61",
          "F1-score\n(With Alignment)": "0.73\n0.64\n0.67\n0.62"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Integrating emotion recognition with speech recognition and speaker diarisation for conversations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "2",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "4",
      "title": "Cross-modality gated attention fusion for multimodal sentiment analysis",
      "authors": [
        "M Jiang",
        "S Ji"
      ],
      "year": "2022",
      "venue": "Cross-modality gated attention fusion for multimodal sentiment analysis",
      "arxiv": "arXiv:2208.11893"
    },
    {
      "citation_id": "5",
      "title": "IEMOCAP: In-teractive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "6",
      "title": "WhisperX: Time accurate speech transcription of long form audio",
      "authors": [
        "M Bain",
        "J Huh",
        "T Han",
        "A Zisserman"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2023-78"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition with ASR transcripts: A comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "venue": "Proc. SLT",
      "doi": "10.1109/SLT61566.2024.10832143"
    },
    {
      "citation_id": "8",
      "title": "emoDARTS: Joint optimisation of CNN and sequential neural network architectures for superior speech emotion recognition",
      "authors": [
        "Thejan Rajapakshe",
        "Rajib Rana",
        "Sara Khalifa",
        "Berrak Sisman",
        "Bjorn Schuller",
        "Carlos Busso"
      ],
      "year": "2024",
      "venue": "emoDARTS: Joint optimisation of CNN and sequential neural network architectures for superior speech emotion recognition",
      "arxiv": "arXiv:2403.14083"
    },
    {
      "citation_id": "9",
      "title": "Speech sentiment analysis via pretrained features from end-to-end ASR models",
      "authors": [
        "W Lu",
        "Y Chen",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "authors": [
        "Z Li",
        "Y Wang",
        "T Chen"
      ],
      "year": "2025",
      "venue": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "arxiv": "arXiv:2501.01103"
    },
    {
      "citation_id": "11",
      "title": "Multimodal multi-loss fusion network for sentiment analysis",
      "authors": [
        "Z Wu",
        "Z Gong",
        "J Koo",
        "J Hirschberg"
      ],
      "year": "2024",
      "venue": "Proc. NAACL HLT"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition in dyadic dialogues with attentive interaction modeling",
      "authors": [
        "J Zhao",
        "S Chen",
        "J Liang",
        "Q Jin"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Temporal context in speech emotion recognition",
      "authors": [
        "Yangyang Xia",
        "Li-Wei Chen",
        "Alexander Rudnicky",
        "Richard Stern"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Transcribe-to-diarize: Neural speaker diarization for unlimited number of speakers using end-to-end speaker-attributed ASR",
      "authors": [
        "Naoyuki Kanda",
        "Xiong Xiao",
        "Yashesh Gaur",
        "Xiaofei Wang",
        "Zhong Meng",
        "Zhuo Chen",
        "Takuya Yoshioka"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Streaming speaker-attributed ASR with token-level speaker embeddings",
      "authors": [
        "N Kanda",
        "J Wu",
        "Y Wu",
        "X Xiao",
        "Z Meng",
        "X Wang",
        "Y Gaur",
        "Z Chen",
        "J Li",
        "T Yoshioka"
      ],
      "venue": "Streaming speaker-attributed ASR with token-level speaker embeddings",
      "arxiv": "arXiv:2203.16685.2022"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion diarization: Which emotion appears when?",
      "authors": [
        "Y Wang",
        "M Ravanelli",
        "A Yacoubi"
      ],
      "year": "2023",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "17",
      "title": "Meeting recognition with continuous speech separation and transcription-supported diarization",
      "authors": [
        "Christoph Thilo Von Neumann",
        "Tobias Boeddeker",
        "Marc Cord-Landwehr",
        "Reinhold Delcroix",
        "Haeb-Umbach"
      ],
      "year": "2024",
      "venue": "Proc. HSCMA Workshop at ICASSP"
    },
    {
      "citation_id": "18",
      "title": "pyannote.audio: neural building blocks for speaker diarization",
      "authors": [
        "Hervé Bredin",
        "Ruiqing Yin",
        "Juan Manuel Coria",
        "Gregory Gelly",
        "Pavel Korshunov",
        "Marvin Lavechin",
        "Diego Fustes",
        "Hadrien Titeux",
        "Wassim Bouaziz",
        "Marie-Philippe Gill"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "arxiv": "arXiv:1908.10084"
    }
  ]
}