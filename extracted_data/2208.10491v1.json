{
  "paper_id": "2208.10491v1",
  "title": "Improving Speech Emotion Recognition Through Focus And Calibration Attention Mechanisms",
  "published": "2022-08-21T08:04:22Z",
  "authors": [
    "Junghun Kim",
    "Yoojin An",
    "Jihie Kim"
  ],
  "keywords": [
    "speech recognition",
    "emotion",
    "attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Attention has become one of the most commonly used mechanisms in deep learning approaches. The attention mechanism can help the system focus more on the feature space's critical regions. For example, high amplitude regions can play an important role for Speech Emotion Recognition (SER). In this paper, we identify misalignments between the attention and the signal amplitude in the existing multi-head self-attention. To improve the attention area, we propose to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA) mechanism in combination with the multi-head self-attention. Through the FA mechanism, the network can detect the largest amplitude part in the segment. By employing the CA mechanism, the network can modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts. To evaluate the proposed method, experiments are performed with the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed framework significantly outperforms the state-of-the-art approaches on both datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The field of speech recognition has been spotlighted as a promising field of research with the rapid development of Automatic Speech Recognition (ASR). In particular, Speech Emotion Recognition (SER) is one of the most critical technologies for criminal investigation, medical treatment, real-time sentiment analysis, etc. In light-weight SER applications such as real-time emotion recognition on mobile devices, models with speech modality only are more desirable than heavier multimodal (text and speech) models. We aim at improving SER accuracy for such tasks. Feature sets appropriated for SER have been researched and used in INTERSPEECH 2009 (IS09) Emotion Challenge  [1] , AVEC Challenge  [2]  and extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [3] . These feature sets are usually hand-crafted Low-Level Descriptors (LLDs). However, thanks to the development of deep learning models, recent studies  [4, 5, 6, 7]  effectively use task-specific features extracted directly from the amplitude-related features such as spectrograms or Mel Frequency Cepstral Coefficients (MFCCs). Following the recent studies, we use MFCCs for our end-to-end approach.\n\nAlthough deep learning approaches provide good performance, the results are difficult to understand or explain as they provide black-box models. A way to mitigate the shortcoming is the use of attention mechanisms. The attention maps extracted from the reasoning process help understand the attention distribution across the feature space. Also, the attention mechanism can improve the performance by focusing more on the feature space's critical regions. Since images and texts can be intuitively analyzed with attention distribution, many studies in computer vision and natural language processing  [8, 9, 10, 11]  have been conducted to improve attention performance by focusing more on the critical regions and adjusting misalignments. However, the critical regions in speech signals are not as intuitive as images or texts, making it difficult to understand and analyze the attention maps. In SER,  [12]  showed that the highest amplitude plays an important role in emotion recognition performance because each emotion has a different decibel level. Based on the study, we hypothesized that detecting the largest amplitude and learning the associated representations can improve emotion recognition performance. In experiments, we evaluate the hypothesis by visualizing the attention map and showing the performance improvement based on the attention alignment adjustment.\n\nWe first explore whether multi-head self-attention  [13] , one of the most commonly used attention mechanisms in speech recognition, detects the critical region (the highest amplitude) in SER. In the exploration, we identify misalignments between the attention and the amplitude through a visualization of the attention map in the existing multi-head self-attention. To alleviate the attention maps' misalignment and improve the performance, in this paper, we propose a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA) mechanism. The FA mechanism used in the document summary task  [14]  employs a Gaussian distribution to focus on the areas with salient information. In this work, FA plays a role in detecting the largest amplitude part in the segment. There are multiple attention alignments due to the multi-head self-attention, and not all attention heads have an appropriate alignment. We propose the new CA mechanism to modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts.\n\nIn the experiments, we use IEMOCAP  [15]  and RAVDESS  [16]  datasets, which are popular SER benchmark datasets, and we demonstrate that our approach achieves state-of-the-art results. Then we visualize the proposed methods' attention map and demonstrate that adjusting misalignments can improve SER performance.\n\nOur contributions are as follows:\n\n• We apply a focus-attention mechanism in SER to detect the largest amplitude part in the segment.\n\n• We propose a novel calibration-attention mechanism to modulate the information flow and improve the utilization of surrounding contexts.\n\n• In experiments, we achieve state-of-the-art performance with significant improvement over the existing state-ofthe-art approaches on both IEMOCAP and RAVDESS datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ser Models",
      "text": "At present, an attention mechanism is being used in almost every deep learning application. The attention mechanism can improve the performance by focusing more on the feature space's critical regions. Attentions can also mitigate the black box problem, which is a disadvantage of the deep learning model structure, to some extent. In the field of speech recognition, the attention mechanism has been used in tasks such as ASR  [17, 18]  and SER  [19, 20, 21] . In particular, after  [13]  showed a strong performance of self-attention in the machine translation task, self-attention was also attempted to assign more weight to the critical regions in the SER task  [6, 22]  and proved its effectiveness.  [6]  used the self-attention mechanism to focus on the emotion that appears in a particular part, not in the whole utterance.  [22]  combined Dilated Residual Network (DRN) and multihead self-attention to enhance the importing of emotion-salient information, and we denote this model as DRN-MHSA in this paper. Other state-of-the-art models in SER are as follows. Audio-BRE  [23]  uses a Bidirectional Recurrent Encoder (BRE) model based on the Long Short-Term Memory (LSTM) using MFCCs. A-DCNN  [24]  uses a SincNet filter layer  [25]  to learn custom filter banks from speech audio. Unlike  [6, 22]  using existing self-attention, we propose an improved self-attention using FA and CA mechanisms and verify the assumption that detecting the largest amplitude and learning expressive representations improves the emotion recognition performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Approach",
      "text": "In this paper, we propose to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA) mechanism in combination with multi-head self-attention. The FA plays a role in detecting the salient information, and the CA can modulate the information. This model architecture can improve the selfattention by learning better attention distribution in SER. The overall architecture of our model is illustrated in Fig.  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Head Self-Attention",
      "text": "As with the concept of  [13] , the elements of the input sequences X = (x1, x2, ..., xm) are projected onto three different representations (query, key, and value) through a linear function. We define three representations as query Q ∈ R m×dm , key K ∈ R m×dm , and value V ∈ R m×dm . Self-attention is ob-tained by a scaled-dot product with query, key, and value:\n\nwhere dm is the dimension of linear projection output. Instead of performing a self-attention once with dmdimensional query, key, and value, performing a self-attention h times in parallel with d h = dm/h dimensional queries, keys, and values can jointly attend to information from different representation subspaces. Each calculated attention heads are concatenated and once again projected.\n\nwhere Hi ∈ R dm×d h is i-th attention head,\n\nare the learnable weight matrices.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Focus-Attention Mechanism",
      "text": "A Focus-Attention (FA) mechanism was used within selfattention sub-layers to obtain salient information during encoding for the document summary task  [14] . In our work, the FA mechanism detects the largest amplitude part as salient information in the segment during the encoding. This mechanism models a focal bias, which is the regularization term on attention score determined by the center position scalar and the coverage scope scalar. At the i-th sequence step, the center position scalar µi ∈ R and the coverage scope scalar σi ∈ R are calculated through two linear projection processes:\n\nwhere Wp ∈ R dm×dm and Wg ∈ R dm×dm are learnable shared weight matrices, Uc ∈ R dm and U d ∈ R dm are learnable weight vectors. G = 1 m m i=1 Qi ∈ R dm is the mean vector providing complementary information. In addition, we regulate the µi and σi to values [0, m].\n\nThe focal bias fi,j ∈ R is obtained and added to the attention before the softmax calculation.\n\nwhere i, j ∈ {1, 2, ..., m}. Pj ∈ R is the absolute position of the coefficient vector xj in the MFCCs and ⊕ denotes elementwise summation. Moreover, we further adapt the FA mechanism into the multi-head manner as in Eq. 2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Calibration-Attention Mechanism",
      "text": "There are multiple attention alignments due to the multi-head self-attention, and not all attention heads have an appropriate alignment. Therefore, we need to give more weight to the attention head with the appropriate alignment. We propose a novel Calibration-Attention (CA) mechanism to modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts.\n\nThe following is the calibration process to modulate the information flow by obtaining calibration information g ∈ R h and calibration score s ∈ R h . First, calibration information g is obtained by using Global Max Pooling (GMP) from the original attention head. When this calibration information g passes through Fully Connected (FC) layer using a nonlinear function, the calibration score s is obtained. This calibration score s has a value between 0 and 1. Finally, the information flow is modulated by multiplying the calibration score s with the original attention head.\n\nwhere H o ∈ R h×m×m and H s ∈ R h×m×m are attention head before / after calibration, respectively, Ws ∈ R h×h and bs ∈ R h are the weight matrix and bias vector. As shown in Fig.  1 , first, we stack two 1D Convolutional layers with a kernel size of (3, 1) and a channel size of 64. Each Convolutional layer is followed by batch normalization and max-pooling with kernel and stride size  (2, 1) . Then, we stack two bidirectional LSTM layers with a hidden size of 256. The LSTM layer's output is connected by multi-head self-attention with a head size of 8, and finally, two FC layers are stacked. The FA mechanism is added before the softmax calculation in the scaled dot-product process, and CA mechanism operates before the multi-head is concatenated.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Model Architecture",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Experimental Setup",
      "text": "Dataset. We choose IEMOCAP  [15]  and RAVDESS  [16]  datasets, which are popular in SER, for the research of emotion classification. In IEMOCAP, following the previous research method, we add 'exciting' class to happy class and use four classes {happiness, anger, sadness, and neutral}, including each {1636, 1103, 1084, 1708} utterances. RAVDESS contains 8 classes {calmness, happiness, sadness, anger, fear, surprise, disgust, and neutral}. To compare the performance with the previous approaches under the same condition, we use the same 10fold and 5-fold cross-validation in IEMOCAP and RAVDESS, where 8, 1, 1 folds and 3, 1, 1 folds are train set, validation set, and test set, respectively. Training. Cross-entropy is employed as a loss function, and the Adam optimizer  [27]  with a learning rate of 3e-4 is employed. We train for 100 epochs and use a batch size of 128. Metrics. Weighted accuracy (WA) and unweighted accuracy (UA) are used to assess the model performance. Following the recent studies  [22, 23, 25, 28, 29, 30, 31] , we use the averages from the 10-fold and 5-fold cross-validation as experimental results of IEMOCAP and RAVDESS, respectively. Baselines. The state-of-the-art models compared with our model are as follows: Audio-BRE  [23] , DRN-MHSA  [22] , A-DCNN  [24] , and Audio-CNN-xvector  [28]  in IEMOCAP; Deep-CNN  [29] , Head Fusion  [30] , and QCNN  [31]  in RAVDESS.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance Evaluation",
      "text": "We compare our model with the state-of-the-art models evaluated in the same metric setting (the most commonly used 10fold and 5-fold cross-validation in IEMOCAP and RAVDESS, respectively) and speech modality. The basic multi-head selfattention model is denoted as BMHSA, the model with the FA mechanism added to BMHSA as MHSA-FA, and the model with CA mechanism added to MHSA-FA as MHSA-FACA. Table  1  shows the performance of MHSA-FACA compared with state-of-the-art models in IEMOCAP. Our model outperforms the state-of-the-art models by at least 2.21% in WA and at least 4.43% in UA. Table  2  shows comparison results on RAVDESS. MHSA-FACA outperforms the state-of-the-art models by at least 5.05% in WA and at least 4.6% in UA.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model",
      "text": "WA(%) UA(%) audio-BRE  [23]  64.60 65.20 DRN-MHSA  [22]  -67.40 A-DCNN  [24]  69.80 -Audio-CNN-xvector  [28]  66.60 68.40 MHSA-FACA (ours) 72.01 72.83\n\nTable  2 : Comparison results on RAVDESS.\n\nModel WA(%) UA(%) Deep-CNN  [29]  -71.67 Head Fusion  [30]  77.80 77.40 QCNN  [31]  -77.87 MHSA-FACA (ours) 82.85 82.47",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "We compare the performances according to the proposed module combination in IEMOCAP and show the results in Table  3 . MHSA-FA improves performance by 1.9% in WA and 1.78% in UA over BMHSA. Also, MHSA-FACA brings additional performance improvements of 0.45% in WA and 0.53% in UA compared with MHSA-FA.\n\nTo see how the proposed method changes attention distribution, the three models' attention maps are visualized and shown in Fig.  2 . The utterances are divided into 0.5 sec (50 sequences in the attention map) segments. The segments have a hop of 0.1 sec (10 sequences in the attention map) for the sliding. The largest amplitude in the raw waveform in Fig.  2 (a ) is represented by a red line, corresponding to in Fig.  2  (b) a sequence of 30 in the orange segment, 20 in the yellow segment, and 10 in the green segment, respectively. In the each attention map, the vertical direction means the query axis and the horizontal one is the key axis. BMHSA has failed to detect the largest amplitude and shows no particular pattern in the attention map. MHSA-FA detects the largest amplitude. Therefore, in each query, the attention is focused on the key corresponding to the largest amplitude. MHSA-FACA improves the utilization of surrounding contexts. If it is not a query of the largest amplitude, the attention is focused on the key corresponding to the largest amplitude. However, if it is a query of the largest amplitude, the attention is focused on the surrounding key instead of the largest amplitude, as shown as red boxes in Fig.  2  (b). Thus, MHSA-FACA seems to make use context information better in the inference process than MHSA-FA. As we predicted that detecting the largest amplitude and learning expressive representations improves emotion recognition performance, the model actually shows a significant performance improvement when it detects the largest amplitude. Also, when the model makes use of the surrounding context, it shows an additional performance improvement.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a focus-attention mechanism and a novel calibration-attention mechanism to improve self-attention in SER by alleviating the attention maps' misalignment. Using the focus-attention mechanism, the network can detect the largest amplitude part in the segment. Employing the calibration-attention mechanism, the network can modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts. The proposed framework achieves state-of-the-art performance with significant improvement over the existing approaches.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 3.1. Multi-Head Self-Attention",
      "page": 2
    },
    {
      "caption": "Figure 1: Network modules and the overall architecture.",
      "page": 3
    },
    {
      "caption": "Figure 1: , ﬁrst, we stack two 1D Convolutional",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) is the raw waveform of the utterance. The utterance is divided into 0.5-second segments, with each segment marked in",
      "page": 4
    },
    {
      "caption": "Figure 2: The utterances are divided into 0.5 sec (50 sequences",
      "page": 4
    },
    {
      "caption": "Figure 2: (a) is repre-",
      "page": 4
    },
    {
      "caption": "Figure 2: (b) a sequence",
      "page": 4
    },
    {
      "caption": "Figure 2: (b). Thus,",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "audio-BRE [23]\nDRN-MHSA [22]\nA-DCNN [24]\nAudio-CNN-xvector [28]",
          "WA(%)": "64.60\n-\n69.80\n66.60",
          "UA(%)": "65.20\n67.40\n-\n68.40"
        },
        {
          "Model": "MHSA-FACA (ours)",
          "WA(%)": "72.01",
          "UA(%)": "72.83"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "ISCA"
    },
    {
      "citation_id": "3",
      "title": "Av+ ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "S Jaiswal",
        "E Marchi",
        "D Lalanne",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "4",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Interspeech. ISCA"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Normalized and geometry-aware self-attention network for image captioning",
      "authors": [
        "L Guo",
        "J Liu",
        "X Zhu",
        "P Yao",
        "S Lu",
        "H Lu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Squeeze-and-attention networks for semantic segmentation",
      "authors": [
        "Z Zhong",
        "Z Lin",
        "R Bidart",
        "X Hu",
        "I Daya",
        "Z Li",
        "W.-S Zheng",
        "J Li",
        "A Wong"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "How does selective mechanism improve self-attention networks",
      "authors": [
        "X Geng",
        "L Wang",
        "X Wang",
        "B Qin",
        "T Liu",
        "Z Tu"
      ],
      "year": "2020",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "Guiding attention for selfsupervised learning with transformers",
      "authors": [
        "A Deshpande",
        "K Narasimhan"
      ],
      "year": "2020",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing: Findings"
    },
    {
      "citation_id": "13",
      "title": "Segment repetition based on high amplitude to enhance a speech emotion recognition",
      "authors": [
        "B Prayitno",
        "S Suyanto"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "14",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "Improving abstractive document summarization with salient information modeling",
      "authors": [
        "Y You",
        "W Jia",
        "T Liu",
        "W Yang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "17",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "A comparison of sequence-to-sequence models for speech recognition",
      "authors": [
        "R Prabhavalkar",
        "K Rao",
        "T Sainath",
        "B Li",
        "L Johnson",
        "N Jaitly"
      ],
      "year": "2017",
      "venue": "Interspeech. ISCA"
    },
    {
      "citation_id": "19",
      "title": "Streaming automatic speech recognition with the transformer model",
      "authors": [
        "N Moritz",
        "T Hori",
        "J Le"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L.-R Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "21",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "22",
      "title": "Attention-enhanced connectionist temporal classification for discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Interspeech. ISCA"
    },
    {
      "citation_id": "23",
      "title": "Dilated residual network with multi-head self-attention for speech emotion recognition",
      "authors": [
        "R Li",
        "Z Wu",
        "J Jia",
        "S Zhao",
        "H Meng"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Attention driven fusion for multi-modal emotion recognition",
      "authors": [
        "D Priyasad",
        "T Fernando",
        "S Denman",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "27",
      "title": "Convolutional, long short-term memory, fully connected deep neural networks",
      "authors": [
        "T Sainath",
        "O Vinyals",
        "A Senior",
        "H Sak"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "29",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "31",
      "title": "Head fusion: Improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset",
      "authors": [
        "M Xu",
        "F Zhang",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition using quaternion convolutional neural networks",
      "authors": [
        "A Muppidi",
        "M Radfar"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}