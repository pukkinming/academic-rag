{
  "paper_id": "2509.05634v1",
  "title": "On The Contribution Of Lexical Features To Speech Emotion Recognition",
  "published": "2025-09-06T07:40:27Z",
  "authors": [
    "David Combei"
  ],
  "keywords": [
    "speech emotion recognition",
    "acoustic features",
    "lexical features",
    "SSL"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although paralinguistic cues are often considered the primary drivers of speech emotion recognition (SER), we investigate the role of lexical content extracted from speech and show that it can achieve competitive-and in some cases higher-performance compared to acoustic models. On the MELD dataset, our lexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to 49.3% for an acousticonly pipeline with a larger parameter count. Furthermore, we analyze different self-supervised (SSL) speech and text representations, conduct a layer-wise study of transformer-based encoders, and evaluate the effect of audio denoising.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) has gained considerable interest in both research and industry, driven by its potential to improve human-computer interaction and health analytics  [1] . SER applications range from emotion-aware virtual assistants and customer service bots to clinical monitoring and personalized user interfaces  [2] . SER approaches rely on paralinguistic (acoustic) information, on how something is said, rather than the explicit lexical content of an audio sample. A variety of acoustic features are extracted from speech signals using classic speech processing techniques to serve as training or evaluation data for a machine learning task. These include acoustic features like fundamental frequency, energy, vocal quality indicators, formant frequencies and frequency-domain representations such as Mel-Frequency Cepstral Coefficients (MFCCs) or Linear Predictive Cepstral Coefficients (LPCC)  [3] ,  [4] . While paralinguistic features are considered to be crucial, relying only on acoustics has its limitations. Studies have shown that acoustic-based SER models often yield suboptimal accuracy, especially in real-life settings  [5] -  [7] . In this work, we challenge the long-standing paralinguistic paradigm which held that \"how you say it\" dominates \"what you say\" in detecting emotions using AI. In real-world conditions, both modalities carry weight. This paper embraces that perspective and aims to systematically examine the contributions of spoken words versus prosodic cues in emotion recognition. By leveraging natural emotional speech data, we seek to determine how much the words themselves contribute to detecting emotions, and whether relying only on lexical information can further push the boundaries of SER in real-world scenarios. To summarize, our contributions are as follows:\n\n• We benchmark different SSL models.\n\n• We show that the classifier does not need to be complex and the SSL derived representations are powerful enough.   [9] , a transformer framework explicitly incorporating hierarchical attention in speech by processing frames, phonemes, words, and utterances in succession while keeping temporal information across them, which enabled fine-grained attention modeling and significantly reduced computational costs compared to standard transformers at that time. Building upon this idea, SpeechFormer++  [10]  followed and further optimized hierarchical attention, being 60% faster for paralinguistic tasks on MELD, demonstrating superior performance and inference time. Other approaches have investigated deformable and dynamic attention mechanisms to better handle temporal variability in speech. Deformable Speech Transformer (DST) introduced by Chen et al.  [6]  utilized learnable window positioning to adaptively locate emotionally relevant regions within speech signals, improving robustness on MELD. Similarly, DWFormer by Chen et al.  [8]  proposed dynamic windowing strategies with cross-window information interaction, allowing more precise temporal localization of emotional expressions, which is crucial for the multi-turn dialogues in MELD.\n\nMore recently, self-supervised representations have become central in most audio tasks, SER being no exception. Zhao et al. introduced TF-Mamba  [7] , which integrated temporalfrequency state-space modeling to jointly capture temporal and spectral emotional features from self-supervised representations and has an ablation study, comparing different frontends. Their method outperformed conventional SER models in terms of efficiency and latency on MELD, but DST still yields the state-of-the-art score. These methods highlight the trend of designing efficient and expressive architectures that balance fine-grained temporal dynamics with practical inference speed which are key challenges posed by real-world datasets like MELD. Beyond purely acoustic modeling, Atmaja et al.  [5]  conducted a comprehensive survey on bimodal emotion recognition, emphasizing the value of fusing acoustic and lexical features for MELD and similar datasets. This is aligned with older but foundational studies such as Rozgic et al.  [3] , who demonstrated that combining speech acoustics with text transcriptions improves emotion classification performance on these datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Dataset Description",
      "text": "The Multimodal EmotionLines Dataset (MELD)  [11]  has become a pivotal benchmark for emotion recognition research, providing multi-speaker conversations annotated with emotional states across text, audio, and video modalities. MELD enables evaluation models in real-world conditions. It contains seven emotions (anger, disgust, fear, joy, neutral, sadness and surprise) assigned by majority voting out of five workers assigned to annotate the utterances. The average duration of an utterance is 3.59 seconds. MELD has three partitions: train, development, and test. In our experiments (Table  I  and Table  II ) the reported results are on the development set and the best performing models on this partition were used to compute the final scores on the test one, with the results presented in Table  III .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Self-Supervised Models",
      "text": "Our approach relies on the self-supervised representations extracted from large-scale pre-trained models. For the acoustic modality, we employ wav2vec 2.0  [12]  family variants: wav2vec2-xls-r-2b 1 , wav2vec-BERT 2.0  [13] , and wav2vec2-SER 2 . All are used in a frozen setup to act as robust feature extractors without updating their weights. These self-supervised speech models were pre-trained on thousands of hours of raw audio and are known to capture fine-grained phonetic and prosodic cues relevant for speech emotion recognition  [23] .\n\nFor the lexical modality, we integrate a set of strong transformer-based language models to extract semantic and contextual information from transcriptions: BERT-base, BERT-large  [14] , XLM-RoBERTa  [15] , and DeBERTa  [16] . These models are also frozen during the feature extraction phase, ensuring consistent and transferable text embeddings while reducing computational cost.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Extracting Lexical Representations From Speech",
      "text": "To begin the lexical-based experimental analysis, we first convert speech into text using an ASR system. Specifically, 1 https://huggingface.co/facebook/wav2vec2-xls-r-2b 2 https://huggingface.co/r-f/wav2vec-english-speech-emotion-recognition we employ the Whisper-large-v3  [17]  model for transcription, as it provides robust performance in diverse acoustic conditions and multilingual settings. The resulting transcripts are then fed into the text SSL models to extract the lexical representations which are then fed into the classifier.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Classifier And Training Setup",
      "text": "For the purpose of comparing acoustic and lexical characteristics and experimenting with various self-supervised models, because the SSL-derived representations have been proven to be strong enough on their own  [19] ,  [21] , we adopt a simple multi-layer perceptron (MLP) with three hidden layers as our classifier. For the speech representations, we apply average pooling over the time frames, while for the lexical representations, average pooling is performed over the token dimension. This results in a fixed-size, 1xf eatures vector serving as the input to the MLP.\n\nThe MLP was trained for 500 epochs with a learning rate of 3e -5 with the Adam optimizer in each scenario, irrespective of the SSL or modality.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Layer-Wise Analysis",
      "text": "To better understand how emotional information is learned across different layers of self-supervised models, we conduct a layer-wise analysis by extracting representations from each transformer block of the speech and lexical encoders. For the speech modality, we analyze hidden states from individual layers of wav2vec2-xls-r-2b, wav2vec-BERT 2.0, and wav2vec2-SER. For the text modality, we do the same for BERT-base, BERT-large, XLM-RoBERTa, and DeBERTa. Each layer's output is average-pooled over time frames or tokens and then used for training, development and evaluation on the same MLP. This systematic evaluation allows us to identify which layers are more effective for emotion recognition. By comparing performance across layers, we aim to see if the intermediate representations learned by selfsupervised models are better aligned with our task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "F. Data Denoising",
      "text": "Due to the noise present in the MELD dataset and to improve the robustness of speech emotion recognition to realworld noise conditions, we apply data denoising as a preprocessing step. Specifically, we adopt the speech enhancement by using DEMUCS  [18]  as a denoising neural network, which performs end-to-end enhancement directly in the time domain. This approach is capable of suppressing background noise and reverberation while preserving speech quality. Enhanced speech signals are then fed into the self-supervised models to extract cleaner representations and extend the experiments using them.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Acoustic-Based Speech Emotion Recognition",
      "text": "We evaluate the performance of the acoustic-based speech emotion recognition pipeline by using frozen self-supervised speech representations and training a fixed MLP classifier, as   I  shows the layer-wise WF1 (column 1) across all transformer layers over the development partition of the MELD dataset. Additionally, we applied the same experimental pipeline to the denoised audio samples, performing layer-wise analysis for denoised wav2vec2-xls-r-2b (column 5), denoised wav2vec-BERT 2.0 (column 6), and denoised wav2vec2-SER representations (column 7).\n\nInterestingly, the mid-level representations of the wav2vec2-xls-r-2b model achieve the best performance, yielding a 48.9% WF1 on top of the MLP. This result is on par in performance with the best-performing acoustic-based models from the literature, demonstrating that effective emotion recognition can be achieved even with simple classifiers when leveraging powerful self-supervised features. Moreover, the observed peak performance at intermediate layers suggests that these layers capture more emotionally relevant phonetic features compared to final layers, where overly abstracted information is retained and fine-grained information might be lost. This trend aligns with prior findings in self-supervised speech learning, where early or midlevel transformer representations have been shown to encode information highly correlated with paralinguistic tasks  [19] ,  [21] . Analyzing the impact of the denoising pre-processing step (columns marked with D-in Table  I ) reveals a consistent degradation of performance across all acoustic models. Despite the MELD dataset containing noise, aggressive denoising using neural networks such as DEMUCS, appears to remove not only noise but also subtle non-verbal vocalizations that are crucial for emotion recognition. This outcome highlights that conventional speech enhancement techniques, while effective for other speech applications, might be detrimental for tasks like SER, where paralinguistic nuances matter. Therefore, taskspecific noise reduction strategies or learning noise-robust representations directly within the model might be more beneficial.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Lexical-Based Speech Emotion Recognition",
      "text": "As a next step, we evaluate the potential of converging the speech emotion recognition task to a natural language processing (NLP) task using only lexical features by employing the same ideas as before, but instead of using the speech pre-trained models, the task will rely on text pre-trained auto-encoders. Although the dataset contains transcriptions, we would like to simulate real-world scenarios where no transcription is available, by passing the raw audio waveforms through Whisper-large-v3 to obtain the transcriptions and then fed those into the auto-encoders to obtain the final representations followed by the classifier.\n\nThe results presented in Table  II  demonstrates a clear trend: lexical-based models not only achieve competitive performance compared to acoustic-based systems but in many cases surpass them, with the best model reaching a WF1 of 51.73% on the development set. This suggests that the semantic information captured by text representations can encode emotional relevant features with surprising results.\n\nAnother observation is that, similar to the acoustic models, intermediate layers of the text-based encoders tend to provide the highest performance. This indicates that emotional content is best captured when representations are not in the final layers where overfitting to general language modeling might arise, losing capability of capturing emotional relations. Moreover, the lexical pipeline benefits from the robustness of modern ASR systems like Whisper, which enables highquality transcriptions even under real-life scenarios, where no transcriptions are available. However, slight inaccuracies in transcription may introduce artifacts or errors in the entire pipeline. To test this hypothesis, we perform emotion recognition using the transcripts in the metadata. The results show that, when using the manual transcriptions, the WF1 score is 60.9%. This result is highlighting a potential for future research on ASR error correction for speech emotion recognition tasks.\n\nFinally, the exploration of denoised audio feeding into ASR showed mostly degradation in most layers, likely due to distortions introduced by the denoising process. This reinforces the need for specialized enhancement models tailored for downstream tasks like SER, rather than generic speech enhancement (i.e. DEMUCS was pre-trained for music source separation).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Comparison With State-Of-The-Art",
      "text": "Table  III  summarizes our best-performing acoustic and lexical pipelines on the MELD test set against prior stateof-the-art systems. Our acoustic model (Layer26 XLS-R-2B + MLP) achieves 49.3% WF1, while our lexical system",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model",
      "text": "Modality WF1 SpeechFormer  [9]  Acoustic 41.9 Modality-Conversion  [20]  Lexical 43.1 SpeechFormer++  [10]  Acoustic 47.0 DWFormer  [8]  Acoustic 48.5 TF-Mamba  [7]  Acoustic 48.5 Deformable Speech Transformer (DST)  [6]  Acoustic 48.9 Layer26 W2V2-XLS-R-2B + MLP (Ours) Acoustic 49.3 Whisper + Layer19 DeBERTa + MLP (Ours) Lexical 51.5\n\n(Whisper transcriptions + DeBERTa Layer19 + MLP) reaches 51.5% WF1. These results highlight the potential to rethink SER pipelines by leveraging semantic content via ASR, enabling more efficient and effective emotion recognition that is deployable in real-world scenarios.\n\nThis approach has its own limitations, because on datasets such as RAVDESS  [22] , where an utterance has multiple emotion audio files, the lexical-based approach won't work, since the classifier will learn a lot of noise due to multiple annotations on that utterance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusions",
      "text": "In this work, we systematically explored acoustic-and lexical-based speech emotion recognition on the MELD dataset, trying to identify if extracting lexical features from speech will provide any insights. Our results show that lexical content can carry substantial emotional information, with features extracted via ASR and text SSL, achieving superior performance (51.5% WF1) compared to our best acousticbased model (49.3% WF1). While denoising and ASR are a great idea for this task, both need polishing to make them suitable. These findings suggest that treating speech emotion recognition as an NLP problem can unlock simpler and more powerful solutions in certain scenarios.\n\nFuture directions will include exploring ASR error correction and denoising strategies tailored for SER and a multimodal architecture with trainable modality-specific weights, allowing the model to learn how much to rely on acoustic versus lexical features. This will let us quantitatively assess the relative importance of each modality for emotion recognition.\n\nSuch efforts could lead to even more robust and practical emotion recognition systems suitable for real-world applications.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48",
          "W2V2-XLS-R-2B": "38.89\n38.43\n40.78\n41.33\n42.77\n41.51\n42.76\n43.42\n44.28\n45.79\n45.60\n45.67\n46.39\n47.11\n45.72\n46.30\n46.82\n45.74\n46.74\n46.86\n47.02\n47.72\n47.15\n47.39\n47.97\n47.67\n48.89\n48.34\n47.52\n48.46\n47.83\n48.03\n48.76\n48.42\n48.41\n47.83\n48.09\n47.55\n46.92\n46.59\n46.57\n44.92\n47.13\n45.12\n44.78\n44.60\n44.02\n43.85\n42.36",
          "W2V-BERT 2.0": "30.93\n38.45\n41.58\n44.51\n44.88\n47.74\n47.89\n46.84\n46.37\n44.82\n47.28\n47.14\n47.32\n47.82\n48.00\n47.07\n47.41\n45.23\n45.14\n44.10\n45.47\n42.53\n42.64\n40.46\n38.27\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-",
          "W2V2-SER": "38.62\n40.29\n41.81\n42.04\n43.34\n44.24\n44.01\n43.58\n43.64\n45.56\n45.13\n45.38\n44.57\n46.02\n46.59\n45.57\n44.07\n43.69\n43.61\n43.57\n42.77\n42.67\n42.64\n42.21\n42.81\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-",
          "D -W2V2-XLS-R-2B": "37.34\n37.79\n38.93\n40.82\n41.13\n41.04\n41.36\n42.39\n43.75\n43.07\n44.32\n43.83\n46.46\n45.47\n45.40\n45.13\n45.65\n45.29\n44.00\n45.09\n45.72\n45.45\n45.71\n45.58\n45.32\n45.78\n45.55\n46.38\n45.63\n45.41\n46.15\n46.62\n46.25\n46.25\n46.62\n46.3\n45.65\n45.49\n44.25\n44.88\n45.25\n44.3\n44.08\n45.08\n43.88\n44.65\n44.05\n43.31\n43.33",
          "D -W2V-BERT 2.0": "31.91\n38.96\n40.91\n42.86\n44.72\n46.28\n46.88\n45.95\n46.12\n45.72\n45.06\n46.27\n48.81\n47.76\n45.73\n45.37\n47.03\n46.18\n43.26\n42.82\n43.67\n42.24\n41.89\n40.77\n37.71\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-",
          "D -W2V2-SER": "39.28\n40.13\n39.57\n41.51\n41.26\n42.31\n43.73\n43.37\n42.72\n42.48\n43.19\n42.39\n43.42\n42.78\n42.81\n41.71\n41.73\n41.68\n40.73\n40.95\n39.82\n39.41\n39.64\n40.96\n40.57\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24",
          "BERT-BASE": "43.54\n44.48\n43.87\n44.27\n45.29\n46.32\n45.90\n45.93\n47.40\n48.57\n48.78\n48.08\n47.98\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-",
          "BERT-LARGE": "44.78\n45.18\n44.63\n45.33\n46.07\n46.32\n46.34\n45.35\n45.54\n45.67\n46.34\n46.24\n47.23\n46.21\n48.14\n48.01\n47.88\n49.29\n49.50\n51.04\n49.57\n49.37\n49.80\n48.85\n48.40",
          "XLM-ROBERTA": "44.03\n40.97\n42.15\n43.10\n42.84\n44.42\n43.28\n44.95\n44.55\n45.10\n44.73\n45.30\n45.75\n45.56\n45.81\n46.15\n46.69\n47.16\n46.35\n47.48\n46.60\n47.37\n46.48\n45.92\n44.17",
          "DEBERTA": "43.93\n43.75\n45.13\n45.98\n47.09\n46.73\n47.47\n47.67\n47.37\n47.72\n47.31\n47.06\n48.26\n49.54\n49.28\n50.56\n50.94\n50.40\n49.91\n51.73\n51.01\n49.95\n49.40\n47.34\n47.40",
          "D -BERT-BASE": "43.02\n42.38\n41.99\n42.69\n43.37\n44.36\n43.43\n44.70\n46.63\n46.46\n46.44\n46.09\n45.66\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-",
          "D -BERT-LARGE": "41.84\n42.54\n41.40\n42.15\n42.59\n43.16\n43.79\n43.83\n42.69\n44.38\n44.69\n44.67\n44.22\n44.02\n45.21\n46.25\n46.28\n47.00\n46.78\n46.73\n46.25\n46.76\n46.29\n45.63\n46.59",
          "D -XLM-ROBERTA": "40.75\n40.45\n42.15\n41.85\n42.49\n43.02\n43.53\n44.06\n44.30\n43.39\n43.37\n42.35\n43.96\n43.84\n45.07\n43.86\n45.27\n45.05\n46.15\n47.09\n46.06\n45.85\n46.05\n44.37\n45.08",
          "D -DEBERTA": "41.55\n43.68\n42.50\n45.27\n44.52\n44.73\n45.59\n45.41\n46.56\n46.37\n46.01\n47.07\n46.47\n47.16\n47.35\n47.92\n48.79\n48.65\n48.30\n48.82\n47.82\n48.68\n45.60\n44.83\n45.26"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "SpeechFormer\n[9]\nModality-Conversion [20]\nSpeechFormer++ [10]\nDWFormer\n[8]\nTF-Mamba [7]\nDeformable Speech Transformer\n(DST)\n[6]",
          "Modality": "Acoustic\nLexical\nAcoustic\nAcoustic\nAcoustic\nAcoustic",
          "WF1": "41.9\n43.1\n47.0\n48.5\n48.5\n48.9"
        },
        {
          "Model": "Layer26 W2V2-XLS-R-2B + MLP (Ours)\nWhisper + Layer19 DeBERTa + MLP (Ours)",
          "Modality": "Acoustic\nLexical",
          "WF1": "49.3\n51.5"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Bert-Base Bert-Large Xlm-Roberta Deberta D -Bert-Base D -Bert-Large D -Xlm-Roberta D -Deberta Layer"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Cross-modal fusion techniques for utterance-level emotion recognition from text and speech",
      "authors": [
        "J Luo",
        "H Phan",
        "J Reiss"
      ],
      "venue": "*Proc. IEEE Int. Conf. Acoust. Speech Signal Process.* (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "Artificial intelligence-driven customer service: Enhancing personalization, loyalty, and customer satisfaction",
      "authors": [
        "D Patil"
      ],
      "year": "2024",
      "venue": "Artificial intelligence-driven customer service: Enhancing personalization, loyalty, and customer satisfaction",
      "doi": "10.2139/ssrn.5057432"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition via graph-based representations",
      "authors": [
        "A Pentari",
        "G Kafentzis",
        "M Tsiknakis"
      ],
      "year": "2024",
      "venue": "*Scientific Reports*",
      "doi": "10.1038/s41598-024-52989-2"
    },
    {
      "citation_id": "5",
      "title": "A review on speech emotion recognition: A survey, recent advances, challenges, and the influence of noise",
      "authors": [
        "S George",
        "P Ilyas"
      ],
      "year": "2024",
      "venue": "*Neurocomputing*",
      "doi": "10.1016/j.neucom.2023.127015"
    },
    {
      "citation_id": "6",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Tris Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "*Speech Communication*",
      "doi": "10.1016/j.specom.2022.03.002"
    },
    {
      "citation_id": "7",
      "title": "DST: Deformable Speech Transformer for emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "*Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Temporal-frequency state space duality: An efficient paradigm for speech emotion recognition",
      "authors": [
        "J Zhao",
        "F Wang",
        "K Li",
        "Y Wei",
        "S Tang",
        "S Zhao",
        "X Sun"
      ],
      "year": "2025",
      "venue": "*Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "DWFormer: Dynamic Window transFormer for speech emotion recognition",
      "authors": [
        "S Chen",
        "X Xing",
        "W Zhang",
        "W Chen",
        "X Xu"
      ],
      "year": "2023",
      "venue": "*Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "SpeechFormer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2022",
      "venue": "*Proc. Interspeech*"
    },
    {
      "citation_id": "11",
      "title": "SpeechFormer++: A hierarchical efficient framework for paralinguistic speech processing",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "*IEEE/ACM Trans. Audio, Speech, Lang. Process.*"
    },
    {
      "citation_id": "12",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "*Proc. 57th Annual Meeting of the Association for Computational Linguistics (ACL)*"
    },
    {
      "citation_id": "13",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "14",
      "title": "Seamless: Multilingual expressive and streaming speech translation",
      "authors": [
        "Seamless Communication"
      ],
      "year": "2023",
      "venue": "Seamless: Multilingual expressive and streaming speech translation",
      "arxiv": "arXiv:2312.05187"
    },
    {
      "citation_id": "15",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "*Proc. NAACL-HLT*"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "A Conneau"
      ],
      "venue": "*Proc. ACL*, 2020"
    },
    {
      "citation_id": "17",
      "title": "DeBERTa: Decoding-enhanced BERT with disentangled attention",
      "authors": [
        "P He",
        "X Liu",
        "J Gao",
        "W Chen"
      ],
      "year": "2021",
      "venue": "*Proc. ICLR*"
    },
    {
      "citation_id": "18",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "19",
      "title": "Real time speech enhancement in the waveform domain",
      "authors": [
        "A Defossez",
        "G Synnaeve",
        "Y Adi"
      ],
      "venue": "*Proc. Interspeech*, 2020"
    },
    {
      "citation_id": "20",
      "title": "Unmasking real-world audio deepfakes: A data-centric approach",
      "authors": [
        "D Combei",
        "A Stan",
        "D Oneata",
        "N Müller",
        "H Cucu"
      ],
      "year": "2025",
      "venue": "*Proc. Interspeech*"
    },
    {
      "citation_id": "21",
      "title": "A change of heart: Improving speech emotion recognition through speech-to-text modality conversion",
      "authors": [
        "Z Taghavi",
        "A Satvaty",
        "H Sameti"
      ],
      "venue": "A change of heart: Improving speech emotion recognition through speech-to-text modality conversion"
    },
    {
      "citation_id": "22",
      "title": "TADA: Training-free attribution and out-of-domain detection of audio deepfakes",
      "authors": [
        "A Stan",
        "D Combei",
        "D Oneata",
        "H Cucu"
      ],
      "year": "2025",
      "venue": "*Proc. Interspeech*"
    },
    {
      "citation_id": "23",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English"
    },
    {
      "citation_id": "24",
      "title": "An analysis of large speech models-based representations for speech emotion recognition",
      "authors": [
        "A Stânea",
        "V Strilet",
        "C Strilet",
        "A Stan"
      ],
      "year": "2023",
      "venue": "Proc. 2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)"
    }
  ]
}