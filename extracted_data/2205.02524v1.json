{
  "paper_id": "2205.02524v1",
  "title": "M2R2: Missing-Modality Robust Emotion Recognition Framework With Iterative Data Augmentation",
  "published": "2022-05-05T09:16:31Z",
  "authors": [
    "Ning Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper deals with the utterance-level modalities missing problem with uncertain patterns on emotion recognition in conversation (ERC) task. Present models generally predict the speaker's emotions by its current utterance and context, which is degraded by modality missing considerably. Our work proposes a framework Missing-Modality Robust emotion Recognition (M2R2), which trains emotion recognition model with iterative data augmentation by learned common representation. Firstly, a network called Party Attentive Network (PANet) is designed to classify emotions, which tracks all the speakers' states and context. Attention mechanism between speaker with other participants and dialogue topic is used to decentralize dependence on multi-time and multi-party utterances instead of the possible incomplete one. Moreover, the Common Representation Learning (CRL) problem is defined for modalitymissing problem. Data imputation methods improved by the adversarial strategy are used here to construct extra features to augment data. Extensive experiments and case studies validate the effectiveness of our methods over baselines for modality-missing emotion recognition on two different datasets. Impact Statement",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) has now attracted increasing attention as its potential ability to mine and analyze the connections from the heavily incremental volume of publicly available conversational data  [3]   [9] . Furthermore, ERC can be used as a tool for psychological analysis in healthcare systems  [28]  and human-interacted emotion-aware conversation that requires an understanding of the user's emotions  [31]   [6] .\n\nTraditional machine learning methods according to textual features such as Na√Øve Bayes classifier  [35] , NLTK vader  [5] , etc, have been proposed for ERC task  [34] . As deep learning develops, many neural networks models are employed to classify the emotion at each utterance.  [27]  uses autoencoder to detect multiple emotions in conversation transcripts. Graph convolutional networks are exploited by  [23]  and  [42]  to model the inter-relationship between conversation topic and its different parties for better emotion representations.  [14]  employs LSTM cells and Bert hierarchically for utterance context considering the sequential essence of the conversation, which was followed by  [29]  firstly using recurrent neural network (RNN) to recognize emotions. Moreover, many NLP techniques and ideas are introduced in ERC.  [10]  incorporates different elements of commonsense and builds upon them to learn interactions between all conversation parties, and  [44]  proposes contextaware affective graph attention mechanism to interpret contextual utterances hierarchically.\n\nWhile solving the ERC task by unimodal (mostly textual) data is not enough because not only that the emotions are subjective mixed feelings  [6] , but also the labeling difficulty in real-life conversation scenarios. Through multiple channels, humans express emotions and feelings (such as voice speech, textual meaning, facial expression, body gestures, movements) provides multimodal data. Multimodal emotion recognition systems are studied alongside unimodal solutions as they offer higher accuracy of classification  [1] . Many systems consider multimodal data for ERC.  [13] ,  [12]  and  [21]  introduce attention mechanism in conversation context extraction by pooling all proceeding global states related to concatenated utterance modalities. To better fuse multi modalities under different representation spaces,  [32] ,  [36] ,  [15]  propose kinds of novel crossmodality modules to capture intra-and inter-modal influence.\n\nIn the real world, however, data often suffer from utterance-level incompleteness  [19]  with various patterns, which greatly degrades the integration and complement between different modalities. For the ERC task, there are various causes for desired data to be temporarily missing: sensors can fail so that the corresponding signal is no longer available, or its outcome may be corrupted by noise. In another situation, faces with obvious expressions may be blocked or move drastically and then disappear from the view of the sensor  [39] .\n\nFew works focus on the modality-missing problem at utterance level. Typical data imputation algorithm  [37] [24]  [40]  are aimed at separate, non-sequential data.  [26]  pad missing modalities by zeros or repeated data with advanced training strategies.  [43]  learns robust joint multimodal representations by bidirectional encoders with cascade-connected residual imagination module.  [7]  performs infinite imputations by integrating all modalities, which was followed by  [25]  which leverages several classifiers under all combinations of different modalities. By building features reconstruction module,  [20]  employs Bayesian neural network to employ data uncertainty as feature regularization. This paper deals with the utterance-level modalities missing problem with uncertain patterns on emotion recognition in conversation (ERC) task. We propose a novel framework Missing-Modality Robust emotion Recognition (M2R2), which trains emotion recognition model with iterative data augmentation by learned common representation.\n\nWe first introduce Party Attentive Network (PANet), which tracks all the speakers' states and context. Attention mechanism between speaker with other participants and dialogue topic is used to decentralize dependence on multi-time and multi-party utterances instead of the possible incomplete one. Moreover, we define the Common Representation Learning (CRL) problem for modality-missing data augmentation. Data imputation methods improved by the adversarial strategy are used here to construct extra features. Extensive experiments and case studies validate the effectiveness of our methods over baselines for modalitymissing emotion recognition on two different datasets. The contributions are summarized as follows:\n\n1. We propose a novel M2R2 framework with iterative data augmentation on missing-modality datasets, which alleviate the performance decline caused by modalities incompleteness greatly.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Statements",
      "text": "Suppose a multimodal conversation scene with T length, let\n\ndefine audio, text and visual representations respectively. And there are Q parties/speakers q 1 , q 2 , . . . , q Q in this conversation. ERC task is to predict the emotion label e q I(t) t at time t, where I(t) ‚àà 1, 2, . . . , Q indicates the mapping from time t to index of current party. At each time t, the constituent utterance\n\n, where ‚äï means concatenation here, and this multimodal representations u t is extracted as described in section 4.2. Current systems use GRU cells  [4]  to update hidden state and result representation. At time step t, the GRU cell computes a new hidden state h t by GRU(x t , h t-1 ) as follows:\n\nwhere D H is the dimension of common representation. To employ log likelihood, we assume that (1) Each modality is conditionally independent;\n\n(2) Utterance are independent and identically distributed (i.i.d).\n\nThis assumption gives a probability aspect for CRL. Under the above assumption (1), the probability of label y can be viewed as p(y, S|h) = p(y|h)p(S|h),\n\nwhere p(S|h) = p(u 1 |h)p(u 2 |h) . . . p(u M |h) and y is treated as another semantic modality, with M being the number of modalities. Here we omit the time token for simplification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this work, we propose a Missing-Modality Robust emotion Recognition framework (M2R2) for the ERC task. Fig.  2  illustrates the overview and detailed module structures. M2R2 updates PANet and CRL with iterative data augmentation by each other's latent representations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "M2R2 Framework",
      "text": "Update the reconstruction parameters and common representation;\n\nTesting Stage Input: Incomplete testing conversation dataset:    A big challenge lies in narrowing the gap between training and testing in multimodal representation learning, originating from their difference. We introduce a fine-tuned strategy to address this problem. The consistency of common representation from training to testing stage is ensured by optimizing the objective function L P defined in Eq.(  17 ), getting a new set of parameters {Œò m ut } M m=1 that applied in test stage. Then test common representation can be calculated by CRL h * . Finally, this common representation is sent to the trained PANet model with test utterance (incomplete) to get predicted emotion labels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Party Attentive Network",
      "text": "Apparently the emotion e t is mainly determined by the utterance u t . In addition, we assume that (1) the conversation scene has context by proceeding utterances; (2) every party(speaker) has its emotional context by its utterances 3) the emotion of one party is affected by the other party's emotion context.\n\nOur PANet model is shown in Fig.  2(b ). We model the conversation scene state by global state, which is updated by the previous speaker's party-states, emotional states and current speaker's utterance, to encode shared information between all parties. It tracks party state, emotional state and utterance of all parties, using all proceeding global state jointly with utterance to get global context for party state.\n\nFurthermore, each party is modeled by party state, which changes as a new utterance flows in, tracking parties' dynamic representation related to the emotional state. Like global context, all proceeding party states with utterance are exploited to get party context for emotion state. Finally, the model infers the emotional states of different parties from the corresponding party states along with other parties'. The emotional states are then sent to the classification head to get the label. Global State The purpose of global state (g ‚àà R D G , D G represents dimension) is to model and track conversation scene and shared information between its parties. Current state g t focuses on current utterance's impact while proceeding global ones g 1 , . . . , g t-1 reveals the change of conversation representation, which provide meaningful context for party state. At time t, the global state g t updated through GRU G as follows:\n\nwhere\n\nis the previous speaker's party state, e q I(t- To fully take the global state into consideration, we employ Attention Mechanism  [38]  to calculate global context c g t = ATTN([g 1 , g 2 , . . . , g t ], u t ) related to current utterance:\n\nwhere Œ± t ‚àà R t-1 is the attention weight which will be higher for important factors assigned for all proceeding global state related to current utterance, c t ‚àà R D G is the weighted sum of proceeding global state by Œ± t . The scheme of attention module for global is shown in Fig.  2(b ). Then party state p q is updated respectively by employing global context, current utterance and previous emotion state: p q t = GRU q P (p q t-1 , u t ‚äï c t ‚äï e q t-1 ), q ‚àà {1, 2, . . . , Q}, (6) where p q t-1 is previous party state, e q t-1 is previous emotional state. Notice that party attention updates using its party and emotion state instead of the previous speaker's, which is different from global states. Emotion State We assume that every party in the conversation has its emotional state behind party state and is affected by other ones, and it varies under different global states. It shows how the conversation scene and other parties' state can affect party emotion. Thus we exploit all parties' state related to global state to represent party context:\n\nwhere function ATTN is defined as Eq.(  5 ), p q t is the state of party q at time t and c pq t ‚àà R D P is the party context of p q . The scheme of attention module for party context is shown in Fig.  2(b) .\n\nAll the party context {c p1 t , c p2 t , . . . , c p Q t } and current global state g t are then used to update emotion state by\n\nwhere e q t ‚àà R E . Emotion Classification Head We employ a simple perceptron C to classify the speaker's emotion state as final emotion label: ≈∑t = arg max C(e\n\nwhere ≈∑t is the predicted emotion label. Finally, we employ categorical cross-entropy along with L2-regularization as the objective function:\n\nwhere x, y is the time-sequence data with labels, T is the total number of utterance in a conversation, P t is the distribution of emotion labels for utterance u t , y t is the ground truth, Œª E is the L-2 weight and Œò E represents the parameters of PANet model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Common Representation Learning",
      "text": "Enlightened by  [41]  and take the assumptions in section 2, we use CPM-Nets to learn common representation. Model p(u i |h) as follows:\n\np(u m |h) ‚àù e -lr(u m ,fm(h;Œò m u )) , m ‚àà {1, 2, . . . , M },\n\nwhere Œò m u are the parameters responsible for the mapping f i (‚Ä¢) from common representation h to utterance modality u m , with l r (‚Ä¢) being the reconstruction loss function.\n\nThus, the log-likelihood function can be written as\n\nThe common representation h t is used to reconstruct data u m by the loss l r (u m , f m (h; Œò m u )), which is balanced by the classification loss l c (y n , g(h t |Œò y )) encourage the alignment with the ground truth.\n\nFor classification task, we define reconstruction loss function as\n\nwhere the mapping direction is from common representation to different modalities, making sure all the samples (regardless of their missing patterns) comparable.\n\nAccordingly, the classification loss is defined as l c (y t , ≈∑t ) = max(0, ‚àÜ(y t , ≈∑) + E h‚àºH(y) F (h, h t )\n\nwhere ≈∑ = arg y‚ààY max E h‚àºH(y) F (h, h t ) is the predicted label and H(y) is the set of common representation for y.The loss ‚àÜ(y t , ≈∑) is 0 if y t = ≈∑ else 1.\n\nTo better employ observed data and to promote data imputation, we introduce adversarial strategy in our algorithm. Learning common representation is guided simultaneously by observed data, unobserved data and labels. The adversarial loss is calculated by wasserstein GAN(WGAN)  [11]  as:\n\nwhere √ªm = f m (h n ; Œò m u ) is the generated missing view data, P G = ( ‚àá √ªm D m (√ª m ) 2 -1) 2 is the gradient penalty in WGAN and Œª G is the balanced parameter. (1 -s m ) indicates whether or not the sample at scene n and modality m should be computed. {u m i } I i are the observed data, with the total number of samples being I. The discriminator D m of m-th view defined by parameter Œò m d determinate the imputation data between the observed ones.\n\nFinally, the overall loss function is shown as:\n\nwhere S, y is the incomplete input data with labels and h is the common representation learned by reconstruction function f m . The hyper-parameters Œª R , Œª C , Œª A are used to adjust reconstruction loss, classification loss and adversarial loss respectively.\n\nAt testing stage, we employ fine-tune strategy to ensure the consistency by adjusting the loss function defined as Eq.(  13 ) to narrow the gap between training set and testing set:\n\nwhere Œò m ut is the parameters that control the mapping from test input to its common representation, and S represents the incomplete testing dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Setup",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "We test our model and algorithm on the ERC datasets: IEMOCAP  [2] , and MELD  [30] . IEMOCAP is a two-party dataset, while MELD is a multiple-party one. We split both the datasets into train/val and test samples approximately by the ratio 80/20, guaranteeing that they do not share any party. The detailed conversations' and utterances' distribution of train/val/test samples are shown in Table  4  IEMOCAP This dataset is acted, multimodal and multispeaker containing approximately 12 hours of dyadic sessions with markers on the face, head, and hands. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit specific types of emotions (happiness, anger, sadness, frustration, neutral state). MELD Multimodal EmotionLines Dataset has been created by enhancing and extending the EmotionLines dataset. MELD contains the same conversation instances available in EmotionLines, but it also encompasses audio and visual modality along with the text. MELD has more than 1400 conversations and 13000 utterances from the Friends TV series. Multiple speakers participated in the conversations. Each utterance in the conversation has been labeled by any of these seven emotions (happiness, anger, sadness, disgust, surprise, fear, neutral).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "Textual Feature We employ convolutional neural networks (CNN) for textual feature extraction. Following  [17] , we obtain n-gram features from each utterance using three distinct convolution filters of sizes 3,4 and 5 respectively, each having 50 feature maps. Outputs are then subjected to max-pooling and rectified linear unit (ReLU) activation. These activations are concatenated and fed to a 100dimensional dense layer, regarded as the textual utterance representation. Then the whole network is trained with the emotion labels.\n\nAudio and Visual Feature We follow the identical procedure with  [21] , employing 3D-CNN and openSMILE  [8]  to extract audio and visual feature  [16] , respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baselines",
      "text": "bc-LSTM Biredectional LSTM  [29]  is used to capture thecontext from the surrounding utterances to generate context-aware utterance representation. However, this model does not differentiate among the speakers. DialogueRNN models the emotion of utterances in a conversation with speaker, context and emotion information from neighbour utterances  [21] . These factors are modeled using three separate GRU networks to keep track of the individual speaker states.\n\nICON uses two GRU networks to learn the utterance representations for conversations between two participants. The output of the two speakers' GRUs is then connected using another GRU that helps perform explicit inter-speaker modeling.\n\nKET or Knowledge enriched transformers dynamically leverages external commonsense knowledge using hierarchical self-attention and context-aware graph attention.\n\nMMER is composed of several sub-modules: three feature extractors, three unimodal classifiers, three bimodal classifiers and one trimodal classifier  [25] . It leverages a multimodal multitask approach to tackle the problem of missing modality at test time and training time. SMIL uses building features reconstruction network to solve the problem of missing modality. It uses a Bayesian neural network to assess the data uncertainty as feature regularization to overcome model and data bias produced by the feature reconstruction network  [20] . SMVAE essentially performs infinite imputations by integrating out the missing data  [7] . It first develops a multi-view variational autoencoder model for fusing multi-modality emotional data and builds a semi-supervised emotion recognition algorithm.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Network Architectures",
      "text": "We employ bi-directional GRU  [33]  cells in our PANet model, where global state dimension D G , party state dimension D P , emotional state dimension D E being 512, 512, 256 respectively. For emotion classification head, a simple perceptron composed of 2-layer linear layers attached with ReLU activation is used. For CPM-Nets, we employ the fully connected networks with batch normalization and LeakyReLU for all generators f m . Similarly, fully connected networks with LeakyReLU activation structures are employed for discriminators D m . L2 regularization is used with the value of the trade-off parameter being 0.001 for f m and D m .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Parameter Settings",
      "text": "At training state, we use Adam optimizer  [18]  for all parameter update, where Œ≤ 1 = 0.5, Œ≤ 2 = 0.999. The learning rate for PANet model Œ± E is 1e-4 with the L2 regularization being 1e-5. And learning rates for the generators and discriminators Œ± E in CPM-Nets are all 1e-3. The tradeoff parameters Œª R , Œª P , Œª A of reconstruction, classification and adversarial losses in CRL problem are 1, 10, 10 respectively. Specifically, the parameter for gradient penalty Œª G in WGAN is default as 1. And we train generators after discriminators update 2 times in order to guarantee the stability in GAN structure  [22] . In each M2R2 epoch, we train PANet model n E times and CPM-Nets for n P times.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Panet Compared With Baselines",
      "text": "Firstly we test our PANet model and baselines at different missing rate defined as Œ∑ = t m S t / t m u m t in section 2 ranging from 0.0 to 0.6 by step 0.1 repeatedly 5 times. To guarantee that no blank utterance is sent to the model, every row of the missing matrix must have one element, where zeros vector is used here for the absent modalities. Additional hidden representations are not used here.\n\nComparison results of IEMOCAP&MELD datasets are shown in Fig  3 , where there are the following observations: 1) Our PANet model performs better at both accuracy and F1-score than baselines when the dataset is complete; 2) As missing rate Œ∑ increases, the degradation of our model is much faint than baselines, especially on MELD dataset. For example, the relative dropping accuracy of PANet at MELD dataset from Œ∑ = 0 to Œ∑ = 0.3 is 1.55%, while the relative dropping accuracy of the second-best DialogueRNN is 2.96%; 3) Our model is more robust to the modalities missing rate than other baselines. For instance, the variance of our PANet model related to the missing rate Œ∑ on the MELD dataset is 2.364 compared with 3.54099 of DialogueRNN.\n\nWe can also observe that our algorithm performs better on MELD dataset than IEMOCAP regardless the accuracy or F1-score metric, the possible explanation is that MELD is a film cut of TV-shows, where the visual and audio expression is much more clear, exaggerative and severely changes than IEMOCAP as shown in Fig.  4 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "M2R2 Compared With Baselines",
      "text": "We also test the detailed emotion prediction results of all classes on both datasets under the missing rate Œ∑ = 0.5. As Table .1(b)) shows, our model performs better than baselines on both IEMOCAP and MELD datasets at missing rate Œ∑ = 0.5, where metrics are accuracy and F1 score. Significantly, the M2R2 framework can detect Neutral and Frustrated emotion more accurately, gaining far more accuracy and F1 socre than the second-best DialogueRNN model. We presumed that the exploit of party attention related to the global state provides more hidden emotion representa- tion for the speaker, which neutralizes the potential opposite emotions from all parties in a conversation.\n\nIn further discussion, we test the accuracy and F1 score of both IEMOCAP and MELD datasets with other multimodal data reconstruction algorithms. The comparison is in  datasets, where accuracy is about 1% better than the secondbest SMIL algorithm on both datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Case Study",
      "text": "Context Attention We test our global and party attention mechanism on IEMOCAP dataset when missing rate Œ∑ = 0.2, where the global and different parties' attention weight Œ± G , Œ± P1 , Œ± P2 at scene time turn 7 is shown in Fig.  5 .\n\nObservations can be summarized as follwing:\n\n(1) Dependency on utterance at different distances. The attention weight of all conversation turns related to chosen turns shows that neighboring and distant utterances can influence the current emotional state. For example, the emotion at turn 7, where the model correctly predicts 'Disgust', is influenced by the nearby utterance at turn 2 and the distant utterance at turn 43.\n\nA similar dependency trend is also shared by party attention mechanism such as the nearby utterance at turn 6 and the distant utterance at turn 39 both influence the current emotion prediction at turn 7.\n\n(2) Dependency on future utterances. The figure also reveals that future utterances can influence the proceeding emotion greatly. In this discussion, we can observe that the turn 7 contributes almost half to proceeding turn 7 of global attention Œ± G and similarly, turn 36, 37 and turn 39 have a significant influence on their proceeding turn 7 of Œ± P1 , Œ± P2 respectively.\n\n(3) Dependency on different parties. Like common sense in daily life, one speaker's emotional state changes from the speaker's internal shift and other speakers' influence. For example, both turn 8 and 43 strongly attend to the emotion prediction of turn 7 while turn 8 is  uttered by party q I(8) and turn 43 belongs to the other party.\n\nTo avoid the exception error in analysis, we test the global attention weight at all turns in a conversation scene of the IEMOCAP dataset. The Fig.  6  shows the result. As can be observed, early utterances attend to early scene emotion prediction more strongly than other times and accordingly, late utterances attend more strongly to late scene emotion prediction. This trend can be explained by sequential neural networks' memory capacity and time.    obvious clustering features and margins, especially for sad emotions, which lie at the lower right corner of the scatter map.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "This paper's main novelty is the introduction of party attention and M2R2 framework. We remove these two components once a time and test on the IEMOCAP dataset at missing rate Œ∑ = 0.5 to study the influence on them, where the results are shown in Table  3 .\n\nAs expected, the proposed two components improve the accuracy and F1 score by about 3% percent in total. In addition, the M2R2 is more influential than party attention, where the accuracy falls about 2.20% than 0.9%.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusions",
      "text": "We proposed the PANet model and M2R2 Framework for ERC task with missing modalities at the utterance level. The proposed PANet model based on RNN can handle incomplete utterances by spreading dependency on all conversation parties. Our algorithm learns PANet and CPM-Nets of incomplete data jointly, where extended datasets with hidden features for CPM-Nets and common representation for PANet can provide more versatile information for ERC task. We empirically validate that the proposed algorithm is relatively robust to sequential data with missing modalities. More structures and techniques with suitable common representation learning methods should be tested, which we plan to explore in the future.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Possible causes of data missing at utterances in a conversation: 1) target face out of image boundary; 3) audio corrupted by noise;",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of Missing-Modality Robust emotion Recognition framework on ERC task. a) The procedure of M2R2. PANet",
      "page": 4
    },
    {
      "caption": "Figure 3: , where there are the following observations:",
      "page": 7
    },
    {
      "caption": "Figure 3: Results of different emotion recognition models with missing rate.",
      "page": 8
    },
    {
      "caption": "Figure 4: Visual/Facial modality comparison of MELD and",
      "page": 8
    },
    {
      "caption": "Figure 5: Global and different parties‚Äô attention weight related to",
      "page": 9
    },
    {
      "caption": "Figure 6: Global attention weight of all turns in one test conversa-",
      "page": 10
    },
    {
      "caption": "Figure 7: Reconstruction and classiÔ¨Åcation loss at training and test-",
      "page": 10
    },
    {
      "caption": "Figure 8: t-SNE result of commom representation of IEMOCAP",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Turn Party": "",
          "Utterance": "",
          "Emotion": "Label\nPredicted"
        },
        {
          "Turn Party": "Pùüê",
          "Utterance": "Yeah, they are all told the same \nthing.  They are all told that ‚Ä¶\nthey just get in on the right spot, \nthan you than maybe life will start.",
          "Emotion": "Disgust\nDisgust"
        },
        {
          "Turn Party": "",
          "Utterance": "",
          "Emotion": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Party Attention\nM2R2 Framework",
          "Metrics": "Accuracy\nF1 score"
        },
        {
          "Component": "(cid:37)\n(cid:37)\n(cid:34)\n(cid:37)\n(cid:34)\n(cid:34)",
          "Metrics": "57.82\n57.43\n58.68\n58.16\n60.01\n59.63"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal emotion recognition using deep learning",
      "authors": [
        "M Sharmeen",
        "Abdullah Saleem",
        "Abdullah",
        "Mohammed Siddeeq Y Ameen Ameen",
        "Subhi Sadeeq",
        "Zeebaree"
      ],
      "year": "2021",
      "venue": "Journal of Applied Science and Technology Trends"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Sentiment analysis is a big suitcase",
      "authors": [
        "E Cambria",
        "Soujanya Poria",
        "Alexander Gelbukh",
        "Mike Thelwall"
      ],
      "year": "2017",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "4",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "5",
      "title": "Sentiment analysis on conversations in collaborative active learning as an early predictor of performance",
      "authors": [
        "Nasrin Dehbozorgi",
        "Mary Maher",
        "Mohsen Dorodchi"
      ],
      "year": "2020",
      "venue": "2020 IEEE Frontiers in Education Conference (FIE)"
    },
    {
      "citation_id": "6",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "Laurence Devillers",
        "Laurence Vidrascu",
        "Lori Lamel"
      ],
      "year": "2005",
      "venue": "Emotion and Brain"
    },
    {
      "citation_id": "7",
      "title": "Semisupervised deep generative modelling of incomplete multimodality emotional data",
      "authors": [
        "Changde Du",
        "Changying Du",
        "Hao Wang",
        "Jinpeng Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia, MM '18"
    },
    {
      "citation_id": "8",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Emotion detection: a technology review",
      "authors": [
        "Jose Maria",
        "Garcia- Garcia",
        "Maria Victor Mr Penichet",
        "Lozano"
      ],
      "year": "2017",
      "venue": "Proceedings of the XVIII international conference on human computer interaction"
    },
    {
      "citation_id": "10",
      "title": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. Cosmic: Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "11",
      "title": "Improved training of wasserstein gans",
      "authors": [
        "Ishaan Gulrajani",
        "Faruk Ahmed",
        "Martin Arjovsky",
        "Aaron Vincent Dumoulin",
        "Courville"
      ],
      "year": "2017",
      "venue": "Improved training of wasserstein gans",
      "arxiv": "arXiv:1704.00028"
    },
    {
      "citation_id": "12",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "13",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. North American Chapter"
    },
    {
      "citation_id": "14",
      "title": "",
      "authors": [
        "Chenyang Huang",
        "Amine Trabelsi",
        "Osmar R Za√Øane"
      ],
      "year": "2019",
      "venue": "",
      "arxiv": "arXiv:1904.00132"
    },
    {
      "citation_id": "15",
      "title": "Transformer-based label set generation for multi-modal multi-label emotion detection",
      "authors": [
        "Xincheng Ju",
        "Dong Zhang",
        "Junhui Li",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Efficient multiscale 3d cnn with fully connected crf for accurate brain lesion segmentation",
      "authors": [
        "Konstantinos Kamnitsas",
        "Christian Ledig",
        "F Virginia",
        "Joanna Newcombe",
        "Andrew Simpson",
        "David Kane",
        "Daniel Menon",
        "Ben Rueckert",
        "Glocker"
      ],
      "year": "2017",
      "venue": "Medical image analysis"
    },
    {
      "citation_id": "17",
      "title": "Convolutional neural networks for sentence classification. arXiv: Computation and Language",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Convolutional neural networks for sentence classification. arXiv: Computation and Language"
    },
    {
      "citation_id": "18",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Partial multiview clustering",
      "authors": [
        "Shao-Yuan",
        "Yuan Li",
        "Zhi-Hua Jiang",
        "Zhou"
      ],
      "year": "2014",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "20",
      "title": "Smil: Multimodal learning with severely missing modality",
      "authors": [
        "Mengmeng Ma",
        "Jian Ren",
        "Long Zhao",
        "Sergey Tulyakov",
        "Cathy Wu",
        "Xi Peng"
      ],
      "year": "2021",
      "venue": "Smil: Multimodal learning with severely missing modality",
      "arxiv": "arXiv:2103.05677"
    },
    {
      "citation_id": "21",
      "title": "An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria",
        "Dialoguernn"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Which training methods for gans do actually converge",
      "authors": [
        "Lars Mescheder",
        "Andreas Geiger",
        "Sebastian Nowozin"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "23",
      "title": "I-gcn: Incremental graph convolution network for conversation emotion detection",
      "authors": [
        "Weizhi Nie",
        "Rihao Chang",
        "Minjie Ren",
        "Yuting Su",
        "Anan Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "A bayesian missing value estimation method for gene expression profile data",
      "authors": [
        "Shigeyuki Oba",
        "Ichiro Masa Aki Sato",
        "Morito Takemasa",
        "Kenichi Monden",
        "Shin Matsubara",
        "Ishii"
      ],
      "year": "2003",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "25",
      "title": "Multimodal multitask emotion recognition using images, texts and tags",
      "authors": [
        "Pag√© Mathieu",
        "Brahim Fortin",
        "Chaib-Draa"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM Workshop on Crossmodal Learning and Application, WCRML '19"
    },
    {
      "citation_id": "26",
      "title": "Training strategies to handle missing modalities for audio-visual expression recognition. arXiv: Audio and Speech Processing",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2020",
      "venue": "Training strategies to handle missing modalities for audio-visual expression recognition. arXiv: Audio and Speech Processing"
    },
    {
      "citation_id": "27",
      "title": "Autoencoder for semisupervised multiple emotion detection of conversation transcripts",
      "authors": [
        "Duc-Anh Phan",
        "Yuji Matsumoto",
        "Hiroyuki Shindo"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "29",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "30",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "32",
      "title": "Interactive multimodal attention network for emotion recognition in conversation",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Xiaoqi Shi",
        "Weizhi Nie"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "33",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "Mike Schuster",
        "Kuldip K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE transactions on Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition through human conversation using machine learning techniques",
      "authors": [
        "Ch",
        "M Sekhar",
        "Srinivasa",
        "A Rao",
        "Debnath Keerthi Nayani",
        "Bhattacharyya"
      ],
      "year": "2021",
      "venue": "Machine Intelligence and Soft Computing"
    },
    {
      "citation_id": "35",
      "title": "Comparison of multinomial naive bayes algorithm and logistic regression for intent classification in chatbot",
      "authors": [
        "Muhammad Yusril",
        "Helmi Setyawan",
        "Rolly Maulana Awangga",
        "Safif Rafi"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Applied Engineering (ICAE)"
    },
    {
      "citation_id": "36",
      "title": "A multimodal fuzzy inference system using a continuous facial expression representation for emotion detection",
      "authors": [
        "Catherine Soladi√©",
        "Hanan Salam",
        "Catherine Pelachaud",
        "Nicolas Stoiber",
        "Renaud S√©guier"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "37",
      "title": "Missing value estimation methods for dna microarrays",
      "authors": [
        "Olga Troyanskaya",
        "Michael Cantor",
        "Gavin Sherlock",
        "Patrick Brown",
        "Trevor Hastie",
        "Robert Tibshirani",
        "David Botstein",
        "Russ Altman"
      ],
      "year": "2001",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "38",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "39",
      "title": "Building a robust system for multimodal emotion recognition. Emotion recognition: A pattern analysis approach",
      "authors": [
        "Johannes Wagner",
        "Florian Lingenfelser",
        "Elisabeth Andr√©"
      ],
      "year": "2015",
      "venue": "Building a robust system for multimodal emotion recognition. Emotion recognition: A pattern analysis approach"
    },
    {
      "citation_id": "40",
      "title": "Multi-source learning for joint analysis of incomplete multi-modality neuroimaging data",
      "authors": [
        "Lei Yuan",
        "Yalin Wang",
        "Paul Thompson",
        "A Vaibhav",
        "Jieping Narayan",
        "Ye"
      ],
      "year": "2012",
      "venue": "Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "41",
      "title": "Deep partial multi-view learning",
      "authors": [
        "Changqing Zhang",
        "Yajie Cui",
        "Zongbo Han",
        "Joey Zhou",
        "Huazhu Fu",
        "Qinghua Hu"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "42",
      "title": "Modeling both contextand speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "43",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "44",
      "title": "Knowledgeenriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Knowledgeenriched transformer for emotion detection in textual conversations",
      "arxiv": "arXiv:1909.10681"
    }
  ]
}