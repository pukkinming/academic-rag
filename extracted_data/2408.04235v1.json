{
  "paper_id": "2408.04235v1",
  "title": "Lldif: Diffusion Models For Low-Light Emotion Recognition",
  "published": "2024-08-08T05:41:09Z",
  "authors": [
    "Zhifeng Wang",
    "Kaihao Zhang",
    "Ramesh Sankaranarayana"
  ],
  "keywords": [
    "Low-Light",
    "emotion recognition",
    "diffusion model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper introduces LLDif, a novel diffusion-based facial expression recognition (FER) framework tailored for extremely low-light (LL) environments. Images captured under such conditions often suffer from low brightness and significantly reduced contrast, presenting challenges to conventional methods. These challenges include poor image quality that can significantly reduce the accuracy of emotion recognition. LLDif addresses these issues with a novel two-stage training process that combines a Label-aware CLIP (LA-CLIP), an embedding prior network (PNET), and a transformer-based network adept at handling the noise of low-light images. The first stage involves LA-CLIP generating a joint embedding prior distribution (EPD) to guide the LLformer in label recovery. In the second stage, the diffusion model (DM) refines the EPD inference, ultilising the compactness of EPD for precise predictions. Experimental evaluations on various LL-FER datasets have shown that LLDif achieves competitive performance, underscoring its potential to enhance FER applications in challenging lighting conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the domain of computer vision, precisely identifying facial emotions presents a notable challenge, particularly in extremely low-light environments. Such environments can significantly impair the quality of captured images, leading to degraded visibility of facial features, which are crucial for precise emotion recognition. This degradation not only destroys the basic structure of the face but also introduces noise and distortion, further complicating the task for emotion recognition algorithms. In Fig.  1 , the low-light image (LL) at the top shows a child's face that is shadowed and details are obscured, making it challenging to discern fine facial expressions. The histograms indicate that most pixel values are clustered toward the darker end of the spectrum, which suggests limited brightness and contrast in the image. In the normal-light image (CI) at the bottom, the child's face is clearly visible with good detail, essential for recognizing emotions. The histograms show a more even distribution of pixel values across the Fig.  1 : Top: the low-light image (LL) shows a child's face that is shadowed and details are obscured, making it challenging to discern fine facial expressions. Bottom: In the normal-light image (CI) at the bottom, the child's face is clearly visible with good detail, essential for recognizing emotions. spectrum, with higher frequencies in the mid to high ranges, indicating better brightness and contrast. Traditional facial expression recognition (FER) methods  [10, 13, 14, 17, 22, 29]  perform well under normal-light conditions; however, their effectiveness is considerably diminished in low-light scenarios due to the loss of subtle facial structures. There is a need for robust methodologies that can overcome the challenges posed by low brightness while maintaining high accuracy in emotion recognition.\n\nCurrently, several approaches have been developed to tackle the challenge of learning from noisy data in the field of emotion recognition. RUL  [25]  proposes to improve facial expression recognition by weighing uncertainties based on the difficulty of samples to enhance performance in noisy environments. SCN  [16]  addresses uncertainties in facial expression recognition efforts by using a selfattention block to choose training samples and correcting uncertain labels by using a relabeling approach, thereby improving the learning process's dependability. However, both methods require relabeling the samples based on the samples' difficulties. EAC  [26]  addresses noisy labels by using flipped image consistency and selective features, preventing the model from relying on misleading features and thereby improving learning accuracy. However, when these techniques are used in low-light images, they encounter challenges. In particular, RUL  [25]  and EAC  [26]  are based on the assumption of minimal losses. In extremely low-light settings, where clear, fine facial details are lacking, these approaches might mistakenly equate challenging samples with noisy ones since both can display high loss values in the training of low-light images.  To solve these issues, this paper proposes a novel method for handling noisy images in low-light conditions, departing from the conventional method of identifying noisy samples by their loss values. Instead, we introduce a distinctive approach centered on learning the joint distribution of noise labels and images via feature extraction and label restoration. We aim to create a diffusion-based network for facial expression recognition (FER) that use the capabilities of diffusion models (DMs) for effective label restoration by aligning them with their related images. To achieve this, we present LLDif. Considering the transformer's capability to handle long-range pixel dependencies, we employ transformers as the foundational blocks of the LLDif architecture. We organize transformer blocks in a U-Net configuration to form the Low-Light Transformer (LLformer), which is aimed at extracting features at multiple levels. The LLformer comprises two parallel networks: the DTNet, tasked with extracting latent features from lowlight images at various depths, and the DLNet, which focuses on identifying the similarities between low-light images and facial landmarks. LLDif adopts a two-stage training approach: (1) In the first stage, as illustrated in Fig.  2  (a), we use LA-CLIP to process the low-light image along with its image caption and label, generating a Joint Embedding Prior Distribution (EPD) Z. This EPD is then utilized to guide the LLformer in label restoration.  (2)  In the second stage, shown in Fig.  2 (b ), the diffusion model (DM) can be trained to deduce the accurate EPD directly from low-light images. Owing to the compactness of EPD Z, the DM can make highly accurate EPD predictions, achieving consistent high accuracy after only a few iterations.\n\nThis study offers several notable contributions, detailed as follows: 1) We introduce a innovative diffusion-based approach designed to address the challenges encountered in facial expression recognition, particularly those arising from diminished brightness and contrast in low-light conditions. 2) Our LLDif model harnesses the powerful distribution mapping capabilities of diffusion models (DMs) to generate an accurate embedding prior distribution (EPD), significantly enhancing the precision and reliability of facial expression recognition (FER) results. This method stands out for its independence from the need to understand the dataset's uncertainty distribution, distinguishing it from prior approaches. 3) Extensive testing has demonstrated that LLDif achieves impressive performance in emotion recognition tasks across three low-light FER datasets, underscoring its effectiveness.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Facial Expression Recognition. Facial Expression Recognition (FER)  [18, 23, 24]  focuses on enabling computers to interact with humans by identifying human facial expressions. In recent years, the accuracy of recognizing expressions under normal-light conditions has seen substantial improvements. Kollias et al.  [4]  introduces a CNN-RNN hybrid method that leverages multi-level visual features for dimensional emotion recognition. Zhao et al.  [28]  introduces Former-DFER, a dynamic transformer that combines spatial and temporal transformers to robustly capture facial features against occlusions and pose variations, achieving top performance on an emotion recognition dataset. The Expression Snippet Transformer (EST)  [8]  enhances video-based facial expression recognition by decomposing videos into expression snippets for detailed intra-and inter-snippet analysis, significantly outperforming conventional CNN-based approaches. Vazquez et al.  [15]  introduces a Transformer-based model, pre-trained on unlabeled ECG datasets and fine-tuned on the AMIGOS dataset, achieving top emotion recognition performance by leveraging attention mechanisms to emphasize relevant signal parts.\n\nDiffusion Models. Diffusion models are now utilized across a wide range of tasks, including image enhancement for higher resolution, as mentioned by  Shang et al. (2024)    [12] , and creative image modifications, as highlighted by  Yang et al. (2023)    [21] . Moreover, the latent features captured by diffusion models have proven beneficial for classification tasks such as image classification, as noted by  Han et al. (2022)    [3] , and for segmentation in medical imaging, as demonstrated by  Wu et al. (2024)    [20] . Zhang et al.  [27]  introduces a novel approach for editing single images using pre-trained diffusion models, combining model-based guidance with patch-based fine-tuning to prevent overfitting and enable high-resolution content creation and manipulation based on textual descriptions. Rahman et al.  [11]  presents a diffusion model-based approach for medical image segmentation that learns from collective expert insights to generate a variety of accurate segmentation masks, outperforming existing models in capturing natural variations and evaluated by a new metric aligned with clinical standards.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Label-Aware Clip",
      "text": "The key idea of LA-CLIP is to train the feature learner F l to output low-light features while simultaneously predicting the image's label. As summarized in Fig.  2  (a), the low-light feature embedding f I c is matched with the image's caption f t c . Moreover, the low-light label embedding f I l , predicted by the feature learner F l , is aligned with the input label embedding f t l . This module helps to create embeddings that correlate visual features with textual annotations, which could be vital for low-light emotion recognition. It is designed to support the LLformer in label restoration, leveraging pre-trained models to guide the network in accurately predicting labels for low-light images.\n\nAs depicted in the yellow box of Fig.  2  during stage 2, P N ET s1 employs cross-attention layers to infer the Embedding Prior Distribution (EPD) Z. Following this extraction, DTNet leverages the EPD to aid in label recovery. Within DTNet, as shown in the same yellow box of Fig.  2 , the architecture comprises DMNet and DGNet. We use the pre-trained LA-CLIP model to get the low-light feature embedding f l c and low-light label embedding f I l ; these embeddings are then input into PNETs1. The output from PNETs1 is the EPD Z, denoted as Z ∈ R C . This process is detailed in (Eq. 1):\n\nSubsequently, Z is fed into the DTNet in Fig.  3 , acting as adjustable parameters to support the process of label restoration, as detailed in Equation  (2) .\n\nhere, W represents the weights of a fully connected layer, LN denotes layer normalization and • symbolizes element-wise multiplication. In DMNet Fig.  3  (b), we process the entire image to extract detailted information. The features F ′ are converted into three different vectors: key K, query Q, and value V , through a convolutional layer. These vectors are reshaped as to focus on, and generate an attention map A ∈ R C ′′ ×C ′′ . This operation in DMNet is depicted in the following equation Equation  ( 3):\n\nwhere α serves as a tunable parameter during the training phase. Following this, the DGNet focuses on extracting both local and neighboring features through aggregation. This is achieved by employing a small Convolution (1 × 1) to extract local features, and a larger Convolution (3 × 3) to collect information from adjacent pixels. Furthermore, a specialized gating mechanism is utilized to ensure only the most important information is captured. The entire process within DGNet is depicted in the following equation (Eq. (  4 )):",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Dynamic Landmarks And Image Network (Dlnet)",
      "text": "Within the DLNet, a cross window attention approach is utilized to process features from both 2D facial landmarks and related images taken in low-light conditions. We start by dividing the low-light image features, denoted as X ll ∈ R N ×D , into various distinct, non-overlapping windows x ll ∈ R M ×D . In parallel, features from facial landmarks, represented as X f l ∈ R C×H×W , are downscaled to align with the dimensions of these windows, yielding x f l ∈ R c×h×w , where the dimension c matches D and the production h and w equate to M . This setup enables the application of cross-attention between features of facial landmarks and low-light images, as depicted in Equation (Eq. 7).\n\nwhere w O , w K , w Q and w V represent the weight matrices, and b denotes the corresponding positional bias. This cross-attention mechanism is implemented on every window of the lowlight image, termed as MHCA. The equations that describe the transformer encoder within LLDif are presented as follows (Eq. (  9 )):\n\nthe fusion of output features F from DTNet and O from DLNet is required to produce the combined multi-scale features x 1 , x 2 , and x 3 . This involves concatenating the corresponding features:\n\nFollowing this, the fused features X undergo additional processing through standard transformer blocks.\n\nwhere M SA denotes the self-attention blocks with multiple heads and LN refers to the layer normalization. The definition of the training loss is given as follows (Eq. (  13 )):\n\nOur model is trained using the cross-entropy loss function, where M is the number of distinct classes, and N signifies the total count of samples. Here, y ic indicates whether class c is the correct classification for observation i, and p ic is the probability predicted by the model.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Diffusion Model For Label Restoration",
      "text": "In the second stage, as shown in Fig.  2 (b ), the diffusion model's (DM) strong capabilities are employed to approximate the joint Embedding Prior Distribution (EPD). Initially, the pre-trained LA-CLIP and PNET S1 is used to acquire the EPD Z ∈ R C . Following this, the diffusion technique is applied to Z, resulting in a generated sample Z T ∈ R C , as explained in (Eq. (  14 )):\n\nhere, T represents the total count of diffusion steps. The variable α t is defined as 1 -β t , and ᾱT denotes the cumulative product of α i for all steps from 0 to T . The term β t is a predetermined hyper-parameter, while N (.) signifies the standard Gaussian distribution.\n\nDuring the reverse process of the diffusion model, low-light images x are fed into PNET s2 to derive a conditional vector x s2 ∈ R C as outlined in Equation  (15) .\n\nwhere PNET s2 includes convolutional layer, residual layer and linear layer, which will ensure the output's dimension of PNET s2 is same as PNET s1 .\n\nThe denoising network, represented as ϵ θ , estimate the noise for each specific time step t. It processes the current noisy data Z ′ t , the time step t, and a conditional vector x s2 , which is obtained from the low-light image via the stage-two prior distribution network PNET s2 . The estimated noise, expressed as ϵ θ (Concat(Z ′ t , t, x s2 )), is then utilized in the subsequent formula to determine the denoised data Z ′ t-1 for the upcoming step, as illustrated in Equation (  16 ):\n\nAfter T iterations, we get the final embedding prior distribution (EPD), symbolized as Z ′ 0 . The stage-two prior distribution network (PNET s2 ), together with the denoising network and the Low-Light Transformer (LLformer), are jointly optimized through the total loss function L total , as depicted in Equation  (18) .\n\nIn this formula, Z norm (i) and Znorm (i) refer to the EPDs derived from LA-CLIP and LLDif S2 , respectively, both normalized through softmax. The term L kl represents a form of the Kullback-Leibler divergence, computed over C dimensions. The total loss, L total , is formulated by adding the Kullback-Leibler divergence loss L kl (Eq. 17) to the Cross-Entropy loss L ce (Eq. 13 ARM  [14]  90.42 DACL  [2]  83.52 DACL  [2]  88.61 POSTER++  [9]  92.21 POSTER++  [9]  86.46 POSTER++  [9]  94.44 RUL  [25]  88.98 RUL  [25]  85.00 RUL  [25]  87.83 DAN  [19]  89.70 DAN  [19]  85.48 DAN  [19]  88.77 SCN  [16]  87.03 SCN  [16]  83.11 SCN  [16]  89.55 EAC  [26]  90.35 EAC  [26]  86.18 EAC  [26]  72.32 MANet  [29]  88 recognition (FER) algorithms under low-light conditions. Likewise, the RAF-DB dataset  [7]  includes 7 emotional categories and mirrors the testing and training configuration of LL-RAF-DB dataset. The expression distribution is consistent across both datasets. LL-FERPlus dataset expands the scope to low-light conditions, presenting a comprehensive collection of 7,178 for testing and 28,709 images for training in low-light settings. The FERPlus dataset  [1] , an extension of the FER2013 dataset, is enriched with additional labels from ten different annotators and features the same quantity of training and testing images as the LL-FERPlus dataset.\n\nLL-KDEF dataset contains 4,900 images captured under low-light conditions, taken from five unique angles. It comprises 3,920 images in the training set and 980 in the testing set. The KDEF dataset  [6] , with an identical total of 4,900 images, is a comprehensive collection in which each facial expression is photographed from five distinct viewpoints, ensuring a broad spectrum of clear visual information.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implementation Details",
      "text": "We use Adobe Lightroom  [5]  to create three benchmark low-light facial expression recognition (LL-FER) datasets, simulating degraded image conditions by adjusting the exposure, white balance, highlights, and shadows. The experimental setup utilized PyTorch for model training, which was carried out on a GTX-3090 GPU. For optimization, the Adam algorithm was chosen, with the training spanning 200 epochs. The adopted training settings specified an initial learning rate of 3.5 × 10 -4 , a batch size of 64, and a weight decay parameter set to 1 × 10 -4 . Table  4 : Evaluation of accuracy (%) compared to SOTA FER methods on the LL-KDEF Dataset.\n\nDAN  [19]  POSTER++  [9]  EAC  [26]  MANet  [29]  RUL  [25]  SCN  [16]     are based on the ResNet-18 architecture. However, POSTER++  [9]  stands out by adopting the Vision Transformer architecture. In contrast, our model introduces a novel approach by incorporating a 'Diffusion' backbone, moving away from the traditional ResNet-18 design. In the Table  2 , the proposed method attains the highest accuracy of 82.26%, which is a notable enhancement over other methodologies. POSTER++  [9]  registers the second highest accuracy with 80.76%, followed by DAN  [19] at 79.27%. The EAC  [26] , MANet  [29] , RUL  [25] , Fig.  6 : Confidence score of different methods on KDEF dataset. Accuracy for each method is marked on the top. The baseline  [16]  method fails as FER data have small inter-class distances. DAN  [19]  and POSTER++  [9]  have relative high confidence score while they still fall a lot in low confidence score area. Our method can effectively separates different emotion samples on clear and low-light images.\n\nSCN  [16] , and DACL  [2]  algorithms show a relative low accuracy from 75.20% to 78.72%. Table  3 , which focuses on the LL-FERPlus Dataset, shows \"Ours\" with a leading accuracy of 82.25%, marginally surpassing POSTER++'s  [9]  81.44%. In the Table  4 , \"Ours\" shows the highest accuracy at 92.97%, which is significantly higher than the other methods listed. The second most accurate method is POSTER++  [9] , with an accuracy of 88.93%. Other methods such as DAN  [19] , EAC  [26] , MANet  [29] , RUL  [25] , SCN  [16] , and DACL  [2]  present accuracies ranging from 43.53% to 86.69%. These results underscore the efficiency of the diffusion-based approach within the context of facial expression recognition systems under low-light conditions. Feature Visualization. We used the t-SNE method to illustrate how models discern feature distributions. In contrast to Fig.  5 (a)  and (b) , where the SCN model has difficulty separating different emotion categories, especially in low-light conditions, our LLDif model exhibits effective expression recognition in both clear and degraded low-light images. This indicates that LLDif successfully captures key features crucial for distinguishing between various emotional expressions categories.\n\nVisualization of Confidence Scores. We visualize the distribution of confidence scores for facial expression recognition methods on clear and low-light  images in Fig.  6 . For the baseline method  [16] , the mean confidence score for clear images is 0.41 and for low-light images is 0.34, with an overall accuracy of 0.435. The DAN method  [19]  shows a mean confidence score of 0.42 for clear images and 0.37 for low-light images, with an overall accuracy of 0.820. The POSTER++ method  [9]  has mean scores of 0.46 for clear images and 0.45 for low-light images, achieving an overall accuracy of 0.889. The proposed method exhibits a notably higher confidence level with mean scores of 0.57 for clear images and 0.53 for low-light images, corresponding to a high overall accuracy of 0.929. The proposed method not only shows the highest accuracy but also the small difference in confidence score between clear and low-light images, suggesting robust performance even in challenging lighting conditions.",
      "page_start": 9,
      "page_end": 13
    },
    {
      "section_name": "Ablation Study",
      "text": "This section evaluates the impacts of crucial components within LLDif, including the Diffusion Model (DM), various loss functions, and the insert noise during the training phase, as depicted in Table  5 .  (1)  The contrast between LLDif S2 -V3 and LLDif S2 -V1 underscores the DM's robust ability in accurately predicting the embedding prior distribution EPD.  (2)  The insert noise into the DM's process in LLDif S2 -V4 is demonstrated to enhance the accuracy of EPD predictions.\n\n(3) The efficiency of different loss functions is also examined. The comparison between using L ce in LLDif S2 -V4 (refer to Eq. (  13 )) and L total in LLDif S2 -V2 (refer to Eq. (  18 )) shows that using L ce is required for achieving better accuracy.\n\nImpact of iteration numbers. This section examines how varying the number of iterations in the Diffusion Model (DM) influences the LLDif S2 performance. We experimented with different iteration numbers in LLDif S2 , adjusting the β t value (with α t set as 1 -β t , as outlined in Eq. 14) to ensure the variable Z evolves toward a Gaussian distribution, Z T ∼ N (0, 1). Figures  8  and 7  demonstrate that LLDif S2 's performance notably enhances at 4 iterations. Increasing the iteration number over 4 iterations does not substantially impact model's performance, suggesting the attainment of an optimal threshold. Notably, LLDif S2 reaches convergence more quickly than traditional DM methods, which typically requires over 50 iterations. This enhanced efficiency results from applying DM on the EPD, which is a one-dimensional, concise vector.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we present LLDif, an innovative framework utilising diffusion-based method to enhance facial expression recognition under low-light conditions. Addressing the challenges of image quality degradation in low-light settings, LLDif employs a two-stage training approach, utilizing a label-aware CLIP (LA-CLIP), an embedding prior distribution network (PNET), and a diffusion-based transformer network (LLformer). By integrating advanced architecture like the PNET and LLformer, LLDif can effectively restore emotion labels from degraded lowlight images at multiple scale. Our experiments confirms that LLDif outperforms existing methods, gains competitive performance on three low-light facial expression recognition datasets.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the low-light image (LL) at the top shows a",
      "page": 1
    },
    {
      "caption": "Figure 1: Top: the low-light image (LL) shows a child’s face that is shadowed and",
      "page": 2
    },
    {
      "caption": "Figure 2: The proposed LLDif framework, comprising Label-aware CLIP (LA-",
      "page": 3
    },
    {
      "caption": "Figure 2: (b), the diffusion model (DM) can be trained to deduce the",
      "page": 4
    },
    {
      "caption": "Figure 2: (a), the low-light feature embedding f I",
      "page": 5
    },
    {
      "caption": "Figure 2: during stage 2, PNETs1 employs",
      "page": 5
    },
    {
      "caption": "Figure 2: , the architecture comprises",
      "page": 5
    },
    {
      "caption": "Figure 3: The overview of DTNet, which consists of DGNet and DMNet.",
      "page": 6
    },
    {
      "caption": "Figure 2: (b), the diffusion model’s (DM) strong",
      "page": 7
    },
    {
      "caption": "Figure 4: Emotion distribution for samples in LL-RAF-DB dataset and RAF-DB.",
      "page": 10
    },
    {
      "caption": "Figure 4: Accuracy comparisons between our model and other SOTA FER meth-",
      "page": 10
    },
    {
      "caption": "Figure 5: The predicted feature visualised by t-SNE between our method and SCN.",
      "page": 11
    },
    {
      "caption": "Figure 6: Confidence score of different methods on KDEF dataset. Accuracy for",
      "page": 12
    },
    {
      "caption": "Figure 5: (a) and (b), where the",
      "page": 12
    },
    {
      "caption": "Figure 7: Progressive clustering of features in diffusion space visualized using t-SNE",
      "page": 13
    },
    {
      "caption": "Figure 8: Analyse impacts of iterations in DM.",
      "page": 13
    },
    {
      "caption": "Figure 6: For the baseline method [16], the mean confidence score for",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Stage 1: Label-aware CLIP (LA-CLIP) (b) Stage 2: Label restortion with LLformer Predicted Label": "LL Image y\nSurprise,\nThe Fear,\nfacial Disgust,\nexpression Happy,\nis Sad,\nHappy Anger,\nNeutral\nCaption Label\nFeature Image Text\nLearner Encoder Pretrained Encoder\nCLIP\nConstrastive\nGradient\nFlow\nLearning",
          "Column_2": "Feature Image\nLearner Encoder Input\nConv\nTra B n l s o f c o k rm er Flatten F1 C O1 K Q V K Q V MH + CA\nLN\nDownScale\nMLP\nQ Matm K ul V D D M GN N e e t t Flatten F2 C O2 In O pu u t + tput\nScale Conv\nSoftmax DownScale MHCA\n+\nMa F t C mul Tra B n l s o f c o k rm er Flatten F3 C O3 K Q V K Q V M L L N P\nOutput Low-light Transformer (LLformer) Ou + tput",
          "Column_3": ""
        },
        {
          "(a) Stage 1: Label-aware CLIP (LA-CLIP) (b) Stage 2: Label restortion with LLformer Predicted Label": "Repeat T Times\nDDPM DDPM\nLLformer\n(b) Stage 2: Train Diffusion LLDif (LLDifS2) & Inference Reverse Denoising Process",
          "Column_2": "",
          "Column_3": "DDPM\nem C b a e p d ti d o i n ng E T m ex b t e L d a d b in e g l Low-Light Face\nDiffusion Model Image Cross-\nLL Feature Attention Network\nEmbedding Text DGNet MHCA\nEncoder\nE L m L b L e a d b d e in l g C E L n I c P o T d e e x r t F D o y r n w a a m rd ic n G et a w te o d rk Mult A i-h tt e e a n d tio C n ross\nImage DMNet LN\nCL E I E n P c n I o c m d o a e d g r e e r Par L a o m c e k ters Multi-h N e e a t d w o A r t k tentionNorm La a y li e za r tion\nMLP\nPrio N r e E t x w tr o a r c k tion Feature Learner Fac a i t a te l n L t a io n n d m ne a t r w k o c r r k ossP M e u rc lt e ila p y ti e o r n"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Evaluation of accuracy (%) compared to SOTA FER methods on the",
      "data": [
        {
          "Surprise": "0.4764 0.3367 0.5632 0.3589 0.6885 0.2883 0.3979 0.3388",
          "Disgust": "",
          "Happy": "",
          "Anger": "",
          "Sadness": ""
        },
        {
          "Surprise": "Surprise",
          "Disgust": "Disgust",
          "Happy": "Happy",
          "Anger": "Anger",
          "Sadness": "Sadness"
        },
        {
          "Surprise": "Surprise Fear Disgust Happy Sadness Anger Neutral",
          "Disgust": "",
          "Happy": "",
          "Anger": "",
          "Sadness": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 4: Evaluation of accuracy (%) compared to SOTA FER methods on the",
      "data": [
        {
          "Column_1": "(a) SCN on clear images (b) SCN on low-light images",
          "Column_2": "",
          "Column_3": "",
          "Column_4": ""
        },
        {
          "Column_1": "(c) LLDif on clear images (d) LLDif on low-light images",
          "Column_2": "",
          "Column_3": "",
          "Column_4": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: , \"Ours\" shows the highest accuracy at 92.97%, which is signifi-",
      "data": [
        {
          "BASELINE: 0.435 DAN: 0.820": "POSTER++: 0.889 Ours: 0.929"
        },
        {
          "BASELINE: 0.435 DAN: 0.820": "Confidence Score Confidence Score"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "ICMI"
    },
    {
      "citation_id": "2",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "A Farzaneh",
        "X Qi"
      ],
      "year": "2021",
      "venue": "Facial expression recognition in the wild via deep attentive center loss"
    },
    {
      "citation_id": "3",
      "title": "Card: Classification and regression diffusion models",
      "authors": [
        "X Han",
        "H Zheng",
        "M Zhou"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "4",
      "title": "Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Unpaired image enhancement featuring reinforcementlearning-controlled image editing software",
      "authors": [
        "S Kosugi",
        "T Yamasaki"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "6",
      "title": "Latent-ofer: Detect, mask, and reconstruct with latent vectors for occluded facial expression recognition",
      "authors": [
        "I Lee",
        "E Lee",
        "S Yoo"
      ],
      "year": "2023",
      "venue": "Latent-ofer: Detect, mask, and reconstruct with latent vectors for occluded facial expression recognition"
    },
    {
      "citation_id": "7",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "8",
      "title": "Expression snippet transformer for robust video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "W Wang",
        "C Feng",
        "H Zhang",
        "Z Chen",
        "Y Zhan"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Poster v2: A simpler and stronger facial expression recognition network",
      "authors": [
        "J Mao",
        "R Xu",
        "X Yin",
        "Y Chang",
        "B Nie",
        "A Huang"
      ],
      "year": "2023",
      "venue": "Poster v2: A simpler and stronger facial expression recognition network",
      "arxiv": "arXiv:2301.12149"
    },
    {
      "citation_id": "10",
      "title": "Four-player groupgan for weak expression recognition via latent expression magnification",
      "authors": [
        "W Niu",
        "K Zhang",
        "D Li",
        "W Luo"
      ],
      "year": "2022",
      "venue": "KBS"
    },
    {
      "citation_id": "11",
      "title": "Ambiguous medical image segmentation using diffusion models",
      "authors": [
        "A Rahman",
        "J Valanarasu",
        "I Hacihaliloglu",
        "V Patel"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "12",
      "title": "Resdiff: Combining cnn and diffusion model for image super-resolution",
      "authors": [
        "S Shang",
        "Z Shan",
        "G Liu",
        "L Wang",
        "X Wang",
        "Z Zhang",
        "J Zhang"
      ],
      "year": "2024",
      "venue": "AAAI"
    },
    {
      "citation_id": "13",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "J She",
        "Y Hu",
        "H Shi",
        "J Wang",
        "Q Shen",
        "T Mei"
      ],
      "year": "2021",
      "venue": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition"
    },
    {
      "citation_id": "14",
      "title": "Learning to amend facial expression representation via de-albino and affinity",
      "authors": [
        "J Shi",
        "S Zhu",
        "Z Liang"
      ],
      "year": "2021",
      "venue": "Learning to amend facial expression representation via de-albino and affinity",
      "arxiv": "arXiv:2103.10189"
    },
    {
      "citation_id": "15",
      "title": "Transformer-based self-supervised learning for emotion recognition",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2022",
      "venue": "ICPR"
    },
    {
      "citation_id": "16",
      "title": "Suppressing uncertainties for largescale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Suppressing uncertainties for largescale facial expression recognition"
    },
    {
      "citation_id": "17",
      "title": "Htnet for micro-expression recognition",
      "authors": [
        "Z Wang",
        "K Zhang",
        "W Luo",
        "R Sankaranarayana"
      ],
      "year": "2024",
      "venue": "Htnet for micro-expression recognition"
    },
    {
      "citation_id": "18",
      "title": "Lrdif: Diffusion models for underdisplay camera emotion recognition",
      "authors": [
        "Z Wang",
        "K Zhang",
        "R Sankaranarayana"
      ],
      "year": "2024",
      "venue": "Lrdif: Diffusion models for underdisplay camera emotion recognition",
      "arxiv": "arXiv:2402.00250"
    },
    {
      "citation_id": "19",
      "title": "Distract your attention: Multi-head cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2023",
      "venue": "Biomimetics"
    },
    {
      "citation_id": "20",
      "title": "Medsegdiff-v2: Diffusion-based medical image segmentation with transformer",
      "authors": [
        "J Wu",
        "W Ji",
        "H Fu",
        "M Xu",
        "Y Jin",
        "Y Xu"
      ],
      "year": "2024",
      "venue": "AAAI"
    },
    {
      "citation_id": "21",
      "title": "Paint by example: Exemplar-based image editing with diffusion models",
      "authors": [
        "B Yang",
        "S Gu",
        "B Zhang",
        "T Zhang",
        "X Chen",
        "X Sun",
        "D Chen",
        "F Wen"
      ],
      "year": "2023",
      "venue": "Paint by example: Exemplar-based image editing with diffusion models"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "K Zhang",
        "Y Huang",
        "Y Du",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "23",
      "title": "Authentic emotion mapping: Benchmarking facial expressions in real news",
      "authors": [
        "Q Zhang",
        "Z Wang",
        "Y Liu",
        "Z Qin",
        "K Zhang",
        "S Caldwell",
        "T Gedeon"
      ],
      "year": "2024",
      "venue": "Authentic emotion mapping: Benchmarking facial expressions in real news",
      "arxiv": "arXiv:2404.13493"
    },
    {
      "citation_id": "24",
      "title": "Geometric-aware facial landmark emotion recognition",
      "authors": [
        "Q Zhang",
        "Z Wang",
        "Y Liu",
        "Z Qin",
        "K Zhang",
        "T Gedeon"
      ],
      "year": "2023",
      "venue": "2023 6th International Conference on Software Engineering and Computer Science (CSECS)"
    },
    {
      "citation_id": "25",
      "title": "Relative uncertainty learning for facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "W Deng"
      ],
      "year": "2021",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "26",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "X Ling",
        "W Deng"
      ],
      "year": "2022",
      "venue": "Learn from all: Erasing attention consistency for noisy label facial expression recognition"
    },
    {
      "citation_id": "27",
      "title": "Sine: Single image editing with text-to-image diffusion models",
      "authors": [
        "Z Zhang",
        "L Han",
        "A Ghosh",
        "D Metaxas",
        "J Ren"
      ],
      "year": "2023",
      "venue": "Sine: Single image editing with text-to-image diffusion models"
    },
    {
      "citation_id": "28",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Z Zhao",
        "Q Liu"
      ],
      "year": "2021",
      "venue": "Former-dfer: Dynamic facial expression recognition transformer"
    },
    {
      "citation_id": "29",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Z Zhao",
        "Q Liu",
        "S Wang"
      ],
      "year": "2021",
      "venue": "IEEE TIP"
    }
  ]
}