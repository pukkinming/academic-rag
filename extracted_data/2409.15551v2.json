{
  "paper_id": "2409.15551v2",
  "title": "Revise, Reason, And Recognize: Llm-Based Emotion Recognition Via Emotion-Specific Prompts And Asr Error Correction",
  "published": "2024-09-23T21:07:06Z",
  "authors": [
    "Yuanchao Li",
    "Yuan Gong",
    "Chao-Han Huck Yang",
    "Peter Bell",
    "Catherine Lai"
  ],
  "keywords": [
    "Emotion Recognition",
    "LLM",
    "ASR",
    "Acoustics",
    "Linguistics",
    "Psychology"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Annotating and recognizing speech emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic Speech Recognition (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a REVISE-REASON-RECOGNIZE prompting pipeline for robust LLM-based emotion recognition from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion recognition. Our study aims to refine the use of LLMs in emotion recognition and related domains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Recent studies have suggested that Large Language Models (LLMs) have the ability to reason about emotional content  [1] . This finding has encouraged researchers to further explore the \"emotional intelligence\" (e.g., emotion recognition, interpretation, and understanding) of LLMs. For example,  [2]  developed a psychometric assessment focusing on emotion understanding to compare the emotional intelligence of LLMs and humans. They found that most LLMs achieved above-average Emotional Quotient (EQ) scores, with GPT-4 surpassing 89% of human participants with an EQ of 117.\n\nTherefore, the use of LLMs in text-based emotion recognition has emerged as a resource-efficient and effort-saving alternative to human annotators and traditional emotion classifiers for two main reasons: 1) Emotion recognition requires substantial human effort. Typically, multiple annotators are needed for each sample to reach a majority vote, ensuring accurate assessment. Although platforms like Amazon Mechanical Turk provide a relatively efficient solution, concerns persist regarding privacy leaks, subjective bias, and the reliability of annotations  [3] . and model building, which require careful consideration of which features, models, and algorithms to use.\n\nTo this end, researchers have recently started exploring and utilizing LLMs for emotion annotation and recognition. Their work includes various approaches, such as using multiple-step prompting with LLMs  [4] , examining prompt sensitivity  [5] , integrating outputs from multiple LLMs  [3] , and incorporating acoustic information  [6] . Despite these efforts, understanding of the usage, efficacy, and reliability of LLM-based approaches remains limited, especially on Automatic Speech Recognition (ASR) transcription. Therefore, building upon existing literature, we conduct various experiments on LLM-based emotion recognition to investigate effective prompting practices. Our major contributions are:\n\n• We propose novel prompts for LLM-based emotion recognition, integrating emotion-specific knowledge from acoustics, linguistics, and psychology.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Interest in LLM-based emotion recognition has surged recently, with the availability of pre-trained models. Studies in this area have explored a variety of approaches.  [7]  inferred emotion labels using three different prompting approaches: text generation, mask filling, and textual entailment, employing a fine-grained emotion taxonomy.  [3]  proposed an ensemble approach that integrates outputs from multiple LLMs, leveraging a Mixture of Experts (MoE) reasoning model. They trained emotion classifiers using MoE-generated emotion labels from both ground-truth and ASR transcriptions, and tested these classifiers on ground-truth labels, demonstrating comparable performance in emotion classification.  [4]  employed a multi-step prompting technique with few training samples for text emotion recognition.  [6]  and  [8]  incorporated textual acoustic feature descriptors into prompts.  [9]  investigated several approaches,including in-context learning, fewshot learning, accuracy, generalization, and explanation.  [5]  arXiv:2409.15551v2 [eess.AS] 30 Apr 2025 examined various prompting techniques, including chain-ofthought, role-play, and their variations.\n\nDespite these advancements, we argue that each approach has its limitations, leaving several concerns unaddressed. For example,  [9]  did not study prompting,  [4]  only proposed a multi-step prompting without further exploration. The prompting techniques tested by  [5]  were not specifically designed for emotion.  [6]  and  [8]  only considered basic acoustic features, without incorporating more emotion-related properties. Furthermore, both  [3]  and  [7]  used ASR transcriptions generated by Whisper, whose output has been shown to be robust in emotion recognition even with ASR errors  [10] . However, this setting is not ideal for LLMs handling challenging ASR transcriptions in real-world emotion applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "To address the issues above, we 1) Develop prompts that incorporate emotion-specific knowledge; 2) Incorporate ASR Error Correction (AEC) to refine transcriptions for robust emotion recognition; and 3) Explore LLM training schemes to further improve the performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Prompting With Emotion-Specific Knowledge",
      "text": "In light of the relationship between emotion and relevant disciplines, we extract useful knowledge from acoustics, linguistics, and psychology, to develop emotion-specific prompts for emotion recognition. The prompts are presented in Fig.  1 .\n\nAcoustic information plays a crucial role in distinguishing speech emotions. Features like energy, pitch, and speaking rate have proven useful when been incorporated into prompts as textual descriptors  [6] . Similarly, gender information, which is highly correlated with pitch, has also been shown to be useful  [8] ,  [11] . However, these features are insufficient to fully describe fine-grained differences in emotions beyond the Big Four. Hence, we hypothesize that additional acoustic features can enhance LLMs' emotion recognition ability. We propose including pitch range, jitter, and shimmer to incorporate midlevel prosody (between frame-level and utterance-level) and voice quality.\n\nLinguistic structure is essential for understanding emotion in text. For example,  [12]  investigated the impact of partof-speech, affective score, and utterance length on emotion. However, to our knowledge, only one work  [13]  has utilized linguistics via identifying emotion triggers (i.e., words that elicit the emotion) when prompting LLMs for emotion prediction. Thus, we hypothesize that LLM-based emotion recognition can benefit from more linguistic knowledge as LLM processing is inherently text-based. We propose including ASRemotion relationships among emotion category, Word Error Rate (WER), and utterance length as outlined in  [12] .\n\nPsychological theories, such as self-monitoring, social cognitive theory, and cognitive emotion regulation have proven effective in improved LLMs' performance across various tasks  [14] ,  [15] . Therefore, we hypothesize that LLMs' emotion recognition ability can resonate with their emotional intelligence and thus be enhanced. We propose incorporating positive and negative stimuli from  [14] ,  [15] , as well as create our novel competitive stimuli.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Emotion Recognition With Asr Error Correction",
      "text": "Traditional emotion recognition often struggles with imperfect text  [10] . We argue that it is more challenging to prompt LLMs for emotion recognition on ASR transcription compared to human transcription, due to the presence of word errors. Therefore, we propose the R3 prompting pipeline to perform emotion recognition with AEC and reasoning on ASR transcriptions. The R3 pipeline involves three steps: Revise, where ASR errors are corrected based on N-best hypotheses; Reason, where the LLMs self-explain based on the corrected transcriptions and emotion-specific knowledge; and Recognize, where the emotion is recognized. To incorporate AEC into our prompts, we follow an AEC-specific Alpaca prompt  [16] , which uses the \"You are an ASR error corrector\" instruction, guiding the LLMs to perform error correction. As LLMs have proven their ability in both AEC and emotion recognition  [17] , this format is expected to facilitate seamless integration with our emotion prompting, instructing the LLMs to function simultaneously as both an ASR error corrector and an emotion recognizer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Exploring Llm Training Schemes",
      "text": "To understand how LLM training schemes contribute to emotion recognition, we explore context-aware learning, incontext learning, and instruction tuning. For context-aware learning, we organize the sentences in the conversation order and compare different context windows (i.e., the number of sentences preceding the sentence to be recognized). For incontext learning, we test and compare several few-shot cases. For instruction tuning, we apply Parameter-Efficient Fine-Tuning (PEFT) using LoRA  [18] . We set learning rate, weight decay, and epoch to 1e-4, 1e-5, and 5, respectively, and remain LoRA configuration at its default settings (code available).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Datasets And Models",
      "text": "For the datasets, we use IEMOCAP  [19]  and the Test1 set of MSP-Podcast  [20] . We combine excited with happy and use the Big Four classes (angry, happy, neutral, sad) for IEMOCAP. For MSP-Podcast, we remove other and use the eight classes (angry, happy, neutral, sad, disgusted, surprised, fearful, contemptuous). For the ASR models, we adopt ten popular ones from  [10]  to generate diverse transcripts to form 10-best ASR hypotheses. For the LLMs, we utilize Llama-2 (7b-chat-hf & 13b-chat-hf) and Falcon (7b-instruct). Temperature and max token are set as 1e-4 and 100. Due to limited space, we mainly present the results on IEMOCAP using Llama-2 and omit the full prompting message, presenting only the core text following the literature in Sec. II. Results of using the other model and dataset are presented in necessary experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Results And Discussions",
      "text": "We present the results and discussions based on the following exploration tasks. We replace responses that fall outside the emotion classes with neutral. Unweighted Accuracy (UA) is used to measure the results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Do Wers Have An Impact On Llm Prompting?",
      "text": "In this task, we use the baseline no reasoning prompt: Predict the emotion from {the emotion classes}. Do not show your explanation. From Table  I , we see that WERs do impact LLM prompting. Even the best-performing ASR transcription (i.e., from Whisper large) shows more than a 4% loss compared to ground-truth text. This finding contradicts a previous claim that LLM-based emotion recognition is robust to ASR errors  [3] . We believe this discrepancy arises from their (i) use of Whisper large, which provides relatively accurate transcriptions, and (ii) introduction of a fifth emotion class, 'other', to filter out unconfident labels. However, our setup is more inline with real-world scenarios where emotion recognition is more challenging due to various speaking styles and unconfident labels cannot be filtered. Furthermore, LLM-based performance remains relatively stable within certain WER ranges. The accuracy decrease does not linearly correlate with the WER increase, as seen in traditional deep learning modelbased emotion recognition  [12] . Finally, LLMs benefit from more parameters as the 13b model consistently outperforms the 7b model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Does Emotion-Specific Knowledge Help?",
      "text": "In this task, we use each of the emotion-specific prompts and their combinations for emotion recognition and compare their effectiveness on both ground-truth and ASR transcriptions. For brevity, we use one ASR transcription, whose WER ranked in the middle, as the representative (i.e., HuBERT large). Results are presented in Table. II.\n\nWe can see that: 1) All emotion-specific prompts improve the performance, demonstrating the efficacy of our proposed  approach by incorporating emotion-specific knowledge. However, the improvement is less pronounced on ASR transcription, highlighting the necessity for AEC. 2) Our proposed paralinguistic information improves on  [6] , verifying our hypothesis that additional paralinguistic features are beneficial. Furthermore, the improvement in 8-class is more significant, confirming that these additional features help in distinguishing finer-grained emotions (see Table  III ). 3) Linguistic knowledge generally contributes the most, even on ASR transcription. This means that LLMs benefit from identifying emotional trigger words and understanding the ASR-emotion relationship. This ASR-emotion does apply to ground-truth text, as it is specifically developed for ASR transcription. 4) The steady improvement from psychological knowledge confirms our hypothesis that LLMs' emotion recognition ability can be affected by psychological setting. Interestingly, among the psychological prompts, stimuli with negative affect perform the best. 5) Surprisingly, the baseline reasoning prompt does not improve performance. By investigating the responses, however, we found this is likely due to the LLM hallucinations, where they often described the (acoustic) tone despite having only text input. 6) Majority voting underperforms most single prompts, aligning with the finding of  [6] . Finally, identifying  the best prompt combination for both ground-truth and ASR transcriptions, we see that linguistics contributes the most to the latter by having both trigger words and ASR relationships.\n\n3. Does the proposed R3 prompt work?\n\nIn this task, we use the R3 prompt: You are an ASR error corrector and emotion recognizer. Generate the most likely transcript from {the 10-best ASR hypotheses} and predict the emotion from {the emotion classes} with reasoning based on the provided knowledge. For comparison, we conduct an ablation study, removing AEC or reasoning. We use 4+5+6+8 as the emotion knowledge since it has proven the best. As shown in Table  IV , both AEC and reasoning contribute to the effectiveness of our R3 prompt. Moreover, when incorporating our proposed emotion-specific knowledge, reasoning improves the performance, in contrast to the decrease observed when emotion-specific knowledge was not provided (see Table  II ). This suggests that emotion recognition is particularly challenging for LLMs to reason without relevant information. The examples in Fig.  2  illustrate how the R3 prompt helps LLMs in reasoning with emotion-specific knowledge, regardless of whether the recognition is correct.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Do Llm Training Schemes Help?",
      "text": "In this task, we apply the R3 prompt with context-aware learning (windows of 5 and 25), in-context learning (5-and 10-shot), and instruction tuning. For instruction tuning, we perform cross-validation by applying PEFT on every four sessions, testing on the remaining session, and then averaging. We do not compare performance across these three approaches due to their different settings. From Table  V , it is evident that each LLM training scheme improves the baseline performance of using R3 on 10-best ASR hypotheses (49.72 in Table  IV ). For context-aware learning and in-context few-shot learning, longer context windows and more samples yield higher accuracy. Instruction tuning leads to the highest performance. Notably, a long context window also results in UA greater than 60%, indicating the potential to utilize conversational knowledge in real-world LLM-based emotion recognition without tuning the models.\n\n5. Are LLMs sensitive to minor prompt variations?\n\nIn this task, we investigate whether LLM-based emotion recognition is sensitive to minor prompt variations. During our experiments, we observed that prompts with slight differences but the same meaning, such as variations in word choice or the order of provided emotion classes, can largely impact task performance. In Table  VI , we modify the baseline no reasoning prompt by changing either the word Predict to Select or the order of the emotion classes. This aligns with recent findings that LLMs can behave differently due to subtle changes in prompt formatting, such as separators and case, regardless of model size, number of fewshot examples, or instruction tuning  [21] . We believe this issue is a major factor hindering the widespread use of LLMs for emotion recognition and similar tasks, thus suggest that future studies evaluating LLMs with prompts would better report the performance across plausible prompt variations.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this work, we propose emotion-specific prompts by incorporating relevant knowledge from acoustics, linguistics, and psychology. We also compare LLM-based emotion recognition on both ground-truth and ASR transcriptions, confirming the necessity of AEC. Consequently, we develop the REVISE-REASON-RECOGNIZE prompting pipeline that integrates AEC, reasoning, and emotion recognition, which proves effective. Additionally, by investigating several LLM training schemes, we confirm the value of longer context windows, more few-shot samples, and instruction tuning. Finally, we uncover the sensitivity of LLMs to minor prompt variations. This research is expected to bridge the gap between existing studies on LLMs and emotion recognition.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Acoustic information plays a crucial role in distinguishing",
      "page": 2
    },
    {
      "caption": "Figure 1: Emotion-specific prompts used in this work.",
      "page": 2
    },
    {
      "caption": "Figure 2: Examples of LLM reasoning with emotion-specific knowledge.",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrate how the R3 prompt helps",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "yuanchao.li@ed.ac.uk"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "Abstract—Annotating\nand recognizing\nspeech emotion using"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "prompt\nengineering\nhas\nrecently\nemerged with\nthe\nadvance-"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "ment of Large Language Models\n(LLMs), yet\nits\nefficacy and"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "reliability\nremain\nquestionable.\nIn\nthis\npaper, we\nconduct\na"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "systematic\nstudy on this\ntopic, beginning with the proposal of"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "novel prompts that incorporate emotion-specific knowledge from"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "acoustics,\nlinguistics, and psychology. Subsequently, we examine"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "the effectiveness of LLM-based prompting on Automatic Speech"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "Recognition (ASR) transcription, contrasting it with ground-truth"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "transcription.\nFurthermore, we\npropose\na REVISE-REASON-"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "RECOGNIZE prompting pipeline for robust LLM-based emotion"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "recognition from spoken language with ASR errors. Additionally,"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "experiments on context-aware learning,\nin-context\nlearning, and"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "instruction tuning are performed to examine\nthe usefulness of"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "LLM training schemes\nin this direction. Finally, we investigate"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "the sensitivity of LLMs to minor prompt variations. Experimental"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "results demonstrate the efficacy of\nthe emotion-specific prompts,"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "ASR error correction, and LLM training schemes for LLM-based"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "emotion recognition. Our study aims\nto refine the use of LLMs"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "in emotion recognition and related domains."
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "Index Terms—Emotion Recognition, LLM, ASR, Acoustics,"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "Linguistics, Psychology"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "I.\nINTRODUCTION"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "Recent studies have suggested that Large Language Models"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "(LLMs) have the ability to reason about emotional content [1]."
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "This finding has encouraged researchers to further explore the"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "“emotional intelligence” (e.g., emotion recognition, interpreta-"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "tion, and understanding) of LLMs. For example, [2] developed"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "a psychometric assessment focusing on emotion understanding"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "to compare the emotional\nintelligence of LLMs and humans."
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "They found that most LLMs\nachieved above-average Emo-"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "tional Quotient\n(EQ)\nscores, with GPT-4 surpassing 89% of"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "human participants with an EQ of 117."
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "Therefore,\nthe use of LLMs\nin text-based emotion recog-"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "nition has\nemerged as\na\nresource-efficient\nand effort-saving"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "alternative to human annotators and traditional emotion clas-"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "sifiers for\ntwo main reasons: 1) Emotion recognition requires"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "substantial\nhuman\neffort. Typically, multiple\nannotators\nare"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "needed for\neach sample\nto reach a majority vote,\nensuring"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "accurate\nassessment. Although\nplatforms\nlike Amazon Me-"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "chanical Turk provide a relatively efficient solution, concerns"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "persist\nregarding privacy leaks,\nsubjective bias,\nand the\nre-"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "liability of\nannotations\n[3]. 2) Despite\nthe\nadvancements of"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "state-of-the-art deep learning technologies, training an emotion"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "classifier\ninvolves multiple steps,\nincluding feature extraction"
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": ""
        },
        {
          "1University of Edinburgh, 2MIT CSAIL, 3NVIDIA Research": "∗Work done at MIT, now with xAI Corp."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "examined various prompting techniques,\nincluding chain-of-": "thought,\nrole-play, and their variations."
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "Despite these advancements, we argue that each approach"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "has its limitations,\nleaving several concerns unaddressed. For"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "example,\n[9] did not\nstudy prompting,\n[4] only proposed a"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "multi-step prompting without further exploration. The prompt-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "ing techniques tested by [5] were not specifically designed for"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "emotion.\n[6] and [8] only considered basic acoustic features,"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "without\nincorporating more\nemotion-related properties. Fur-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "thermore, both [3] and [7] used ASR transcriptions generated"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "by Whisper, whose output has been shown to be\nrobust\nin"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "emotion\nrecognition\neven with ASR errors\n[10]. However,"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "this setting is not\nideal\nfor LLMs handling challenging ASR"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "transcriptions in real-world emotion applications."
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "III. METHODOLOGY"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "To address\nthe issues above, we 1) Develop prompts\nthat"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "incorporate emotion-specific knowledge; 2)\nIncorporate ASR"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "Error Correction\n(AEC)\nto\nrefine\ntranscriptions\nfor\nrobust"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "emotion recognition; and 3) Explore LLM training schemes"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "to further\nimprove the performance."
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "A. Prompting with Emotion-Specific Knowledge"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "In light of\nthe relationship between emotion and relevant"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "disciplines, we extract useful knowledge from acoustics,\nlin-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "guistics, and psychology,\nto develop emotion-specific prompts"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "for emotion recognition. The prompts are presented in Fig. 1."
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "Acoustic information plays a crucial role in distinguishing"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "speech emotions. Features like energy, pitch, and speaking rate"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "have proven useful when been incorporated into prompts as"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "textual descriptors [6]. Similarly, gender information, which is"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "highly correlated with pitch, has also been shown to be useful"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "[8],\n[11]. However,\nthese\nfeatures\nare\ninsufficient\nto\nfully"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "describe fine-grained differences in emotions beyond the Big"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "Four. Hence, we hypothesize that additional acoustic features"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "can enhance LLMs’ emotion recognition ability. We propose"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "including pitch range,\njitter, and shimmer to incorporate mid-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "level prosody\n(between frame-level and utterance-level) and"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "voice quality."
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "Linguistic structure is essential for understanding emotion"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "in\ntext. For\nexample,\n[12]\ninvestigated\nthe\nimpact\nof\npart-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "of-speech,\naffective\nscore,\nand utterance\nlength on emotion."
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "However,\nto\nour\nknowledge,\nonly\none work\n[13]\nhas\nuti-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "lized linguistics via identifying emotion triggers\n(i.e., words"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": ""
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "that\nelicit\nthe\nemotion) when prompting LLMs\nfor\nemotion"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "hypothesize\nthat\nLLM-based\nemotion\nprediction. Thus, we"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "recognition can benefit from more linguistic knowledge as LLM"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "processing is inherently text-based. We propose including ASR-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "emotion relationships among emotion category, Word Error"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "Rate (WER), and utterance length as outlined in [12]."
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "Psychological theories, such as self-monitoring, social cog-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "nitive\ntheory,\nand cognitive\nemotion regulation have proven"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "effective in improved LLMs’ performance across various tasks"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "hypothesize\nthat\nLLMs’\nemotion\n[14],\n[15]. Therefore, we"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "recognition\nability\ncan\nresonate with\ntheir\nemotional\nintel-"
        },
        {
          "examined various prompting techniques,\nincluding chain-of-": "ligence\nand\nthus\nbe\nenhanced. We\npropose\nincorporating"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "EMOTION RECOGNITION ACCURACY BY USING EMOTION-SPECIFIC"
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "PROMPTS. ↑: HIGHER THE BETTER."
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "Ground-truth"
        },
        {
          "using the other model and dataset are presented in necessary": "the results and discussions based on the follow-",
          "TABLE II": "44.50"
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "43.83 (−0.70)"
        },
        {
          "using the other model and dataset are presented in necessary": "that",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "45.59 (+1.09)"
        },
        {
          "using the other model and dataset are presented in necessary": "the emotion classes with neutral. Unweighted Accuracy (UA)",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "46.25 (+1.75)"
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "46.80 (+2.30)"
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "/"
        },
        {
          "using the other model and dataset are presented in necessary": "1. Do WERs have an impact on LLM prompting?",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "45.90 (+1.40)"
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "47.43 (+2.93)"
        },
        {
          "using the other model and dataset are presented in necessary": "reasoning prompt:",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "45.81 (+1.31)"
        },
        {
          "using the other model and dataset are presented in necessary": "classes}. Do",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "45.72 (+1.22)"
        },
        {
          "using the other model and dataset are presented in necessary": "see",
          "TABLE II": "48.96 (+4.46)"
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "/"
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": ""
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": "IEMOCAP. LLM: Llama-2-7b-chat-hf."
        },
        {
          "using the other model and dataset are presented in necessary": "",
          "TABLE II": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4 + 5 + 8": "4 + 5 + 6 + 8\n/\n44.47 (+3.11)"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "Data:\nIEMOCAP. LLM: Llama-2-7b-chat-hf."
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "approach by incorporating emotion-specific knowledge. How-"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "ever,\nthe improvement\nis\nless pronounced on ASR transcrip-"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "tion,\nhighlighting\nthe\nnecessity\nfor AEC.\n2) Our\nproposed"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "paralinguistic information improves on [6], verifying our hy-"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "pothesis\nthat additional paralinguistic features are beneficial."
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "Furthermore,\nthe improvement\nin 8-class\nis more significant,"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "confirming that these additional features help in distinguishing"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "finer-grained emotions (see Table III). 3) Linguistic knowledge"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "generally\ncontributes\nthe most,\neven\non ASR transcription."
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "This means\nthat LLMs\nbenefit\nfrom identifying\nemotional"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "trigger words\nand understanding the ASR-emotion relation-"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "ship. This ASR-emotion does apply to ground-truth text, as"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "it\nis\nspecifically\ndeveloped\nfor ASR transcription.\n4) The"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "steady improvement\nfrom psychological knowledge confirms"
        },
        {
          "4 + 5 + 8": ""
        },
        {
          "4 + 5 + 8": "our hypothesis\nthat LLMs’\nemotion recognition ability can"
        },
        {
          "4 + 5 + 8": "be affected by psychological setting.\nInterestingly, among the"
        },
        {
          "4 + 5 + 8": "psychological prompts,\nstimuli with negative\naffect perform"
        },
        {
          "4 + 5 + 8": "the best. 5) Surprisingly,\nthe baseline reasoning prompt does"
        },
        {
          "4 + 5 + 8": "not\nimprove\nperformance. By\ninvestigating\nthe\nresponses,"
        },
        {
          "4 + 5 + 8": "however, we found this is likely due to the LLM hallucinations,"
        },
        {
          "4 + 5 + 8": "where they often described the (acoustic)\ntone despite having"
        },
        {
          "4 + 5 + 8": "only text\ninput. 6) Majority voting underperforms most single"
        },
        {
          "4 + 5 + 8": "prompts, aligning with the finding of\n[6]. Finally,\nidentifying"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "EMOTION RECOGNITION ACCURACY ON TRANSCRIPTIONS OF"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "INCREASING WER. ↑: HIGHER THE BETTER. ↓: LOWER THE BETTER."
        },
        {
          "IV. EXPERIMENTS": "A. Datasets and Models",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "For\nthe datasets, we use\nIEMOCAP [19]\nand the Test1",
          "TABLE I": "UA%↑"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "WER%↓ (Transcription source)"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "7b-chat-hf\n13b-chat-hf"
        },
        {
          "IV. EXPERIMENTS": "happy\nset\nof MSP-Podcast\n[20]. We\ncombine\nexcited with",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "0.00 (Ground-truth)\n44.50\n47.43"
        },
        {
          "IV. EXPERIMENTS": "and use the Big Four classes (angry, happy, neutral, sad)\nfor",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "12.3 (Whisper large)\n41.77\n44.27"
        },
        {
          "IV. EXPERIMENTS": "IEMOCAP. For MSP-Podcast, we remove other and use the",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "14.4 (Whisper small)\n41.47\n43.98"
        },
        {
          "IV. EXPERIMENTS": "eight classes (angry, happy, neutral, sad, disgusted, surprised,",
          "TABLE I": "20.2 (Whisper base)\n41.16\n43.70"
        },
        {
          "IV. EXPERIMENTS": "fearful,\ncontemptuous). For\nthe ASR models, we\nadopt\nten",
          "TABLE I": "21.9 (W2V960 large self )\n41.12\n43.59"
        },
        {
          "IV. EXPERIMENTS": "popular ones from [10] to generate diverse transcripts to form",
          "TABLE I": "23.8 (HuBERT large)\n41.36\n43.88"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "26.9 (Whisper tiny)\n40.80\n43.14"
        },
        {
          "IV. EXPERIMENTS": "10-best ASR hypotheses. For\nthe LLMs, we utilize Llama-2",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "27.9 (W2V960 large)\n40.49\n43.10"
        },
        {
          "IV. EXPERIMENTS": "(7b-chat-hf & 13b-chat-hf) and Falcon (7b-instruct). Temperature",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "32.3 (W2V960)\n40.00\n43.01"
        },
        {
          "IV. EXPERIMENTS": "and max\ntoken\nare\nset\nas\n1e-4\nand\n100. Due\nto\nlimited",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "39.1 (Wavlm plus)\n38.01\n40.12"
        },
        {
          "IV. EXPERIMENTS": "space, we mainly\npresent\nthe\nresults\non\nIEMOCAP using",
          "TABLE I": "40.3 (W2V100)\n38.09\n40.19"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "Data:\nIEMOCAP. LLM: Llama-2."
        },
        {
          "IV. EXPERIMENTS": "Llama-2 and omit the full prompting message, presenting only",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "the\ncore\ntext\nfollowing the\nliterature\nin Sec.\nII. Results of",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "using the other model and dataset are presented in necessary",
          "TABLE I": "TABLE II"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "EMOTION RECOGNITION ACCURACY BY USING EMOTION-SPECIFIC"
        },
        {
          "IV. EXPERIMENTS": "experiments.",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "PROMPTS. ↑: HIGHER THE BETTER."
        },
        {
          "IV. EXPERIMENTS": "B. Results and Discussions",
          "TABLE I": "UA%↑"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "Prompt"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "Ground-truth\nHuBERT large"
        },
        {
          "IV. EXPERIMENTS": "We present\nthe results and discussions based on the follow-",
          "TABLE I": "Baseline\n1) No reasoning\n44.50\n41.36"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "2) Reasoning\n43.83 (−0.70)\n40.07 (−1.29)"
        },
        {
          "IV. EXPERIMENTS": "ing exploration tasks. We replace responses\nthat\nfall outside",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "Acoustics\n3) Gender\n45.59 (+1.09)\n42.22 (+0.86)"
        },
        {
          "IV. EXPERIMENTS": "the emotion classes with neutral. Unweighted Accuracy (UA)",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "4) Paraling\n46.25 (+1.75)\n43.02 (+1.66)"
        },
        {
          "IV. EXPERIMENTS": "is used to measure the results.",
          "TABLE I": "Linguistics\n5) Trigger\n46.80 (+2.30)\n43.45 (+2.09)"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "6) ASR relation\n/\n44.10 (+2.74)"
        },
        {
          "IV. EXPERIMENTS": "1. Do WERs have an impact on LLM prompting?",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "Psychology\n7) Pos stimuli\n45.90 (+1.40)\n42.35 (+0.99)"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "8) Neg stimuli\n47.43 (+2.93)\n42.76 (+1.40)"
        },
        {
          "IV. EXPERIMENTS": "In this\ntask, we use\nthe baseline no\nreasoning prompt:",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "9) Cpt stimuli\n45.81 (+1.31)\n41.98 (+0.65)"
        },
        {
          "IV. EXPERIMENTS": "Predict\nthe\nemotion\nemotion\nnot\nfrom {the\nclasses}. Do",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "Majority voting\n45.72 (+1.22)\n42.94 (+1.58)"
        },
        {
          "IV. EXPERIMENTS": "show your\nexplanation. From Table\nI, we\nsee\nthat WERs",
          "TABLE I": "4 + 5 + 8\n48.96 (+4.46)\n44.30 (+2.94)"
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "4 + 5 + 6 + 8\n/\n44.47 (+3.11)"
        },
        {
          "IV. EXPERIMENTS": "do impact LLM prompting. Even the best-performing ASR",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "Data:\nIEMOCAP. LLM: Llama-2-7b-chat-hf."
        },
        {
          "IV. EXPERIMENTS": "transcription (i.e., from Whisper large) shows more than a 4%",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "loss compared to ground-truth text. This finding contradicts a",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "previous claim that LLM-based emotion recognition is robust",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "approach by incorporating emotion-specific knowledge. How-"
        },
        {
          "IV. EXPERIMENTS": "to ASR errors [3]. We believe this discrepancy arises from their",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "ever,\nthe improvement\nis\nless pronounced on ASR transcrip-"
        },
        {
          "IV. EXPERIMENTS": "(i) use of Whisper\nlarge, which provides\nrelatively accurate",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "tion,\nhighlighting\nthe\nnecessity\nfor AEC.\n2) Our\nproposed"
        },
        {
          "IV. EXPERIMENTS": "transcriptions, and (ii)\nintroduction of a fifth emotion class,",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "paralinguistic information improves on [6], verifying our hy-"
        },
        {
          "IV. EXPERIMENTS": "‘other’,\nto filter out unconfident\nlabels. However, our setup is",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "pothesis\nthat additional paralinguistic features are beneficial."
        },
        {
          "IV. EXPERIMENTS": "more inline with real-world scenarios where emotion recog-",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "Furthermore,\nthe improvement\nin 8-class\nis more significant,"
        },
        {
          "IV. EXPERIMENTS": "nition is more challenging due to various speaking styles and",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "confirming that these additional features help in distinguishing"
        },
        {
          "IV. EXPERIMENTS": "unconfident labels cannot be filtered. Furthermore, LLM-based",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "finer-grained emotions (see Table III). 3) Linguistic knowledge"
        },
        {
          "IV. EXPERIMENTS": "performance\nremains\nrelatively\nstable within\ncertain WER",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "generally\ncontributes\nthe most,\neven\non ASR transcription."
        },
        {
          "IV. EXPERIMENTS": "ranges. The accuracy decrease does not\nlinearly correlate with",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "This means\nthat LLMs\nbenefit\nfrom identifying\nemotional"
        },
        {
          "IV. EXPERIMENTS": "the WER increase, as seen in traditional deep learning model-",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "trigger words\nand understanding the ASR-emotion relation-"
        },
        {
          "IV. EXPERIMENTS": "based emotion recognition [12]. Finally, LLMs benefit\nfrom",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "ship. This ASR-emotion does apply to ground-truth text, as"
        },
        {
          "IV. EXPERIMENTS": "more parameters as\nthe 13b model consistently outperforms",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "it\nis\nspecifically\ndeveloped\nfor ASR transcription.\n4) The"
        },
        {
          "IV. EXPERIMENTS": "the 7b model.",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "steady improvement\nfrom psychological knowledge confirms"
        },
        {
          "IV. EXPERIMENTS": "2. Does emotion-specific knowledge help?",
          "TABLE I": ""
        },
        {
          "IV. EXPERIMENTS": "",
          "TABLE I": "our hypothesis\nthat LLMs’\nemotion recognition ability can"
        },
        {
          "IV. EXPERIMENTS": "In this\ntask, we use each of\nthe emotion-specific prompts",
          "TABLE I": "be affected by psychological setting.\nInterestingly, among the"
        },
        {
          "IV. EXPERIMENTS": "and their combinations for emotion recognition and compare",
          "TABLE I": "psychological prompts,\nstimuli with negative\naffect perform"
        },
        {
          "IV. EXPERIMENTS": "their\neffectiveness on both ground-truth and ASR transcrip-",
          "TABLE I": "the best. 5) Surprisingly,\nthe baseline reasoning prompt does"
        },
        {
          "IV. EXPERIMENTS": "tions. For brevity, we use one ASR transcription, whose WER",
          "TABLE I": "not\nimprove\nperformance. By\ninvestigating\nthe\nresponses,"
        },
        {
          "IV. EXPERIMENTS": "ranked\nin\nthe middle,\nas\nthe\nrepresentative\n(i.e., HuBERT",
          "TABLE I": "however, we found this is likely due to the LLM hallucinations,"
        },
        {
          "IV. EXPERIMENTS": "large). Results are presented in Table.\nII.",
          "TABLE I": "where they often described the (acoustic)\ntone despite having"
        },
        {
          "IV. EXPERIMENTS": "We can see that: 1) All emotion-specific prompts improve",
          "TABLE I": "only text\ninput. 6) Majority voting underperforms most single"
        },
        {
          "IV. EXPERIMENTS": "the performance, demonstrating the efficacy of our proposed",
          "TABLE I": "prompts, aligning with the finding of\n[6]. Finally,\nidentifying"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "UA%↑\n54.35\n62.46\n50.74\n54.36\n64.67"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "Data: 10-best of\nIEMOCAP. Model: Llama-2-7b-chat-hf."
        },
        {
          "Prompt": "Para info",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "From Table V,\nit\nis evident\nthat each LLM training scheme"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "improves\nthe baseline performance of using R3 on 10-best"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "ASR hypotheses (49.72 in Table IV). For context-aware learn-"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "ing and in-context\nfew-shot\nlearning,\nlonger context windows"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "and more\nsamples yield higher\naccuracy.\nInstruction tuning"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "leads\nto\nthe\nhighest\nperformance. Notably,\na\nlong\ncontext"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "window also results\nin UA greater\nthan 60%,\nindicating the"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "potential\nto\nutilize\nconversational\nknowledge\nin\nreal-world"
        },
        {
          "Prompt": "Fig. 2.",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "LLM-based emotion recognition without\ntuning the models."
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "5. Are LLMs sensitive to minor prompt variations?"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "In this\ntask, we\ninvestigate whether LLM-based emotion"
        },
        {
          "Prompt": "transcriptions, we see that",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "recognition is sensitive to minor prompt variations. During our"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "experiments, we observed that prompts with slight differences"
        },
        {
          "Prompt": "3. Does the proposed R3 prompt work?",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "but\nthe same meaning,\nsuch as variations\nin word choice or"
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "the\norder\nof\nprovided\nemotion\nclasses,\ncan\nlargely\nimpact"
        },
        {
          "Prompt": "corrector and emotion recognizer. Generate",
          "5\n25\n5\n10\nPEFT": "task performance.\nIn Table VI, we modify the baseline no"
        },
        {
          "Prompt": "transcript",
          "5\n25\n5\n10\nPEFT": "reasoning prompt by changing either\nthe word Predict\nto"
        },
        {
          "Prompt": "emotion",
          "5\n25\n5\n10\nPEFT": "Select or\nthe order of\nthe emotion classes."
        },
        {
          "Prompt": "on the provided knowledge. For comparison, we conduct an",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "TABLE VI"
        },
        {
          "Prompt": "ablation study, removing AEC or reasoning. We use 4+5+6+8",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "PERFORMANCE COMPARISON OF PROMPT VARIATIONS. (A: ANGRY,"
        },
        {
          "Prompt": "as the emotion knowledge since it has proven the best.",
          "5\n25\n5\n10\nPEFT": ""
        },
        {
          "Prompt": "",
          "5\n25\n5\n10\nPEFT": "H:HAPPY, N: NEUTRAL, S: SAD). ↑: HIGHER THE BETTER."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACKNOWLEDGMENT": "",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "to-end speech emotion recognition using self attention mechanism and"
        },
        {
          "ACKNOWLEDGMENT": "We\nthank Pinzhen Chen\n(UoE)\nfor\nhis\nhelp with LLM",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": ""
        },
        {
          "ACKNOWLEDGMENT": "",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "multitask learning.,”\nin Interspeech, 2019, pp. 2803–2807."
        },
        {
          "ACKNOWLEDGMENT": "training and Tiantian Feng (USC)\nfor his\nfeedback on LLM",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[12] Yuanchao Li, Zeyu Zhao, Ondrej Klejch, Peter Bell, and Catherine Lai,"
        },
        {
          "ACKNOWLEDGMENT": "",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "“ASR and emotional speech: A word-level\ninvestigation of\nthe mutual"
        },
        {
          "ACKNOWLEDGMENT": "selection.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": ""
        },
        {
          "ACKNOWLEDGMENT": "",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "impact of speech and emotion recognition,” in Interspeech 2023, 2023."
        },
        {
          "ACKNOWLEDGMENT": "",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[13]\nSmriti Singh, Cornelia Caragea, and Junyi Jessy Li, “Language models"
        },
        {
          "ACKNOWLEDGMENT": "REFERENCES",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": ""
        },
        {
          "ACKNOWLEDGMENT": "",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "(mostly) do not\nconsider\nemotion triggers when predicting emotion,”"
        },
        {
          "ACKNOWLEDGMENT": "[1] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "NAACL 2024, 2024."
        },
        {
          "ACKNOWLEDGMENT": "Choi,\n“Social\nIQa: Commonsense reasoning about social\ninteractions,”",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[14] Cheng Li,\nJindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou,"
        },
        {
          "ACKNOWLEDGMENT": "in EMNLP-IJCNLP, 2019, pp. 4463–4473.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie,\n“Large language"
        },
        {
          "ACKNOWLEDGMENT": "[2] Xuena Wang, Xueting Li, Zi Yin, Yue Wu, and Jia Liu, “Emotional\nin-",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "arXiv\nmodels understand and can be enhanced by emotional\nstimuli,”"
        },
        {
          "ACKNOWLEDGMENT": "telligence of large language models,” Journal of Pacific Rim Psychology,",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "preprint arXiv:2307.11760, 2023."
        },
        {
          "ACKNOWLEDGMENT": "2023.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[15] Xu Wang, Cheng Li, Yi Chang, Jindong Wang, and Yuan Wu, “Negative-"
        },
        {
          "ACKNOWLEDGMENT": "[3]\nTiantian Feng and Shrikanth Narayanan,\n“Foundation model assisted",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "prompt: Leveraging psychology for large language models enhancement"
        },
        {
          "ACKNOWLEDGMENT": "automatic\nspeech\nemotion\nrecognition: Transcribing,\nannotating,\nand",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "via negative emotional stimuli,”\nIJCAI 2024, 2024."
        },
        {
          "ACKNOWLEDGMENT": "augmenting,”\nin ICASSP 2024.\nIEEE, 2024, pp. 12116–12120.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[16] Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh,\nIvan"
        },
        {
          "ACKNOWLEDGMENT": "[4] Kenta Hama, Atsushi Otsuka, and Ryo Ishii,\n“Emotion recognition in",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "Bulyko,\nand Andreas Stolcke,\n“Generative\nspeech\nrecognition\nerror"
        },
        {
          "ACKNOWLEDGMENT": "conversation with multi-step prompting using large\nlanguage model,”",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "correction with large language models and task-activating prompting,”"
        },
        {
          "ACKNOWLEDGMENT": "in International Conference on Human-Computer Interaction. Springer,",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "in IEEE ASRU 2023.\nIEEE, 2023, pp. 1–8."
        },
        {
          "ACKNOWLEDGMENT": "2024, pp. 338–346.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[17] Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai"
        },
        {
          "ACKNOWLEDGMENT": "[5] Mostafa M Amin and Bj¨orn W Schuller,\n“On prompt\nsensitivity of",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr"
        },
        {
          "ACKNOWLEDGMENT": "chatgpt in affective computing,” arXiv preprint arXiv:2403.14006, 2024.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "˙\nZelasko, et al., “Large language model based generative error correction:"
        },
        {
          "ACKNOWLEDGMENT": "[6]\nJennifer Santoso, Kenkichi\nIshizuka,\nand Taiichi Hashimoto,\n“Large",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "A challenge and baselines for speech recognition, speaker\ntagging, and"
        },
        {
          "ACKNOWLEDGMENT": "language model-based emotional\nspeech annotation using context and",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "emotion recognition,” arXiv preprint arXiv:2409.09785, 2024."
        },
        {
          "ACKNOWLEDGMENT": "acoustic feature for speech emotion recognition,” in ICASSP 2024. IEEE,",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[18]\nEdward\nJ Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean"
        },
        {
          "ACKNOWLEDGMENT": "2024, pp. 11026–11030.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "Wang, Lu Wang, Weizhu Chen,\net\nal.,\n“LoRA: Low-rank adaptation"
        },
        {
          "ACKNOWLEDGMENT": "[7]\nTaesik Gong,\nJosh Belanich, Krishna Somandepalli, Arsha Nagrani,",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "of\nlarge\nlanguage models,”\nin International Conference on Learning"
        },
        {
          "ACKNOWLEDGMENT": "Brian Eoff,\nand Brendan Jou,\n“LanSER: Language-model\nsupported",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "Representations (ICLR), 2022."
        },
        {
          "ACKNOWLEDGMENT": "speech emotion recognition,”\nInterspeech 2023, 2023.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[19] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily"
        },
        {
          "ACKNOWLEDGMENT": "[8]\nSiddique Latif, Muhammad Usama, Mohammad\nIbrahim Malik,\nand",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S"
        },
        {
          "ACKNOWLEDGMENT": "Bj¨orn W Schuller,\n“Can\nlarge\nlanguage models\naid\nin\nannotating",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "Narayanan,\n“IEMOCAP:\nInteractive emotional dyadic motion capture"
        },
        {
          "ACKNOWLEDGMENT": "arXiv\npreprint\nspeech\nemotional\ndata?\nuncovering\nnew frontiers,”",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "database,”\nLanguage resources and evaluation, vol. 42, pp. 335–359,"
        },
        {
          "ACKNOWLEDGMENT": "arXiv:2307.06090, 2023.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "2008."
        },
        {
          "ACKNOWLEDGMENT": "[9]\nZixing Zhang, Liyizhe Peng, Tao Pang,\nJing Han, Huan Zhao,\nand",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[20] Reza Lotfian\nand Carlos Busso,\n“Building\nnaturalistic\nemotionally"
        },
        {
          "ACKNOWLEDGMENT": "Bj¨orn W Schuller,\n“Refashioning emotion recognition modelling: The",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "balanced speech corpus by retrieving emotional\nspeech from existing"
        },
        {
          "ACKNOWLEDGMENT": "IEEE Transactions on Computa-\nadvent of generalised large models,”",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "podcast\nrecordings,”\nIEEE Transactions on Affective Computing, vol."
        },
        {
          "ACKNOWLEDGMENT": "tional Social Systems, 2024.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "10, no. 4, pp. 471–483, 2017."
        },
        {
          "ACKNOWLEDGMENT": "[10] Yuanchao Li, Peter Bell, and Catherine Lai,\n“Speech emotion recogni-",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "[21] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr, “Quantifying"
        },
        {
          "ACKNOWLEDGMENT": "tion with ASR transcripts: A comprehensive study on word error rate and",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "language models’\nsensitivity\nto\nspurious\nfeatures\nin\nprompt\ndesign"
        },
        {
          "ACKNOWLEDGMENT": "fusion techniques,” 2024 IEEE Spoken Language Technology Workshop",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "or: How I\nlearned\nto\nstart worrying\nabout\nprompt\nformatting,”\nin"
        },
        {
          "ACKNOWLEDGMENT": "(SLT).\nIEEE, 2024.",
          "[11] Yuanchao Li, Tianyu Zhao,\nand Tatsuya Kawahara,\n“Improved end-": "International Conference on Learning Representations (ICLR), 2024."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Social IQa: Commonsense reasoning about social interactions",
      "authors": [
        "Maarten Sap",
        "Hannah Rashkin",
        "Derek Chen",
        "Ronan Le Bras",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "2",
      "title": "Emotional intelligence of large language models",
      "authors": [
        "Xuena Wang",
        "Xueting Li",
        "Zi Yin",
        "Yue Wu",
        "Jia Liu"
      ],
      "year": "2023",
      "venue": "Journal of Pacific Rim Psychology"
    },
    {
      "citation_id": "3",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "venue": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in conversation with multi-step prompting using large language model",
      "authors": [
        "Kenta Hama",
        "Atsushi Otsuka",
        "Ryo Ishii"
      ],
      "year": "2024",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "5",
      "title": "On prompt sensitivity of chatgpt in affective computing",
      "authors": [
        "M Mostafa",
        "Björn Amin",
        "Schuller"
      ],
      "year": "2024",
      "venue": "On prompt sensitivity of chatgpt in affective computing",
      "arxiv": "arXiv:2403.14006"
    },
    {
      "citation_id": "6",
      "title": "Large language model-based emotional speech annotation using context and acoustic feature for speech emotion recognition",
      "authors": [
        "Jennifer Santoso",
        "Kenkichi Ishizuka",
        "Taiichi Hashimoto"
      ],
      "venue": "Large language model-based emotional speech annotation using context and acoustic feature for speech emotion recognition"
    },
    {
      "citation_id": "7",
      "title": "LanSER: Language-model supported speech emotion recognition",
      "authors": [
        "Taesik Gong",
        "Josh Belanich",
        "Krishna Somandepalli",
        "Arsha Nagrani",
        "Brian Eoff",
        "Brendan Jou"
      ],
      "year": "2023",
      "venue": "LanSER: Language-model supported speech emotion recognition"
    },
    {
      "citation_id": "8",
      "title": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "authors": [
        "Siddique Latif",
        "Muhammad Usama",
        "Ibrahim Malik",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "arxiv": "arXiv:2307.06090"
    },
    {
      "citation_id": "9",
      "title": "Refashioning emotion recognition modelling: The advent of generalised large models",
      "authors": [
        "Zixing Zhang",
        "Liyizhe Peng",
        "Tao Pang",
        "Jing Han",
        "Huan Zhao",
        "Björn Schuller"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with ASR transcripts: A comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "Improved endto-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Yuanchao Li",
        "Tianyu Zhao",
        "Tatsuya Kawahara"
      ],
      "year": "2019",
      "venue": "Improved endto-end speech emotion recognition using self attention mechanism and multitask learning"
    },
    {
      "citation_id": "12",
      "title": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Zeyu Zhao",
        "Ondrej Klejch",
        "Peter Bell",
        "Catherine Lai"
      ],
      "venue": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition"
    },
    {
      "citation_id": "13",
      "title": "Language models (mostly) do not consider emotion triggers when predicting emotion",
      "authors": [
        "Smriti Singh",
        "Cornelia Caragea",
        "Junyi Jessy Li"
      ],
      "year": "2024",
      "venue": "NAACL 2024"
    },
    {
      "citation_id": "14",
      "title": "Large language models understand and can be enhanced by emotional stimuli",
      "authors": [
        "Cheng Li",
        "Jindong Wang",
        "Yixuan Zhang",
        "Kaijie Zhu",
        "Wenxin Hou",
        "Jianxun Lian",
        "Fang Luo",
        "Qiang Yang",
        "Xing Xie"
      ],
      "year": "2023",
      "venue": "Large language models understand and can be enhanced by emotional stimuli",
      "arxiv": "arXiv:2307.11760"
    },
    {
      "citation_id": "15",
      "title": "Negativeprompt: Leveraging psychology for large language models enhancement via negative emotional stimuli",
      "authors": [
        "Xu Wang",
        "Cheng Li",
        "Yi Chang",
        "Jindong Wang",
        "Yuan Wu"
      ],
      "year": "2024",
      "venue": "IJCAI"
    },
    {
      "citation_id": "16",
      "title": "Generative speech recognition error correction with large language models and task-activating prompting",
      "authors": [
        "Chao-Han Huck",
        "Yile Yang",
        "Yi-Chieh Gu",
        "Shalini Liu",
        "Ivan Ghosh",
        "Andreas Bulyko",
        "Stolcke"
      ],
      "year": "2023",
      "venue": "IEEE ASRU 2023"
    },
    {
      "citation_id": "17",
      "title": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition",
      "authors": [
        "Chao-Han Huck",
        "Taejin Yang",
        "Yuan Park",
        "Yuanchao Gong",
        "Zhehuai Li",
        "Yen-Ting Chen",
        "Chen Lin",
        "Yuchen Chen",
        "Kunal Hu",
        "Piotr Dhawan",
        "Żelasko"
      ],
      "year": "2024",
      "venue": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition",
      "arxiv": "arXiv:2409.09785"
    },
    {
      "citation_id": "18",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Phillip Hu",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "19",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Quantifying language models' sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting",
      "authors": [
        "Melanie Sclar",
        "Yejin Choi",
        "Yulia Tsvetkov",
        "Alane Suhr"
      ],
      "venue": "International Conference on Learning Representations (ICLR)"
    }
  ]
}