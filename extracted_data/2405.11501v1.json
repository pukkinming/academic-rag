{
  "paper_id": "2405.11501v1",
  "title": "Dogflw: Dog Facial Landmarks In The Wild Dataset",
  "published": "2024-05-19T09:59:36Z",
  "authors": [
    "George Martvel",
    "Greta Abele",
    "Annika Bremhorst",
    "Chiara Canori",
    "Nareed Farhat",
    "Giulia Pedretti",
    "Ilan Shimshoni",
    "Anna Zamansky"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing for animals is a rapidly expanding research area that is going deeper than automated movement tracking to address animal internal states, like pain and emotions. Facial expressions can serve to communicate information about these states in mammals. However, unlike human-related studies, there is a significant shortage of datasets that would enable the automated analysis of animal facial expressions. Inspired by the recently introduced Cat Facial Landmarks in the Wild dataset, presenting cat faces annotated with 48 facial anatomy-based landmarks, in this paper, we develop an analogous dataset containing 3,274 annotated images of dogs. Our dataset is based on a scheme of 46 facial anatomy-based landmarks. The DogFLW dataset is available from the corresponding author upon a reasonable request.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Due to the internal nature of affective states, including emotions and pain, their recognition in animals is very challenging in light of the lack of a verbal basis for communication. Nevertheless, observing subtle changes in their facial expressions and body language shows promise as a nonintrusive method for studying these states. Mammals produce facial expressions, which have been linked emotional states in a variety of species  [14] . Therefore, it is unsurprising that the number of works addressing automation of animal affect recognition tasks is rapidly growing; Broome et al.  [4]  provides a comprehensive review of state-of-theart works using computer vision to address such recognition in animals. Despite dogs being one of the most well-studied animals in behavior studies, only a few of these works address these species, with solely one work  [1]  focusing on facial expressions and using a dataset collected in a controlled experimental setting.\n\nThe interest in cognitive and behavioral aspects of dogs has been increasing dramatically  [35] . Dogs can serve as valuable clinical models for numerous human disorders, owing to their large size: many canine conditions mirror human diseases, including diabetes, cancers, epilepsy, eye diseases, autoimmune diseases, and other rare diseases  [27] .\n\nOther factors contributing to the widespread interest in dogs include fascination with their origins in the context of domestication, as well as their behavior and cognitive abilities. Moreover, we must improve our understanding of and regulate dog-human interactions and welfare impacts, including working dogs and shelter dogs  [35] .\n\nThe objective measurement of dog facial expressions is crucial for their investigation as indicators of emotional states in different contexts  [3, 41] . Facial expressions have also been studied in the context of understanding doghuman communication (e.g., the impact of dog facial phenotypes on their communication abilities with humans  [44] , the impact of facial features on the ability of humans to understand dogs  [16] , etc.)\n\nThe gold standard for objectively assessing changes in facial expressions in human emotion research is the Facial Action Coding System -FACS  [15] . FACS has recently been adapted for different non-human species, including dogs  [48] . DogFACS has been applied in several studies  [2, 3, 8, 41, 44]  to objectively measure facial changes. However, using this method for facial expression analysis depends on laborious manual annotation, which also requires extensive human training and certification and may be prone to human error or bias  [24] . Some first steps to automated DogFACS were taken by Boneh et al.  [1] .\n\nGeometric morphometrics offers an appealing alternative approach, successfully applied in analyzing cat facial features  [21, 22] . This method uses points (landmarks) on objects as proxies for shape, allowing for the quantification of facial shape changes. This concept is closely related to landmarks extensively studied in the human domain.\n\nIndeed, numerous fundamental methods for detecting, labeling, and aligning facial and body landmarks in humans have emerged  [6, 11, 23, 25, 28, 32, 43, 49] . Typically, datasets with human facial landmarks consist of thousands of images with dozens of landmarks. This abundance of data leads to better model performance, even in challenging scenarios The animal domain, on the other hand, severely needs landmark-related datasets. Table  1  shows the available datasets, including their size and number of landmarks, which generally are extremely small compared to the human domain.\n\nIn the case of domesticated species, which are often artificially selected for specific features, there is a great variation in facial morphology and appearance. It is particularly noticeable in the case of cats and even more so in dogs, making it challenging to create versatile landmark detection models  [9, 12, 48] . With the increase in quantity and quality of animal facial landmark datasets, it has become possible to create cross-view models  [5, 10, 34] , but the number of landmarks in such works is usually relatively small. To solve the problems of determining the emotions or states of animals, several dozens of landmarks are needed  [17, 19, 20] , which are unique for a particular animal.\n\nTo tackle the shortage of datasets, the Cat Facial Landmarks in the Wild (CatFLW) dataset was recently introduced by Martvel et al.  [36] . The dataset comprises over 2,000 images of cat faces in various environments, each with face bounding boxes and 48 facial landmarks as detailed in  [21] . Notably, the utilized landmark scheme is based on the cat's facial anatomy and the CatFACS annotation system  [9] . This approach has facilitated the development of models for automated identification of pain in cats  [18, 37, 38] .\n\nDogs exhibit a wide range of facial features, making them more challenging than cats for automated facial analysis. To fill this gap, this paper introduces the Dog Facial Landmarks in the Wild (DogFLW) dataset, comprising a set of 46 facial landmarks that were defined, based on and guided by the facial anatomy of dogs and the Dog-FACS method. We utilized the Ensemble Landmark Detector (ELD)  [39]  to provide a benchmark for this dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Landmark Scheme",
      "text": "The development of the dog facial landmark scheme was undertaken with the aim of creating a comprehensive framework for analyzing canine facial expressions with the help of three experienced dog behavior researchers certified in DogFACS coding  [48] . To establish the number of landmarks and each landmark location, the experts worked independently and then converged using expert consensus. The obtained final landmark scheme is shown in Figure  1 . All details and landmark descriptions are available in the spreadsheet.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Properties",
      "text": "As a source for our dataset, we used the Stanford Dog dataset  [31] , which contains 20,580 images, 120 breeds and bounding boxes for dogs. Since we are interested in bounding boxes for the dog's face rather than the entire body, we did not utilize the latter in the current study.\n\nFirst, we selected a random subset with an equal amount of images per breed. Then, we filtered the images according to the following criteria: the image contains a single visible dog face, where the dog is in non-laboratory conditions ('in the wild'). Other dogs could be present, but their faces shouldn't be visible for unambiguity of detection. The resulting subset contains 7-40 images per breed (25 on average) of all 120 breeds (3,274 images total), ranging in size from 100 × 103 to 1944 × 2592 pixels. Dogs in images have different sizes, colors, body and head poses, as well as different environments.\n\nEach image is annotated with 46 facial landmarks using the CVAT platform [13] according to the scheme described  above. Unlike the CatFLW  [36] , our dataset has occluded landmarks, all annotated landmarks have a visibility indicator (\"0\" -occluded, \"1\" -visible). Each image is also annotated with a face bounding box, which encompasses the entire face of the animal, along with approximately 10% of the surrounding space. This margin has proven crucial for training face detection models, as it prevents the cropping of important parts of the dog's face, such as the tips of the ears or the mouth. Figure  2  shows some examples of annotated images from the DogFLW dataset.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Benchmarks",
      "text": "For the evaluation, the dataset was randomly divided into the train (2,794 images) and test (480 images) sets. To provide correct metrics, we cropped all the faces by their detected bounding boxes in the preprocessing stage. All the models in this section were trained on a train set, and the final results are provided for the test set.\n\nMetrics. We use Normalized Mean Error (N M E iod ) that preserves the relativity of the error regardless of the size of the image or the scale of the face on it. It is commonly utilized in landmark detection  [7, 46, 52] , and uses MAE as the basis and inter-ocular distance (distance between the outer corners of the two eyes, IOD) for normalization:\n\nwhere M is the number of landmarks in the image, N is the number of images in the dataset, x i j and x ′ i j -the coor-dinates of the predicted and ground truth landmark respectively.\n\nExperimental Setup. We used the Ensemble Landmark Detector (ELD)  [39]  model as a landmark detector, which is justified by its high performance on the CatFLW. We used the YOLOv8  [29]  model as a face detector and the Effi-cientNetV2S  [47]  model as a backbone for the ensemble. All other parameters are similar to the original article.\n\nThe face detection model was trained using standard YOLOv8n training parameters for 100 epochs. The ensemble models were trained for 300 epochs using the mean squared error loss and the ADAM optimizer with a learning rate of 10 -4 and a batch size of 16.\n\nTo train the models, we used the Google Colab cloud service with the NVIDIA TESLA V100 GPU.\n\nAugmentation. We randomly applied different augmentations (rotation, color balance adjustment, brightness and contrast modification, sharpness alteration, application of random blur masks, and addition of random noise) to the training data, doubling the size of the training set. Additionally, when training ensemble models, we mirrored symmetrical regions (ears and eyes) in order to further artificially increase the amount of training data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset Size",
      "text": "In order to examine how the size of the training dataset affects the accuracy of landmark detection, we carried out a series of experiments, training the ELD on random subsets of the training data (averaging the results for 5 iterations) and gradually increasing their size. Figure  3  indicates that the detection accuracy is approaching a plateau when the model is trained on 65% of the data (1800 images). However, the accuracy ceiling has yet to be reached, indicating potential for further work into diversification and expanding the dataset size. The ELD model trained on the whole training set has a NME of 6.52. The detection examples are shown on Figure  4 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ear Types",
      "text": "While conducting experiments, it was observed that the accuracy of localizing landmarks in the ears is noticeably lower compared to other regions (Table  2  shows the NME for different regions). This is presumably due to the diverse shapes and lengths of ears across different breeds, which may include previously unseen ear types and positions in the test set, particularly for floppy or half-floppy ear types.\n\nTo test our hypothesis, we divided the data into three subsets: one with erect ears (pointy), another with hanging ears (floppy), and the third with other types (half-floppy). When Despite the dataset's predominance of breeds with floppy ear types, Table  3  shows that the accuracy of landmark detection on a test set for such dogs is less than for dogs with erect and half-floppy ears. This can be explained by the variety of lengths and positions of such ears.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Breeds",
      "text": "Due to the uneven presence of breeds in the training set, the accuracy of landmark detection for rare breeds may be lower than for frequent breeds. However, it would be incorrect to say that only the number of samples affects the detection error value. The influence of the breed as an additional feature on the accuracy of detection is apparentmany breeds have distinct fur length and texture, as well as facial anatomy.\n\nFigure  5  shows the image distribution of dogs of each breed in the training set and the detection error for the same It is also worth noting that some breeds have non-obvious detection results. In some cases, this could be explained by a significant variation in the position of the ears (Ibizan Hound, Collie, Great Dane) or they being obscure (Chow), which can cause a significant detection error. Breeds with short smooth facial fur (Cardigan, Kelpie, Miniature Pinscher, Dingo, Chihuahua, Malinois, etc.), for which facial features are clearly distinguishable, have the lowest detection error on average.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "The domestic dog is a highly social animal with a complex, elaborate communication system via facial signaling, which also underwent the most extreme morphological changes due to domestication and selective breeding. In this paper, we introduce a landmark scheme that is grounded in dog facial musculature. We further present an annotated dataset and benchmark detection results aimed at advancing the field of dog facial analysis. As expected, the model has the most difficulties with ear landmarks, which can be improved by enriching the dataset with more samples with diverse ear types. A similar strategy can be taken to improve the performance on certain breeds. Our future work includes utilizing the presented scheme and dataset to classify the internal states of dogs.\n\nThe presented dataset holds great promise for canine cognition, emotion, health, and welfare research. We hope that it will aid the scientific community in utilizing AIdriven methods to deepen our understanding of our best friends' behavior and emotional world.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: All details and landmark descriptions are available in the",
      "page": 2
    },
    {
      "caption": "Figure 1: Annotated Dog’s Face. Image of a dog with a face",
      "page": 2
    },
    {
      "caption": "Figure 2: Examples of annotated images from the DogFLW.",
      "page": 3
    },
    {
      "caption": "Figure 2: shows some examples of annotated",
      "page": 3
    },
    {
      "caption": "Figure 3: indicates that",
      "page": 3
    },
    {
      "caption": "Figure 4: Figure 3. Impact of the training subset size on the normalised",
      "page": 4
    },
    {
      "caption": "Figure 5: shows the image distribution of dogs of each",
      "page": 4
    },
    {
      "caption": "Figure 4: Landmark detection examples on pre-cropped images from the test set. Red — ground truth, blue — predictions.",
      "page": 4
    },
    {
      "caption": "Figure 5: The distribution of images in the training set for each breed of dogs and the detection errors for the same breeds in the",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of animal facial landmarks datasets",
      "page": 2
    },
    {
      "caption": "Table 1: shows the available",
      "page": 2
    },
    {
      "caption": "Table 2: shows the NME",
      "page": 4
    },
    {
      "caption": "Table 2: Normalised mean error of landmark detection on the test",
      "page": 4
    },
    {
      "caption": "Table 3: shows that the accuracy of landmark de-",
      "page": 4
    },
    {
      "caption": "Table 3: Normalised mean error of landmark detection on the test",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Explainable automated recognition of emotional states from canine facial expressions: the case of positive anticipation and frustration",
      "authors": [
        "Tali Boneh-Shitrit",
        "Marcelo Feighelstein",
        "Annika Bremhorst",
        "Shir Amir",
        "Tomer Distelfeld",
        "Yaniv Dassa",
        "Sharon Yaroshetsky",
        "Stefanie Riemer",
        "Ilan Shimshoni",
        "Daniel Mills"
      ],
      "year": "2022",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "2",
      "title": "Evaluating the accuracy of facial expressions as emotion indicators across contexts in dogs",
      "authors": [
        "Bremhorst",
        "H Mills",
        "Würbel",
        "Riemer"
      ],
      "year": "2021",
      "venue": "Animal cognition"
    },
    {
      "citation_id": "3",
      "title": "Figure 5. The distribution of images in the training set for each breed of dogs and the detection errors for the same breeds in the test set",
      "venue": "Figure 5. The distribution of images in the training set for each breed of dogs and the detection errors for the same breeds in the test set"
    },
    {
      "citation_id": "4",
      "title": "Differences in facial expressions during positive anticipation and frustration in dogs awaiting a reward",
      "authors": [
        "Annika Bremhorst",
        "Nicole Sutter",
        "Hanno Würbel",
        "Daniel Mills",
        "Stefanie Riemer"
      ],
      "year": "2019",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "5",
      "title": "Going deeper than tracking: A survey of computer-vision based recognition of animal pain and emotions",
      "authors": [
        "Sofia Broomé",
        "Marcelo Feighelstein",
        "Anna Zamansky",
        "Carreira Gabriel",
        "Pia Lencioni",
        "Francisca Haubro Andersen",
        "Marwa Pessanha",
        "Hedvig Mahmoud",
        "Albert Kjellström",
        "Salah"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "6",
      "title": "A two-step learning method for detecting landmarks on faces from different domains",
      "authors": [
        "R Erickson",
        "Bruna Nascimento",
        "Vieira Frade"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "7",
      "title": "Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "8",
      "title": "Robust face landmark estimation under occlusion",
      "authors": [
        "P Xavier",
        "Pietro Burgos-Artizzu",
        "Piotr Perona",
        "Dollár"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Dogs and humans respond to emotionally competent stimuli by producing different facial actions",
      "authors": [
        "Cátia Caeiro",
        "Kun Guo",
        "Daniel Mills"
      ],
      "year": "2017",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "10",
      "title": "Development and application of catfacs: Are human cat adopters influenced by cat facial expressions?",
      "authors": [
        "Anne Cátia C Caeiro",
        "Bridget Burrows",
        "Waller"
      ],
      "year": "2017",
      "venue": "Applied Animal Behaviour Science"
    },
    {
      "citation_id": "11",
      "title": "Cross-domain adaptation for animal pose estimation",
      "authors": [
        "Jinkun Cao",
        "Hongyang Tang",
        "Hao-Shu Fang",
        "Xiaoyong Shen",
        "Cewu Lu",
        "Yu-Wing Tai"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Structured feature learning for pose estimation",
      "authors": [
        "X Chu",
        "W Ouyang",
        "H Li",
        "X Wang"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Extending the MaqFACS to measure facial movement in japanese macaques (macaca fuscata) reveals a wide repertoire potential",
      "authors": [
        "Catia Correia-Caeiro",
        "Kathryn Holmes",
        "Takako Miyabe-Nishiwaki"
      ],
      "year": "2021",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "14",
      "title": "Facial expression: An underutilized tool for the assessment of welfare in mammals",
      "authors": [
        "Kris Descovich",
        "Jennifer Wathan",
        "Matthew Leach",
        "Hannah Buchanan-Smith",
        "Paul Flecknell",
        "David Farningham",
        "Sarah-Jane Vick"
      ],
      "year": "2017",
      "venue": "ALTEX-Alternatives to animal experimentation"
    },
    {
      "citation_id": "15",
      "title": "Facial Action Coding System: Manual",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Facial Action Coding System: Manual"
    },
    {
      "citation_id": "16",
      "title": "Can my human read my flat face? the curious case of understanding the contextual cues of extremely brachycephalic dogs",
      "authors": [
        "Petra Eretová",
        "Quanxiao Liu",
        "Lucie Přibylová",
        "Helena Chaloupková",
        "Viktória Bakos",
        "Rita Lenkei",
        "Péter Pongrácz"
      ],
      "year": "2024",
      "venue": "Applied Animal Behaviour Science"
    },
    {
      "citation_id": "17",
      "title": "Facial expressions of pain in cats: the development and validation of a feline grimace scale",
      "authors": [
        "Ryota Marina C Evangelista",
        "Vivian Watanabe",
        "Beatriz Sy Leung",
        "Elizabeth O' Monteiro",
        "Daniel Toole",
        "Paulo Pang",
        "Steagall"
      ],
      "year": "2019",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "18",
      "title": "Explainable automated pain recognition in cats",
      "authors": [
        "Marcelo Feighelstein",
        "Lea Henze",
        "Sebastian Meller",
        "Ilan Shimshoni",
        "Ben Hermoni",
        "Michael Berko",
        "Friederike Twele",
        "Alexandra Schütter",
        "Nora Dorn",
        "Sabine Kästner"
      ],
      "year": "2023",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "19",
      "title": "Automated recognition of pain in cats",
      "authors": [
        "Marcelo Feighelstein",
        "Ilan Shimshoni",
        "Lauren Finka",
        "P Stelio",
        "Daniel Luna",
        "Anna Mills",
        "Zamansky"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "20",
      "title": "Predicting dog emotions based on posture analysis using deeplabcut",
      "authors": [
        "Kim Ferres",
        "Timo Schloesser",
        "Peter Gloor"
      ],
      "year": "2022",
      "venue": "Future Internet"
    },
    {
      "citation_id": "21",
      "title": "Geometric morphometrics for the study of facial expressions in non-human animals, using the domestic cat as an exemplar",
      "authors": [
        "Lauren R Finka",
        "P Stelio",
        "Juliana Luna",
        "Yorgos Brondani",
        "John Tzimiropoulos",
        "Mcdonagh",
        "J Mark",
        "Marcello Farnworth",
        "Daniel Ruta",
        "Mills"
      ],
      "year": "2019",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "22",
      "title": "The application of geometric morphometrics to explore potential impacts of anthropocentric selection on animals' ability to communicate via the face: The domestic cat as a case study",
      "authors": [
        "Lauren R Finka",
        "P Stelio",
        "Daniel Luna",
        "Mark Mills",
        "Farnworth"
      ],
      "year": "2020",
      "venue": "Frontiers in Veterinary Science"
    },
    {
      "citation_id": "23",
      "title": "Pfld: A practical facial landmark detector",
      "authors": [
        "X Guo",
        "S Li",
        "J Zhang",
        "J Ma",
        "L Ma",
        "W Liu",
        "H Ling"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "24",
      "title": "Automated facial action coding system for dynamic analysis of facial expressions in neuropsychiatric disorders",
      "authors": [
        "Jihun Hamm",
        "Christian Kohler",
        "Ruben Gur",
        "Ragini Verma"
      ],
      "year": "2011",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "25",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "26",
      "title": "Pose-informed face alignment for extreme head pose variations in animals",
      "authors": [
        "Charlie Hewitt",
        "Marwa Mahmoud"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "27",
      "title": "Canine models of human rare disorders",
      "authors": [
        "K Marjo",
        "Hannes Hytönen",
        "Lohi"
      ],
      "year": "2016",
      "venue": "Canine models of human rare disorders"
    },
    {
      "citation_id": "28",
      "title": "Pixel-in-pixel net: Towards efficient facial landmark detection in the wild",
      "authors": [
        "H Jin",
        "S Liao",
        "L Shao"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "",
      "authors": [
        "G Jocher",
        "A Chaurasia",
        "J Qiu"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "30",
      "title": "Animalweb: A large-scale hierarchical dataset of annotated animal faces",
      "authors": [
        "Muhammad Haris Khan",
        "John Mcdonagh",
        "Salman Khan",
        "Muhammad Shahabuddin",
        "Aditya Arora",
        "Fahad Shahbaz Khan",
        "Ling Shao",
        "Georgios Tzimiropoulos"
      ],
      "year": "2019",
      "venue": "Animalweb: A large-scale hierarchical dataset of annotated animal faces"
    },
    {
      "citation_id": "31",
      "title": "Novel dataset for fine-grained image categorization",
      "authors": [
        "Aditya Khosla",
        "Nityananda Jayadevaprakash",
        "Bangpeng Yao",
        "Li Fei-Fei"
      ],
      "year": "2002",
      "venue": "First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Structured landmark detection via topology-adapting deep graph learning",
      "authors": [
        "W Li",
        "Y Lu",
        "K Zheng",
        "H Liao",
        "C Lin",
        "J Luo",
        "C.-T Cheng",
        "J Xiao",
        "L Lu",
        "C.-F Kuo",
        "S Miao"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "33",
      "title": "Dog breed classification using part localization",
      "authors": [
        "Jiongxin Liu",
        "Angjoo Kanazawa",
        "David Jacobs",
        "Peter Belhumeur"
      ],
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "34",
      "title": "",
      "authors": [
        "Springer"
      ],
      "year": "2012",
      "venue": ""
    },
    {
      "citation_id": "35",
      "title": "Interspecies knowledge transfer for facial keypoint detection",
      "authors": [
        "Jae Yong",
        "Lee Rashid",
        "Xiuye Gu"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "36",
      "title": "The new era of canine science: reshaping our relationships with dogs",
      "authors": [
        "Aubrey Evan L Maclean",
        "Harold Fine",
        "Eric Herzog",
        "Mia Strauss",
        "Cobb"
      ],
      "venue": "Frontiers in Veterinary Science"
    },
    {
      "citation_id": "37",
      "title": "Cat facial landmarks in the wild dataset",
      "authors": [
        "George Martvel",
        "Nareed Farhat",
        "Ilan Shimshoni",
        "Anna Zamansky",
        "Catflw"
      ],
      "year": "2023",
      "venue": "Cat facial landmarks in the wild dataset",
      "arxiv": "arXiv:2305.04232"
    },
    {
      "citation_id": "38",
      "title": "Automated pain recognition in cats using facial landmarks: Dynamics matter",
      "authors": [
        "George Martvel",
        "Teddy Lazebnik",
        "Marcelo Feighelstein",
        "Lea Henze",
        "Sebastian Meller",
        "Ilan Shimshoni",
        "Friederike Twele",
        "Alexandra Schutter",
        "Nora Dorn",
        "Sabine Kastner",
        "Lauren Finka",
        "Stelio Luna",
        "Daniel Mills",
        "Holger Volk",
        "Anna Zamansky"
      ],
      "year": "2023",
      "venue": "Automated pain recognition in cats using facial landmarks: Dynamics matter"
    },
    {
      "citation_id": "39",
      "title": "Automated landmark-based cat facial analysis and its applications",
      "authors": [
        "George Martvel",
        "Teddy Lazebnik",
        "Marcelo Feighelstein",
        "Sebastian Meller",
        "Ilan Shimshoni",
        "Lauren Finka",
        "Stelio Luna",
        "Daniel Mills",
        "Holger Volk",
        "Anna Zamansky"
      ],
      "year": "2023",
      "venue": "Automated landmark-based cat facial analysis and its applications"
    },
    {
      "citation_id": "40",
      "title": "Automated detection of cat facial landmarks",
      "authors": [
        "George Martvel",
        "Ilan Shimshoni",
        "Anna Zamansky"
      ],
      "year": "2024",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "A deep learning approach for dog face verification and recognition",
      "authors": [
        "Guillaume Mougeot",
        "Dewei Li",
        "Shuai Jia"
      ],
      "year": "2019",
      "venue": "Lecture Notes in Computer Science"
    },
    {
      "citation_id": "42",
      "title": "Audience effect on domestic dogs' behavioural displays and facial expressions",
      "authors": [
        "Giulia Pedretti",
        "Chiara Canori",
        "Sarah Marshall-Pescini",
        "Rupert Palme",
        "Annalisa Pelosi",
        "Paola Valsecchi"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "43",
      "title": "Facial image-based automatic assessment of equine pain",
      "authors": [
        "Francisca Pessanha",
        "Albert Ali Salah",
        "Thijs Van Loon",
        "Remco Veltkamp"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L.-C Chen"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "45",
      "title": "What is written on a dog's face? evaluating the impact of facial phenotypes on communication between humans and canines",
      "authors": [
        "Colleen Courtney L Sexton",
        "Jake Buckley",
        "Francys Lieberfarb",
        "Erin Subiaul",
        "Brenda Hecht",
        "Bradley"
      ],
      "year": "2023",
      "venue": "Animals"
    },
    {
      "citation_id": "46",
      "title": "Cafm: A 3d morphable model for animals",
      "authors": [
        "Yifan Sun",
        "Noboru Murata"
      ],
      "year": "2020",
      "venue": "IEEE Winter Applications of Computer Vision Workshops (WACVW)"
    },
    {
      "citation_id": "47",
      "title": "Deep convolutional network cascade for facial point detection",
      "authors": [
        "Yi Sun",
        "Xiaogang Wang",
        "Xiaoou Tang"
      ],
      "year": "2013",
      "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "V Quoc",
        "Le",
        "Efficientnet"
      ],
      "year": "2019",
      "venue": "Rethinking model scaling for convolutional neural networks"
    },
    {
      "citation_id": "49",
      "title": "Paedomorphic facial expressions give dogs a selective advantage",
      "authors": [
        "Bridget Waller",
        "Kate Peirce",
        "C Cátia",
        "Linda Caeiro",
        "Anne Scheider",
        "Sandra Burrows",
        "Juliane Mccune",
        "Kaminski"
      ],
      "year": "2013",
      "venue": "PLoS one"
    },
    {
      "citation_id": "50",
      "title": "Anchorface: An anchorbased facial landmark detector across large poses",
      "authors": [
        "Z Xu",
        "B Li",
        "Y Yuan",
        "M Geng"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artifi-cial Intelligence, AAAI 2021, Thirty-Third Confer-ence on Innovative Applications of Artificial Intelli-gence, IAAI 2021, The Eleventh Symposium on Ed-ucational Advances in Artificial Intelligence, EAAI 2021"
    },
    {
      "citation_id": "51",
      "title": "Human and sheep facial landmarks localisation by triplet interpolated features",
      "authors": [
        "Heng Yang",
        "Renqiao Zhang",
        "Peter Robinson"
      ],
      "year": "2015",
      "venue": "Human and sheep facial landmarks localisation by triplet interpolated features"
    },
    {
      "citation_id": "52",
      "title": "Cat head detection -how to effectively exploit shape and texture features",
      "authors": [
        "Weiwei Zhang",
        "Jian Sun",
        "Xiaoou Tang"
      ],
      "year": "2008",
      "venue": "ECCV"
    },
    {
      "citation_id": "53",
      "title": "Learning and transferring multi-task deep representation for face alignment",
      "authors": [
        "Zhanpeng Zhang",
        "Ping Luo",
        "Chen Loy",
        "Xiaoou Tang"
      ],
      "year": "2014",
      "venue": "Learning and transferring multi-task deep representation for face alignment"
    }
  ]
}