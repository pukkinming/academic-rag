{
  "paper_id": "2011.05594v1",
  "title": "Wadenet: Wavelet Decomposition Based Cnn For Speech Processing",
  "published": "2020-11-11T06:43:03Z",
  "authors": [
    "Prithvi Suresh",
    "Abhijith Ragav"
  ],
  "keywords": [
    "Wavelet Decomposition",
    "Speech Processing",
    "End-to-End Deep Learning",
    "CNNs"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Existing speech processing systems consist of different modules, individually optimized for a specific task such as acoustic modelling or feature extraction. In addition to not assuring optimality of the system, the disjoint nature of current speech processing systems make them unsuitable for ubiquitous health applications. We propose WaDeNet, an end-toend model for mobile speech processing. In order to incorporate spectral features, WaDeNet embeds wavelet decomposition of the speech signal within the architecture. This allows WaDeNet to learn from spectral features in an end-to-end manner, thus alleviating the need for feature extraction and successive modules that are currently present in speech processing systems. WaDeNet outperforms the current state of the art in datasets that involve speech for mobile health applications such as non-invasive emotion recognition. WaDeNet achieves an average increase in accuracy of 6.36% when compared to the existing state of the art models. Additionally, WaDeNet is considerably lighter than a simple CNNs with a similar architecture.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A substantial part of current speech modelling systems is feature extraction  [1, 2] . However, the advent of Deep Learning (DL) has caused a significant paradigm shift in how these signals are modelled. Over the last 3 decades, advanced \"handcrafted\" transformations such as the Mel-frequency cepstral coefficients (MFCC) were used owing to a rise in accuracy in their use with Hidden Markov Model (HMM) systems  [3] . However, these transformations caused a loss of information. DL models sought to make up for this loss by learning naive spectral  [4]  or waveform  [5]  features from the speech signal thus reducing the dependency on hand-crafted features. In fact, Deng et al.  [6]  showed that DL models benefit from using simpler spectrograms over manually extracted MFCCs. Additionally, when compared to complex features, raw spectral features preserve more information, making them suitable Authors with equal contribution for handling large variability across users (speaking styles, accents, etc.).\n\nCurrent systems for speech processing contain different modules that perform different tasks, such as an acoustic module and a classifier  [7] . Every module has its own training process that optimizes a different objective function, each differing from the true evaluation criteria. Thus, global optimality of the entire system is not guaranteed in spite of module optimality  [8] . Additionally, development of these systems require extensive hyper parameter tuning by experts  [9] . These shortcomings curb the ability of current systems to be deployed in a mobile environment, ushering in the need for a powerful end-to-end model.\n\nKeeping in mind these disadvantages, we propose an endto-end Deep Learning model for speech processing. Due to its end-to-end nature, these models find use in mobile health applications such as outpatient telemetry and ubiquitous affective computing tasks. We chose to perform our experiments on tasks involving speech towards emotion recognition and pervasive pathological voice detection, since they are important mobile affective computing tasks. The nature of this task makes it appropriate for our experiments since it demands the ability of the solution to be run on edge devices.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "In an attempt to provide a complete end-to-end solution, an architecture that learns to derive features and suitably learn to classify from these features is to be realised. To this end, a convolutional feature extractor followed by a series of fully connected layers for classification is proposed. Convolutional Neural Network (CNN) architectures have shown success as temporal feature extractors in speech processing tasks  [10] . By connecting the feature extractor and fully connected layers, and training the resultant neural network, an end-to-end pipeline is achieved. This eliminates the need for feature extraction via complex signal processing techniques.\n\nFor our experiments we used two architectures -a naive 1D CNN (Naive CNN) and an enhanced CNN incorporating wavelet decomposition. Both CNNs contain the same fully connected layers but vary in the feature extractor. Their architectures are explained in succeeding sections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fully Connected Block:",
      "text": "This block consists of a series of Fully Connected Layers, each being succeeded by ReLU activation and a dropout of probability 0.5 to 1 All convolutions refer to one dimensional convolutions unless specified.\n\nprevent overfitting. Depending on the complexity of the task, the number of activation units and number of layers is empirically altered.\n\nThe Naive CNN comprises of a N Convolutional Blocks followed by a Fully Connected Block. The output feature map of the n th Convolutional Block has dimensions (2 n-1 ×c , l/2 n ) where c is the number of output channels of the first Convolutional Block and l is the length of the input. The last Convolutional Block's output feature map is flattened before passing it on to the Fully Connected Block. Softmax activation is applied to the output of the last layer of the Fully Connected Block.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Wavelet Decomposition Net (Wadenet)",
      "text": "From the analysis of the architecture of the Naive CNN it is clear that there are some shortcomings. Firstly, increasing the depth of this network for better information capture will possibly introduce the problem of vanishing gradients. Secondly, the frequency composition of each window, which is vital for speech processing, is unknown to the model. In an effort to address these drawbacks, we propose Wavelet Decomposition Net (WaDeNet), an enhancement of the Naive CNN. By inserting an Inception-Residual Block at the end of each Convolutional Block, WaDeNet has an increased network depth while alleviating vanishing gradients. The Inception-Residual Block consists of multiple convolutions with different kernel sizes performed in parallel as shown in Fig.  2 . Following this, channel wise concatenation of these representations is done.\n\nThe second shortcoming is countered by introducing frequency information into the model by incorporating a multiresolutional representation. This is achieved by embedding the Wavelet Transform into the architecture, which is well experimented with for image reconstruction  [11, 12, 13] . As shown in Fig  1 , a representation of a particular order of the Discrete Wavelet Transformation (DWT) of the signal is con- This approach is fundamentally different from using signal processing techniques to extract features that consist of both time and frequency information followed by training a classifier using these features. By incorporating the DWT into the network, the temporal information of the signal is not only retained but also learnt on in an end-to-end manner. Additionally, through feature map concatenation, the number of parameters is drastically reduced while improving performance, as discussed in Section 3.3. This makes it suitable for mobile health applications such has emotion and stress monitoring.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Preproccesing",
      "text": "We run our experiments on popular datasets which consist of speech data suitable for mobile health applications such as emotion recognition and pathological voice recognition. The datasets and their respective descriptions are as follows:\n\n1. EmoDB  [14] : This database consists of 10 actors who simulate 7 different emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral) by uttering sentences used in everyday communication. There are 535 utterances in the dataset, amounting to 1487 seconds and an average utterance length of about 2.77 seconds.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ravdess [15]:",
      "text": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) consists of 12 male and 12 female actors speaking each expression in a North American Accent. The actors speak each statement with 8 different emotions -anger, calm, disgust, fear, happiness, sadness, surprise, neutral -at two levels of intensities, except when speaking in a neutral manner.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Voiced [16]:",
      "text": "The VOice ICar fEDerico II Database (VOICED) contains 208 voice samples of adults between the age 18 and 70, of which 150 are pathological and the rest healthy. Each sample is consists of uninterrupted vocalization of the vowel 'a' for 5 seconds.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Tess [17]:",
      "text": "The Toronto Emotional Speech Set (TESS) consists of 200 words spoken by two female actors. Each of the voice samples emulate 7 different emotions namely anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral.\n\nThe voice samples are segmented into windows of 320 ms each with a 75% overlap. Each window is labelled depending on the dataset. For instance, a window in the EmoDB dataset is labelled with the corresponding emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation",
      "text": "For our experiments, N = 4, c i = 64 and k = 3 for each convolution was chosen. Stochastic Gradient Descent with an initial learning rate of 0.001 was used to optimize the weights of the model by minimizing the categorical cross entropy. WaDeNet was trained for 150 epochs on 60% of the data with a learning rate scheduler which reduced the learning rate by a factor of 10 after 50 epochs. These constants were empirically decided after extensive experimentation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baseline Comparison",
      "text": "We compare the performance of the Naive CNN and WadeNet on the datasets discussed in the previous section. Since the F1 score is a more reliable metric for multi-class classification problems, we report it in addition to the overall accuracy of the model's performance on the test data (20% of the total data). It is evident that the absence of spectral features translates to a subpar performance as evident from the performance of the Naive CNN (Table  1 ). In contrast, WaDeNet outperforms the Naive CNN with an average increase in accuracy and F1 score of 13.1% and 13.9% respectively. Additionally, WaDeNet has a maximum improvement up to 38.7% in the F1 score on the RAVDESS dataset.\n\nFurthermore, WaDeNet is considerably lighter than the Naive CNN, with an average reduction of 72.26% in the number of model parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Benchmarking",
      "text": "In order to validate WaDeNet, we evaluate WaDeNet's performance against existing research work that portray state of the art performance on the datasets considered. The experimental conditions for each benchmark were replicated as specified in the respective work. As seen in Table  2 , WaDeNet performs considerably better than the current state of the art models across all the datasets, with an average increase in accuracy of 6.36%, with a maximum of 11.4% on the RAVDESS dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper we propose WaDeNet, an end-to-end architecture capable of learning spectral information through Wavelet Decomposition from a raw speech signal. WaDeNet outperforms the Naive CNN with the same number of fully connected layers with a 72.26% reduction in model parameters. Moreover, WaDeNet also beats the current state of the art accuracy on all the datasets considered in Section 3.1. Since WaDeNet is end-to-end, it does not require disjoint training processes of different modules. This advantage combined with the benefits of a light model makes it suitable for mobile speech processing applications.\n\nAdditionally, we intend to perform quantization and replace standard convolutions with mobile convolutions to further optimize the model for mobile deployment.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: WaDeNet Architecture for N = 4 (k = 3)",
      "page": 2
    },
    {
      "caption": "Figure 2: Inception Residual Block",
      "page": 2
    },
    {
      "caption": "Figure 2: Following this, channel wise concatenation",
      "page": 2
    },
    {
      "caption": "Figure 1: , a representation of a particular order of the",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "EmoDB",
          "Model": "Naive CNN",
          "Accuracy": "0.843",
          "F1 Score": "0.829",
          "Model Parameters": "172,368,455"
        },
        {
          "Dataset": "",
          "Model": "WaDeNet",
          "Accuracy": "0.934",
          "F1 Score": "0.928",
          "Model Parameters": "47,156,391"
        },
        {
          "Dataset": "RAVDESS",
          "Model": "Naive CNN",
          "Accuracy": "0.497",
          "F1 Score": "0.484",
          "Model Parameters": "173,955,656"
        },
        {
          "Dataset": "",
          "Model": "WaDeNet",
          "Accuracy": "0.872",
          "F1 Score": "0.871",
          "Model Parameters": "47,156,904"
        },
        {
          "Dataset": "VOICED",
          "Model": "Naive CNN",
          "Accuracy": "0.961",
          "F1 Score": "0.947",
          "Model Parameters": "90,057,282"
        },
        {
          "Dataset": "",
          "Model": "WaDeNet",
          "Accuracy": "0.998",
          "F1 Score": "0.997",
          "Model Parameters": "26,182,306"
        },
        {
          "Dataset": "TESS",
          "Model": "Naive CNN",
          "Accuracy": "0.961",
          "F1 Score": "0.962",
          "Model Parameters": "173,953,607"
        },
        {
          "Dataset": "",
          "Model": "WaDeNet",
          "Accuracy": "0.982",
          "F1 Score": "0.982",
          "Model Parameters": "47,156,391"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: ). In contrast, WaDeNet outper-",
      "data": [
        {
          "Dataset": "",
          "Number\nof classes": "",
          "Accuracy": "Benchmark"
        },
        {
          "Dataset": "EmoDB",
          "Number\nof classes": "7",
          "Accuracy": "0.870 [18]"
        },
        {
          "Dataset": "RAVDESS",
          "Number\nof classes": "8",
          "Accuracy": "0.757 [19]"
        },
        {
          "Dataset": "VOICED",
          "Number\nof classes": "2",
          "Accuracy": "0.984 [20]"
        },
        {
          "Dataset": "TESS",
          "Number\nof classes": "7",
          "Accuracy": "N/A"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Linear predictive coding",
      "authors": [
        "O' Douglas",
        "Shaughnessy"
      ],
      "year": "1988",
      "venue": "IEEE potentials"
    },
    {
      "citation_id": "3",
      "title": "Perceptually based linear predictive analysis of speech",
      "authors": [
        "Hynek Hermansky",
        "Hisashi Hanson",
        "Wakita"
      ],
      "year": "1985",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Voice recognition algorithms using mel frequency cepstral coefficient (mfcc) and dynamic time warping (dtw) techniques",
      "authors": [
        "Lindasalwa Muda",
        "Mumtaj Begam",
        "Irraivan Elamvazuthi"
      ],
      "year": "2010",
      "venue": "Voice recognition algorithms using mel frequency cepstral coefficient (mfcc) and dynamic time warping (dtw) techniques",
      "arxiv": "arXiv:1003.4083"
    },
    {
      "citation_id": "5",
      "title": "Speech recognition from spectral dynamics",
      "authors": [
        "Hynek Hermansky"
      ],
      "year": "2011",
      "venue": "Sadhana"
    },
    {
      "citation_id": "6",
      "title": "Waveform-based speech recognition using hidden filter models: Parameter selection and sensitivity to power normalization",
      "authors": [
        "Hamid Sheikhzadeh",
        "Li Deng"
      ],
      "year": "1994",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "7",
      "title": "Binary coding of speech spectrograms using a deep autoencoder",
      "authors": [
        "Li Deng",
        "L Michael",
        "Dong Seltzer",
        "Alex Yu",
        "Abdel-Rahman Acero",
        "Geoff Mohamed",
        "Hinton"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "8",
      "title": "An overview of end-to-end automatic speech recognition",
      "authors": [
        "Dong Wang",
        "Xiaodong Wang",
        "Shaohe Lv"
      ],
      "year": "2019",
      "venue": "Symmetry"
    },
    {
      "citation_id": "9",
      "title": "Towards end-to-end speech recognition with deep convolutional neural networks",
      "authors": [
        "Ying Zhang",
        "Mohammad Pezeshki",
        "Philémon Brakel",
        "Saizheng Zhang",
        "Cesar Laurent Yoshua",
        "Aaron Bengio",
        "Courville"
      ],
      "year": "2017",
      "venue": "Towards end-to-end speech recognition with deep convolutional neural networks",
      "arxiv": "arXiv:1701.02720"
    },
    {
      "citation_id": "10",
      "title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding",
      "authors": [
        "Yajie Miao",
        "Mohammad Gowayyed",
        "Florian Metze"
      ],
      "year": "2015",
      "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "11",
      "title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition",
      "authors": [
        "O Abdel-Hamid",
        "A Mohamed",
        "H Jiang",
        "G Penn"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Multi-level wavelet convolutional neural networks",
      "authors": [
        "Pengju Liu",
        "Hongzhi Zhang",
        "Wei Lian",
        "Wangmeng Zuo"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Wavelet convolutional neural networks",
      "authors": [
        "Shin Fujieda",
        "Kohei Takayama",
        "Toshiya Hachisuka"
      ],
      "year": "2018",
      "venue": "Wavelet convolutional neural networks",
      "arxiv": "arXiv:1805.08620"
    },
    {
      "citation_id": "14",
      "title": "Dcwcnn: A deep cascade of wavelet based convolutional neural networks for mr image reconstruction",
      "authors": [
        "Balamurali Sriprabha Ramanarayanan",
        "Keerthi Murugesan",
        "Mohanasankar Ram",
        "Sivaprakasam"
      ],
      "year": "2020",
      "venue": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)"
    },
    {
      "citation_id": "15",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "16",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "17",
      "title": "A new database of healthy and pathological voices",
      "authors": [
        "Ugo Cesari",
        "Giuseppe Pietro",
        "Elio Marciano",
        "Ciro Niri",
        "Giovanna Sannino",
        "Laura Verde"
      ],
      "year": "2018",
      "venue": "Computers & Electrical Engineering"
    },
    {
      "citation_id": "18",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "Kathleen Pichora-Fuller",
        "Kate Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS)"
    },
    {
      "citation_id": "19",
      "title": "A light-weight artificial neural network for speech emotion recognition using average values of mfccs and their derivatives",
      "authors": [
        "P Nantasri",
        "E Phaisangittisagul",
        "J Karnjana",
        "S Boonkla",
        "S Keerativittayanun",
        "A Rugchatjaroen",
        "S Usanavasin",
        "T Shinozaki"
      ],
      "year": "2020",
      "venue": "2020 17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology"
    },
    {
      "citation_id": "20",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "Anjali Bhavan",
        "Pankaj Chauhan",
        "Rajiv Ratn Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "21",
      "title": "Deep neural network for automatic classification of pathological voice signals",
      "authors": [
        "Lili Chen",
        "Junjiang Chen"
      ],
      "year": "2020",
      "venue": "Journal of Voice"
    }
  ]
}