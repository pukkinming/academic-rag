{
  "paper_id": "2002.11312v2",
  "title": "Multitask Learning And Multistage Fusion For Dimensional Audiovisual Emotion Recognition",
  "published": "2020-02-26T06:06:00Z",
  "authors": [
    "Bagus Tris Atmaja",
    "Masato Akagi"
  ],
  "keywords": [
    "multitask learning",
    "multistage fusion",
    "audiovisual emotion recognition",
    "dimensional emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Due to its ability to accurately predict emotional state using multimodal features, audiovisual emotion recognition has recently gained more interest from researchers. This paper proposes two methods to predict emotional attributes from audio and visual data using a multitask learning and a fusion strategy. First, multitask learning is employed by adjusting three parameters for each attribute to improve the recognition rate. Second, a multistage fusion is proposed to combine results from various modalities' final prediction. Our approach used multitask learning, employed at unimodal and early fusion methods, shows improvement over single-task learning with an average CCC score of 0.431 compared to 0.297. A multistage method, employed at the late fusion approach, significantly improved the agreement score between true and predicted values on the development set of data (from [0.537, 0.565, 0.083] to [0.68, 0.656, 0.443]) for arousal, valence, and liking.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic emotion recognition has been approached using two perspectives: the categorical view and the dimensional view. While most researchers attempted to categorize human emotion within different categories (e.g. happiness, anger, etc.), dimensional emotion recognition is the more challenging task as it seeks to label the emotions as degrees rather than as categories. From dimensional perspective, emotion is described in 2 or 3 attributes  [1] . Valence (pleasantness) and arousal (emotion intensity) are the two most common dimensions in 2D emotion models. In 3D models, either dominance (degree of control) or liking is used. Another model, such as expectancy, can be added as a 4th dimension (4D)  [2] .\n\nIn this paper, we evaluated three emotional dimensions/attributes: arousal, valence, and liking, which have been obtained from the dataset in  [3] . The task is to obtain the most accurate prediction on a specific metric. As a regression task, the most common metric is the error between true value and predicted emotion degree. However, recent researchers  [3]  introduced correlation measurement to determine the agreement between true value and predicted emotion degree.\n\nTwo approaches are commonly used to minimize the loss of learning process functionality and to obtain the best model to predict emotion dimension, i.e., single-task learning (STL) and multitask learning (MTL). Single-task learning minimizes single loss function only in multiple output learning. For example, when learning to predict arousal, valence, and liking in dimensional emotion, only arousal is minimized. The other dimensions, valence and liking, are ignored in the learning process. By minimizing the error of arousal, the result of the learning process can be used to predict one dimension (arousal) or all three dimensions (arousal, valence, and liking).\n\nThe problem with single-task learning is that, when it is used to predict multiple outputs, three scores are predicted using single loss function. A high score in one dimension usually resulted in a lower score on the other dimensions. To address this issue, we introduced the use of multitask learning when minimizing error between true emotion and predicted emotion degree for all emotion dimensions.\n\nThe common approach in multitask learning is that the same weighting factors are used for each loss function in learning process. Therefore, the total is the sum of three each loss functions from each emotion dimensions. The method we propose in this paper is intended to obtain a balanced score by assigning a different weighting factor to each loss function for each emotion dimension.\n\nAs emotion comes from many modalities, the creation of a fusion strategy to accommodate those modalities is also a challenge. The standard method is by combining the features among different modalities in either the same or different networks. This is called an early fusion strategy. Two or more feature sets are then trained to map those inputs onto labels. Another strategy is the use of late fusion. In this strategy, each modality is trained in its network using its label. The recognition results for each modality are then grouped to find arXiv:2002.11312v2 [eess.AS] 9 Mar 2020 the highest probability corresponding to the labels. The results from early fusion and late fusion also can be fused by combining those results in support vector regression (SVR). The result from this last step can be repeated in a multistage direction to improve the recognition rate.\n\nOur contributions of this paper can be summarized as follows, (1) the use of multitask learning to minimize the loss function using three parameters for three emotion attributes from audiovisual features; (2) the fusion strategy by analyzing unimodal and bimodal features on early fusion and late fusion, and combining early-late fusion using multistage SVR to improve audiovisual emotion recognition rate.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multitask learning. One of the problems in machine learning is to obtain the appropriate cost function or loss function to model the data. Most problems in regression analysis use error calculation between the true value and predicted value the loss function. The choice of the loss function is frequently determined by the metric used for evaluation. In the case of dimensional emotion recognition, Ringeval et al. of proposed the use of a concordance correlation coefficient (CCC) to score the performance of predicted emotion attributes  [3] .\n\nParthasarathy and Busso used multitask learning to minimize mean squared error (MSE) in dimensional emotion recognition  [4] . The authors used two parameters to weigh loss function of three emotion attributes: arousal, valence, and dominance. Despite the weighting factor for both arousal and valence being determined, the weighting factor for dominance is obtained by subtracting 1 from the weighting factors of arousal and valence. All weighting factors lie in a range of 0-1 with a 33.3% possibility that one value is zero. It was also found that the best parameters are 0.7 and 0.3 for arousal and valence. In this case, dominance is ignored in learning process, which can be viewed as two-task learning which is similar to single task learning.\n\nUsing two approaches to multitask learning, such as shared layer and independent layer, the authors also achieved an improvement of CCC score compared to baseline singletask learning  [4] . As the system learned better on the larger network than on the smaller one, the larger the network used, the greater improvement obtained, Chen et al. also used multitask learning with MSE as the loss function  [5] . Although the improvement of CCC score from the given baseline is achieved, the performance comparison to single-task learning is not specified. This potentially leads to a difficulty in determining whether the improvement came from multitask learning or other used strategies. Multimodal Fusion. As emotion can be recognized from many modalities, e.g., speech, facial image, movement, and linguistic information, the use of multimodal technique to accommodate many features is often considered in such systems. The dataset described in  [3]  includes multimodal emo-tion features from audio and visual. Busso et al. provided an emotion dataset from speech and gesture, including facial expressions and hand movements  [6] . The improved version of that dataset provided an affective database with audiovisual information which promoting naturalness within the (acted) recording  [7] .\n\nTo deal with various features extracted from multimodal datasets, several categories of feature fusion have been developed by researchers  [5, 8, 9, 10, 11] . Most strategies can be divided into early fusion and late fusion. In an early fusion method, also known as feature level fusion, features from different modalities are combined before performing classification. In late fusion method, also known as decision level fusion, the final decision probabilities are given by each unimodal model results by such methods like SVR.\n\nRingeval et al. provides baseline fusion method for late fusion strategy from SEWA dataset  [3, 12] . The results from each modality or feature set can be combined using a static regressor, i.e., SVR to make the final decision of predicted emotion attribute scores from given results of several modalities.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data And Feature Sets",
      "text": "The SEWA dataset  [12]  provided in  [3]  is used in this research. The dataset contains audiovisual recordings from: Chinese, English, German, Greek, Hungarian, and Serbian, but only German (DE) and Hungarian (HU) are used in this work as they did not provide test label in other languages. Three attributes provided to represent emotional states i.e.: arousal, valence, and liking. The scores of those attributes are obtained from annotation of several native speakers: six Germans and five Hungarians. From 96 subjects, 68 subjects (34 each) are used in training, and the rest 28 subjects (14 each) are used for validation/development.\n\nIn addition to the dataset, the authors of paper  [3]  also provided baseline features which are shown in Table  1 . Instead of generating new a feature set, we applied the multitask learning and multimodal audiovisual fusion to those feature sets.\n\nFor both audio and visual features, the same processing blocks are used, i.e., 4.0 s of window length and 100 ms of hop size, where the label is also given for each 0.1 s. The longest 1768 sequences (label numbers) is then used for all subjects by padding zeros for other sequences below this number. For bimodal feature fusion, audio and visual features are concatenated before they are fed into the classifier.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multitask Learning Based On Ccc Loss",
      "text": "CCC is the common metric in dimensional emotion recognition to measure the agreement between true emotion dimen-Table  1 . Audio and visual feature sets evaluated in this research. Feature sets highlighted in bold are used for bimodal/multimodal emotion recognition.\n\naudio eGeMAPS  [13, 14] , Bag-of-Audio-Word eGeMAPS (BoAW-e)  [15] , eGeMAPS functional, DeepSpectrum (DS)  [16] , MFCCs, BoAW MFCCs (BoAW-M)  [15] , MFCCs functionals. visual Facial Activation Units (FAUs)  [17] ,\n\nFAUs functionals, ResNet  [18] , VGG  [19] . sion with predicted emotion degree. The CCC is formulated\n\nwhere ρ xy is the Pearson coefficient correlation between x and y, σ is standard deviation, and µ is a mean value. This CCC is based on Lin's calculation  [20] . The range of CCC is from -1 (perfect disagreement) to 1 (perfect agreement). Therefore, the CCC loss function (CCCL) to maximize the agreement between true value and prediction emotion can be defined as\n\nIn single-task learning, the loss function is one of the loss functions from arousal (CCCL aro ), valence (CCCL val ), or liking (CCCL lik ). In multitask learning, when CCC loss is used as a single metric for all arousal, valence, and liking, the CCCL total is a combination of those three CCC loss functions:\n\nwhere α, β, and γ are the weighting factors for each emotion dimension loss function. In a common approach, α, β, and γ are set to be 1, while in  [4] , γ is set to be 1 -(α + β) to minimize MSE. In that approach, all weighting factors are in range 0-1.\n\nIn this paper, we use all three parameters, and the sum of those weighting factors is not limited to only 0-1. As the goal is to strengthen CCC, CCC loss is used instead of MSE.\n\nAs shown in Fig.  1 , the audiovisual emotion recognition system consists of 3 LSTM layers with 256, 128, and 64 units. A dropout layer with a factor of 0.4 is added after each LSTM layer. A RMSprop optimizer is used with a learning rate of 0.0005 and 34 batch size for 50 epochs in one experiment.\n\nTo compensate for the delay when making an annotation, the label is shifted 0.1 to the front in the training process and shifted back in writing the prediction.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multistage Fusion Using Svr",
      "text": "In Fig.  1 , the system produces a prediction of arousal, valence, and liking degree from bimodal audio and visual feature sets. This result can be combined with the other results from the unimodal or bimodal (early) fusion using SVR (from different feature set), and the resulting prediction from SVR also can be input to the same SVR system (implemented using scikit-learn tool  [21] ). In Figure  2 , this combination of early fusion and late fusion is illustrated in three stages. First, the result from unimodal, named as result #1, and multimodal (bimodal), named as result #2, or unimodal and unimodal are trained using SVR method. This learning process results in a new result (namely, result #3 in that Figure ). The result #3 from late fusion is fed again to SVR method results in result #4. Result #4 is fed again to SVR method results in result #5. This multistage fusion can be performed n-times to gain improvement of CCC score.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multitask Learning Results",
      "text": "To evaluate the effectiveness of the proposed MTL method versus STL and previous MTL methods, we compared CCC scores among those methods. Table  2  shows CCC scores for different attributes with its average. Our proposed MTL2 outperforms STL and previous proposed MTL1. To find the optimum parameter of α, β, and γ, we performed random search for those parameters in range 0-1. The parameters used in Table 2 are the optimum ones i.e. 0.7, 0.2, and 1.0 for α, β, and γ, respectively. Our proposed MTL learning with three parameters outperforms STL and previous MTL  [4] . For STL approaches, both arousal and valence obtained the highest CCC score when its attribute is optimized. Although the liking is optimized in STL3, it remains the most difficult to estimate. This problem should be addressed in future research.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multistage Fusion Results",
      "text": "To obtain multistage fusion results, the following steps are performed, 1. Unimodal emotion recognition: This step is performed to investigate the importance feature set for bimodal or multimodal fusion. 2. Bimodal fusion: This step is performed by concatenating two feature sets, from different or same modality. 3. Multimodal fusion: While the first two steps are performed using DNN, this third step is performed using SVR by combining results from unimodal or bimodal emotion recognition. 4. Multistage fusion: Output from multimodal SVR can be combined using the same SVR to improve the recognition rate of emotion recognition. We run experiments on unimodal feature sets by inputting one feature set into a system to find which feature sets gives better performance. For this purpose, we use small networks with previously explained LSTM layers (implemented in Keras  [22] ). From 12 feature sets, we choose 7 feature sets by highest average CCC scores. The combination of seven feature sets resulted in 21 pairs of bimodal feature sets. Note that the definition of bimodal here is not audio and visual modalities but a pair of two feature sets. From unimodal and bimodal results, we choose the 11 highest CCC scores and input those 11 results to SVR to perform multimodal audiovisual emotion recognition by late fusion.\n\nThis last multimodal fusion using SVR can be regarded as 1-stage feature fusion. By inputting the result from SVR to the same SVR system, a 2-stage multimodal fusion can be performed. We limited this multistage multimodal fusion to 5 repetitions. The result of CCC scores for arousal, valence, and liking from 1 to 5 stages is shown in Fig.  3 . That figure shows that CCC scores improved as the number of stages increased.  In comparison with unimodal, bimodal, and multimodal fusion (1 stage), multistage fusion gained significant improvements. The proposed multistage fusion could improve CCC score of liking attribute, which is the most challenging attribute in this task, from 0.083 (baseline unimodal) to 0.443. Other two attributes obtained relative improvement over baseline results of 26.63% and 16.11% respectively for arousal and valence.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "A multitask learning strategy is proposed to balance the CCC score among arousal, valence, and liking by adjusting parameters for those attributes. The result shows that by using different weighting factors for each emotional dimension, an improvement in terms of CCC scores can be obtained. Using weighting factors of 0.7, 0.2, and 1.0 for arousal, valence, and liking, respectively, for MTL parameters, we achieved an improvement of average CCC score from 0.297 using STL to 0.431 using our MTL. To deal with multimodal fusion of several feature sets, we proposed a multistage fusion using SVR method. This proposed method improves the CCC score on development test significantly for bilingual emotion recognition (DE+HU), especially on liking attribute i.e., [0.680, 0.656, 0.443] with an average CCC score of 0.593.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of Deep Neural Network (DNN) with",
      "page": 3
    },
    {
      "caption": "Figure 1: , the audiovisual emotion recognition",
      "page": 3
    },
    {
      "caption": "Figure 2: The ﬂow of multistage SVR to combine early and late",
      "page": 3
    },
    {
      "caption": "Figure 1: , the system produces a prediction of arousal, va-",
      "page": 3
    },
    {
      "caption": "Figure 2: , this combination of",
      "page": 3
    },
    {
      "caption": "Figure 3: That ﬁgure shows",
      "page": 4
    },
    {
      "caption": "Figure 3: CCC score among attributes from the different num-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Audio and visual feature sets evaluated in this re-",
      "data": [
        {
          "Column_1": "FAUsfunctionals,ResNet[18],VGG[19].\nLoss calculation (Eq. 3)\nAudio Dense(1) CCCLaro\nFeature\n(1768) L S T M L S T M L S T M CCCLval\n(2 5 6 ) (1 2 8 ) ( 6 4 ) Dense(1) CCCLtot\nVisual #1 #2 #3\nF e a t u r e CCCLlik\n( 1 7 6 8 ) Dense(1)\nFig.1.ThearchitectureofDeepNeuralNetwork(DNN)with\na multitask learning approach for minimizing loss function\nfromthreedenselayers. Thenumberinsidethebracketrep-\nresentsunits.\nsionwithpredictedemotiondegree. TheCCCisformulated\n2ρ σ σ\nCCC = xy x y , (1)",
          "stage 1 stage 2 stage 3\nunimodal\nResult #1 Result #1 Result #1 Result #1\nSVR\nResult #2 Result #2 SVR Result #2 Result #2\nbimodal/ SVR\nearly fusion Result #3 Result #3 Result #3\nlate fusion\nResult #4 Result #4\nlate fusion Result #5\nlate fusion\nFig.2. TheflowofmultistageSVRtocombineearlyandlate\nfusionresults.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Audio and visual feature sets evaluated in this re-",
      "data": [
        {
          "Column_1": "Dense(1)",
          "CCCLval": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Three dimensions of emotion",
      "authors": [
        "Russel"
      ],
      "year": "1980",
      "venue": "J Pers Soc Psychol"
    },
    {
      "citation_id": "3",
      "title": "The world of emotions is not two-dimensional",
      "authors": [
        "Johnny Rj Fontaine",
        "Klaus Scherer",
        "Etienne Roesch",
        "Phoebe Ellsworth"
      ],
      "year": "2007",
      "venue": "Psychological science"
    },
    {
      "citation_id": "4",
      "title": "AVEC 2019 workshop and challenge: State-of-mind, depression with AI, and crosscultural affect recognition",
      "authors": [
        "Fabien Ringeval"
      ],
      "year": "2019",
      "venue": "Proc. of the 2019 on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "5",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "Jointly predicting arousal, valence and dominance with multi-task learning"
    },
    {
      "citation_id": "6",
      "title": "Multimodal multi-task learning for dimensional and continuous emotion recognition",
      "authors": [
        "Shizhe Chen",
        "Qin Jin",
        "Jinming Zhao",
        "Shuai Wang"
      ],
      "year": "2017",
      "venue": "Proc. of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "7",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "8",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "9",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor Fusion Network for Multimodal Sentiment Analysis"
    },
    {
      "citation_id": "10",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "Majumder",
        "Hazarika",
        "E Gelbukh",
        "Cambria",
        "Poria"
      ],
      "year": "2018",
      "venue": "Multimodal sentiment analysis using hierarchical fusion with context modeling"
    },
    {
      "citation_id": "11",
      "title": "Multi-modal Multicultural Dimensional Continues Emotion Recognition in Dyadic Interactions",
      "authors": [
        "Jinming Zhao",
        "Shizhe Chen"
      ],
      "year": "2018",
      "venue": "Multi-modal Multicultural Dimensional Continues Emotion Recognition in Dyadic Interactions"
    },
    {
      "citation_id": "12",
      "title": "Speech Emotion Recognition Using Speech Feature and Word Embedding",
      "authors": [
        "Kiyoaki Bagus Tris Atmaja",
        "Masato Shirai",
        "Akagi"
      ],
      "year": "2019",
      "venue": "Speech Emotion Recognition Using Speech Feature and Word Embedding"
    },
    {
      "citation_id": "13",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Bjorn Schuller",
        "Kam Star"
      ],
      "year": "2019",
      "venue": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "arxiv": "arXiv:1901.02839"
    },
    {
      "citation_id": "14",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "Florian Eyben"
      ],
      "year": "2016",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "15",
      "title": "Recent developments in openSMILE, the munich open-source multimedia feature extractor",
      "authors": [
        "Florian Eyben",
        "Felix Weninger",
        "Florian Gross",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia -MM '13"
    },
    {
      "citation_id": "16",
      "title": "openXBOW -Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit",
      "authors": [
        "Maximilian Schmitt",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "17",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "Shahin Amiriparian"
      ],
      "year": "2017",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "18",
      "title": "OpenFace 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "19",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "20",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE conf. on comput. vision and pattern recognit"
    },
    {
      "citation_id": "21",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I-Kuei Lawrence",
        "Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "22",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "F Pedregosa"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "23",
      "title": "Keras",
      "authors": [
        "Chollet Franc"
      ],
      "year": "2015",
      "venue": "Keras"
    }
  ]
}