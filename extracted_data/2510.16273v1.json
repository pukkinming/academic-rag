{
  "paper_id": "2510.16273v1",
  "title": "Musetok: Symbolic Music Tokenization For Generation And Semantic Understanding",
  "published": "2025-10-18T00:04:48Z",
  "authors": [
    "Jingyue Huang",
    "Zachary Novack",
    "Phillip Long",
    "Yupeng Hou",
    "Ke Chen",
    "Taylor Berg-Kirkpatrick",
    "Julian McAuley"
  ],
  "keywords": [
    "Representation Learning",
    "Music Tokenization",
    "Symbolic Music Generation",
    "Music Understanding"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Discrete representation learning has shown promising results across various domains, including generation and understanding in image, speech and language. Inspired by these advances, we propose Muse-Tok, a tokenization method for symbolic music, and investigate its effectiveness in both music generation and understanding tasks. MuseTok employs the residual vector quantized-variational autoencoder (RQ-VAE) on bar-wise music segments within a Transformerbased encoder-decoder framework, producing music codes that achieve high-fidelity music reconstruction and accurate understanding of music theory. For comprehensive evaluation, we apply Muse-Tok to music generation and semantic understanding tasks, including melody extraction, chord recognition, and emotion recognition. Models incorporating MuseTok outperform previous representation learning baselines in semantic understanding while maintaining comparable performance in content generation. Furthermore, qualitative analyses on MuseTok codes, using ground-truth categories and synthetic datasets, reveal that MuseTok effectively captures underlying musical concepts from large music collections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Discrete representation learning aims to train models to represent data within a finite set of discrete codes  [1] . It has proven effective across diverse generative tasks, including image generation  [1] [2] [3] , neural speech codec  [4] , generative retrieval for recommender systems  [5]  and signal-level music generation  [6, 7] . In music information retrieval (MIR), discrete representations have also been applied to genre classification  [8]  and melody transcription  [9] . Such methods span a wide range of compression bottlenecks, from lightly compressed discrete codes for improved modeling  [7] , to highly compressed codes capturing deep semantic information  [10] . However, work in discrete representation learning currently lags in the symbolic music domain. While some limited previous research has explored the use of discrete representations for classification tasks  [11, 12]  or controllable generation  [13] [14] [15] , such work only focused on the specific application rather than on general representations for diverse tasks, with limited attention to how one should learn semantic embeddings, nor to the quality of such representations.\n\nThus, we introduce MuseTok, the first tokenization method for general symbolic music representations that can support multiple applications, including symbolic music generation and semantic music understanding in multiple perspectives. We leverage an encoderdecoder architecture with residual quantization  [3]  to learn bar-wise music residual codes through reconstruction, on top of music sequences derived by REMI+  [15] . Analysis of code usage and simi-larities demonstrates its effectiveness in capturing music theoretical concepts, such as textures and musical intervals.\n\nRegarding the applications, for music generation, we employ a Transformer decoder to predict MuseTok codes, then pass codes to another Transformer decoder to generate REMI+ events. For semantic music understanding, three classification tasks are considered to assess the note-level, bar-level and song-level music semantics embedded in the codes. We adopt public-domain symbolic music data for model training, including a large-scale dataset PDMX  [16, 17]  and several small datasets  [18] [19] [20] [21] [22] [23]  spanning diverse genres, with a main focus on piano pieces to explore discrete representation learning in a single-instrument setting. Our contributions are three-fold: • We propose MuseTok, the first discrete representation learning framework of symbolic music for general purpose, applicable to both generation and understanding tasks. • MuseTok achieves comparable performance on symbolic music generation and superior performance on two of three classification tasks to previous baselines, demonstrating its effectiveness on content generation and semantic understanding. • We provide analysis on how MuseTok learns underlying musical concepts, such as key, interval, time signature, and texture. We present generation samples and more details about datasets and experiments on the website  1  . Code implementation and checkpoints are open sourced 2  .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Music Representation Learning. The success of representation learning methods  [1] [2] [3] [4] [5] [6] [7] [24] [25] [26]  has inspired exploration in the symbolic music domain. For music understanding, BERT-like models have been applied for classification tasks  [11, 12] , while contrastive methods integrate music with language  [27] . For generation, VAEbased models disentangle latent variables to encode attributes like chord and texture, enabling controllable generation  [14, 28]  and style transfer  [13, 15] . Unlike prior work targeting specific tasks, this paper focuses on general symbolic music representations, exploring their broad potential in music downstream tasks and quality. Symbolic Music Encoding. Various encoding formats have been proposed for symbolic music generation. MIDI-Message  [29, 30]  and REMI  [31]  encode MIDI data as sequence of events like note, beat and time shift. Later work introduced representations for compound attributes  [19] , multiple instrument tracks  [15, 32, 33] , expressive performance  [34, 35]  and emotion control  [20, 36] . This paper investigates discrete music representation learning on the bar-level segments, introducing a new tokenization for music generation and understanding.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Music Tokenization",
      "text": "As illustrated in Fig.  1  (left), we adopt the idea of RQ-VAE  [3]  to construct an encoder-decoder architecture with residual quantization (RQ) blocks to learn discrete representations of symbolic music.\n\nEncoder. Symbolic music input is converted into a REMI+  [15]  sequence X = {X1, • • • , XB} over B bars, where X b contains all REMI+ events within the b-th bar. A Transformer encoder Pϵ processes each bar to produce latent embeddings z1, • • • , zB.\n\nResidual Quantization. Residual quantization blocks RQ discretize each z b into embeddings r and corresponding codes c (indices) from codebooks\n\nwhere D is the number of codebooks or quantization depth, (c d b ,r d b ) is the retrieved code (index) and corresponding embedding in the\n\nso that their embeddings are nearest to the residuals:\n\nTo capture different granularities of music contents, the codes and embeddings in D codebooks are not shared.\n\nDecoder. After obtaining all aggregated embeddings {r b =Σ D d=1 r d b |b = 1, ..., B} from the RQ module, a Transformer decoder P δ decodes these embeddings to predict the music sequence X in an autoregressive mode. The reconstruction objective is:\n\nwhere xt denotes the t-th event in X, as\n\nand bar(t) is the bar index where the t-th event lies.\n\nCodebook Utility. To better utilize the codebook during training, we adopt SimVQ  [37]  and rotation trick  [38]  to improve codebook utility and reconstruction quality, with the commitment objective as\n\nwhere sg is the stop-gradient operator, W d is the linear transformation per codebook from SimVQ. The final training objective is the combination of two objectives: L = Lrecon + Lcommit. Codebooks are updated by the exponential moving average  [1] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Music Generation",
      "text": "As shown in Fig.  1",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Music Understanding",
      "text": "We employ three classification tasks to assess semantic understanding of MuseTok at the note, bar and song levels in Fig.  1",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "(Right).",
      "text": "Melody Extraction. The symbolic-domain melody extraction aims to identify melody notes from single-track polyphonic music  [11] . A classifier is trained to assign each pitch event xt of X to one of three classes, vocal melody, instrumental melody or accompaniment, using the code embedding r b (b = bar(t)) as a condition to provide note-level semantic context, as in the MuseTok decoding process.\n\nChord Recognition. We introduce a symbolic-domain chord recognition task that extracts chord progressions from single-track polyphonic music to evaluate the harmony information captured by MuseTok. Given the code embedding r b for a bar, a classifier predicts a chord label for each beat from a set of predefined categories.\n\nEmotion Recognition. Emotion recognition classifies a song into one of four categories defined by high/low positiveness and high-/low activation  [39] , evaluating the song-level semantic capability of MuseTok. For a given song, a classifier is trained to take the code embeddings r1, • • • , rB as input and predicts its emotion label.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets And Pre-Processing",
      "text": "We collect piano pieces from the large-scale PDMX  [16]  and six small datasets covering diverse genres: POP909  [18] , EMOPIA  [20] , Pop1k7  [19] , Hymnal  [22] , Multipianomide  [21]  and Ragtime  [23] . Pieces with time signature changes are segmented into clips with a consistent time signature and at least 8 bars. Since most samples maintain constant tempo and velocity, these performance-related attributes are removed to focus on structural and harmonic aspects. To improve structural consistency across datasets, we align note onsets and durations to valid sheet music positions. After pre-processing and REMI+  [15]  encoding, the resulting 195,187 sequences (83.7% monophonic, 13.1% chorale and 3.2% polyphonic) are randomly split into training, validation and test sets at an 8:1:1 ratio, yielding a vocabulary of 140 music events. Evaluation Metrics. We evaluate reconstruction quality and codebook utility using three metrics. Perplexity (PPL) measures how well a model predicts a sequence of music events, defined as inversely proportional to the log-probability of the test split. Accuracy (Acc) is computed as 1 -D(X, X ′ )/|X|, where D(X, X ′ ) refers to the edit distance between the reconstructed sequence X ′ and the original X. Both metrics are evaluated on three texture groups: monophonic, chorale, and polyphonic, reflecting increasing musical complexity. Codebook utility (Util) measures the fraction of codes used at least once when encoding the test set, averaged across quantization layers.\n\nResults. From Table  1 , most models exhibit highest perplexity and lowest reconstruction accuracy on polyphonic pieces, due to their complex textures and chord progressions. Incorporating diverse datasets during training improves reconstruction on PDMX (mostly monophonic and chorale pieces) compared to training on PDMX alone, validating the effectiveness of balanced sampling and preprocessing in aligning dataset distributions. Among MuseTok-Small variants, combining SimVQ with rotation tricks yields the highest codebook utility and best reconstruction quality. MuseTok-Large further approaches or surpasses the VAE upper bound, particularly improving on polyphonic pieces over MuseTok-Small. Based on these results, we select MuseTok-Large as the tokenization model for generation and understanding tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Music Generation",
      "text": "Model Settings. The first-stage generator is a 12-layer, 16-head Transformer with 1024 hidden dim., totaling 152M parameters, trained with sequence length 256 using the same hyperparameters as tokenization, converging in ∼200k steps over 4 days. The second stage adopts the trained tokenization decoder. Datasets are augmented by offline key transposition (±6). Unbalanced pieces are resampled during training. During inference, nucleus sampling  [40]  is applied (τ =1.1, p=0.9), followed by top-k downsampling (k=30).\n\nBaselines. We compare two baselines of standard symbolic music encoding methods, a Transformer decoder trained on REMI+ sequence with the same datasets and model size as ours (REMI)  [31] , and Anticipatory Music Transformer (AMT)  [41]  using provided music-small-100k checkpoint, on a music continuation task, which allows reliable assessments through relative comparison. VAE-based models like MuseMorphose  [13]  and FIGARO  [15]  are not compared as they require reference music for generation.\n\nEvaluation Metrics. We evaluate continuation results using two objective metrics and a subjective listening test. To quantitatively evaluate the similarity to the primers, we adopt bar-wise chroma similarity sim chr and grooving similarity simgrv  [13] , measuring tonal closeness via cosine similarity of chroma vectors  [42]  and rhythmic resemblance via grooving vectors  [43] . Sequence-level similarity is computed by averaging the highest similarity scores between each generated bar and the primer bars.\n\nIn the listening test, based on 4-bar primers, participants rate generated continuations on a 5-point Likert scale for four aspects: Pitch (Pit.), Structure (Str.), Harmony (Har.), and Development (Dev.). We collected 24 responses from participants spanning a wide spectrum of musical expertise, each evaluating 8 groups of random samples, yielding 192 ratings per model per aspect.\n\nResults. As shown in Table  2 , MuseTok outperforms both baselines on objective metrics, demonstrating effective harmonic and rhythmic continuation. On subjective metrics, it lags behind REMI and AMT on Pitch, producing more out-of-key notes that also affect Harmony perception. However, it performs comparably on Structure and Development, matching REMI and surpassing AMT, highlighting its potential for developing musical ideas through code generation.\n\nBeyond these metrics, our two-stage generation model is more robust with long-context primers. Encoding 16-bar music with REMI+ produces ∼ 800 events, whereas our code sequence remains fixed at 256 codes when D=16, supporting long-term generation. However, this fixed depth can introduce noise for simpler primers. From Table  1 , monophonic pieces reconstruct well with only 8 codes, with extra codes may act as a bias during generation. These results highlight the need for adaptive quantization and generation strategies for varying musical complexity.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Music Understanding",
      "text": "Model Settings and Baselines. The melody extraction task is evaluated on POP909  [18]  using a 3-layer, 4-head Transformer with 128 hidden dim. as classifier, compared with Bi-LSTM (RNN)  [44]  and MIDI-BERT  [11]  trained on REMI  [31]  sequences. The chord recognition task is evaluated on POP909 with 133 labels  (11   EMOPIA  [20]  with a 2-layer MLP with 256 hidden dim., compared against RNN, MIDI-BERT  [11]  and MusicBERT  [12] . For this task, MuseTok is retrained with velocity included in the REMI+ encoding.\n\nResults. Classification accuracies are reported in Table  3 . Our model outperforms all baselines on emotion recognition, demonstrating the ability of MuseTok to capture song-level semantic information. The learned codes also excel at modeling harmony in the challenging 133-class chord recognition task. Lower performance on melody extraction, along with out-of-key pitch generation above, highlights the need for improved melody modeling of tokenization.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "How Musetok Learns Music?",
      "text": "To explore the musical concepts learned by MuseTok, we conduct two case studies on MuseTok-Small: code usage frequencies across music groups and code embedding similarities on synthetic datasets.\n\nCode Usage Frequency. Embedding Similarity. This study examines how code embeddings change across codebooks when applying pitch shifting (key transposition) on music samples. Fig.  3  illustrates the cosine similarity of code embeddings (y-axis) between original and transposed samples across all 8 codebooks, where transposed samples are generated by shifting all notes from -6 to 6 semitones (x-axis). Embeddings in the first codebook (d=1) maintain over 70% similarity across transpositions, while deeper codebooks gradually diverge. This indicates that (1) transposition-invariant attributes, such as rhythmic information (onset, duration) and relative melodic contour, are mainly processed in earlier codebooks, and (2) absolute pitch information is further processed in deeper codebooks. Although attributes are not fully disentangled, MuseTok demonstrates an unsupervised ability to separate musical concepts across codebooks via data-driven learning.\n\nAnother observation relates to musical intervals. Across all codebooks, the highest similarity scores (excluding zero-shift) occur at ±4 (major third), followed by ±5 (perfect fourth) and ±3 (minor third), while ±6 (augmented fourth) yield the lowest. This suggests that MuseTok captures interval concepts and their relative prevalence in music (major third, perfect forth, and minor third are commonly-used development), as well as interval symmetry, shown by consistent rankings for ascending and descending shifts. These observations suggest that MuseTok effectively captures fundamental musical concepts, including rhythm, texture, and interval, even without explicit supervision from musical annotations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduce MuseTok, a discrete representation learning framework for symbolic music. RQ-VAE is applied on bar-wise music segments to learn music codes with high-fidelity reconstruction capability. We investigate the quality of learned codes through symbolic music generation and classification tasks in multiple perspectives, showing its effectiveness on both content generation and semantic understanding. Further qualitative analyses reveal the underlying musical concepts learned by MuseTok. In the future, we wish to focus on adaptive tokenization methods, leading to better music generation performance across all music groups.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of MuseTok (left) and its downstream generation (middle) and understanding (right) tasks.",
      "page": 2
    },
    {
      "caption": "Figure 1: (left), we adopt the idea of RQ-VAE [3] to",
      "page": 2
    },
    {
      "caption": "Figure 1: (middle), with the trained tokenization model,",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) and (b) present the top-50 most",
      "page": 4
    },
    {
      "caption": "Figure 2: (c) and (d). Unlike textures, MuseTok almost omits the time signa-",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates the cosine similarity of",
      "page": 4
    },
    {
      "caption": "Figure 2: Top-50 used codes across three texture groups, or across six",
      "page": 4
    },
    {
      "caption": "Figure 3: The cosine similarity of code embeddings (y-axis) between",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , monophonic pieces reconstruct well with only 8",
      "data": [
        {
          "1.123±0.007 1.086±0.007 1.159±0.021": "1.093±0.005 1.169±0.021 1.498±0.053\n1.151±0.008 1.268±0.027 1.667±0.059\n1.129±0.006 1.242±0.025 1.620±0.057\n1.106±0.005 1.174±0.021 -\n1.091±0.005 1.081±0.010 1.217±0.030",
          "96.70 83.59 82.21": "97.76 80.85 62.92\n80.55 64.04 54.59\n85.87 66.19 56.15\n97.91 78.11 -\n99.58 93.71 82.68"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Neural discrete representation learning",
      "authors": [
        "Aäron Van Den Oord",
        "Oriol Vinyals",
        "Koray Kavukcuoglu"
      ],
      "year": "2017",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "3",
      "title": "Generating diverse high-fidelity images with VQ-VAE-2",
      "authors": [
        "Ali Razavi"
      ],
      "year": "2019",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "4",
      "title": "Autoregressive image generation using residual quantization",
      "authors": [
        "Doyup Lee",
        "Chiheon Kim",
        "Saehoon Kim",
        "Minsu Cho",
        "Wook-Shin Han"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "5",
      "title": "Soundstream: An end-to-end neural audio codec",
      "authors": [
        "Neil Zeghidour",
        "Alejandro Luebs",
        "Ahmed Omran",
        "Jan Skoglund",
        "Marco Tagliasacchi"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "6",
      "title": "Recommender systems with generative retrieval",
      "authors": [
        "Shashank Rajput",
        "Nikhil Mehta",
        "Anima Singh",
        "Raghunandan Hulikal Keshavan",
        "Trung Vu",
        "Lukasz Heldt",
        "Lichan Hong",
        "Yi Tay",
        "Q Vinh",
        "Jonah Tran",
        "Maciej Samost",
        "Ed Kula",
        "Mahesh Chi",
        "Sathiamoorthy"
      ],
      "year": "2023",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "7",
      "title": "Jukebox: A generative model for music",
      "authors": [
        "Prafulla Dhariwal",
        "Heewoo Jun",
        "Christine Payne",
        "Jong Kim",
        "Alec Radford",
        "Ilya Sutskever"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "8",
      "title": "Simple and controllable music generation",
      "authors": [
        "Jade Copet",
        "Felix Kreuk",
        "Itai Gat",
        "Tal Remez",
        "David Kant",
        "Gabriel Synnaeve",
        "Yossi Adi",
        "Alexandre Défossez"
      ],
      "year": "2023",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "9",
      "title": "Codified audio language modeling learns useful representations for music information retrieval",
      "authors": [
        "Rodrigo Castellon",
        "Chris Donahue",
        "Percy Liang"
      ],
      "year": "2021",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "10",
      "title": "Melody transcription via generative pre-training",
      "authors": [
        "Chris Donahue",
        "John Thickstun",
        "Percy Liang"
      ],
      "year": "2022",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "11",
      "title": "Musiclm: Generating music from text",
      "authors": [
        "Andrea Agostinelli",
        "I Timo",
        "Zalán Denk",
        "Jesse Borsos",
        "Mauro Engel",
        "Antoine Verzetti",
        "Qingqing Caillon",
        "Aren Huang",
        "Adam Jansen",
        "Marco Roberts",
        "Tagliasacchi"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "12",
      "title": "Midibert-piano: Large-scale pre-training for symbolic music understanding",
      "authors": [
        "Yi-Hui Chou",
        "I-Chun Chen",
        "Chin-Jui Chang",
        "Joann Ching",
        "Yi-Hsuan Yang"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "13",
      "title": "Musicbert: Symbolic music understanding with large-scale pretraining",
      "authors": [
        "Mingliang Zeng",
        "Xu Tan",
        "Rui Wang",
        "Zeqian Ju",
        "Tao Qin",
        "Tie-Yan Liu"
      ],
      "year": "2021",
      "venue": "Findings of ACL"
    },
    {
      "citation_id": "14",
      "title": "MuseMorphose: Full-song and fine-grained piano music style transfer with one Transformer VAE",
      "authors": [
        "Shih-Lun Wu",
        "Yi-Hsuan Yang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "15",
      "title": "Learning interpretable representation for controllable polyphonic music generation",
      "authors": [
        "Ziyu Wang",
        "Dingsu Wang",
        "Yixiao Zhang",
        "Gus Xia"
      ],
      "year": "2020",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "16",
      "title": "FIGARO: Generating symbolic music with fine-grained artistic control",
      "authors": [
        "Luca Dimitri Von Rütte",
        "Yannic Biggio",
        "Thomas Kilcher",
        "Hofmann"
      ],
      "year": "2023",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "17",
      "title": "PDMX: A large-scale public domain musicxml dataset for symbolic music processing",
      "authors": [
        "Phillip Long",
        "Zachary Novack",
        "Taylor Berg-Kirkpatrick",
        "Julian Mcauley"
      ],
      "year": "2025",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "18",
      "title": "Generating symbolic music from natural language prompts using an llm-enhanced dataset",
      "authors": [
        "Weihan Xu",
        "Julian Mcauley",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov",
        "Hao-Wen Dong"
      ],
      "year": "2024",
      "venue": "CoRR"
    },
    {
      "citation_id": "19",
      "title": "A pop-song dataset for music arrangement generation",
      "authors": [
        "Ziyu Wang",
        "Ke Chen",
        "Junyan Jiang",
        "Yiyi Zhang",
        "Maoran Xu",
        "Shuqi Dai",
        "Gus Xia"
      ],
      "year": "2020",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "20",
      "title": "Compound Word Transformer: Learning to compose full-song music over dynamic directed hypergraphs",
      "authors": [
        "Wen-Yi Hsiao",
        "Jen-Yu Liu",
        "Yin-Cheng Yeh",
        "Yi-Hsuan Yang"
      ],
      "year": "2021",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "21",
      "title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "Hsiao-Tzu Hung",
        "Joann Ching",
        "Seungheon Doh",
        "Nabin Kim",
        "Juhan Nam",
        "Yi-Hsuan Yang"
      ],
      "year": "2021",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "22",
      "title": "Classical piano midi page",
      "venue": "Classical piano midi page"
    },
    {
      "citation_id": "23",
      "title": "Humnal.net",
      "venue": "Humnal.net"
    },
    {
      "citation_id": "24",
      "title": "Rag's rag",
      "venue": "Rag's rag"
    },
    {
      "citation_id": "25",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "26",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "27",
      "title": "Large-scale contrastive languageaudio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Yusong Wu",
        "Ke Chen",
        "Tianyu Zhang",
        "Yuchen Hui",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "28",
      "title": "Clamp: Contrastive language-music pre-training for cross-modal symbolic music information retrieval",
      "authors": [
        "Shangda Wu",
        "Dingyao Yu",
        "Xu Tan",
        "Maosong Sun"
      ],
      "year": "2023",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "29",
      "title": "PIANOTREE VAE: structured representation learning for polyphonic music",
      "authors": [
        "Ziyu Wang",
        "Yiyi Zhang",
        "Yixiao Zhang",
        "Junyan Jiang",
        "Ruihan Yang",
        "Gus Xia",
        "Junbo Zhao"
      ],
      "year": "2020",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "30",
      "title": "Music Transformer: Generating music with long-term structure",
      "authors": [
        "Cheng-Zhi Anna Huang",
        "Ashish Vaswani",
        "Jakob Uszkoreit",
        "Ian Simon",
        "Curtis Hawthorne",
        "Noam Shazeer",
        "Andrew Dai",
        "Matthew Hoffman",
        "Monica Dinculescu",
        "Douglas Eck"
      ],
      "year": "2019",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "31",
      "title": "Performance rnn: Generating music with expressive timing and dynamics",
      "authors": [
        "Ian Simon",
        "Sageev Oore"
      ],
      "year": "2017",
      "venue": "Performance rnn: Generating music with expressive timing and dynamics"
    },
    {
      "citation_id": "32",
      "title": "Pop Music Transformer: Beatbased modeling and generation of expressive pop piano compositions",
      "authors": [
        "Yu-Siang Huang",
        "Yi-Hsuan Yang"
      ],
      "year": "2020",
      "venue": "Proc. ACM Multimed"
    },
    {
      "citation_id": "33",
      "title": "Multitrack Music Transformer",
      "authors": [
        "Hao-Wen",
        "Ke Dong",
        "Shlomo Chen",
        "Julian Dubnov",
        "Taylor Mcauley",
        "Berg-Kirkpatrick"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "34",
      "title": "MMM : Exploring conditional multi-track music generation with the transformer",
      "authors": [
        "Jeffrey Ens",
        "Philippe Pasquier"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "35",
      "title": "The piano inpainting application",
      "authors": [
        "Gaëtan Hadjeres",
        "Léopold Crestel"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "36",
      "title": "Pertok: Expressive encoding and modeling of symbolic musical ideas and variations",
      "authors": [
        "Julian Lenz",
        "Anirudh Mani"
      ],
      "year": "2024",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "37",
      "title": "Emotion-driven piano music generation via two-stage disentanglement and functional representation",
      "authors": [
        "Jingyue Huang",
        "Ke Chen",
        "Yi-Hsuan Yang"
      ],
      "year": "2024",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "38",
      "title": "Addressing representation collapse in vector quantized models with one linear layer",
      "authors": [
        "Yongxin Zhu",
        "Bocheng Li",
        "Yifei Xin",
        "Linli Xu"
      ],
      "year": "2024",
      "venue": "CoRR"
    },
    {
      "citation_id": "39",
      "title": "Restructuring vector quantization with the rotation trick",
      "authors": [
        "Christopher Fifty",
        "Ronald Junkins",
        "Dennis Duan",
        "Aniketh Iger",
        "Jerry Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Ré"
      ],
      "year": "2024",
      "venue": "CoRR"
    },
    {
      "citation_id": "40",
      "title": "A circumplex model of affect",
      "authors": [
        "James Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "41",
      "title": "The curious case of neural text degeneration",
      "authors": [
        "Ari Holtzman",
        "Jan Buys",
        "Maxwell Forbes",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "42",
      "title": "Anticipatory music transformer",
      "authors": [
        "John Thickstun",
        "David Leo",
        "Wright Hall",
        "Chris Donahue",
        "Percy Liang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Mach. Learn. Res"
    },
    {
      "citation_id": "43",
      "title": "Realtime chord recognition of musical sound: a system using common lisp music",
      "authors": [
        "Takuya Fujishima"
      ],
      "year": "1999",
      "venue": "Proc. ICMC"
    },
    {
      "citation_id": "44",
      "title": "Towards characterisation of music via rhythmic patterns",
      "authors": [
        "Simon Dixon",
        "Fabien Gouyon",
        "Gerhard Widmer"
      ],
      "year": "2004",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "45",
      "title": "A structured self-attentive sentence embedding",
      "authors": [
        "Zhouhan Lin",
        "Minwei Feng",
        "Cícero Nogueira Dos Santos",
        "Mo Yu",
        "Bing Xiang",
        "Bowen Zhou",
        "Yoshua Bengio"
      ],
      "year": "2017",
      "venue": "Proc. ICLR"
    }
  ]
}