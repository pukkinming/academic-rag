{
  "paper_id": "2403.14083v1",
  "title": "Emodarts: Joint Optimisation Of Cnn & Sequential Neural Network Architectures For Superior Speech Emotion Recognition",
  "published": "2024-03-21T02:26:30Z",
  "authors": [
    "Thejan Rajapakshe",
    "Rajib Rana",
    "Sara Khalifa",
    "Berrak Sisman",
    "Bjorn W. Schuller",
    "Carlos Busso"
  ],
  "keywords": [
    "speech emotion recognition",
    "neural architecture search",
    "deep learning",
    "DARTS"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is crucial for enabling computers to understand the emotions conveyed in human communication. With recent advancements in Deep Learning (DL), the performance of SER models has significantly improved. However, designing an optimal DL architecture requires specialised knowledge and experimental assessments. Fortunately, Neural Architecture Search (NAS) provides a potential solution for automatically determining the best DL model. The Differentiable Architecture Search (DARTS) is a particularly efficient method for discovering optimal models. This study presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network (SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature supports the selection of CNN and LSTM coupling to improve performance. While DARTS has previously been used to choose CNN and LSTM operations independently, our technique adds a novel mechanism for selecting CNN and SeqNN operations in conjunction using DARTS. Unlike earlier work, we do not impose limits on the layer order of the CNN. Instead, we let DARTS choose the best layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms conventionally designed CNN-LSTM models and surpasses the best-reported SER results achieved through DARTS on CNN-LSTM by evaluating our approach on the IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "This section delves into the existing literature on using DARTS and NAS for SER. Notably, our exploration reveals a limited number of papers in this space. We therefore extend our review to encompass relevant papers in related fields to provide a comprehensive perspective. For completeness, we also include studies employing CNNs, LSTM networks, and their joint utilisation for SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Emotion Recognition Using Cnn And Lstm",
      "text": "One of the earliest uses of CNN networks in SER is reported by  Zheng et al. in 2015 [14] . The authors introduced a spectrum generated from an audio signal to a CNN network and output the recognised emotion. The authors report that they can surpass the SVM-based classification performance and reach 40% classification accuracy for a five-class classification using the IEMOCAP dataset. The earliest work combining CNN and LSTM for SER is by  Trigeorgis et al. in 2016 [15] . The authors show an impressive improvement by a fully self-learnt representation over traditional expert-crafted features on dimensional emotion recognition.\n\nZhaoa et al.  [1]  show that using CNN and LSTM networks combined in the same SER model produces better results than using only CNN. Using the IEMOCAP dataset, they obtained a speaker-independent accuracy of 52% by using a log-Mel spectrogram as the input feature. Their SER approach utilises an LSTM layer to learn contextual dependencies in local features, while a CNN-based layer learns the local features.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Application Of Nas And Darts In Ser And Related Fields",
      "text": "The first paper suggesting NAS in SER was by  Zhang et al. in 2018 [16] . The authors employ a controller network that shapes the architecture by the number of layers and nodes per layer and the hyperparameter activation function of a child network by reinforcement learning. They show an improvement over human-designed architectures and random searches of these.\n\nZoph and Le  [17]  use reinforcement learning to optimise an RNN network that develops model architectures to maximise the resulting accuracy of the generated model. As a result, they develop outstanding models for the CIFAR-10 and Penn Treebank datasets. They were able to develop a convolutional network architecture for the CIFAR-10 dataset which has a 3.65 error rate and a recurrent network architecture for Penn Treebank with 62.4 perplexity.\n\nEven though NAS is primarily used to find optimised architecture for complex and large models, researchers have also studied the possibility of using NAS to design smaller deep neural network models. Liberis et al.  [18]  develop a NAS system called µNAS to design smaller neural architectures that can run on microcontroller units. They improve the top-1 accuracy by 4.8% in image classification problems while reducing the memory footprint up to 13 times. Similarly, Gong et al.  [19]  study the feasibility of using NAS for reducing deep learning models to deploy on resourceconstrained edge devices.\n\nTraditional NAS consumes much computational power and time to achieve the optimal model for a given problem. In 2018, Liu et al.  [20]  came up with a differentiable approach to solving the optimisation by continuous relaxation of the architecture representation. This approach is more compute efficient and high performing as the search space is not discrete and non-differentiable. They produce highperforming CNN and RNN architectures for tasks such as image recognition and language modelling within a fraction of the search cost of traditional NAS algorithms. DARTS\n\nhas been popular in the past three years with many studies carried on for extending and improving the algorithm  [21] ,  [22] ,  [11] ,  [23] ,  [24] ,  [25] . Wu et al.  [10]  proposed a uniform path dropout strategy to optimise candidate architecture. They use SER as their DARTS application and the IEMOCAP dataset to develop an SER model with an accuracy of 56.28% for a fourclass classification problem using discrete Fourier transform spectrograms extracted from audio as input. In their work, the authors specify layer order as two convolution layers at first, followed by a max-pooling layer, a convolution layer. They use DARTS to select the optimum parameters for each layer. We, on the other hand, do not specify the layer sequence and instead enable DARTS to select the ideal design with minimal interference.\n\nEmotionNAS is a two-branch NAS strategy introduced by Sun et al.  [9]  in 2023. The authors use DARTS to optimise their two models in two branches, the CNN model and RNN model, which use a spectrogram and a waveform as inputs, respectively. They obtained an unweighted accuracy of 72.1% from the combined model for the IEMOCAP dataset. They also report the performance of 63.2% in the spectrogram branch, which only uses a CNN component. The main difference between our approach and the study by Sun et al.  [9]  is that we use a SeqNN component coupled in series with the CNN layer as in Figure  2  while Sun et al.  [9]  use an RNN layer in parallel to the CNN layer in a different branch.\n\nWe conducted preliminary research to determine the feasibility of utilising DARTS for SER in a CNN-LSTM architecture, where we only optimised the CNN network using DARTS  [26] . This paper extends the idea of using DARTS in SER but with more relaxation in the SeqNN component by jointly optimising the whole architecture.\n\nIn recent years, the literature has highlighted the use of attention networks in SER, which has provided superior outcomes  [27] ,  [28] . We added an attention network component to the DARTS search scope to discover whether it improves performance. Zou et. al.  [29]  have introduced a concept called 'co-attention' where many separate inputs from multimodal inputs are fused by co-attention. They used three sets of features MFCC, spectrogram, and Wav2Vec2 features from the IEMOCAP dataset and obtained 72.70% accuracy. Liu et al.  [30]  have utilised an attention-based bi-directional LSTM followed by a CNN layer for a SER problem. They have achieved a significant performance of 66.27% for the IEMOCAP Dataset. Their idea of 'CNN -LSTM attention' paved the foundation for our model architecture.\n\nIn Table  1 , we briefly compare the existing studies with emoDARTS. The comparison clearly shows that, 1) While some studies employ NAS for SER, the utilisation of DARTS in SER is notably limited. 2) Singularly, one study has explored the concept of jointly optimising CNN and SeqNN using DARTS for SER, in which the researchers specified the layer order. However, in our study, we let DARTS determine the optimal network from a relaxed search scope which enables it to select any operation in search space at the optimum layer. 3) Most existing studies primarily focus on the IEMOCAP dataset. In contrast, our study uniquely incorporates three widely recognised SER datasets: IEMOCAP, MSP-IMPROV, and MSP-Podcast to demonstrate the generalisation power of emoDARTS.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emodarts Framework",
      "text": "The proposed 'emoDARTS' uses DARTS to improve SER using a CNN-SeqNN network, which was motivated by studies that showed increased SER performance when CNN and LSTM layers were combined  [5] ,  [7] ,  [8] ,  [30] . We represent our network as a multi-component DARTS network, with the input fed into a CNN component and the output from the CNN component fed into a SeqNN component, but all components are optimised jointly during the architecture search phase, delivering an optimal architecture (Figure  2 ). DARTS uses a differentiable approach to network optimisation. A computation cell is the DARTS algorithm's fundamental unit. It aims to optimise the cell so that the architecture can function to its maximum performance. A DARTS cell is described as a directed graph, with each node representing a feature (representation) and each edge representing an operation that can be performed to a representation. One unique feature of this network is that each node is connected to all of its previous nodes by an edge, as seen in Figure  3 (a) . If the output of the node j is x (j) and the operation 'o' on the edge connecting the nodes i and j is o (i,j) , x (j) can be obtained by the Equation  1 :\n\nIn the beginning, the candidate search space is generated by combining each node of the DARTS cell with all the candidate operations (with multiple links between nodes), as illustrated in Figure  3 (b) . Equation  1 incorporates a weight parameter α to identify the optimal edge (operation) connecting two nodes, i and j, from the candidate search space of all operations. Equation 2 describes how the node's output be represented.\n\nThen, the continuous relaxation of the search space updates the weights (α i,j ) of the edges. The final architecture can be obtained by selecting the operation between two nodes with the highest weight (o (i,j) * ) by using Equation  3 .\n\nThe searched discrete cell architecture is shown in Figure  3  (d).\n\nThe number of cells (C or N ) in a component is a parameter for the DARTS algorithm that specifies how many DARTS cells are stacked to form a component in the model. Each cell takes the last two cells' output as input. If the output from each cell t is y t and the function within the cell is f , then y t can be represented as;\n\nDARTS' CNN component has two types of CNN cells: 'normal' and 'reduction' cells. It sets the stride to one in normal cells and two in reduction cells, resulting in a downsampled output in the reduction cells. This downsampling allows the model to eliminate the duplication of intermediate characteristics, reducing complexity.\n\nWe decided to jointly optimise the CNN and SeqNN components rather than individually since it is important for downstream components (in this case, the CNN component) to understand the behaviour of upstream components (Se-qNN). Joint optimisation in a multiple-component network improves architecture search in various ways, including: 1. the back-propagation of loss minimisation flows through all the components in a single compute graph; and 2. it reduces the time required in the search phase by searching the architecture of the whole network at once.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Feature Selection",
      "text": "We use the widely used IEMOCAP  [31] , MSP-IMPROV  [32] , and MSP-Podcast  [33]  datasets for our experiments. Our study takes the improvised subset of IEMOCAP and the four categorical labels, happiness, sadness, anger, and neutral as classes from the datasets. We employ five-fold crossvalidation with at least one speaker out in our training and evaluations. At each fold, the training dataset is divided into two subsets, 'search', and 'training', by a 70/30 fraction. The 'search' set is used in the architecture search; the 'training' set is used in optimising the searched architecture, and the remaining testing dataset is used to infer and obtain the testing performance of the searched and optimised model. This way, we manage to split the dataset into three sets in each cross-validation session. The IEMOCAP dataset has five sessions with ten actors and two unique speakers in each. We use one session for the testing dataset and four sessions for the search and training datasets. Similarly, MSP-Improv comprises six sessions including twelve actors. We take one session in the testing dataset and the remaining five sessions in the search and training dataset. MSP-Podcast includes a speaker ID with each audio utterance, and we group the entire dataset by the speaker and divide it by the 70/30 rule.\n\nIn this research, we use Mel Frequency Cepstral Coefficients (MFCC) as input features to the model. MFCC has been used as the input feature in many SER studies in the literature  [34] ,  [35]  and has proven to obtain promising results. Some machine learning research uses the Mel Filter bank as an input feature when the algorithm is not vulnerable to strongly correlated data. We picked the MFCC for this study since the deep learning model is produced automatically and we do not want to infer the model's sensitivity to correlated input. We extract 128 MFCCs from each 8-second audio utterance from the dataset. If the audio utterance length is less than 8 seconds, we added padding with zeros while the lengthier utterances are truncated. The MFCC extraction from the Librosa python library  [36]  outputs a shape 128 × 512, downsampled with max pooling, to create a spectrogram of the shape 128 × 128.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baseline Models",
      "text": "We compare the performance of emoDARTS for SER with three models developed without DARTS (w/o DARTS) : 1) CNN, 2) CNN+ LSTM, and 3) CNN+LSTM with attention as baseline models. The CNN baseline model consists of a CNN layer (kernel size=2, stride=2, and padding=2) followed by a Max-Pooling layer (kernel size=2 and stride=2). Two dense layers then processes the output from the Max-Pooling layer after applying a dropout of 0.3. Finally, the last dense layer has four output units resembling the four emotion classes, and the model outputs the probability estimation of each emotion for a given input by a Softmax function.\n\nThe CNN+LSTM baseline model is built, including an additional bi-directional LSTM layer of 128 units after the Max-Pooling layer. An attention layer is added to the LSTM layer in the 'CNN+LSTM attention' baseline model. Figure  4  shows the architecture of the CNN+LSTM attention baseline model",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Darts Configuration",
      "text": "We divide the cell search space operations into the two separate parts CNN and SeqNN based on the components they apply. Table  2  lists the type of operations used in each component. The cell search space of the CNN component consists of pooling operations such as 3 × 3 max pooling (i = 3) and 3 × 3 average pooling (i = 3), convolutional operations such as 3 × 3 and 5 × 5 separable convolutions (i = 3, 5), 3 × 3 and 5 × 5 dilated convolution (i = 3, 5),  7 × 1 -1 × 7 factorised convolution (i = 7), identity connections, and no connections while the SeqNN component consists of operations such as RNN of layers 1 through 4 (j = 1, 2, 3, 4), RNN of layers 1 and 2 with attention (j = 1, 2), LSTM of layers 1 through 4 (j = 1, 2, 3, 4), LSTM of layers 1 and 2 with attention (j = 1, 2), identity connections and no connections. We use stochastic gradient descent with a learning rate from 0.025 to 0.001 using a cosine annealing schedule as the optimiser to optimise the weights of the operations. The search is run for 300 epochs.\n\nIn our experiments, we use four DARTS cells (C = 4) for the CNN component following the work of Liu et al.  [20]  and two DARTS cells (N = 2) for the SeqNN component. The intuition of using N = 2 for the SeqNN component is discussed in section 6.2. As defined in  [20] , we apply reduction cells at every 1 3 C th and 2 3 C th position of the layers in CNN component. We randomly initialise α values and the DARTS search algorithm optimises α values related to each operation. The output from the CNN component is flattened to a vector before passing to the SeqNN component to adjust the input dimension of the RNN and LSTM layers.\n\nOnce the search operation completes, it outputs the architecture of DARTS cells, which is called \"genome\". We create a deep learning model with the CNN component having four CNN cells and the SeqNN component having two SeqNN cells. This model is trained for 300 epochs with the training set of the datasets to minimise the loss.\n\nWe use the popular deep learning library PyTorch  [37]  for model development and training. The experiments are run on an NVIDIA A100 GPU with 40 GB of VRAM. We published the source code related to our research in a dedicated GitHub repository, allowing for smooth replication of our research findings 1 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation",
      "text": "We report the results using the Unweighted Accuracy (UA%), calculated by dividing the total of all classes' recall by their number. This is recognised to depict unbalanced data workloads intrinsic to SER accurately. We additionally provide the Weighted Accuracy (WA%) mainly to compare our results with relevant studies  [9] ,  [10] . Last but not least, we also report the number of parameters of the model as an indication of the model's complexity, calculated by adding all trainable parameters in the created model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cnn Only Model",
      "text": "We initially assess the performance of the CNN-only model generated by DARTS (CNN -DARTS) compared to our benchmark model, specifically CNN -w/o DARTS, using the IEMOCAP dataset. The results, detailed in Table  3 , reveal that the DARTS-generated CNN model outperforms the performance of the baseline SER model. Additionally, Table  3  illustrates the performance of the DARTS-generated model with eight cells (C = 8), showing a lower performance compared to its counterpart with C = 4. This decline in performance with an increased number of cells indicates a rise in the model's complexity, leading to overfitting and subsequent accuracy reduction.\n\nWe further examine the results from the CNN branch of Sun et al.'s EmotionNAS model  [9]  to highlight performance enhancements. For a direct and clear comparison of performance, we specifically utilise the 'Spectrogram Branch' of EmotionNAS, contrasting it with our 'CNN -DARTS' model. This focused comparison is chosen to ensure a fair evaluation since both models share a similar architecture. We see that the performance of our CNN -DARTS models surpasses the performance of the CNN branch of Emotion-NAS by at least 5%. It is worth noting that the 'whole model' of EmotionNAS has a different architecture, employing a branched structure, while emoDARTS utilises a stacked architecture.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emodarts Model",
      "text": "We analyse the performance of the CNN-SeqNN model generated by DARTS (emoDARTS) in contrast to the SER models optimised without DARTS (w/o DARTS) and visualise this in Figure  5  and Table  4 . The graph shows that the NAS-generated SER model performs better than the baseline SER model developed without DARTS for the three datasets.\n\nWe also compare the performance of the SER model generated by our approach with the most related studies, 'EmotionNAS' of Sun H. et al., and the 'CNN RNN att'  in the related literature.\n\nIt is further worthwhile to investigate the rationale for increased performance when compared to the results of Wu et al.'s 'CNN RNN att'  [10]  system. We suggest the improved performance is due to the relaxed candidate operations order rather than the pre-defined layer order. In Wu et al.'s study  [10] , for example, the initial layers are pre-defined to be convolutional layers. The DARTS algorithm must select the best convolutional layer from a pool of just CNN layers. In contrast, our technique allows DARTS to choose among many operations such as convolutions, pooling, and skip connections. Figure  6  shows one such use scenario, in which the DARTS searched architecture consists of pooling layers in the initial segments.\n\nFigure  6  shows a visualisation of the architecture for each type of cell (normal and reduction cell of the CNN component, and cell in the SeqNN component) searched by DARTS for the emoDARTS model. It is visible that DARTS has selected three LSTM based operations for the SeqNN component and only one of them contains attention. This shows that jointly optimising the emoDARTS model has enabled the DARTS framework to choose optimum operations rather than blindly choosing layers with 'attention' for all the operations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Restricting The Search Scope",
      "text": "We study the impact on the performance of the searched model by restricting the search scope for the SeqNN component. We divide the search scope into five segments namely 'LSTM Only', 'LSTM-Att. Only', 'RNN Only', and 'RNN-Att. Only'. Table  6  shows the DARTS operations allowed as the candidate operations in the SeqNN component during the search phase.\n\nTable  7  shows the performance and number of parameters of the searched model when the candidate search operations are restricted. Here, we study the effect on the performance of the searched architecture when the search algorithm was only given a restricted set of operations. For example, the 'LSTM Only' study only allowed to use operations from lstm 1, lstm 2, lstm 3, and lstm 4. We try to identify the most important types of genome operations that we can use in the search algorithm. This approach allows to use of only the important operations in the search scope and optimises the memory utilisation in the search phase.\n\nComparing the trials 'emoDARTS' and 'LSTM Only' in the IEMOCAP dataset, we can observe that even though the number of parameters has tripled in the 'LSTM Only' scenario, the performance (UA%) has not increased. This indicates that increasing the number of parameters just by increasing the complexity of the model does not tend to give better performance, but the model components should be compatible with each other.\n\nNotably, models using 'RNN Only' genomic operations achieve the second-highest accuracy despite having much fewer trainable parameters. Figure  8  depicts the cell architecture, which consists mostly of pooling layers and skip connection operations that do not have any training parameters and hence do not contribute to the total number of trainable parameters.\n\nWe provide in Table  7  results as well as a scatter plot for better visualisation in Figure  7 , where the mean UA% in the vertical axis, standard deviation of UA% in the horizontal axis and size of the markers indicates the number of parameters. The polts indicate which model gives better",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "Throughout this study, we encountered various challenges.\n\nIn this section, we report the key challenges and our strategies for overcoming them. The three primary challenges we faced were:\n\n1) Optimising the GPU Memory utilisation 2) Converging to a local minima 3) High Standard Deviation of the results",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Optimising The Gpu Memory Utilisation",
      "text": "The DARTS algorithm conceptualises the search problem as a network graph, establishing multiple edges between each node. The quantity of edges corresponds to the defined candidate operations. These operations encompass various possibilities, ranging from simple CNN, pooling, and RNN layers to intricate modules like an LSTM-attention module.\n\nIn the search phase, a super-neural network is constructed, resulting in multiple instances of neural network layers or modules within this overarching structure. Fig.  9 . Example graph of a DARTS cell which has four nodes and candidate operations are \"lstm 1\", \"lstm 2\", and \"lstm att 1\". The same edge colour denotes the same type of operation.\n\nFigure  9  shows an example of a graph inside a DARTS cell that has four nodes and the candidate operations are \"lstm 1\", \"lstm 2\", and \"lstm att 1\", where \"lstm 1\" is a single layer LSTM component, \"lstm 2\" is a double layer LSTM component, and \"lstm att 1\" is an attention induced single layer LSTM component. According to the example, a single DARTS cell should initiate 6×lstm 1 layers, 6×lstm 2 layers, and 6×lstm att 1 layers. If the search configuration has 4 cells, we have to initialise 4 instances of cells where all the weight and bias parameters have to be initialised in the computing device. This will increase the GPU memory utilisation.\n\nProviding a higher number of nodes in a cell, a higher number of cells, and expanding the array of candidate operations will increase the amount of GPU memory utilisation and eventually will exhaust the GPU memory capacity failing the search operation.\n\nTo optimise the GPU memory utilisation, we recommend conducting an assessment to determine the set of possible search operations and hyperparameters such as the number of layers, cells, and nodes inside the cell considering the GPU resources available.\n\nOn the other hand, based on the results of Table  7 , we should not be restricted only to a single type of network but rather should consist of a variety of network architectures. We selected the set of candidate operations indicated in Table 6 under 'emoDARTS' based on GPU resource availability and on the premise that all sorts of candidate operations should be available in the search space.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Converging To A Local Minima",
      "text": "Throughout the course of our experiments, we attempted various configurations for the number of cells and nodes. We observed that the SeqNN module converges to a local minimum when the number of cells and number of nodes is greater than 3. The output of the searched genome for the SeqNN module contained all \"skip connect\" which indicates identity operations are used instead of any RNN or LSTM operations. Figure  10  shows one such instance DARTS SeqNN genome.\n\nWe were able to address the challenge by reducing the complexity of the candidate search graph by reducing the number of cells and the number of nodes inside a cell. More research is, however, needed to manage a more complex search network.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "High Standard Deviation In The Results",
      "text": "An important observation derived from our results is the high standard deviation. This can be attributed to the dataset-splitting method we employed. Specifically, we adopt speaker-independent dataset splitting, where the training and validation sets are segregated based on the speaker. In this configuration, any audio utterance from a particular speaker in the validation set remains unseen by the model during training. Consequently, the DARTSoptimised model is not trained to handle the data distribution of the validation set. To tackle this challenge, potential solutions include dataset poisoning and enhancing the generalisation capabilities of the SER model by incorporating dropout layers.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "In conclusion, this paper introduced an innovative approach to enhancing speech emotion recognition (SER) using differentiable architecture search by DARTS. Our primary focus was on tailoring DARTS for a joint configuration of a Convolutional Neural Network (CNN) and a Sequential Neural Network, deviating from previous studies by allowing DARTS to autonomously determine the optimal layer order for the CNN within the DARTS cell without imposing constraints.\n\nA comprehensive evaluation was conducted, comparing our proposed method with baseline models developed without DARTS and various genome operations, including LSTM only, LSTM with attention only, RNN only, and RNN with attention only. The detailed assessments consistently demonstrate the superior performance of our proposed method. Contrasting with existing studies further validates the effectiveness and superiority of our approach, considering parameter size and accuracy as essential dimensions for comparison.\n\nNotably, our study extends beyond the confines of the commonly used IEMOCAP dataset, incorporating two additional datasets, MSP-IMPROV and MSP-Podcast. This extension showcases the superior performance of our proposed method across diverse datasets, affirming its generalisation capability.\n\nFurthermore, we shared valuable insights gained from our experiences, addressing challenges related to GPU exhaustion and converging to local minima. These insights serve as practical guidance for researchers, helping them navigate potential pitfalls and optimise the application of DARTS in SER.\n\nFuture efforts will need to deal with neural architecture for further modern architectures such as transformers and translating the made findings beyond the targeted field of application.",
      "page_start": 9,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed architecture of emoDARTS passes the input fea-",
      "page": 2
    },
    {
      "caption": "Figure 1: , with an attention network seamlessly",
      "page": 2
    },
    {
      "caption": "Figure 2: while Sun et",
      "page": 3
    },
    {
      "caption": "Figure 2: The emoDARTS architecture comprises input features processed",
      "page": 3
    },
    {
      "caption": "Figure 3: DARTS employs steps (a) to (d) to search cell architectures: (a) initialises the graph, (b) forms a search space, (c) updates edge weights,",
      "page": 4
    },
    {
      "caption": "Figure 3: (a). If the output of the node j is x(j) and",
      "page": 4
    },
    {
      "caption": "Figure 3: (b). Equation 1 incorporates a",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the architecture of the CNN+LSTM attention baseline",
      "page": 5
    },
    {
      "caption": "Figure 4: Visualisation of the CNN+LSTM attention baseline model. The",
      "page": 5
    },
    {
      "caption": "Figure 5: and Table 4. The graph shows that",
      "page": 6
    },
    {
      "caption": "Figure 5: Comparison of UA% between the datasets the NAS gener-",
      "page": 6
    },
    {
      "caption": "Figure 6: DARTS searched tth cell structure for the CNN Normal Cell (a), CNN Reduction Cell (b), and SeqNN cell (c) for the emoDARTS model.",
      "page": 7
    },
    {
      "caption": "Figure 6: shows one such use scenario, in which",
      "page": 7
    },
    {
      "caption": "Figure 6: shows a visualisation of the architecture for",
      "page": 7
    },
    {
      "caption": "Figure 8: depicts the cell ar-",
      "page": 7
    },
    {
      "caption": "Figure 7: , where the mean UA%",
      "page": 7
    },
    {
      "caption": "Figure 7: Visualisation of results for the studies restricting the search space for the three datasets: IEMOCAP, MSP-IMPROV, and MSP-Podcast.",
      "page": 8
    },
    {
      "caption": "Figure 8: DARTS searched cell structure for CNN and SeqNN cells when",
      "page": 8
    },
    {
      "caption": "Figure 9: Example graph of a DARTS cell which has four nodes and",
      "page": 9
    },
    {
      "caption": "Figure 9: shows an example of a graph inside a DARTS",
      "page": 9
    },
    {
      "caption": "Figure 10: shows one such instance",
      "page": 9
    },
    {
      "caption": "Figure 10: DARTS genome of the SeqNN module where the search algo-",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , we briefly compare the existing studies with",
      "data": [
        {
          "Paper": "",
          "Focus": "SER",
          "Dataset": "IEMOCAP"
        },
        {
          "Paper": "Zoph and Le 2016 [17]",
          "Focus": "✗",
          "Dataset": "✗"
        },
        {
          "Paper": "Zhang et al. 2018 [14]",
          "Focus": "✓",
          "Dataset": "✓"
        },
        {
          "Paper": "Liu et al. 2018 [20]",
          "Focus": "✗",
          "Dataset": "✗"
        },
        {
          "Paper": "Gong et al. 2019 [19]",
          "Focus": "✗",
          "Dataset": "✗"
        },
        {
          "Paper": "Liberis et al. 2020 [18]",
          "Focus": "✗",
          "Dataset": "✗"
        },
        {
          "Paper": "Wu et al. 2022 [10]",
          "Focus": "✓",
          "Dataset": "✓"
        },
        {
          "Paper": "Sun et al. 2023 [9]",
          "Focus": "✓",
          "Dataset": "✓"
        },
        {
          "Paper": "emoDARTS",
          "Focus": "✔",
          "Dataset": "✔"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "CNN",
          "Operation": "max pool\nixi",
          "Description": "Max Pooling layer with kernel i"
        },
        {
          "Component": "CNN",
          "Operation": "avg pool\nixi",
          "Description": "Average Pooling layer with ker-\nnel i"
        },
        {
          "Component": "CNN",
          "Operation": "dil\nconv ixi",
          "Description": "Dilated Convolution layer with\nkernel i, and dilation 2"
        },
        {
          "Component": "CNN",
          "Operation": "sep conv ixi",
          "Description": "Two Convolution\nlayers with\nkernel i"
        },
        {
          "Component": "CNN",
          "Operation": "conv ix1\n1xi",
          "Description": "Two Convolution\nlayers with\nfirst\nkernel\n(ix1)\nand\nsecond\n(1xi)"
        },
        {
          "Component": "SeqNN",
          "Operation": "lstm j",
          "Description": "LSTM with j layers"
        },
        {
          "Component": "SeqNN",
          "Operation": "j\nlstm att",
          "Description": "LSTM of j layers with Attention"
        },
        {
          "Component": "SeqNN",
          "Operation": "rnn j",
          "Description": "RNN with j layers"
        },
        {
          "Component": "SeqNN",
          "Operation": "j\nrnn att",
          "Description": "RNN of j layers with Attention"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 7: shows the performance and number of param- oftrainableparameters.",
      "data": [
        {
          "Scope": "emoDARTS",
          "Candidate Operations": "lstm 1,\nlstm 2,\nlstm 3,\nlstm 4,\nlstm att\n1,\nlstm att\n2,\nrnn 1,\nrnn 2,\nrnn 3,\nrnn 4,\nrnn att\n1, rnn att 2"
        },
        {
          "Scope": "LSTM Only",
          "Candidate Operations": "lstm 1, lstm 2, lstm 3, lstm 4"
        },
        {
          "Scope": "LSTM-Att. Only",
          "Candidate Operations": "lstm att\n1, lstm att\n2"
        },
        {
          "Scope": "RNN Only",
          "Candidate Operations": "rnn 1, rnn 2, rnn 3, rnn 4"
        },
        {
          "Scope": "RNN-Att. Only",
          "Candidate Operations": "rnn att\n1, rnn att 2"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "2",
      "title": "Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition",
      "authors": [
        "M Jalal",
        "R Milner",
        "T Hain"
      ],
      "year": "2020",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "3",
      "title": "A Review on Speech Emotion Recognition Using Deep Learning and Attention Mechanism",
      "authors": [
        "E Lieskovská",
        "M Jakubec",
        "R Jarina",
        "M Chmulík",
        "Y.-F Liao",
        "P Bours",
        "C Kwan"
      ],
      "venue": "Electronics 2021"
    },
    {
      "citation_id": "4",
      "title": "Multitask Learning From Augmented Auxiliary Data for Improving Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Towards temporal modelling of categorical speech emotion recognition",
      "authors": [
        "W Han",
        "H Ruan",
        "X Chen",
        "Z Wang",
        "H Li",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "6",
      "title": "Image denoising and restoration with CNN-LSTM Encoder Decoder with Direct Attention",
      "authors": [
        "K Haque",
        "M Yousuf",
        "R Rana"
      ],
      "year": "2018",
      "venue": "Image denoising and restoration with CNN-LSTM Encoder Decoder with Direct Attention"
    },
    {
      "citation_id": "7",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "8",
      "title": "Self Supervised Adversarial Domain Adaptation for Cross-Corpus and Cross-Language Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition",
      "authors": [
        "H Sun",
        "Z Lian",
        "B Liu",
        "Y Li",
        "L Sun",
        "C Cai",
        "J Tao",
        "M Wang",
        "Y Cheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "10",
      "title": "Neural Architecture Search for Speech Emotion Recognition",
      "authors": [
        "X Wu",
        "S Hu",
        "Z Wu",
        "X Liu",
        "H Meng"
      ],
      "year": "2022",
      "venue": "ICASSP, IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "11",
      "title": "PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search",
      "authors": [
        "Y Xu",
        "L Xie",
        "X Zhang",
        "X Chen",
        "G.-J Qi",
        "Q Tian",
        "H Xiong"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "12",
      "title": "Electric load demand forecasting with RNN cell generated by DARTS",
      "authors": [
        "G Biju",
        "G Pillai",
        "J Seshadrinath"
      ],
      "year": "2019",
      "venue": "IEEE Region 10 Annual International Conference, Proceedings/TENCON"
    },
    {
      "citation_id": "13",
      "title": "Recurrent Neural Network Architecture Search for Geophysical Emulation",
      "authors": [
        "R Maulik",
        "R Egele",
        "C Polytechnique",
        "B Lusch",
        "P Balaprakash"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis"
    },
    {
      "citation_id": "14",
      "title": "An experimental study of speech emotion recognition based on deep convolutional neural networks",
      "authors": [
        "W Zheng",
        "J Yu",
        "Y Zou"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "15",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "16",
      "title": "Evolving Learning for Analysing Mood-Related Infant Vocalisation",
      "authors": [
        "Z Zhang",
        "J Han",
        "K Qian",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings IN-TERSPEECH 2018, 19. Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "17",
      "title": "Neural Architecture Search with Reinforcement Learning",
      "authors": [
        "B Zoph",
        "Q Le"
      ],
      "year": "2016",
      "venue": "5th International Conference on Learning Representations, ICLR 2017 -Conference Track Proceedings"
    },
    {
      "citation_id": "18",
      "title": "µNAS: Constrained Neural Architecture Search for Microcontrollers",
      "authors": [
        "E Liberis",
        "L Dudziak",
        "N Lane"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st Workshop on Machine Learning and Systems"
    },
    {
      "citation_id": "19",
      "title": "Mixed precision neural architecture search for energy efficient deep learning",
      "authors": [
        "C Gong",
        "Z Jiang",
        "D Wang",
        "Y Lin",
        "Q Liu",
        "D Pan"
      ],
      "year": "2019",
      "venue": "IEEE/ACM International Conference on Computer-Aided Design"
    },
    {
      "citation_id": "20",
      "title": "DARTS: Differentiable Architecture Search",
      "authors": [
        "H Liu",
        "K Simonyan",
        "Y Yang"
      ],
      "year": "2018",
      "venue": "7th International Conference on Learning Representations"
    },
    {
      "citation_id": "21",
      "title": "Progressive Neural Architecture Search",
      "authors": [
        "C Liu",
        "B Zoph",
        "M Neumann",
        "J Shlens",
        "W Hua",
        "L.-J Li",
        "L Fei-Fei",
        "A Yuille",
        "J Huang",
        "K Murphy"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "22",
      "title": "DARTS+: Improved Differentiable Architecture Search with Early Stopping",
      "authors": [
        "H Liang",
        "S Zhang",
        "J Sun",
        "X He",
        "W Huang",
        "K Zhuang",
        "Z Li"
      ],
      "year": "2019",
      "venue": "DARTS+: Improved Differentiable Architecture Search with Early Stopping"
    },
    {
      "citation_id": "23",
      "title": "FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions",
      "authors": [
        "A Wan",
        "X Dai",
        "P Zhang",
        "Z He",
        "Y Tian",
        "S Xie",
        "B Wu",
        "M Yu",
        "T Xu",
        "K Chen",
        "P Vajda",
        "J Gonzalez"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "24",
      "title": "Stabilizing Differentiable Architecture Search via Perturbation-based Regularization",
      "authors": [
        "X Chen",
        "C.-J Hsieh"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning"
    },
    {
      "citation_id": "25",
      "title": "Cyclic Differentiable Architecture Search",
      "authors": [
        "H Yu",
        "H Peng",
        "Y Huang",
        "J Fu",
        "H Du",
        "L Wang",
        "H Ling"
      ],
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Enhancing Speech Emotion Recognition Through Differentiable Architecture Search",
      "authors": [
        "T Rajapakshe",
        "R Rana",
        "S Khalifa",
        "B Sisman",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Enhancing Speech Emotion Recognition Through Differentiable Architecture Search"
    },
    {
      "citation_id": "27",
      "title": "Attention Based Fully Convolutional Network for Speech Emotion Recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "Attention Based Fully Convolutional Network for Speech Emotion Recognition"
    },
    {
      "citation_id": "28",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2018 -Proceedings",
      "year": "2018",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2018 -Proceedings"
    },
    {
      "citation_id": "29",
      "title": "3-D Convolutional Recurrent Neural Networks with Attention Model for Speech Emotion Recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "30",
      "title": "Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "ICASSP, IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "31",
      "title": "Speech emotion recognition based on convolutional neural network with attentionbased bidirectional long short-term memory network and multitask learning",
      "authors": [
        "Z Liu",
        "M Han",
        "B Wu",
        "A Rehman"
      ],
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "32",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "33",
      "title": "MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE transactions on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "36",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "37",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "38",
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury Google",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison",
        "A Xamla",
        "E Yang",
        "Z Devito",
        "M Raison Nabla",
        "A Tejani",
        "S Chilamkurthy",
        "Q Ai",
        "B Steiner",
        "L Facebook",
        "J Facebook",
        "S Chintala"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}