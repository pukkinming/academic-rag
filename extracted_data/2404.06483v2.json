{
  "paper_id": "2404.06483v2",
  "title": "Rhythmmamba: Fast, Lightweight, And Accurate Remote Physiological Measurement",
  "published": "2024-04-09T17:34:19Z",
  "authors": [
    "Bochao Zou",
    "Zizheng Guo",
    "Xiaocheng Hu",
    "Huimin Ma"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Remote photoplethysmography (rPPG) is a method for noncontact measurement of physiological signals from facial videos, holding great potential in various applications such as healthcare, affective computing, and anti-spoofing. Existing deep learning methods struggle to address two core issues of rPPG simultaneously: understanding the periodic pattern of rPPG among long contexts and addressing large spatiotemporal redundancy in video segments. These represent a trade-off between computational complexity and the ability to capture long-range dependencies. In this paper, we introduce Rhyth-mMamba, a state space model-based method that captures long-range dependencies while maintaining linear complexity. By viewing rPPG as a time series task through the proposed frame stem, the periodic variations in pulse waves are modeled as state transitions. Additionally, we design multitemporal constraint and frequency domain feed-forward, both aligned with the characteristics of rPPG time series, to improve the learning capacity of Mamba for rPPG signals. Extensive experiments show that RhythmMamba achieves stateof-the-art performance with 319% throughput and 23% peak GPU memory.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Blood Volume Pulse (BVP) is a vital physiological signal, further enabling the extraction of key signs such as heart rate (HR) and heart rate variability (HRV). Photoplethysmography (PPG) is a non-invasive monitoring method that utilizes optical means to measure changes in blood volume within living tissues. The physiological mechanism of PPG stems from variations in blood volume during cardiac contraction and relaxation in subcutaneous blood vessels, leading to changes in light absorption and scattering. These changes result in periodic color signal variations on imaging sensors, which are imperceptible to the human eye  (Verkruysse, Svaasand, and Nelson 2008; Chen and McDuff 2018) . Traditionally, BVP extraction requires the use of contact sensors, which brings inconvenience and limitations. In recent years, non-contact methods for obtaining BVP, particularly rPPG, have garnered increasing attention  (McDuff 2023; Li, Yu, and Shi 2023; Choi, Kang, and Kim 2024) .\n\nEarly rPPG research primarily relied on traditional signal processing methods to recover weak rPPG signals from facial videos, which are susceptible to interference from environmental light, motion, and other noises. In complex environments, relying solely on signal processing methods often struggles to achieve satisfactory accuracy. In recent years, data-driven methods have become mainstream, represented by convolutional neural networks (CNNs) and transformers. However, CNNs have limited receptive fields and transformer-based architectures exhibit mediocre performance in capturing long-term dependencies from the computational complexity perspective, especially when dealing with long video sequences.\n\nRecently, Mamba  (Dao and Gu 2024)  has emerged with its selective state space model, striking a balance between maintaining linear complexity and facilitating long-term dependency modeling. It has been successfully applied to various artificial intelligence tasks such as video understanding  (Li et al. 2025) . For rPPG tasks that typically require longterm monitoring and are suitable for deployment on mobile devices, the linear complexity and ability to capture longterm dependencies give Mamba an advantage.\n\nHowever, the direct application of Mamba to rPPG tasks performs poorly. Our experiments reveal that embedding spatiotemporal information into token sequences through patch embedding leads to spatial information significantly disrupting Mamba's comprehension of temporal information (see Section 4.5). This phenomenon may stem from Mamba's linear modeling characteristics, where the states are associated with the temporal phases of the rPPG signals. The incorporation of spatial information increases the dimensionality of the state transition process, thereby adding complexity and impeding the model's learning efficacy.\n\nMotivated by the aforementioned discussion, we propose RhythmMamba, a state space model-based architecture for remote physiological measurement. The proposed frame stem embeds spatial information from a single frame into the channels, thereby viewing the rPPG task as a time series task, allowing the periodic variations in pulse waves to be modeled as state transitions. As shown in Fig.  1  Figure  1 : A schematic diagram of state transitions. Considering the periodic nature of rPPG, the rPPG signal can be represented using a finite number of states. Where h(t) represents the state vector, x(t) represents the input vector, and y(t) represents the output vector.\n\nsitions within the state space. The periodic nature of rPPG allows the signal to be represented using a finite set of states.\n\nAdditionally, we design multi-temporal constraint and frequency domain feed-forward, both aligned with the characteristics of rPPG time series, to improve the learning capacity of Mamba for rPPG signals. By learning from the same sequences with varying temporal lengths, a single Mamba block can simultaneously be constrained by the periodicity of long sequences and the trends of short sequences. Subsequently, through frequency domain feed-forward, the learned temporal features by Mamba undergo inter-channel spatial interaction in the frequency domain, enabling a better discernment of the periodic nature of rPPG signals.\n\nThe main contributions are as follows:\n\n• We propose RhythmMamba, which leverages state space models to model periodic variations as state transitions, combining multi-temporal constraints Mamba and frequency domain feed-forward to learn the quasi-periodic patterns of rPPG. To the best of our knowledge, this is the first work to investigate state space models in the rPPG domain.\n\n• In response to the observed phenomenon where spatial information interferes with Mamba's understanding of temporal sequences, we design the frame stem to embed spatial information into channels, reducing the dimensionality of state transitions to boost Mamba's learning.\n\n• We conduct extensive experiments on intra-dataset and cross-dataset scenarios. The results demonstrate that Rhyth-mMamba achieves state-of-the-art performance with 319% throughput and 23% GPU memory, as illustrated in Fig.  2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Remote Physiological Measurement",
      "text": "Early research on rPPG primarily relied on traditional signal processing methods to recover weak rPPG signals from facial videos  (Verkruysse, Svaasand, and Nelson 2008; Poh, McDuff, and Picard 2010; De Haan and Jeanne 2013; Wang et al. 2016) . In recent years, data-driven approaches have dominated due to their remarkable performance, showcasing a trend in the transition of backbone from 2D CNNs  ( Špetlík, Franc, and Matas 2018; Niu et al. 2018; Chen and McDuff 2018; Niu et al. 2020; Liu et al. 2020)  to 3D CNNs  (Yu, Li, and Zhao 2019; Yu et al. 2019; Zhao et al. 2021; Li, Yu    and Shi 2023) and further to transformers  (Yu et al. 2022 (Yu et al. , 2023;; Liu et al. 2023a; Shao et al. 2023; Liu et al. 2024; Zou et al. 2024) . However, none of them have been able to effectively address the two core issues of rPPG: understanding the periodic pattern of rPPG among long contexts and addressing large spatiotemporal redundancy in video segments. This dilemma underscores a trade-off between computational complexity and the ability to capture longrange dependencies, thereby presenting a barrier to deploying rPPG solutions on mobile devices. Although previously dominant 3D CNNs and video transformers have effectively tackled one of the above issues by utilizing local convolutions or long-range attention, they fail to address both problems simultaneously. Unlike them, the proposed Rhythm-Mamba can capture long-range dependencies while maintaining linear complexity, making it fast, lightweight, and accurate for remote physiological measurement.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Vision Mamba",
      "text": "Recently, Mamba has distinguished itself with a datadependent state space model (SSM) and a selection mechanism utilizing parallel scanning, striking a balance between maintaining linear complexity and facilitating longterm dependency modeling. Compared to transformers with quadratic complexity attention  (Vaswani et al. 2017; Arnab et al. 2021) , Mamba excels at handling long sequences with linear complexity. Subsequently, the immense potential of Mamba has sparked a series of works  (Zhu et al. 2024; Patro and Agneeswaran 2024; Li et al. 2025) , demonstrating superior performance and higher GPU efficiency of Mamba over Transformers on downstream vision tasks. However, unlike other video tasks, rPPG signals are particularly weak and highly susceptible to noise from factors such as lighting and motion, making the direct application of the traditional Mamba architecture to rPPG tasks perform poorly. In contrast to previous works, we view the rPPG task as the time series task, fully integrating spatial information into the channels and designing multiple modules tailored for time series to boost Mamba's learning. Where \"+\" represents addition, \"×\" represents multiplication, \"σ\" represents the activation layer, and trapezoid represents the linear layer.\n\n3 Methodology Section 3.1 introduces the general framework of Rhythm-Mamba, followed by the presentation of its main components: the frame stem in Section 3.2, the multi-temporal constraint Mamba in Section 3.3, and lastly, the frequency domain feed-forward in Section 3.4.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The General Framework Of Rhythmmamba",
      "text": "As shown in Figure  3 , RhythmMamba consists of frame stem, multi-temporal constraint Mamba, frequency domain feed-forward, and rPPG predictor head. The frame stem utilizes diff-fusion, self-attention, and frame average pooling to extract rPPG features and embed all spatial information into channels. Specifically, given an RGB video input X ∈ R 3×T ×H×W , X stem = f rame stem(X), where X stem ∈ R T ×C , and C, T, W, H indicate channel, sequence length, width, and height, respectively. Subsequently, the output of the frame stem will be fed into the multi-temporal constraint Mamba. The tokens will be sliced into sequences of varying temporal lengths, followed by the processing of hidden information between tokens with the SSM. Then, the output of Mamba will be passed into the frequency domain feed-forward, facilitating the interaction of information across multiple channels. The outputs of Mamba and feed-forward network (FFN) will undergo normalization and residual connections. The two outputs have dimensions identical to the output of the frame stem X stem ∈ R T ×C . Finally, the rPPG features will be projected into PPG waves through the predictor head.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Frame Stem",
      "text": "In the field of video understanding, existing transformerbased and Mamba-based methods typically embed spatiotemporal information into token sequences through patch embedding  (Arnab et al. 2021; Zhu et al. 2024; Dosovitskiy et al. 2020) . Previous works on rPPG have also been based on such foundational models for improvements. For transformer-based methods, spatiotemporal token sequences can inspire long-range spatiotemporal attention both within frames and across frames. However, we found that for linear Mamba, spatial information may interfere with Mamba's understanding of temporal sequences [see section 4.5]. The frame stem is utilized to initially extract rPPG features and embed spatial information fully into the channels, thereby boosting the learning of state transitions in the multitemporal constraint Mamba and the channel interactions in the frequency domain feed-forward.\n\nFirstly, the diff-fusion module  (Zou et al. 2024 ) integrates frame differences into the raw frames, enabling frame-level representation awareness of BVP wave variations. This effectively enhances the features of rPPG with a small additional computational cost. Additionally, for rPPG, highfrequency information across frames and low-frequency information within frames are required. Therefore, relatively large convolutional kernels are used to obtain low-frequency information within frames, ensuring that spatial information is fully incorporated into the channels. Here, 'relatively large' refers to the size relative to the image resolution, enabling a large receptive field.\n\nSpecifically, for an RGB video input X ∈ R 3×T ×H×W , temporal shift is initially applied to obtain X t-2 , X t-1 , X t , X t+1 and X t+2 . Subsequently, frame differences between consecutive frames are computed in reverse chronological order, yielding D t-2 , D t-1 , D t+1 , and D t+2 . The frame differences and the raw frames are then passed through Stem 1 for feature extraction. Stem 1 consists of a 2D convolution layer with (7 × 7) kernel, followed by batch normalization (BN), ReLU, and MaxPool. The input dimension is 3 when taking raw frames as input, and 12 when taking the concatenation of frame differences as input.\n\n(1)\n\nThen frame differences X dif f and raw frames X raw are merged and the feature representation is further enhanced through Stem 2 , which consists of a 2D convolution layer with (7 × 7) kernel, followed by BN, ReLU and MaxPool.\n\nSubsequently, at the resolution of (16 × 16), Stem 3 utilizes a convolution layer with (5 × 5) kernel to fully integrate spatial information into the channels, followed by BN. Before frame-level global average pooling, a self-attention module is employed to enhance skin regions with rPPG signals in the spatial domain. This self-attention module utilizes sigmoid activation followed by L1 normalization, which is softer than softmax and generates fewer masks  (Liu et al. 2023a ). The attention mask can be computed as:\n\nFinally, the attention output X attn ∈ R C×T ×H/8×W/8 , undergoes global average pooling within each frame, resulting in the stem output X stem ∈ R T ×C .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Temporal Constraint Mamba",
      "text": "Previous studies  (Hu et al. 2022; Kong, Bian, and Jiang 2022; Dai et al. 2022 ) have shown the effectiveness of modeling periodic tasks with multi-temporal scales, primarily achieved by the extraction and fusion of features from different temporal scales. Unlike these studies, we replace multitemporal fusion with multi-temporal constraint, which better aligns with the characteristics of the Mamba. Specifically, we slice a video segment into numerous sub-segments of varying lengths to constrain a single Mamba block, rather than downsampling the video segment to different resolutions and using multiple Mamba blocks to extract and fuse features. The aim is to subject a Mamba block to both the periodic constraints of long sequences and the trend constraints of short sequences simultaneously, instead of extracting different features from multi-temporal scales.\n\nAfter the frame stem, the token sequence can be regarded as a time series, and the state transition of Mamba can be interpreted as the temporal phase shift. Due to the quasiperiodicity of rPPG signals, the signal can be represented using a finite set of states. Specifically, as illustrated by the multi-temporal constraint Mamba in Figure  3 , the input X stem is first linearly projected and then processed through three weight-shared paths. Along these paths, the input is sliced into temporal sequences of varying lengths. For the i th path, the sequence is divided into 2 i-1 sub-sequences, each of which undergoes sequential processing through a convolution layer, activation layer, and selective state space model (see supplementary material A for details). Subsequently, they are recombined into a sequence of the original length, forming the output of the i th path, denoted as X pathi . The output before projection can be represented as follows:\n\n(4)",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Frequency Domain Feed-Forward",
      "text": "The FFN employs linear transformations to project data into a higher-dimensional space before mapping it back into a lower-dimensional space. Through this channel interaction, deeper features are extracted. In previous rPPG studies, spatio-temporal FFN was frequently applied, which introduced a depthwise 3D convolution layer between the two linear layers of the vanilla FFN, to refine the local inconsistency and provide relative positional cues.  (Yu et al. 2022 ).\n\nIn our study, due to the input sequences being solely timedependent, channel interaction in the frequency domain enables a better discernment of the periodic nature of rPPG signals. So we introduce frequency domain feed-forward, which adds a frequency domain linear layer between the two linear layers of the vanilla FFN. The frequency domain linear layer consists of three stages: domain conversion, frequency domain channel interaction, and domain inversion.\n\nDomain Conversion/Inversion. Domain conversion and inversion utilize fast Fourier transform and inverse Fourier transform, respectively. The utilization of the Fourier transform enables the decomposition of rPPG signals into their constituent frequencies, facilitating the recognition of periodic patterns in the rPPG signals. We transform the input H(t) to the frequency domain H(f ) as follows:\n\n(5)\n\nWhere f represents frequency, t represents time, and the subscripts re and im denote the real and imaginary components of the corresponding complex data, respectively. After channel Interaction in the frequency domain, inverse Fourier transform is employed to revert to the temporal domain:\n\nFrequency Domain Channel Interaction. Through the frame stem, spatial information is embedded into the channels, each of which is treated as a time series. Consequently, the frequency domain features obtained after domain conversion can clearly represent the signal's frequency composition. This allows channel interactions in the frequency domain to refine noise interference and more easily focus on critical channels. Specifically, channel interaction is implemented through a linear layer, the theoretical feasibility of which has been demonstrated by  (Yi et al. 2024) . For complex input H ∈ R T ×C , given complex weight matrix W ∈ R C×C and complex bias B ∈ R C , according to the rules of complex multiplication, it can be expressed as:\n\nAfter inverse FFT transform, the frequency domain feedforward outputs through linear projection, resulting in X F F N ∈ R T ×C .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset And Performance Metric",
      "text": "The experiments of remote physiological measurement were conducted on four publicly available datasets: PURE  (Stricker, Müller, and Gross 2014) , UBFC-rPPG  (Bobbia et al. 2019) , VIPL-HR  (Niu et al. 2019) , and MMPD  (Tang et al. 2023) . PURE comprises 59 1-minute videos, documenting records of 10 subjects, each engaging in six different activities. UBFC-rPPG consists of 42 videos, recording 42 subjects. These videos were derived from a setting where subjects participated in a time-limited digital game. VIPL-HR includes 2,378 RGB videos from 107 participants, captured using three RGB cameras, with an unstable fps. MMPD includes 660 1-minute videos, documenting records of 33 subjects. Participants engaged in four different activities under four distinct lighting conditions. Metircs. The evaluation was conducted using five metrics for video-level heart rate estimations: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), Pearson Correlation Coefficient (ρ), and Signal-to-Noise Ratio (SNR).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "The proposed RhythmMamba was implemented based on PyTorch, and we utilized an open-source rPPG toolbox  (Liu et al. 2023b ) to conduct a fair comparison against several state-of-the-art methods. In the pre-processing, video inputs were divided into segments of 160 frames. Facial recognition was applied on the first frame of each segment, followed by cropping and resizing of the facial region. These adjustments were then maintained throughout the subsequent frames. In the post-processing, a second-order Butterworth filter (cutoff frequencies: 0.75 and 2.5 Hz) was applied to filter the rPPG waveform, and power spectral density was computed by the Welch algorithm for further heart rate estimation. Following the protocol outlined in  (Yu et al. 2020) , random upsampling, downsampling, and horizontal flipping were applied for data augmentation. The experiment was conducted on NVIDIA RTX 3090.\n\nLoss. We employed a loss function that integrates constraints from both the temporal and frequency domains  (Yu et al. 2020) . The negative Pearson correlation coefficient is utilized as temporal constraint L T ime , while crossentropy between the power spectral density of prediction and the HR derived from the power spectral density of ground truth, is employed as frequency constraint L F req .\n\nLF req = CE(maxIndex(P SD(P P Ggt)), P SD(P P G pred )),\n\nwhere P SD represents Power Spectral Density and maxIndex represents the index of the maximum value. The overall loss is expressed by:\n\nComparison. We compare our method with state-of-theart approaches in intra-dataset testing  ( Špetlík, Franc, and Matas 2018; Chen and McDuff 2018; Yu, Li, and Zhao 2019; Liu et al. 2020; Gideon and Stent 2021; Lu, Han, and Zhou 2021; Yu et al. 2022; Liu et al. 2023a; Li, Yu, and Shi 2023; Lu et al. 2023; Li and Yin 2023; Yue, Shi, and Ding 2023; Yu et al. 2023; Sun and Li 2024; Zou et al. 2024 ). Building on this, additional comparisons with  (Verkruysse, Svaasand, and Nelson 2008; Poh, McDuff, and Picard 2010; De Haan and Jeanne 2013; Pilz et al. 2018",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Intra-Dataset Evaluation",
      "text": "We conducted intra-dataset evaluation on the PURE and UBFC datasets to validate the feasibility of the Mamba architecture. For the evaluation of the PURE dataset, we followed the protocols outlined in  (Lu, Han, and Zhou 2021) , splitting the dataset sequentially into training and testing sets with a ratio of 6:4. Similarly, for the evaluation of the UBFC dataset, we followed the protocols in  (Lu, Han, and Zhou 2021) , selecting the first 30 samples as the training set and the remaining 12 samples as the testing set. Due to the absence of the validation set, we selected the checkpoint from the last epoch for testing and compared them with the reported results from previous methods. As shown in Table  1 , on the PURE dataset, our method outperformed all state-ofthe-art methods across all metrics, achieving the minimum MAE (0.23) and RMSE (0.34). On the UBFC dataset, our method also achieved comparable performance to others. Due to the relative simplicity of PURE and UBFC, the performance of state-of-the-art methods on these datasets is nearing saturation. To further evaluate the performance, we employed the more challenging dataset. For the VIPL-HR dataset, we followed the subject-exclusive 5-fold crossvalidation protocol, as outlined in  (Niu et al. 2019; Yu et al. 2022) . For the MMPD dataset, following the protocols outlined in  (Zou et al. 2024) , the dataset was sequentially split into training, validation, and testing sets with a ratio of 7:1:2. As shown in Table  1 , our method achieved comparable performance to the previous methods. This indicates that RhythmMamba can accurately extract weak rPPG signals and understand their periodic nature, which provides ample empirical evidence for the feasibility of the Mamba architecture in the rPPG task.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Cross-Dataset Evaluation",
      "text": "To objectively evaluate the generalization capability to outof-distribution data, we followed the protocols outlined in  (Liu et al. 2023b ) for cross-dataset evaluation. The models were trained on either the PURE or UBFC datasets and tested on the PURE, UBFC, and MMPD datasets. The training dataset was sequentially split into training and validation sets with a ratio of 8:2. All comparative methods were implemented based on the rPPG toolbox  (Liu et al. 2023b) . As shown in Table  2 , the proposed RhythmMamba also achieved SOTA performance, demonstrating its capability in modeling domain-invariant features and generalizing to unseen domains. Based on the comparisons in Tables  1  and 2 , the improvement in cross-dataset results is less significant compared to intra-dataset results, possibly due to the fine-grained token-wise self-attention, which may have an advantage in capturing domain-invariant features. Nevertheless, founded on fewer parameters and lower computational complexity, RhythmMamba showcases its potential in realworld applications through its robustness and generalization in complex environments. Additional visualization results can be found in Supplementary Material B.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "Ablation studies were conducted on the MMPD dataset to assess the impact of different modules.  Impact of Spatial Information. As shown in Table  4 , the ablation study of spatial information is presented, where both position embedding and temporal embedding were implemented using learnable parameters. For a fair comparison, the vanilla FFN was used, as the frequency domain FFN might have an advantage with purely temporal token sequences. It is evident that tokenized spatiotemporal information performs poorly, even with temporal embedding or position embedding. The integration of spatial information increases the dimensionality of the state transition process, thereby elevating complexity and making the model more difficult to train. We task as a time series task, embedding spatial information into the channels, with each channel being treated as a purely temporal sequence. This ensures that the state transition process occurs purely along the temporal dimension, while spatial information interactions are facilitated through subsequent channel interactions, effectively resolving this issue.\n\nImpact of Key Modules. As illustrated in Table  3 , the comparison between the first four rows and the last row indicates the significant roles played by these modules. The difffusion module enables frame-level representation awareness of BVP wave variations, effectively enhancing rPPG features with a small additional computational cost. The use of relatively large convolution kernels and self-attention allows for the integration of spatial information into channels effectively, thereby providing sufficient information for subsequent processing. The multi-temporal constraint Mamba constrains a single Mamba block simultaneously to shortterm trends and periodic patterns, facilitating the accurate comprehension of rPPG features.\n\nImpact of Frequency Domain Feed-forward. As shown in Table  3 , the last four rows show that the frequency domain FFN plays an important role. Among them, vanilla FFN refers to using two linear layers to compose the FFN, and Spatio-Temporal FFN refers to the addition of a depth- wise convolution layer between the linear layers  (Yu et al. 2022) . Frequency domain FFN adds a frequency domain linear layer between the linear layers, effectively extracting the most critical frequency domain features in rPPG signals.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Computational Cost",
      "text": "We conducted a 30-second inference test at a resolution of 128×128, reporting the parameters, average MACs per frame, average throughput per frame, and average peak GPU memory usage per frame. As shown in",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "We approach the rPPG task as a time series task, designing multiple modules that align with the temporal characteristics of rPPG signals to boost state space model learning. This approach boasts strong long-range dependency modeling capabilities while maintaining linear complexity. It achieves state-of-the-art performance both within and across datasets with a faster and more lightweight design. However, since Mamba's state transitions align closely with the periodic variations of rPPG signals, we believe that Mamba's potential in rPPG extends beyond the current results. Our utilization of periodic priors is currently limited and we would like to delve into this more deeply in the future.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A State Space Model",
      "text": "The state space model (SSM) is a type of linear timeinvariant system that maps inputs to outputs through hidden layers. It is modeled by the following ordinary differential equations:\n\nWhere A ∈ R N ×N is the evolution matrix, B ∈ R N ×1 and C ∈ R 1×N are the projection matrices. N represents the sequence length. This continuous system is challenging to apply, while the SSM in Mamba serves as its discrete counterpart. It discretizes continuous parameters A and B into their discrete counterparts A and B using a time-scale parameter ∆. This transformation typically employs the zeroorder hold method:\n\nThe practical application of this discretization form is hindered by its inherent sequential nature. Nevertheless, it can be effectively represented through a convolution operation:\n\nWhere K ∈ R N denotes a structured convolution kernel, and * indicates a convolution operation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B Visualization",
      "text": "As shown in the left half of Figure a, we visualize the spectrum of an example from MMPD. From top to bottom, these represent the averaging of spectra across all channels in the frequency domain feed-forward of the last block, the power spectral density of the PPG signal before bandpass filtering, and the power spectral density of the PPG after bandpass filtering. The range from 0.75 Hz to 2.5 Hz corresponds to the heart rate frequency band. Since the spectrum of the Fre FFN is obtained through FFT, the number of frequency points in the spectrum depends on the input length, resulting in a less smooth output. Nevertheless, it still accurately captures the frequency domain characteristics of the BVP ground truth.\n\nAs shown in the right half of Figure a, an example of the PPG waveform is provided to demonstrate the efficacy and precision of our approach. Notably, our model robustly captures the peaks and variations of PPG signals, serving as the key basis for predicting heart rate from video data. Additionally, the scatter plots and Bland-Altman plots in",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C Arbitrary Length Videos Input",
      "text": "We attempt to test the trained RhythmMamba model on videos of arbitrary lengths, with the longest test video being limited to 60 seconds due to dataset constraints. As observed in Table a, the trained model enables seamless adaptation to video segments of any length without performance degradation. Only when the test length is reduced to 1 second (second row), which is close to or shorter than a single heartbeat, does some performance degradation occur. The results at various test lengths clearly demonstrate that RhythmMamba has effectively learned the periodic pattern of rPPG.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A schematic diagram of state transitions. Consid-",
      "page": 2
    },
    {
      "caption": "Figure 2: Performance and efficiency evaluation for intra-",
      "page": 2
    },
    {
      "caption": "Figure 3: The framework of RhythmMamba. It consists of frame stem, multi-temporal constraint Mamba, frequency domain",
      "page": 3
    },
    {
      "caption": "Figure 3: , RhythmMamba consists of frame",
      "page": 3
    },
    {
      "caption": "Figure 3: , the input",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fre FFN\n2.5\n0.75\n2\n1.5\n1\n0.5\n0\n0\n0.75\n1.5\n2.25\n3.0\n3.75\n4.5\n5.25\n6.0\n6.75\n7.5\nFrequency (Hz)": "Before Filter\n2.5\n2\n1.5\n1\n0.5\n0\n0\n0.75\n1.5\n2.25\n3.0\n3.75\n4.5\n5.25\n6.0\n6.75\n7.5\nFrequency (Hz)",
          "Before Filter\n3\nGround Truth\nPrediction\n2.5\n2\n1.5\n1\n0.5\n0\n-0.5\n-1\n-1.5\n-2\n0\n1\n2\n3\n4\n5\nTime (s)": ""
        },
        {
          "Fre FFN\n2.5\n0.75\n2\n1.5\n1\n0.5\n0\n0\n0.75\n1.5\n2.25\n3.0\n3.75\n4.5\n5.25\n6.0\n6.75\n7.5\nFrequency (Hz)": "",
          "Before Filter\n3\nGround Truth\nPrediction\n2.5\n2\n1.5\n1\n0.5\n0\n-0.5\n-1\n-1.5\n-2\n0\n1\n2\n3\n4\n5\nTime (s)": "After Filter\n2\n1.5\n1\n0.5\n0\n-0.5\n-1\n-1.5\n0\n1\n2\n3\n4\n5\nTime (s)"
        },
        {
          "Fre FFN\n2.5\n0.75\n2\n1.5\n1\n0.5\n0\n0\n0.75\n1.5\n2.25\n3.0\n3.75\n4.5\n5.25\n6.0\n6.75\n7.5\nFrequency (Hz)": "After Filter\n2.5\n2\n1.5\n1\n0.5\n0\n0\n0.75\n1.5\n2.25\n3.0\n3.75\n4.5\n5.25\n6.0\n6.75\n7.5\nFrequency (Hz)",
          "Before Filter\n3\nGround Truth\nPrediction\n2.5\n2\n1.5\n1\n0.5\n0\n-0.5\n-1\n-1.5\n-2\n0\n1\n2\n3\n4\n5\nTime (s)": ""
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Vivit: A video vision transformer",
      "authors": [
        "A Arnab",
        "M Dehghani",
        "G Heigold",
        "C Sun",
        "M Lučić",
        "C Schmid"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "2",
      "title": "Unsupervised skin tissue segmentation for remote photoplethysmography",
      "authors": [
        "S Bobbia",
        "R Macwan",
        "Y Benezeth",
        "A Mansouri",
        "J Dubois"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "3",
      "title": "Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces",
      "authors": [
        "C Casado",
        "M López"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "4",
      "title": "Deepphys: Video-based physiological measurement using convolutional attention networks",
      "authors": [
        "W Chen",
        "D Mcduff"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "5",
      "title": "Fusion-Vital: Video-RF Fusion Transformer for Advanced Remote Physiological Measurement",
      "authors": [
        "J.-H Choi",
        "K.-B Kang",
        "K.-T Kim"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "MS-TCT: multi-scale temporal convtransformer for action detection",
      "authors": [
        "R Dai",
        "S Das",
        "K Kahatapitiya",
        "M Ryoo",
        "F Brémond"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
      "authors": [
        "T Dao",
        "A Gu"
      ],
      "year": "2024",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "8",
      "title": "Robust pulse rate from chrominance-based rPPG",
      "authors": [
        "G De Haan"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "9",
      "title": "Improved motion robustness of remote-PPG by using the blood volume pulse signature",
      "authors": [
        "G De Haan",
        "A Van Leest"
      ],
      "year": "2014",
      "venue": "Physiological measurement"
    },
    {
      "citation_id": "10",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "11",
      "title": "The way to my heart is through contrastive learning: Remote photoplethysmography from unlabelled video",
      "authors": [
        "J Gideon",
        "S Stent"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "12",
      "title": "Transrac: Encoding multi-scale temporal correlation with transformers for repetitive action counting",
      "authors": [
        "H Hu",
        "S Dong",
        "Y Zhao",
        "D Lian",
        "Z Li",
        "S Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "MTT: Multi-scale temporal transformer for skeleton-based action recognition",
      "authors": [
        "J Kong",
        "Y Bian",
        "M Jiang"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "14",
      "title": "Learning motion-robust remote photoplethysmography through arbitrary resolution videos",
      "authors": [
        "J Li",
        "Z Yu",
        "J Shi"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Videomamba: State space model for efficient video understanding",
      "authors": [
        "K Li",
        "X Li",
        "Y Wang",
        "Y He",
        "Y Wang",
        "L Wang",
        "Y Qiao"
      ],
      "year": "2025",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Contactless Pulse Estimation Leveraging Pseudo Labels and Self-Supervision",
      "authors": [
        "Z Li",
        "L Yin"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "17",
      "title": "Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer",
      "authors": [
        "M Liu",
        "J Tang",
        "H Li",
        "J Qi",
        "S Li",
        "K Wang",
        "Y Wang",
        "H Chen"
      ],
      "year": "2024",
      "venue": "Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer",
      "arxiv": "arXiv:2402.04798"
    },
    {
      "citation_id": "18",
      "title": "Multitask temporal shift attention networks for on-device contactless vitals measurement",
      "authors": [
        "X Liu",
        "J Fromm",
        "S Patel",
        "D Mcduff"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "Efficientphys: Enabling simple, fast and accurate camera-based cardiac measurement",
      "authors": [
        "X Liu",
        "B Hill",
        "Z Jiang",
        "S Patel",
        "D Mcduff"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Dual-gan: Joint bvp and noise modeling for remote physiological measurement",
      "authors": [
        "X Liu",
        "G Narayanswamy",
        "A Paruchuri",
        "X Zhang",
        "J Tang",
        "Y Zhang",
        "S Sengupta",
        "S Patel",
        "Y Wang",
        "D Mcduff",
        "H Lu",
        "H Han",
        "S Zhou"
      ],
      "year": "2021",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "21",
      "title": "Neuron Structure Modeling for Generalizable Remote Physiological Measurement",
      "authors": [
        "H Lu",
        "Z Yu",
        "X Niu",
        "Y.-C Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Camera measurement of physiological vital signs",
      "authors": [
        "D Mcduff"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "23",
      "title": "Synrhythm: Learning a deep heart rate estimator from general to specific",
      "authors": [
        "X Niu",
        "H Han",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "24",
      "title": "Rhythmnet: End-to-end heart rate estimation from face via spatialtemporal representation",
      "authors": [
        "X Niu",
        "S Shan",
        "H Han",
        "X Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "25",
      "title": "Video-based remote physiological measurement via crossverified feature disentangling",
      "authors": [
        "X Niu",
        "Z Yu",
        "H Han",
        "X Li",
        "S Shan",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "26",
      "title": "SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series",
      "authors": [
        "B Patro",
        "V Agneeswaran"
      ],
      "year": "2024",
      "venue": "SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series",
      "arxiv": "arXiv:2403.15360"
    },
    {
      "citation_id": "27",
      "title": "Local group invariance for heart rate estimation from face videos in the wild",
      "authors": [
        "C Pilz",
        "S Zaunseder",
        "J Krajewski",
        "V Blazek"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "28",
      "title": "Noncontact, automated cardiac pulse measurements using video imaging and blind source separation",
      "authors": [
        "M.-Z Poh",
        "D Mcduff",
        "R Picard"
      ],
      "year": "2010",
      "venue": "Optics express"
    },
    {
      "citation_id": "29",
      "title": "TranPhys: Spatiotemporal Masked Transformer Steered Remote Photoplethysmography Estimation",
      "authors": [
        "H Shao",
        "L Luo",
        "J Qian",
        "S Chen",
        "C Hu",
        "J Yang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "30",
      "title": "Visual heart rate estimation with convolutional neural network",
      "authors": [
        "R Špetlík",
        "V Franc",
        "J Matas"
      ],
      "year": "2018",
      "venue": "Proceedings of the British Machine Vision Conference"
    },
    {
      "citation_id": "31",
      "title": "Contrast-Phys+: Unsupervised and Weakly-Supervised Video-Based Remote Physiological Measurement via Spatiotemporal Contrast",
      "authors": [
        "R Stricker",
        "S Müller",
        "H.-M Gross",
        "Ieee",
        "Z Sun",
        "X Li"
      ],
      "year": "2014",
      "venue": "The 23rd IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "32",
      "title": "MMPD: Multi-Domain Mobile Video Physiology Dataset",
      "authors": [
        "J Tang",
        "K Chen",
        "Y Wang",
        "Y Shi",
        "S Patel",
        "D Mcduff",
        "X Liu"
      ],
      "year": "2023",
      "venue": "45th Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "33",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "34",
      "title": "Remote plethysmographic imaging using ambient light",
      "authors": [
        "W Verkruysse",
        "L Svaasand",
        "J Nelson"
      ],
      "year": "2008",
      "venue": "Optics express"
    },
    {
      "citation_id": "35",
      "title": "Algorithmic principles of remote PPG",
      "authors": [
        "W Wang",
        "A Den Brinker",
        "S Stuijk",
        "G De Haan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "36",
      "title": "Frequency-domain MLPs are more effective learners in time series forecasting",
      "authors": [
        "K Yi",
        "Q Zhang",
        "W Fan",
        "S Wang",
        "P Wang",
        "H He",
        "N An",
        "D Lian",
        "L Cao",
        "Z Niu"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "37",
      "title": "Autohr: A strong end-to-end baseline for remote heart rate measurement with neural searching",
      "authors": [
        "Z Yu",
        "X Li",
        "X Niu",
        "J Shi",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "38",
      "title": "Remote photoplethysmograph signal measurement from facial videos using spatiotemporal networks",
      "authors": [
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "The British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "39",
      "title": "Remote heart rate measurement from highly compressed facial videos: an end-to-end deep learning solution with video enhancement",
      "authors": [
        "Z Yu",
        "W Peng",
        "X Li",
        "X Hong",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "40",
      "title": "Physformer++: Facial video-based physiological measurement with slowfast temporal difference transformer",
      "authors": [
        "Z Yu",
        "Y Shen",
        "J Shi",
        "H Zhao",
        "Y Cui",
        "J Zhang",
        "P Torr",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "Physformer: Facial video-based physiological measurement with temporal difference transformer",
      "authors": [
        "Z Yu",
        "Y Shen",
        "J Shi",
        "H Zhao",
        "P Torr",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "Facial video-based remote physiological measurement via self-supervised learning",
      "authors": [
        "Z Yue",
        "M Shi",
        "S Ding"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Video-based physiological measurement using 3d central difference convolution attention network",
      "authors": [
        "Y Zhao",
        "B Zou",
        "F Yang",
        "L Lu",
        "A Belkacem",
        "C Chen"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "44",
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
      "authors": [
        "L Zhu",
        "B Liao",
        "Q Zhang",
        "X Wang",
        "W Liu",
        "X Wang",
        "B Zou",
        "Z Guo",
        "J Chen",
        "H Ma"
      ],
      "year": "2024",
      "venue": "Rhythm-Former: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer",
      "arxiv": "arXiv:2402.12788"
    }
  ]
}