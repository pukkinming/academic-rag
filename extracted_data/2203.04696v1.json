{
  "paper_id": "2203.04696v1",
  "title": "Robust Federated Learning Against Adversarial Attacks For Speech Emotion Recognition",
  "published": "2022-03-09T13:19:26Z",
  "authors": [
    "Yi Chang",
    "Sofiane Laridi",
    "Zhao Ren",
    "Gregory Palmer",
    "Björn W. Schuller",
    "Marco Fisichella"
  ],
  "keywords": [
    "student member",
    "IEEE",
    "Sofiane Laridi",
    "Zhao Ren",
    "member",
    "IEEE",
    "Gregory Palmer",
    "Bj örn W. Schuller",
    "Fellow",
    "IEEE",
    "Marco Fisichella Speech emotion recognition",
    "federated learning",
    "adversarial attacks",
    "adversarial training",
    "randomisation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Due to the development of machine learning and speech processing, speech emotion recognition has been a popular research topic in recent years. However, the speech data cannot be protected when it is uploaded and processed on servers in the internet-of-things applications of speech emotion recognition. Furthermore, deep neural networks have proven to be vulnerable to human-indistinguishable adversarial perturbations. The adversarial attacks generated from the perturbations may result in deep neural networks wrongly predicting the emotional states. We propose a novel federated adversarial learning framework for protecting both data and deep neural networks. The proposed framework consists of i) federated learning for data privacy, and ii) adversarial training at the training stage and randomisation at the testing stage for model robustness. The experiments show that our proposed framework can effectively protect the speech data locally and improve the model robustness against a series of adversarial attacks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "V OICE operated smart edge devices -e. g., Amazon Alexa and Google Nest -are becoming increasingly prevalent in our daily lives. These devices have benefited from recent advancements within the field of Machine Learning (ML), where deep neural networks represent the current state-of-the-art approach for analysing speech and recognising emotional states  [1] . In this paper, we address two topical challenges within this area: data privacy and robustness towards adversarial attacks.\n\nDespite recent advances, the performance of ML models remains upper-bounded by the quality and quantity of the samples used for training  [2] . This limitation has implications for the field of Speech Emotion Recognition (SER). The data used in this area often contains private / sensitive information, especially when emotion recognition is applied in mental health applications, e. g., detection of depression  [3] . Therefore, while smart edge devices may record an abundance of data, there are concerns regarding the extent to which these devices may leak private information  [4] . These concerns -along with strict legal and ethical requirements designed to protect user privacy -can prohibit the pooling of user data for centralised ML model training, a practice that often results in a better model. Thus, due to the lack of data from different sources, it can become a challenge to build ML models that are effective for SER. Here, Federated Learning (FL) has recently emerged as an alternative to centralised learning, allowing data holders to collaboratively train a global model without physically sharing their data. Instead, participants -in our case, the users of the smart edge devices -train a copy of a model with local data and iteratively share the resulting parameter updates, often via a centralised entity  [5] . Therefore, thanks to FL, a global model can be obtained without the users' data leaving their respective devices.\n\nWhile FL takes steps towards mitigating privacy concerns when training models using real-world data, questions remain regarding model robustness. In recent years, there has been an abundance of literature demonstrating the vulnerability of deep neural networks towards adversarial attacks  [6] . Adversaries are capable of learning to perturb a small set of pixels within a given sample, barely perceptible to humans, but capable of causing a misclassification. Such attacks are increasingly being used to fool models deployed on smart devices, often with malicious intent  [7] . This creates the need for models that are robust towards adversarial attacks, as well as principled methodologies for measuring robustness.\n\nIn this paper, we take steps towards addressing the challenges outlined above. To the best of the authors' knowledge, we propose the first pipeline to protect federated learning with a comprehensive approach in the domain of SER. Our contributions can be summarised as follows:\n\ni.) A federated learning framework for SER is constructed to protect each speaker's data privacy.\n\nii.) To protect SER models trained with federated learning against a range of adversarial attacks, we propose and compare two single-stage federated defence modelling strategies: adversarial training at the training stage and randomisation at the testing stage. Especially, our experiments demonstrate that randomisation on log Mel spectrograms extracted from speech signals is able to protect the SER models against adversarial attacks.\n\niii.) Finally, we show that our proposed two-stage defence modelling approach, i. e., the combination of adversarial training and randomisation, effectively improves the model robustness over vanilla federated learning (i. e., nature) and federated learning with a single stage of defence.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "This chapter will go through the basic principles of SER and FL. We also provide a recap on adversarial attacks and their types, followed by two techniques to defend against these attacks, namely: i) adversarial training for the single-step adversarial white-box attacks; and ii) randomisation to defend against iterative adversarial white-box attacks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "Human-Computer Interaction (HCI) is an essential part of Artificial Intelligence (AI) research. Real-life HCI applications have been facilitated by research on automatic emotion recognition  [8] , thus improving quality of service and quality of life  [9] . The speech signal is one of the key ways for humans to communicate, since it contains a substantial amount of paralinguistic information (i. e., emotion states, attitudes, etc.)  [10] . In past decades, SER has been a particularly useful research topic for HCI to recognise emotion from speech signals with the methods of computational paralinguistics and machine learning  [11] . In SER tasks, speech signals are often annotated with continuous values (i. e., arousal, valence and dominance  [12] ) and/or discrete labels (e. g., the 'Big Six' proposed by Ekman  [13] : anger, disgust, fear, happiness, sadness and surprise). The continuous and discrete labels can be converted to each other using a circumplex model  [14] .\n\nIn the past years, a set of acoustic features have proven to be effective for distinguishing emotions from speech, including low-level descriptors (e. g., energy features and spectral features) and statistical functionals  [15] ,  [16] . With acoustic features, classic machine learning approaches (e. g., support vector machines) have been successfully employed to analyse emotional states from speech signals  [17] . In contrast to classic machine learning, deep learning mostly deals with either raw speech signals or timefrequency representations, and often shows better performance than classic machine learning in recent advances  [18] ,  [19] . Compared to 1D raw speech signals, 2D time-frequency representations have become more popular  [20] . Typical time-frequency representations include spectrograms, Mel spectrograms, log Mel spectrograms, and Mel Frequency Cepstral Coefficients (MFCCs)  [21] ,  [22] ,  [23] . In this work, we will use log Mel spectrograms as the input of Convolutional Neural Networks (CNNs) due to the Mel scale's linearity in the human ear's auditory properties  [24]  and log Mel spectrograms' good performance as the input of CNNs in acoustic tasks  [25] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Federated Learning",
      "text": "FL has become a popular solution for applications that can benefit from a decentralized, collaborative and privacy preserving learning process. Application domains that can benefit from an FL approach include: intelligent industrial production  [26] , healthcare  [27]  and, the focus of our current study, SER  [7] . FL consists of a number of client machines hosting a decentralized training dataset, where each sub-dataset resides and remains on the respective client machine throughout the training process  [28] . Instead of sharing data, the clients share the parameters of an ML model, typically via a centralized server hosting a global model, or via a decentralized peer-to-peer topology  [5] . This global model can be downloaded by each client to perform local updates. After a specified number of epochs, each client sends their optimized model parameters back to the server, where they are aggregated -for instance using federated averaging  [29] . The aggregated parameters are subsequently made available for download, and the next training round can begin.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "White-Box Adversarial Attacks",
      "text": "The digital transformation in mobile networks and computing has recently led to a dramatic increase in the number of Internet users, connections, and Internet-of-Things (IoT) devices, as well as network capabilities and application requirements. With the digital proliferation, increasingly sophisticated attack strategies are emerging to take the offensive against Deep Neural Networks (DNNs). A thorough investigation has been conducted on the vulnerability of DNNs to various malicious attacks capable of creating adversarial examples with the goal of breaking the prediction of DNNs  [30] . There are two major categories of these attacks:   [31] .\n\nIn this paper, we investigate the extent to which we can design robust defence schemes for models obtained via FL against the following adversarial white-box attacks:\n\nThe Fast Gradient Sign Method (FGSM) generates an adversarial sample x adv from an individual input sample x ∈ X (where X represents a set of samples) in a single step, using the sign of the gradient ∇ of a loss function L of the network  [32] :\n\nwhere is a positive constant that determines the scale of the perturbation , and θ denotes the model parameters.\n\nProjected Gradient Attack (PGD)  [33] ,  [34]  is a more powerful multi-step variant of FGSM:\n\nwhere i is the iteration step, and α is a constant value that affects the perturbation's scale. The clip(•) function aims to clip the perturbations into a small interval, i. e., |α sign(∇\n\nwhere η is a positive constant. Compared to FGSM, PGD yields a number of possible adversarial samples in multiple iteration steps, but it has a higher computational complexity  [34] .\n\nDeepFool  [35]  is a typical iterative attack, and aims to find the minimal perturbation r that results in a change in output of a classifier M(•) when applied to a sample x. The assumption is made that M(•) is an affine binary classification function. DeepFool estimates the perpendicular distance and direction from an input x to the decision boundary\n\nis used to multiply r to generate x adv , ensuring that the M(x adv ) crosses the decision boundary:\n\nCorrespondingly, DeepFool in binary classification can be extended to multi-class classification according to the one-vs-all classification scheme  [35] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Adversarial Training",
      "text": "Adversarial examples reveal the inherent vulnerability of neural networks due to their linear nature  [32] . In order to fight against the adversarial examples, adversarial training was proposed and initially utilised to train the neural networks with a mixture of adversarial examples and original clean signals to improve the robustness of neural networks  [36] ,  [37] . Ensemble adversarial training was proposed in  [38]  to further augment training data with perturbations generated from other pre-trained models. Max-Margin Adversarial (MMA) was proposed in  [39]  to directly maximise the margins to the decision boundary and minimise the adversarial loss on the decision boundary at the \"shortest successful perturbation\". Moreover, adversarial robustness can also be achieved by including additional unlabelled data and thereby reducing the sample complexity gap between adversarial and clean samples  [40] ,  [41] .\n\nBesides the adversarial training, denoising techniques can also be applied to increase the model's robustness toward adversarial attacks, such as a high-level representation guided denoiser  [42]  and feature denoising  [43] . However, these denoising techniques either under-perform because the defence can be circumvented  [44]  or include extra parameters to train.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Randomisation",
      "text": "Randomisation was introduced in  [45]  as a defence method at the inference time against the adversarial attacks. It contains two layers: random resizing and random padding as indicated in Equation (  4 ). The random resizing layer randomly enlarges the input images with the dimension of (W, H, C) (W : Width, H: Height, C: Channel number) to the dimension of (W , H , C), which is further padded into the dimension of (W , H , C). Through fine-tuning the hyper-parameters during the resizing and padding procedures, randomisation can work effectively to mitigate the adversarial effects.\n\n(4) Whereas adversarial training can effectively improve the models' robustness against single-step attacks (e. g., FGSM)  [37] ,  [38] , image transformations (e. g., resizing, padding) have been found to mitigate the effects of iterative adversarial attacks (e. g., PGD, DeepFool) due to their weak generalisation ability  [45] . In this work, we also combine adversarial training with randomisation to better mitigate the effects of single-step as well as iterative adversarial attacks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "While there exist a few research studies on federated learning for SER, as well as adversarial attack on SER in centralised settings, to the best of the authors' knowledge, our work is the first exploration of improving federated learning's capability for privacy protection against adversarial attacks in this area. The potential benefits of FL for SER's typically sensitive data -and ability to deliver promising results compared with SOTA approaches -were highlighted in a recent study by  [46] . A number of real world applications have been identified for Federated-SER, including: monitoring the general mood of elderly in care homes  [47] ; the healthcare industry in general, a vulnerable sector that often falls victim to cyber-attacks and data breaches, where data is typically highly sensitive and distributed in nature  [48] ; multi-modal (i. e., face video and speech) emotion recognition systems to improve work culture and the environment in post-pandemic times  [49] ; and depression treatment robots that do not need to transfer users' videos and conversation data to the server  [50] .\n\nWe note that there have also been recent works focused on privacy attacks on Federated-SER, where the goal is to retrieve / recover sensitive information from trained models, typically as a result of the models over-fitting on the training data  [47] ,  [51] . A further concern are model poisoning attacks, where FL participants deliberately attempt to sabotage the model so that it misclassifies specific samples, potentially those of relevance to competitors  [52] . Model poisoning attacks differ from the white-box attacks that are the focus of our current work, since for white-box attacks, the goal is to perturb samples to cause a misclassification at inference time.\n\nA comprehensive summary of FL challenges -including communication costs, resource allocation, privacy and security -can be found in a survey  [53] . Meanwhile, for SER in centralised settings, there exists a number of works on adversarial attacks and corresponding defence schemes. An end-to-end scheme was proposed to generate adversarial emotional speech data in  [54] . In  [55] , the first black-box adversarial attacks were used to deceive SER systems, and adversarial training and Generative Adversarial Networks (GANs) were explored to enhance the SER model's robustness. Furthermore, a similarity-based adversarial training was proposed to protect SER models against white-box adversarial attacks in our prior study  [22] . As improving the transferability of adversarial attacks can facilitate the investigation of improving SER models' robustness, the transferability of black-box adversarial attacks was enhanced by lifelong learning in another study  [31] .\n\nThere have also been numerous studies on defensive methods, e. g., adversarial training and ensemble diversity  [56] . However, most of them are not suitable for distributed devices or for distributed machine learning. The main reason is that the devices are scattered and may be exposed to different attacks simultaneously. The first attempt to adopt the concept of FL to provide a defence framework against various white-box attacks on distributed networks was studied in  [7] . The authors present the F DA 3 approach, which is able to pool the knowledge of defending against attacks from different sources. However, the approach assumes an attack monitor capable of determining which type of white-box attack is being applied. In contrast, our current work focuses on mitigating the effects of adversarial attacks on Federated-SER at inference time through randomisation, an approach that works well on image data  [45] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Description",
      "text": "In this section, we formally define our problem setting, introducing the terminology and notations that will be used in the remainder of the paper. In particular, we formally define the FL scenario that we are studying as well as the application of white-box attacks. In Table  1 , we provide a summary of the notations used.\n\nFederated Learning Scenario: We assume a FL setting with N data owners {F 1 , ..., F N } and their respective subdatasets {D 1 , ..., D N }. Each D i contains a set of samples X i and corresponding labels Y i for clients i ∈ [1, N ]  [57] . A conventional approach towards ML model training would be to pool these subdatasets D 1 ∪ D 1 ... ∪ D N and train a model M ϑ . However, in FL, the data owners F i do not wish to share their data. Instead, each participant F i obtains and updates a local copy of a model White-box attacks on federated learning models: We shall assume that our FL model M θ is vulnerable towards whitebox attacks, e. g., someone has obtained a copy of the trained model M θ , and can generate adversarial samples x adv,i ∈ X adv that would cause a misclassification for M θ (x adv,i ), where i represents the ID of the original (unperturbed) sample x i .\n\nPerformance Measure: The goal in FL is to obtain a model M θ that achieves a comparable performance V θ (e. g., with respect to accuracy) to what can be achieved using conventional centralised training. The key distinction is being that during FL training, the data D i physically remains with participant F i , thereby preserving privacy during the model training process 1 . Formally, the objective of FL is to obtain a minimal δ accuracy loss: |V θ -V ϑ | < δ  [57] . In our problem setting, we also have the challenge of adversarial white-box attacks. Therefore, with a slight abuse of terminology, we also want to minimise: |V θ,X adv -V ϑ,X | < δ, where V θ,X adv is the performance of the model M θ when given a set of perturbed samples X adv obtained via a white-box attack.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Proposed Methodology",
      "text": "To tackle the challenges discussed in Section 1, we introduce a novel federated learning pipeline for SER that is robust towards white-box attacks on speech data (cf. Fig.  1 ). Under the assumption that each client already has its own speech data, our pipeline consists of three components. First, we employ a data preprocessing module that extracts the log Mel spectrograms from speech signals of each client. The log Mel spectrograms are further used as the input of SER models. Second, we propose a federated adversarial learning module, which is the defence strategy at the training time. Specifically, in the federated adversarial training module, the SER models learnt by FL in Section 2.2 are defended with adversarial training illustrated in Section 2.4. Finally, a randomisation module for defence at the inference time is proposed to further improve the SER models' robustness with the approach of randomisation outlined in Section 2.5. In the following, we will give details about each of the three modules.\n\n1. While overall there are fewer privacy-specific threats associated with FL, the models themselves have the same vulnerabilities towards inference-based attacks as models obtained via conventional training  [58] . This topic however, is outside of the scope of our current work. However, for readers who are interested in this topic, we recommend a recent survey  [59] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "As stated previously, our pipelines assumes that each participating client F i to the FL has its own separate audio data D i . A preprocessing step is necessary before feeding the data to the local model. Log Mel spectrograms are extracted from each client's audio data. Compared to regular spectrograms, log Mel spectrograms use the Mel scale on the y axis instead of linearly scaled frequency.As a result, each client F i would have its own Log Mel spectrogram data X i that will be used as input to its local model M θi,t .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Defence At Training Time -Adversarial Fl",
      "text": "In Fig.  1 (a) , adversarial federated learning is the first stage of defence within our pipeline. To avoid training more complex neural networks and the obfuscated gradient effects  [44] , we do not use a denoising technique. The adversarial training applied in this work is vanilla adversarial training, which provides better performance on original data and comparable performance on adversarial data compared to similarity-based adversarial training  [60] .\n\nFor our adversarial learning module, we generate adversarial samples using white-box attacks. To obtain adversarial samples, the input is X i and Y i , the original samples and labels from client F i (for a client i ∈ [1, N ]). Adversarial federated learning follows the same training principle outlined for vanilla federated learning. However, at the beginning of each round t, training data samples are divided randomly to X i and X i with equal number of samples, this ensures having different set of samples in each round.\n\nUsing the local model of the last round M θi,t , white-box attacks are applied to X i , generating the adversarial samples X adv,i . Subsequently, the union of X i ∪ X adv,i is used for training. The loss function is defined as follows:\n\nwhere α is a constant value used to adjust the ratio of loss value based on the original data samples L(θ i , X i , Y i ) and adversarial samples L(θ i , X adv,i , Y i ). Then, when the local training is complete, the new weights θ i,t are sent to the FL server for the next round t + 1. Given that we need a trained model to generate adversarial samples, in the first round, we only use the original samples X i for each respective client i ∈ [1, N ], and begin our adversarial training from the second round onwards.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Defence At Inference Time -Randomisation",
      "text": "As depicted in Fig.  1  (b), two randomisation layers at the inference time are applied: random resizing and random padding. The random resizing layer randomly enlarges the input log Mel spectrogram with the dimension of (W, H, 1) to the dimension of (W , H , 1), Fig.  1 : Overview of the proposed framework. Please refer to Table  1  for notations. 1.) Split data X i to X i & X i . 2.) Generate adversarial data X adv,i from X i . 3.) Train local model M θi,t on the pooled X i ∪ X adv,i 4.) Send trained model's weights θ i,t to server. 5.) Aggregate all the clients' weights, resulting in θ i,t+1 . 6.) Send back θ i,t+1 to the clients for the next round t + 1.\n\nwhere W and H are randomly chosen from a reasonably small range, which means |W -W | and |H -H| are small:\n\nBy fine-tuning the hyperparameters for randomisation, we pursue two goals. First, randomisation has a minimal impact on the models' performance on the original data. Second, it mitigates the adversarial attacks' effects on the models' performance. Using a small enough range has been shown to have no significant effect on model performance on the original dataset  [45] . Next, a random padding layer fills in certain values (e. g., zero) around the resized spectrograms, resulting in the final dimension (W , H , 1). The two computationally efficient randomisation layers make the models more robust against adversarial samples generated using one-step and iterative attacks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Results",
      "text": "In this section we evaluate our pipeline using the Database of Elicited Mood in Speech (DEMoS)  [61] , a popular SER dataset. First we shall discuss the properties of this dataset and our approach for obtaining a data split suitable for FL. We give more details about this dataset in subsection 6.1. Then, in subsection 6.2, we describe our experimental setup, including details on how we extracted the log Mel spectrograms from the audi data, model architectures, the setting used for adversarial federated training and randomisation, reproducibility, and evaluation metrics. Finally we discuss our experimental results in subsection 6.3.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "The DEMoS  [61]  is an Italian emotional speech corpus of 7.7 hours of audio recordings, collected from 68 speakers (23 females and 45 males). The speakers' emotion were induced by an arousal-valence progression. In total, 9, 365 emotional and 332 neutral speech samples were recorded. As the neutral state is a minority class, it is not considered in our experiment. Therefore, we employ the 9, 365 emotional speech samples (average duration: 2.86 seconds ± standard deviation: 1.26 seconds) annotated into seven classes, including anger, disgust, fear, guilt, happiness, sadness, and surprise. In our experiment, all audio recordings were sampled with 16 kHz. Because federated learning aims to perform on each speaker's data in a long-term manner for personalised SER-related applications, we divide the database into a training set and a test set with a speaker-dependent strategy rather than the 'classic' speakerindependent one. Each actor's speech is divided into an 80 % training set and a 20 % test set, each of which composes the global training set and test set, respectively. The emotion distribution is described in Table  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "Log Mel Spectrograms Extraction: We set the audio sample length as the maximum one as 5.884 seconds, which means shorter audio samples will be self-repeated. We set the sliding window as 512 time frames, the overlap as 256 time frames, and the Mel bins as 64 in number for the log Mel spectrogram extraction  [22] . As a result, the generated log Mel spectrograms share the dimensions of (373,  64) , where 373 is the dimension along the time steps, and 64 is the number of Mel frequencies.\n\nModel's Architecture: VGG and VGG-like architectures have been successfully applied in classifying spectrogram images for a number of audio tasks  [60] ,  [62] . Therefore, in this work, we trained VGG-15 models in the federated setting. As shown in Fig.  3 , the architecture of VGG-15 consists of five convolutional blocks with the output channel numbers of 64, 128, 256, 512 and 512, each of which is followed by a local max pooling layer with a kernel size of (2, 2). Each convolutional layer is followed by a batch normalisation layer and a 'ReLU' activation function  [63]  to stabilise and accelerate the training process  [64] . Before the final two fully connected layers for the final classification, a global average pooling layer is applied.\n\nAdversarial Federated Training: In order to perform adversarial training, adversarial data should be generated first using one of the white-box attacks. For the iterative attack DeepFool, the maximum iteration for the optimisation procedure in is set to 5, since the study  [35]  found that DeepFool empirically converges in less than 3 iterations for perturbation to fool the classifier. For better comparison, we also set the maximum iteration for PGD to 5. Moreover, we set the norm of the generated perturbation for FGSM, PGD and DeepFool as l ∞ , l ∞ and l 2 respectively. The in Equation (  1 ), η in Equation (  2 ) and ζ in Equation (  3 ) are set to 0.05, 0.05 and 0.02 respectively. In addition, settings for the adversarial attack are the same for training and testing. Some The batch size for training is set to 8 to account for limited memory, while for validation and testing it is set to 1 to maximise the ability to randomise. The optimiser \"Adam\"  [65]  is applied with a fixed learning rate of 0.001.\n\nFor the implementation of FL we use our framework based on the Flower FL framework  [66]  (an open source framework for Federated Learning), where we have adapted Docker to containerise our FL clients and FL servers. This means that each FL client runs in a Docker container and communicates with the FL server container. Once the FL training is complete, a model with the aggregated weights of the last round is stored for evaluation.\n\nRandomisation: The original dimension of the generated log Mel spectrogram is (373, 64, 1). In Equation (  6 ), the W is randomly chosen in  [373, 380) , and H is randomly chosen in  [64, 66) . After the resizing, we apply the random padding on the boundaries of the log Mel spectrogram by the value 0.5 and after the padding, the final (W , H , 1) dimension is (380, 66, 1). We fine-tune the hyper-parameters (i. e., the range that W are H chosen from, and values of W and H ) of the randomisation based on the set-aside validation dataset.\n\nReproducibility: The open-source Adversarial Robustness Toolbox library (ART)  [67]  is used to generate the adversarial data. In terms of hardware, the experiments are conducted on an Nvidia DGX machine 2 , which has 8 × NVIDIA A100 GPUs with a total of 320 GB GPU memory and an AMD 7742 64 cores 2.25 GHz CPU with 512 GB of RAM. DGX is a powerful machine that allowed us to run the FL experiments.\n\nEvaluation Metrics: Compared with accuracy (i. e., weighted average recall), Unweighted Average Recall (UAR) can better evaluate models' classification performance on imbalanced datasets  [11] . UAR is used to further evaluate the converged federated models' performance on data where randomisation operations have been applied, adversarial examples and randomised adversarial examples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "Fig.  4  illustrates the UAR score of the trained FL models, comparing the nature federated learnt model and the federated adversarial trained models with PGD, FGSM and DeepFool, respectively. The models are evaluated on the original (unperturbed) test data every 10 rounds. The four mentioned models converge to a UAR score of around 90 % with diverse convergence times. We can see that, the three federated adversarial training algorithms share similar convergence speed with the nature model, where all models herein converge after 150 rounds. Therefore, we conduct further tests for the above models after 150 rounds until the final 300 rounds, every 10 rounds.\n\nFrom Fig.  5 , there are three major findings. Firstly, Fig.  5a  and Fig.  5b  show that adversarial learnt models perform better than nature models; the randomisation layer herein itself does not significantly reduce the model's performance. Secondly, from Fig.  5c , we can see that our adversarial training strategy is successful, helping models fight against the adversarial attacks, besides DeepFool. Thirdly, by comparing Fig.  5c  and Fig.  5d , we can observe that randomisation can further help mitigate the effects of adversarial attack, especially for nature models under FGSM, PGD and DeepFool attacks and adversarial federated learnt models under DeepFool attacks. However, we do not see an improvement for the federated adversarial models under FGSM and PGD attacks 2. https://www.nvidia.com/en-us/data-center/dgx-station-a100 after randomisation. Our evidence supports that adversarial training is the key step in defending the models against the FGSM and PDG attacks under our settings, which have strong transferability. On the other hand, DeepFool is more likely to over-fit on the target models, and thus randomisation can better help to destroy the structure of the perturbations. From Table  3 , there are several observations. Firstly, for both the nature models and federated adversarial trained models, randomisation is successful in mitigating the adversarial attacks. When we apply randomisation onto the original test dataset, the performance only drops by a small amount (usually less than absolute 1 % on UAR). When randomisation is applied on the generated adversarial examples, it improves considerably the performance. This is because randomisation probably destroys the specific adversarial perturbations pattern  [45] . Specifically, randomisation helps the models mitigate the DeepFool attack, improving the UAR from 2.96 % to 71.34 % (p < 0.001 in a onetailed z-test) for the nature model and from 1.98 % to 72.32 % for the adversarial federated learnt model (p < 0.001 in a one-tailed z-test). Moreover, the UAR improvements for nature models under FGSM (from 20.82 % to 47.91 %) and PGD (9.96 % to 41.55 %) attacks are also significant (p < 0.001 in a one-tailed z-test).\n\nSecondly, with adversarial training, models' performances under the FGSM (20.82 % to 89.72 %) and PGD (9.96 % to 87.31 %) attacks improve a lot (p < 0.001 in a one-tailed ztest). This result is consistent with existing work in the literature, where single-step attacks (e. g., FGSM) and PGD were shown to be more transferable and less prone towards over-fitting  [45] . However, we observe that the adversarial training does not help the models to mitigate the DeepFool adversarial effects. The reason is that DeepFool iterates until the generated attack samples successfully fool the classifier, in which process DeepFool is more likely to over-fit on the specific models, and has less transferability  [45] .\n\nThe confusion matrices of adversarial federated learnt models' performances on the randomised adversarial test dataset after 300 rounds can be referred to in Fig.  6 . From Fig.  6a  and Fig.  6b , we can see that most test samples are labeled correctly. However, in Fig.  6c , many samples are misclassified as the Disgust emotionespecially the fear emotion. This can be caused by the somewhat similar acoustic characteristics of these negative feelings.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Discussion And Future Work",
      "text": "Our study demonstrates a novel pipeline that can be used to efficiently mitigate adversarial white-box attack on SER data. In this section we consider: the main takeaways from our work with respect to the performance of our two layer defence against adversarial attacks; a comparison with work on centralised training on DEMoS; and finally, considerations regarding future work that could be conducted with our pipeline. White-box attack comparison: In our work, we analyse two types of adversarial white-box attacks, single-step attacks (i. e., FGSM) and iterative attacks (e. g., DeepFool). Overall, our findings are consistent with previous work on adversarial attacks.\n\nAfter conducting the experiments, we come to the conclusion that single-step attacks are weaker than iterative attacks, but more transferable  [45] . By comparing the effects of adversarial training on defending against single-step and iterative attacks, we observe that adversarial training mitigates better the effects of singlestep attacks (i. e., FGSM) than iterative attacks (e. g., DeepFool). We believe that the cause for this is due to the nature of the iterative attacks, which perform multiple iterations to find the minimal perturbations that can fool the classifier. In other words, perturbations generated by iterative attacks tend to over-fit on the parameters of the specific classifier  [45] , which makes the iterative attacks more powerful. Thus, adversarial training doesn't effectively defend against the iterative attacks.\n\nAs a result, we propose the randomisation -the second defence strategy within our pipeline -to be more effective, via utilising resizing and padding by random proportions on the test data at inference time. By this, randomisation tends to destroy the specific structures of perturbations. Our experimental results agree on the previous statement, and show that the added randomisation makes the models more robust against iterative attacks, where DeepFool attacks are applied.\n\nComparison with centralised training: The VGG model architecture that we use in our experiments is similar to the one utilised in the study of  [22] . The use-case was for the centralised adversarial training on the DEMoS corpus in order to reduce the impact of the FGSM attack where a highest UAR -86.7 % was achieved on the unperturbed original test data and 82.1 % on the perturbed test data. However, in that work, data is split to training, developing &testing, in a speaker-independent way, where training data contains a set of speakers that were not present in the test data. In our work, on the other hand, the data is split in a speakerdependent way. As a result of the splitting strategy that our FL workflow restricts by design, a direct comparison between our results and those from  [22]  cannot be made.\n\nFuture work: In the future, our work can be extended and further evaluated, below we discuss three directions that we are particularly keen to explore. Firstly, in real-life communication scenarios, clients (e. g., mobile devices) are distributed in various environments and with different languages. And potentially, each client can have a different dataset. This raises the research question of how our framework will perform and how robust can the learnt FL model be in such heterogeneous environment.\n\nSecondly, if attackers use an ensemble of adversarial examples generated by different algorithms, how does our workflow defend against the unseen adversarial attacks? and to what extend does it generalise over these new attacks.\n\nThirdly, there are also numerous opportunities for applying our pipeline within a different domain. For instance, in the context of intelligent industrial production, where the datasets are recorded on industrial production machines (e. g., computer numerical control machines) via acoustic and vibration analytic devices. How transferable is our federated adversarial learning framework to this domain, and how does it perform and defend against adversarial attacks in such a scenario?",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this article, we proposed a framework that is composed of two defence stages, the first is the federated adversarial training that at the training time and the other one at the inference time, to mitigate white-box adversarial effects on the federated learnt speech emotion recognition models. We conducted experiments on the database of elicited mood in speech using the VGG-15 architecture against three attack methods. The experimental results indicated that adversarial federated learning can better fight against attacks with higher transferability and that randomisation works better on stronger iterative DeepFool attacks. By combining the two defence strategies together, we achieved significant improvement under adversarial attacks. In particular, the unweighted average recalls on the randomised adversarial test dataset were 90.15 % for the fast gradient sign method attack, 89.20 % for the projected gradient attack and 72.32 % for the DeepFool attack. These findings make federated learning in speech emotion recognition appear a sound option to help collect sufficient experience for tomorrow's engines in real-world applications.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Under the assumption",
      "page": 4
    },
    {
      "caption": "Figure 1: (a), adversarial federated learning is the ﬁrst stage of",
      "page": 4
    },
    {
      "caption": "Figure 1: (b), two randomisation layers at the inference",
      "page": 4
    },
    {
      "caption": "Figure 1: Overview of the proposed framework. Please refer to Table 1 for notations. 1.) Split data Xi to X ′",
      "page": 5
    },
    {
      "caption": "Figure 2: One example of a log Mel spectrogram from the DEMoS",
      "page": 6
    },
    {
      "caption": "Figure 3: The VGG-15 architecture. Speciﬁcally, building blocks",
      "page": 6
    },
    {
      "caption": "Figure 2: It can be",
      "page": 6
    },
    {
      "caption": "Figure 4: Performance comparison on test data between a vanilla",
      "page": 7
    },
    {
      "caption": "Figure 4: illustrates the UAR score of the trained FL models,",
      "page": 7
    },
    {
      "caption": "Figure 5: , there are three major ﬁndings. Firstly, Fig. 5a",
      "page": 7
    },
    {
      "caption": "Figure 5: b show that adversarial learnt models perform better",
      "page": 7
    },
    {
      "caption": "Figure 5: c, we can see that our adversarial training strategy is",
      "page": 7
    },
    {
      "caption": "Figure 5: c and Fig. 5d, we",
      "page": 7
    },
    {
      "caption": "Figure 6: From Fig. 6a and Fig. 6b, we",
      "page": 7
    },
    {
      "caption": "Figure 6: c, many samples are misclassiﬁed as the Disgust emotion –",
      "page": 7
    },
    {
      "caption": "Figure 5: Performance on DEMoS every 10 rounds until 300 rounds. (a) The results of models tested on the original test dataset; (b) The",
      "page": 8
    },
    {
      "caption": "Figure 6: Confusion matrices for adversarial training models under",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Notations\n(X , Y)": "(x, y)",
          "Description\nA set of samples X with corresponding labels Y.": "We deﬁne an individual sample-label pair as (x, y) where x ∈ X and y ∈ Y respectively."
        },
        {
          "Notations\n(X , Y)": "I",
          "Description\nA set of samples X with corresponding labels Y.": "Sample IDs."
        },
        {
          "Notations\n(X , Y)": "F",
          "Description\nA set of samples X with corresponding labels Y.": "A set of data owners (federated learning clients)."
        },
        {
          "Notations\n(X , Y)": "Di",
          "Description\nA set of samples X with corresponding labels Y.": "Dataset, or in the context of federated learning a set of datasets, where each Di belongs to a data owner Fi."
        },
        {
          "Notations\n(X , Y)": "M",
          "Description\nA set of samples X with corresponding labels Y.": "A machine learning model such as a deep neural network."
        },
        {
          "Notations\n(X , Y)": "θ, ϑ",
          "Description\nA set of samples X with corresponding labels Y.": "Parameters (weights) of a machine learning model."
        },
        {
          "Notations\n(X , Y)": "V",
          "Description\nA set of samples X with corresponding labels Y.": "Performance measure for a machine learning model M, for instance accuracy."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "#": "Anger",
          "Train": "1,155",
          "Test": "322",
          "(cid:80)": "1,477"
        },
        {
          "#": "Disgust",
          "Train": "1,354",
          "Test": "324",
          "(cid:80)": "1,678"
        },
        {
          "#": "Fear",
          "Train": "927",
          "Test": "229",
          "(cid:80)": "1,156"
        },
        {
          "#": "Guilt",
          "Train": "898",
          "Test": "231",
          "(cid:80)": "1,129"
        },
        {
          "#": "Happiness",
          "Train": "1,127",
          "Test": "268",
          "(cid:80)": "1,395"
        },
        {
          "#": "Sadness",
          "Train": "1,228",
          "Test": "302",
          "(cid:80)": "1,530"
        },
        {
          "#": "Surprise",
          "Train": "802",
          "Test": "198",
          "(cid:80)": "1,000"
        },
        {
          "#": "(cid:80)",
          "Train": "7,491",
          "Test": "1,874",
          "(cid:80)": "9,365"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: , there are several observations. Firstly, for",
      "data": [
        {
          "Models": "",
          "Test data": "",
          "Attacks": "No"
        },
        {
          "Models": "Nature",
          "Test data": "Original",
          "Attacks": "94.08"
        },
        {
          "Models": "",
          "Test data": "Randomised",
          "Attacks": "92.75"
        },
        {
          "Models": "",
          "Test data": "Adversarial",
          "Attacks": "-"
        },
        {
          "Models": "",
          "Test data": "Randomised adversarial",
          "Attacks": "-"
        },
        {
          "Models": "Adversarial\nfederated\nlearnt",
          "Test data": "Original",
          "Attacks": "-"
        },
        {
          "Models": "",
          "Test data": "Randomised",
          "Attacks": "-"
        },
        {
          "Models": "",
          "Test data": "Adversarial",
          "Attacks": "-"
        },
        {
          "Models": "",
          "Test data": "Randomised adversarial",
          "Attacks": "-"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "2",
      "title": "Overview and importance of data quality for machine learning tasks",
      "authors": [
        "A Jain",
        "H Patel",
        "L Nagalapatti",
        "N Gupta",
        "S Mehta",
        "S Guttula",
        "S Mujumdar",
        "S Afzal",
        "R Mittal",
        "V Munigala"
      ],
      "year": "2020",
      "venue": "Proc. KDD, virtual event"
    },
    {
      "citation_id": "3",
      "title": "Machine learning in major depression: From classification to treatment outcome prediction",
      "authors": [
        "S Gao",
        "V Calhoun",
        "J Sui"
      ],
      "year": "2018",
      "venue": "CNS neuroscience & therapeutics"
    },
    {
      "citation_id": "4",
      "title": "Anti leakage: Protecting privacy hidden in our speech",
      "authors": [
        "H Zhu",
        "Y Zhang",
        "X Guo",
        "X.-Y Li"
      ],
      "year": "2021",
      "venue": "Proc. BigCom, Virtual event"
    },
    {
      "citation_id": "5",
      "title": "Federated learning: Challenges, methods, and future directions",
      "authors": [
        "T Li",
        "A Sahu",
        "A Talwalkar",
        "V Smith"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "6",
      "title": "Simple black-box adversarial attacks on deep neural networks",
      "authors": [
        "N Narodytska",
        "S Kasiviswanathan"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "7",
      "title": "FDA 3 : Federated defense against adversarial attacks for cloud-based IIoT applications",
      "authors": [
        "Y Song",
        "T Liu",
        "T Wei",
        "X Wang",
        "Z Tao",
        "M Chen"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "8",
      "title": "A survey on automatic multimodal emotion recognition in the wild",
      "authors": [
        "G Sharma",
        "A Dhall"
      ],
      "year": "2021",
      "venue": "Advances in Data Science: Methodologies and Applications"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition in naturalistic speech and language -a survey",
      "authors": [
        "F Weninger",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "Emotion Recognition: A Pattern Analysis Approach"
    },
    {
      "citation_id": "10",
      "title": "Serbian emotional speech database: design, processing and evaluation",
      "authors": [
        "S Jovicic",
        "Z Kasic",
        "M Dordevic",
        "M Rajkovic"
      ],
      "year": "2004",
      "venue": "Proc. SPECOM"
    },
    {
      "citation_id": "11",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "13",
      "title": "Expression and the nature of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1984",
      "venue": "Approaches to emotion"
    },
    {
      "citation_id": "14",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "15",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "16",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "The INTERSPEECH 2020 computational paralinguistics challenge: Elderly emotion, breathing & masks",
      "authors": [
        "B Schuller",
        "A Batliner",
        "C Bergler",
        "E.-M Messner",
        "A Hamilton",
        "S Amiriparian",
        "A Baird",
        "G Rizos",
        "M Schmitt",
        "L Stappen"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "18",
      "title": "Emonet: A transfer learning framework for multi-corpus speech emotion recognition",
      "authors": [
        "M Gerczuk",
        "S Amiriparian",
        "S Ottl",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Learning deep multimodal affective features for spontaneous speech emotion recognition",
      "authors": [
        "S Zhang",
        "X Tao",
        "Y Chuang",
        "X Zhao"
      ],
      "year": "2021",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "Proc. PlatCon"
    },
    {
      "citation_id": "22",
      "title": "Generating and protecting against adversarial attacks for deep speech-based emotion recognition models",
      "authors": [
        "Z Ren",
        "A Baird",
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "24",
      "title": "Speaker recognition system using dynamic time warping matching and mel-scale frequency cepstral coefficients",
      "authors": [
        "Y Xue"
      ],
      "year": "2020",
      "venue": "Proc. CSPS, Changbaishan, China"
    },
    {
      "citation_id": "25",
      "title": "Caa-net: Conditional atrous cnns with attention for explainable device-robust acoustic scene classification",
      "authors": [
        "Z Ren",
        "Q Kong",
        "J Han",
        "M Plumbley",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "Failure prediction in production line based on federated learning: an empirical study",
      "authors": [
        "N Ge",
        "G Li",
        "L Zhang",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "Journal of Intelligent Manufacturing"
    },
    {
      "citation_id": "27",
      "title": "Federated learning for healthcare informatics",
      "authors": [
        "J Xu",
        "B Glicksberg",
        "C Su",
        "P Walker",
        "J Bian",
        "F Wang"
      ],
      "year": "2021",
      "venue": "Journal of Healthcare Informatics Research"
    },
    {
      "citation_id": "28",
      "title": "Towards federated learning at scale: System design",
      "authors": [
        "K Bonawitz",
        "H Eichner",
        "W Grieskamp",
        "D Huba",
        "A Ingerman",
        "V Ivanov",
        "C Kiddon",
        "J Konečnỳ",
        "S Mazzocchi",
        "H Mcmahan"
      ],
      "year": "2019",
      "venue": "Proc. SysML"
    },
    {
      "citation_id": "29",
      "title": "Federated learning: Strategies for improving communication efficiency",
      "authors": [
        "J Konečnỳ",
        "H Mcmahan",
        "F Yu",
        "P Richtárik",
        "A Suresh",
        "D Bacon"
      ],
      "year": "2016",
      "venue": "Proc. NIPS"
    },
    {
      "citation_id": "30",
      "title": "Adversarial example attacks in the physical world",
      "authors": [
        "H Ren",
        "T Huang"
      ],
      "year": "2021",
      "venue": "International Journal of Machine Learning and Cybernetics"
    },
    {
      "citation_id": "31",
      "title": "Enhancing transferability of black-box adversarial attacks via lifelong learning for speech emotion recognition models",
      "authors": [
        "Z Ren",
        "J Han",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "32",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "33",
      "title": "Towards deep learning models resistant to adversarial attacks",
      "authors": [
        "A Madry",
        "A Makelov",
        "L Schmidt",
        "D Tsipras",
        "A Vladu"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "34",
      "title": "Adversarial attacks on spoofing countermeasures of automatic speaker verification",
      "authors": [
        "S Liu",
        "H Wu",
        "H -Y. Lee",
        "H Meng"
      ],
      "year": "2019",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "35",
      "title": "DeepFool: a simple and accurate method to fool deep neural networks",
      "authors": [
        "S.-M Moosavi-Dezfooli",
        "A Fawzi",
        "P Frossard"
      ],
      "year": "2016",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "36",
      "title": "Intriguing properties of neural networks",
      "authors": [
        "C Szegedy",
        "W Zaremba",
        "I Sutskever",
        "J Bruna",
        "D Erhan",
        "I Goodfellow",
        "R Fergus"
      ],
      "year": "2014",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "37",
      "title": "Adversarial machine learning at scale",
      "authors": [
        "A Kurakin",
        "I Goodfellow",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "38",
      "title": "Ensemble adversarial training: Attacks and defenses",
      "authors": [
        "F Tramèr",
        "A Kurakin",
        "N Papernot",
        "I Goodfellow",
        "D Boneh",
        "P Mcdaniel"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "39",
      "title": "MMA training: Direct input space margin maximization through adversarial training",
      "authors": [
        "G Ding",
        "Y Sharma",
        "K Lui",
        "R Huang"
      ],
      "year": "2020",
      "venue": "Proc. ICLR, Virtual event"
    },
    {
      "citation_id": "40",
      "title": "Unlabeled data improves adversarial robustness",
      "authors": [
        "Y Carmon",
        "A Raghunathan",
        "L Schmidt",
        "P Liang",
        "J Duchi"
      ],
      "year": "2019",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "41",
      "title": "Are labels required for improving adversarial robustness",
      "authors": [
        "J Uesato",
        "J.-B Alayrac",
        "P.-S Huang",
        "R Stanforth",
        "A Fawzi",
        "P Kohli"
      ],
      "year": "2019",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "42",
      "title": "Defense against adversarial attacks using high-level representation guided denoiser",
      "authors": [
        "F Liao",
        "M Liang",
        "Y Dong",
        "T Pang",
        "X Hu",
        "J Zhu"
      ],
      "year": "2018",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "43",
      "title": "Feature denoising for improving adversarial robustness",
      "authors": [
        "C Xie",
        "Y Wu",
        "L Maaten",
        "A Yuille",
        "K He"
      ],
      "year": "2019",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "44",
      "title": "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
      "authors": [
        "A Athalye",
        "N Carlini",
        "D Wagner"
      ],
      "year": "2018",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "45",
      "title": "Mitigating adversarial effects through randomization",
      "authors": [
        "C Xie",
        "J Wang",
        "Z Zhang",
        "Z Ren",
        "A Yuille"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "46",
      "title": "Federated learning for speech emotion recognition applications",
      "authors": [
        "S Latif",
        "S Khalifa",
        "R Rana",
        "R Jurdak"
      ],
      "year": "2020",
      "venue": "Proc. IPSN"
    },
    {
      "citation_id": "47",
      "title": "Privacy is what we care about: Experimental investigation of federated learning on edge devices",
      "authors": [
        "A Das",
        "T Brunschwiler"
      ],
      "year": "2019",
      "venue": "Proc. AIChallengeIoT"
    },
    {
      "citation_id": "48",
      "title": "Sustainability of healthcare data analysis iot-based systems using deep federated learning",
      "authors": [
        "E Haya",
        "A Moayad",
        "G Mohsen"
      ],
      "year": "2021",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "49",
      "title": "Federated learning meets human emotions: A decentralized framework for human-computer interaction for iot applications",
      "authors": [
        "P Chhikara",
        "P Singh",
        "R Tekchandani",
        "N Kumar",
        "M Guizani"
      ],
      "year": "2021",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "50",
      "title": "Federated learning application on depression treatment robots (dtbot)",
      "authors": [
        "Y Liu",
        "R Yang"
      ],
      "year": "2021",
      "venue": "Proc. ICCRD"
    },
    {
      "citation_id": "51",
      "title": "Privacy attacks for automatic speech recognition acoustic models in a federated learning framework",
      "authors": [
        "N Tomashenko",
        "S Mdhaffar",
        "M Tommasi",
        "Y Estève",
        "J.-F Bonastre"
      ],
      "year": "2022",
      "venue": "Privacy attacks for automatic speech recognition acoustic models in a federated learning framework",
      "arxiv": "arXiv:2111.03777"
    },
    {
      "citation_id": "52",
      "title": "Poisoning attack in federated learning using generative adversarial nets",
      "authors": [
        "J Zhang",
        "J Chen",
        "D Wu",
        "B Chen",
        "S Yu"
      ],
      "year": "2019",
      "venue": "Proc. Trust-Com/BigDataSE"
    },
    {
      "citation_id": "53",
      "title": "Federated learning in mobile edge networks: A comprehensive survey",
      "authors": [
        "W Lim",
        "N Luong",
        "D Hoang",
        "Y Jiao",
        "Y.-C Liang",
        "Q Yang",
        "D Niyato",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Communications Surveys Tutorials"
    },
    {
      "citation_id": "54",
      "title": "Crafting adversarial examples for speech paralinguistics applications",
      "authors": [
        "Y Gong",
        "C Poellabauer"
      ],
      "year": "2017",
      "venue": "Proc. DYNAMICS"
    },
    {
      "citation_id": "55",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Proc. NIPS"
    },
    {
      "citation_id": "56",
      "title": "Improving adversarial robustness via promoting ensemble diversity",
      "authors": [
        "T Pang",
        "K Xu",
        "C Du",
        "N Chen",
        "J Zhu"
      ],
      "year": "2019",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "57",
      "title": "Federated machine learning: Concept and applications",
      "authors": [
        "Q Yang",
        "Y Liu",
        "T Chen",
        "Y Tong"
      ],
      "year": "2019",
      "venue": "ACM Trans. Intell. Syst. Technol"
    },
    {
      "citation_id": "58",
      "title": "Membership inference attacks against machine learning models",
      "authors": [
        "R Shokri",
        "M Stronati",
        "C Song",
        "V Shmatikov"
      ],
      "year": "2017",
      "venue": "Proc. SP"
    },
    {
      "citation_id": "59",
      "title": "A survey on security and privacy of federated learning",
      "authors": [
        "V Mothukuri",
        "R Parizi",
        "S Pouriyeh",
        "Y Huang",
        "A Dehghantanha",
        "G Srivastava"
      ],
      "year": "2021",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "60",
      "title": "Generating and protecting against adversarial attacks for deep speech-based emotion recognition models",
      "authors": [
        "Z Ren",
        "A Baird",
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc İCASSP"
    },
    {
      "citation_id": "61",
      "title": "Demos: An italian emotional speech corpus",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "62",
      "title": "Covnet: A transfer learning framework for automatic covid-19 detection from crowd-sourced cough sounds",
      "authors": [
        "Y Chang",
        "X Jing",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Frontiers in Digital Health"
    },
    {
      "citation_id": "63",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "V Nair",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Proc. ICML, Madison, WI"
    },
    {
      "citation_id": "64",
      "title": "Attentionbased convolutional neural networks for acoustic scene classification",
      "authors": [
        "Z Ren",
        "Q Kong",
        "K Qian",
        "M Plumbley",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "DCASE"
    },
    {
      "citation_id": "65",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "66",
      "title": "Flower: A friendly federated learning research framework",
      "authors": [
        "D Beutel",
        "T Topal",
        "A Mathur",
        "X Qiu",
        "T Parcollet",
        "P De Gusmão",
        "N Lane"
      ],
      "year": "2020",
      "venue": "Flower: A friendly federated learning research framework",
      "arxiv": "arXiv:2104.06523"
    },
    {
      "citation_id": "67",
      "title": "",
      "authors": [
        "M.-I Nicolae",
        "M Sinn",
        "M Tran",
        "B Buesser",
        "A Rawat",
        "M Wistuba",
        "V Zantedeschi",
        "N Baracaldo",
        "B Chen",
        "H Ludwig",
        "I Molloy",
        "B Edwards"
      ],
      "venue": ""
    }
  ]
}