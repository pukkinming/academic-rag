{
  "paper_id": "2208.04994v1",
  "title": "Generative Data Augmentation Guided By Triplet Loss For Speech Emotion Recognition",
  "published": "2022-08-09T18:39:42Z",
  "authors": [
    "Shijun Wang",
    "Hamed Hemati",
    "Jón Guðnason",
    "Damian Borth"
  ],
  "keywords": [
    "speech emotion recognition",
    "speech augmentation",
    "cross lingual"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is crucial for humancomputer interaction but still remains a challenging problem because of two major obstacles: data scarcity and imbalance. Many datasets for SER are substantially imbalanced, where data utterances of one class (most often Neutral) are much more frequent than those of other classes. Furthermore, only a few data resources are available for many existing spoken languages. To address these problems, we exploit a GAN-based augmentation model guided by a triplet network, to improve SER performance given imbalanced and insufficient training data. We conduct experiments and demonstrate: 1) With a highly imbalanced dataset, our augmentation strategy significantly improves the SER performance (+8% recall score compared with the baseline). 2) Moreover, in a cross-lingual benchmark, where we train a model with enough source language utterances but very few target language utterances (around 50 in our experiments), our augmentation strategy brings benefits for the SER performance of all three target languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is a task aiming to understand the underlying emotional information in speech. In recent years, there has been a surge of interest in SER, because emotion conveys crucial information during human-computer interaction. However, two obstacles usually hinder the application of SER: the imbalance and scarcity issues of the SER dataset. We can further describe the issues from two perspectives.\n\nFirstly, SER datasets have a common issue that the speech data distributions are non-uniform or highly skewed among different emotion classes. Speech utterances labeled as \"Neutral\" are much more frequent than those labeled as \"Happy, Angry, etc.\". The data scarcity of various emotion classes results in highly imbalanced datasets. To alleviate the data imbalance issue, a common way is to generate synthetic data through data augmentation techniques. Authors in  [1]  use vanilla Generative Adversarial Networks (GANs)  [2]  to generate synthetic openSMILE acoustic features  [3]  for the training of the SER task. In  [4] , the authors employ a CycleGAN  [5]  to transfer openSMILE feature vectors from one emotion class to another. Instead of openSMILE features, some approaches do augmentation on low-level acoustic features (e.g. Mel-Spectrogram), which is an advantage for other tasks. Approaches in  [6, 7]  apply a starGAN  [8]  to convert speech utterances of Neutral emotion to other emotions. Authors in  [9]  modify a GAN to perform augmentation for imbalanced data and they empirically demonstrate that this GAN can improve the SER performance under imbalanced datasets.\n\nAnother important aspect to consider the scarcity issue is the availability of data for various existing spoken languages. There are very few data resources available for many languages, which is a significant barrier to the research and application of SER for these languages. One way to address this problem is by training an SER model on one or more languages with enough data (source), combined with very few training utterances of other languages (target). The idea is to use the knowledge gained from source languages to improve the performance on low-resource target languages  [10] . Such SER training strategy that spans different languages, i.e. cross-lingual SER, has been widely studied  [11, 12, 13] . Although adding source languages improves the SER performance on target languages, there is still room to improve the cross-lingual SER task.\n\nIn this paper, We propose an augmentation strategy, which can be used to address the data imbalance and scarcity problem. One component of our augmentation approach is a GAN, whose generator is employed for augmentation. The generator is inspired by  [14] , where authors successfully perform automatic augmentation for multiple downstream tasks. To stabilize the GAN or improve its performance, additional auxiliary objectives are usually introduced during training  [15, 16, 17] . Thus, we apply a representation learner based on triplet loss to learn emotion representations from the original data. And the learned knowledge of the representation learner is used to increase and stabilize the augmentation performance for the SER task.\n\nWe summarize our contributions as follows: 1) We propose a GAN-based augmentation for the SER task. 2) We demonstrate that our augmentation approach can largely improve the SER performance with a highly imbalanced dataset (+8% recall with our augmentation). 3) Moreover, for the cross-lingual SER task, our augmentation improves the performance for three different target languages with only around 50 utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "The proposed SER data augmentation method is designed with two augmentation requirements: preserving the emotion class and providing rich variance. This means that an augmented utterance should have the same emotion label as the original one while also differing sufficiently from it. Our system setup and corresponding loss functions reflect these requirements.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Components",
      "text": "Our approach consists of a GAN and a representation learner. The generator in our GAN augment the Mel-Spectrograms. The representation learner enables the GAN to present high augmentation variance and stabilize the augmentation.\n\nThe GAN is shown on the top of Fig.  1 , it contains one generator and one discriminator, while we denote the inputted Mel- B I e q l l s r r w / 9 1 3 f r j s N Z w G 0 T t y S 1 K F E 2 7 e / h m F C s p g K T T h W a u A 6 q f Z y L D U j n M 5 r w 0 z R F J M J H t G B o Q L H V H n 5 I v k c X R g l R F E i z R E a L d T f G z m O V R H O T M Z Y j 9 W q V 4 j / e Y N M R 7 d e z k S a a S r I 8 q E o 4 0 g n q K g B h U x S o v n M E E w k M 1 k R G W O J i T Z l 1 U w J 7 u q X 1 0 n 3 q u F e N 5 q P z X r r r q y j C m d w D p f g w g 2 0 4 A H a 0 A E C U 3 i G V 3 i z c u v F e r c + l q M V q 9 w 5 h T + w P n 8 A z e C T x w = = < / l a t e x i t >            The training pipeline of our representation learner. BOTTOM:\n\nThe loss to guide the augmentor to preserve the emotion information.\n\nSpectrograms as X. In the rest of the paper, we refer to the generator as the augmentor since it is used to perform augmentation on Mel-Spectrograms. The augmentor employs an encoder to map a Mel-Spectrogram X1 into a latent vector z, given a Gaussian noise n. Then, z is fed to the decoder to generate a tensor P with the same shape as X1. To avoid excessive removal of features from the original Mel-Spectrogram, we constrain the decoder output P with an l1-norm operation as used in  [14] .\n\nWe then use a hyper-parameter to control the augmentation intensity and obtain the augmented Mel-Spectrogram X1. For brevity, in this paper, we denote our augmentor as AU G(X).\n\nFinally, we feed the discriminator D(X) with X = X1 or X2, where X2 is another sample from the original dataset. D(X) aims to determine whether the input is augmented or not. Overall, we apply the non-saturating loss  [2]  to train our GAN,\n\nTo achieve better augmentation, we further implement a representation learner. Its target is to learn meaningful emotion representations that can later be used to improve our augmentor. In contrast to a classification loss, a triplet loss is applied to train more discriminative emotion representations  [18, 19] . As illustrated in the middle of Fig.  1 , our representation learner maps the Mel-Spectrogram X into an emotion representation vector r. The objective is to attract the representations of XA (Anchor) and XP (Positive), whose emotion classes are the same, while repelling representations of XA and XN (Negative), since they are from different emotion classes. We define the triplet loss of our representation learner as,\n\nwhere dist(•, •) is l1 distance, and β is the margin of the triplet loss. The margin is a hyper-parameter that defines how far away the dissimilarities should be. The representation learner is then employed to guide the augmentor to preserve the emotion class after the augmentation. Thus, we introduce the loss LEMO to penalize the augmentor for generating an output that is of different emotion class to the input. The setup is shown at the bottom of Fig.  1 . As we can see, we freeze the representation learner, and an augmented Mel-Spectrogram XP is used to supersede XP as one of the inputs for the triplet loss. The motivation is to use the knowledge repel shared < l a t e x i t s h a 1 _ b a s e 6 4 = \" 0 O 1 I s q j k R y D 8 F u o V s e l s T q Q f J R 0 = \" > A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I q M u i G 5 c V 7 A P a s W Q y m T Y 0 k w x J R i l D / 8 O N C 0 X c + i / u / B s z 7 S y 0 9 U D I 4 Z x 7 y c k J E s 6 0 c d 1 v p 7 S y u r a + U d 6 s b G 3\n\nm 8 g j f r y X q x 3 q 2 P R W n F K n u O w R 9 Y n z 8 E N J j 7 < / l a t e x i t > Xaug1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" W L Z 9 G w d 0   O c g q 8 X J S g R y N f v m r N 4 h p G j F p q E C t u 5 6 b G D 9 D Z T g V b F r q p Z o l S M c 4 Z F 1 L J U Z M + 9 n 8 3 i k 5 s 8 q A h L G y J Q 2 Z q 7 8 n M o y 0 n k S B 7 Y z Q j P S y N x P / 8 7 q p C a / 9 j M s k N U z S x a I w F c T E Z P Y 8 G X D F q B E T S 5 A q b m 8 l d I Q K q b E R l W w I 3 v L L q 6 R 1 U f U u q 7 X 7 W q V + k 8 d R h B M 4 h X P w 4 A r q c A c N a A I F A c / w C m / O o / P i v D s f i 9 a C k 8 8 c w x 8 4 n z 8 a 2 J A G < / l a t e x i t >\n\nW h U T g r v 8 8 i r p n N f d i 3 r j r l F r X h d x l N E x O k F n y E W X q I l u U Q u 1 E U W P 6 B m 9 o j f r y X q x 3 q 2 P R W v J K m a O 0 B 9 Y n z / t p 5 R x < / l a t e x i t > ✏ aug1     of the representation learner to guide the augmentor to preserve emotional information. We define the loss as,",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "< L A T E X I T S H A 1 _ B A S E 6 4 = \" A J H 7 8 K S D A U G O U S C 6 G 7 A V S H I S Y 9 8 = \" >",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "O Z G N 5 H X A F N A N O A K Y 5 3 C C S S B R W 6 A W T 6 5 N F E W C P W C Z U 9 D Q B L Y I J W U J G I T A S B 1 E Z A S S K 8 V J K F K B S U S P 3 7 Z P T D + B",
      "text": "we define the model loss as,\n\nwhere L M odel are weighted by wg, wr and we.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Auxiliary Objectives For Augmentation",
      "text": "To achieve better augmentation, apply the representation learner to provide auxiliary objectives during training. We first expect the augmentor can provide augmentation variance. In other words, for the same Mel-Spectrogram, the augmentaor can produce various augmented versions. Otherwise, identical mapping or barely perceivable augmentation of the augmentaor might not be helpful for the SER task. Thus, as shown on the top of Fig.  2 , for the same input X, we first obtain two augmented Mel-Spectrograms Xaug1 and Xaug2 based on different noise n and augmentation intensity . We then send these two augmented Mel-Spectrograms into the representation learner to acquire representations raug1 and raug2. We freeze the representation learner as well in order to utilize its knowledge of the original data. Our target is to repel these two representations, which can force the augmentor to generate various augmented versions whose representations are dissimilar, even from the same input. We define the auxiliary variance loss as,\n\nWe enforce the dissimilarity by minimizing the dot product (dot(•, •)) between the representations from two augmented Mel-Spectrograms. However, our initial experiments showed that LV AR leads to unstable augmentation, we observe this issue by t-SNE visualisation  [20] . As shown on the left in Fig.  3 , each point depicts the emotion representation r derives from Mel-Spectrogram in the dataset IEMOCAP  [21] . For each emotion class, we randomly pick 30 speech utterances and augment each sample 3 times with different noise and augmentation intensities. In the figure, the augmented representations from different emotions are not distinct and do not form clusters. The reason is that to meet the augmentation variance loss LV AR, the augmentor might perform arbitrarily to generate augmented versions whose representations are excessively far from original data, which means the emotion class information is damaged. Such behavior is against the loss LEMO (Eq. 3, aiming to preserve the emotion information), however, since the dataset is small, the augmentor is less penalized by the original data.\n\nTo enable our augmentor to preserve the emotion information while providing variant augmentations, we need to balance LEMO (Eq. 3) and LV AR (Eq. 5). Rather than doing tedious fine-tuning of the weights of these two losses, we achieve the balance by introducing another auxiliary loss as shown at the bottom of Fig.  2 . As we can see, another triplet loss is applied, but the positive and the negative inputs are both augmented. The motivation is to prevent the representations of the augmented Mel-Spectrograms from tangling together, by constraining the augmentor not only with original data, but also with augmented versions themselves. In other words, the augmented versions should be close to original versions if they belong to the same emotion, while be far from other augmented version when they from different emotion classes. Therefore, the triplet loss is defined as, LBAL = max(dist(rA, rP )dist(rA, rN ) + β, 0).\n\nAs we can observe from the t-SNE visualisation on the right of Fig.  3 , with the addition of the balancing loss, the augmented representations have a clear boundary between different classes. We conduct ablation study to demonstrate the contributions of our proposed losses further. Finally, we define the total loss as,\n\nwhere wv and w b are loss weights.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training Phases",
      "text": "We repeat the following steps until our models converge: 1) Representation Learner: We update our representation learner by LREP (Eq. 2) from original data X.\n\n2) Discriminator: We freeze the augmentor, and obtain augmented Mel-Spectrograms X from original data X. The discriminator D is updated by X and X.\n\n3) Augmentation Variance: We freeze the representation learner, and utilize it to enhance the augmentation variance of the augmentor, by the loss LV AR (Eq. 5).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4) Emotion Preservation And Balance:",
      "text": "We update the augmentor. We freeze the representation learner, the discriminator D, then generate augmented Mel-Spectrograms X from the augmentor. The augmentor is finally updated from the discriminator loss, LEMO (Eq. 3) and LBAL (Eq. 6).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "In our augmentor, the encoder consists of 3 convolutional layers with LeakyReLU activation function and maps the input X into a latent 128D vector z. The decoder employs two deconvolutional layers with ReLU activation function to convert z into a tensor P whose shape is the same as the input. The discriminator first uses four convolutional layers with the LeakyReLU activation function to process the input, then uses an LSTM and an attention layer  [22]  to discern whether the input is original or augmented. Our representation learner employs 5 convolutional layers with LeakyReLU and an LSTM to process the Mel-Spectrogram, then the same attention layer in the discriminator is used to produce the 128 dimensional emotion representation.\n\nFor augmentation intensity , we randomly sample from a uniform distribution U(0.05, 0.3), because we found a random performs better than a fixed one as in  [14] . We set the β in LREP (Eq. 2), LEMO (Eq. 3) and LBAL (Eq. 6) as 7. For loss weights, we set wg, wr and wv to 1, we to 10 and w b to 8. We train all of our models with Adam optimizer for 30k iterations, and set the learning rate to 1e-6.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment And Results",
      "text": "The proposed augmentation method is demonstrated with SER tasks. In short, the results show that: 1) Our augmentor can improve the SER with a highly imbalanced dataset. 2) Our augmentor can improve the cross-lingual SER task with very few training utterances from the target language. 3) the importance of the contribution of LV AR (Eq. 5, for adding augmentation variance), and LBAL (Eq. 6, for balancing the emotion preservation and providing variance).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Preprocessing And Ser Classifier",
      "text": "In all our experiments, we use the same acoustic features and same emotion classifier as in  [9] . To extract Mel-Spectrograms from waveform files, we use a 50-millisecond window and a 50 percent overlap ratio, with 128 Mel Coefficients. A VGG19 architecture  [23]  is employed as our emotion classifier, which takes a fix-sized 128 × 128 Mel-Spectrogram (128 frames) as the input and predicts the emotion class. During the evaluation, the predicted class is determined by the majority voting of segments for each Mel-Spectrogram. To train our GAN and representation learner, we use 512 × 128 Mel-Spectrograms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "Imbalanced SER and Ablation Study: For the experiments of the Imbalanced SER (Sec. 3.2.1) and Ablation Study (Sec. 3.2.3) We train and test our models on IEMOCAP  [24] . Like many other SER works  [1, 9, 25] , we only focus on 4 classes (Angry, Sad, Neutral and Happy), resulting in 5531 speech utterances of about 7 hours total duration. Furthermore, in order to simulate the data imbalance issue, we follow  [9]  and randomly remove 80% of each class except Neutral. Since IEMO-CAP is originally split into 5 sessions, we conduct 5 fold crossvalidation, by using 4 sessions for training and 1 for testing. We train our SER classifier with original and augmented utterances. We augment each original training sample four times, resulting in a hybrid original-augmented training dataset that is roughly the same size as the original non-reduced dataset. Cross-Lingual SER: For the experiment of the Cross-Lingual SER (Sec. 3.2.2), our datasets are listed in Tab. 1. These datasets span four languages, with IEMOCAP and ESD serving as source languages and the remaining datasets serving as target languages. Although ESD originally has both Chinese and En- glish speech utterances, we only use the Chinese portion in this experiment. To create IEMOCAP SUB and ESD SUB, we randomly selected utterances from IEMOCAP and ESD, in order to make all of the target language datasets the same size. Since each dataset has different class numbers, one of the consistent ways to investigate cross-lingual SER task is by considering the binary positive/negative valence classification problem, like in many other related works  [11, 12] . We follow the same method to map emotions into valance, and split 25% of the target language datasets for our evaluation. During the training of the SER classifier, we augment each original training sample from the target language 20 times, since the target language's data is severely lacking (only 50 utterances in some experiment cases).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ser On Imbalanced Dataset",
      "text": "Models: We follow  [9]  and train our SER classifier with: NoAUG: the 80% reduced dataset; AUG: the 80% reduced dataset and the augmented utterances.\n\nResults: We list the Unweighted Average Recall (UAR) results reported by  [9]  and ours in Tab. 2. One thing to note is that, although using the identical setup in  [9]  to train our NoAUG model, the results (the 2nd and 4th rows) differ because the 80% removed component is randomly chosen. The average UAR results, on the other hand, are close, implying that our comparison is fair. By comparing the two AUG models, we can see that our augmentation strategy is roughly 5% higher than the augmentation approach in  [9] , which means a better SER performance can be achieved with our augmentation approach.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Lingual Ser",
      "text": "Models: We train our SER classifier with: Low Target (LT), 100% of the source language data but only 10% of the target language dataset; LT AUG, same as LT, but the augmented data is also fed during training; Full Target (FT), 100% of the source language data and 75% of the target; FT AUG, same as FT, but we add the augmented data from our augmentor.\n\nResults: The UAR results on 25% of the target language datasets are shown in Fig.  4 . By comparing with LT and LT AUG, our augmentation approach increases UAR performance by around 5%, given only 10% of the target language data. The performance can be further improved with the augmented data, when the target language data is additionally given, based on the results between FT and FT AUG.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "Models: For ablation study, we use the 80% reduced dataset to train an SER classifier with and without augmentation. To train the augmentor, we consider three cases: 1) with only L M odel ; 2) with L M odel and auxiliary variance loss LV AR; 3) with the total loss that consists of both auxiliary terms LV AR and LBAL.\n\nResults: The UAR results are listed in Tab. 3. We can observe that without LV AR and LBAL, performance can only improve little because the assistance of an augmentor with low augmentation variance is limited. After we apply the loss LV AR, as we mentioned in Sec. 2.2, The augmentor arbitrarily performs augmentation and destroys the emotion information, resulting in a significant fall in performance. Lastly, by introducing LBAL, we can obtain the best result by balancing augmentation variance and emotion preservation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This work presents a GAN-based augmentation approach to alleviate the data imbalance and scarcity issue for the SER task. Specifically, we conduct experiments and demonstrate: 1) Even with a severely imbalanced dataset, our augmentation approach can significantly increase SER performance. 2) With only about 50 training utterances of the target languages provided, our augmentation approach considerably enhances SER performance for these low-resource languages.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , it contains one gen-",
      "page": 1
    },
    {
      "caption": "Figure 1: TOP: The training pipeline of our GAN. MIDDLE:",
      "page": 2
    },
    {
      "caption": "Figure 1: , our representation learner maps",
      "page": 2
    },
    {
      "caption": "Figure 2: TOP: The auxiliary loss to guide the augmentor to",
      "page": 2
    },
    {
      "caption": "Figure 2: , for the same input X, we ﬁrst obtain",
      "page": 2
    },
    {
      "caption": "Figure 3: , each point depicts",
      "page": 2
    },
    {
      "caption": "Figure 3: T-SNE visualization of the emotion representations",
      "page": 3
    },
    {
      "caption": "Figure 2: As we can see, another triplet loss is applied,",
      "page": 3
    },
    {
      "caption": "Figure 3: , with the addition of the balancing loss, the augmented",
      "page": 3
    },
    {
      "caption": "Figure 4: UAR results of cross-lingual SER. TOP: English",
      "page": 4
    },
    {
      "caption": "Figure 4: By comparing with LT and",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RepresentationLearner\nshared\nRepresentationLearner": "",
          "Column_2": "RepresentationLearner",
          "Column_3": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "shared\nRepresentationLearner": "shared"
        },
        {
          "shared\nRepresentationLearner": "RepresentationLearner"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: Ablation study results (UAR) of the contribution of",
      "data": [
        {
          "Column_1": "",
          "Source: IEMOCAP (English)\nLT LT_AUG\nFT FT_AUG\nRAU": "",
          "Column_3": "FT FT_AUG",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": ""
        },
        {
          "Column_1": "",
          "Source: IEMOCAP (English)\nLT LT_AUG\nFT FT_AUG\nRAU": "RAU",
          "Column_3": "LLTT\nFFTT",
          "Column_4": "Chines\nLLTT__AA\nFFTT__AA",
          "Column_5": "e\nUUGG\nUUGG",
          "Column_6": "",
          "Column_7": "Ital\nSource: ES",
          "Column_8": "ian\nD (Chinese",
          "Column_9": ")",
          "Column_10": "",
          "Column_11": "Ger",
          "Column_12": "man",
          "Column_13": "",
          "Column_14": "",
          "Column_15": ""
        },
        {
          "Column_1": "",
          "Source: IEMOCAP (English)\nLT LT_AUG\nFT FT_AUG\nRAU": "",
          "Column_3": "UA\no 3 t",
          "Column_4": "English\nR re\narge",
          "Column_5": "sults\nt lan",
          "Column_6": "",
          "Column_7": "Ital\ncross-\nages.",
          "Column_8": "ian\nlingual\nBOTTO",
          "Column_9": "",
          "Column_10": "ER.\n: C",
          "Column_11": "Ger\nTO\nhine",
          "Column_12": "man\nP:\nse",
          "Column_13": "En\n(so",
          "Column_14": "",
          "Column_15": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "3",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "NIPS"
    },
    {
      "citation_id": "4",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Groß",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Recent developments in opensmile, the munich open-source multimedia feature extractor"
    },
    {
      "citation_id": "5",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "Unpaired imageto-image translation using cycle-consistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "7",
      "title": "Stargan for emotional speech conversion: Validated by data augmentation of endto-end emotion recognition",
      "authors": [
        "G Rizos",
        "A Baird",
        "M Elliott",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "An improved stargan for emotional voice conversion: Enhancing voice quality and data augmentation",
      "authors": [
        "X He",
        "J Chen",
        "G Rizos",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "9",
      "title": "Stargan: Unified generative adversarial networks for multidomain image-to-image translation",
      "authors": [
        "Y Choi",
        "M.-J Choi",
        "M Kim",
        "J.-W Ha",
        "S Kim",
        "J Choo"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Data augmentation using gans for speech emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "12",
      "title": "Cross lingual speech emotion recognition using canonical correlation analysis on principal component subspace",
      "authors": [
        "H Sagha",
        "J Deng",
        "M Gavryukova",
        "J Han",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "13",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Transfer learning for improving speech emotion classification accuracy"
    },
    {
      "citation_id": "14",
      "title": "Cross lingual cross corpus speech emotion recognition",
      "authors": [
        "S Goel",
        "H Beigi"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "15",
      "title": "Viewmaker networks: Learning views for unsupervised representation learning",
      "authors": [
        "A Tamkin",
        "M Wu",
        "N Goodman"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "16",
      "title": "Selfsupervised gans via auxiliary rotation loss",
      "authors": [
        "T Chen",
        "X Zhai",
        "M Ritter",
        "M Lucic",
        "N Houlsby"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "17",
      "title": "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
      "authors": [
        "R Yamamoto",
        "E Song",
        "J.-M Kim"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Training gans with stronger augmentations via contrastive discriminator",
      "authors": [
        "J Jeong",
        "J Shin"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "19",
      "title": "Retrieving speech samples with similar emotional content using a triplet loss function",
      "authors": [
        "J Harvill",
        "M Abdelwahab",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition from variable-length inputs with triplet loss function",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian"
      ],
      "year": "2018",
      "venue": "Speech emotion recognition from variable-length inputs with triplet loss function"
    },
    {
      "citation_id": "21",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "22",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "23",
      "title": "Attention-augmented endto-end multi-task learning for emotion prediction from speech",
      "authors": [
        "Z Zhang",
        "B Wu",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "E Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "26",
      "title": "Augmenting generative adversarial networks for speech emotion recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "27",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "28",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "29",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "LREC"
    }
  ]
}