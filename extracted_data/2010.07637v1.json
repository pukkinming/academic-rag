{
  "paper_id": "2010.07637v1",
  "title": "Dialoguetrm: Exploring The Intra-And Inter-Modal Emotional Behaviors In The Conversation",
  "published": "2020-10-15T10:10:41Z",
  "authors": [
    "Yuzhao Mao",
    "Qi Sun",
    "Guang Liu",
    "Xiaojie Wang",
    "Weiguo Gao",
    "Xuan Li",
    "Jianping Shen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) is essential for building empathetic human-machine systems. Existing studies on ERC primarily focus on summarizing the context information in a conversation, however, ignoring the differentiated emotional behaviors within and across different modalities. Designing appropriate strategies that fit the differentiated multi-modal emotional behaviors can produce more accurate emotional predictions. Thus, we propose the Di-alogueTransformer to explore the differentiated emotional behaviors from the intra-and inter-modal perspectives. For intra-modal, we construct a novel Hierarchical Transformer that can easily switch between sequential and feed-forward structures according to the differentiated context preference within each modality. For inter-modal, we constitute a novel Multi-Grained Interactive Fusion that applies both neuronand vector-grained feature interactions to learn the differentiated contributions across all modalities. Experimental results show that DialogueTRM outperforms the state-of-the-art by a significant margin on three benchmark datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversations (ERC) aims to identify the emotions of interlocutors from their textual, visual, and acoustic emotional expressions in conversations  (Poria et al. 2019b) . Techniques in ERC can be widely applied in many fields, e.g., AI interviews, personalized dialogue systems, e-health services, opinion mining systems, etc.\n\nExisting studies on ERC primarily focus on summarizing the context information by capturing the self-and interpersonal dependencies in a conversation. Self-dependency deals with the aspect of emotional influence that speakers have on themselves during a conversation (Kuppens, Allen, and Sheeber 2010). Inter-personal dependency relates to the emotional influences that the counterparts induce into a speaker  (Morris and Keltner 2000) . Various models have been proposed to better utilize the two different emotional influences  (Jiao et al. 2019; Ghosal et al. 2019; Majumder et al. 2019; Li et al. 2020) .\n\nDespite the progress of previous studies on ERC, few analyses are conducted on the differentiated emotional behaviors within and across different modalities. Notice that language, vision, and speech are three modalities that a machine may process to perceive human emotions. Thus, one specific emotion can be conveyed through the three modalities of emotional expression. However, the point is that the emotional behaviors are not strictly synchronized either within or across modalities when expressing the specific emotion, especially in a conversation. This phenomenon can be interpreted from two aspects as follows:\n\nOne is from the intra-modal aspect that emotional expressions in different modalities have differentiated preferences for conversational context 1  . For instance, in Figure  1 , let us focus on the temporal dimension within every single modality. Noting from the textual point, it is hard to recognize the \"angry\" emotion based on the only utterance \"My sandwich\" at time 7, while it becomes easy for recognition by looking back to infer that A is angry because B ate his sandwich. From the visual and acoustic point, the emotional tendency of A at time 7 can be captured instantaneously from the faces or voices. Thus, we argue that textual expressions are cumulatively produced and strongly dependent on preceding utterances. Therefore, context-dependent settings are preferred. On the other hand, visual and acoustic expressions are burst instantaneously so that the context information becomes less useful for recognizing the current emotion. Thus, context-free settings are preferred.\n\nThe other is from the inter-modal aspect that emotional expressions in different modalities have differentiated contributions for emotional predictions. It may be observed in Figure  1 , if we focus on the spatial dimension across multiple modalities. For person B at time 2, textual and acoustic expressions contribute more to the prediction of \"neutral\" emotion than visual expression (smiling faces often lead to \"happy\" emotion). Thus, to fuse information from multiple modalities, it is essential to learn the contributions or weights of different modalities when predicting emotions. However, representations of different modalities often fall into different spaces, which makes it hard to directly measure the contributions. To tackle this situation, it is essential to first make it comparable between neuron features of different modalities, and then learn the importance of each vector constructed by corresponding neurons. Such multigrained fusion strategies can better learn the contributions of different modalities.\n\nIn this paper, we propose a DialogueTRansforMer (Dia-logueTRM) to explore the intra-and inter-modal emotional behaviors in a conversation. For intra-modal, the temporal dependency within every single modality is the major factor. A Hierarchical Transformer (HT) is constructed by cascading the Transformer  (Vaswani et al. 2017 ) and the Bidirectional Encoder Representations from Transformers (BERT)  (Devlin et al. 2019) . By applying specially designed attention masks, the HT can switch between sequential and feed-forward structures to manage the context preference within every modality. For inter-modal, the spatial dependency across all modalities is the major factor. A Multi-Grained Interactive Fusion (MGIF) is constituted by stacking the multi-modal gate and the Transformer. The multimodal gate is designed for neuron-grained fusion that deals with the feature space problem, and the Transformer is employed for vector-grained fusion that learns the contribution of the gated representations. By comprehensively modeling the intra-and inter-modal emotional behaviors in a conversation, our DialogueTRM achieves more accurate emotional predictions than State-Of-The-Art (SOTA) in ERC.\n\nIn precise terms, our contribution can be summarized as:\n\n• We explore the intra-and inter-modal emotional behaviors in a conversation for ERC. Specifically, -For intra modal temporal modeling, we construct a novel HT module that can manage the differentiated context preference within each modality. -For inter-modal spatial modeling, we constitute a novel MGIF module that applies multi-modal fusion through both neuron-and vector-grained interactive weighting across all modalities.\n\n• The proposed DialogueTRM achieves SOTA performance on three ERC benchmark datasets and outperforms SOTA fusion techniques in ERC settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotions are hidden mental states associated with thoughts and feelings  (Poria et al. 2019b) . Without physiological signals, they are only perceivable through human behaviors like textual utterances, visual expressions, and acoustic signals.\n\nEmotion recognition is an interdisciplinary field that spans psychology, cognitive science, machine learning, natural language processing, and others (Picard 2010). It involves handling textual, visual, and acoustic sources of data. Early studies on emotion recognition are usually singlemodal oriented  (Ekman 1993; Schröder 2003; Strapparava, Valitutti, and others 2004) . Pioneers have explored the advantages of combining facial expressions and speech signals to predict emotions  (Tzirakis et al. 2017; Wöllmer et al. 2010; Datcu and Rothkrantz 2014; Zeng et al. 2007 ). Recent studies  (Tsai et al. 2019a; Liang et al. 2018; Wang et al. 2019; Tsai et al. 2019b ) have considered all the three modalities, whose primary focus is on fusion strategy while ignoring the emotional dynamics in a conversation. Notice that  (Tsai et al. 2019b; Liang et al. 2018 ) also consider the intra-modal and inter-modal interactions, however, the authors ignore the context preference for each modality.\n\nEmotion recognition in conversations is different from traditional emotion recognition. Rather than treating emotions as static states, ERC involves emotional dynamics in a conversation, in which the context plays a vital role. By comparing with the recent proposed ERC approaches  (Hazarika et al. 2018a; Majumder et al. 2019) ,  (Poria et al. 2019b ) discovered that traditional emotion recognition approaches  (Colneriĉ and Demsar 2018; Kratzwald et al. 2018; Shaheen et al. 2014 ) failed to perform well, because the same utterance within variant context may reveal different emotions. ERC is an advancing topic in the recent few years.  (Poria et al. 2017 ) only captures the self-dependency.  (Hazarika et al. 2018b ) and  (Hazarika et al. 2018a ) distinguish the self and inter-personal dependencies.  (Majumder et al. 2019 ) extends ERC to multi-party conversation.  (Ghosal et al. 2019 ) uses the Graph Convolutional Network (GCN) to model complex interactions between interlocutors.  (Li et al. 2020 ) focus on the party-ignorant transferring of emotion in a conversation. Our model focuses on the emotional behaviors within and across multiple modalities in a conversation.\n\nMulti-modal Fusion seeks to generate a single representation to boost a specific task involving multiple modalities when building classifiers or other predictors. The fusion technics are underestimated in ERC. Multi-modal features are directly concatenated  (Hazarika et al. 2018a)  or simply ignored  (Wang et al. 2020 ) in previous studies. There are several typical approaches for multi-modal fusion, such as concatenation  (Majumder et al. 2019) , bilinear  (Kiros, Salakhutdinov, and Zemel 2014), addition (Mao et al. 2014) , differential operation  (Wu et al. 2019 ), gate (Mao et al. 2018) , and attention  (Tsai et al. 2019a) . Depending on the interactions between features, the above approaches can be categorized into linear weighting fusion (first three) and interactive weighting fusion (latter three).  (Gu et al. 2019)  fuses multiple modalities from the sub-view granularities. Our MGIF considers both the neuron and vector granularities and applies multi-grained fusion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Task Formulation",
      "text": "The input in ERC can be defined as a sequence of emotional expressions in L rounds of conversation between N interlocutors, formulated as follows:\n\nwhere x λ τ is the τ -th emotional expression produced by the λ-th interlocutor. It comprises three sources of utterancealigned data in textual (t), visual (v) and acoustic (a) modalities, respectively. Let τ = i, λ = j, x j i be the target emotional expression, K be the context window size. According to whether the interlocutors are distinguished in a conversation, the context is categorized into the individual context ϕ(  1  presents an example of the two types of context. The objective is to predict the emotion of x j i given context information in multi-modal settings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model",
      "text": "The model architecture comprises three modules as depicted in Figure  2 . The bottom is the HT module to capture intramodal emotional behaviors. The middle is the MGIF module to capture inter-modal emotional behaviors. The top is the discriminator module to predict emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hierarchical Transformer",
      "text": "To manage the differentiated context preferences, we construct a novel Hierarchical Transformer (HT) module, shown at the bottom of Figure  2 , by cascading the Transformer and the BERT. The structure of HT can be easily switched between a sequential and a feed-forward model through specially designed attention masks in both the Transformer and the BERT. A standard Transformer is composed of a stack of several identical layers. Each layer has a multi-head selfattention mechanism and a simple position-wise fully connected feed-forward network. The BERT is a variant of the Transformer. It inherits most of the advantages of the Transformer and extends particular strategies in dealing with the inputs. The attention mask is a manipulative component in both the Transformer and the BERT that controls whether or not a specific temporal position should be considered in the attention phase. Hierarchical Structure. The hierarchical structure is constructed by using the BERT as the branches to handle the inputs and using the Transformer as the backbone to wrap all the information output from the BERT.\n\nThe input of the BERT consists of the target emotional expression x j i = x j i,(t) ,x j i,(v) ,x j i,(a) and the individual context ϕ(x j i , X, K). BERT has 3 key operations in dealing with the pair. 1) Pack the target and context in one sequence; 2) Add [CLS] at the head of a sequence as the classification embedding. 3) Add [SEP ] and learnable segment embeddings to differentiate the target and context. The input of the BERT is constituted as {[CLS], x j i , [SEP ], ϕ(x j i , X, K)} . The last hidden layer at [CLS] position is used as the output features. The computation of the BERT can be denoted as\n\nwhere f j i is the feature of the target, i and j index the conversation and interlocutors, respectively. Since the next stage is independent of interlocutors, we omit the index j.\n\nLet F = {f 1 , ..., f L } be the features output from the branches in conversational order. The input of the Transformer are the feature of the target f i and the features of the conversational context φ(f i , F, K) = {f i-K , ..., f i-1 }. The output representation is the last hidden layer at the target position. The computation of the Transformer is\n\nwhere r i is the representation of the target. Since we have three-modal inputs, r i consists of r i,(t) , r i,(v) , and r i,(a) . Context Preference. We adopt two kinds of context preference settings managed by two types of attention masks. One is the context-dependent settings that are suitable for modeling textual expressions. In these settings, the attention mask sets all positions to ones, such that the model forms a sequential structure. Another is the context-free settings that are suitable for modeling expressions in visual and acoustic modalities. In these settings, the attention mask sets the positions of the target to ones and those of the context to zeros, such that the model changes to a feed-forward structure. More precisely, the textual representation hierarchically maintains the self and the inter-personal dependencies from the individual and the conversational context. The visual and acoustic representations preserve instant information within every emotional expression. The three-modal representations with async contextualized information are fed to the MGIF module to learn the spatial dependency across all modalities.\n\nMulti-modal encoding. The three-modal inputs are first encoded to three-modal features, and then consecutively fed to the embedding layer of BERT. For textual expression, sentences are encoded to a sequence of WordPiece  (Devlin et al. 2019 ) embeddings learned by pre-trained BERT. For visual expression, videos are encoded by pre-trained 3D-CNN  (Tran et al. 2015) . For acoustic expression, audios are encoded by pre-trained openSMILE  (Eyben 2010) .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Grained Interactive Fusion",
      "text": "To learn the multi-modal contributions, we constitute a novel Multi-Grained Interactive Fusion (MGIF) module, shown in the middle of Figure  2 , by stacking the multi-modal gate and xi-3,(t) 1 φ(xi-3,(t) 1 ,X ,3)\n\nfi-3\n\nfi-1    1  where, K = 3, N = 3. In the HT module, the BERT consecutively receives three-modal inputs, and the Transformer wraps all the outputs from BERT. Attention masks manage the context preference by switching the model structure. In the MGIF module, the gate simultaneously receives three-modal representations from HT to make the neuron-grained fusion, and the Transformer wraps the gated representations to make vector-grained fusion.\n\nthe Transformer. The multi-modal gate is for neuron-grained fusion that adjusts the neuron features of different modalities at the same position. The Transformer is for vector-grained fusion that learns the importance of the gated representations via the built-in attention mechanism.\n\nNeuron-Grained Fusion. The issue of multi-modal fusion is that the neuron features at the same position are incomparable between different modalities, which causes the vector features of different modalities to fall into different spaces. To make the neuron features comparable, we apply the multi-modal gate  (Arevalo et al. 2020 ) to allocate contrastive weights to neuron features across different modalities. The weights of neurons are interactively computed and contrastively learned along with the training process. After training, the multi-modal gate can learn the relative importance between neuron feature of different modalities at the same position, so that the feature space problem can be alliviated. Let r i,(t) , r i,(v) , r i,(a) ∈ R Dr be the textual, visual, and acoustic representations at the i-th step output from the HT module, the gate operation between textual and visual representations is computed as\n\nwhere h i,(t) , h i,(v) ∈ R D h are the projections of r i,(t) and r i,(v) . * is referring to the Hadamard product whose function is to use neurons in one vector to weight the neurons of its counterpart at the same position. σ is the sigmoid function that maps the weights to (0, 1). Our strategy is using neurons in z to weight neurons in h i,(t) and using neurons in 1 -z to weight neurons in h i,(v) , where, z ∈ R D h is computed by feature interactions among r i,(t) , r i,(v) , and their Hadamard product, [ ; ] denotes the concatenation. Note that \"1-\" operation performs like softmax in attention mechanism. It normalizes the weights of neurons at the same position, so that the contribution of the neurons can be contrastively learned. The contrastive weighting makes neurons at the same position comparable and additive. h i,(tv) is the bi-modal fused representation. W and • are the weight matrices and dot product, respectively. We simplify the above computation as\n\nSimilarly, we can obtain\n\nh i,(av) = GAT E(r i,(a) , r i,(v) )\n\nwhere h i ( * ) represents the neuron-grained fused vectors.\n\nVector-Grained Fusion. There are three fused vectors after neuron-grained fusion. We need to learn the contribution of each vector for emotional predictions. In vector-grained fusion, we allocate one weight to an entire vector indicating its importance. Attention  (Cho et al. 2014 ) is an effective approach for vector-grained interactive weighting. We use another Transformer as the fusion module, where the builtin multi-head attention learns the interactive weights for vector-grained features. Following strategies in BERT, the input is constituted by adding a special embedding [CLS] at the head, which is {[CLS], h i,(tv) , h i,(ta) , h i,(av) }. The order of the input is fixed. By feeding the input to the Transformer, the vector-grained fusion can be computed as\n\nwhere T RM is the Transformer. u i is the output of the last hidden layer at [CLS] position for making predictions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Discriminator",
      "text": "The discriminator, shown in the top of Figure  2 , uses a twolayer perceptron with hidden layer activated by tanh. The output can be the softmax for categorized emotion or linear layer for consecutive emotion, formulated as\n\nwhere, W are the weight matrices, ŷi is the predicted emotion. We use cross-entropy and mean-squared-error loss for categorized and consecutive emotion, respectively,\n\nwhere, L is the conversation length, y i is the ground truth.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Datasets",
      "text": "Three datasets are incorporated for ERC, including  IEMO-CAP (Busso et al. 2008) , MELD  (Poria et al. 2019a) , and AVEC  (Schuller et al. 2012) , where the first two orient categorized emotions, and the last orients consecutive emotions. IEMOCAP consists of dyadic conversation videos between pairs of 10 speakers. Each utterance is annotated with one of the six emotional types, including happy, sad, neutral, angry, excited, and frustrated. We follow the previous studies that use the first four sessions for training and use the last session for testing. The validation is randomly extracted conversations from the training set with a ratio of 0.2.\n\nMELD consists of multi-party conversation videos collected from Friends TV series. Each utterance is annotated with one of the seven emotional types, including anger, disgust, sadness, joy, neutral, surprise, and fear. MELD provides official splits for training, validation, and testing. The visual source is hard to use unless the speaker can be tracked, which is not the focus of our work. Thus, experiments on MELD do not use visual information.\n\nAVEC consists of dyadic conversation videos between human-agent interactions. It gives every 0.2 seconds four real value annotations to denote the degree of four emotional perceptions from visual and acoustic perspectives, which are  Valence, Arousal, Expectancy, and Power (Mehrabian 1996) . We follow strategies  (Majumder et al. 2019 ) that use the mean of the continuous values within one emotional expression as the annotation. AVEC provides official splits for training and testing. We make a random extraction from the training set with a ratio of 0.2 for validation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "The textual features are initialized from BERT and finetuned along with the entire model. The visual and acoustic features are fixed 512-and 100-dimensional vectors, respectively, obtained from the open-source project 2    (Majumder et al. 2019) . All features are projected to 768 dimensions to match the input size of the BERT. The BERT is implemented using off-the-shelf pre-trained BERT-base model in \"Transformers\" 3  with default parameters. The Transformer uses a 6-layer, 12-head-attention, and 768-hidden-unit structure implemented by PyTorch using default parameters. Atin both BERT and Transformer are adopted to manage the settings of context preference. The neurongrained fusion receives three-modal representations of 768 dimensions. The vector-grained fusion adopts a 4-layer, 12head-attention, and 768-hidden-unit Transformer. We use AdamW  (Loshchilov and Hutter 2018)  as optimizer with initial learning rate being 6e -6, β 1 = 0.9, β 2 = 0.999, L2 weight decay of 0.01, learning rate warms up over the first 1, 200 steps, and linear decay of the learning rate. All the results are based on an average of 5 runs. For simplifying and easy-to-reproduce purposes, the proposed model does not expand to multi-GPU settings. Our hardware affords a maximum conversational context size of 14. Larger context can achieve better performance  (Hazarika et al. 2018a) , which is beyond the concern of this paper.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines And Sota",
      "text": "We investigate previous studies tested on the three multimodal ERC datasets and use them as benchmarks.\n\nscLSTM  (Poria et al. 2017)  is the earliest study that we can track in the task of ERC. It makes predictions considering only the individual context.\n\nTL-ERC  (Hazarika et al. 2019 ) uses BERT for context modeling. It uses RNN to encode outputs from the BERT and uses another RNN for decoding.\n\nDialogueRNN  (Majumder et al. 2019 ) is a hierarchical attention-based model with two GRUs capturing the context, and another GRU tracking emotional states.\n\nDialogueGCN  (Ghosal et al. 2019 ) uses GCN to model utterance interactions among interlocutors by considering speaker positions in the historical conversation.\n\nAGHMN (Jiao, Lyu, and King 2019) finetunes sentence representation and uses GRU to wrap the attention-weighted representations rather than summing them up.\n\nBiERU (Li et al. 2020)(SOTA) applies a party-ignorant bidirectional emotional recurrent unit that fully utilized both sides of the conversational context for emotional predictions",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Main Results",
      "text": "We present the categorized emotion results on IEMOCAP and MELD in Table  2  and the consecutive emotions results on AVEC in Table  3 . We follow  (Majumder et al. 2019 ) that uses weighted ACCuracy (ACC) and F1 Score (F1) to evaluate the categorized emotions on IEMOCAP and MELD, and uses pearson's corRelation coefficient (R) to evaluate the consecutive emotions on AVEC.\n\nFor categorized emotion results on IEMOCAP and MELD in Table  2 , it can be noticed that the average scores of our DialogueTRM significantly outperform the benchmarks, indicating 4.3%, 7.1% improvements on IEMOCAP, 10.4%, 4.5% improvements on MELD from SOTA average ACC and F1, respectively. For consecutive emotions results    2 , DialogueTRM is outstanding in \"neutral\". \"Neutral\" is hard from the textual point because some emotional words, such as the red \"excited\" in Figure  5 , could mislead the predictions. Dia-logueTRM can avoid such an issue by referring emotional information from multiple modalities. Fourthly, The outstanding performance of R score on \"Valence\" emotion in Table  3  benefits from BERT features, as TL-ERC using BERT also achieves outstanding performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Inter-Modal Results",
      "text": "We present the fusion results of F1 in Table  4  on IEMO-CAP. The point is the uplift from text-only to multi-modal which is denoted as subscripts in the multi-modal column. The \"67.67\" result in the text-only column is based on our intra-modal module fed with text-only features.\n\nComparison with SOTA. DialogueRNN is the SOTA model for ERC in multi-modal settings. It uses concatenation to fuse multi-modal information. The uplift of MGIF Symbol † denotes all modalities use context-dependent settings.\n\nis seven times higher than that of DialogueRNN. The significance benefits from two factors. One is from the interactive weighting. The other is from the multi-grained fusion. MulT  (Tsai et al. 2019a ) is the SOTA fusion technique for emotion recognition. It uses a Transformer to temporally fuse the multi-modal features in an encoder-decoder fashion. Thus, all modalities must use context-dependent settings. For a fair comparison, both MGIF † and MulT † 4  are fed with context-dependent multi-modal features output from our the HT module. The uplift of MGIF † is nearly tripled than that of MulT †. The superiority of MGIF benefits from interactions between features of varied granularities.\n\nAblation study. The bottom three rows in Table  4  are the ablation results of MGIF module. The results are based on three-modal representations with preferred context information output from the HT module. TRM is short for Transformer. 1) Gate+TRM vs. TRM is a comparison of feeding the Transformer with or without gated features. It indicates the necessity for neuron-grained fusion. 2) Gate+TRM vs. Gate+Concat is a comparison between the linear and interactive weighting for fusion. It indicates that interactive weighting (TRM) is better than linear weighting (Concat). 3) The superiority of Gate+TRM over the others indicates that the multi-grained fusion is better than single-grained fusion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Intra-Modal Results",
      "text": "In this experiment, we first analyze the context preference within each modality and then discuss the different structures for summarizing the textual context. All results are based on the F1 score on IEMOCAP. Context Preference. Due to the different context preference settings, the uplift of MGIF is nearly doubled than that of MGIF † as presented in Table  4 . To detail the analysis for context preference, we depict the comparative performance of our model between context-free and context-dependent settings in Figure  3 . There are 2 points worth mentioning. 1) Textual modality performs much better in context-dependent settings. 2) Visual and acoustic modalities always achieve better performance in context-free settings whether as an individual or after fusion. Note that, to focus on the context preference for visual and acoustic modalities, the context preference settings of textual modality are fixed. Temporal module. From the results in Figure  3 , we notice that the textual modality dominates the performance, and the performance is strongly affected by the context information. In this experiment, we present the performance of different structures for summarizing the textual context. BERT makes predictions by summarizing individual context. DialogueRNN uses three GRUs with global and local attention to interactively maintain three states in a conversation. DialogueTRM uses just the Transformer. For fairness, DialogueRNN also uses contextualized features output from BERT. The comparison is depicted in figure  4 . Notice that DialogueTRM performs better than the others. Dia-",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Pa Pb",
      "text": "There's like mystery here, there's magic. It's like a little bit of the unexplainable. I just can't see how you're not interested.\n\nGod, I don't get it. you know the first time we came here, you said it was the best night of your life?\n\nAnd last year, I remember distinctly you said, you were so excited to get here that you don' t remember you stubbed your toe until we were in the car. logueRNN is slightly better than BERT which means it can hardly extract more useful information from BERT features.\n\nCase Study\n\nWe analyze cases that are incorrectly predicted from the text but correctly predicted from the multi-modal. Among the cases, \"neutral\" and \"frustrated\" are in the majority with the ratios of about 30.38% and 27.85%, respectively. It means multi-modal contributes more to \"neutral\" and \"frustrated\" cases. Moreover, about 85.41% \"neutral\" and 70.45% \"frustrated\" cases are rectified from negative emotions, which means multi-modal provides easy-todistinguish information for negative emotions. The reason is probably that human tends to use neutral words to cover their negative emotions but still showing up in the face or speech.\n\n2) Figure  5  lists a conversation case. The incorrectly textpredicted \"excited\" emotion at time t+3 is probably because of the misleading word \"excited\" in the utterance while rectified by multi-modal.\n\n3) The utterance \"yeah.\" appears 23 times in the test set. Given only the current utterance, the accuracy is \"43.48%\". After adding context, it increases to \"65.22%\". After adding multi-modal, it reaches \"73.91%\".",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes the DialogueTRM with the HT and MGIF modules for respectively capturing the intraand inter-modal emotional behaviors in a conversation. It achieves SOTA performance on ERC datasets. Besides, the MGIF module outperforms the SOTA fusion approaches in ERC settings. In the experiment, we analyze the context preferences of emotional expression in different modalities and discuss several cases to reveal the benefit of our model.\n\nIn the future, we will further explore the emotional behaviors in conversation. It is also possible to incorporate the MGIF inside of the Transformer for constructing a more advanced temporal module.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example of a multi-modal conversation.",
      "page": 1
    },
    {
      "caption": "Figure 1: , if we focus on the spatial dimension across multi-",
      "page": 2
    },
    {
      "caption": "Figure 2: The bottom is the HT module to capture intra-",
      "page": 3
    },
    {
      "caption": "Figure 2: , by cascading the Transformer and",
      "page": 3
    },
    {
      "caption": "Figure 2: , by stacking the multi-modal gate and",
      "page": 3
    },
    {
      "caption": "Figure 2: Model architecture. The ﬁgure is an example struc-",
      "page": 4
    },
    {
      "caption": "Figure 2: , uses a two-",
      "page": 4
    },
    {
      "caption": "Figure 5: , could mislead the predictions. Dia-",
      "page": 6
    },
    {
      "caption": "Figure 3: Context perferences results on IEMOCAP. T, A",
      "page": 7
    },
    {
      "caption": "Figure 3: There are 2 points worth mentioning. 1)",
      "page": 7
    },
    {
      "caption": "Figure 4: Temporal module result on IEMOCAP.",
      "page": 7
    },
    {
      "caption": "Figure 5: Conversation cases with MP (Multi-modal-",
      "page": 7
    },
    {
      "caption": "Figure 5: lists a conversation case. The incorrectly text-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Person A\nPerson B": "Dialogue\nTime"
        },
        {
          "Person A\nPerson B": "Vision\nText\nAcoustics\nVision\nAcoustics"
        },
        {
          "Person A\nPerson B": "A: I cannot find my sandwich."
        },
        {
          "Person A\nPerson B": "1"
        },
        {
          "Person A\nPerson B": "[Neutral]"
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": "B: I am sorry, but I believe I ate your \n2"
        },
        {
          "Person A\nPerson B": "sandwich. [Neutral]"
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": "3\nA: You ate my sandwich? [Surprise]"
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": "4"
        },
        {
          "Person A\nPerson B": "B: It was a mistake. [Neutral]"
        },
        {
          "Person A\nPerson B": "A: Oh, really? It was my sandwich"
        },
        {
          "Person A\nPerson B": "5"
        },
        {
          "Person A\nPerson B": "with label on it. [Neutral]"
        },
        {
          "Person A\nPerson B": "B: Please...please calm down. It may \n6"
        },
        {
          "Person A\nPerson B": "happen to anyone. [Fear]"
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": "7"
        },
        {
          "Person A\nPerson B": "A: My sandwich.[Angry]"
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": "Figure 1: Example of a multi-modal conversation."
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": "language, vision, and speech are three modalities that a ma-"
        },
        {
          "Person A\nPerson B": "chine may process to perceive human emotions. Thus, one"
        },
        {
          "Person A\nPerson B": "speciﬁc emotion can be conveyed through the three modal-"
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": "ities of\nemotional\nexpression. However,\nthe point\nis\nthat"
        },
        {
          "Person A\nPerson B": "the\nemotional behaviors\nare not\nstrictly synchronized ei-"
        },
        {
          "Person A\nPerson B": "ther within or across modalities when expressing the speciﬁc"
        },
        {
          "Person A\nPerson B": "emotion, especially in a conversation. This phenomenon can"
        },
        {
          "Person A\nPerson B": "be interpreted from two aspects as follows:"
        },
        {
          "Person A\nPerson B": "One is from the intra-modal aspect that emotional expres-"
        },
        {
          "Person A\nPerson B": "sions in different modalities have differentiated preferences"
        },
        {
          "Person A\nPerson B": "for conversational context1. For instance, in Figure 1, let us"
        },
        {
          "Person A\nPerson B": "focus on the temporal dimension within every single modal-"
        },
        {
          "Person A\nPerson B": "ity. Noting from the textual point,\nit\nis hard to recognize"
        },
        {
          "Person A\nPerson B": "the “angry” emotion based on the only utterance “My sand-"
        },
        {
          "Person A\nPerson B": "wich” at\ntime 7, while it becomes easy for\nrecognition by"
        },
        {
          "Person A\nPerson B": "looking back to infer that A is angry because B ate his sand-"
        },
        {
          "Person A\nPerson B": "wich. From the visual and acoustic point, the emotional ten-"
        },
        {
          "Person A\nPerson B": "dency of A at\ntime 7 can be captured instantaneously from"
        },
        {
          "Person A\nPerson B": "the faces or voices. Thus, we argue that\ntextual expressions"
        },
        {
          "Person A\nPerson B": "are cumulatively produced and strongly dependent on pre-"
        },
        {
          "Person A\nPerson B": "ceding utterances. Therefore, context-dependent settings are"
        },
        {
          "Person A\nPerson B": ""
        },
        {
          "Person A\nPerson B": "1In this paper, the notion of context denotes the temporally pre-"
        },
        {
          "Person A\nPerson B": "ceding information of the current expression in a conversation."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ties when building classiﬁers or other predictors. The fusion": "technics are underestimated in ERC. Multi-modal\nfeatures"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "are directly concatenated (Hazarika et al. 2018a) or\nsim-"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "ply ignored (Wang et al. 2020)\nin previous studies. There"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "are several\ntypical approaches for multi-modal fusion, such"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "as concatenation (Majumder et al. 2019), bilinear\n(Kiros,"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "Salakhutdinov, and Zemel 2014), addition (Mao et al. 2014),"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "differential operation (Wu et\nal. 2019), gate\n(Mao et\nal."
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": ""
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "2018), and attention (Tsai et al. 2019a). Depending on the"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": ""
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "interactions between features,\nthe above approaches can be"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": ""
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "categorized into linear weighting fusion (ﬁrst\nthree) and in-"
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": ""
        },
        {
          "ties when building classiﬁers or other predictors. The fusion": "teractive weighting fusion (latter\nthree).\n(Gu et al. 2019)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "preferred. On the other hand, visual and acoustic expressions": "are burst instantaneously so that the context information be-",
          "Related Work": ""
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "Emotions are hidden mental states associated with thoughts"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "comes less useful for recognizing the current emotion. Thus,",
          "Related Work": ""
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "and feelings (Poria et al. 2019b). Without physiological sig-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "context-free settings are preferred.",
          "Related Work": ""
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "nals, they are only perceivable through human behaviors like"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "The other\nis from the inter-modal aspect\nthat emotional",
          "Related Work": "textual utterances, visual expressions, and acoustic signals."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "expressions in different modalities have differentiated con-",
          "Related Work": "Emotion recognition is\nan\ninterdisciplinary ﬁeld\nthat"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "tributions for emotional predictions. It may be observed in",
          "Related Work": "spans psychology, cognitive science, machine learning, nat-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "Figure 1, if we focus on the spatial dimension across multi-",
          "Related Work": "ural\nlanguage processing, and others\n(Picard 2010).\nIt\nin-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "ple modalities. For person B at\ntime 2,\ntextual and acoustic",
          "Related Work": "volves handling textual, visual, and acoustic sources of data."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "expressions contribute more to the prediction of “neutral”",
          "Related Work": "Early studies on emotion recognition are usually single-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "emotion than visual expression (smiling faces often lead to",
          "Related Work": "modal oriented (Ekman 1993; Schr¨oder 2003; Strapparava,"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "“happy” emotion). Thus,\nto fuse information from multi-",
          "Related Work": "Valitutti, and others 2004). Pioneers have explored the ad-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "ple modalities,\nit\nis essential\nto learn the contributions or",
          "Related Work": "vantages of combining facial expressions and speech sig-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "weights of different modalities when predicting emotions.",
          "Related Work": "nals to predict emotions (Tzirakis et al. 2017; W¨ollmer et"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "However,\nrepresentations of different modalities often fall",
          "Related Work": "al. 2010; Datcu and Rothkrantz 2014; Zeng et al. 2007)."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "into different spaces, which makes it hard to directly mea-",
          "Related Work": "Recent studies (Tsai et al. 2019a; Liang et al. 2018; Wang"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "sure the contributions. To tackle this situation,\nit\nis essen-",
          "Related Work": "et al. 2019; Tsai et al. 2019b) have considered all\nthe three"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "tial\nto ﬁrst make it comparable between neuron features of",
          "Related Work": "modalities, whose primary focus is on fusion strategy while"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "different modalities, and then learn the importance of each",
          "Related Work": "ignoring the emotional dynamics in a conversation. Notice"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "vector constructed by corresponding neurons. Such multi-",
          "Related Work": "that (Tsai et al. 2019b; Liang et al. 2018) also consider the"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "grained fusion strategies can better\nlearn the contributions",
          "Related Work": "intra-modal and inter-modal\ninteractions, however,\nthe au-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "of different modalities.",
          "Related Work": "thors ignore the context preference for each modality."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "In this paper, we propose a DialogueTRansforMer (Dia-",
          "Related Work": "Emotion recognition in conversations is different from"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "logueTRM) to explore the intra- and inter-modal emotional",
          "Related Work": "traditional emotion recognition. Rather\nthan treating emo-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "behaviors\nin a\nconversation. For\nintra-modal,\nthe\ntempo-",
          "Related Work": "tions as static states, ERC involves emotional dynamics in a"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "ral dependency within every single modality is\nthe major",
          "Related Work": "conversation, in which the context plays a vital role. By com-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "factor. A Hierarchical Transformer\n(HT)\nis constructed by",
          "Related Work": "paring with the recent proposed ERC approaches (Hazarika"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "cascading the Transformer\n(Vaswani et al. 2017) and the",
          "Related Work": "et al. 2018a; Majumder et al. 2019),\n(Poria et al. 2019b)"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "Bidirectional Encoder Representations\nfrom Transformers",
          "Related Work": "discovered that\ntraditional emotion recognition approaches"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "(BERT) (Devlin et al. 2019). By applying specially designed",
          "Related Work": "(Colneriˆc and Demsar 2018; Kratzwald et al. 2018; Shaheen"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "attention masks,\nthe HT can switch between sequential and",
          "Related Work": "et al. 2014) failed to perform well, because the same utter-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "feed-forward structures\nto manage the context preference",
          "Related Work": "ance within variant context may reveal different emotions."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "within every modality. For\ninter-modal,\nthe spatial depen-",
          "Related Work": "ERC is an advancing topic in the recent few years. (Poria et"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "dency across all modalities\nis\nthe major\nfactor. A Multi-",
          "Related Work": "al. 2017) only captures the self-dependency. (Hazarika et al."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "Grained Interactive Fusion (MGIF) is constituted by stack-",
          "Related Work": "2018b) and (Hazarika et al. 2018a) distinguish the self and"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "ing the multi-modal gate and the Transformer. The multi-",
          "Related Work": "inter-personal dependencies. (Majumder et al. 2019) extends"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "modal gate is designed for neuron-grained fusion that deals",
          "Related Work": "ERC to multi-party conversation. (Ghosal et al. 2019) uses"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "with the feature space problem, and the Transformer is em-",
          "Related Work": "the Graph Convolutional Network (GCN) to model complex"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "ployed for vector-grained fusion that learns the contribution",
          "Related Work": "interactions between interlocutors. (Li et al. 2020) focus on"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "of the gated representations. By comprehensively modeling",
          "Related Work": "the party-ignorant transferring of emotion in a conversation."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "the intra- and inter-modal emotional behaviors in a conver-",
          "Related Work": "Our model\nfocuses on the emotional behaviors within and"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "sation, our DialogueTRM achieves more accurate emotional",
          "Related Work": "across multiple modalities in a conversation."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "predictions than State-Of-The-Art (SOTA) in ERC.",
          "Related Work": "Multi-modal Fusion seeks to generate a single represen-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "tation to boost a speciﬁc task involving multiple modali-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "In precise terms, our contribution can be summarized as:",
          "Related Work": ""
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "ties when building classiﬁers or other predictors. The fusion"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "• We explore the intra- and inter-modal emotional behav-",
          "Related Work": "technics are underestimated in ERC. Multi-modal\nfeatures"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "iors in a conversation for ERC. Speciﬁcally,",
          "Related Work": "are directly concatenated (Hazarika et al. 2018a) or\nsim-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "ply ignored (Wang et al. 2020)\nin previous studies. There"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "– For\nintra modal\ntemporal modeling, we\nconstruct\na",
          "Related Work": "are several\ntypical approaches for multi-modal fusion, such"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "novel HT module that can manage the differentiated",
          "Related Work": "as concatenation (Majumder et al. 2019), bilinear\n(Kiros,"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "context preference within each modality.",
          "Related Work": "Salakhutdinov, and Zemel 2014), addition (Mao et al. 2014),"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "differential operation (Wu et\nal. 2019), gate\n(Mao et\nal."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "– For inter-modal spatial modeling, we constitute a novel",
          "Related Work": ""
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "2018), and attention (Tsai et al. 2019a). Depending on the"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "MGIF module that applies multi-modal fusion through",
          "Related Work": ""
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "interactions between features,\nthe above approaches can be"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "both neuron- and vector-grained interactive weighting",
          "Related Work": ""
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "categorized into linear weighting fusion (ﬁrst\nthree) and in-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "across all modalities.",
          "Related Work": ""
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "",
          "Related Work": "teractive weighting fusion (latter\nthree).\n(Gu et al. 2019)"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "• The proposed DialogueTRM achieves SOTA performance",
          "Related Work": "fuses multiple modalities from the sub-view granularities."
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "on three ERC benchmark datasets and outperforms SOTA",
          "Related Work": "Our MGIF considers both the neuron and vector granulari-"
        },
        {
          "preferred. On the other hand, visual and acoustic expressions": "fusion techniques in ERC settings.",
          "Related Work": "ties and applies multi-grained fusion."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "all the information output from the BERT."
        },
        {
          "Task Formulation": "The input in ERC can be deﬁned as a sequence of emotional",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "The input of the BERT consists of the target emotional ex-"
        },
        {
          "Task Formulation": "expressions in L rounds of conversation between N inter-",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "pression xj"
        },
        {
          "Task Formulation": "locutors, formulated as follows:",
          "inputs and using the Transformer as the backbone to wrap": "i = (cid:104)xj\ni,(a)(cid:105) and the individual context\ni,(v),xj\ni,(t),xj"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "ϕ(xj"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "i , X, K). BERT has 3 key operations in dealing with the"
        },
        {
          "Task Formulation": "X = {xλ",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "(1)\nτ = (cid:104)xλ\nτ,(a)(cid:105)|τ ∈ [1, L], λ ∈ [1, N ]},\nτ,(v),xλ\nτ,(t),xλ",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "pair. 1) Pack the target and context in one sequence; 2) Add"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "[CLS] at the head of a sequence as the classiﬁcation embed-"
        },
        {
          "Task Formulation": "where xλ\nis the τ -th emotional expression produced by the\nτ",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "ding. 3) Add [SEP ] and learnable segment embeddings to"
        },
        {
          "Task Formulation": "λ-th interlocutor.\nIt comprises\nthree sources of utterance-",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "differentiate the target and context. The input of the BERT is"
        },
        {
          "Task Formulation": "aligned data in textual (t), visual (v) and acoustic (a) modal-",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "constituted as {[CLS], xj\ni , X, K)} . The last\ni , [SEP ], ϕ(xj"
        },
        {
          "Task Formulation": "ities, respectively. Let τ = i, λ = j, xj\ni be the target emo-",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "hidden layer at [CLS] position is used as the output features."
        },
        {
          "Task Formulation": "tional expression, K be the context window size. According",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "The computation of the BERT can be denoted as"
        },
        {
          "Task Formulation": "to whether\nthe interlocutors are distinguished in a conver-",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "sation, the context is categorized into the individual context",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "f j"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "(2)\ni = BERT ([CLS], xj\ni , [SEP ], ϕ(xj\ni , X, K)),"
        },
        {
          "Task Formulation": "ϕ(xj",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "τ |τ ∈ [i−K, i), λ = j} and the conversa-\ni , X, K) = {xλ",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "is the feature of the target, i and j index the conver-\nwhere f j"
        },
        {
          "Task Formulation": "tional context φ(xj\nτ |τ ∈ [i − K, i), λ ∈ [1, N ]}.\ni , X, K) = {xλ",
          "inputs and using the Transformer as the backbone to wrap": "i"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "sation and interlocutors, respectively. Since the next stage is"
        },
        {
          "Task Formulation": "Table 1 presents an example of the two types of context. The",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "independent of interlocutors, we omit the index j."
        },
        {
          "Task Formulation": "objective is to predict the emotion of xj\ni given context infor-",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "from the\nLet F = {f1, ..., fL} be the features output"
        },
        {
          "Task Formulation": "mation in multi-modal settings.",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "branches\nin conversational order. The input of\nthe Trans-"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "former are the feature of\nthe target fi and the features of"
        },
        {
          "Task Formulation": "Table 1: A conversation example, in which context size K =",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "the conversational context φ(fi, F, K) = {fi−K, ..., fi−1}."
        },
        {
          "Task Formulation": "3, conversation rounds L = 8, interlocuter number N = 3.",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "The output representation is the last hidden layer at the target"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "position. The computation of the Transformer is"
        },
        {
          "Task Formulation": "X = {x1\nconversation\n1, x1\n2, x2\n3, x1\n4, x3\n5, x2\n6, x1\n7, x2\n8}",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "(3)\nri = T RM (φ(fi, F, K), fi),"
        },
        {
          "Task Formulation": "x1\ntarget",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "is the representation of\nthe target. Since we have\nwhere ri"
        },
        {
          "Task Formulation": "ϕ(x1\nindividual context\n7, X, K) = {x1\n4}",
          "inputs and using the Transformer as the backbone to wrap": "three-modal inputs, ri consists of ri,(t), ri,(v), and ri,(a)."
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "Context Preference. We adopt\ntwo kinds of context pref-"
        },
        {
          "Task Formulation": "φ(x1\nconversational context\n7, X, K) = {x1\n4, x3\n5, x2\n6}",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "erence settings managed by two types of attention masks."
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "One is the context-dependent settings that are suitable for"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "modeling textual expressions.\nIn these settings,\nthe atten-"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "tion mask sets all positions\nto ones,\nsuch that\nthe model"
        },
        {
          "Task Formulation": "Model",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "forms a sequential structure. Another is the context-free set-"
        },
        {
          "Task Formulation": "The model architecture comprises three modules as depicted",
          "inputs and using the Transformer as the backbone to wrap": "tings\nthat are suitable for modeling expressions\nin visual"
        },
        {
          "Task Formulation": "in Figure 2. The bottom is the HT module to capture intra-",
          "inputs and using the Transformer as the backbone to wrap": "and acoustic modalities. In these settings, the attention mask"
        },
        {
          "Task Formulation": "modal emotional behaviors. The middle is the MGIF module",
          "inputs and using the Transformer as the backbone to wrap": "sets the positions of the target to ones and those of the con-"
        },
        {
          "Task Formulation": "to capture inter-modal emotional behaviors. The top is the",
          "inputs and using the Transformer as the backbone to wrap": "text to zeros, such that the model changes to a feed-forward"
        },
        {
          "Task Formulation": "discriminator module to predict emotions.",
          "inputs and using the Transformer as the backbone to wrap": "structure. More precisely,\nthe textual representation hierar-"
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "chically maintains the self and the inter-personal dependen-"
        },
        {
          "Task Formulation": "Hierarchical Transformer",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "cies from the individual and the conversational context. The"
        },
        {
          "Task Formulation": "To manage the differentiated context preferences, we con-",
          "inputs and using the Transformer as the backbone to wrap": "visual and acoustic representations preserve instant\ninfor-"
        },
        {
          "Task Formulation": "struct a novel Hierarchical Transformer (HT) module, shown",
          "inputs and using the Transformer as the backbone to wrap": "mation within every emotional expression. The three-modal"
        },
        {
          "Task Formulation": "at the bottom of Figure 2, by cascading the Transformer and",
          "inputs and using the Transformer as the backbone to wrap": "representations with async contextualized information are"
        },
        {
          "Task Formulation": "the BERT. The structure of HT can be easily switched be-",
          "inputs and using the Transformer as the backbone to wrap": "fed to the MGIF module to learn the spatial dependency"
        },
        {
          "Task Formulation": "tween a sequential and a feed-forward model\nthrough spe-",
          "inputs and using the Transformer as the backbone to wrap": "across all modalities."
        },
        {
          "Task Formulation": "cially designed attention masks in both the Transformer and",
          "inputs and using the Transformer as the backbone to wrap": "Multi-modal encoding. The three-modal\ninputs are ﬁrst"
        },
        {
          "Task Formulation": "the BERT. A standard Transformer is composed of a stack",
          "inputs and using the Transformer as the backbone to wrap": "encoded to three-modal features, and then consecutively fed"
        },
        {
          "Task Formulation": "of several identical layers. Each layer has a multi-head self-",
          "inputs and using the Transformer as the backbone to wrap": "to the embedding layer of BERT. For\ntextual expression,"
        },
        {
          "Task Formulation": "attention mechanism and a simple position-wise fully con-",
          "inputs and using the Transformer as the backbone to wrap": "sentences are encoded to a sequence of WordPiece (Devlin"
        },
        {
          "Task Formulation": "nected feed-forward network. The BERT is a variant of the",
          "inputs and using the Transformer as the backbone to wrap": "et al. 2019) embeddings learned by pre-trained BERT. For"
        },
        {
          "Task Formulation": "Transformer. It inherits most of the advantages of the Trans-",
          "inputs and using the Transformer as the backbone to wrap": "visual expression, videos are encoded by pre-trained 3D-"
        },
        {
          "Task Formulation": "former and extends particular strategies in dealing with the",
          "inputs and using the Transformer as the backbone to wrap": "CNN (Tran et al. 2015). For acoustic expression, audios are"
        },
        {
          "Task Formulation": "inputs. The attention mask is a manipulative component\nin",
          "inputs and using the Transformer as the backbone to wrap": "encoded by pre-trained openSMILE (Eyben 2010)."
        },
        {
          "Task Formulation": "both the Transformer and the BERT that controls whether or",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "",
          "inputs and using the Transformer as the backbone to wrap": "Multi-Grained Interactive Fusion"
        },
        {
          "Task Formulation": "not a speciﬁc temporal position should be considered in the",
          "inputs and using the Transformer as the backbone to wrap": ""
        },
        {
          "Task Formulation": "attention phase.",
          "inputs and using the Transformer as the backbone to wrap": "To learn the multi-modal contributions, we constitute a novel"
        },
        {
          "Task Formulation": "Hierarchical Structure. The hierarchical structure is con-",
          "inputs and using the Transformer as the backbone to wrap": "Multi-Grained Interactive Fusion (MGIF) module, shown in"
        },
        {
          "Task Formulation": "structed by using the BERT as the branches to handle the",
          "inputs and using the Transformer as the backbone to wrap": "the middle of Figure 2, by stacking the multi-modal gate and"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: where,K =3, N =3. In the HT",
      "data": [
        {
          "Individual Context\nTarget": "attention mask\nattention mask"
        },
        {
          "Individual Context\nTarget": "2\n[CLS]\n[SEP]\n2,X ,3)\nxi-1,(t)\nφ(xi-1,(t)"
        },
        {
          "Individual Context\nTarget": "fi-2"
        },
        {
          "Individual Context\nTarget": "BERT"
        },
        {
          "Individual Context\nTarget": "Individual Context\nTarget"
        },
        {
          "Individual Context\nTarget": "attention mask\nattention mask"
        },
        {
          "Individual Context\nTarget": "3\n[CLS]\n[SEP]\n3,X ,3)\nxi-2,(t)\nφ(xi-2,(t)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: where,K =3, N =3. In the HT",
      "data": [
        {
          "+": ""
        },
        {
          "+": "1-\nNeuron-grained"
        },
        {
          "+": "gate"
        },
        {
          "+": "fusion\ngate"
        },
        {
          "+": "tanh\nσ\ntanh"
        },
        {
          "+": "gate"
        },
        {
          "+": ""
        },
        {
          "+": "ri,(a)"
        },
        {
          "+": "ri,(v)"
        },
        {
          "+": "ri,(t)\nri,(v)\nri,(t)"
        },
        {
          "+": ""
        },
        {
          "+": ""
        },
        {
          "+": "Acoustic"
        },
        {
          "+": "Visual\nHierarchical Transformer"
        },
        {
          "+": "Textual\n(HT)\nri"
        },
        {
          "+": "Transformer\nConversational Context\nTarget\nBackbone"
        },
        {
          "+": "attention mask\nattention mask"
        },
        {
          "+": "fi-3\nfi-2\nfi-1\nfi"
        },
        {
          "+": ""
        },
        {
          "+": "fi-3\nfi"
        },
        {
          "+": "BERT\nBERT"
        },
        {
          "+": "Individual Context\nIndividual Context\nTarget\nTarget\nInterlocutor1"
        },
        {
          "+": "attention mask\nattention mask\nattention mask\nattention mask"
        },
        {
          "+": "1\n1\n[CLS]\n[SEP]\n[CLS]\n[SEP]\n1,X ,3)\n1,X ,3)\nxi-3,(t)\nxi,(t)\nφ(xi-3,(t)\nφ(xi,(t)"
        },
        {
          "+": "fi-1"
        },
        {
          "+": "BERT\nBranches\nInterlocutor2\nIndividual Context\nTarget"
        },
        {
          "+": "attention mask\nattention mask"
        },
        {
          "+": "2\n[CLS]\n[SEP]\n2,X ,3)\nxi-1,(t)\nφ(xi-1,(t)"
        },
        {
          "+": ""
        },
        {
          "+": "fi-2"
        },
        {
          "+": "BERT"
        },
        {
          "+": "Individual Context\nTarget\nInterlocutor3"
        },
        {
          "+": "attention mask\nattention mask"
        },
        {
          "+": "3\n[CLS]\n[SEP]\n3,X ,3)\nxi-2,(t)\nφ(xi-2,(t)"
        },
        {
          "+": ""
        },
        {
          "+": "Figure 2: Model architecture. The ﬁgure is an example struc-"
        },
        {
          "+": ""
        },
        {
          "+": "ture by referring Table 1 where, K = 3, N = 3.\nIn the HT"
        },
        {
          "+": "module,\nthe BERT consecutively receives\nthree-modal\nin-"
        },
        {
          "+": ""
        },
        {
          "+": "puts, and the Transformer wraps all the outputs from BERT."
        },
        {
          "+": "Attention masks manage the context preference by switch-"
        },
        {
          "+": "ing the model structure. In the MGIF module, the gate simul-"
        },
        {
          "+": "taneously receives three-modal representations from HT to"
        },
        {
          "+": "make the neuron-grained fusion, and the Transformer wraps"
        },
        {
          "+": "the gated representations to make vector-grained fusion."
        },
        {
          "+": ""
        },
        {
          "+": ""
        },
        {
          "+": "the Transformer. The multi-modal gate is for neuron-grained"
        },
        {
          "+": "fusion that adjusts the neuron features of different modalities"
        },
        {
          "+": "at\nthe same position. The Transformer is for vector-grained"
        },
        {
          "+": "fusion that learns the importance of the gated representations"
        },
        {
          "+": "via the built-in attention mechanism."
        },
        {
          "+": "Neuron-Grained Fusion. The\nissue of multi-modal\nfu-"
        },
        {
          "+": "sion is that\nthe neuron features at\nthe same position are in-"
        },
        {
          "+": "comparable between different modalities, which causes the"
        },
        {
          "+": ""
        },
        {
          "+": "vector\nfeatures of different modalities to fall\ninto different"
        },
        {
          "+": "spaces. To make the neuron features comparable, we apply"
        },
        {
          "+": "the multi-modal gate (Arevalo et al. 2020) to allocate con-"
        },
        {
          "+": "trastive weights to neuron features across different modali-"
        },
        {
          "+": ""
        },
        {
          "+": "ties. The weights of neurons are interactively computed and"
        },
        {
          "+": "contrastively learned along with the training process. After"
        },
        {
          "+": "training,\nthe multi-modal gate can learn the relative impor-"
        },
        {
          "+": "tance between neuron feature of different modalities at\nthe"
        },
        {
          "+": "same position, so that\nthe feature space problem can be al-"
        },
        {
          "+": "liviated. Let ri,(t), ri,(v), ri,(a) ∈ RDr be the textual, visual,"
        },
        {
          "+": "and acoustic representations at the i-th step output from the"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "(13)\nPi ="
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "consecutive emotion\nWcon ·oi"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "(cid:40) arg max\ncategorized emotion\nPi[k]"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "k\n(14)\nyi ="
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "consecutive emotion\nPi"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "is the predicted emo-\nwhere, W are the weight matrices, ˆyi"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "tion. We use cross-entropy and mean-squared-error loss for"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "categorized and consecutive emotion, respectively,"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "1 L\nL(cid:88) i\ncategorized emotion\nyi log( ˆyi)"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": " "
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "Loss =\n(15)"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "1 L\nL(cid:88) i\nconsecutive emotion\n(yi − ˆyi)2"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "is the ground truth.\nwhere, L is the conversation length, yi"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "Experiment"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "Datasets"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "Three datasets are incorporated for ERC,\nincluding IEMO-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "CAP\n(Busso et al. 2008), MELD (Poria et al. 2019a), and"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "AVEC (Schuller et al. 2012), where the ﬁrst two orient cate-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "gorized emotions, and the last orients consecutive emotions."
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "IEMOCAP consists of dyadic\nconversation videos be-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "tween pairs of 10 speakers. Each utterance is annotated with"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "one of the six emotional types, including happy, sad, neutral,"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "angry, excited, and frustrated. We follow the previous stud-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "ies that use the ﬁrst\nfour sessions for\ntraining and use the"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "last session for testing. The validation is randomly extracted"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "conversations from the training set with a ratio of 0.2."
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "MELD consists of multi-party conversation videos col-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "lected from Friends TV series. Each utterance is annotated"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "with one of the seven emotional types, including anger, dis-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "gust,\nsadness,\njoy, neutral,\nsurprise, and fear. MELD pro-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "vides ofﬁcial splits for training, validation, and testing. The"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "visual source is hard to use unless the speaker can be tracked,"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "which is not\nthe focus of our work. Thus, experiments on"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "MELD do not use visual information."
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "AVEC consists of dyadic\nconversation videos between"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "human-agent\ninteractions.\nIt gives every 0.2 seconds\nfour"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "real value annotations to denote the degree of four emotional"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "perceptions\nfrom visual\nand acoustic perspectives, which"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "are Valence, Arousal, Expectancy, and Power\n(Mehrabian"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "1996). We follow strategies (Majumder et al. 2019) that use"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "the mean of the continuous values within one emotional ex-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "pression as the annotation. AVEC provides ofﬁcial splits for"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "training and testing. We make a random extraction from the"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "training set with a ratio of 0.2 for validation."
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "Implementation Details"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": ""
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "The textual features are initialized from BERT and ﬁnetuned"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "along with the entire model. The visual and acoustic fea-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "tures are ﬁxed 512- and 100-dimensional vectors,\nrespec-"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "tively, obtained from the open-source project2\n(Majumder"
        },
        {
          "categorized emotion\n(cid:26)sof tmax(Wcat ·oi)": "et al. 2019). All\nfeatures are projected to 768 dimensions"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: , DialogueTRM achieves SOTA per- Symbol†denotesallmodalitiesusecontext-dependentsettings.",
      "data": [
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": ""
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "sad"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "ACC"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "67.7"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "-"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "75.1"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "89.14"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "68.3"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "80.6"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": "71.04"
        },
        {
          "Table 2: Categorized emotion results on IEMOCAP and MELD.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: , DialogueTRM achieves SOTA per- Symbol†denotesallmodalitiesusecontext-dependentsettings.",
      "data": [
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": "Table 3: Consecutive emotion results on AVEC."
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": "Model\nValence\nArousal\nExpectancy\nPower"
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": ""
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": "scLSTM\n0.14\n0.23\n0.25\n-0.04"
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": "TL-ERC\n0.65\n0.42\n0.35\n-0.03"
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": ""
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": ""
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": "DialogueRNN*\n0.35\n0.59\n0.37\n0.37"
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": ""
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": "0.64\nBiERU*\n0.36\n0.38\n0.37"
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": ""
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": "0.756\n0.4\n0.4\nDialogueTRM\n0.52"
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": ""
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": ""
        },
        {
          "Symbol * in both Table 2 and 3 indicate that models are fed with extra succeeding context.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: , DialogueTRM achieves SOTA per- Symbol†denotesallmodalitiesusecontext-dependentsettings.",
      "data": [
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "0.756\n0.4\n0.4\nDialogueTRM\n0.52"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "on AVEC in Table 3, DialogueTRM achieves SOTA per-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "formance in most of\nthe criteria, which are 16.3%, 5.3%,"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "8.1% improvements from SOTA in terms of R for Valence,"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "Expectancy\nand\nPower,\nrespectively. The\nimprovements"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "are mainly due to two factors. 1) Characteristic temporal"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "modeling that satisﬁes the differentiated context preference"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "for each modality. 2) Multi-grained spatial modeling that"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "provides comprehensive multi-modal\nfusion from different"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "granularities. There\nare\n4\ninteresting\npoints worth men-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "tioning. Firstly, DialogueTRM outperforms TL-ERC (using"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "BERT), which reveals the superiority of our model structure"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "rather\nthan simply using BERT. Secondly, DialogueTRM"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "outperforms models using succeeding context, including Di-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "alogueRNN, DialogueGCN and BiERU, which means our"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "model is more practical in real conversations. Thirdly, from"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "the results of individual emotions in Table 2, DialogueTRM"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "is outstanding in “neutral”. “Neutral” is hard from the tex-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "tual point because some emotional words, such as the red"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "“excited” in Figure 5, could mislead the predictions. Dia-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "logueTRM can avoid such an issue by referring emotional in-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "formation from multiple modalities. Fourthly, The outstand-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "ing performance of R score on “Valence” emotion in Table 3"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "beneﬁts from BERT features, as TL-ERC using BERT also"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "achieves outstanding performance."
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "Inter-modal Results"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "We present\nthe fusion results of F1 in Table 4 on IEMO-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "CAP. The point\nis the uplift from text-only to multi-modal"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "which is denoted as subscripts in the multi-modal column."
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "The “67.67” result\nin the text-only column is based on our"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "intra-modal module fed with text-only features."
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "Comparison with\nSOTA. DialogueRNN is\nthe\nSOTA"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": ""
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "model for ERC in multi-modal settings. It uses concatena-"
        },
        {
          "0.64\nBiERU*\n0.36\n0.38\n0.37": "tion to fuse multi-modal\ninformation. The uplift of MGIF"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PA": "",
          "Utterances": "",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "There's like mystery here, there's magic.",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "It's like a little bit of the unexplainable. I",
          "PB": "",
          "TP": "Exc",
          "MP": "Neu",
          "GT": "Neu"
        },
        {
          "PA": "",
          "Utterances": "just can't see how you're not interested.",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "Yeah.",
          "PB": "",
          "TP": "Neu",
          "MP": "Fru",
          "GT": "Fru"
        },
        {
          "PA": "",
          "Utterances": "",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "God, I don't get it. you know the first",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "time we came here, you said it was the",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "",
          "PB": "",
          "TP": "Exc",
          "MP": "Neu",
          "GT": "Neu"
        },
        {
          "PA": "",
          "Utterances": "best night of your life?",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "And last year, I remember distinctly you",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "said, you were so excited to get here that",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "",
          "PB": "",
          "TP": "Exc",
          "MP": "Neu",
          "GT": "Neu"
        },
        {
          "PA": "context-dependent",
          "Utterances": "you don't remember you stubbed your toe",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        },
        {
          "PA": "",
          "Utterances": "until we were in the car.",
          "PB": "",
          "TP": "",
          "MP": "",
          "GT": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "And last year, I remember distinctly you": "said, you were so excited to get here that"
        },
        {
          "And last year, I remember distinctly you": "T+3\nExc\nNeu\nNeu"
        },
        {
          "And last year, I remember distinctly you": "you don't remember you stubbed your toe"
        },
        {
          "And last year, I remember distinctly you": "until we were in the car."
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "Figure\n5:\nConversation\ncases with MP\n(Multi-modal-"
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "Predicted), TP\n(Text-Predicted)\nand GT (Ground-Truth)"
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "emotions, where ’Neu’, ’Exc’, ’Fru’ stands for neutral, ex-"
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "cited and frustrated, respectively."
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "logueRNN is slightly better than BERT which means it can"
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "hardly extract more useful information from BERT features."
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "Case Study"
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "1) We analyze cases that are incorrectly predicted from the"
        },
        {
          "And last year, I remember distinctly you": "text but correctly predicted from the multi-modal. Among"
        },
        {
          "And last year, I remember distinctly you": "the\ncases,\n“neutral”\nand\n“frustrated”\nare\nin\nthe major-"
        },
        {
          "And last year, I remember distinctly you": "ity with the ratios of about 30.38% and 27.85%,\nrespec-"
        },
        {
          "And last year, I remember distinctly you": "tively.\nIt means multi-modal contributes more to “neutral”"
        },
        {
          "And last year, I remember distinctly you": "and “frustrated” cases. Moreover, about 85.41% “neutral”"
        },
        {
          "And last year, I remember distinctly you": "and\n70.45% “frustrated”\ncases\nare\nrectiﬁed\nfrom nega-"
        },
        {
          "And last year, I remember distinctly you": "tive emotions, which means multi-modal provides easy-to-"
        },
        {
          "And last year, I remember distinctly you": "distinguish information for negative emotions. The reason is"
        },
        {
          "And last year, I remember distinctly you": "probably that human tends to use neutral words to cover their"
        },
        {
          "And last year, I remember distinctly you": "negative emotions but still showing up in the face or speech."
        },
        {
          "And last year, I remember distinctly you": "2) Figure 5 lists a conversation case. The incorrectly text-"
        },
        {
          "And last year, I remember distinctly you": "predicted “excited” emotion at time t+3 is probably because"
        },
        {
          "And last year, I remember distinctly you": "of the misleading word “excited” in the utterance while rec-"
        },
        {
          "And last year, I remember distinctly you": "tiﬁed by multi-modal. 3) The utterance “yeah.” appears 23"
        },
        {
          "And last year, I remember distinctly you": "times in the test set. Given only the current utterance,\nthe"
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "accuracy is “43.48%”. After adding context,\nit\nincreases to"
        },
        {
          "And last year, I remember distinctly you": ""
        },
        {
          "And last year, I remember distinctly you": "“65.22%”. After adding multi-modal, it reaches “73.91%”."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "I. 2019. Real-time emotion recognition via attention gated"
        },
        {
          "References": "[Arevalo et al. 2020] Arevalo,\nJ.;\nSolorio,\nT.; Montes-y",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "hierarchical memory network. AAAI."
        },
        {
          "References": "Gomez, M.; and Gonz´alez, F. A.\n2020. Gated multimodal",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "networks. Neural Computing and Applications 1–20.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Kiros, Salakhutdinov, and Zemel 2014] Kiros,\nR.;"
        },
        {
          "References": "[Busso et al. 2008] Busso,\nC.;\nBulut, M.;\nLee,\nC.-C.;",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "Salakhutdinov, R.;\nand Zemel, R.\n2014.\nMultimodal"
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "conference on\nneural\nlanguage models.\nIn International"
        },
        {
          "References": "Kazemzadeh, A.; Mower, E.; Kim, S.; Chang, J. N.; Lee,",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "S.; and Narayanan, S. S.\n2008.\nIemocap: Interactive emo-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "machine learning, 595–603."
        },
        {
          "References": "tional dyadic motion capture database. Language resources",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Kratzwald et al. 2018] Kratzwald, B.;\nIlic, S.; Kraus, M.;"
        },
        {
          "References": "and evaluation 42(4):335.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "Feuerriegel, S.; and Prendinger, H.\n2018.\nDecision sup-"
        },
        {
          "References": "[Cho et al. 2014] Cho, K.; van Merri¨enboer, B.; Gulcehre,",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "port with text-based emotion recognition: Deep learning for"
        },
        {
          "References": "C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio,",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "affective computing. arXiv preprint arXiv:1803.06397."
        },
        {
          "References": "Y\n. 2014. Learning phrase representations using rnn encoder-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Kuppens, Allen, and Sheeber 2010] Kuppens,\nP.;\nAllen,"
        },
        {
          "References": "decoder\nfor\nstatistical machine\ntranslation.\nIn EMNLP,",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "N. B.; and Sheeber, L. B.\n2010.\nEmotional\ninertia and"
        },
        {
          "References": "1724–1734.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "Psychological\nscience\npsychological\nmaladjustment."
        },
        {
          "References": "[Colneriˆc and Demsar 2018] Colneriˆc, N.,\nand Demsar,\nJ.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "21(7):984–991."
        },
        {
          "References": "2018.\nEmotion recognition on twitter: Comparative study",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Li et al. 2020] Li, W.; Shao, W.;\nJi, S.;\nand Cambria, E."
        },
        {
          "References": "IEEE transactions on affective\nand training a unison model.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "2020. Bieru: Bidirectional emotional recurrent unit for con-"
        },
        {
          "References": "computing.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "versational sentiment analysis."
        },
        {
          "References": "[Datcu and Rothkrantz 2014] Datcu,\nD.,\nand\nRothkrantz,",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Liang et al. 2018] Liang, P. P.; Liu, Z.; Zadeh, A. B.; and"
        },
        {
          "References": "L. J. 2014. Semantic audio-visual data fusion for automatic",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "Morency, L.-P.\n2018. Multimodal\nlanguage analysis with"
        },
        {
          "References": "emotion recognition. Emotion recognition: a pattern analy-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "recurrent multistage fusion.\nIn EMNLP, 150–161."
        },
        {
          "References": "sis approach 411–435.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Loshchilov and Hutter 2018] Loshchilov,\nI., and Hutter, F."
        },
        {
          "References": "[Devlin et al. 2019] Devlin, J.; Chang, M.-W.; Lee, K.; and",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "2018.\nFixing weight decay regularization in adam, corr,"
        },
        {
          "References": "Toutanova, K. 2019. Bert: Pre-training of deep bidirectional",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "the ICLR 2018 Confer-\nabs/1711.05101.\nIn Proceedings of"
        },
        {
          "References": "transformers for language understanding.\nIn NAACL, 4171–",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "ence, volume 30."
        },
        {
          "References": "4186.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Majumder et al. 2019] Majumder, N.; Poria, S.; Hazarika,"
        },
        {
          "References": "[Ekman 1993] Ekman, P. 1993. Facial expression and emo-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "D.; Mihalcea, R.; Gelbukh, A.; and Cambria, E. 2019. Dia-"
        },
        {
          "References": "tion. American psychologist 48(4):384.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "loguernn: An attentive rnn for emotion detection in conver-"
        },
        {
          "References": "[Eyben 2010] Eyben, e.\n2010. Opensmile:\nthe munich ver-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "sations.\nIn AAAI, volume 33, 6818–6825."
        },
        {
          "References": "satile and fast open-source audio feature extractor.\nIn Pro-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Mao et al. 2014] Mao,\nJ.; Xu, W.; Yang, Y.; Wang,\nJ.;"
        },
        {
          "References": "ceedings of the 18th ACM international conference on Mul-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "Huang, Z.; and Yuille, A. 2014. Deep captioning with mul-"
        },
        {
          "References": "timedia, 1459–1462.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "arXiv preprint\ntimodal\nrecurrent neural networks (m-rnn)."
        },
        {
          "References": "[Ghosal et al. 2019] Ghosal, D.; Majumder, N.; Poria, S.;",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "arXiv:1412.6632."
        },
        {
          "References": "Chhaya, N.;\nand Gelbukh, A.\n2019.\nDialogueGCN: A",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Mao et al. 2018] Mao, Y.; Zhou, C.; Wang, X.; and Li, R."
        },
        {
          "References": "graph convolutional neural network for emotion recognition",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "2018.\nShow and tell more: Topic-oriented multi-sentence"
        },
        {
          "References": "in conversation.\nIn EMNLP, 154–164.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "image captioning.\nIn IJCAI, 4258–4264."
        },
        {
          "References": "[Gu et al. 2019] Gu, Y.; Lyu, X.; Sun, W.; Li, W.; Chen, S.;",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Mehrabian 1996] Mehrabian, A.\n1996.\nPleasure-arousal-"
        },
        {
          "References": "Li, X.; and Marsic,\nI.\n2019. Mutual correlation attentive",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "dominance: A general\nframework for describing and mea-"
        },
        {
          "References": "factors in dyadic fusion networks for speech emotion recog-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "suring individual differences in temperament. Current Psy-"
        },
        {
          "References": "the 27th ACM International Con-\nnition.\nIn Proceedings of",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "chology 14(4):261–292."
        },
        {
          "References": "ference on Multimedia, 157–166.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Morris and Keltner 2000] Morris, M. W.,\nand Keltner, D."
        },
        {
          "References": "[Hazarika et al. 2018a] Hazarika, D.; Poria, S.; Mihalcea, R.;",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "2000.\nHow emotions work: The social\nfunctions of emo-"
        },
        {
          "References": "Cambria, E.; and Zimmermann, R. 2018a.\nIcon: interactive",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "tional expression in negotiations. Research in organizational"
        },
        {
          "References": "conversational memory network for multimodal emotion de-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "behavior 22:1–50."
        },
        {
          "References": "tection.\nIn EMNLP, 2594–2604.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "[Hazarika et al. 2018b] Hazarika, D.; Poria, S.; Zadeh, A.;",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Picard 2010] Picard, R. W.\n2010.\nAffective computing:"
        },
        {
          "References": "Cambria, E.; Morency, L.-P.; and Zimmermann, R.\n2018b.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "IEEE Transactions on Affective Com-\nfrom laughter to ieee."
        },
        {
          "References": "Conversational memory network for emotion recognition in",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "puting 1(1):11–17."
        },
        {
          "References": "dyadic dialogue videos.\nIn NAACL, 2122–2132.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Poria et al. 2017] Poria, S.; Cambria, E.; Hazarika, D.; Ma-"
        },
        {
          "References": "[Hazarika et al. 2019] Hazarika, D.; Poria, S.; Zimmermann,",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "jumder, N.; Zadeh, A.; and Morency, L.-P.\n2017. Context-"
        },
        {
          "References": "R.; and Mihalcea, R. 2019. Emotion recognition in conver-",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "dependent sentiment analysis in user-generated videos.\nIn"
        },
        {
          "References": "sations with transfer learning from generative conversation",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "ACL, 873–883."
        },
        {
          "References": "modeling. arXiv preprint arXiv:1910.04980.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": ""
        },
        {
          "References": "",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "[Poria et al. 2019a] Poria, S.; Hazarika, D.; Majumder, N.;"
        },
        {
          "References": "[Jiao et al. 2019]\nJiao, W.; Yang, H.; King,\nI.;\nand Lyu,",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "Naik, G.; Cambria, E.; and Mihalcea, R.\n2019a. Meld: A"
        },
        {
          "References": "M. R. 2019. HiGRU: Hierarchical gated recurrent units for",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "multimodal multi-party dataset\nfor emotion recognition in"
        },
        {
          "References": "utterance-level emotion recognition.\nIn NAACL, 397–406.",
          "[Jiao, Lyu, and King 2019]\nJiao, W.; Lyu, M. R.; and King,": "conversations.\nIn ACL, 527–536."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "and Hovy, E. 2019b. Emotion recognition in conversation:",
          "Differential networks\nfor visual question answering.": "AAAI, volume 33, 8997–9004.",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "IEEE\nResearch challenges, datasets, and recent advances.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "",
          "Differential networks\nfor visual question answering.": "[Zeng et al. 2007] Zeng, Z.; Hu, Y.; Roisman, G.",
          "In": "I.; Wen,"
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Access 7:100943–100953.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "",
          "Differential networks\nfor visual question answering.": "Z.; Fu, Y.; and Huang, T. S.\n2007.",
          "In": "Audio-visual sponta-"
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Schr¨oder 2003] Schr¨oder, M. 2003. Experimental study of",
          "Differential networks\nfor visual question answering.": "neous emotion recognition.\nIn Artiﬁcal",
          "In": "intelligence for hu-"
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "affect bursts. Speech communication 40(1-2):99–116.",
          "Differential networks\nfor visual question answering.": "man computing. Springer. 72–90.",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Schuller et al. 2012] Schuller, B.; Valster, M.; Eyben, F.;",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Cowie, R.; and Pantic, M.\n2012. Avec 2012:\nthe continu-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "the\nous audio/visual emotion challenge.\nIn Proceedings of",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "14th ACM international conference on Multimodal\ninterac-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "tion, 449–456.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Shaheen et al. 2014] Shaheen, S.; El-Hajj, W.; Hajj, H.; and",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Elbassuoni, S.\n2014. Emotion recognition from text based",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "2014\nIEEE Inter-\non\nautomatically\ngenerated\nrules.\nIn",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "national Conference on Data Mining Workshop, 383–392.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "IEEE.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Strapparava, Valitutti, and others 2004] Strapparava,\nC.;",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Valitutti, A.;\net\nal.\n2004.\nWordnet\naffect:\nan affective",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "extension of wordnet.\nIn Lrec, volume 4,\n40. Citeseer.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Tran et al. 2015] Tran, D.; Bourdev, L.; Fergus, R.; Torre-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "sani, L.; and Paluri, M. 2015. Learning spatiotemporal fea-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "tures with 3d convolutional networks.\nIn Proceedings of the",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "IEEE international conference on computer vision, 4489–",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "4497.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Tsai et al. 2019a] Tsai, Y.-H. H.; Bai, S.; Liang, P. P.; Kolter,",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "J. Z.; Morency, L.-P.; and Salakhutdinov, R. 2019a. Multi-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "modal\ntransformer\nfor unaligned multimodal\nlanguage se-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "quences.\nIn ACL.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Tsai et al. 2019b] Tsai, Y.-H. H.; Liang, P. P.; Zadeh, A.;",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Morency, L.-P.; and Salakhutdinov, R. 2019b. Learning fac-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "torized multimodal representations.\nIn ICLR.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Tzirakis et al. 2017] Tzirakis, P.; Trigeorgis, G.; Nicolaou,",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "M. A.; Schuller, B. W.; and Zafeiriou, S.\n2017.\nEnd-to-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "end multimodal emotion recognition using deep neural net-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "works. IEEE Journal of Selected Topics in Signal Processing",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "11(8):1301–1309.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.;",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polo-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "sukhin, I. 2017. Attention is all you need.\nIn NIPS, 5998–",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "6008.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Wang et al. 2019] Wang, Y.; Shen, Y.; Liu, Z.; Liang, P. P.;",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Zadeh, A.; and Morency, L.-P. 2019. Words can shift: Dy-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "namically adjusting word representations using nonverbal",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "behaviors.\nIn AAAI, volume 33, 7216–7223.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Wang et al. 2020] Wang, Y.; Zhang, J.; Ma, J.; Wang, S.; and",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Xiao, J. 2020. Contextualized emotion recognition in con-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "the 21th\nversation as sequence tagging.\nIn Proceedings of",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "Annual Meeting of the Special Interest Group on Discourse",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "and Dialogue, 186–195.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[W¨ollmer et al. 2010] W¨ollmer, M.; Metallinou, A.; Eyben,",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "F.; Schuller, B.; and Narayanan, S. 2010. Context-sensitive",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "multimodal emotion recognition from speech and facial ex-",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "INTER-\npression\nusing\nbidirectional\nlstm modeling.\nIn",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "SPEECH, 2362–2365.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        },
        {
          "[Poria et al. 2019b] Poria, S.; Majumder, N.; Mihalcea, R.;": "[Wu et al. 2019] Wu, C.; Liu, J.; Wang, X.; and Li, R. 2019.",
          "Differential networks\nfor visual question answering.": "",
          "In": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation",
      "authors": [
        "J Arevalo",
        "T Solorio",
        "M Montes-Y Gomez",
        "F González",
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan",
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2008",
      "venue": "Neural Computing and Applications 1-20"
    },
    {
      "citation_id": "2",
      "title": "Semantic audio-visual data fusion for automatic emotion recognition. Emotion recognition: a pattern analysis approach",
      "authors": [
        "Demsar ; Colneriĉ",
        "N Colneriĉ",
        "J Demsar",
        "D Datcu",
        "L Rothkrantz"
      ],
      "year": "2014",
      "venue": "Semantic audio-visual data fusion for automatic emotion recognition. Emotion recognition: a pattern analysis approach"
    },
    {
      "citation_id": "3",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Devlin"
      ],
      "year": "1993",
      "venue": "NAACL"
    },
    {
      "citation_id": "4",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "E Eyben"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Mutual correlation attentive factors in dyadic fusion networks for speech emotion recognition",
      "authors": [
        "Ghosal"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Hazarika"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "7",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Hazarika"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "authors": [
        "Hazarika"
      ],
      "year": "2019",
      "venue": "NAACL",
      "arxiv": "arXiv:1910.04980"
    },
    {
      "citation_id": "9",
      "title": "Multimodal neural language models",
      "authors": [
        "Salakhutdinov Kiros",
        "R Zemel ; Kiros",
        "R Salakhutdinov",
        "R Zemel"
      ],
      "year": "2014",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "10",
      "title": "Decision support with text-based emotion recognition: Deep learning for affective computing",
      "authors": [
        "Kratzwald"
      ],
      "year": "2010",
      "venue": "Psychological science",
      "arxiv": "arXiv:1803.06397"
    },
    {
      "citation_id": "11",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "Liang"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "12",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Loshchilov",
        "I Hutter ; Loshchilov",
        "F Hutter",
        "Majumder"
      ],
      "year": "2014",
      "venue": "Deep captioning with multimodal recurrent neural networks (m-rnn)",
      "arxiv": "arXiv:1412.6632"
    },
    {
      "citation_id": "13",
      "title": "Pleasure-arousaldominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian ; Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "14",
      "title": "How emotions work: The social functions of emotional expression in negotiations",
      "authors": [
        "Keltner ; Morris",
        "M Morris",
        "D Keltner"
      ],
      "year": "2000",
      "venue": "Research in organizational behavior"
    },
    {
      "citation_id": "15",
      "title": "Affective computing: from laughter to ieee",
      "authors": [
        "R Picard ; Picard"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Contextdependent sentiment analysis in user-generated videos",
      "authors": [
        "Poria"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "17",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Poria"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Poria"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "19",
      "title": "Experimental study of affect bursts",
      "authors": [
        "M Schröder ; Schröder"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "20",
      "title": "Avec 2012: the continuous audio/visual emotion challenge",
      "authors": [
        "Schuller"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition from text based on automatically generated rules",
      "authors": [
        "Shaheen"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Data Mining Workshop"
    },
    {
      "citation_id": "22",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "Strapparava",
        "C Valitutti ; Strapparava",
        "A Valitutti"
      ],
      "year": "2004",
      "venue": "Lrec"
    },
    {
      "citation_id": "23",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Tsai"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "24",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Tsai"
      ],
      "year": "2017",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "25",
      "title": "Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional lstm modeling",
      "authors": [
        "Wöllmer"
      ],
      "year": "2010",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "26",
      "title": "Differential networks for visual question answering",
      "authors": [
        "Wu"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "27",
      "title": "Audio-visual spontaneous emotion recognition",
      "authors": [
        "Zeng"
      ],
      "year": "2007",
      "venue": "Artifical intelligence for human computing"
    }
  ]
}