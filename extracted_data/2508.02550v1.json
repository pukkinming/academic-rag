{
  "paper_id": "2508.02550v1",
  "title": "Stakeholder Perspectives On Humanistic Implementation Of Computer Perception In Healthcare: A Qualitative Study",
  "published": "2025-08-04T16:01:56Z",
  "authors": [
    "Kristin M. Kostick-Quenet",
    "Meghan E. Hurley",
    "Syed Ayaz",
    "John Herrington",
    "Casey Zampella",
    "Julia Parish-Morris",
    "Birkan Tunç",
    "Gabriel Lázaro-Muñoz",
    "J. S. Blumenthal-Barby",
    "Eric A. Storch"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Background Computer perception (CP) technologies-including digital phenotyping, affective computing and related passivesensing-approaches-offer unprecedented opportunities to personalize healthcare, especially mental healthcare, yet they also provoke concerns about privacy, bias and the erosion of empathic, relationship-centered practice. A comprehensive understanding of perceived risks and benefits from those who design, deploy and experience these tools in real-world settings remains elusive. \n Objective This study aimed to explore key stakeholder perspectives on potential benefits, risks and concerns around the integration of CP technologies into patient care. Better understanding of these concerns is crucial for responding to and mitigating such concerns via design implementation strategies that augment, rather than compromise, patient-centered and humanistic care and associated outcomes. \n Methods We conducted in-depth, semi-structured interviews with 102 stakeholders spanning the CP life cycle: adolescent patients (n = 20) and their caregivers (n = 20), frontline clinicians (n = 20), technology developers (n = 21) and ethics, legal, policy or philosophy scholars (n = 21). Interviews (~ 45 min each) explored perceived benefits, risks and implementation challenges of CP in clinical care. Transcripts underwent thematic analysis by a multidisciplinary team; reliability was enhanced through double coding-and consensus adjudication. Results Stakeholders articulated seven interlocking concern domains: (1) trustworthiness and data integrity; (2) patient-specific-relevance; (3) utility and workflow integration; (4) regulation and governance; (5) privacy and data protection; (6) direct and indirect patient harms; and (7) philosophical critiques of reductionism. A cross-cutting insight was the primacy of context and subjective meaning in determining whether CP outputs are clinically valid and actionable. Participants warned that without attention to these factors, algorithms risk mis-classification and relational harm. To operationalize humanistic safeguards, we propose \"personalized roadmaps\": codesigned plans that predetermine which metrics will be monitored-, how and when feedback is shared, thresholds for clinical action, and procedures for reconciling discrepancies between algorithmic inferences and lived experience. Roadmaps embed patient education, dynamic consent, and tailored feedback, thereby aligning CP deployment with patient autonomy, therapeutic alliance and ethical transparency. \n Conclusions This multistakeholder study provides the first comprehensive, evidence-based account of relational, technical and governance challenges raised by CP tools in clinical care. By translating these insights into personalized roadmaps, we offer a practical framework for developers, clinicians and policymakers seeking to harness continuous behavioral data while preserving the humanistic core of care.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Computer perception (CP) tools, including digital phenotyping, affective computing, computational behavioral analysis and other approaches that entail continuous and passive data collection using wearables and smartphone sensing, have been positioned as a remedy for longstanding diagnostic and therapeutic blind spots in mental healthcare. The term \"computer perception\" references the artificial intelligence (AI) subfield of computer \"vision\" but acknowledges a wider range of perceptive modalities beyond vision alone (e.g., \"hearing\" through microphones; motion detection through accelerometers, etc.). By leveraging sensors already embedded in everyday devices, these systems promise scalable, accessible surveillance of mood, cognition, and social behavior, potentially addressing the field's chronic reliance on infrequent patient self-reports and clinician observation  [1, 2] . CP tools also promise a personalized and patient-tailored diagnostic and therapeutic approach, in line with precision medicine goals  [3, 4, 5] . Early studies suggest that CP-derived markers can forecast relapse in bipolar disorder, detect prodromal psychosis, tailor just-in-time behavioral prompts, and potentially widen access to mental health care. Yet the very features that make CP appealing also expose patients to unprecedented privacy risks, algorithmic bias, and the potential erosion of empathic, relationshipcentered care  [1, 6, 7, 8] .\n\nEthicists, regulators, and frontline stakeholders caution that integrating such pervasive sensing into care can imperil core values of confidentiality, fairness, and relational trust  [9, 10, 11, 12] . These impacts can be exacerbated by opaque algorithms, unclear pathways for secondary data reuse, and difficulties obtaining meaningful informed consent in continuous monitoring scenarios.\n\nA limited number of studies  [13, 14, 15]  provide a foundation for understanding some of these concerns; however, no empirical research to date offers a comprehensive view of the wide-ranging perspectives held by diverse stakeholder groups regarding the benefits and risks of integrating CP into care. The present study addresses this gap through an empirical exploration of diverse stakeholder perspectives, with special attention to impacts on humanistic, relationship-centered care.\n\nThe rationale for focusing on humanistic care is to balance enthusiasm for these tools with a recognition that healing is a human enterprise grounded in empathy, dignity, and shared decisionmaking. Humanistic and humanized care frameworks  [16, 17, 18]  remind us that respectful dialogue, cultural sensitivity, and patient partnership are interwoven into the moral fabric of good practice  [19] . Whether CP ultimately augments or erodes that fabric depends on how well designers, clinicians, and regulators anticipate a spectrum of ethical concerns voiced by those who will build, deploy, or live with these systems. This study therefore turns to those diverse stakeholders -developers, clinicians, patients, caregivers, and ethics, legal and policy-scholarsto ask how their concerns can guide the integration of CP in ways that preserve, rather than diminish, the humanization of care.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Background",
      "text": "What makes CP technologies unique is that they involve algorithmic inferences about a person's moment-to-moment mental, or socio-behavioral state or predicted outcomes such as mood relapse, suicidality, or treatment response  [2, 12, 20, 21] . These inferences are enabled by the ingestion of vast amounts of behavioral, physiological, and environmental signals from (usually) ordinary connected devices such as smartphones and wearables. Less often, they may involve implantable systems to continuously monitor physiological  [22, 23]  or neural activity  [24] .\n\nIn psychiatric contexts, the approach is often called digital phenotyping, entailing the use of smartphones, wearables, and ambient sensors to stream accelerometry, GPS traces, keystroke dynamics, speech acoustics, heart rate variability, and other passively captured metadata. Those streams are preprocessed, feature-engineered  [25, 26]  and fed into statistical or deep learning models. Parallel work in affective computing  [27]  extends the approach to facial microexpressions, vocal prosody, or text sentiment to classify discrete emotions or arousal levels in real time.  [59]  extends the approach to facial micro-expressions, vocal prosody, or text sentiment to classify discrete emotions or arousal levels in real time.\n\nBecause CP systems sit at the intersection of pervasive sensing and advancements in AI, they raise many of the same ethical issues highlight by AI systems. Concerns about algorithmic bias, transparency, explainability, interpretability, fairness and other aspects of \"trustworthy\" AI  [28, 29]  are relevant. The rarity by which CP tools are validated on large, diverse validation cohorts means that algorithmic performance is likely to vary dramatically across demographic and clinical groups, raising reliability concerns and potentially amplifying health disparities  [30] . Critics have also warned against over-reliance on algorithmic inferences about patients' health status  [31, 32] ,\n\nespecially \"black box\" systems that resist clinical scrutiny and accountability and compromise informed clinical decision-making  [33] . Others  [15, 34]  underscore legal uncertainties surrounding liability in cases of error, patient harm, or mismanagement of outputs or other feedback. The U.S.\n\nNational Institute of Standards and Technology's AI Risk Management Framework and the EU AI Act both categorize health-related CP tools as \"high risk,\" demanding rigorous safety, fairness, and oversight provisions  [35, 36] .\n\nSimilar to other AI systems, CP tools thrive on voluminous data sets, not only across individuals but for each individual, often called individual \"big data\" or \"deep data\"  [37] . Ethical critiques thus consistently foreground privacy vulnerabilities associated with sensitive behavioral data  [13, 14, 38, 39] , and there is expert consensus  [13]  around the need for privacy and innovative consent approaches. Scholars  [34, 40, 41]  caution against unwanted or involuntary disclosure to third parties, such as insurers or employers, in scenarios where data are controlled by consumergrade device companies. Dynamic consent models have also been proposed  [39]  to replace onetime or broad consent approaches with ongoing, granular permissions; however, feasibility remains challenging  [42] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Challenges For Humanistic Care",
      "text": "Critics  [9, 43, 44]  have also converged on a deeper worry: as algorithms assume a larger share of the responsibility to observe and listen, the relational core of care risks being reduced to a \"metrics management\" exercise, whereby clinicians and patients spend their limited time consulting data trends rather than discussing the patient's lived experience and therapeutic goals.\n\nClinicians fear that multimodal dashboards could displace narrative dialogue, shifting the burden of self-monitoring and, by extension, responsibility for changes in functioning, onto patients in ways that compromise dignity and mutual trust  [9, [45] [46] [47]  and overprioritize technological over humanistic solutions  [48] . Some warn that automated detection and treatment of illness may weaken the rapport and goal alignment that bolsters the therapeutic alliance, unless paired with explicit, empathic communication strategies  [49] .\n\nA limited set of empirical work reinforces these cautions. One study  [47]  documents mental health clinician enthusiasm toward gaining rich, real-time insights but also highlights concerns about workflow overload and potential for automation bias, i.e., deferring to algorithmic judgments even when they conflict with a clinician's intuitions or a patient's lived story. Another study  [46]  highlighted clinicians' concerns that prioritizing passive data trends over self-reported narratives or active responses to clinical assessments could reduce opportunities for patients to reflect upon their mental health, leading to reduced patient engagement. Experts  [9, 21]  have raised flags that such asymmetries can tilt encounters toward dehumanization and require careful planning and implementation to achieve the goal of making otherwise invisible patterns visible and clinically useful.\n\nThese relational stakes bring long-standing ethical principles into focus and urge clinicians and researchers to keep dignity, empathy, patient empowerment, and shared decision-making at the forefront of clinical care. However, it remains-unclear how best to do this in ways that engage multiple and often competing perspectives. Our study addresses this gap by exploring the range of concerns through interviews with over 100 stakeholders across the CP lifecycle. We catalogue considerations that extend beyond well-elaborated privacy and bias debates to the lessoperationalized relational harms that data-centric care may impose. By situating these concerns within established humanistic frameworks of dignity, empathy, and shared decision-making  [17, 18] , we offer an anticipatory roadmap for researchers, developers, clinicians and patients. The goal is not merely to identify technical fixes, but to ensure that as CP systems mature, they deepen rather than diminish the person-centered relationships that remain the centerpiece of care.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Methods",
      "text": "As part of 4-year study funded by the National Center for Advancing Translational Sciences (R01TR004243), we conducted in-depth, semi-structured interviews (total n=102), including with adolescent patients (n=20) and caregiver (n=20) dyads, clinicians (n=20), and developers (n=21), and ethics, legal, policy and philosophy scholars (n=21) to explore their perspectives on potential benefits, risks and concerns around the integration of CP technologies into their care. Respondents were recruited from a \"sister\" study (5R01MH125958) aiming to validate CP tools designed to quantify objective digital biobehavioral markers of socio-emotional functioning.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Participants. Participants Included A Clinical Sample Of Adolescents (Aged 12-17 Years)",
      "text": "with varied diagnoses, including Autism, Tourette's, Anxiety, Obsessive-Compulsive Disorder (OCD), and Attention Deficit/Hyperactivity Disorder (ADHD), as well as their caregivers (typically biological parents, Table  1 ). Diagnostic presentations for all adolescents were confirmed by expert providers using established clinical measures. Adolescent-caregiver dyads were referred to the current study by the sister study's coordinator and then contacted by a research assistant via phone or email to schedule an interview. Clinicians and developers (Table  2 ) were identified through online literature search and through existing professional networks. Participants were interviewed between January 2023 and August 2023.  Data Analysis. Interviews were audio-recorded, transcribed verbatim, and analyzed using MAXQDA software. Led by a qualitative methods expert (KK-Q), team members developed a codebook to identify thematic patterns in adolescent and caregiver responses to questions addressing the topics above. Each interview was coded by merging work from two separate coders to reduce interpretability bias and enhance reliability. We used Thematic Content Analysis  [50, 51]  to inductively identify themes by progressively abstracting relevant quotes, a process that entails reading every quotation to which a given code was attributed, paraphrasing each quotation (primary abstraction) and further identifying which constructs were addressed by each quotation (secondary abstraction), and organizing constructs into themes. To enhance the validity of our findings, all abstractions were validated by at least one other member of the research team. In rare cases where abstractions reflected different interpretations, members of our research team met to reach consensus.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "Stakeholders raised a wide range of concerns around the following themes: 1)\n\nTrustworthiness and Integrity of CP Technologies (Table  3 ); 2) Patient-Specific Relevance (Table  4 ); 3) Utility and Implementation Challenges (Table  5 ); 4) Regulation and Governance (Table  6 ); 5) Data Privacy and Protection (Table  7 ); 6) Patient Harms (Table  8 ); and 7) Philosophical Critiques (Table  9 ). All themes and sub-themes are elaborated below, with illustrative quotations in the associated Tables  3 4 5 6 7 8 9 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Trustworthiness And Integrity Of Cp Technologies",
      "text": "Data Quality Constraints and Confounds. Developers, more than other groups, raised concerns about the reliability of data streams from consumer-grade devices, noting that variations in user behavior and differences in hardware performance can make it difficult to distinguish true physiological changes from sensor errors. They cautioned that without standardized protocols for device calibration and data collection, models built on these inputs risk failing when deployed across different environments or patient populations.\n\nAlgorithmic Bias and Generalizability. Participants from all groups also expressed concern over other types of algorithmic bias. Some scholars emphasized that many AI models are trained on homogenous datasets, limiting their applicability to broader, more diverse groups. They explained that because these algorithms often derive from data collected predominantly in  \"How do we account for even the fact that disorders that we are trying to detect discreetly and separately from each other, might actually be... Like problems of living, they might be network problems rather than sort of distinct entities that can be detected and discerned.\" (ELPP_01)",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Uncertain",
      "text": "Illness Ontologies \"…I think the biggest thing that scares me… we don't really have any objective markers… we're kind of assuming that there's an objective entity that we can find, and that data's going to be the answer…\" (ELPP_16) ; \"...part of what I think has been so problematic for mental health is the diagnostic scheme is A, doesn't really have a strong scientific basis using any kind of objective measures and B, link in any way to treatment response or etiology, either one of those. So in some ways, it's a harder task than it is when you're looking for digital phenotyping in some other areas of medicine. And I guess the best example is using AI to read mammograms where you have a ground truth from having done a biopsy and which tumors are malignant and which are not. We just don't have that here. We don't have a biopsy, we don't have any solid footing that we're going to go up against, and that to me is the biggest challenge for the field.\" (ELPP_20)  \"I'm a very type A person. Go, go, go. To stay home for a while or don't get out of bed... even if I'm really stressed or having a bad day, I don't do that, because I'll always have a to-do list. Even if I'm having a bad day, I have to do these things. [The system] might not realize sometimes that I'm overly stressed or upset, just because I'm still going on with my normal day.\" (P_17) \"Even objective data needs confirmation from subjective reports, need to understand what patients were going through internally when data was collected -have good quotes on this; \"I think [these technologies would be useless without subjective input] in the sense that there's no good healthcare without patient consent and input because it's about their health.",
      "page_start": 12,
      "page_end": 17
    },
    {
      "section_name": "It'S Not About What We Think Is Going On With Their Health, It'S About Their Experience Of It.",
      "text": "We should not use it if we are not going to take what the patient says into consideration.\" (C_12) Capturing Individual heterogeneity \"There's a huge amount of individual variability, and I think one of the things we learned is this is harder than it looks.\" (ELPP_20) S\"o, I think some people are very emotive and other people ... For example, for some people when they become anxious, they're nervous, they're fidgety, they're sweating, they're tremulous... But there are some people when they become anxious, they look disinterested... not tuned in... not paying attention. So people will express those emotions differently. And would a computer be able to pick that up?\" (CG_01) \"If I'm sad, it's pretty obvious, if I'm pissed off, it's pretty obvious, but I know a lot of other people it would probably be harder. Is [the algorithm] going to be able to pick up on the same exact signs it would for someone else?\" (P_19)",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Utility And Implementation Challenges",
      "text": "Role of CP in Clinical Care. Stakeholders from all groups voiced a set of interrelated concerns about how CP tools are integrated into clinical workflows. Scholars and clinicians cautioned that clinicians may lean too heavily on algorithmic outputs, risking a form of \"deskilling\" in which they stop rigorously scrutinizing the data for quality or epistemic inconsistencies. They warned that clinicians may come to accept CP suggestions uncritically (automation bias), thereby sidelining the human, relational interpretations arrived at through patient-provider dialogue.\n\nManaging Risk and Liability. Clinicians, more than other groups, highlighted the dual dangers of missed events and over-alerting. They noted that false negatives -instances where the system fails to detect deterioration -could leave patients unprotected, while excessive false positives could overwhelm clinicians and erode confidence in the tool, ultimately undermining patient safety rather than enhancing it. Clinicians also raised concerns about whether they may eventually be expected to use CP tools as they continue to evolve, or held liable if they choose not to, compromising their autonomy in clinical decision making.\n\nBarriers to Utility. All stakeholder groups stressed that CP outputs must be interpretable and meaningful in real-world contexts to be actionable. Clinicians stressed that data trends and inferences must be delivered through intuitive summaries and visualizations, accompanied by concise, actionable recommendations. They described how this is complicated by observations that the clinical significance of data trends is likely to vary from one situation to another (see Patient Specific Relevance above), complicating reliable interpretation. Developers and clinicians also expressed concerns about the potential for confirmation bias, where users may cherry-pick data that confirms their expectations, undermining the aim of these technologies to provide novel informational value to clinical assessments.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Table 5. Utility & Implementation Challenges",
      "text": "Themes Illustrative Quotes",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Role Of Cp In Clinical Care",
      "text": "Tech Overreliance Clinician Deskilling \"And there is research about clinicians relying too much on the decision support and not questioning it. A bit like somebody on a conveyor looking at a conveyor belt in a manufacturing factory where there was AI sensing for defects. In the end they're not doing anything. They're like someone sitting in a driverless car. They're not really a human in the loop... Who's making life changing decisions or life affecting decisions?\" (ELPP_04) \"And so I think what you're seeing is that there may be a distributional gulf that increases in the quality of hyper users with maintaining the domain expertise and low intensity users who become in fact fully reliant on the systems.\" (D_19) \"In the worst case scenario, I was thinking about how would I feel about this if I was a first year therapist in grad school doing training, and I wonder... It could be that it's useful because it's like, 'Oh, here's some things to consider.' But it also could be that it makes someone doubt their own burgeoning clinical judgments or, 'Well, the computer said this, so I guess I must have missed that and let's go with that.'\" (C_15) Automation Bias \"If an AI just presents one recommendation, then the user could be prone to automation bias. Automation bias being the over-reliance or the complacency of just accepting whatever the AI says, without... And are we as rigorous in looking at the data when there's AI assisting?\" (D_15) \"If there's some sort of problem with [the inferences] and you're just sort of taking it as like, \"Oh, okay, this is what it says\", it could make you make the wrong sorts of decisions. Or again, maybe make you think...  '",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Unclear / Insufficient Regulatory Frameworks",
      "text": "Regulatory Grey Zone \"So, one challenge that's coming up in the mental health field is where apps are pitched as being about, well-being rather than necessarily treating a mental health condition. An app might be doing very similar things, tracking the same data, continuous monitoring, and potentially suggesting certain things that might be going on. But because it's built in terms of wellness, then that takes it outside of the governance regime of medical devices.\" (ELPP_15) \"Yeah, I mean, I think one important question to ask is, is this thing offering me medical advice? Or is this thing practicing medicine? And if the answer is yes, then it should have the requisite regulatory approvals to do so. And if not, then it should be very, very clear that the clinician is in the loop...I think that's where any company maybe could get in trouble in that gray area. There is a pretty bright line between practicing medicine and not. Monitoring & Surveillance. Stakeholders also observed that when individuals feel monitored rather than supported, they may withhold information, worry about data misuse, and question their providers' trustworthiness. This concern may be particularly relevant for vulnerable populations, such as people experiencing psychosis, who may perceive passive monitoring as surveillance, and older adults who may have difficulty using wearables and apps, highlighting the need for adaptive protocols, additional safeguards, and alternative engagement strategies that respect each patient's autonomy and comfort. Stakeholders noted that passive monitoring can shift the experience from feeling supported to feeling observed, leading individuals to withhold information, worry about how their data might be used, and question providers' trustworthiness.\n\nThis effect may be especially pronounced among vulnerable groups; for example, people experiencing psychosis may interpret continuous tracking as intrusive surveillance, or members of historically exploited populations may hold significant reservations. Appropriateness \"I think there's a question about whether we should be doing that, whether that is an appropriate way to integrate these kinds of tools into clinical care when they're going to be tracking people in their homes, in their private lives, revealing information that is sensitive and completely not health-related to their clinician..",
      "page_start": 22,
      "page_end": 28
    },
    {
      "section_name": ".\" (Elpp_16)",
      "text": "There is quite a distinct ickiness factor to being able to look that deeply into someone's personal life. As a clinician, when I've had the opportunity to work with data like that or look at data like that, even when it's de-identified, it's quite uncomfortable. \"We are selling out our healthcare data to groups like Google, Amazon, etc. We are setting up our healthcare systems in a situation that healthcare professionals can no longer control their own data…\" (ELPP_14) Commercial Influence \"I think that as healthcare systems begin to act like corporations, the corporatization of healthcare data will mean that monetizing facial expression data, location data, mood data, wearables, is going to be an incredible risk for healthcare systems. So I'm very worried that healthcare systems have to be reinforced socially as having a fiduciary duty to patients as patients, rather than... to stakeholders.\" (D_19) \"If my doctor had told me a year ago, I really want you to sleep better, so try this out,\" and I did it for a while and I thought it was only my doctor looking at it.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Monitoring & Surveillance",
      "text": "Can Exacerbate Patient Distrust \"I think that some of the risks that I'm concerned about are how it would negatively impact the public's stigma towards mental health care. I think that it could dissuade people from engaging in that. It feels like policing, which is not great, and I think for marginalized communities, extra not great... I haven't really seen how this is applicable to kids, but people already perceive me as policing them because I'm a mandated reporter, so there's that.\" (C_13) \"And a reasonable response might be, 'Well, this is just another form of surveillance. Why do they do this?' And especially, again, in people who might be vulnerable with mental health issues, the last thing you want to do is give someone something that they think is increasing surveillance if they're having some issues around surveillance.\" (ELPP_14)",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Patient Impacts And Harms",
      "text": "Stakeholders highlighted numerous ways that the above concerns may translate into direct or indirect harms for patients:\n\nHarms Due to Inaccurate or Premature Diagnoses. Stakeholders from all groups cautioned that algorithmic assessments delivered without sufficient clinical context can trigger a cascade of inappropriate interventions. They warned that acting on false positives or early \"flags\" could expose patients to unnecessary tests, treatments, or stigma long before a human expert has had a chance to validate the finding. They also pointed out the negative impacts on patients when algorithmic conclusions may differ from patients' own perceptions and experiences, creating conflict with no clear resolution protocols.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Diminished Human Connection In Healthcare.",
      "text": "A recurring theme, particularly among clinicians and patients, was the breakdown of human connection in healthcare. Many stakeholders noted that an over-reliance on data-driven CP tools could transform healthcare into a more transactional and less empathetic process. Clinicians especially highlighted the importance of maintaining therapeutic relationships characterized by respect, empathy and alliance, warning that digital tools, while potentially efficient, could diminish the \"human touch\" that is central to healing. Many patients and caregivers echoed this concern, fearing that healthcare interactions could become increasingly impersonal. Scholars and clinicians discussed the potential for digital health tools to contribute to epistemic injustice, whereby patients' lived experiences might be undervalued in favor of data-driven assessments. Some stakeholders worried that the emphasis on objective data could lead to the dismissal of patients' subjective experiences, particularly in complex areas like mental health where self-reports from patients already receive high levels of scrutiny. Such dismissals, they argued, could result in a loss of patient autonomy and a dehumanization of care, particularly if clinicians and patients allow algorithmic inferences to play an increasingly larger role relative to human judgment in decision-making.\n\nResponsibility Shifts and \"Empowerment\" Pitfalls. Another significant concern raised by clinicians was the shifting of responsibility from healthcare providers to patients. With the increasing use of digital tools to monitor and manage health, patients are often expected to take on a greater role in their own care. While some viewed this as empowering, many clinicians worried that it could overwhelm patients, particularly those who might not have the skills, knowledge or interest to interpret data feedback. This shift could lead to feelings of confusion and stress among patients.\n\nEthics scholars also highlighted that while the rhetoric of \"empowerment\" is often used to promote these tools, it effectively pushes responsibility onto individuals, especially those with greater resources, while leaving vulnerable populations with insufficient mechanisms to address complex health inequalities. They pointed out that this shift not only burdens patients with the expectation of managing their health independently but also leads to blame when improvements do not occur, potentially exacerbating feelings of shame or anxiety. Certain ethics and policy scholars argued that this trend is reinforced by the technology sector's view of patients as consumers, rather than individuals needing care, framing health management as an individual, rather than collective, responsibility.\n\nAdditionally, clinicians noted the risk of patients deferring responsibility to technology, such as smartphones, under the assumption that these tools will manage their health, which can diminish active patient involvement in care. Patients who come to understand that their devices may \"speak\" for them may be less inclined (and eventually less able) to reflect on and articulate their experiences and behavioral patterns on their own.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Access Inequities And Disproportionate Burdens To Vulnerable Populations.",
      "text": "Clinicians and scholars, primarily, voiced further concerns about the potential of CP tools to exacerbate inequities and disproportionately burden vulnerable populations. Scholars emphasized that marginalized groups, including those experiencing poverty, homelessness, and other forms of marginalization, may be excluded from the benefits of these technologies due to lack of access or capacity. For example, individuals without extensive access to or familiarity with technology might struggle to effectively use or trust these technologies, potentially limiting the potential benefits they may confer, and biasing training data sets in ways that perpetuate harmful biases and further exacerbate inequities.\n\nFurther, caregivers and ethicists, in particular, raised significant reservations around CP tools being leveraged or co-opted for surveillance, especially of communities with a history of being monitored, such as psychiatric and vulnerable groups. Pressured consent emerged as another concern, particularly for individuals in lower social positions who might feel compelled to use these tools despite discomfort or uncertainty. Finally, the risk of involuntary monitoring or detention was highlighted, with concerns that misdiagnoses or inaccurate data could lead to wrongful decisions, severely impacting individuals' rights and treatment.\n\nThreats to Privacy and Self-Determination. Stakeholders from all groups voiced alarms about threats to privacy and autonomy posed by digital health tools. They expressed concern about the potential misuse of sensitive health data and the lack of transparency in how this information is collected and used. Scholars emphasized the need for stronger regulatory frameworks to ensure that patients' privacy is protected and that they have control over their personal health data. The feared that without adequate safeguards, the widespread adoption of these technologies could lead to breaches of trust and unauthorized access to sensitive information.\n\nClinicians pointed out that some patient populations are likely to be more disproportionately impacted by these concerns than others and may require especially robust clinical justifications and potentially enhanced protections or alternative approaches for using CP in ways that will benefit their care and ensure their rights to self-determination and discrimination avoidance.  Harmful Biases \"Who has access to this tool?... To the extent that that's going to create a bias in the data sets that we get and the people that have access to this type of care, those are our potential concerns as well.\" (ELPP_09)",
      "page_start": 33,
      "page_end": 34
    },
    {
      "section_name": "Epistemic Injustice And",
      "text": "Pressured Consent \"That's an inherent problem in all method of tracking or surveillance. I always worry about, there's people who inherently are less empowered than others in virtue of their social status … might feel less comfortable saying no to something like this, and it might actually exacerbate their mental health... Do they feel this obligation to really do this quite invasive thing that might make them feel less at ease than they already are?\" (ELPP_12)\n\nInvoluntary Monitoring or Detention \"So, the idea of introducing these tools that might be used at scale to diagnose people and then to make treatment decisions for people who might be subject to an involuntary detention order is scary, I think, where we are not actually sure that the tools are tracking anything. … there are going to be instances where they get it wrong and then decisions are going to be made that have very, very serious implications for people, for their rights, for their treatment …\" (ELPP_16).",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Threats To Privacy And Self-Determination*",
      "text": "Discrimination \"If [CP data could] I don't know how to describe it, work as a bias. He's applying to college right now. Is there a potential that something could exclude him from a scholarship or an activity?\" (CG_19); \"...thinking from a bad movie plot, could it be that someone's like, well, we're going to eradicate all gender dysphoria, so anyone who's gender, this gets spiked on their data, we're pulling all them and we're going to whatever, whatever. That sort of thing. I don't know. I'm sure for anything that there's good things about, there are ways that not good people can make it bad.\" (C_15) \"Identity theft… that there would be a clone of my daughter somewhere…\" (CG_16)",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Psychological Burdens & Preoccupation",
      "text": "\"There is a well-known phenomenon  [   for that individual in the context of work, family, or lifestyle factors. D'Alfonso and colleagues  [9]  have referred to this distinction as \"manual\" versus \"AI-driven\" use of CP, which highlights the degree of human involvement in data interpretations. At the time of writing, most CP tools are not yet robust enough to fully rely on AI-driven inferences and thus require a significant degree of human interpretation to be clinically useful. However, as we argue elsewhere  [52] , this will not always be the case, and -following the ascendant curve of AI in other domains -CP algorithms are likely to advance to the point of offering valid, accurate, patient-specific and trustworthy inferences. The need to establish humanistic approaches well in advance is a consensus goal.",
      "page_start": 37,
      "page_end": 43
    },
    {
      "section_name": "Novel Insights: The Importance Of Context And Subjectivity",
      "text": "Our respondents pointed out two fundamental considerations for effectively and humanely translating CP tools into care that have yet to be elaborated elsewhere: the importance of context and subjectivity in determining the clinical significance of CP outputs. Stakeholders across all groups stressed that observable behaviors -steps, voice tone, facial micro-movements -are clinically actionable only when clinicians understand what those behaviors mean for the individual who produces them and how the surrounding situation shapes that meaning.\n\nThis warning echoes the \"theory of constructed emotion\" advanced by Barrett et al.  [53]  and like-minded scholars  [54] [55] [56] [57]",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "A Prototype For Humanistic Care With Cp",
      "text": "To address these challenges, we introduce the concept of personalized roadmaps  [62]  for integrating CP into clinical care -a structured, co-designed plan that embeds humanistic values into every stage of digital phenotyping. Rather than treating data feedback as a series of discreet disclosures, personalized roadmaps are developed collaboratively by patients, caregivers, and clinician researchers at the point of consent. Together, they specify:\n\n• Which metrics (e.g., activity patterns, speech markers, sleep variability) will be monitored and shared,\n\n• When and how these data will be returned -whether in real time, during clinic visits, through periodic summaries, or some strategic (non-arbitrary) mix of approaches,\n\n• Thresholds for action, delineating what combinations of signals should trigger outreach, referral, or adjustment of treatment,\n\n• Conflict resolution procedures for managing epistemic conflicts when CP outputs diverge from a patient's self-report or a clinician's judgment.\n\nThis iterative framework balances patient agency with clinical and ethical guardrails, inviting patients to contribute lived knowledge (for instance, recognizing that reduced text messaging often precedes mood dips), while researchers share their clinical expertise, and both parties anticipate and come to shared understandings about how their perspectives may be enriched by predictive insights from CP data trends and inferences. This approach is based on a belief, articulated by others  [49] , that technology and humane care are not mutually exclusive, and in fact, can be symbiotic. The personalized roadmap is intended to foster that symbiosis, offer a living decisionsupport tool that aligns computational power with at least three operationalized, person-centered goals of care, including:",
      "page_start": 45,
      "page_end": 46
    },
    {
      "section_name": "Innovating Consent For Cp Approaches",
      "text": "As CP technologies shift from the realms of clinical research into care, these roadmaps will support clinical teams with a fiduciary responsibility to educate patients about the anticipated Other scholars have likewise argued that static, one-time signatures are inadequate for the continuous, highly contextual data streams generated by CP tools. A systematic review of ethical considerations for passive data sensing  [64]  proposes interactive informed consent interfaces that let participants add social annotations, \"talkback\" questions, and multimodal visual aids -features shown to improve comprehension and engagement  [65, 66] . Others have called for consent models that are context-sensitive  [67, 68] , giving patients the ability to recalibrate permissions as circumstances change, and enabling built-in data expiration options, allowing individuals to set automatic sunset dates  [69] . These consent innovations should be integrated into the personalized roadmap architecture to ensure that consent is not a static but an evolving agreement.",
      "page_start": 47,
      "page_end": 48
    },
    {
      "section_name": "Operationalizing Humanistic Use Of Cp",
      "text": "Most would agree that maintaining a sense of humanity in care is critical -and in fact, we already have a reasonably clear vision of what humanistic practice looks like, even if current systems fall short. Humanistic care is compassionate, respectful, and empathetic. It is also collaborative, culturally sensitive, and empowering. The formative research presented here corroborates a substantial body of prior work  [70] [71] [72]  showing how diverse stakeholders conceptualize and idealize humanistic care. In other words, further studies to delineate what constitutes humanistic practice and to demonstrate its benefits for patients, clinicians, and communities are no longer the priority; that foundational work has already been carried out.\n\nInstead, what is now required is rigorous, context-specific evidence for which CP integration strategies most effectively embody these established humanistic care ideals, i.e., which organizational policies, device design relational practices, and value-based attitudes to incorporate and which to eschew. We still lack evidence-based guidelines for integrating CP; and the only way to develop them is to investigate a wide spectrum of implementation contexts and determine which combinations of features produce desired outcomes, for which patients, and under what circumstances. Our analysis highlights several feature domains that require systematic evaluation:\n\n• Data handling: collection methods, governance structures, and privacy safeguards\n\n• Feedback logistics: cadence, routing, and escalation pathways\n\n• Patient support: education, engagement, and shared-decision tools\n\n• Analytics: modelling choices, interpretive aids, and decision-support mechanisms\n\n• Interface design: usability, accessibility, and visualization elements\n\n• Workflow integration: infrastructure requirements and task allocation\n\n• Clinician readiness: training, supervision, and capacity building Each domain contains multiple variables whose effects will differ by setting. Treating these variables as elements in a \"constellation\" and iteratively testing how their configurations shape clinical and humanistic outcomes will allow us to pinpoint the scenarios in which specific approaches add value and those in which they do not. Such empirical investigation may reveal that CP approaches are not for everyone, or every clinical scenario.",
      "page_start": 48,
      "page_end": 50
    },
    {
      "section_name": "Concluding Reflections",
      "text": "Integrating CP technologies into everyday clinical workflows surfaces specific tensions that can undermine even the most deeply held humanistic ideals. Countless forces compete with our ability or desire to deliver humanistic care. In the case of CP, one of the most pervasive is the shared conviction-among clinicians, patients, and caregivers alike-that data speak more objectively than lived experience. As our stakeholders warned, centering illness interpretations on digital signals risks recasting patients' stories through the lens of machine-generated feedback.\n\nAnthropologists describe this phenomenon as an \"idiom\": a culturally patterned mode of expression, whether verbal, behavioral, or somatic, through which distress or wellbeing is communicated in ways that reflect shared meaning, based on local beliefs and values. Classic idioms of distress-\"heavy heart  [73, 74] ,\" \"ataque de nervios  [75] ,\" or notions of hot-cold imbalance  [76] -operate less as discrete biomedical signs than as symbolic languages that link individual suffering to broader cultural meanings, social relationships, and moral concerns. If data become the dominant idiom through which we express or even conceptualize illness, the less able we may become to recognize, convey and intervene in the complex multitude of factors influencing illness.\n\nThese idiomatic shifts pose far graver threats than concerns about false alarms, opaque metrics, or data privacy -issues that, while critically important, are largely tractable and already commanding intense scholarly and technical attention. By contrast, the deeper danger lies in narrowing our collective capacity to perceive human realities by privileging quantifiable signals over the nuanced psychosocial factors that shape how illness is understood and experienced. In this light, dehumanized care reflects not merely a violation of respect or rights, but a siphoningoff of human insight, potentially leading to an atrophy of clinicians' curiosity and compassion and of patients' ability to articulate their own experiences.\n\nIronically, this outcome runs counter to CP's original promise: to deliver objective, reliable windows into complex disease states and, in doing so, draw us nearer to the ground truths of human suffering. Data alone cannot constitute those truths. The critical question -one that our study helps to inform -is how to weave these deep data into care in ways that enhance, rather than diminish, the humanistic foundations of care.",
      "page_start": 50,
      "page_end": 51
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table 1: ). Diagnostic presentations for all adolescents were confirmed",
      "data": [
        {
          "Table 1. Demographics for Interviewed Adolescents and Caregivers": "Adolescents \nCaregivers \nTOTAL \nn = \n% Total \nn = \n% Total \n \n20 \n50% \n20 \n50% \nn=40"
        },
        {
          "Table 1. Demographics for Interviewed Adolescents and Caregivers": "12 \n30% \nMale  \nGender \nFemale  \n8 \n20%"
        },
        {
          "Table 1. Demographics for Interviewed Adolescents and Caregivers": "0 \n0% \n1 \n3% \nAmerican Indian or Alaska Native \nAsian  \n5 \n13% \nAA/Black Native Hawaiian or Other  \nEthnicity \nPacific Islander \n0 \n0% \nWhite \nHispanic or Latino \n17 \n43% \nNot Hispanic or Latino \n4 \n10% \n16 \n40%"
        },
        {
          "Table 1. Demographics for Interviewed Adolescents and Caregivers": "- \n- \nMarried and living with spouse  \n- \n- \nWidowed  \nMarital \nDivorced  \n- \n- \nStatus \nSeparated  \nNever Married  \n- \n- \n- \n-"
        },
        {
          "Table 1. Demographics for Interviewed Adolescents and Caregivers": "- \n- \nHigh school only or less  \nTrade school/Associate's degree  \n- \n- \n \nBachelor's degree  \nMaster's degree  \n- \n- \nDoctoral degree  \n- \n-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Demographics for Interviewed Clinicians, Developers, and ELPP",
      "data": [
        {
          "- \n-": "- \n- \n- \n- \n- \n-",
          "4 \n20%": "18 \n90% \n0 \n0% \n2 \n10%",
          "20%": "90% \n0% \n10%"
        },
        {
          "- \n-": "4 \n20% \n5 \n25% \n3 \n15% \n4 (1 self- reported) \n20% \n1 \n5% \n9 \n45%",
          "4 \n20%": "- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n-",
          "20%": "20% \n25% \n15% \n20% \n5% \n40%"
        },
        {
          "- \n-": "14.9 \n(s.d. 2.2)",
          "4 \n20%": "48.3 \n(s.d. 6.4)",
          "20%": ""
        },
        {
          "- \n-": "",
          "4 \n20%": "",
          "20%": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: Demographics for Interviewed Clinicians, Developers, and ELPP",
      "data": [
        {
          "Table 2. Demographics for Interviewed Clinicians, Developers, and ELPP": "Clinicians \nDevelopers \nScholars \nTOTAL \n \n \nn = \n% Total \nn = \n% Total \nn = \n% Total \n \n \n \n20 \n32% \n21 \n34% \n21 \n34% \nn=62"
        },
        {
          "Table 2. Demographics for Interviewed Clinicians, Developers, and ELPP": "Male  \nGender \nFemale"
        },
        {
          "Table 2. Demographics for Interviewed Clinicians, Developers, and ELPP": "Clinician \nClinician-Researcher \nClinician-Developer \nDeveloper \nProfession \nEthicist \nLawyer \nPhilosopher \nOther"
        },
        {
          "Table 2. Demographics for Interviewed Clinicians, Developers, and ELPP": "Psychiatry \nPsychology \nNeuroscience \nIndustry \nAcademic \nSpecialty \nCross-Sector \nEthics \nLaw \nPhilosophy \nOther"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Accuracy, Validity and Trustworthiness of CP Tools",
      "data": [
        {
          "Table 3. Accuracy, Validity and Trustworthiness of CP Tools": "Themes"
        },
        {
          "Table 3. Accuracy, Validity and Trustworthiness of CP Tools": "Data Quality  \nConstraints & \nConfounds"
        },
        {
          "Table 3. Accuracy, Validity and Trustworthiness of CP Tools": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "\"Of course there's the hardware element of this as \nwell in terms of light sensitivity to darker skin,"
        },
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "One challenge here is that these commercial \nwearables don't tell you when the device is worn \nby the person. But, research grade devices do...  \nBut when [validation] is done outside of a lab... in \nfree living settings, that's actually when you have \nsome of these big challenges. (D_06)"
        },
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "I remember reading that some groups of people, \nwhen they speak, they don't move their body a \nlot, so they don't have a lot of body language. So \nfor the type of machine that reads body \nlanguage, I wouldn't really be effective with \nthem. (P_11)"
        },
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "Only 5% of the models in individualized clinical \nprediction models in psychiatry … get externally \nvalidated, which means that 95% are not \ngeneralizable.”  (C_09)"
        },
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "“If you use them in a context where you have \nvery little data, they will overfit. And, now you \nhave a true problem with generalizability.” \n(D_06)"
        },
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "But I feel like it could easily become something \nwhere it can marginalize a group and not give a \ncertain group of people the right care because it \nmisunderstands something or it's created by a \ncertain race of people. And then, it only applies to \nthat certain race of people... Or gender might play a \nrole in it. If it's created by men, then women may \nnot be able to use it as efficiently. (P_07)"
        },
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "\"I think there's not very good data that we have \nin general on various ethnic populations. So, I \nworry about that. I think we're making a lot of \ngeneralizations, and I know that the likelihood of \nsome populations to be able to give data, just \naccess to the population is limited. And so, I worry \nabout making leaps for whether it's a minority \nperson or an age group that's hard to reach, \nolder, younger, and anyone whose data is not \nthere on...\" (C_08)"
        },
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "\"Who has access to this tool?… That’s going to \ncreate a bias in the data sets that we get and the \npeople that have access to this type of care, those \nare our potential concerns as well.” (ELPP_09)"
        },
        {
          "widespread use, which is people can't get reliable \ndata first.\" (D_18)": "\"It's really easy to get convenience samples that \nare primarily white and primarily high SES \nfamilies, the kinds of families who can take a day \noff work to come in for a research study. And that \nis not the average child on the spectrum. And so \nI do have big concerns that computer vision \nfolks will not sufficiently attend to the kinds of"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "variables that could really impact things.\" \n(C_19)": "\"I see a lot of stuff about how poorly trained in \nterms of demographic groups, the facial affect \ntechnology is. So maybe it works great on me \nbecause I'm a middle-aged white guy and there's \nlots of pictures of me. But if it thinks that Black \nfaces are angry because it was trained on mug \nshots, and there's more people arrested who are \nBlack, not because they commit more crimes, but \nbecause that's where the police are, then the \nsystem is just eating its tail, creating all kinds of \nperpetuating of injustice... The data on which \nthings like the facial computing ethics stuff is \ntrained [is] going to have problems like that.\" \n(D_08)"
        },
        {
          "variables that could really impact things.\" \n(C_19)": "\"The issue was that was the training stage. That \nthere was hand labeling done beforehand and then \nthe AI system will only do what it was trained to \ndo in the first place. So that bias issue is very \nmuch present in emotion based questions as \nwell.\" (ELPP_07)"
        },
        {
          "variables that could really impact things.\" \n(C_19)": "“…you can think of many different use cases… \n[for] algorithms for particular populations or for \npurposes for which they were not initially \ntrained or intended.” (ELPP_17)"
        },
        {
          "variables that could really impact things.\" \n(C_19)": "\"There are these intangible aspects… cultural, \nhistorical aspects of how we think about emotions \nthat don’t necessarily get reflected in these \nmodel building…” (ELPP_17)"
        },
        {
          "variables that could really impact things.\" \n(C_19)": "\"My brain immediately goes to facial recognition \ntechnology that's used in criminal legal systems \nand how bias is so deeply baked into that. And I'm \nthinking about the cultural constraints of \naffective expression and gesture... I see a lot of \nfirst generation kids whose parents are refugees, or \nimmigrants in their adulthood, and I don't even \nknow enough about how acculturation impacts \naffective expression. I think that's a concern for \nme...\" (C_13)"
        },
        {
          "variables that could really impact things.\" \n(C_19)": "\"How can you really control for the fact that a \nsmile might mean something in different \ncontexts? So not only within cultures, but across \ncultures…” (D_03)"
        },
        {
          "variables that could really impact things.\" \n(C_19)": "\"A lot of this tech kind of assumes that the \ndiagnostic tools we have are cross-cultural... \nBut... Those categories might not be completely \ntrue.\" (ELPP_01)"
        },
        {
          "variables that could really impact things.\" \n(C_19)": "\"You might inadvertently...create...novel kinds \nof clusters that do not necessarily map onto our \npreexisting conceptual understandings of \ncategories\" (ELPP_18)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\"What are we training to? Are we training to a \nPHQ-9? Are we training to a Hamilton? Are we \ntraining to a clinical diagnosis or training the \nDSM? It is not really clear to me. I think that's \nstill been the biggest handicap for this field...\" \n(ELPP_20)": "\"How do we account for even the fact that \ndisorders that we are trying to detect discreetly and \nseparately from each other, might actually be... \nLike problems of living, they might be network \nproblems rather than sort of distinct entities \nthat can be detected and discerned.\" (ELPP_01)"
        },
        {
          "\"What are we training to? Are we training to a \nPHQ-9? Are we training to a Hamilton? Are we \ntraining to a clinical diagnosis or training the \nDSM? It is not really clear to me. I think that's \nstill been the biggest handicap for this field...\" \n(ELPP_20)": "“…I think the biggest thing that scares me… we \ndon’t really have any objective markers… we’re \nkind of assuming that there’s an objective entity \nthat we can find, and that data’s going to be the \nanswer…” (ELPP_16)"
        },
        {
          "\"What are we training to? Are we training to a \nPHQ-9? Are we training to a Hamilton? Are we \ntraining to a clinical diagnosis or training the \nDSM? It is not really clear to me. I think that's \nstill been the biggest handicap for this field...\" \n(ELPP_20)": "; \"...part of what I think has been so problematic \nfor mental health is the diagnostic scheme is A, \ndoesn't really have a strong scientific basis using \nany kind of objective measures and B, link in any \nway to treatment response or etiology, either one of \nthose. So in some ways, it's a harder task than it is \nwhen you're looking for digital phenotyping in \nsome other areas of medicine. And I guess the best \nexample is using AI to read mammograms where \nyou have a ground truth from having done a biopsy \nand which tumors are malignant and which are \nnot. We just don't have that here. We don't have \na biopsy, we don't have any solid footing that \nwe're going to go up against, and that to me is \nthe biggest challenge for the field.\" (ELPP_20)"
        },
        {
          "\"What are we training to? Are we training to a \nPHQ-9? Are we training to a Hamilton? Are we \ntraining to a clinical diagnosis or training the \nDSM? It is not really clear to me. I think that's \nstill been the biggest handicap for this field...\" \n(ELPP_20)": "\"I think in terms of the idea of reverse inference, so \nyou have an outward signal to detect an interior \nstate... So from a clinical side of things, you have \nproblems of diagnosis, is what you're sensing \nfrom the outward. Does that have anything to \ndo with what's going on inside a person?... I \nthink [the assumption] that [there's] a \nconnection between internal and external \nexpression that's forever lasting and reliable is \nproblematic, I think.\" (ELPP_07)"
        },
        {
          "\"What are we training to? Are we training to a \nPHQ-9? Are we training to a Hamilton? Are we \ntraining to a clinical diagnosis or training the \nDSM? It is not really clear to me. I think that's \nstill been the biggest handicap for this field...\" \n(ELPP_20)": "“I think [physiological changes] can tell us \nsomething about the internal state of a patient... Is \nthe heart rate spiking? What is heart rate \nvariability? What is the skin conductance? How \nmuch are you sweating? You can tell all these \nthings. [But] I don't think you can use that data \nto then tell us something about what a person is \nfeeling.” (ELPP_01)"
        },
        {
          "\"What are we training to? Are we training to a \nPHQ-9? Are we training to a Hamilton? Are we \ntraining to a clinical diagnosis or training the \nDSM? It is not really clear to me. I think that's \nstill been the biggest handicap for this field...\" \n(ELPP_20)": "If the model doesn't do what it's supposed to do, \nyou can't open the box and say, \"Ah, this is why \nit doesn't work.\" (D_06)"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 4: Patient-Specific Relevance",
      "data": [
        {
          "Table 4. Patient-Specific Relevance": "Themes"
        },
        {
          "Table 4. Patient-Specific Relevance": "Accounting \nfor \nHeterogeneity  \nin Symptom \nExpression & \nSubjectivity"
        },
        {
          "Table 4. Patient-Specific Relevance": ""
        },
        {
          "Table 4. Patient-Specific Relevance": ""
        },
        {
          "Table 4. Patient-Specific Relevance": ""
        },
        {
          "Table 4. Patient-Specific Relevance": ""
        },
        {
          "Table 4. Patient-Specific Relevance": "Accounting \nfor Context & \nMeaning"
        },
        {
          "Table 4. Patient-Specific Relevance": ""
        },
        {
          "Table 4. Patient-Specific Relevance": ""
        },
        {
          "Table 4. Patient-Specific Relevance": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Temporal \nContexts": "",
          "\"Maybe if something in particular happened that made you sad for a \nperiod of time but it's not permanent, then I know that I wouldn't want \nthat to be taken out of proportions and maybe it would become a \n'thing', even if I didn't want it to, it was just something \ntemporary...stuff like actual emotions, I don't know... sometimes those \nchange really fast and I feel different... I don't know if it would be able \nto pick up on that as well as it thinks it can.\" (P_14)": "\"I think for the most part I'd be fine with [passive data collection], but if \nit's constantly picking up on the five minutes that I just didn't get \nexactly what I wanted and it would just be like, 'Oh my gosh, you're not \nfeeling okay, you are possibly depressed,' and it's like, 'No, it was just a \nfive minute thing.' I feel like it could eventually get to the point where \nit's annoying...\" (P_19)"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 5: Utility & Implementation Challenges",
      "data": [
        {
          "Table 5. Utility & Implementation Challenges": "Themes"
        },
        {
          "Table 5. Utility & Implementation Challenges": "Role of CP in Clinical \nCare"
        },
        {
          "Table 5. Utility & Implementation Challenges": ""
        },
        {
          "Table 5. Utility & Implementation Challenges": ""
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Automation Bias": "",
          "\"If an AI just presents one \nrecommendation, then the user could be \nprone to automation bias. Automation bias \nbeing the over-reliance or the complacency \nof just accepting whatever the AI says, \nwithout... And are we as rigorous in looking \nat the data when there's AI assisting?\" (D_15)": "\"If there's some sort of problem with [the \ninferences] and you're just sort of taking it as \nlike, \"Oh, okay, this is what it says\", it could \nmake you make the wrong sorts of \ndecisions. Or again, maybe make you \nthink... 'Maybe they do have a personality \nthing, I wasn't detecting that, but maybe \nthey do.' And maybe you just sort of \nmuddle the picture up a little bit in the \nworst case scenario.\" (C_15)"
        },
        {
          "Automation Bias": "Loss of \nHuman/Relational \nInterpretation",
          "\"If an AI just presents one \nrecommendation, then the user could be \nprone to automation bias. Automation bias \nbeing the over-reliance or the complacency \nof just accepting whatever the AI says, \nwithout... And are we as rigorous in looking \nat the data when there's AI assisting?\" (D_15)": "\"AI is good at picking out patterns… but \nquite bad at understanding what that \npattern means in the context of the \nindividual… it’s still really about the \nclinician and their patient.” (C_04)"
        },
        {
          "Automation Bias": "",
          "\"If an AI just presents one \nrecommendation, then the user could be \nprone to automation bias. Automation bias \nbeing the over-reliance or the complacency \nof just accepting whatever the AI says, \nwithout... And are we as rigorous in looking \nat the data when there's AI assisting?\" (D_15)": "\"Then, obviously, the false negative too, \nright? I mean, as a clinician, I worry about, \nsay, there were an adverse outcome, \nfollowing what looked like normal readings \nfrom the technology. Does that put me in \nthe hook for litigations and so on?\" (C_16)"
        },
        {
          "Automation Bias": "",
          "\"If an AI just presents one \nrecommendation, then the user could be \nprone to automation bias. Automation bias \nbeing the over-reliance or the complacency \nof just accepting whatever the AI says, \nwithout... And are we as rigorous in looking \nat the data when there's AI assisting?\" (D_15)": "\"And I think other risks are also the \nliability. There's a reason why, when we \nautomate depression screeners, we exclude \nthe question that asked about suicidality, \nbecause there's an obligation to then take \nthat seriously... what would happen if the \nsystem isn't able to respond in the moment \nto that?\" (C_13)"
        },
        {
          "Automation Bias": "Consistency of \nMeaning",
          "\"If an AI just presents one \nrecommendation, then the user could be \nprone to automation bias. Automation bias \nbeing the over-reliance or the complacency \nof just accepting whatever the AI says, \nwithout... And are we as rigorous in looking \nat the data when there's AI assisting?\" (D_15)": "\"If you're a company and your whole \nbusiness model is based on sending out alerts \nwhen somebody's having a psychiatric \nepisode, it has to be based on a real \nfinding, on a real phenomenon... If you roll \nit out at scale, it has to be something that's \nrepeatable. [But] making sense of the data \nis a lot more difficult than people realize... \nIt's harder to extract the gold than what \npeople really think.\" (D06)"
        },
        {
          "Automation Bias": "",
          "\"If an AI just presents one \nrecommendation, then the user could be \nprone to automation bias. Automation bias \nbeing the over-reliance or the complacency \nof just accepting whatever the AI says, \nwithout... And are we as rigorous in looking \nat the data when there's AI assisting?\" (D_15)": "\"People don't understand. Let's say I make a \ndecision for a patient on software version \n1.0.3.1, and then somewhere in the \nbackground it got updated. Now, the \nalgorithm says I should have made a \ndifferent decision, or it shifts the \nprobabilities in my differential diagnosis \ndifferently.... Now what do I do in this \ncase? There's a legal issue, there is a care \nissue. It's not like a thermometer, which is"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "going to be thermometer yesterday and today \nthe same way. An algorithm doesn't behave \nthat way;  it has a fluidity...  which we do \nnot know...  how to really deal with it.  \n(D_18)": "\"Even if you trust the heart rate variability \nnumbers that come out of this actigraph that \nyou're using, then what do they correlate to? \nDo they always mean this? Do they always \nmean that? How do we know that it means \nthat in this population or that population? \nWhat is the practical effect really?\" (C_20)"
        },
        {
          "going to be thermometer yesterday and today \nthe same way. An algorithm doesn't behave \nthat way;  it has a fluidity...  which we do \nnot know...  how to really deal with it.  \n(D_18)": "\"In radiology specifically and diagnostic \nimaging fields, the clinicians'  ability is \nthey're experts in interpreting images and \nbeing able to communicate that \nknowledge... We know that clinicians like \nradiologists have training in being able to \nidentify whether there's a pulmonary nodule \nor not, but I think we need to recognize \nthat the introduction of these (CP) models, \nif not designed carefully, can lead to \nconfirmation bias, which is looking at \nthings in the image to confirm what you \nwant to see in the image. [Conversely], if \nyou're someone, you do not consider \nyourself at an expert level, let's say in \ntraining, you will be in a cognitive \ndissonance stage where you are \nquestioning yourself and believing that the \nmodel is somehow truthful, and that can \nlead to more inefficiencies than efficiency if \nthis happens a lot.\" (ELPP_10)"
        },
        {
          "going to be thermometer yesterday and today \nthe same way. An algorithm doesn't behave \nthat way;  it has a fluidity...  which we do \nnot know...  how to really deal with it.  \n(D_18)": "\"If I was recording all my weight and GSR \nand if I was recording EEG during sleep, \npulse oximetry at home and blood pressure, \nand... uploading that into some electronic \ncare record, will my GP feel responsible to \nread that and interpret that? That kind of \nburden of lots and lots of information and \ndata.. The more signals that we're \nproviding, then they need to be \ninterpreted. But is it by a computer, by a \nhuman or both?\" (D_15)"
        },
        {
          "going to be thermometer yesterday and today \nthe same way. An algorithm doesn't behave \nthat way;  it has a fluidity...  which we do \nnot know...  how to really deal with it.  \n(D_18)": "\"I think that what will probably happen on \nthe clinical side is that people will get into \nwearables and these things, patients \nthemselves. And, it may be difficult for \nclinicians to make sense of the data or \nthese metrics because it might be a different \ndevice, it could be something very \ncomplicated. So, I could see on the clinician \nside that clinicians might feel \noverwhelmed.\" (D_06)"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Risk of Non-Actionable Metrics": "",
          "\"A six-minute walk test might seem like a \nmeaningful measure to a patient going into \na clinical evaluation for how well they \nambulate, but we've seen over the course of \nsome decades that it's not actually \nmeaningful.\" (ELPP_19)": "\"...even if you trust the heart rate variability \nnumbers that come out of this actigraph that \nyou're using, then what do they correlate to? \nDo they always mean this? Do they always \nmean that? How do we know that it means \nthat in this population or that population? \nWhat is the practical effect really?\" (C_20)"
        },
        {
          "Risk of Non-Actionable Metrics": "False Alarms / Positives",
          "\"A six-minute walk test might seem like a \nmeaningful measure to a patient going into \na clinical evaluation for how well they \nambulate, but we've seen over the course of \nsome decades that it's not actually \nmeaningful.\" (ELPP_19)": "\"Having the measure is never a problem. It's \nhow it gets used and who it's provided to, \nthat can create the problem. And so, people \ncan be falsely alarmed by these kind of \nthings.\" (D_13)"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table 6: Regulation & Governance of CP Technologies",
      "data": [
        {
          "Table 6. Regulation & Governance of CP Technologies": "Themes"
        },
        {
          "Table 6. Regulation & Governance of CP Technologies": "Acceptability & \nUptake Requires \nRegulation"
        },
        {
          "Table 6. Regulation & Governance of CP Technologies": ""
        },
        {
          "Table 6. Regulation & Governance of CP Technologies": ""
        },
        {
          "Table 6. Regulation & Governance of CP Technologies": "Unclear / \nInsufficient \nRegulatory \nFrameworks"
        },
        {
          "Table 6. Regulation & Governance of CP Technologies": ""
        },
        {
          "Table 6. Regulation & Governance of CP Technologies": ""
        },
        {
          "Table 6. Regulation & Governance of CP Technologies": "Responsibility \nfor Ethical Tech \nDevelopment \nand Compliance"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Deploying Proprietary \nvs. Open-Source \nAlgorithms": "Unclear Liability",
          "\"I would say that my concern, looking around my [the tech \ndevelopment] industry, is the blase attitudes that folks have \nwhen deploying proprietary algorithms for clinical use cases. \nBecause we don't want our care system or clinical research \npractices to be in a place where they are dependent on a \nprivate algorithm to give access to care. We've made that \nmistake in the past... being dependent on organizations like \nthat... and so I wish that there were more checks and balances, or \nan awareness on that.\" (D_13)": "\"Are [patients] responsible for seeing this alert and \ncommunicating? Is it  internet-connected and alerting the clinical \nteam? If [so], is it only for research purposes, and therefore you \n[the patient] actually have to pick up the phone and call your \ndoctor? Or does it create this false safety net that you think \nthat your doctor is monitoring you because you have this \nwearable on you, but actually there's nothing that is alerting \nthe doctor to act on your behalf? So, I think making sure that \nit's an inclusive approach and communicating to a patient \nhow it works and what they're responsible for, and what the \nclinical team is responsible for, would be incredibly \nimportant.\" (ELPP_19)"
        },
        {
          "Deploying Proprietary \nvs. Open-Source \nAlgorithms": "",
          "\"I would say that my concern, looking around my [the tech \ndevelopment] industry, is the blase attitudes that folks have \nwhen deploying proprietary algorithms for clinical use cases. \nBecause we don't want our care system or clinical research \npractices to be in a place where they are dependent on a \nprivate algorithm to give access to care. We've made that \nmistake in the past... being dependent on organizations like \nthat... and so I wish that there were more checks and balances, or \nan awareness on that.\" (D_13)": "\"What's different here is the passive track... [that] tracking \nhappening in someone's personal and private life outside of a \nclinical encounter... I think there are lots of open questions \nthere about what the clinical relationship looks like when \nthat's happening, and where the duty of care extends to.\" \n(ELPP_16)"
        },
        {
          "Deploying Proprietary \nvs. Open-Source \nAlgorithms": "Accounting for                      \nLived Experience",
          "\"I would say that my concern, looking around my [the tech \ndevelopment] industry, is the blase attitudes that folks have \nwhen deploying proprietary algorithms for clinical use cases. \nBecause we don't want our care system or clinical research \npractices to be in a place where they are dependent on a \nprivate algorithm to give access to care. We've made that \nmistake in the past... being dependent on organizations like \nthat... and so I wish that there were more checks and balances, or \nan awareness on that.\" (D_13)": "\"Any project that's trying to develop a technology for clinical \ncare should absolutely have... at least one person, if not \nmultiple people, with lived experience. If [it's] for depression \nand anxiety, well, you need people who've experienced \ndepression and anxiety involved as co-leads on the project. ...A \nlot of tech companies and organizations are developing \ntechnologies because there's money in it, not necessarily because \nthey actually want to solve a problem or care about the end, the \nperson who it's going to impact. [Those] people have \nalternative understandings of their symptomology, which \naren't medical understandings. And they're valuable... Those \nkinds of understandings need to be incorporated into the \ntechnologies.\" (ELPP_16)"
        },
        {
          "Deploying Proprietary \nvs. Open-Source \nAlgorithms": "Need for \nInterdisciplinary \nCollaboration",
          "\"I would say that my concern, looking around my [the tech \ndevelopment] industry, is the blase attitudes that folks have \nwhen deploying proprietary algorithms for clinical use cases. \nBecause we don't want our care system or clinical research \npractices to be in a place where they are dependent on a \nprivate algorithm to give access to care. We've made that \nmistake in the past... being dependent on organizations like \nthat... and so I wish that there were more checks and balances, or \nan awareness on that.\" (D_13)": "\"I'm working on teams with machine learning experts, with \nclinicians, with clinician researchers, with stakeholder groups, et \ncetera... general population consumers. One of the issues... is \nyou get people who are really well-versed in this area. You get \npeople who know the technology and are quite protective of \ntheir understanding of the technology. I see it happen all the \ntime: You get in meetings and the person who talks really \nwell about the technology basically runs the show.\" \n(ELPP_14)"
        },
        {
          "Deploying Proprietary \nvs. Open-Source \nAlgorithms": "Understanding How                    \n& Whether CP Matters",
          "\"I would say that my concern, looking around my [the tech \ndevelopment] industry, is the blase attitudes that folks have \nwhen deploying proprietary algorithms for clinical use cases. \nBecause we don't want our care system or clinical research \npractices to be in a place where they are dependent on a \nprivate algorithm to give access to care. We've made that \nmistake in the past... being dependent on organizations like \nthat... and so I wish that there were more checks and balances, or \nan awareness on that.\" (D_13)": "\"What is the clinical meaningfulness, even if we know that, \n'Yeah, this is accurate.' Those are some of the questions that \ncome to my mind.\" (C_20)"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table 7: Data Privacy & Protection",
      "data": [
        {
          "Table 7. Data Privacy & Protection": "Themes"
        },
        {
          "Table 7. Data Privacy & Protection": "Consent & \nAwareness"
        },
        {
          "Table 7. Data Privacy & Protection": ""
        },
        {
          "Table 7. Data Privacy & Protection": ""
        },
        {
          "Table 7. Data Privacy & Protection": ""
        },
        {
          "Table 7. Data Privacy & Protection": ""
        },
        {
          "Table 7. Data Privacy & Protection": ""
        },
        {
          "Table 7. Data Privacy & Protection": ""
        },
        {
          "Table 7. Data Privacy & Protection": ""
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Coercion": "",
          "\"We almost moved into this very coercive mode where the \nalternatives, if you don't have these digital tools in some of the austere \nsettings we're doing work right now, the alternative is basically \nnothing. So, what you're saying is like, 'I will give you this care or do \nthis research and we will give you all of these things, but you have \nto give up all of your data.' There is no alternative here.\" (C_07)": "\"It's like, 'Give us all this information and we'll provide you really \ngood healthcare, but you can opt out. But just understand that we \nmight not give you the healthcare that you need.' There's not \nactually options there.\" (ELPP_14)"
        },
        {
          "Coercion": "Patient \nCommunication & \nUnderstanding",
          "\"We almost moved into this very coercive mode where the \nalternatives, if you don't have these digital tools in some of the austere \nsettings we're doing work right now, the alternative is basically \nnothing. So, what you're saying is like, 'I will give you this care or do \nthis research and we will give you all of these things, but you have \nto give up all of your data.' There is no alternative here.\" (C_07)": "\"The other thing is about consent... how would we set up consent for \nthis information to be communicated to us? Especially with this \npassive data collection...\" (C_4)"
        },
        {
          "Coercion": "",
          "\"We almost moved into this very coercive mode where the \nalternatives, if you don't have these digital tools in some of the austere \nsettings we're doing work right now, the alternative is basically \nnothing. So, what you're saying is like, 'I will give you this care or do \nthis research and we will give you all of these things, but you have \nto give up all of your data.' There is no alternative here.\" (C_07)": "\"And of course, it might differ for what kind of disease or condition \nyou're experiencing, but I think in lots of cases people are quite \nvulnerable and it's really questionable whether they truly \nunderstand what the technology is doing, how it functions, what \nkind of tools it's using, what the end purposes is. And that could then \ntake on specific forms that are actually not beneficial for the patient.\" \n(ELPP_08)"
        },
        {
          "Coercion": "Perceived vs. Actual     \nData Sensitivity",
          "\"We almost moved into this very coercive mode where the \nalternatives, if you don't have these digital tools in some of the austere \nsettings we're doing work right now, the alternative is basically \nnothing. So, what you're saying is like, 'I will give you this care or do \nthis research and we will give you all of these things, but you have \nto give up all of your data.' There is no alternative here.\" (C_07)": "\"Most people are not concerned about something like accelerometer \ndata or things of that sort. [But] All those studies have \ndemonstrated that you can identify the person... The perceived risk \nof accelerometer data is extremely low. But, that's only perceived \nrisk. The actual risk is actually higher for that category. For GPS, \npeople are much more aware of that risk. But, again, if you have a \nFacebook account, you already incur that risk of privacy loss, \nspecifically related to where you are... The resolution might be plus \nminus 100 meters, 200 meters, as opposed to GPS which is plus minus \nthree or four meters. But, still, there's location there.\" (D_06)"
        },
        {
          "Coercion": "Patient Right                 \n\"Not to Know\"",
          "\"We almost moved into this very coercive mode where the \nalternatives, if you don't have these digital tools in some of the austere \nsettings we're doing work right now, the alternative is basically \nnothing. So, what you're saying is like, 'I will give you this care or do \nthis research and we will give you all of these things, but you have \nto give up all of your data.' There is no alternative here.\" (C_07)": "\"But if they learn something about themselves they didn't want to \nknow, that's hard.\" (ELPP_03); \"Yeah, exactly. That's definitely an \nethical dilemma. We have gene tests that do the same thing now. You \ncan test someone to see if they have a gene for Alzheimer's disease or \nHuntington's disease before they ever developed symptoms of that and \nbefore you would even know. I think it's a similar thing, but there are \nethical implications to knowing that information and whether or \nnot that information gets shared with patients and whether they \nwant to know that information.\" (C_14)"
        },
        {
          "Coercion": "Uncertain                         \nSecondary Uses",
          "\"We almost moved into this very coercive mode where the \nalternatives, if you don't have these digital tools in some of the austere \nsettings we're doing work right now, the alternative is basically \nnothing. So, what you're saying is like, 'I will give you this care or do \nthis research and we will give you all of these things, but you have \nto give up all of your data.' There is no alternative here.\" (C_07)": "\"I have no problem whatsoever using an app where I'm supposed to be \nclicking: 'I feel like I'm an eight out of 10 in terms of anxiety today' or \nletting it track my heart rate, letting it track my gait... The GPS stuff \nseems like it starts to cross some lines too and I would just have a \nlot of concerns about how that data could be used....\" (CG_12)"
        },
        {
          "Coercion": "Discrimination \nPotential",
          "\"We almost moved into this very coercive mode where the \nalternatives, if you don't have these digital tools in some of the austere \nsettings we're doing work right now, the alternative is basically \nnothing. So, what you're saying is like, 'I will give you this care or do \nthis research and we will give you all of these things, but you have \nto give up all of your data.' There is no alternative here.\" (C_07)": "\"Who knows how corporations may use that [CP data] in different \nways to flag or discriminate against your child later on based on \nthis kind of information, so that would be a concern, especially in \nthe workplace.\" (CG_02)"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\"Companies are sharing and using data for purposes other than why the \ndata was initially collected... If you really don't want your data used \nfor other purposes, then there's not a lot of protections that exist \nright now.\" (C_14)": "\"The more I learn about how the regulation works, what law \ngoverns what data can move and who can share what with [who]... \nEvery entity can be fully lawful, law-abiding and then still... the \nframework, regulatory framework leaves holes, leaks for people to \nexploit.\" (D_18)"
        },
        {
          "\"Companies are sharing and using data for purposes other than why the \ndata was initially collected... If you really don't want your data used \nfor other purposes, then there's not a lot of protections that exist \nright now.\" (C_14)": "\"This information [should be] kept exclusively for the treatment \npurposes of the pathology the patient comes in to treat..patient data \nshould be the property of the patient, and only used within the \nrealm that they have agreed upon.\" (C_10)"
        },
        {
          "\"Companies are sharing and using data for purposes other than why the \ndata was initially collected... If you really don't want your data used \nfor other purposes, then there's not a lot of protections that exist \nright now.\" (C_14)": "“We are selling out our healthcare data to groups like Google, Amazon, \netc. We are setting up our healthcare systems in a situation that \nhealthcare professionals can no longer control their own data…” \n(ELPP_14)"
        },
        {
          "\"Companies are sharing and using data for purposes other than why the \ndata was initially collected... If you really don't want your data used \nfor other purposes, then there's not a lot of protections that exist \nright now.\" (C_14)": "\"I think that as healthcare systems begin to act like corporations, the \ncorporatization of healthcare data will mean that monetizing facial \nexpression data, location data, mood data, wearables, is going to be \nan incredible risk for healthcare systems. So I'm very worried that \nhealthcare systems have to be reinforced socially as having a fiduciary \nduty to patients as patients, rather than... to stakeholders.\" (D_19)"
        },
        {
          "\"Companies are sharing and using data for purposes other than why the \ndata was initially collected... If you really don't want your data used \nfor other purposes, then there's not a lot of protections that exist \nright now.\" (C_14)": "\"If my doctor had told me a year ago, I really want you to sleep better, \nso try this out,\" and I did it for a while and I thought it was only my \ndoctor looking at it. [That becomes] very different, if I could imagine \nads being shown to me based on maybe Instagram somehow has \naccess to my Oura data on my phone and can see that I'm acting \nmanic, and they try and sell me specific things because of how my \nheart has behaved in the last week. The crossing of the streams... \nthinking about what the potential benefits and harms are, that'd be the \nthing that would concern me... The corporate environments, where \nthe machinery that turns data into money, the people in those \nrooms, they don't live by [the] same rules [as us].\" (D_08)"
        },
        {
          "\"Companies are sharing and using data for purposes other than why the \ndata was initially collected... If you really don't want your data used \nfor other purposes, then there's not a lot of protections that exist \nright now.\" (C_14)": "\"I think that some of the risks that I'm concerned about are how it \nwould negatively impact the public's stigma towards mental health \ncare. I think that it could dissuade people from engaging in that. It feels \nlike policing, which is not great, and I think for marginalized \ncommunities, extra not great... I haven't really seen how this is \napplicable to kids, but people already perceive me as policing them \nbecause I'm a mandated reporter, so there's that.\" (C_13)"
        },
        {
          "\"Companies are sharing and using data for purposes other than why the \ndata was initially collected... If you really don't want your data used \nfor other purposes, then there's not a lot of protections that exist \nright now.\" (C_14)": "\"And a reasonable response might be, 'Well, this is just another form \nof surveillance. Why do they do this?' And especially, again, in \npeople who might be vulnerable with mental health issues, the last \nthing you want to do is give someone something that they think is \nincreasing surveillance if they're having some issues around \nsurveillance.\" (ELPP_14)"
        }
      ],
      "page": 30
    },
    {
      "caption": "Table 8: Patient Impacts and Harms",
      "data": [
        {
          "Table 8. Patient Impacts and Harms": "Themes"
        },
        {
          "Table 8. Patient Impacts and Harms": "Harms Due to Inaccurate or Premature"
        }
      ],
      "page": 34
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\"What if, based solely on this computer technology, some \nkind of medication is prescribed and maybe the computer \ngot it wrong, or just something wasn't right, and then they were \ntaking the wrong kind of medicine? That, I could see being an \nissue.\" (CG_20)": "“At this stage, I’m not really sure there are many (CP models) \nthat are clinically validated outside of specific controlled \nenvironments. And even if they were to be reliable in \nidentifying things that do seem to correlate with what we know \nas depression or schizophrenia or whatever, there are going to \nbe instances where they get it wrong and then decisions are \ngoing to be made that have very, very serious implications \nfor people.” (ELPP_16)"
        },
        {
          "\"What if, based solely on this computer technology, some \nkind of medication is prescribed and maybe the computer \ngot it wrong, or just something wasn't right, and then they were \ntaking the wrong kind of medicine? That, I could see being an \nissue.\" (CG_20)": "“What we tend to forget or maybe overlook is quite often in \nmental health, the human contact is really, really important. \nSo if we’re doing anything that, in fact, isolates people from \ntheir support networks or makes them think that they \nshould be perhaps using this technology instead of seeking \nassistance with their human networks… that can be really \ndamaging and really dangerous for people with mental \nhealth issues... the care relationship is really important in \nhealthcare, incredibly important in mental health and \npsychiatric health. That relationship, there’s that \ninterdependent relationship. We rely on each other.\" \n(ELPP_14)"
        },
        {
          "\"What if, based solely on this computer technology, some \nkind of medication is prescribed and maybe the computer \ngot it wrong, or just something wasn't right, and then they were \ntaking the wrong kind of medicine? That, I could see being an \nissue.\" (CG_20)": "...I think there's relationships that are formed with your \ndoctor, and I just don't see that computers can ever replace \nthat. I hope they don't... but if we get to a place where they \ndo, I think that mental health in kids and adults will be \nworse off altogether because of the lack of value on \nrelationships, not just with your healthcare provider, but in \ngeneral. (CG_17)"
        },
        {
          "\"What if, based solely on this computer technology, some \nkind of medication is prescribed and maybe the computer \ngot it wrong, or just something wasn't right, and then they were \ntaking the wrong kind of medicine? That, I could see being an \nissue.\" (CG_20)": "I think there's a lot lost in terms of the physician or \ntherapist‐patient relationship... So it used to be we would sit \nface to face and talk. Now... there's a computer there that's \nconstantly being typed on and there's not a lot of eye contact... \nthat does concern me, that we're going to be more and more \ndrawn into a virtual meta world rather than being in the \nreal world with each other. So I would want to see these used \nin conjunction with real human interactions... (C_03)"
        },
        {
          "\"What if, based solely on this computer technology, some \nkind of medication is prescribed and maybe the computer \ngot it wrong, or just something wasn't right, and then they were \ntaking the wrong kind of medicine? That, I could see being an \nissue.\" (CG_20)": "“We hear a lot of rhetoric, which is actually, I think, quite \ndamaging. …  this notion of empowerment [with digital health \ntechnologies]. … Really, what’s actually happening is we’re \npushing responsibility away from the state onto individuals. \nPeople who are advantaged might be able to take charge and \nrespond to that. A lot of people who need care aren’t in the \nposition to do that. So what we end up doing is we push blame. \nRather than making people empowered, we actually make \nthem responsible, and then we blame them for their health \ninequalities and their health issues... And then you get \nindividuals who don’t see improvements. It can actually then \nworsen their self-image where it’s like, ‘Oh, gee, I’ve got all \nthis support and I still can’t sort it out. Everyone else is. I’ve \ngot this tech. Why can’t I get this? I’m empowered. This is \nnow my responsibility. Why can’t I do something about \nthis?’” (ELPP_14)"
        }
      ],
      "page": 35
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "“I mean, I feel apprehensive about the term empowerment \nbecause immediately I ask, well, who's saying someone is \ngoing to be empowered? Is it the person describing their \nown situation? Or is it someone projecting that onto others? \nEmpowerment is a word that seems to have been floating \naround mental health policy and probably other health policies \nsince the 1990s... And was part of a much broader trend \ntowards transitioning from seeing a person as a patient to \nseeing them as a consumer of services... And I think \nempowerment is something that has just continued through \nmental health services and other areas of health, and is \ndefinitely being taken on by the technology sector, because it is \nexactly seen as an individualizing responsibility passed on to \nthe person who's reframed as... a platform user.  I think \nthat narrative can hide a lot of those things...\" (ELPP_15)": "\"[The CP] system might be doing it for them... 'I don't have \nto worry about that because my iPhone is doing it for me' or \nwhatever.\" (C_15)"
        },
        {
          "“I mean, I feel apprehensive about the term empowerment \nbecause immediately I ask, well, who's saying someone is \ngoing to be empowered? Is it the person describing their \nown situation? Or is it someone projecting that onto others? \nEmpowerment is a word that seems to have been floating \naround mental health policy and probably other health policies \nsince the 1990s... And was part of a much broader trend \ntowards transitioning from seeing a person as a patient to \nseeing them as a consumer of services... And I think \nempowerment is something that has just continued through \nmental health services and other areas of health, and is \ndefinitely being taken on by the technology sector, because it is \nexactly seen as an individualizing responsibility passed on to \nthe person who's reframed as... a platform user.  I think \nthat narrative can hide a lot of those things...\" (ELPP_15)": "“There might be a large proportion of the population that \nbenefits. … But we need to look at the boundary cases and the \nmarginalized populations and the vulnerable and the extreme \nharm that we can do to them. … Are we systemizing \ndisadvantage? Is there systemic forms of discrimination? \nAnd with a lot of technology at the moment, the answer \nlooks like it’s going to be, yes, there is. There’s systemic \ndisadvantage.  Who’s missing out? Okay, our aboriginal \npopulations tend to be, our vulnerable groups, people who \nexperience poverty, people who experience homelessness, our \nculturally diverse populations, our populations, LGBTQI+ plus \npopulations. … So these things really stack up and become \nproblematic.” (ELPP_14)"
        },
        {
          "“I mean, I feel apprehensive about the term empowerment \nbecause immediately I ask, well, who's saying someone is \ngoing to be empowered? Is it the person describing their \nown situation? Or is it someone projecting that onto others? \nEmpowerment is a word that seems to have been floating \naround mental health policy and probably other health policies \nsince the 1990s... And was part of a much broader trend \ntowards transitioning from seeing a person as a patient to \nseeing them as a consumer of services... And I think \nempowerment is something that has just continued through \nmental health services and other areas of health, and is \ndefinitely being taken on by the technology sector, because it is \nexactly seen as an individualizing responsibility passed on to \nthe person who's reframed as... a platform user.  I think \nthat narrative can hide a lot of those things...\" (ELPP_15)": "“We often forget that, certainly around psychiatric health, but \ncertainly around a lot of our marginalized vulnerable \ncommunities, they have a real fear of surveillance and \ninterference. … We’ve surveilled them and interfered in their \nlives so incredibly … So if you give them a technology … a \nreasonable response might be, ‘Well, this is just another \nform of surveillance. Why do they do this?’” (ELPP_14);"
        },
        {
          "“I mean, I feel apprehensive about the term empowerment \nbecause immediately I ask, well, who's saying someone is \ngoing to be empowered? Is it the person describing their \nown situation? Or is it someone projecting that onto others? \nEmpowerment is a word that seems to have been floating \naround mental health policy and probably other health policies \nsince the 1990s... And was part of a much broader trend \ntowards transitioning from seeing a person as a patient to \nseeing them as a consumer of services... And I think \nempowerment is something that has just continued through \nmental health services and other areas of health, and is \ndefinitely being taken on by the technology sector, because it is \nexactly seen as an individualizing responsibility passed on to \nthe person who's reframed as... a platform user.  I think \nthat narrative can hide a lot of those things...\" (ELPP_15)": "\"Big Brother is what I think of right away, and the potential \n[data] misuse.” (CG_16)"
        },
        {
          "“I mean, I feel apprehensive about the term empowerment \nbecause immediately I ask, well, who's saying someone is \ngoing to be empowered? Is it the person describing their \nown situation? Or is it someone projecting that onto others? \nEmpowerment is a word that seems to have been floating \naround mental health policy and probably other health policies \nsince the 1990s... And was part of a much broader trend \ntowards transitioning from seeing a person as a patient to \nseeing them as a consumer of services... And I think \nempowerment is something that has just continued through \nmental health services and other areas of health, and is \ndefinitely being taken on by the technology sector, because it is \nexactly seen as an individualizing responsibility passed on to \nthe person who's reframed as... a platform user.  I think \nthat narrative can hide a lot of those things...\" (ELPP_15)": "\"[Based on what] these technologies could determine or \nperceive... would there then be automated [action]? Just the \nimplications of that. I think that that is something that I always \nhave concerns about... We're developing this technology for a \nhypothetical good in mind, but that technology could then \nbe... leveraged into something that's more literal policing, \nlike would the police be dispatched if there's a mental health \ncrisis? And knowing that police involvement during mental \nhealth crises always goes badly for the folks who are \nexperiencing mental health crises, that's definitely a risk.\" \n(C_13)"
        },
        {
          "“I mean, I feel apprehensive about the term empowerment \nbecause immediately I ask, well, who's saying someone is \ngoing to be empowered? Is it the person describing their \nown situation? Or is it someone projecting that onto others? \nEmpowerment is a word that seems to have been floating \naround mental health policy and probably other health policies \nsince the 1990s... And was part of a much broader trend \ntowards transitioning from seeing a person as a patient to \nseeing them as a consumer of services... And I think \nempowerment is something that has just continued through \nmental health services and other areas of health, and is \ndefinitely being taken on by the technology sector, because it is \nexactly seen as an individualizing responsibility passed on to \nthe person who's reframed as... a platform user.  I think \nthat narrative can hide a lot of those things...\" (ELPP_15)": "“In terms of things to worry about, I think I probably worry \nabout the idea of, really, persistent surveillance … that’s \nproblematic for me that they turn into a tool of \nsurveillance...\" (ELPP_09)"
        }
      ],
      "page": 36
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Harmful Biases": "Pressured Consent",
          "\"Who has access to this tool?... To the extent that that’s going \nto create a bias in the data sets that we get and the people \nthat have access to this type of care, those are our potential \nconcerns as well.” (ELPP_09)": "“That’s an inherent problem in all method of tracking or \nsurveillance. I always worry about, there’s people who \ninherently are less empowered than others in virtue of their \nsocial status … might feel less comfortable saying no to \nsomething like this, and it might actually exacerbate their \nmental health... Do they feel this obligation to really do this \nquite invasive thing that might make them feel less at ease \nthan they already are?” (ELPP_12)"
        },
        {
          "Harmful Biases": "Involuntary Monitoring \nor Detention",
          "\"Who has access to this tool?... To the extent that that’s going \nto create a bias in the data sets that we get and the people \nthat have access to this type of care, those are our potential \nconcerns as well.” (ELPP_09)": "“So, the idea of introducing these tools that might be used at \nscale to diagnose people and then to make treatment \ndecisions for people who might be subject to an involuntary \ndetention order is scary, I think, where we are not actually sure \nthat the tools are tracking anything. … there are going to be \ninstances where they get it wrong and then decisions are \ngoing to be made that have very, very serious implications \nfor people, for their rights, for their treatment …” \n(ELPP_16)."
        },
        {
          "Harmful Biases": "Discrimination",
          "\"Who has access to this tool?... To the extent that that’s going \nto create a bias in the data sets that we get and the people \nthat have access to this type of care, those are our potential \nconcerns as well.” (ELPP_09)": "\"If [CP data could] I don't know how to describe it, work as a \nbias. He's applying to college right now. Is there a potential \nthat something could exclude him from a scholarship or an \nactivity?\" (CG_19); “...thinking from a bad movie plot, could it \nbe that someone’s like, well, we’re going to eradicate all \ngender dysphoria, so anyone who’s gender, this gets spiked \non their data, we’re pulling all them and we’re going to \nwhatever, whatever. That sort of thing. I don’t know. I’m sure \nfor anything that there’s good things about, there are ways that \nnot good people can make it bad.” (C_15)"
        },
        {
          "Harmful Biases": "",
          "\"Who has access to this tool?... To the extent that that’s going \nto create a bias in the data sets that we get and the people \nthat have access to this type of care, those are our potential \nconcerns as well.” (ELPP_09)": "\"Identity theft… that there would be a clone of my daughter \nsomewhere…” (CG_16)"
        },
        {
          "Harmful Biases": "Psychological Burdens & \nPreoccupation",
          "\"Who has access to this tool?... To the extent that that’s going \nto create a bias in the data sets that we get and the people \nthat have access to this type of care, those are our potential \nconcerns as well.” (ELPP_09)": "\"There is a well-known phenomenon [in] diabetes called \ndiabetes distress … continuously being exposed to your data \n… can actually lead to … your glucose is better managed, but \nnow you are psychologically stressed out about it so it has \nother consequences. So you may be solving one problem and \ncreating another one.” (D_18);"
        },
        {
          "Harmful Biases": "",
          "\"Who has access to this tool?... To the extent that that’s going \nto create a bias in the data sets that we get and the people \nthat have access to this type of care, those are our potential \nconcerns as well.” (ELPP_09)": "\"Maybe [my daughter] would hyper-focus on the results, like \nhow people say don’t Google your results because your eye \nitches and now you have cancer behind your optic nerve… So \nthat might be something because she does tend to hyper-focus \nand obsess sometimes. So that would be a concern.” (CG_05);"
        },
        {
          "Harmful Biases": "",
          "\"Who has access to this tool?... To the extent that that’s going \nto create a bias in the data sets that we get and the people \nthat have access to this type of care, those are our potential \nconcerns as well.” (ELPP_09)": "I don’t want [my daughter] to have to think about [data \ncollection] on a regular basis and know that, ‘Oh my gosh, \neverything I’m feeling, everything I’m doing is being \nrecorded somewhere.’ I wouldn’t want it to affect her.” \n(CG_20)"
        },
        {
          "Harmful Biases": "",
          "\"Who has access to this tool?... To the extent that that’s going \nto create a bias in the data sets that we get and the people \nthat have access to this type of care, those are our potential \nconcerns as well.” (ELPP_09)": "“Folks would argue, ‘Yeah, but it’s all just more \ninformation. You can do with it what you will.’ But that’s not \npsychologically how people work. If you tell me you have an \nincreased risk of this, it’s not just information. Now, I’m \nsomeone who has this. And how do I go about my day? So I"
        }
      ],
      "page": 37
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "really worry about returning ‘insights,’ A, before they’re \nsuper validated, but also how you return them.” (ELPP_12)": "\"The manner in which you deploy and develop products around \nthose algorithms. My primary care doctor is annoyed at how \noften her patients will reach out to her and say, 'Oh my God, \nmy Apple Watch told me this.' And that is an example of the \nclinician being berated with data that is a distraction to \nthem and their efforts to provide good care to their \npatients.\" (D_13)"
        },
        {
          "really worry about returning ‘insights,’ A, before they’re \nsuper validated, but also how you return them.” (ELPP_12)": "“[Information about my feelings or behaviors being \nautomatically collected in the background] might make me \nkind of self-conscious about it and I don’t know, it might \naffect the way I act because if I’m trying to act a certain \nway to act for these devices, it might be changing me, which \nI don’t know if that’s a good thing.” (P_14);"
        },
        {
          "really worry about returning ‘insights,’ A, before they’re \nsuper validated, but also how you return them.” (ELPP_12)": "“So if we’re recording your voice...is it going to affect how \nyou interact with other people because it’s being recorded a \ncertain way? What are the sequelae for your social and \npersonal life going to be?… There’s the people who can \nalways say, ‘Well, I don’t want to do this anymore,’ and then \nthere’s those who feel beholden to it and will continue to trudge \nalong. So, yeah, sure, that’s something that I do worry about \nwith that kind of application.” (ELPP_12)"
        },
        {
          "really worry about returning ‘insights,’ A, before they’re \nsuper validated, but also how you return them.” (ELPP_12)": "I think maybe even the emotions could be very out of context, \nand if it were in context, that would be way too invasive. So \nthat that's an issue. Monitoring location, I really don't like that. \nAgain, anything that would have to do with food intake and \neven exercise. Just things that if she's aware is being \ngathered, I don't know if it would be honest [accurate], \nbecause I think it would change the way that she would act. \nCG_07"
        },
        {
          "really worry about returning ‘insights,’ A, before they’re \nsuper validated, but also how you return them.” (ELPP_12)": "“We humans lie. I don’t know how that would be taken into \naccount… If you might sense that, ‘Oh, all of this has been \npicked up. Do I have to start acting a certain way? Do I \nneed to start saying certain things? Is my response truthful? \nIs it not?' And so forth.” (CG_16)"
        },
        {
          "really worry about returning ‘insights,’ A, before they’re \nsuper validated, but also how you return them.” (ELPP_12)": "“But then I worry, my bigger concern is then taking that and \nthen relaying back to the patients or the users … It’s one thing \nto see raw data, but once you do the insight thing and you \nput the stamp of scientific approval on it, that means a lot to \npeople. And so they see that and they say, ‘Oh my God, I’m \ndepressed.’ Or, ‘Oh my God, I have a sleep disorder,’ or \nwhatever. … And then they potentially self-incorporate that \ninto their identities.”  \n(ELPP_12)"
        },
        {
          "really worry about returning ‘insights,’ A, before they’re \nsuper validated, but also how you return them.” (ELPP_12)": "Yeah, it depends a little bit again on our understanding of \nvulnerability and whether or not we also have a sense of who \nmight be more susceptible to these types of, I would say, habit-\nforming technology. … But let’s say they’re at the moment \nengineered to habitualizing people to using this technology. \nEven this would be problematic in the consumer domain. \nMy understanding is that at Stanford University, you can \nactually take an engineering course where you can learn how to \nmake apps more... addictive, if you will.” (ELPP_18)"
        }
      ],
      "page": 38
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feedback Impacts on \nAutonomy": "",
          "“Should people generally consent to getting these kinds of \nprompts and recommendations that guide their behavior in \ndaily life with the promise that they feel better after a while, or \ndoes this amount to a paternalistic nudging approach where \nself-autonomy is compromised?” (ELPP_18)": "“Yes, we should respect the lived experience, the client’s \nsubjective perspective, but what were to happen if there was \nthis hypothetical oracle, this oracular system that was \nalmost perfect, and it spat out something that was contra \nthe patient simply because the patient wasn’t fully aware of \nsomething? … going against the patient may create some \nother ripple negative repercussions.” (ELPP_13)"
        },
        {
          "Feedback Impacts on \nAutonomy": "",
          "“Should people generally consent to getting these kinds of \nprompts and recommendations that guide their behavior in \ndaily life with the promise that they feel better after a while, or \ndoes this amount to a paternalistic nudging approach where \nself-autonomy is compromised?” (ELPP_18)": "“At a time in history where I suppose mental health and \ndisability activists and people with that lived experience are \ntrying to push back to some extent on expert knowledge as the \nbasis for decision-making about policies and programming that \naffects people with mental health conditions... in some ways, \ncomputational technologies like digital phenotyping are \nreally promoting a kind of expert monitoring, that has the \npotential to make claims about what is happening with a \nperson’s internal state that may be quite different from \nwhat that person themselves is experiencing... that might \nhave an impact on then how that person self perceives or \nhow others perceive them.” (ELPP_15)"
        },
        {
          "Feedback Impacts on \nAutonomy": "",
          "“Should people generally consent to getting these kinds of \nprompts and recommendations that guide their behavior in \ndaily life with the promise that they feel better after a while, or \ndoes this amount to a paternalistic nudging approach where \nself-autonomy is compromised?” (ELPP_18)": "\"I wouldn't want a provider to be using that data to exclude \nwhat I'm telling them there in that moment... [about \nsomething] you're very concerned [about]... Because I think \nthat a parent's observations should be every bit as \nimportant as the data, if not more depending on what exactly \nit is we're talking about. (CG_12)"
        },
        {
          "Feedback Impacts on \nAutonomy": "",
          "“Should people generally consent to getting these kinds of \nprompts and recommendations that guide their behavior in \ndaily life with the promise that they feel better after a while, or \ndoes this amount to a paternalistic nudging approach where \nself-autonomy is compromised?” (ELPP_18)": "\"One of my concerns would be how it would affect the doctor \nrelying on, like if you were advocating for your child or if you \nwere advocating for yourself, I would worry that they would \nplace more emphasis on the technology, the information, \nthan they would on [your concerns]... And I realize some \npeople aren't good at communicating, but then sometimes we \nhave these mom instincts that are stronger, I feel like, than \nanything really.  (CG_17)"
        },
        {
          "Feedback Impacts on \nAutonomy": "",
          "“Should people generally consent to getting these kinds of \nprompts and recommendations that guide their behavior in \ndaily life with the promise that they feel better after a while, or \ndoes this amount to a paternalistic nudging approach where \nself-autonomy is compromised?” (ELPP_18)": "I understand self-optimization, that's just some people's \npersonalities... If one or two people want to do that, that's fine. \nMy concern is that it becomes more culturally acceptable to \nthe point where we'll be starting to expect this of people the \nway no one's allowed to not have a smartphone today, you \ncan't exist in society and not have a smartphone, it's just \nabsurd, to get to a point where we're just expecting of people \nto have that level of self-surveillance. And I think that there's \nsome losses there with respect to how we exist in the world, \nthis constant quantifying of the self... Using it for a clear, \nclinical indication, versus using it for a project of self-\noptimization, I think that's a distinction that probably also \nmaps onto the difference between the direct-to-consumer \nand the medical setting. (ELPP_12)"
        }
      ],
      "page": 39
    },
    {
      "caption": "Table 9: Philosophical Critiques of CP",
      "data": [
        {
          "Table 9. Philosophical Critiques of CP": "Themes"
        },
        {
          "Table 9. Philosophical Critiques of CP": "CP is Insufficient to Capture \nEmotional States"
        },
        {
          "Table 9. Philosophical Critiques of CP": ""
        },
        {
          "Table 9. Philosophical Critiques of CP": ""
        },
        {
          "Table 9. Philosophical Critiques of CP": "CP Cannot Infer \nEmotion via Behavior"
        },
        {
          "Table 9. Philosophical Critiques of CP": ""
        }
      ],
      "page": 41
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CP Algorithms Embed \nHuman Biases": "CP Inferences not \nMore Valuable than Subjective, \nPatient Insights",
          "\"You can only train an algorithm on data that is pre-coded by a \nhuman being. Someone has to do that manual human labor first. And \ndigital phenotyping, no matter how much they sort of claim, they can't \nget away from that problem.\" (ELPP_01)": "\"Why do we assume that this kind of data that we might collect is \ngoing to be more objective or more reliable than engaging with \npatients about the phenomenological experiences of their illness? I \nthink that's a bigger question. It's like, okay, well yeah, maybe the data is \nhelpful, but it's not necessarily better than what people actually tell \nus about their experiences. They should have equal footing when \nwe're trying to understand illness and health.\" (ELPP_16)"
        },
        {
          "CP Algorithms Embed \nHuman Biases": "CP Reflects (Erroneous) \nTechno-Solutionism",
          "\"You can only train an algorithm on data that is pre-coded by a \nhuman being. Someone has to do that manual human labor first. And \ndigital phenotyping, no matter how much they sort of claim, they can't \nget away from that problem.\" (ELPP_01)": "\"I certainly see there's a lot of potential for these technologies, but my \nworry is that we have a conversation led by a certain perspective… this \nidea of solutionism or technological solutionism… So that causes me a \nlot of concerns, because what we then do is we start to shape these \nsocial, political, cultural healthcare questions based around a tech \nperspective. And what that does is it overlooks a lot of issues that we \nsee.\" (ELPP_14)"
        }
      ],
      "page": 42
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Advancing digital sensing in mental health research",
      "authors": [
        "S Akre",
        "D Seok",
        "C Douglas",
        "A Aguilera",
        "S Carini",
        "J Dunn",
        "M Hotopf",
        "D Mohr",
        "A Bui",
        "N Freimer"
      ],
      "year": "2024",
      "venue": "Npj Digital Medicine",
      "doi": "10.1038/s41746-024-01343-x"
    },
    {
      "citation_id": "2",
      "title": "Digital Phenotyping: Technology for a New Science of Behavior",
      "authors": [
        "T Insel"
      ],
      "year": "2017",
      "venue": "JAMA",
      "doi": "10.1001/jama.2017.11295"
    },
    {
      "citation_id": "3",
      "title": "Wearable Devices: Implications for Precision Medicine and the Future of Health Care",
      "authors": [
        "M Babu",
        "Z Lautman",
        "X Lin",
        "M Sobota",
        "M Snyder"
      ],
      "year": "2024",
      "venue": "Annual Review of Medicine",
      "doi": "10.1146/annurev-med-052422-020437"
    },
    {
      "citation_id": "4",
      "title": "Digital phenotyping from wearables using AI characterizes psychiatric disorders and identifies genetic associations. medRxiv: The Preprint Server for Health Sciences",
      "authors": [
        "J Liu",
        "B Borsari",
        "Y Li",
        "S Liu",
        "Y Gao",
        "X Xin",
        "S Lou",
        "M Jensen",
        "D Garrido-Martin",
        "T Verplaetse",
        "G Ash",
        "J Zhang",
        "M Girgenti",
        "W Roberts",
        "M Gerstein"
      ],
      "year": "2024",
      "venue": "Digital phenotyping from wearables using AI characterizes psychiatric disorders and identifies genetic associations. medRxiv: The Preprint Server for Health Sciences",
      "doi": "10.1101/2024.09.23.24314219"
    },
    {
      "citation_id": "5",
      "title": "Digital Phenotyping in Bipolar Disorder: Which Integration with Clinical Endophenotypes and Biomarkers?",
      "authors": [
        "L Orsolini",
        "M Fiorani",
        "U Volpe"
      ],
      "year": "2020",
      "venue": "International Journal of Molecular Sciences",
      "doi": "10.3390/ijms21207684"
    },
    {
      "citation_id": "6",
      "title": "Digital Phenotyping for Monitoring Mental Disorders: Systematic Review",
      "authors": [
        "P Bufano",
        "M Laurino",
        "S Said",
        "A Tognetti",
        "D Menicucci"
      ],
      "year": "2023",
      "venue": "Journal of Medical Internet Research",
      "doi": "10.2196/46778"
    },
    {
      "citation_id": "7",
      "title": "Promises and challenges of human computational ethology",
      "authors": [
        "D Mobbs",
        "T Wise",
        "N Suthana",
        "N Guzmán",
        "N Kriegeskorte",
        "J Leibo"
      ],
      "year": "2021",
      "venue": "Neuron",
      "doi": "10.1016/j.neuron.2021.05.021"
    },
    {
      "citation_id": "8",
      "title": "Digital Phenotyping for the Busy Psychiatrist: Clinical Implications and Relevance",
      "authors": [
        "J Torous",
        "A Gershon",
        "R Hays",
        "J.-P Onnela",
        "J Baker"
      ],
      "year": "2019",
      "venue": "Digital Phenotyping for the Busy Psychiatrist: Clinical Implications and Relevance",
      "doi": "10.3928/00485713-20190417-01"
    },
    {
      "citation_id": "9",
      "title": "Ethical Dimensions of Digital Phenotyping Within the Context of Mental Healthcare",
      "authors": [
        "S D'alfonso",
        "S Coghlan",
        "S Schmidt",
        "S Mangelsdorf"
      ],
      "year": "2024",
      "venue": "Journal of Technology in Behavioral Science",
      "doi": "10.1007/s41347-024-00423-9"
    },
    {
      "citation_id": "10",
      "title": "Toward clinical digital phenotyping: A timely opportunity to consider purpose, quality, and safety",
      "authors": [
        "K Huckvale",
        "S Venkatesh",
        "H Christensen"
      ],
      "year": "2019",
      "venue": "Npj Digital Medicine",
      "doi": "10.1038/s41746-019-0166-1"
    },
    {
      "citation_id": "11",
      "title": "Data mining for health: Staking out the ethical territory of digital phenotyping",
      "authors": [
        "N Martinez-Martin",
        "T Insel",
        "P Dagum",
        "H Greely",
        "M Cho"
      ],
      "year": "2018",
      "venue": "Npj Digital Medicine",
      "doi": "10.1038/s41746-018-0075-8"
    },
    {
      "citation_id": "12",
      "title": "Personal Sensing: Understanding Mental Health Using Ubiquitous Sensors and Machine Learning",
      "authors": [
        "D Mohr",
        "M Zhang",
        "S Schueller"
      ],
      "year": "2017",
      "venue": "Annual Review of Clinical Psychology",
      "doi": "10.1146/annurev-clinpsy-032816-044949"
    },
    {
      "citation_id": "13",
      "title": "Ethical Development of Digital Phenotyping Tools for Mental Health Applications: Delphi Study",
      "authors": [
        "N Martinez-Martin",
        "H Greely",
        "M Cho"
      ],
      "year": "2021",
      "venue": "JMIR mHealth and uHealth",
      "doi": "10.2196/27343"
    },
    {
      "citation_id": "14",
      "title": "Ethical Issues in Democratizing Digital Phenotypes and Machine Learning in the Next Generation of Digital Health Technologies",
      "authors": [
        "M Mulvenna",
        "R Bond",
        "J Delaney",
        "F Dawoodbhoy",
        "J Boger",
        "C Potts",
        "R Turkington"
      ],
      "year": "2021",
      "venue": "Philosophy & Technology",
      "doi": "10.1007/s13347-021-00445-8"
    },
    {
      "citation_id": "15",
      "title": "An Ethics Checklist for Digital Health Research in Psychiatry: Viewpoint",
      "authors": [
        "F Shen",
        "B Silverman",
        "P Monette",
        "S Kimble",
        "S Rauch",
        "J Baker"
      ],
      "year": "2022",
      "venue": "Journal of Medical Internet Research",
      "doi": "10.2196/31146"
    },
    {
      "citation_id": "16",
      "title": "What are the core elements of patient-centred care? A narrative review and synthesis of the literature from health policy, medicine and nursing",
      "authors": [
        "A Kitson",
        "A Marshall",
        "K Bassett",
        "K Zeitz"
      ],
      "year": "2013",
      "venue": "Journal of Advanced Nursing",
      "doi": "10.1111/j.1365-2648.2012.06064.x"
    },
    {
      "citation_id": "17",
      "title": "The humanization of healthcare: A value framework for qualitative research",
      "authors": [
        "L Todres",
        "Kathleen Galvin",
        "I Holloway"
      ],
      "year": "2009",
      "venue": "International Journal of Qualitative Studies on Health and Well-Being",
      "doi": "10.1080/17482620802646204"
    },
    {
      "citation_id": "18",
      "title": "Nursing: The Philosophy and Science of Caring",
      "authors": [
        "J Watson"
      ],
      "year": "2008",
      "venue": "Nursing: The Philosophy and Science of Caring"
    },
    {
      "citation_id": "19",
      "title": "Shared Decision-Making in Mental Health Care (No. HHS Publication No. SMA-09-4371)",
      "year": "2010",
      "venue": "Center for Mental Health Services, Substance Abuse and Mental Health Services Administration"
    },
    {
      "citation_id": "20",
      "title": "Digital phenotyping: A global tool for psychiatry",
      "authors": [
        "T Insel"
      ],
      "year": "2018",
      "venue": "World Psychiatry",
      "doi": "10.1002/wps.20550"
    },
    {
      "citation_id": "21",
      "title": "Digital Phenotyping: Data-Driven Psychiatry to Redefine Mental Health",
      "authors": [
        "A Oudin",
        "R Maatoug",
        "A Bourla",
        "F Ferreri",
        "O Bonnot",
        "B Millet",
        "F Schoeller",
        "S Mouchabac",
        "V Adrien"
      ],
      "year": "2023",
      "venue": "Journal of Medical Internet Research",
      "doi": "10.2196/44502"
    },
    {
      "citation_id": "22",
      "title": "Ethical Concerns for Remote Computer Perception in Cardiology: New Stages for Digital Health Technologies",
      "authors": [
        "K Kostick-Quenet",
        "J Estep",
        "J Blumenthal-Barby"
      ],
      "year": "2024",
      "venue": "Circulation: Cardiovascular Quality and Outcomes",
      "doi": "10.1161/CIRCOUTCOMES.123.010717"
    },
    {
      "citation_id": "23",
      "title": "Multimodal digital phenotyping of diet, physical activity, and glycemia in Hispanic/Latino adults with or at risk of type 2 diabetes",
      "authors": [
        "A Pai",
        "R Santiago",
        "N Glantz",
        "W Bevier",
        "S Barua",
        "A Sabharwal",
        "D Kerr"
      ],
      "year": "2024",
      "venue": "Npj Digital Medicine",
      "doi": "10.1038/s41746-023-00985-7"
    },
    {
      "citation_id": "24",
      "title": "Disruption of neural periodicity predicts clinical response after deep brain stimulation for obsessive-compulsive disorder",
      "authors": [
        "N Provenza",
        "S Reddy",
        "A Allam",
        "S Rajesh",
        "N Diab",
        "G Reyes",
        "R Caston",
        "K Katlowitz",
        "A Gandhi",
        "R Bechtold",
        "H Dang",
        "R Najera",
        "N Giridharan",
        "K Kabotyanski",
        "F Momin",
        "M Hasen",
        "G Banks",
        "B Mickey",
        "B Kious",
        "S Sheth"
      ],
      "year": "2024",
      "venue": "Nature Medicine",
      "doi": "10.1038/s41591-024-03125-0"
    },
    {
      "citation_id": "25",
      "title": "From smartphone data to clinically relevant predictions: A systematic review of digital phenotyping methods in depression",
      "authors": [
        "I Leaning",
        "N Ikani",
        "H Savage",
        "A Leow",
        "C Beckmann",
        "H Ruhé",
        "A Marquand"
      ],
      "year": "2024",
      "venue": "Neuroscience & Biobehavioral Reviews",
      "doi": "10.1016/j.neubiorev.2024.105541"
    },
    {
      "citation_id": "26",
      "title": "Harvard T.H. Chan School of Public Health",
      "year": "2024",
      "venue": "Harvard T.H. Chan School of Public Health"
    },
    {
      "citation_id": "27",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations. NEJM AI",
      "authors": [
        "J Alderman",
        "J Palmer",
        "E Laws",
        "M Mccradden",
        "J Ordish",
        "M Ghassemi",
        "S Pfohl",
        "N Rostamzadeh",
        "H Cole-Lewis",
        "B Glocker",
        "M Calvert",
        "T Pollard",
        "J Gill",
        "J Gath",
        "A Adebajo",
        "J Beng",
        "C Leung",
        "S Kuku",
        "L.-A Farmer",
        "X Liu"
      ],
      "year": "2024",
      "venue": "Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations. NEJM AI",
      "doi": "10.1056/AIp2401088"
    },
    {
      "citation_id": "29",
      "title": "Trustworthy & Responsible AI Resource Center-AI Risks and Trustworthiness",
      "year": "2025",
      "venue": "Trustworthy & Responsible AI Resource Center-AI Risks and Trustworthiness"
    },
    {
      "citation_id": "30",
      "title": "Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data",
      "authors": [
        "D Adler",
        "C Stamatis",
        "J Meyerhoff",
        "D Mohr",
        "F Wang",
        "G Aranovich",
        "S Sen",
        "T Choudhury"
      ],
      "year": "2024",
      "venue": "Npj Mental Health Research",
      "doi": "10.1038/s44184-024-00057-y"
    },
    {
      "citation_id": "31",
      "title": "Bias in medical AI: Implications for clinical decision-making",
      "authors": [
        "J Cross",
        "M Choma",
        "J Onofrey"
      ],
      "year": "2024",
      "venue": "Bias in medical AI: Implications for clinical decision-making",
      "doi": "10.1371/journal.pdig.0000651"
    },
    {
      "citation_id": "32",
      "title": "Automation Bias and Assistive AI: Risk of Harm From AI-Driven Clinical Decision Support",
      "authors": [
        "R Khera",
        "M Simon",
        "J Ross"
      ],
      "year": "2023",
      "venue": "JAMA",
      "doi": "10.1001/jama.2023.22557"
    },
    {
      "citation_id": "33",
      "title": "Stigma, biomarkers, and algorithmic bias: Recommendations for precision behavioral health with artificial intelligence",
      "authors": [
        "C Walsh",
        "B Chaudhry",
        "P Dua",
        "K Goodman",
        "B Kaplan",
        "R Kavuluru",
        "A Solomonides",
        "V Subbian"
      ],
      "year": "2020",
      "venue": "JAMIA Open",
      "doi": "10.1093/jamiaopen/ooz054"
    },
    {
      "citation_id": "34",
      "title": "Returning Individual Research Results from Digital Phenotyping in Psychiatry",
      "authors": [
        "F Shen",
        "Matthew Baum",
        "Nicole Martinez-Martin",
        "Adam Miner",
        "Melissa Abraham",
        "Catherine Brownstein",
        "Cortez",
        "Nathan",
        "Barbara Evans",
        "Laura Germine",
        "David Glahn",
        "Christine Grady",
        "Ingrid Holm",
        "Elisa Hurley",
        "Sara Kimble",
        "Lázaro-Muñoz",
        "Gabriel",
        "Kimberlyn Leary",
        "Marks",
        "Mason",
        "Patrick Monette",
        "Onnela",
        "Jukka-Pekka",
        "B Silverman"
      ],
      "year": "2024",
      "venue": "The American Journal of Bioethics",
      "doi": "10.1080/15265161.2023.2180109"
    },
    {
      "citation_id": "35",
      "title": "Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300",
      "year": "2008",
      "venue": "Official Journal of the European Union"
    },
    {
      "citation_id": "36",
      "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0) (No. NIST AI 100-1; p. NIST AI 100-1)",
      "authors": [
        "E Tabassi"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence Risk Management Framework (AI RMF 1.0) (No. NIST AI 100-1; p. NIST AI 100-1)",
      "doi": "10.6028/NIST.AI.100-1"
    },
    {
      "citation_id": "37",
      "title": "Deep Data and Precision Health",
      "authors": [
        "A Bahmani"
      ],
      "year": "2022",
      "venue": "Inside Precision Medicine",
      "doi": "10.1089/ipm.09.04.12"
    },
    {
      "citation_id": "38",
      "title": "Ethical considerations for integrating multimodal computer perception and neurotechnology",
      "authors": [
        "M Hurley",
        "A Sonig",
        "J Herrington",
        "E Storch",
        "G Lázaro-Muñoz",
        "J Blumenthal-Barby",
        "K Kostick-Quenet"
      ],
      "year": "2024",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2024.1332451"
    },
    {
      "citation_id": "39",
      "title": "Digital phenotyping and sensitive health data: Implications for data governance",
      "authors": [
        "I Perez-Pozuelo",
        "D Spathis",
        "J Gifford-Moore",
        "J Morley",
        "J Cowls"
      ],
      "year": "2021",
      "venue": "Journal of the American Medical Informatics Association",
      "doi": "10.1093/jamia/ocab012"
    },
    {
      "citation_id": "40",
      "title": "Stakeholder Perspectives on Trust and Transparency around Digital Health Data. Under review at JMIR mHealth and uHealth",
      "authors": [
        "C Deeney",
        "A Sonig",
        "M Hurley",
        "B Tunc",
        "E Storch",
        "J Herrington",
        "J Blumenthal-Barby",
        "K Kostick-Quenet"
      ],
      "venue": "Stakeholder Perspectives on Trust and Transparency around Digital Health Data. Under review at JMIR mHealth and uHealth"
    },
    {
      "citation_id": "41",
      "title": "Fit for purpose? Affective Computing EU data protection law",
      "authors": [
        "A Häuselmann"
      ],
      "year": "2021",
      "venue": "International Data Privacy Law",
      "doi": "10.1093/idpl/ipab008"
    },
    {
      "citation_id": "42",
      "title": "A rapid review of the benefits and challenges of dynamic consent",
      "authors": [
        "W Lay",
        "L Gasparini",
        "W Siero",
        "E Hughes"
      ],
      "year": "2025",
      "venue": "Research Ethics",
      "doi": "10.1177/17470161241278064"
    },
    {
      "citation_id": "43",
      "title": "Missing Each Other: How to Cultivate Meaningful Connections",
      "authors": [
        "E Brodkin",
        "A Pallathra"
      ],
      "year": "2021",
      "venue": "Missing Each Other: How to Cultivate Meaningful Connections"
    },
    {
      "citation_id": "44",
      "title": "Physician Perspectives on the Potential Benefits and Risks of Applying Artificial Intelligence in Psychiatric Medicine: Qualitative Study",
      "authors": [
        "A Stroud",
        "S Curtis",
        "I Weir",
        "J Stout",
        "B Barry",
        "W Bobo"
      ],
      "year": "2025",
      "venue": "JMIR Mental Health"
    },
    {
      "citation_id": "45",
      "title": "Personal responsibility for health: The impact of digitalisation",
      "authors": [
        "A Martani",
        "G Starke"
      ],
      "year": "2019",
      "venue": "Journal of Medical Law and Ethics",
      "doi": "10.7590/221354020X15815920230933"
    },
    {
      "citation_id": "46",
      "title": "Understanding Mental Health Clinicians' Perceptions and Concerns Regarding Using Passive Patient-Generated Health Data for Clinical Decision-Making: Qualitative Semistructured Interview Study",
      "authors": [
        "J Nghiem",
        "D Adler",
        "D Estrin",
        "C Livesey",
        "T Choudhury"
      ],
      "year": "2023",
      "venue": "JMIR Formative Research",
      "doi": "10.2196/47380"
    },
    {
      "citation_id": "47",
      "title": "Clinician perspectives on how digital phenotyping can inform client treatment",
      "authors": [
        "S Schmidt",
        "S Alfonso"
      ],
      "year": "2023",
      "venue": "Acta Psychologica",
      "doi": "10.1016/j.actpsy.2023.103886"
    },
    {
      "citation_id": "48",
      "title": "Wearables, the Marketplace and Efficiency in Healthcare: How Will I Know That You're Thinking of Me?",
      "authors": [
        "M Howard"
      ],
      "year": "2021",
      "venue": "Philosophy & Technology",
      "doi": "10.1007/s13347-021-00473-4"
    },
    {
      "citation_id": "49",
      "title": "The digital transformation of medicine can revitalize the patient-clinician relationship",
      "authors": [
        "H Warraich",
        "R Califf",
        "H Krumholz"
      ],
      "year": "2018",
      "venue": "Npj Digital Medicine",
      "doi": "10.1038/s41746-018-0060-2"
    },
    {
      "citation_id": "50",
      "title": "Transforming qualitative information: Thematic analysis and code development (pp. xvi, 184)",
      "authors": [
        "R Boyatzis"
      ],
      "year": "1998",
      "venue": "Transforming qualitative information: Thematic analysis and code development (pp. xvi, 184)"
    },
    {
      "citation_id": "51",
      "title": "Using thematic analysis in psychology",
      "authors": [
        "V Braun",
        "V Clarke"
      ],
      "year": "2006",
      "venue": "Qualitative Research in Psychology",
      "doi": "10.1191/1478088706qp063oa"
    },
    {
      "citation_id": "52",
      "title": "Rethinking Ethics for an Era of Trusted Computational Tools",
      "authors": [
        "K Kostick-Quenet",
        "M Hurley",
        "J Herrington",
        "E Storch"
      ],
      "venue": "Rethinking Ethics for an Era of Trusted Computational Tools"
    },
    {
      "citation_id": "53",
      "title": "Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest: A Journal of the American Psychological Society",
      "doi": "10.1177/1529100619832930"
    },
    {
      "citation_id": "54",
      "title": "Digital Phenotyping for Mental Health: Reviewing the Challenges of Using Data to Monitor and Predict Mental Health Problems",
      "authors": [
        "R Birk",
        "G Samuel"
      ],
      "year": "2022",
      "venue": "Current Psychiatry Reports",
      "doi": "10.1007/s11920-022-01358-9"
    },
    {
      "citation_id": "55",
      "title": "Can digital data diagnose mental health problems? A sociological exploration of \"digital phenotyping",
      "authors": [
        "R Birk",
        "G Samuel"
      ],
      "year": "2020",
      "venue": "Sociology of Health & Illness",
      "doi": "10.1111/1467-9566.13175"
    },
    {
      "citation_id": "56",
      "title": "A Neuroscientist explains the origins of emotions",
      "authors": [
        "A Chen"
      ],
      "year": "2017",
      "venue": "A Neuroscientist explains the origins of emotions"
    },
    {
      "citation_id": "57",
      "title": "Professional actors demonstrate variability, not stereotypical expressions, when portraying emotional states in photographs",
      "authors": [
        "Le Mau",
        "T Hoemann",
        "K Lyons",
        "S Fugate",
        "J Brown",
        "E Gendron",
        "M Barrett"
      ],
      "year": "2021",
      "venue": "Nature Communications",
      "doi": "10.1038/s41467-021-25352-6"
    },
    {
      "citation_id": "58",
      "title": "The theory of constructed emotion: An active inference account of interoception and categorization",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "Social Cognitive and Affective Neuroscience",
      "doi": "10.1093/scan/nsw154"
    },
    {
      "citation_id": "59",
      "title": "Context in emotion perception",
      "authors": [
        "L Barrett",
        "B Mesquita",
        "M Gendron"
      ],
      "year": "2011",
      "venue": "Current Directions in Psychological Science",
      "doi": "10.1177/0963721411422522"
    },
    {
      "citation_id": "60",
      "title": "Emotions as computations",
      "authors": [
        "A Emanuel",
        "E Eldar"
      ],
      "year": "2023",
      "venue": "Neuroscience & Biobehavioral Reviews",
      "doi": "10.1016/j.neubiorev.2022.104977"
    },
    {
      "citation_id": "61",
      "title": "Digital Measures That Matter to Patients: A Framework to Guide the Selection and Development of Digital Measures of Health",
      "authors": [
        "C Manta",
        "B Patrick-Lake",
        "J Goldsack"
      ],
      "year": "2020",
      "venue": "Digital Biomarkers",
      "doi": "10.1159/000509725"
    },
    {
      "citation_id": "62",
      "title": "Personalized Roadmaps for Returning Results From Digital Phenotyping",
      "authors": [
        "K Kostick-Quenet",
        "J Herrington",
        "E Storch"
      ],
      "year": "2024",
      "venue": "The American Journal of Bioethics: AJOB",
      "doi": "10.1080/15265161.2023.2296454"
    },
    {
      "citation_id": "63",
      "title": "What patients and caregivers want to know when consenting to the use of digital behavioral markers",
      "authors": [
        "A Sonig",
        "C Deeney",
        "M Hurley",
        "E Storch",
        "J Herrington",
        "G Lázaro-Muñoz",
        "C Zampella",
        "B Tunc",
        "J Parish-Morris",
        "J Blumenthal-Barby",
        "K Kostick-Quenet"
      ],
      "year": "2024",
      "venue": "NPP-Digital Psychiatry and Neuroscience",
      "doi": "10.1038/s44277-024-00022-9"
    },
    {
      "citation_id": "64",
      "title": "Passive data collection and use in healthcare: A systematic review of ethical issues",
      "authors": [
        "N Maher",
        "J Senders",
        "A Hulsbergen",
        "N Lamba",
        "M Parker",
        "J.-P Onnela",
        "A Bredenoord",
        "T Smith",
        "M Broekman"
      ],
      "year": "2019",
      "venue": "International Journal of Medical Informatics",
      "doi": "10.1016/j.ijmedinf.2019.06.015"
    },
    {
      "citation_id": "65",
      "title": "If you build it, they will come: Unintended future uses of organised health data collections",
      "authors": [
        "K O'doherty",
        "E Christofides",
        "J Yen",
        "H Bentzen",
        "W Burke",
        "N Hallowell",
        "B Koenig",
        "D Willison"
      ],
      "year": "2016",
      "venue": "BMC Medical Ethics",
      "doi": "10.1186/s12910-016-0137-x"
    },
    {
      "citation_id": "66",
      "title": "Ethical Implications of User Perceptions of Wearable Devices",
      "authors": [
        "L Segura Anaya",
        "A Alsadoon",
        "N Costadopoulos",
        "P Prasad"
      ],
      "year": "2018",
      "venue": "Science and Engineering Ethics",
      "doi": "10.1007/s11948-017-9872-8"
    },
    {
      "citation_id": "67",
      "title": "Consent and engagement, security, and authentic living using wearable and mobile health technology",
      "authors": [
        "K Kreitmair",
        "M Cho",
        "D Magnus"
      ],
      "year": "2017",
      "venue": "Nature Biotechnology",
      "doi": "10.1038/nbt.3887"
    },
    {
      "citation_id": "68",
      "title": "Dynamic Consent for Sensor-Driven Research",
      "authors": [
        "H Lee",
        "U Lee"
      ],
      "year": "2021",
      "venue": "Thirteenth International Conference on Mobile Computing and Ubiquitous Network (ICMU)",
      "doi": "10.23919/ICMU50196.2021.9638790"
    },
    {
      "citation_id": "69",
      "title": "Personalized Consent Flow in Contemporary Data Sharing for Medical Research: A Viewpoint",
      "authors": [
        "E Rake",
        "M Van Gelder",
        "D Grim",
        "B Heeren",
        "L Engelen",
        "T Van De Belt"
      ],
      "year": "2017",
      "venue": "BioMed Research International",
      "doi": "10.1155/2017/7147212"
    },
    {
      "citation_id": "70",
      "title": "Humanizing the ICU Patient: A Qualitative Exploration of Behaviors Experienced by Patients, Caregivers, and ICU Staff",
      "authors": [
        "M Basile",
        "E Rubin",
        "M Wilson",
        "J Polo",
        "S Jacome",
        "S Brown",
        "G Heras La Calle",
        "V Montori",
        "N Hajizadeh"
      ],
      "year": "2021",
      "venue": "Critical Care Explorations",
      "doi": "10.1097/CCE.0000000000000463"
    },
    {
      "citation_id": "71",
      "title": "Humanization of Care: Key Elements Identified by Patients, Caregivers, and Healthcare Providers",
      "authors": [
        "I Busch",
        "F Moretti",
        "G Travaini",
        "A Wu",
        "M Rimondini"
      ],
      "year": "2019",
      "venue": "A Systematic Review. The Patient -Patient-Centered Outcomes Research",
      "doi": "10.1007/s40271-019-00370-1"
    },
    {
      "citation_id": "72",
      "title": "Humanized Care From the Nurse-Patient Perspective in a Hospital Setting: A Systematic Review of Experiences Disclosed in Spanish and Portuguese Scientific Articles",
      "authors": [
        "M Meneses-La-Riva",
        "J Suyo-Vega",
        "V Fernández-Bedoya"
      ],
      "year": "2021",
      "venue": "Frontiers in Public Health",
      "doi": "10.3389/fpubh.2021.737506"
    },
    {
      "citation_id": "73",
      "title": "The 'Heart' of Things: A conceptual metaphoric analysis of heart and related body parts in Thai, Japanese and English",
      "authors": [
        "E Berendt",
        "K Tanita"
      ],
      "year": "2011",
      "venue": "The 'Heart' of Things: A conceptual metaphoric analysis of heart and related body parts in Thai, Japanese and English"
    },
    {
      "citation_id": "74",
      "title": "My Heart Die in Me",
      "authors": [
        "K Fabian",
        "J Fannoh",
        "G Washington",
        "W Geninyan",
        "B Nyachienga",
        "G Cyrus",
        "J Hallowanger",
        "J Beste",
        "D Rao",
        "B Wagenaar"
      ],
      "year": "2018",
      "venue": "Idioms of Distress and the Development of a Screening Tool for Mental Suffering in Southeast Liberia",
      "doi": "10.1007/s11013-018-9581-z"
    },
    {
      "citation_id": "75",
      "title": "Anxiety and Anxiety Disorders in Young People",
      "authors": [
        "S Koydemir",
        "C Essau"
      ],
      "year": "2018",
      "venue": "Understanding Uniqueness and Diversity in Child and Adolescent Mental Health",
      "doi": "10.1016/B978-0-12-815310-9.00005-8"
    },
    {
      "citation_id": "76",
      "title": "Principle of Hot and Cold and Its Clinical Application in Latin American and Caribbean Medicines",
      "authors": [
        "C Vásquez-Londoño",
        "L Cubillos-Cuadrado",
        "A Forero-Ozer",
        "P Escobar-Espinosa",
        "D Cubillos-López",
        "D Castaño-Betancur"
      ],
      "year": "2021",
      "venue": "Advances in Experimental Medicine and Biology",
      "doi": "10.1007/978-3-030-80983-6_5"
    }
  ]
}