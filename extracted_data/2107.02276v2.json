{
  "paper_id": "2107.02276v2",
  "title": "Sarcasm Detection: A Comparative Study",
  "published": "2021-07-05T21:20:29Z",
  "authors": [
    "Hamed Yaghoobian",
    "Hamid R. Arabnia",
    "Khaled Rasheed"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sarcasm detection is the task of identifying irony 1 containing utterances in sentimentbearing text. However, the figurative and creative nature of sarcasm poses a great challenge for affective computing systems performing sentiment analysis. This article compiles and reviews the salient work in the literature of automatic sarcasm detection. Thus far, three main paradigm shifts have occurred in the way researchers have approached this task: 1) semisupervised pattern extraction to identify implicit sentiment, 2) use of hashtag-based supervision, and 3) incorporation of context beyond target text. In this article, we provide a comprehensive review of the datasets, approaches, trends, and issues in sarcasm and irony detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Sarcasm poses a major challenge for sentiment analysis models  (Liu et al., 2010) , mainly because sarcasm enables one speaker or writer to conceal their true intention of contempt and negativity under a guise of overt positive representation. Thus, recognizing sarcasm and verbal irony is critical for understanding people's actual sentiments and beliefs  (Maynard and Greenwood, 2014) . The figurativeness and subtlety inherent in its sentiment display, a positive surface with a contemptuous intent (e.g., \"He has the best taste in music!\"), or a negative surface with an admiring tone (e.g., \"She always makes dry jokes!\"), makes the task of its identification a challenge for both humans and machines.\n\nEvidently, sarcasm and irony are well-studied phenomena in linguistics, psychology, and cognitive science. In this article, we do not survey the several representations and taxonomies of sarcasm in linguistics  (Campbell and Katz, 2012; Camp, 2012; Ivanko and Pexman, 2003; Eisterhold et al., 2006; Wilson, 2006) , and focus on a descriptive account of the computational attempts at automatic sarcasm detection. Empirical studies of this linguistic device refer to methods to predict if a given user-generated text is sarcastic or not. From a computational perspective, this task is formulated as a binary classification problem. Previous research on automated sarcasm detection has primarily focused on lexical, pragmatic resources  (Kreuz and Caucci, 2007)  along with interjections, punctuation, sentimental shifts, etc., found in sentences. Nonetheless, sarcasm is often manifested implicitly with no expressed lexical cues. Its identification is reliant on common sense and connotative knowledge that come naturally to most humans but makes machines struggle when extratextual information is essentially required. Sarcastic utterances are often expressed in such nuanced ways that should be distinguished from a similar phenomenon called humble-bragging, which is a self-representational verbal strategy that appears as a complaint concealed within a bragging  (Wittels, 2012) , as in \"I am a perfectionist at times, it is so hard to deal with\". To the best of our knowledge, there have been few computational studies that distinguish sarcasm from humble-bragging.\n\nThe remainder of this article is organized as follows. We split the literature along two discernible foci, content-and context-based methods discussed in Sections 2 and 3 respectively, and then classify empirical approaches to sarcasm detection within each section into rule-based, statistical, and deep learning-based. Models investigated in this section base their identification of sarcasm on lexical and pragmatic indicators in English 2  language use on social media. There is a myriad of novel and intuitive attempts in the literature that fall in this category. We review and categorize studies in this section based on approaches 2.1 (rule-based, semi-supervised and unsupervised), and features 2.2 (n-gram, sentiment, pragmatics, and patterns) used.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Rule-Based",
      "text": "Rule-based attempts look for evidence and indicators of sarcasm and rely on those in forms of rules.  Veale and Hao (2010)  look for sarcastic similes (e.g., \"as private as a park-bench\") in the specific query pattern of \"as * as a *\" on Google and using a nine-step approach reveal that 18% of unique similes are ironical.\n\nHashtags (or their equivalent, given the social media platform) have been utilized by users to denote sarcasm on Twitter (e.g., #sarcasm, #not) or on Reddit (e.g., /s). Or similarly, if the sentiment of a hashtag does not comply with the rest of the sentence, it is labeled as sarcastic.\n\nBharti et al. (  2015 ) use a combination of two approaches in their study of sarcasm. They propose a parsing algorithm that looks for sentiment-bearing situations and identifies sarcasm in forms of a contradiction of negative (or positive) sentiment and positive (or negative) situation. They also look for the co-occurrence of interjection hyperbolic words like \"wow\", \"yay\", etc. at the start of tweets, and intensifiers like \"absolutely\", \"huge\" e.g., \"Wow, that's a huge discount, I'm not buying anything!! #sarcasm.\" Similarly,  Riloff et al. (2013)  find a positive/negative contrast between a sentiment and a situation helpful, and indicative of sarcasm, e.g., \"I'm so pleased mom woke me up with vacuuming my room this morning. :)\". Likewise,  Van Hee et al. (2018b)  speculate that sentiment incongruity within an utterance signifies sarcasm. To this end, they gather all real-world concepts that carry an implicit sentiment and label them with either a \"positive\" or \"negative\" sentiment label. For example, \"going to the dentist\" is often associated with a negative sentiment. Although their model does not surpass the baseline, they highlight the difficulty and importance of incorporating sarcasm detection into sentiment classifiers. They view their efforts as an extension of the seminal work by  Greene and Resnik (2009)  to use a concept called syntactic packaging to demonstrate the influence of syntactic choices on the perceived implicit sentiment of news headlines.\n\nOne of the earliest work is Tepperman et al.'s that identifies sarcasm in spoken dialogues and relies heavily on cues like laughter, pauses, speaker's gender, and spectral features; their data is restricted to sarcastic utterances that contain the expression 'yeah-right'.  Carvalho et al. (2009)  improve the accuracy of their sarcasm model by using oral or gestural clues in user comments, such as emoticons, onomatopoeic expressions (e.g., achoo, haha, grr, ahem) for laughter, heavy punctuation marks, quotation marks, and positive interjections.  Davidov et al. (2010) ;  Tsur et al. (2010)  utilize syntactic and patternbased linguistic features to construct their feature vectors.  Barbieri et al. (2014)  take a similar approach and extend previous work by relying on the inner structure of utterances such as unexpectedness, the intensity of the terms, or imbalance between registers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Sets",
      "text": "In this section, we go over the salient textual features effectively utilized toward the detection of sarcasm. Most studies use bag-of-words to an extent. Nonetheless, in addition to these, the use of several other sets of features have been reported. Table  1  summarizes the main content-based features most commonly used in the literature. We discuss contextual features (i.e., features reliant on the codification of information presented beyond text) in Section 3.  Reyes et al. (2012)  introduce a set of humordependent or irony-dependent features related to ambiguity, unexpectedness, and emotional scenario. Ambiguity features cover structural, morphosyntactic, semantic ambiguity, while unexpectedness features gauge semantic relatedness. As we discussed,  Riloff et al. (2013) , in addition to a rulebased classifier, use a set of patterns, specifically positive verbs and negative situation phrases, as features.  Liebrecht et al. (2013)  use bigrams and trigrams and similarly,  Reyes et al. (2013)  look into skip-gram and character-level features. In a kindred effort,  Ptáček et al. (2014)  use word-shape and pointedness features.  Barbieri et al. (2014)  include seven sets of features such as maximum/minimum/gap of intensity of adjectives and adverbs, max/min/average number of synonyms and synsets for words in the target text, and so on.  Buschmeier et al. (2014)  incorporate ellipsis, hyperbole, and imbalance in their set of features.  Joshi et al. (2015)  use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruity-based features.  Mishra et al. (2016)  propose a novel approach for investigating the salient features of sarcasm in text. They designed a set of gaze-based features such as average fixation duration, regression count, skip count, etc., based on annotations from their eye-tracking experiments. In addition, they also utilize complex gaze features based on saliency graphs, created by treating words as vertices and saccades (i.e., quick jumping of gaze between two positions of rest) between a pair of words as edges.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Learning-Based Methods",
      "text": "In the following, we delve more into supervised learning, semi-supervised learning, unsupervised learning, structural and hybrid learning. A brief descriptive account of these approaches toward predictive sarcasm identification in text is given below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Supervised Learning",
      "text": "In traditional machine learning approaches, most work on statistical detection of sarcasm has relied on various combinatory forms of Random Forests (RF), Support Vector Machines (SVM), Decision trees (DT), Naïve Bayes (NB) and Neural Networks (NN)  (Davidov et al., 2010; Joshi et al., 2015 Joshi et al., , 2016;; Kreuz and Caucci, 2007; Reyes and Rosso, 2012; Tepperman et al., 2006; Tsur et al., 2010) . For instance, González-Ibánez et al. (  2011 ) use SVM with sequential minimal optimization (SMO) and Logistic Regression (LogR), which are usually used toward sentiment analysis, to identify discriminating features.  Riloff et al. (2013)  utilize a hybrid SVM system that outperformed the SVM classifier. Similarly, the use of balanced winnow algorithms to determine high-ranking features  (Liebrecht et al., 2013) , Naive Bayes and Decision Trees for multiple pairs of labels among irony, humor, politics, and education  (Reyes et al., 2013)  and fuzzy clustering for sarcasm detec-tion  (Mukherjee and Bala, 2017)  are reported.  Bamman and Smith (2015)  present the use of binary Logistic Regression and SVM-HMM toward incorporating the sequential nature of output labels into a conversation. Likewise,  Joshi et al. (2015)  report that sequence labeling algorithms are more useful for conversational data as opposed to classification methods. They use SVM-HMM and SEARN as the sequence labeling algorithms.  Liu et al. (2014)  present a multi-strategy ensemble learning approach (MSELA) including Bagging, Boosting, etc., to handle the imbalance between sarcastic and non-sarcastic samples.\n\nWhile rule-based approaches mostly rely upon lexical information and require no training, machine learning invariably makes use of training data and exploits different types of information sources (or features), such as bags of words, syntactic patterns, sentiment information or semantic relatedness. Earliest attempts in this line use similarity between word embeddings as features for sarcasm detection. Ghosh and Veale (2016) use a combination of convolutional neural networks, LSTM followed by a DNN.  Van Hee et al. (2018a)  propose a model that identifies sarcastic tweets and subsequently differentiates the type (out of four classes) of expressed sarcasm. The systems that were submitted for both subtasks represent a variety of neural-network-based approaches (i.e., CNNs, RNNs, and (bi-)LSTMs) exploiting word and character embeddings as well as handcrafted features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Semi-Supervised Learning",
      "text": "This form of machine learning, which falls between unsupervised learning and supervised learning, uses a minimal quantity of annotated (labeled) data and a large amount of un-annotated (unlabelled) data during training  (Tsur et al., 2010) . The presence of the unlabelled datasets and the open access to the unlabelled datasets is the feature that differentiates the semi-supervised from supervised learning.  Davidov et al. (2010)  employ a semi-supervised learning approach for automatic sarcasm identification using two different forms of text, tweets from Twitter, and product reviews from Amazon. A total number of 66,000 products and book reviews are collected in their study, and both syntactic and pattern-based features are extracted. The sentiment polarity of 1 to 5 is chosen on the training phase for each training data. The authors report a performance of %77 precision.    2016 ) propose an unsupervised framework for domainindependent irony detection. They build on probabilistic topic models originally defined for sentiment analysis. These models are extensions of the well-known Latent Dirichlet Allocation (LDA) model  (Blei et al., 2003) . They propose Topic-Irony model (TIM), which is able to model irony toward different topics in a fully unsupervised setting, enabling each word in a sentence to be generated from the same irony-topic distribution. They enrich their model with a neural language lexicon derived through word embeddings. In a similar attempt, Mukherjee and Bala (2017) utilize both supervised and unsupervised settings. They use Naïve Bayes for supervised and Fuzzy C-means (FCM) clustering for unsupervised learning. Justifiably, FCM does not perform as effectively as NB.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Context-Based Models",
      "text": "Making sense of sarcastic expressions is heavily reliant on the background knowledge and contextual dependencies that are formally diverse. As an example, a sarcastic post from Reddit, \"I'm sure Hillary would've done that, lmao.\" requires prior knowledge about the event, i.e., familiarly with Hillary Clinton's perceived habitual behavior at the time the post was made. Similarly, sarcastic posts like \"But atheism, yeah *that's* a religion!\" require background knowledge, precisely due to the nature of topics like atheism which is often subject to extensive argumentation and is likely to provoke sarcastic construction and interpretation. The proposed models in this section utilize both content and contextual information required for sarcasm detection. In addition, there has been a growing interest in using neural language models for pre-training for various tasks in natural language processing. We go over the utilization of existing language models e.g., BERT, XLNet, etc. toward sarcasm detection in section 3.1.\n\nWallace et al. (  2014 ) claim that human annotators consistently rely on contextual information to make judgments regarding sarcastic intent. Accordingly, recent studies attempt to leverage various forms of contextual information mostly external to the utterance, toward more effective sarcasm identification. Intuitively, in the case of Amazon product reviews, knowing the type of books an individual typically likes might inform our judgment: someone who mostly reads and reviews Dostoevsky is statistically being ironic if they write a laudatory review of Twilight. Evidently, many people genuinely enjoy reading Twilight, and so if the review is written subtly, it will likely be difficult to discern the author's intent without this preferential background. Therefore,  Mukherjee and Bala (2017)  report that including features independent of the text leads to ameliorating the performance of sarcasm models. To this end, studies take three forms of context as feature: 1) author context  (Hazarika et al., 2018; Bamman and Smith, 2015) , 2) conversational context  (Wang et al., 2015) , and 3) topical context  (Ghosh and Veale, 2017) . Another popular line of research utilizes various user embedding techniques that encode users' stylometric and personality features to improve their sarcasm detection models  (Hazarika et al., 2018) . Their model, CAS-CADE, utilizes user embeddings that encode stylometric and personality features of the users. When used along with content-based feature extractors such as Convolutional Neural Networks (CNNs), a significant boost in the classification performance on a large Reddit corpus is achieved. Similarly to how a user controls the degree of sarcasm in a comment, they extrapolate that the ensuing discourse of comments belonging to a particular discussion forum contains contextual information relevant to the sarcasm identification. They embed topical information that selectively incurs bias towards the degree of sarcasm present in the comments of a discussion. For example, comments on political leaders or sports matches are generally more prone to sarcasm than natural disasters. Contextual information extracted from the discourse of a discussion can also provide background knowledge or cues about the discussion topic. To extract the discourse features, they take a similar approach of document modeling performed for stylometric features.  Agrawal et al. (2020)  formulate the task of sarcasm detection as a sequence classification problem by leveraging the natural shifts in various emotions over the course of a piece of text.  Li et al. (2020)  propose a semi-supervised method for contextual sarcasm detection in online discussion forums. They adopt author and topic sarcastic prior preference as context embedding that provides a simple yet representative background knowledge.  Nimala et al. (2020)  also propose an unsupervised probabilistic relational model to identify common sarcasm topics based on the sentiment distribution of words in tweets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sarcasm Detection Using Pre-Trained Language Models",
      "text": "Given the highlighted importance of context to capture figurative language phenomena and the difficulties of data annotation, transfer learning approaches are gaining attention in various domain adaptation problems. In particular, the utilization of pre-trained embeddings such as Global Vectors (GloVe)  (Pennington et al., 2014), and ELMo (Peters et al., 2018)  or leveraging Trans-former seq2seq methods such as BERT (Bidirectional Encoder Representations from Transformers  (Devlin et al., 2019) , RoBERTa  (Liu et al., 2019) , and XLNet  (Yang et al., 2019) , etc. are witnessing a surge.  Potamias et al. (2020)  propose Recurrent CNN RoBERTA (RCNN-RoBERTa), a hybrid neural architecture building on RoBERTA architecture, which is further enhanced with the employment and devise of a recurrent convolutional neural network. They report a performance with an accuracy of %79 on SARC dataset  (Khodak et al., 2018) . Similarly,  Dadu and Pant (2020)  use an ensemble of RoBERTa and ALBERT  (Lan et al., 2019)  on Get it #OffMyChest dataset  (Jaidka et al., 2020)  achieve a performance of %85 accuracy with F 1 score of 0.55.  Javdan et al. (2020)  use BERT along with aspect-based sentiment analysis to extract the relation between context dialogue sequence and response. They obtain an F1 score of 0.73 on the Twitter dataset and 0.73 over the Reddit dataset 3  . We expect to see more studies geared toward leveraging pre-trained contextual embeddings and transformers toward sarcasm detection in the upcoming years.   2013 ) only uses #not to collect and label their tweets. While collecting sarcastic tweets using this method is undemanding, the inclusion of non-sarcastic tweets can be challenging since tweets containing #notsarcastic may not represent a general non-sarcastic text  (Bamman and Smith, 2015) .\n\nAnother approach is to collect the non-sarcastic tweets of users whose sarcastic tweets are also present in the dataset. To ensure collection of true sarcasm, some studies like  Fersini et al. (2015)  manually verified the initial hashtag-based tweets using annotators. Reddit is the other popular platform for researchers to collect sarcasm using hashtag \"/s\" (Reddit's equivalent of \"#sarcasm\" on Twitter).  Khodak et al. (2018)  present SARC, a large-scale self-annotated corpus for sarcasm that contains more than a million examples of sarcastic/nonsarcastic statements made on Reddit.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Long Text",
      "text": "Lukin and Walker (2013) use the Internet Argument Corpus (IAC)  (Walker et al., 2012)  which contains a set of 390,704 posts in 11,800 discussions extracted from the online debate site 4forums.com, annotated for several dialogic and argumentative markers, one of them being sarcasm. Reyes and Rosso (2014) collect a dataset of movie and book reviews, along with news articles marked with sarcasm and sentiment. In an earlier study,  Reyes and Rosso (2012)  garner 11,000 reviews of products with sarcastic expressions. Filatova (2012) present a corpus generation experiment where they collect regular and sarcastic Amazon product reviews. This resulting corpus can be used for identifying sarcasm on two levels: a document and a text utterance, where a text utterance can be as short as a sentence and as long as a whole document.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Transcripts And Dialogues",
      "text": "Sarcasm is often expressed in the context of a conversation, as a response projecting contemptuous intent.  Tepperman et al. (2006)  uses 131 call center transcripts to look for occurrences of \"yeah right\" as a marker of sarcasm. Similarly,  Rakov and Rosenberg (2013)  through crowdsourcing collect sentences from an MTV show called \"Daria.  \" Joshi et al. (2016)  also present a manually annotated transcript of the popular sitcom \"Friends.\"",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "Sarcasm detection research has seen a significant surge in interest in the past few years, which justifiably calls for an investigation. This article focuses on approaches to automatic sarcasm detection in text. We discern three major paradigms in the history of sarcasm detection research: the use of hashtag-driven supervised learning toward building annotated datasets, semi-supervised pattern extraction to identify implicit sentiment, and the utilization of extra-textual information as context (e.g., user's characteristic profiling). While rule-based approaches attempt to capture any indication of sarcasm in the form of rules, statistical methods use features like shifts in sentiment, specific semi-supervised patterns, etc. Deep learning techniques have also been used to incorporate context, e.g., additional stylometric features of authors in conversations and the nature of discussion topics. An underlying theme of these past approaches (either in terms of rules or features) is predicated on sarcasm's contemptuous nature. Novel techniques to incorporate contextual insight have also been explored, mostly centered on the emerging direction toward utilizing language models.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion.": "",
          "punctuation, sentimental shifts, etc., found in sen-": "tences. Nonetheless, sarcasm is often manifested"
        },
        {
          "tion.": "",
          "punctuation, sentimental shifts, etc., found in sen-": "implicitly with no expressed lexical cues.\nIts iden-"
        },
        {
          "tion.": "1\nIntroduction",
          "punctuation, sentimental shifts, etc., found in sen-": ""
        },
        {
          "tion.": "",
          "punctuation, sentimental shifts, etc., found in sen-": "tiﬁcation is reliant on common sense and conno-"
        },
        {
          "tion.": "Sarcasm poses\na major\nchallenge\nfor\nsentiment",
          "punctuation, sentimental shifts, etc., found in sen-": "tative knowledge that come naturally to most hu-"
        },
        {
          "tion.": "analysis models (Liu et al., 2010), mainly because",
          "punctuation, sentimental shifts, etc., found in sen-": "mans but makes machines\nstruggle when extra-"
        },
        {
          "tion.": "sarcasm enables one speaker or writer to conceal",
          "punctuation, sentimental shifts, etc., found in sen-": "textual information is essentially required. Sarcas-"
        },
        {
          "tion.": "their true intention of contempt and negativity un-",
          "punctuation, sentimental shifts, etc., found in sen-": "tic utterances are often expressed in such nuanced"
        },
        {
          "tion.": "der a guise of overt positive representation. Thus,",
          "punctuation, sentimental shifts, etc., found in sen-": "ways\nthat\nshould be distinguished\nfrom a\nsimi-"
        },
        {
          "tion.": "recognizing sarcasm and verbal\nirony is critical",
          "punctuation, sentimental shifts, etc., found in sen-": "lar phenomenon called humble-bragging, which"
        },
        {
          "tion.": "for understanding people’s actual sentiments and",
          "punctuation, sentimental shifts, etc., found in sen-": "is a self-representational verbal\nstrategy that ap-"
        },
        {
          "tion.": "beliefs (Maynard and Greenwood, 2014). The ﬁg-",
          "punctuation, sentimental shifts, etc., found in sen-": "pears\nas\na\ncomplaint\nconcealed within\na\nbrag-"
        },
        {
          "tion.": "urativeness and subtlety inherent\nin its sentiment",
          "punctuation, sentimental shifts, etc., found in sen-": "as\nin “I am a perfection-\nging (Wittels, 2012),"
        },
        {
          "tion.": "display, a positive surface with a contemptuous in-",
          "punctuation, sentimental shifts, etc., found in sen-": "ist at\ntimes,\nit\nis\nso hard to deal with”.\nTo the"
        },
        {
          "tion.": "tent (e.g., “He has the best\ntaste in music!”), or a",
          "punctuation, sentimental shifts, etc., found in sen-": "best of our knowledge,\nthere have been few com-"
        },
        {
          "tion.": "negative surface with an admiring tone (e.g., “She",
          "punctuation, sentimental shifts, etc., found in sen-": "putational\nstudies\nthat distinguish sarcasm from"
        },
        {
          "tion.": "always makes dry jokes!”), makes the task of\nits",
          "punctuation, sentimental shifts, etc., found in sen-": "humble-bragging."
        },
        {
          "tion.": "identiﬁcation a challenge for both humans and ma-",
          "punctuation, sentimental shifts, etc., found in sen-": ""
        },
        {
          "tion.": "",
          "punctuation, sentimental shifts, etc., found in sen-": "The\nremainder of\nthis\narticle\nis organized as"
        },
        {
          "tion.": "chines.",
          "punctuation, sentimental shifts, etc., found in sen-": ""
        },
        {
          "tion.": "",
          "punctuation, sentimental shifts, etc., found in sen-": "follows.\nWe\nsplit\nthe\nliterature\nalong two dis-"
        },
        {
          "tion.": "",
          "punctuation, sentimental shifts, etc., found in sen-": "cernible foci, content- and context-based methods"
        },
        {
          "tion.": "1Irony\nis\nconsidered\nan\numbrella\nterm that\nalso\ncov-",
          "punctuation, sentimental shifts, etc., found in sen-": ""
        },
        {
          "tion.": "ers\nsarcasm; distinguishing between these two rhetoric de-",
          "punctuation, sentimental shifts, etc., found in sen-": "and\ndiscussed in Sections 2 and 3 respectively,"
        },
        {
          "tion.": "vices is a further challenge for ﬁgurative language process-",
          "punctuation, sentimental shifts, etc., found in sen-": ""
        },
        {
          "tion.": "",
          "punctuation, sentimental shifts, etc., found in sen-": "then classify empirical approaches to sarcasm de-"
        },
        {
          "tion.": "In short, sarcasm often bears an ele-\ning (Far´ıas et al., 2016).",
          "punctuation, sentimental shifts, etc., found in sen-": ""
        },
        {
          "tion.": "",
          "punctuation, sentimental shifts, etc., found in sen-": "tection within each section into rule-based, statis-"
        },
        {
          "tion.": "ment of scorn and derision that irony does not (Lee and Katz,",
          "punctuation, sentimental shifts, etc., found in sen-": ""
        },
        {
          "tion.": "1998).",
          "punctuation, sentimental shifts, etc., found in sen-": "tical, and deep learning-based."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "phenomena in linguistics, psychology, and cogni-"
        },
        {
          "Abstract": "Sarcasm detection\nis\nthe\ntask\nof\nidentify-",
          "Evidently,\nsarcasm and irony are well-studied": "tive science.\nIn this article, we do not survey the"
        },
        {
          "Abstract": "ing irony1 containing utterances in sentiment-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "several representations and taxonomies of sarcasm"
        },
        {
          "Abstract": "bearing text. However,\nthe ﬁgurative and cre-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "in\nlinguistics\n(Campbell and Katz,\n2012; Camp,"
        },
        {
          "Abstract": "ative nature of sarcasm poses a great challenge",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "2012; Ivanko and Pexman, 2003; Eisterhold et al.,"
        },
        {
          "Abstract": "for\naffective\ncomputing systems performing",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "2006; Wilson, 2006), and focus on a descriptive ac-"
        },
        {
          "Abstract": "sentiment analysis. This article compiles and",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "count of\nthe computational attempts at automatic"
        },
        {
          "Abstract": "reviews\nthe salient work in the literature of",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "automatic sarcasm detection.\nThus far,\nthree",
          "Evidently,\nsarcasm and irony are well-studied": "sarcasm detection.\nEmpirical\nstudies of\nthis\nlin-"
        },
        {
          "Abstract": "main paradigm shifts have occurred in the way",
          "Evidently,\nsarcasm and irony are well-studied": "guistic\ndevice\nrefer\nto methods\nto\npredict\nif\na"
        },
        {
          "Abstract": "researchers have approached this task: 1) semi-",
          "Evidently,\nsarcasm and irony are well-studied": "given user-generated text is sarcastic or not. From"
        },
        {
          "Abstract": "supervised pattern extraction to identify im-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "a\ncomputational perspective,\nthis\ntask is\nformu-"
        },
        {
          "Abstract": "plicit sentiment, 2) use of hashtag-based super-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "lated as\na binary\nclassiﬁcation problem.\nPrevi-"
        },
        {
          "Abstract": "vision, and 3) incorporation of context beyond",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "ous research on automated sarcasm detection has"
        },
        {
          "Abstract": "target\ntext.\nIn this article, we provide a com-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "primarily focused on lexical, pragmatic resources"
        },
        {
          "Abstract": "prehensive review of the datasets, approaches,",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "(Kreuz and Caucci, 2007) along with interjections,"
        },
        {
          "Abstract": "trends, and issues in sarcasm and irony detec-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "tion.",
          "Evidently,\nsarcasm and irony are well-studied": "punctuation, sentimental shifts, etc., found in sen-"
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "tences. Nonetheless, sarcasm is often manifested"
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "implicitly with no expressed lexical cues.\nIts iden-"
        },
        {
          "Abstract": "1\nIntroduction",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "tiﬁcation is reliant on common sense and conno-"
        },
        {
          "Abstract": "Sarcasm poses\na major\nchallenge\nfor\nsentiment",
          "Evidently,\nsarcasm and irony are well-studied": "tative knowledge that come naturally to most hu-"
        },
        {
          "Abstract": "analysis models (Liu et al., 2010), mainly because",
          "Evidently,\nsarcasm and irony are well-studied": "mans but makes machines\nstruggle when extra-"
        },
        {
          "Abstract": "sarcasm enables one speaker or writer to conceal",
          "Evidently,\nsarcasm and irony are well-studied": "textual information is essentially required. Sarcas-"
        },
        {
          "Abstract": "their true intention of contempt and negativity un-",
          "Evidently,\nsarcasm and irony are well-studied": "tic utterances are often expressed in such nuanced"
        },
        {
          "Abstract": "der a guise of overt positive representation. Thus,",
          "Evidently,\nsarcasm and irony are well-studied": "ways\nthat\nshould be distinguished\nfrom a\nsimi-"
        },
        {
          "Abstract": "recognizing sarcasm and verbal\nirony is critical",
          "Evidently,\nsarcasm and irony are well-studied": "lar phenomenon called humble-bragging, which"
        },
        {
          "Abstract": "for understanding people’s actual sentiments and",
          "Evidently,\nsarcasm and irony are well-studied": "is a self-representational verbal\nstrategy that ap-"
        },
        {
          "Abstract": "beliefs (Maynard and Greenwood, 2014). The ﬁg-",
          "Evidently,\nsarcasm and irony are well-studied": "pears\nas\na\ncomplaint\nconcealed within\na\nbrag-"
        },
        {
          "Abstract": "urativeness and subtlety inherent\nin its sentiment",
          "Evidently,\nsarcasm and irony are well-studied": "as\nin “I am a perfection-\nging (Wittels, 2012),"
        },
        {
          "Abstract": "display, a positive surface with a contemptuous in-",
          "Evidently,\nsarcasm and irony are well-studied": "ist at\ntimes,\nit\nis\nso hard to deal with”.\nTo the"
        },
        {
          "Abstract": "tent (e.g., “He has the best\ntaste in music!”), or a",
          "Evidently,\nsarcasm and irony are well-studied": "best of our knowledge,\nthere have been few com-"
        },
        {
          "Abstract": "negative surface with an admiring tone (e.g., “She",
          "Evidently,\nsarcasm and irony are well-studied": "putational\nstudies\nthat distinguish sarcasm from"
        },
        {
          "Abstract": "always makes dry jokes!”), makes the task of\nits",
          "Evidently,\nsarcasm and irony are well-studied": "humble-bragging."
        },
        {
          "Abstract": "identiﬁcation a challenge for both humans and ma-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "The\nremainder of\nthis\narticle\nis organized as"
        },
        {
          "Abstract": "chines.",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "follows.\nWe\nsplit\nthe\nliterature\nalong two dis-"
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "cernible foci, content- and context-based methods"
        },
        {
          "Abstract": "1Irony\nis\nconsidered\nan\numbrella\nterm that\nalso\ncov-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "ers\nsarcasm; distinguishing between these two rhetoric de-",
          "Evidently,\nsarcasm and irony are well-studied": "and\ndiscussed in Sections 2 and 3 respectively,"
        },
        {
          "Abstract": "vices is a further challenge for ﬁgurative language process-",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "then classify empirical approaches to sarcasm de-"
        },
        {
          "Abstract": "In short, sarcasm often bears an ele-\ning (Far´ıas et al., 2016).",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "",
          "Evidently,\nsarcasm and irony are well-studied": "tection within each section into rule-based, statis-"
        },
        {
          "Abstract": "ment of scorn and derision that irony does not (Lee and Katz,",
          "Evidently,\nsarcasm and irony are well-studied": ""
        },
        {
          "Abstract": "1998).",
          "Evidently,\nsarcasm and irony are well-studied": "tical, and deep learning-based."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: summarizes the main content-based fea-",
      "data": [
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "corporating sarcasm detection into sentiment clas-"
        },
        {
          "2\nContent-based methods": "Models investigated in this section base their iden-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "siﬁers. They view their efforts as an extension of"
        },
        {
          "2\nContent-based methods": "tiﬁcation of sarcasm on lexical and pragmatic in-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "the seminal work by Greene and Resnik (2009) to"
        },
        {
          "2\nContent-based methods": "dicators in English2 language use on social media.",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "use a concept called syntactic packaging to demon-"
        },
        {
          "2\nContent-based methods": "There is a myriad of novel and intuitive attempts in",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "strate the inﬂuence of syntactic choices on the per-"
        },
        {
          "2\nContent-based methods": "the literature that fall\nin this category. We review",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "ceived implicit sentiment of news headlines."
        },
        {
          "2\nContent-based methods": "and categorize studies in this section based on ap-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "One of\nthe earliest work is Tepperman et al.’s"
        },
        {
          "2\nContent-based methods": "proaches 2.1 (rule-based, semi-supervised and un-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "that\nidentiﬁes\nsarcasm in spoken dialogues\nand"
        },
        {
          "2\nContent-based methods": "sentiment,\nsupervised), and features 2.2 (n-gram,",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "relies\nheavily\non\ncues\nlike\nlaughter,\npauses,"
        },
        {
          "2\nContent-based methods": "pragmatics, and patterns) used.",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "speaker’s gender, and spectral\nfeatures;\ntheir data"
        },
        {
          "2\nContent-based methods": "2.1\nRule-based",
          "they highlight\nthe difﬁculty and importance of in-": "is\nrestricted\nto sarcastic\nutterances\nthat\ncontain"
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "the expression ‘yeah-right’. Carvalho et al. (2009)"
        },
        {
          "2\nContent-based methods": "Rule-based attempts\nlook for evidence and indi-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "improve\nthe\naccuracy\nof\ntheir\nsarcasm model"
        },
        {
          "2\nContent-based methods": "cators of\nsarcasm and rely on those in forms of",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "by\nusing\noral\nor\ngestural\nclues\nin\nuser\ncom-"
        },
        {
          "2\nContent-based methods": "rules. Veale and Hao (2010) look for sarcastic sim-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "ments,\nsuch as emoticons, onomatopoeic expres-"
        },
        {
          "2\nContent-based methods": "iles (e.g., “as private as a park-bench”) in the spe-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "achoo,\nhaha,\ngrr,\nsions\n(e.g.,\nahem)\nfor\nlaugh-"
        },
        {
          "2\nContent-based methods": "ciﬁc query pattern of “as * as a *” on Google",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "ter,\nheavy\npunctuation marks,\nquotation marks,"
        },
        {
          "2\nContent-based methods": "and using a nine-step approach reveal that 18% of",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "and positive interjections. Davidov et al.\n(2010);"
        },
        {
          "2\nContent-based methods": "unique similes are ironical.",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "Tsur et al.\n(2010)\nutilize\nsyntactic\nand\npattern-"
        },
        {
          "2\nContent-based methods": "Hashtags\n(or\ntheir equivalent, given the social",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "based linguistic features to construct\ntheir feature"
        },
        {
          "2\nContent-based methods": "media platform) have been utilized by users to de-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "vectors.\nBarbieri et al.\n(2014)\ntake a similar ap-"
        },
        {
          "2\nContent-based methods": "note sarcasm on Twitter (e.g., #sarcasm, #not) or",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "proach and extend previous work by relying on the"
        },
        {
          "2\nContent-based methods": "on Reddit (e.g.,\n/s). Or similarly,\nif the sentiment",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "inner structure of utterances\nsuch as unexpected-"
        },
        {
          "2\nContent-based methods": "of a hashtag does not comply with the rest of the",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "ness,\nthe intensity of\nthe terms, or\nimbalance be-"
        },
        {
          "2\nContent-based methods": "sentence,\nit is labeled as sarcastic.",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "tween registers."
        },
        {
          "2\nContent-based methods": "Bharti et al. (2015) use a combination of two ap-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "proaches in their study of sarcasm. They propose a",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "2.2\nFeature sets"
        },
        {
          "2\nContent-based methods": "parsing algorithm that looks for sentiment-bearing",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "situations and identiﬁes sarcasm in forms of a con-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "In this section, we go over the salient\ntextual fea-"
        },
        {
          "2\nContent-based methods": "tradiction of negative (or positive) sentiment and",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "tures effectively utilized toward the detection of"
        },
        {
          "2\nContent-based methods": "positive\n(or negative)\nsituation.\nThey also look",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "sarcasm. Most studies use bag-of-words to an ex-"
        },
        {
          "2\nContent-based methods": "for\nthe\nco-occurrence\nof\ninterjection\nhyperbolic",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "tent. Nonetheless,\nin addition to these,\nthe use of"
        },
        {
          "2\nContent-based methods": "words\nlike “wow”, “yay”,\netc.\nat\nthe\nstart of",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "several other sets of\nfeatures have been reported."
        },
        {
          "2\nContent-based methods": "tweets, and intensiﬁers like “absolutely”, “huge”",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "the main content-based fea-\nTable 1 summarizes"
        },
        {
          "2\nContent-based methods": "e.g., “Wow,\nthat’s a huge discount,\nI’m not buy-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "tures most commonly used in the literature. We"
        },
        {
          "2\nContent-based methods": "ing anything!!\n#sarcasm.”\nSimilarly, Riloff et al.",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "discuss contextual features (i.e., features reliant on"
        },
        {
          "2\nContent-based methods": "(2013) ﬁnd a positive/negative contrast between a",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "the codiﬁcation of\ninformation presented beyond"
        },
        {
          "2\nContent-based methods": "sentiment and a situation helpful,\nand indicative",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "text) in Section 3."
        },
        {
          "2\nContent-based methods": "of sarcasm, e.g., “I’m so pleased mom woke me up",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "Reyes et al.\n(2012)\nintroduce\na\nset of humor-"
        },
        {
          "2\nContent-based methods": "with vacuuming my room this morning.\n:)”. Like-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "dependent or\nirony-dependent\nfeatures\nrelated to"
        },
        {
          "2\nContent-based methods": "wise, Van Hee et al.\n(2018b) speculate that\nsenti-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "ambiguity,\nunexpectedness,\nand\nemotional\nsce-"
        },
        {
          "2\nContent-based methods": "ment incongruity within an utterance signiﬁes sar-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "nario. Ambiguity features cover\nstructural, mor-"
        },
        {
          "2\nContent-based methods": "casm. To this end,\nthey gather all real-world con-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "phosyntactic, semantic ambiguity, while unexpect-"
        },
        {
          "2\nContent-based methods": "cepts\nthat\ncarry an implicit\nsentiment\nand label",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "edness features gauge semantic relatedness. As we"
        },
        {
          "2\nContent-based methods": "them with either a “positive” or “negative” senti-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "discussed, Riloff et al. (2013), in addition to a rule-"
        },
        {
          "2\nContent-based methods": "ment\nlabel.\nFor example,\n“going to the dentist”",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "based classiﬁer, use a set of patterns, speciﬁcally"
        },
        {
          "2\nContent-based methods": "is often associated with a negative sentiment. Al-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "positive verbs and negative situation phrases,\nas"
        },
        {
          "2\nContent-based methods": "though their model does not surpass the baseline,",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "(2013) use bigrams and\nfeatures. Liebrecht et al."
        },
        {
          "2\nContent-based methods": "2Most\nresearch in sarcasm detection exists\nfor English.",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "trigrams\nand similarly, Reyes et al.\n(2013)\nlook"
        },
        {
          "2\nContent-based methods": "Nonetheless, research in the following languages has been re-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "",
          "they highlight\nthe difﬁculty and importance of in-": "into skip-gram and character-level\nfeatures.\nIn"
        },
        {
          "2\nContent-based methods": "ported also: Utalian, Czech, Dutch, Greek, Indonesian, Chi-",
          "they highlight\nthe difﬁculty and importance of in-": ""
        },
        {
          "2\nContent-based methods": "nese, and Hindi.",
          "they highlight\nthe difﬁculty and importance of in-": "Pt´aˇcek et al.\n(2014) use word-\na kindred\neffort,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "(2014) include seven sets of features such as max-",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "Bamman and Smith\n(2015)\npresent\nthe\nuse\nof"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "imum/minimum/gap of intensity of adjectives and",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "binary Logistic Regression and SVM-HMM to-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "adverbs, max/min/average number of\nsynonyms",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "ward incorporating the sequential nature of output"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "and synsets\nfor words\nin the target\ntext,\nand so",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "labels\ninto a conversation.\nLikewise,\nJoshi et al."
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "on. Buschmeier et al.\n(2014)\nincorporate ellipsis,",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "(2015)\nreport\nthat\nsequence\nlabeling\nalgorithms"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "hyperbole, and imbalance in their set of\nfeatures.",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "are more useful for conversational data as opposed"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "Joshi et al.\n(2015) use features\ncorresponding to",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "to classiﬁcation methods.\nThey use SVM-HMM"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "the linguistic theory of\nincongruity. The features",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "and SEARN as the sequence labeling algorithms."
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "are classiﬁed into two sets:\nimplicit and explicit",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "Liu et al. (2014) present a multi-strategy ensemble"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "incongruity-based features.",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "learning approach (MSELA)\nincluding Bagging,"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "Mishra et al.\n(2016) propose a novel approach",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "Boosting, etc.,\nto handle the imbalance between"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "for investigating the salient features of sarcasm in",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "sarcastic and non-sarcastic samples."
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "text.\nThey designed a set of gaze-based features",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "While rule-based approaches mostly rely upon"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "such as average ﬁxation duration, regression count,",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "lexical\ninformation and require no training, ma-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "skip count, etc., based on annotations\nfrom their",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "chine\nlearning invariably makes use of\ntraining"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "eye-tracking experiments.\nIn addition,\nthey also",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "data\nand exploits different\ntypes of\ninformation"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "utilize\ncomplex gaze features based on saliency",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "sources (or features), such as bags of words, syn-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "graphs, created by treating words as vertices and",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "tactic patterns, sentiment\ninformation or semantic"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "saccades (i.e., quick jumping of gaze between two",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "relatedness. Earliest attempts in this line use sim-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "positions of rest) between a pair of words as edges.",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "ilarity between word embeddings as features\nfor"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "sarcasm detection.\nGhosh and Veale (2016) use"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "2.3\nLearning-based methods",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": ""
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "a combination of convolutional neural networks,"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "In the following, we delve more into supervised",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "LSTM followed by a DNN. Van Hee et al. (2018a)"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "learning,\nsemi-supervised learning, unsupervised",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "propose\na model\nthat\nidentiﬁes\nsarcastic\ntweets"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "learning,\nstructural and hybrid learning. A brief",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "and subsequently\ndifferentiates\nthe\ntype\n(out of"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "descriptive\naccount\nof\nthese\napproaches\ntoward",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "four classes) of expressed sarcasm. The systems"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "predictive sarcasm identiﬁcation in text\nis given",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "that were submitted for both subtasks represent a"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "below.",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "variety of neural-network-based approaches\n(i.e.,"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "CNNs, RNNs, and (bi-)LSTMs) exploiting word"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "2.3.1\nSupervised learning",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": ""
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "and character embeddings as well as handcrafted"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "In\ntraditional\nmachine\nlearning\napproaches,",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": ""
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "features."
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "most\nwork\non\nstatistical\ndetection\nof\nsar-",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": ""
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "2.3.2\nSemi-supervised learning"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "casm\nhas\nrelied\non\nvarious\ncombinatory",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": ""
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "forms\nof Random Forests\n(RF),\nSupport Vec-",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "This\nform of machine\nlearning, which falls be-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "tor Machines\n(SVM),\nDecision\ntrees\n(DT),",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "tween unsupervised learning and supervised learn-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "Na¨ıve Bayes\n(NB)\nand Neural Networks\n(NN)",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "ing, uses a minimal quantity of annotated (labeled)"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "(Davidov et al.,\n2010;\nJoshi et al.,\n2015,\n2016;",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "data\nand\na\nlarge\namount\nof\nun-annotated\n(un-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "Kreuz and Caucci, 2007; Reyes and Rosso, 2012;",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "labelled) data during training (Tsur et al., 2010)."
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "Tepperman et al.,\n2006; Tsur et al.,\n2010).\nFor",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "The presence of\nthe unlabelled datasets\nand the"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "(2011) use SVM\ninstance, Gonz´alez-Ib´anez et al.",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "open access to the unlabelled datasets\nis the fea-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "with sequential minimal optimization (SMO) and",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "ture that differentiates\nthe semi-supervised from"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "Logistic Regression\n(LogR), which\nare\nusually",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "supervised learning. Davidov et al. (2010) employ"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "used toward sentiment\nanalysis,\nto identify\ndis-",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "a semi-supervised learning approach for automatic"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "Riloff et al.\n(2013) utilize\ncriminating features.",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "sarcasm identiﬁcation using two different\nforms"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "a\nhybrid\nSVM system that\noutperformed\nthe",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "of text,\ntweets from Twitter, and product\nreviews"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "SVM classiﬁer.\nSimilarly,\nthe use of balanced",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "from Amazon. A total number of 66,000 products"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "winnow algorithms\nto\ndetermine\nhigh-ranking",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "and book reviews are collected in their study, and"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "features (Liebrecht et al., 2013), Naive Bayes and",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "both syntactic and pattern-based features are ex-"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "Decision Trees for multiple pairs of labels among",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "tracted. The sentiment polarity of 1 to 5 is chosen"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "irony, humor, politics, and education (Reyes et al.,",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "on the training phase for each training data. The"
        },
        {
          "shape\nand\npointedness\nfeatures.\nBarbieri et al.": "2013)\nand\nfuzzy\nclustering\nfor\nsarcasm detec-",
          "tion\n(Mukherjee and Bala,\n2017)\nare\nreported.": "authors report a performance of %77 precision."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Study": "Reyes et al. (2012)",
          "Features Used": "Structural, morphosyntactic and semantic ambiguity features"
        },
        {
          "Study": "Tsur et al. (2010)",
          "Features Used": "Internal syntactic patterns and punctuations"
        },
        {
          "Study": "Gonz´alez-Ib´anez et al. (2011)",
          "Features Used": "User mentions (replies), emoticons, N-grams, dictionary- and,\nsentiment-lexicon-based"
        },
        {
          "Study": "",
          "Features Used": "features"
        },
        {
          "Study": "Liebrecht et al. (2013)",
          "Features Used": "N-grams, emotion marks, intensiﬁers"
        },
        {
          "Study": "Hern´andez-Far´ıas et al. (2015)",
          "Features Used": "Length of tweet, capitalization, punctuation marks, and emoticons"
        },
        {
          "Study": "Far´ıas et al. (2016)",
          "Features Used": "Lexical markers and structural features,"
        },
        {
          "Study": "Mishra et al. (2016)",
          "Features Used": "Cognitive features extracted from eye-movement patterns of human readers"
        },
        {
          "Study": "Joshi et al. (2016)",
          "Features Used": "Features based on word embedding similarity"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "Table 1: Features used for Statistical Classiﬁers"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "2.3.3\nUnsupervised learning\nat the time the post was made. Similarly, sarcastic"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "posts like “But atheism, yeah *that’s* a religion!”"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "Unsupervised learning in automatic sarcasm iden-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "require background knowledge, precisely due to"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "tiﬁcation\nis\nstill\nin\nits\ninfancy,\nand most\nap-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "the nature of\ntopics\nlike atheism which is often"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "proaches are clustering-based, which are mostly"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "subject\nto extensive\nargumentation and is\nlikely"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "applicable to pattern recognition. Nudged by the"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "to provoke\nsarcastic\nconstruction and interpreta-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "limitations and difﬁculties inherent\nin labeling the"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "tion. The proposed models in this section utilize"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "datasets (i.e., time- and labor-intensivity) in super-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "both content and contextual\ninformation required"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "vised learning methods,\nresearchers seek to elim-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "for sarcasm detection.\nIn addition,\nthere has been"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "inate such exertions by focusing on the develop-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "a growing interest\nin using neural\nlanguage mod-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "ment of unsupervised models. Nozza et al. (2016)"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "els for pre-training for various tasks in natural lan-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "propose an unsupervised framework for domain-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "guage processing. We go over\nthe utilization of"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "independent\nirony detection. They build on prob-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "existing language models e.g., BERT, XLNet, etc."
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "abilistic topic models originally deﬁned for\nsen-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "toward sarcasm detection in section 3.1."
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "timent analysis.\nThese models are extensions of"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "the well-known Latent Dirichlet Allocation (LDA)"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "Wallace et al.\n(2014) claim that human annota-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "model\n(Blei et al., 2003).\nThey propose Topic-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "tors consistently rely on contextual\ninformation to"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "Irony model (TIM), which is able to model\nirony"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "make judgments\nregarding sarcastic\nintent.\nAc-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "toward different\ntopics in a fully unsupervised set-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "cordingly,\nrecent\nstudies attempt\nto leverage var-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "ting, enabling each word in a sentence to be gener-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "ious\nforms of contextual\ninformation mostly ex-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "ated from the same irony-topic distribution. They"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "ternal\nto the utterance,\ntoward more effective sar-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "enrich their model with a neural\nlanguage lexicon"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "casm identiﬁcation.\nIntuitively,\nin\nthe\ncase\nof"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "derived through word embeddings.\nIn a similar"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "Amazon\nproduct\nreviews,\nknowing\nthe\ntype\nof"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "attempt, Mukherjee and Bala (2017) utilize both"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "books an individual\ntypically likes might\ninform"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "supervised and unsupervised settings.\nThey use"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "our judgment:\nsomeone who mostly reads and re-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "Na¨ıve Bayes\nfor\nsupervised and Fuzzy C-means"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "views Dostoevsky is\nstatistically being ironic\nif"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "(FCM) clustering for unsupervised learning.\nJus-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "they write a laudatory review of Twilight.\nEvi-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "tiﬁably, FCM does not perform as effectively as"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "dently, many people genuinely enjoy reading Twi-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "NB."
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "light, and so if the review is written subtly,\nit will"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "likely be difﬁcult\nto discern the\nauthor’s\nintent"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "3\nContext-based models"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "without\nthis preferential background.\nTherefore,"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "Mukherjee and Bala (2017)\nreport\nthat\nincluding\nMaking sense of sarcastic expressions\nis heavily"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "features\nindependent of\nthe text\nleads\nto amelio-\nreliant on the background knowledge and contex-"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "rating the performance of\nsarcasm models.\nTo\ntual dependencies\nthat are formally diverse.\nAs"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "this end,\nstudies\ntake\nthree\nforms of context\nas\nan example,\na sarcastic post\nfrom Reddit,\n“I’m"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "feature:\n1) author context\n(Hazarika et al., 2018;\nsure Hillary would’ve done that,\nlmao.”\nrequires"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "Bamman and Smith, 2015), 2) conversational con-\nprior knowledge about\nthe event,\ni.e.,\nfamiliarly"
        },
        {
          "Joshi et al. (2016)\nFeatures based on word embedding similarity": "text\n(Wang et al., 2015),\nand 3)\ntopical\ncontext\nwith Hillary Clinton’s perceived habitual behavior"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: State-of-the-artNN classifiers and results on",
      "data": [
        {
          "cussion can also provide background knowledge": "or cues about\nthe discussion topic. To extract\nthe",
          "toward leveraging pre-trained contextual": "dings and transformers toward sarcasm detection",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "discourse features,\nthey take a similar approach of",
          "toward leveraging pre-trained contextual": "in the upcoming years.",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "document modeling performed for stylometric fea-",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "",
          "toward leveraging pre-trained contextual": "Method\nAcc",
          "embed-": "F1"
        },
        {
          "cussion can also provide background knowledge": "tures.",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "",
          "toward leveraging pre-trained contextual": "ELMo (Peters et al., 2018)\n0.70",
          "embed-": "0.70"
        },
        {
          "cussion can also provide background knowledge": "Agrawal et al. (2020) formulate the task of sar-",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "casm detection as a sequence classiﬁcation prob-",
          "toward leveraging pre-trained contextual": "NBSVM (Wang and Manning, 2012)\n0.65",
          "embed-": "0.65"
        },
        {
          "cussion can also provide background knowledge": "lem by leveraging the natural shifts in various emo-",
          "toward leveraging pre-trained contextual": "XLnet (Yang et al., 2019)\n0.76",
          "embed-": "0.76"
        },
        {
          "cussion can also provide background knowledge": "Li et al.\ntions over\nthe course of a piece of\ntext.",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "",
          "toward leveraging pre-trained contextual": "BERT-cased\n0.76",
          "embed-": "0.76"
        },
        {
          "cussion can also provide background knowledge": "(2020) propose a semi-supervised method for con-",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "",
          "toward leveraging pre-trained contextual": "RoBERTa (Liu et al., 2019)\n0.77",
          "embed-": "0.77"
        },
        {
          "cussion can also provide background knowledge": "textual sarcasm detection in online discussion fo-",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "",
          "toward leveraging pre-trained contextual": "CASCADE (Hazarika et al., 2018)\n0.74",
          "embed-": "0.75"
        },
        {
          "cussion can also provide background knowledge": "rums. They adopt author and topic sarcastic prior",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "",
          "toward leveraging pre-trained contextual": "Ilic et al. (2018)\n0.79",
          "embed-": "-"
        },
        {
          "cussion can also provide background knowledge": "preference as context embedding that provides a",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "",
          "toward leveraging pre-trained contextual": "Khodak et al. (2018)\n0.77",
          "embed-": "-"
        },
        {
          "cussion can also provide background knowledge": "simple yet\nrepresentative background knowledge.",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        },
        {
          "cussion can also provide background knowledge": "Nimala et al. (2020) also propose an unsupervised",
          "toward leveraging pre-trained contextual": "RCNN-RoBERTa (Potamias et al., 2020)\n0.79",
          "embed-": "0.78"
        },
        {
          "cussion can also provide background knowledge": "probabilistic relational model to identify common",
          "toward leveraging pre-trained contextual": "",
          "embed-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: State-of-the-artNN classifiers and results on",
      "data": [
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "of\nresearch utilizes various user embedding tech-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "rectional\nEncoder Representations\nfrom Trans-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "niques that encode users’ stylometric and person-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "formers (Devlin et al., 2019), RoBERTa (Liu et al.,"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "ality features\nto improve their\nsarcasm detection",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "2019), and XLNet (Yang et al., 2019), etc. are wit-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "models (Hazarika et al., 2018). Their model, CAS-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "nessing a surge."
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "CADE, utilizes user embeddings that encode stylo-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "Potamias et al. (2020) propose Recurrent CNN"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "metric and personality features of the users. When",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "RoBERTA (RCNN-RoBERTa),\na\nhybrid\nneural"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "used along with content-based feature extractors",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "architecture building on RoBERTA architecture,"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "such as Convolutional Neural Networks (CNNs), a",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "which is\nfurther enhanced with the employment"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "signiﬁcant boost\nin the classiﬁcation performance",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "and devise of a recurrent convolutional neural net-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "on a large Reddit corpus is achieved. Similarly to",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "work.\nThey\nreport\na\nperformance with an\nac-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "how a user controls the degree of sarcasm in a com-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "curacy of %79 on SARC dataset\n(Khodak et al.,"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "ment,\nthey extrapolate that\nthe ensuing discourse",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "2018).\nSimilarly, Dadu and Pant\n(2020) use\nan"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "of comments belonging to a particular discussion",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "ensemble of RoBERTa and ALBERT (Lan et al.,"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "forum contains contextual\ninformation relevant\nto",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "2019) on Get it #OffMyChest dataset (Jaidka et al.,"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "the sarcasm identiﬁcation. They embed topical\nin-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "2020)\nachieve\na\nperformance\nof %85\naccuracy"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "formation that selectively incurs bias towards the",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "Javdan et al.\n(2020) use\nwith F 1 score of 0.55."
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "degree of sarcasm present\nin the comments of a",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "BERT along with aspect-based sentiment analysis"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "discussion.\nFor example, comments on political",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "to extract the relation between context dialogue se-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "leaders or sports matches are generally more prone",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "quence and response. They obtain an F1 score of"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "to sarcasm than natural disasters.\nContextual\nin-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "0.73 on the Twitter dataset and 0.73 over the Red-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "formation extracted from the discourse of a dis-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "dit dataset3. We expect to see more studies geared"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "cussion can also provide background knowledge",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "toward leveraging pre-trained contextual\nembed-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "or cues about\nthe discussion topic. To extract\nthe",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "dings and transformers toward sarcasm detection"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "discourse features,\nthey take a similar approach of",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "in the upcoming years."
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "document modeling performed for stylometric fea-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "Method\nAcc\nF1"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "tures.",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "ELMo (Peters et al., 2018)\n0.70\n0.70"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "Agrawal et al. (2020) formulate the task of sar-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "casm detection as a sequence classiﬁcation prob-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "NBSVM (Wang and Manning, 2012)\n0.65\n0.65"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "lem by leveraging the natural shifts in various emo-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "XLnet (Yang et al., 2019)\n0.76\n0.76"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "Li et al.\ntions over\nthe course of a piece of\ntext.",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "BERT-cased\n0.76\n0.76"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "(2020) propose a semi-supervised method for con-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "RoBERTa (Liu et al., 2019)\n0.77\n0.77"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "textual sarcasm detection in online discussion fo-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "CASCADE (Hazarika et al., 2018)\n0.74\n0.75"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "rums. They adopt author and topic sarcastic prior",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "Ilic et al. (2018)\n0.79\n-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "preference as context embedding that provides a",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "Khodak et al. (2018)\n0.77\n-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "simple yet\nrepresentative background knowledge.",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "Nimala et al. (2020) also propose an unsupervised",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "RCNN-RoBERTa (Potamias et al., 2020)\n0.79\n0.78"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "probabilistic relational model to identify common",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "Table 2: State-of-the-art NN classiﬁers and results on"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "sarcasm topics based on the sentiment distribution",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "Reddit Politics dataset"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "of words in tweets.",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "3.1\nSarcasm detection using pre-trained",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "4\nDatasets"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "language models",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "This section outlines the datasets used for compu-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "Given the highlighted\nimportance\nof\ncontext\nto",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "tational studies on sarcasm detection. Commonly,"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "capture ﬁgurative\nlanguage\nphenomena\nand the",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "they are divided into three\ncategories\nshort\ntext"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "difﬁculties\nof data\nannotation,\ntransfer\nlearning",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "(e.g., Tweets, Reddits),\nlong text (e.g., discussions"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "approaches\nare\ngaining\nattention\nin various\ndo-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "on forums),\ntranscripts\n(e.g., conversational\ntran-"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "main adaptation problems.\nIn particular,\nthe uti-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "scripts of a TV show or a call center). Short\ntext"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "lization of pre-trained embeddings such as Global",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "3Twitter and Reddit datasets used for\nin this study were"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "2014),\nand\nVectors\n(GloVe)\n(Pennington et al.,",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": ""
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "provided in the shared task on Sarcasm Detection, organized"
        },
        {
          "(Ghosh and Veale, 2017).\nAnother popular\nline": "ELMo (Peters et al., 2018) or\nleveraging Trans-",
          "former\nseq2seq methods\nsuch\nas BERT (Bidi-": "at Codalab."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "can contain only one (possibly sarcastic) utterance,": "whereas long text may contain a sarcastic sentence",
          "and book reviews, along with news articles marked": "with sarcasm and sentiment.\nIn an earlier\nstudy,"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "among other non-sarcastic sentences that could po-",
          "and book reviews, along with news articles marked": "Reyes and Rosso\n(2012)\ngarner\n11,000\nreviews"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "tentially function as context.",
          "and book reviews, along with news articles marked": "Filatova\nof products with sarcastic expressions."
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "(2012)\npresent\na\ncorpus\ngeneration\nexperiment"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "4.1\nShort text",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "where they collect\nregular and sarcastic Amazon"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "This\ncategory\nof data\nis\nthe\ndominant\nform of",
          "and book reviews, along with news articles marked": "product reviews. This resulting corpus can be used"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "expression\non social media, mostly\nas\na direct",
          "and book reviews, along with news articles marked": "for identifying sarcasm on two levels: a document"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "result of restriction on text\nlength. Consequently,",
          "and book reviews, along with news articles marked": "and a text utterance, where a text utterance can be"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "this\ntype\nof\ntext\nis\nrife with\nabbreviations\nto",
          "and book reviews, along with news articles marked": "as short as a sentence and as long as a whole doc-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "make efﬁcient use of space on platforms such as",
          "and book reviews, along with news articles marked": "ument."
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Twitter. Two main approaches are utilized toward",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "annotation of\ntweets: Manual and hashtag-based.",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "4.3\nTranscripts and dialogues"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Riloff et al.\n(2013);\nMaynard and Greenwood",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "Sarcasm is often expressed\nin the\ncontext of\na"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "(2014); Mishra et al.\n(2016); Pt´aˇcek et al.\n(2014)",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "conversation,\nas a\nresponse projecting contemp-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "introduce manually annotated datasets of sarcastic",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "Tepperman et al.\n(2006) uses 131\ntuous\nintent."
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "utterances.\nMost\nannotation\napproaches\nin the",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "call\ncenter\ntranscripts\nto\nlook\nfor\noccurrences"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "literature are conducted using hashtags\nto create",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "of\n“yeah right”\nas\na marker of\nsarcasm.\nSimi-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "labeled datasets.\nSarcastic\nintent\nin English is",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "larly, Rakov and Rosenberg (2013) through crowd-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "commonly\nand\nculturally\ncommunicated\nusing",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "sourcing\ncollect\nsentences\nfrom an MTV show"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "hashtags\nsuch\nas\n#sarcasm,\n#sarcastic,\n#not.",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "Joshi et al.\n(2016) also present a\ncalled “Daria.”"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Davidov et al.\n(2010);\nGonz´alez-Ib´anez et al.",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "manually annotated transcript of\nthe popular\nsit-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "(2011);\nReyes et al.\n(2012)\nuse\nhashtag-based",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "com “Friends.”"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Liebrecht et al.\n(2013) only\ndatasets of\ntweets.",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "uses #not\nto collect and label\ntheir tweets. While",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "collecting\nsarcastic\ntweets\nusing\nthis method",
          "and book reviews, along with news articles marked": "5\nConclusion"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "is\nundemanding,\nthe\ninclusion\nof\nnon-sarcastic",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "tweets\ncan be\nchallenging since\ntweets\ncontain-",
          "and book reviews, along with news articles marked": "Sarcasm detection research has seen a signiﬁcant"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "ing\n#notsarcastic may\nnot\nrepresent\na\ngeneral",
          "and book reviews, along with news articles marked": "surge in interest\nin the past\nfew years, which jus-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "2015).\nnon-sarcastic\ntext\n(Bamman and Smith,",
          "and book reviews, along with news articles marked": "tiﬁably calls for an investigation.\nThis article fo-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Another approach is\nto collect\nthe non-sarcastic",
          "and book reviews, along with news articles marked": "cuses on approaches\nto automatic sarcasm detec-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "tweets of users whose\nsarcastic\ntweets\nare\nalso",
          "and book reviews, along with news articles marked": "tion in text. We discern three major paradigms"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "present\nin the dataset. To ensure collection of true",
          "and book reviews, along with news articles marked": "in the history of sarcasm detection research:\nthe"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "(2015)\nsarcasm,\nsome\nstudies\nlike Fersini et al.",
          "and book reviews, along with news articles marked": "use of hashtag-driven supervised learning toward"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "manually veriﬁed the initial hashtag-based tweets",
          "and book reviews, along with news articles marked": "building annotated datasets,\nsemi-supervised pat-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "using annotators.",
          "and book reviews, along with news articles marked": "tern extraction to identify implicit sentiment, and"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Reddit\nis\nthe\nother\npopular\nplatform for\nre-",
          "and book reviews, along with news articles marked": "the utilization of extra-textual\ninformation as con-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "searchers\nto\ncollect\nsarcasm using\nhashtag\n“/s”",
          "and book reviews, along with news articles marked": "text\n(e.g., user’s characteristic proﬁling). While"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "(Reddit’s\nequivalent\nof\n“#sarcasm”\non Twitter).",
          "and book reviews, along with news articles marked": "rule-based approaches attempt to capture any indi-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Khodak et al. (2018) present SARC, a large-scale",
          "and book reviews, along with news articles marked": "cation of sarcasm in the form of\nrules, statistical"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "self-annotated\ncorpus\nfor\nsarcasm that\ncontains",
          "and book reviews, along with news articles marked": "methods use features like shifts in sentiment, spe-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "more\nthan\na million\nexamples\nof\nsarcastic/non-",
          "and book reviews, along with news articles marked": "ciﬁc semi-supervised patterns, etc. Deep learning"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "sarcastic statements made on Reddit.",
          "and book reviews, along with news articles marked": "techniques have also been used to incorporate con-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "text, e.g., additional stylometric features of authors"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "4.2\nLong text",
          "and book reviews, along with news articles marked": ""
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "",
          "and book reviews, along with news articles marked": "in conversations and the nature of discussion top-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Lukin and Walker\n(2013) use\nthe\nInternet Argu-",
          "and book reviews, along with news articles marked": "ics. An underlying theme of these past approaches"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "ment Corpus\n(IAC)\n(Walker et al., 2012) which",
          "and book reviews, along with news articles marked": "(either in terms of rules or features)\nis predicated"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "contains a set of 390,704 posts in 11,800 discus-",
          "and book reviews, along with news articles marked": "on sarcasm’s\ncontemptuous nature.\nNovel\ntech-"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "sions\nextracted from the online debate\nsite 4fo-",
          "and book reviews, along with news articles marked": "niques to incorporate contextual\ninsight have also"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "rums.com, annotated for\nseveral dialogic and ar-",
          "and book reviews, along with news articles marked": "been explored, mostly centered on the emerging"
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "gumentative markers, one of them being sarcasm.",
          "and book reviews, along with news articles marked": "direction toward utilizing language models."
        },
        {
          "can contain only one (possibly sarcastic) utterance,": "Reyes and Rosso (2014) collect a dataset of movie",
          "and book reviews, along with news articles marked": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "standing.\nIn Proceedings of the 2019 Conference of"
        },
        {
          "References": "Ameeta Agrawal, Aijun An,\nand Manos Papagelis.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "the North American Chapter of\nthe Association for"
        },
        {
          "References": "2020.\nLeveraging transitions of emotions for\nsar-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Computational Linguistics: Human Language Tech-"
        },
        {
          "References": "the 43rd Inter-\ncasm detection.\nIn Proceedings of",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "nologies, Volume 1 (Long and Short Papers), pages"
        },
        {
          "References": "national ACM SIGIR Conference on Research and",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "4171–4186."
        },
        {
          "References": "Development in Information Retrieval, pages 1505–",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "1508.",
          "deep bidirectional\ntransformers for language under-": "Jodi Eisterhold, Salvatore Attardo, and Diana Boxer."
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "2006. Reactions to irony in discourse: evidence for"
        },
        {
          "References": "David Bamman and Noah A Smith. 2015.\nContextu-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Journal of Pragmat-\nthe least disruption principle."
        },
        {
          "References": "alized sarcasm detection on twitter.\nIn Ninth inter-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "ics, 38(8):1239–1256."
        },
        {
          "References": "national AAAI conference on web and social media.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "Citeseer.",
          "deep bidirectional\ntransformers for language under-": "Delia Iraz´u Herna´ndez Far´ıas, Viviana Patti, and Paolo"
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Rosso. 2016.\nIrony detection in twitter: The role"
        },
        {
          "References": "Francesco Barbieri, Horacio Saggion, and Francesco",
          "deep bidirectional\ntransformers for language under-": "of affective content. ACM Transactions on Internet"
        },
        {
          "References": "Ronzano. 2014.\nModelling sarcasm in twitter,\na",
          "deep bidirectional\ntransformers for language under-": "Technology (TOIT), 16(3):1–24."
        },
        {
          "References": "novel approach. In Proceedings of the 5th Workshop",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Elisabetta Fersini, Federico Alberto Pozzi, and Enza"
        },
        {
          "References": "on Computational Approaches to Subjectivity, Senti-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Messina. 2015. Detecting irony and sarcasm in mi-"
        },
        {
          "References": "ment and Social Media Analysis, pages 50–58.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "croblogs: The role of expressive signals and ensem-"
        },
        {
          "References": "Santosh Kumar Bharti, Korra Sathya Babu, and San-",
          "deep bidirectional\ntransformers for language under-": "ble\nclassiﬁers.\nIn 2015 IEEE International Con-"
        },
        {
          "References": "jay Kumar Jena. 2015. Parsing-based sarcasm senti-",
          "deep bidirectional\ntransformers for language under-": "ference on Data Science and Advanced Analytics"
        },
        {
          "References": "ment recognition in twitter data.\nIn 2015 IEEE/ACM",
          "deep bidirectional\ntransformers for language under-": "(DSAA), pages 1–8. IEEE."
        },
        {
          "References": "International Conference on Advances in Social Net-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Elena Filatova. 2012.\nIrony and sarcasm:\nCorpus"
        },
        {
          "References": "works Analysis and Mining (ASONAM), pages 1373–",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "generation and analysis using crowdsourcing.\nIn"
        },
        {
          "References": "1380. IEEE.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Proceedings of the Eighth International Conference"
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "on Language Resources and Evaluation (LREC’12),"
        },
        {
          "References": "David M Blei, Andrew Y Ng, and Michael\nI Jordan.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "pages 392–398."
        },
        {
          "References": "Journal of ma-\n2003.\nLatent dirichlet allocation.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "chine Learning research, 3(Jan):993–1022.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Aniruddha Ghosh and Tony Veale. 2016. Fracking sar-"
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "the\ncasm using neural network.\nIn Proceedings of"
        },
        {
          "References": "Konstantin Buschmeier, Philipp Cimiano, and Roman",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "7th workshop on computational approaches to sub-"
        },
        {
          "References": "Klinger. 2014. An impact analysis of features in a",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "jectivity, sentiment and social media analysis, pages"
        },
        {
          "References": "classiﬁcation approach to irony detection in prod-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "161–169."
        },
        {
          "References": "the 5th Workshop\nuct\nreviews.\nIn Proceedings of",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "on Computational Approaches to Subjectivity, Sen-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Aniruddha Ghosh and Tony Veale. 2017. Magnets for"
        },
        {
          "References": "timent and Social Media Analysis, pages 42–49.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "sarcasm: Making sarcasm detection timely, contex-"
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "the 2017\ntual and very personal.\nIn Proceedings of"
        },
        {
          "References": "Elisabeth Camp. 2012.\nSarcasm, pretense,\nand the",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Conference on Empirical Methods in Natural Lan-"
        },
        {
          "References": "semantics/pragmatics distinction. Noˆus, 46(4):587–",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "guage Processing, pages 482–491."
        },
        {
          "References": "634.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Roberto Gonz´alez-Ib´anez,\nSmaranda Muresan,\nand"
        },
        {
          "References": "John D Campbell and Albert N Katz. 2012. Are there",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Nina Wacholder. 2011.\nIdentifying sarcasm in twit-"
        },
        {
          "References": "necessary conditions for inducing a sense of sarcas-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "the 49th An-\nter:\na closer\nlook.\nIn Proceedings of"
        },
        {
          "References": "tic irony? Discourse Processes, 49(6):459–480.",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "nual Meeting of\nthe Association for Computational"
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "Linguistics: Human Language Technologies, pages"
        },
        {
          "References": "Paula Carvalho, Lu´ıs Sarmento, M´ario J Silva,\nand",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "581–586."
        },
        {
          "References": "Eug´enio De Oliveira. 2009.\nClues\nfor detecting",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "irony in user-generated contents:\noh...!!\nit’s”\nso",
          "deep bidirectional\ntransformers for language under-": "Stephan Greene and Philip Resnik. 2009. More than"
        },
        {
          "References": "the 1st\ninternational\neasy”;-.\nIn Proceedings of",
          "deep bidirectional\ntransformers for language under-": "words: Syntactic packaging and implicit sentiment."
        },
        {
          "References": "CIKM workshop\non Topic-sentiment\nanalysis\nfor",
          "deep bidirectional\ntransformers for language under-": "of\nhuman\nlanguage\ntechnologies:\nIn Proceedings"
        },
        {
          "References": "mass opinion, pages 53–56.",
          "deep bidirectional\ntransformers for language under-": "The 2009 annual conference of\nthe north american"
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "chapter of the association for computational linguis-"
        },
        {
          "References": "Tanvi Dadu and Kartikey Pant. 2020.\nSarcasm detec-",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "tics, pages 503–511."
        },
        {
          "References": "tion using context separators in online discourse.\nIn",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "Proceedings of\nthe Second Workshop on Figurative",
          "deep bidirectional\ntransformers for language under-": "Devamanyu Hazarika, Soujanya Poria, Sruthi Gorantla,"
        },
        {
          "References": "Language Processing, pages 51–55.",
          "deep bidirectional\ntransformers for language under-": "Erik Cambria, Roger Zimmermann, and Rada Mi-"
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "halcea. 2018. Cascade: Contextual sarcasm detec-"
        },
        {
          "References": "Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.",
          "deep bidirectional\ntransformers for language under-": "arXiv preprint\ntion in online discussion forums."
        },
        {
          "References": "Semi-supervised recognition of\nsarcasm in twitter",
          "deep bidirectional\ntransformers for language under-": "arXiv:1805.06413."
        },
        {
          "References": "the fourteenth con-\nand amazon.\nIn Proceedings of",
          "deep bidirectional\ntransformers for language under-": ""
        },
        {
          "References": "ference on computational natural language learning,",
          "deep bidirectional\ntransformers for language under-": "Iraz´u Hern´andez-Far´ıas,\nJos´e-Miguel\nBened´ı,\nand"
        },
        {
          "References": "pages 107–116.",
          "deep bidirectional\ntransformers for language under-": "Paolo Rosso. 2015.\nApplying basic features from"
        },
        {
          "References": "",
          "deep bidirectional\ntransformers for language under-": "sentiment analysis for automatic irony detection.\nIn"
        },
        {
          "References": "Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand",
          "deep bidirectional\ntransformers for language under-": "Iberian Conference on Pattern Recognition and Im-"
        },
        {
          "References": "Kristina Toutanova. 2019.\nBert:\nPre-training of",
          "deep bidirectional\ntransformers for language under-": "age Analysis, pages 337–344. Springer."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Yutaka Matsuo. 2018.\nDeep contextualized word",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "van den Bosch. 2013.\nThe perfect solution for de-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "representations for detecting sarcasm and irony.\nIn",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "tecting sarcasm in tweets# not.\nIn Proceedings of"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Proceedings of the 9th Workshop on Computational",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "the 4th Workshop on Computational Approaches to"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Approaches\nto Subjectivity,\nSentiment and Social",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Subjectivity, Sentiment and Social Media Analysis,"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Media Analysis, pages 2–7.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "pages 29–37."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Stacey L Ivanko and Penny M Pexman. 2003. Context",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Bing Liu et al. 2010.\nSentiment analysis and subjec-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Discourse Pro-\nincongruity and irony processing.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Handbook of natural\ntivity.\nlanguage processing,"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "cesses, 35(3):241–279.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "2(2010):627–666."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Kokil Jaidka, Iknoor Singh, Jiahui Lu, Niyati Chhaya,",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Peng Liu, Wei Chen, Gaoyan Ou, Tengjiao Wang,"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "and Lyle Ungar. 2020. A report of the cl-aff offmy-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Dongqing Yang, and Kai Lei. 2014. Sarcasm detec-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "chest\nshared\ntask: Modeling\nsupportiveness\nand",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "tion in social media based on imbalanced classiﬁca-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "the AAAI-20 Work-\ndisclosure.\nIn Proceedings of",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "tion.\nIn International Conference on Web-Age Infor-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "shop on Affective Content Analysis, New York, USA,",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "mation Management, pages 459–471. Springer."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "AAAI.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Soroush Javdan, Behrouz Minaei-Bidgoli, et al. 2020.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Applying transformers and aspect-based sentiment",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis,"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "analysis approaches on sarcasm detection.\nIn Pro-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Luke\nZettlemoyer,\nand Veselin\nStoyanov.\n2019."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "ceedings of the Second Workshop on Figurative Lan-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Roberta: A robustly optimized bert pretraining ap-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "guage Processing, pages 67–71.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "proach. arXiv preprint arXiv:1907.11692."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Aditya\nJoshi,\nVinita\nSharma,\nand\nPushpak Bhat-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Stephanie Lukin and Marilyn Walker. 2013.\nReally?"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "tacharyya. 2015. Harnessing context incongruity for",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "well. apparently bootstrapping improves the perfor-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "the 53rd An-\nsarcasm detection.\nIn Proceedings of",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "mance of\nsarcasm and nastiness classiﬁers\nfor on-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "nual Meeting of\nthe Association for Computational",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "the Workshop on\nline dialogue.\nIn Proceedings of"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Linguistics and the 7th International Joint Confer-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Language Analysis in Social Media, pages 30–40."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "ence on Natural Language Processing (Volume 2:",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Short Papers), pages 757–762.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Diana G Maynard and Mark A Greenwood. 2014. Who"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "cares about sarcastic tweets?\ninvestigating the im-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "pact of\nsarcasm on sentiment analysis.\nIn LREC"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Bhattacharyya, and Mark Carman. 2016. Are word",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "2014 Proceedings. ELRA."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "embedding-based features useful for sarcasm detec-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "the 2016 Conference on\ntion?\nIn Proceedings of",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Empirical Methods in Natural Language Processing,",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Dey, and Pushpak Bhattacharyya. 2016. Harnessing"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "pages 1006–1011.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "cognitive features for sarcasm detection.\nIn Proceed-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "ings of\nthe 54th Annual Meeting of\nthe Association"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Mikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "for Computational Linguistics (Volume 1: Long Pa-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "2018.\nA large self-annotated corpus\nfor\nsarcasm.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "pers), pages 1095–1104."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "In Proceedings of the Eleventh International Confer-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "ence on Language Resources and Evaluation (LREC",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "2018).",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Shubhadeep Mukherjee and Pradip Kumar Bala. 2017."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Sarcasm detection in microblogs using na¨ıve bayes"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Roger Kreuz and Gina Caucci. 2007.\nLexical\ninﬂu-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "and fuzzy clustering. Technology in Society, 48:19–"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "ences on the perception of\nsarcasm.\nIn Proceed-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "27."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "ings of\nthe Workshop on computational approaches",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "to Figurative Language, pages 1–4.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "K Nimala, R Jebakumar, and M Saravanan. 2020. Sen-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "timent\ntopic sarcasm mixture model\nto distinguish"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman,",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "sarcasm prevalent\ntopics\nbased\non\nthe\nsentiment"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Kevin Gimpel, Piyush Sharma,\nand Radu Soricut.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Journal of Ambient In-\nbearing words in the tweets."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "2019. Albert: A lite bert for self-supervised learn-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "telligence and Humanized Computing, pages 1–10."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "arXiv\npreprint\ning\nof\nlanguage\nrepresentations.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "arXiv:1909.11942.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Debora Nozza, Elisabetta Fersini, and Enza Messina."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "2016. Unsupervised irony detection: a probabilistic"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Christopher\nJ Lee\nand Albert N Katz. 1998.\nThe",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "model with word embeddings. In International Con-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "differential\nrole of\nridicule\nin sarcasm and irony.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "ference on Knowledge Discovery and Information"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Metaphor and symbol, 13(1):1–15.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Retrieval, volume 2, pages 68–76. SCITEPRESS."
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Meimei Li, Chen Lang, Min Yu, Yue Lu, Chao Liu,",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": ""
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Jianguo Jiang, and Weiqing Huang. 2020.\nScx-sd:",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Jeffrey Pennington, Richard Socher, and Christopher D"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Semi-supervised method for contextual sarcasm de-",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "Manning. 2014. Glove: Global vectors for word rep-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "tection.\nIn International Conference on Knowledge",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "the 2014 conference\nresentation.\nIn Proceedings of"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "Science, Engineering and Management, pages 288–",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "on empirical methods in natural\nlanguage process-"
        },
        {
          "Suzana Ilic, Edison Marrese-Taylor, Jorge Balazs, and": "299. Springer.",
          "Christine Liebrecht,\nFlorian Kunneman,\nand Antal": "ing (EMNLP), pages 1532–1543."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Gardner, Christopher Clark, Kenton Lee, and Luke",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "2018b.\nWe usually don’t\nlike going to the den-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Zettlemoyer. 2018. Deep contextualized word repre-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "tist: Using common sense to detect irony on twitter."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "sentations. arXiv preprint arXiv:1802.05365.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Computational Linguistics, 44(4):793–832."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Tony Veale and Yanfen Hao. 2010. Detecting ironic in-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Rolandos\nAlexandros\nPotamias,\nGeorgios\nSiolas,",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "tent in creative comparisons.\nIn ECAI, volume 215,"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "and Andreas-Georgios\nStafylopatis.\n2020.\nA",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "pages 765–770."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "transformer-based approach to irony and sarcasm de-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "tection. Neural Computing and Applications, pages",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Marilyn Walker, Jean E Fox Tree, Pranav Anand, Rob"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "1–12.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Abbott, and Joseph King. 2012.\nA corpus for\nre-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "search on deliberation and debate.\nIn Proceedings"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Tom´aˇs Pt´aˇcek,\nIvan Habernal,\nand Jun Hong. 2014.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "of the Eighth International Conference on Language"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Sarcasm detection\non\nczech\nand\nenglish\ntwitter.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Resources and Evaluation (LREC’12), pages 812–"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "the 25th Inter-\nIn Proceedings of COLING 2014,",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "817."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "national Conference on Computational Linguistics:",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Technical Papers, pages 213–223.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Byron C Wallace, Laura Kertz, Eugene Charniak, et al."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "2014.\nHumans\nrequire context\nto infer\nironic in-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Rachel Rakov and Andrew Rosenberg. 2013.\n” sure,\ni",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "tent\n(so computers probably do,\ntoo).\nIn Proceed-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "did the right thing”: a system for sarcasm detection",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "ings of\nthe 52nd Annual Meeting of\nthe Association"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "in speech.\nIn Interspeech, pages 842–846.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "for Computational Linguistics (Volume 2: Short Pa-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "pers), pages 512–516."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Antonio Reyes and Paolo Rosso. 2012. Making ob-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "jective decisions\nfrom subjective data:\nDetecting",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Sida I Wang and Christopher D Manning. 2012. Base-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Decision support\nsys-\nirony in customer\nreviews.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "lines and bigrams: Simple, good sentiment and topic"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "tems, 53(4):754–760.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "the 50th Annual\nclassiﬁcation.\nIn Proceedings of"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Meeting of\nthe Association for Computational Lin-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Antonio Reyes and Paolo Rosso. 2014. On the difﬁ-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "guistics (Volume 2: Short Papers), pages 90–94."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "culty of automatically detecting irony: beyond a sim-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Zelin Wang, Zhijian Wu, Ruimin Wang, and Yafeng"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Knowledge and Information\nple case of negation.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Ren. 2015.\nTwitter sarcasm detection exploiting a"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Systems, 40(3):595–614.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "context-based model.\nIn international conference"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "on web information systems engineering, pages 77–"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Antonio Reyes,\nPaolo Rosso,\nand Davide Buscaldi.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "91. Springer."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "2012.\nFrom humor\nrecognition to irony detection:",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Data &\nThe ﬁgurative language of\nsocial media.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Deirdre Wilson. 2006. The pragmatics of verbal irony:"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Knowledge Engineering, 74:1–12.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Echo or pretence? Lingua, 116(10):1722–1743."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Antonio Reyes, Paolo Rosso,\nand Tony Veale. 2013.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Humblebrag:\nThe art of\nfalse\nHarris Wittels. 2012."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "A multidimensional\napproach for\ndetecting irony",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "modesty. Grand Central Publishing."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Language\nresources\nand\nin\ntwitter.\nevaluation,",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "47(1):239–268.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Zhilin Yang, Zihang Dai, Yiming Yang,\nJaime Car-"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "bonell, Russ R Salakhutdinov, and Quoc V Le. 2019."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Ellen Riloff, Ashequl Qadir,\nPrafulla Surve, Lalin-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "Xlnet:\nGeneralized autoregressive pretraining for"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "dra De Silva, Nathan Gilbert, and Ruihong Huang.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "in-\nlanguage understanding.\nIn Advances in neural"
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "2013. Sarcasm as contrast between a positive senti-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": "formation processing systems, pages 5753–5763."
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "the\nment and negative situation.\nIn Proceedings of",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "2013 conference on empirical methods\nin natural",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "language processing, pages 704–714.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Joseph\nTepperman,\nDavid\nTraum,\nand\nShrikanth",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Narayanan. 2006.\n” yeah right”: Sarcasm recogni-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "tion for spoken dialogue systems.\nIn Ninth interna-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "tional conference on spoken language processing.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Icwsm-a great catchy name: Semi-supervised recog-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "nition of\nsarcastic\nsentences\nin online product\nre-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "views.\nIn ICWSM, pages 162–169. Washington, DC.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "2018a. Semeval-2018 task 3:\nIrony detection in en-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "glish tweets.\nIn Proceedings of The 12th Interna-",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "tional Workshop on Semantic Evaluation, pages 39–",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        },
        {
          "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt": "50.",
          "Cynthia Van Hee, Els Lefever, and V´eronique Hoste.": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Leveraging transitions of emotions for sarcasm detection",
      "authors": [
        "Ameeta Agrawal",
        "Aijun An",
        "Manos Papagelis"
      ],
      "year": "2020",
      "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "2",
      "title": "Contextualized sarcasm detection on twitter",
      "authors": [
        "David Bamman",
        "Noah Smith"
      ],
      "year": "2015",
      "venue": "Ninth international AAAI conference on web and social media"
    },
    {
      "citation_id": "3",
      "title": "Modelling sarcasm in twitter, a novel approach",
      "authors": [
        "Francesco Barbieri",
        "Horacio Saggion",
        "Francesco Ronzano"
      ],
      "year": "2014",
      "venue": "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "4",
      "title": "Parsing-based sarcasm sentiment recognition in twitter data",
      "authors": [
        "Santosh Kumar Bharti",
        "Korra Sathya Babu",
        "Sanjay Kumar"
      ],
      "year": "2015",
      "venue": "2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)"
    },
    {
      "citation_id": "5",
      "title": "Latent dirichlet allocation",
      "authors": [
        "Andrew David M Blei",
        "Michael Ng",
        "Jordan"
      ],
      "year": "2003",
      "venue": "Journal of machine Learning research"
    },
    {
      "citation_id": "6",
      "title": "An impact analysis of features in a classification approach to irony detection in product reviews",
      "authors": [
        "Konstantin Buschmeier",
        "Philipp Cimiano",
        "Roman Klinger"
      ],
      "year": "2014",
      "venue": "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "7",
      "title": "Sarcasm, pretense, and the semantics/pragmatics distinction",
      "authors": [
        "Elisabeth Camp"
      ],
      "year": "2012",
      "venue": "Noûs"
    },
    {
      "citation_id": "8",
      "title": "Are there necessary conditions for inducing a sense of sarcastic irony?",
      "authors": [
        "D John",
        "Albert Campbell",
        "Katz"
      ],
      "year": "2012",
      "venue": "Discourse Processes"
    },
    {
      "citation_id": "9",
      "title": "Clues for detecting irony in user-generated contents: oh...!! it's\" so easy",
      "authors": [
        "Paula Carvalho",
        "Luís Sarmento",
        "J Mário",
        "Eugénio Silva",
        "Oliveira"
      ],
      "year": "2009",
      "venue": "Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion"
    },
    {
      "citation_id": "10",
      "title": "Sarcasm detection using context separators in online discourse",
      "authors": [
        "Tanvi Dadu",
        "Kartikey Pant"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Figurative Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Semi-supervised recognition of sarcasm in twitter and amazon",
      "authors": [
        "Dmitry Davidov",
        "Oren Tsur",
        "Ari Rappoport"
      ],
      "year": "2010",
      "venue": "Proceedings of the fourteenth conference on computational natural language learning"
    },
    {
      "citation_id": "12",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "13",
      "title": "Reactions to irony in discourse: evidence for the least disruption principle",
      "authors": [
        "Jodi Eisterhold",
        "Salvatore Attardo",
        "Diana Boxer"
      ],
      "year": "2006",
      "venue": "Journal of Pragmatics"
    },
    {
      "citation_id": "14",
      "title": "Irony detection in twitter: The role of affective content",
      "authors": [
        "Delia Irazú",
        "Hernańdez Farías",
        "Viviana Patti",
        "Paolo Rosso"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Internet Technology (TOIT)"
    },
    {
      "citation_id": "15",
      "title": "Detecting irony and sarcasm in microblogs: The role of expressive signals and ensemble classifiers",
      "authors": [
        "Elisabetta Fersini",
        "Alberto Pozzi",
        "Enza Messina"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)"
    },
    {
      "citation_id": "16",
      "title": "Irony and sarcasm: Corpus generation and analysis using crowdsourcing",
      "authors": [
        "Elena Filatova"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "17",
      "title": "Fracking sarcasm using neural network",
      "authors": [
        "Aniruddha Ghosh",
        "Tony Veale"
      ],
      "year": "2016",
      "venue": "Proceedings of the 7th workshop on computational approaches to subjectivity, sentiment and social media analysis"
    },
    {
      "citation_id": "18",
      "title": "Magnets for sarcasm: Making sarcasm detection timely, contextual and very personal",
      "authors": [
        "Aniruddha Ghosh",
        "Tony Veale"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Identifying sarcasm in twitter: a closer look",
      "authors": [
        "Roberto González-Ibánez",
        "Smaranda Muresan",
        "Nina Wacholder"
      ],
      "year": "2011",
      "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "20",
      "title": "More than words: Syntactic packaging and implicit sentiment",
      "authors": [
        "Stephan Greene",
        "Philip Resnik"
      ],
      "year": "2009",
      "venue": "Proceedings of human language technologies: The 2009 annual conference of the north american chapter of the association for computational linguistics"
    },
    {
      "citation_id": "21",
      "title": "Contextual sarcasm detection in online discussion forums",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Sruthi Gorantla",
        "Erik Cambria",
        "Roger Zimmermann",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "Contextual sarcasm detection in online discussion forums",
      "arxiv": "arXiv:1805.06413"
    },
    {
      "citation_id": "22",
      "title": "Applying basic features from sentiment analysis for automatic irony detection",
      "authors": [
        "Irazú Hernández-Farías",
        "José-Miguel Benedí",
        "Paolo Rosso"
      ],
      "year": "2015",
      "venue": "Iberian Conference on Pattern Recognition and Image Analysis"
    },
    {
      "citation_id": "23",
      "title": "Deep contextualized word representations for detecting sarcasm and irony",
      "authors": [
        "Suzana Ilic",
        "Edison Marrese-Taylor",
        "Jorge Balazs",
        "Yutaka Matsuo"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "24",
      "title": "Context incongruity and irony processing",
      "authors": [
        "L Stacey",
        "Penny Ivanko",
        "Pexman"
      ],
      "year": "2003",
      "venue": "Discourse Processes"
    },
    {
      "citation_id": "25",
      "title": "A report of the cl-aff offmychest shared task: Modeling supportiveness and disclosure",
      "authors": [
        "Kokil Jaidka",
        "Iknoor Singh",
        "Jiahui Lu",
        "Niyati Chhaya",
        "Lyle Ungar"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI-20 Workshop on Affective Content Analysis"
    },
    {
      "citation_id": "26",
      "title": "Applying transformers and aspect-based sentiment analysis approaches on sarcasm detection",
      "authors": [
        "Soroush Javdan",
        "Behrouz Minaei-Bidgoli"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Figurative Language Processing"
    },
    {
      "citation_id": "27",
      "title": "Harnessing context incongruity for sarcasm detection",
      "authors": [
        "Aditya Joshi",
        "Vinita Sharma",
        "Pushpak Bhattacharyya"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Are word embedding-based features useful for sarcasm detection?",
      "authors": [
        "Aditya Joshi",
        "Vaibhav Tripathi",
        "Kevin Patel",
        "Pushpak Bhattacharyya",
        "Mark Carman"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "29",
      "title": "A large self-annotated corpus for sarcasm",
      "authors": [
        "Mikhail Khodak",
        "Nikunj Saunshi",
        "Kiran Vodrahalli"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "30",
      "title": "Lexical influences on the perception of sarcasm",
      "authors": [
        "Roger Kreuz",
        "Gina Caucci"
      ],
      "year": "2007",
      "venue": "Proceedings of the Workshop on computational approaches to Figurative Language"
    },
    {
      "citation_id": "31",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Zhenzhong Lan",
        "Mingda Chen",
        "Sebastian Goodman",
        "Kevin Gimpel",
        "Piyush Sharma",
        "Radu Soricut"
      ],
      "year": "2019",
      "venue": "Albert: A lite bert for self-supervised learning of language representations",
      "arxiv": "arXiv:1909.11942"
    },
    {
      "citation_id": "32",
      "title": "The differential role of ridicule in sarcasm and irony",
      "authors": [
        "J Christopher",
        "Albert Lee",
        "Katz"
      ],
      "year": "1998",
      "venue": "Metaphor and symbol"
    },
    {
      "citation_id": "33",
      "title": "Scx-sd: Semi-supervised method for contextual sarcasm detection",
      "authors": [
        "Meimei Li",
        "Chen Lang",
        "Min Yu",
        "Yue Lu",
        "Chao Liu",
        "Jianguo Jiang",
        "Weiqing Huang"
      ],
      "year": "2020",
      "venue": "International Conference on Knowledge Science, Engineering and Management"
    },
    {
      "citation_id": "34",
      "title": "The perfect solution for detecting sarcasm in tweets# not",
      "authors": [
        "Christine Liebrecht",
        "Florian Kunneman",
        "Antal Van Den",
        "Bosch"
      ],
      "year": "2013",
      "venue": "Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "35",
      "title": "Sentiment analysis and subjectivity. Handbook of natural language processing",
      "authors": [
        "Bing Liu"
      ],
      "year": "2010",
      "venue": "Sentiment analysis and subjectivity. Handbook of natural language processing"
    },
    {
      "citation_id": "36",
      "title": "Sarcasm detection in social media based on imbalanced classification",
      "authors": [
        "Peng Liu",
        "Wei Chen",
        "Gaoyan Ou",
        "Tengjiao Wang",
        "Dongqing Yang",
        "Kai Lei"
      ],
      "year": "2014",
      "venue": "International Conference on Web-Age Information Management"
    },
    {
      "citation_id": "37",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "38",
      "title": "Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue",
      "authors": [
        "Stephanie Lukin",
        "Marilyn Walker"
      ],
      "year": "2013",
      "venue": "Proceedings of the Workshop on Language Analysis in Social Media"
    },
    {
      "citation_id": "39",
      "title": "Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis",
      "authors": [
        "G Diana",
        "Mark Maynard",
        "Greenwood"
      ],
      "year": "2014",
      "venue": "LREC 2014 Proceedings"
    },
    {
      "citation_id": "40",
      "title": "Harnessing cognitive features for sarcasm detection",
      "authors": [
        "Abhijit Mishra",
        "Diptesh Kanojia",
        "Seema Nagar",
        "Kuntal Dey",
        "Pushpak Bhattacharyya"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Sarcasm detection in microblogs using naïve bayes and fuzzy clustering",
      "authors": [
        "Shubhadeep Mukherjee",
        "Pradip Kumar"
      ],
      "year": "2017",
      "venue": "Technology in Society"
    },
    {
      "citation_id": "42",
      "title": "Sentiment topic sarcasm mixture model to distinguish sarcasm prevalent topics based on the sentiment bearing words in the tweets",
      "authors": [
        "R Nimala",
        "M Jebakumar",
        "Saravanan"
      ],
      "year": "2020",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "43",
      "title": "Unsupervised irony detection: a probabilistic model with word embeddings",
      "authors": [
        "Debora Nozza",
        "Elisabetta Fersini",
        "Enza Messina"
      ],
      "year": "2016",
      "venue": "International Conference on Knowledge Discovery and Information Retrieval"
    },
    {
      "citation_id": "44",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "45",
      "title": "Deep contextualized word representations",
      "authors": [
        "Mark Matthew E Peters",
        "Mohit Neumann",
        "Matt Iyyer",
        "Christopher Gardner",
        "Kenton Clark",
        "Luke Lee",
        "Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Deep contextualized word representations",
      "arxiv": "arXiv:1802.05365"
    },
    {
      "citation_id": "46",
      "title": "A transformer-based approach to irony and sarcasm detection",
      "authors": [
        "Rolandos Alexandros Potamias",
        "Georgios Siolas",
        "Andreas-Georgios Stafylopatis"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "47",
      "title": "Sarcasm detection on czech and english twitter",
      "authors": [
        "Tomáš Ptáček",
        "Ivan Habernal",
        "Jun Hong"
      ],
      "year": "2014",
      "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers"
    },
    {
      "citation_id": "48",
      "title": "sure, i did the right thing\": a system for sarcasm detection in speech",
      "authors": [
        "Rachel Rakov",
        "Andrew Rosenberg"
      ],
      "year": "2013",
      "venue": "Interspeech"
    },
    {
      "citation_id": "49",
      "title": "Making objective decisions from subjective data: Detecting irony in customer reviews. Decision support systems",
      "authors": [
        "Antonio Reyes",
        "Paolo Rosso"
      ],
      "year": "2012",
      "venue": "Making objective decisions from subjective data: Detecting irony in customer reviews. Decision support systems"
    },
    {
      "citation_id": "50",
      "title": "On the difficulty of automatically detecting irony: beyond a simple case of negation",
      "authors": [
        "Antonio Reyes",
        "Paolo Rosso"
      ],
      "year": "2014",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "51",
      "title": "From humor recognition to irony detection: The figurative language of social media",
      "authors": [
        "Antonio Reyes",
        "Paolo Rosso",
        "Davide Buscaldi"
      ],
      "year": "2012",
      "venue": "Data & Knowledge Engineering"
    },
    {
      "citation_id": "52",
      "title": "A multidimensional approach for detecting irony in twitter. Language resources and evaluation",
      "authors": [
        "Antonio Reyes",
        "Paolo Rosso",
        "Tony Veale"
      ],
      "year": "2013",
      "venue": "A multidimensional approach for detecting irony in twitter. Language resources and evaluation"
    },
    {
      "citation_id": "53",
      "title": "Sarcasm as contrast between a positive sentiment and negative situation",
      "authors": [
        "Ellen Riloff",
        "Ashequl Qadir",
        "Prafulla Surve",
        "Lalindra Silva",
        "Nathan Gilbert",
        "Ruihong Huang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "54",
      "title": "yeah right\": Sarcasm recognition for spoken dialogue systems",
      "authors": [
        "Joseph Tepperman",
        "David Traum",
        "Shrikanth Narayanan"
      ],
      "year": "2006",
      "venue": "Ninth international conference on spoken language processing"
    },
    {
      "citation_id": "55",
      "title": "Icwsm-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews",
      "authors": [
        "Oren Tsur",
        "Dmitry Davidov",
        "Ari Rappoport"
      ],
      "year": "2010",
      "venue": "ICWSM"
    },
    {
      "citation_id": "56",
      "title": "We usually don't like going to the dentist: Using common sense to detect irony on twitter",
      "authors": [
        "Cynthia Van Hee",
        "Els Lefever",
        "Véronique Hoste"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "57",
      "title": "Detecting ironic intent in creative comparisons",
      "authors": [
        "Tony Veale",
        "Yanfen Hao"
      ],
      "year": "2010",
      "venue": "ECAI"
    },
    {
      "citation_id": "58",
      "title": "A corpus for research on deliberation and debate",
      "authors": [
        "Marilyn Walker",
        "Jean Fox Tree",
        "Pranav Anand",
        "Rob Abbott",
        "Joseph King"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "59",
      "title": "Humans require context to infer ironic intent (so computers probably do, too)",
      "authors": [
        "Laura Byron C Wallace",
        "Eugene Kertz",
        "Charniak"
      ],
      "year": "2014",
      "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "60",
      "title": "Baselines and bigrams: Simple, good sentiment and topic classification",
      "authors": [
        "I Sida",
        "Christopher Wang",
        "Manning"
      ],
      "year": "2012",
      "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "61",
      "title": "Twitter sarcasm detection exploiting a context-based model",
      "authors": [
        "Zelin Wang",
        "Zhijian Wu",
        "Ruimin Wang",
        "Yafeng Ren"
      ],
      "year": "2015",
      "venue": "international conference on web information systems engineering"
    },
    {
      "citation_id": "62",
      "title": "The pragmatics of verbal irony: Echo or pretence?",
      "authors": [
        "Deirdre Wilson"
      ],
      "year": "2006",
      "venue": "Lingua"
    },
    {
      "citation_id": "63",
      "title": "Humblebrag: The art of false modesty",
      "authors": [
        "Harris Wittels"
      ],
      "year": "2012",
      "venue": "Humblebrag: The art of false modesty"
    },
    {
      "citation_id": "64",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    }
  ]
}