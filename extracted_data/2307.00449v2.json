{
  "paper_id": "2307.00449v2",
  "title": "A Dual-Stream Recurrence-Attention Network With Global-Local Awareness For Emotion Recognition In Textual Dialog",
  "published": "2023-07-02T01:25:47Z",
  "authors": [
    "Jiang Li",
    "Xiaoping Wang",
    "Zhigang Zeng"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In real-world dialog systems, the ability to understand the user's emotions and interact anthropomorphically is of great significance. Emotion Recognition in Conversation (ERC) is one of the key ways to accomplish this goal and has attracted growing attention. How to model the context in a conversation is a central aspect and a major challenge of ERC tasks. Most existing approaches struggle to adequately incorporate both global and local contextual information, and their network structures are overly sophisticated. For this reason, we propose a simple and effective Dual-stream Recurrence-Attention Network (DualRAN), which is based on Recurrent Neural Network (RNN) and Multi-head ATtention network (MAT). DualRAN eschews the complex components of current methods and focuses on combining recurrence-based methods with attention-based ones. DualRAN is a dual-stream structure mainly consisting of local-and global-aware modules, modeling a conversation simultaneously from distinct perspectives. In addition, we develop two single-stream network variants for DualRAN, i.e., SingleRANv1 and SingleRANv2. According to the experimental findings, DualRAN boosts the weighted F1 scores by 1.43% and 0.64% on the IEMOCAP and MELD datasets, respectively, in comparison to the strongest baseline. On two other datasets (i.e., EmoryNLP and DailyDialog), our method also attains competitive results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is a promising application and has received a great deal of attention from academics in recent years. Emotion Recognition in Conversation (ERC) is a subfield of emotion recognition with special scenarios. ERC offers plenty of potential application scenarios. Examples include (1) in disease diagnosis, to assist the doctor in diagnosing a disease by identifying the emotional status of the patient when talking with a psychologist; (2) in opinion mining, to enhance the service quality of the governmental department or organization by analyzing the public's emotional experience towards the policy or service; (3) in dialog generation, to raise the usability of the system by injecting emotions into the given model; and (4) in recommender systems, to infer the potential preferences of a target user by recognizing the user's emotional states while chatting with customer service.\n\nDistinct from general emotion recognition, ERC not only focuses on the utterance itself but also demands that the contexts of the utterance is sufficiently understood  (Song et al., 2023) . Figure  1  is a general flow that embodies the ERC task. The input of this task is a sequence of utterances in the conversation and the corresponding speakers, and its output is the emotions of these utterances. The emotion of the utterance to be predicted is affected by the utterance itself, the contexts, and the identities of speakers. With the rapid deployment and development of human-computer interaction, there is an urgent need to engage machines that can interact more naturally and humanely with humans. As a result, the importance of building conversational systems that can understand human emotion and intention has grown significantly  (Peng et al., 2020) . The development of ERC, which fits the above-mentioned usage scenarios for dialog systems, is urgent and has attracted increasing research in natural language understanding communities. The emotion of the utterance to be predicted is influenced by the utterance itself, the contexts (solid lines), and the identities of speakers (dashed lines).\n\nPlenty of efforts have been made in context-based modeling, and these ERC models fall into three main categories: recurrence-based approaches, Transformer-based approaches, and graph-based approaches. Recurrence-based methods are sensitive to the order of utterances and treat these utterances as a temporal sequence. COSMIC  (Ghosal et al., 2020)  was a conversational emotion recognition framework based on commonsense knowledge guidance, claiming to alleviate the problems of emotion shift and similar emotion. AGHMN (Wenxiang  Jiao and King, 2020)  was a conversational emotion recognition model based on Gated Recurrent Unit (GRU)  (Chung et al., 2014)  for building a memory bank to capture historical contexts and summarize memories to extract critical information. DialogueCRN  (Hu et al., 2021)  enhanced the extraction and integration of emotional cues and was a contextual reasoning network based on cognitive theory. CauAIN  (Zhao et al., 2022)  introduced commonsense knowledge as a cue for emotion cause detection in conversation, explicitly modeling intra-and inter-speaker dependencies. But these models are struggling to capture the global contextual information of the utterance. To solve this problem, some Transformer-based and graph-based methods have been proposed successively. Benefiting from the advantage of multi-head attention, Transformer-based methods allow for the consideration of long-range contextual information. HiTrans  (Li et al., 2020)  was a context-and speaker-aware model based on the hierarchical Transformer  (Vaswani et al., 2017) . Di-alogXL  (Shen et al., 2021)  was a pioneering work based on the pre-trained language network XLNet  (Yang et al., 2019) , which modified the network structure of XLNet to better model conversational emotion data. CoG-BART  (Li et al., 2022a)  was a conversational emotion recognition model that applied the encoder-decoder model BART  (Lewis et al., 2020)  as a backbone network. Graph-based methods, similar to Transformer-based methods, are capable of modeling contexts of the utterance from a global perspective. SKAIG-ERC  (Li et al., 2021b)  utilized a psychological-knowledgeaware interaction graph to model the historical context and commonsense knowledge of utterance. I-GCN  (Nie et al., 2022)  first represented conversations at different times using a graph structure and then simulated dynamic conversational processes using an incremental graph structure to capture both semantic correlation information of utterances and time-varying information of conversations.\n\nHowever, these methods either focus on the local sequence information of utterance or the global association information of utterance, ignoring the combination of local and global information. Although recurrence-based ERC methods can extract the temporal sequence information of dialog sequences, they tend to capture the nearest contextual information (i.e., focusing on the extraction of local information) and have difficulty in capturing long-range contextual information. Transformer-based and graph-based ERC methods can alleviate these problems, but they do not take into account the temporal information of utterance and have difficulty in adequately capturing the local information of utterance. In addition, some ERC models have an overly complex network structure, such as incorporating commonsense knowledge  (Ghosal et al., 2020; Zhao et al., 2022; Li et al., 2021b) , including multiple complex modules (Wenxiang  Jiao and King, 2020; Hu et al., 2021; Shen et al., 2021) , adopting an encoder-decoder structure  (Li et al., 2022a; Zhu et al., 2021) , etc., consuming numerous computational resources in return for a weak performance gain.\n\nTherefore, in this paper, we provide a simple and effective dual-stream network structure that explores combining recurrence-and attention-based models so that they complement each other. On the basis of Recurrent Neural Network (RNN) and Multi-head ATtention network (MAT), we construct local-aware and global-aware modules, respectively, and propose a Dual-stream Recurrence-Attention Network (DualRAN) for the ERC task. Furthermore, relying on the local-and global-aware modules in DualRAN, we devise two Single-stream Recurrence-Attention Networks (SingleRAN), which can be regarded as two variants of DualRAN. DualRAN differs significantly from most ERC methods in the following aspects: (1) DualRAN is a dualstream structure in which two sub-networks can encode information simultaneously; (2) the network structure of DualRAN is simple and directly combines RNN and MAT; and (3) DualRAN can mine both local and global contextual emotional cues, thus more comprehensively extracting contextual information.\n\nThe proposed DualRAN is more of a framework as its internal components can be flexibly changed, e.g., employing different types of RNNs, adopting skip connections or not, moving the position of the normalization layer, and even modifying the dual-stream to a single-stream structure. We conduct comparative experiments on four public datasets, and the results show that the proposed structure can lead to competitive performance improvements. With numerous ablation experiments, we explore the validity or impact of different modules on performance, e.g., revealing the effectiveness of local-and global-aware modules, the impact of different RNNs, the effect of speaker identity, the influence of skip connection, and so on. Not only that, we also compare two-stream and one-stream structures, as well as conduct sentiment classification (tri-classification) experiments. Our contribution is as follows: â€¢ To enhance the expressive capacity of RNN, we add two skip connections and a feed-forward network layer to the local-aware module inspired by Transformer architecture. In addition, we change the dual-stream structure in DualRAN to a single-stream one and maintain other components unchanged, providing two single-stream recurrence-attention networks, i.e., Sin-gleRANv1 and SingleRANv2.\n\nâ€¢ We conduct extensive experiments on four widely used benchmark datasets, including comparisons with baselines, comparisons with two SingleRANs, ablation studies for different components, and sentiment classification. The empirical results reveal that the proposed DualRAN can effectively model the ERC dataset and still surpass other models without using external commonsense knowledge.\n\nThe remaining sections primarily cover related works, methodology, experimental settings, experimental results & analysis, and conclusion & prospect. In Section 2, we introduce the existing related works. Section 3 corresponds to the methodology of this paper, i.e., we present in detail the DualRAN and its variants proposed in this paper. In Sections 4 and 5, we first describe the experimental setup of this work, and then report, discuss, and analyze the experimental results. The last section (i.e., Section 6) contains our conclusion and prospect of this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Emotion Recognition (ER) is a multidisciplinary research field that covers computer science, cognitive science, psychology, behavior, and sociology. For many years, ER has received broad attention from both academia and industry and has been explored in the domains of computer vision  (Ullah et al., 2022; Karnati et al., 2023) , natural language processing  (Hazarika et al., 2020) , automatic speech recognition  (Chen and Huang, 2021) , and signal processing  (Seal et al., 2020) . Emotion recognition data mainly includes two categories: (1) external emotion data, such as facial expression, speech, text; and (2) internal emotion data, such as electroencephalogram, heart rate, and blood pressure. Compared to external emotion data, internal emotion data (typically called physiological signals) requires specialized sensor devices to collect, and some signals, such as electroencephalogram, are challenging to acquire.\n\nIn this paper, we focus on the study of context-dependent ER, i.e., Emotion Recognition in Conversation (ERC). Significantly distinct from general context-independent-based ER, the ERC task not only needs to extract the emotion of the current sample but also needs to consider contextual modeling. Limited by the difficulty of dataset collection, current ERC tasks mainly adopt external emotion data as input. In addition, among these data, facial expression and speech usually contain a great deal of noise, while text belonging to artificial data contains cleaner emotional information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dialog Emotion Recognition",
      "text": "Emotion Recognition in Conversation (ERC) is a burning and promising task in recent years. Unlike general emotion recognition, ERC involves conversational context. Emotion Recognition in Conversation (ERC) has attracted extensive research attention owing to its wide range of applications. Depending on the structure of the network, there are mainly recurrence-based methods, Transformer-based methods, and graph-based methods.\n\nRecurrence-Based Methods: DialogueRNN  (Majumder et al., 2019)  was an ERC method based on multiple GRUs that incorporated the speaker information for each utterance to provide more reliable contextual information. COS-MIC  (Ghosal et al., 2020)  modeled different aspects of commonsense knowledge by considering mental states, events, actions, and cause-effect relations, and thus extracted complex interactions between personality, events, mental states, intents, and emotions. AGHMN (Wenxiang Jiao and King, 2020) mainly consisted of Hierarchical Memory Network (HMN) and Bi-directional GRU (BiGRU), where HMN was used to extract interaction information between historical utterances and BiGRU was used to summarize recent memory and long-term memory with the help of attention weights. DialogueCRN  (Hu et al., 2021)  constructed a multi-turn reasoning module to perform the intuitive retrieving process and conscious reasoning process, thus simulating the cognitive thinking of humans. BiERU  (Li et al., 2022b ) designed a generalized neural tensor block and a two-channel classifier namely bidirectional emotional recurrent unit to perform contextual feature extraction and sentiment classification. CauAIN  (Zhao et al., 2022)  modeled the context of utterance through the perspective of emotion cause detection and was known as a causal aware interaction network. CauAIN consisted of two main cause-aware interactions, i.e., causal cue retrieval and causal utterance retrieval, which were used to find the causal utterance of the emotion expressed by the target utterance. Recurrence-based methods typically treat a conversation as a sequence and can extract temporal order information in the conversation. However, they tend to focus on nearby contextual information and ignore distant one.\n\nTransformer-Based Methods: HiTrans  (Li et al., 2020 ) extracted the contextual information of the utterance with the help of low-level and high-level Transformers, and it extracted the speaker information with the aid of an auxiliary task called pairwise utterance speaker verification. TOD-KAT  (Zhu et al., 2021)  was a transformer encoder-decoder structure that combined topic representation and commonsense knowledge for conversational emotion recognition. Di-alogXL  (Shen et al., 2021)  was improved in two main ways, the first one was to improve the recurrence mechanism of XLNet from segment-level to utterance-level, and the second one was to replace the original vanilla attention by utilizing dialog-aware self-attention. EmotionFlow  (Song et al., 2022)  encoded the utterances of speakers by connecting contexts and auxiliary tasks, and it applied conditional random fields to capture sequential features at the emotion level. CoG-BART  (Li et al., 2022a ) first adopted the utterance-level Transformer to model the long-range contextual dependencies between utterances, then utilized the supervised contrast learning to solve the similar emotion problem, and finally introduced the auxiliary response generation task to enhance the capability of the model to capture contextual information. Despite the fact that Transformer-based methods can model the context from a global perspective, it is challenging to capture the chronological information in a conversation.\n\nGraph-Based Methods: KI-Net  (Xie et al., 2021)  consisted of two main components to enhance the semantic information of utterances, namely a self-matching module for internal utterance-knowledge interaction and a phraselevel sentiment polarity intensity prediction task. SKAIG-ERC  (Li et al., 2021b)  captured the contextually inferred behavioral action information and future contextually implied intention information leveraging the structure of graph, while the knowledge representation of edges was performed with the help of commonsense knowledge to enhance the emotional expression of the utterance. The approach claimed to model past human actions and future intentions while modeling the mental state of the speaker. S+PAGE  (Liang et al., 2022)  was a graph neural network-based emotion recognition model. The method modeled a conversation as a graph, adding relative location encoding and speaker encoding to the representation of edge weight and edge type, respectively, to better capture speaker-and location-aware conversational structure information. I-GCN  (Nie et al., 2022)  first extracted latent correlation information between utterances with an improved multi-head attention module, then focused on mining the correlation between speakers and utterances to provide guidance for utterance feature learning from another perspective. LR-GCN  (Ren et al., 2022)  first integrated the contextual information and speaker dependencies by utilizing the potential relationship graph network, and then it extracted potential associations between utterances with the multi-head attention mechanism to fully explore the potential relationships between utterances. Analogous to the Transformer-based methods, graph-based methods take into account the global contextual associations and neglect the temporal information due to the structure of the graph.\n\nAlthough the above approaches model the context to varying extents, the considered perspectives are not comprehensive enough. Recurrence-based ERC focuses on local modeling, making it extremely difficult to consider the context from a global perspective, while Transformer-and graph-based approaches share the problems of often neglecting local and temporal modeling. Additionally, most of the models are overly complex in structure, but the performance gains are not significant enough. Instead, our DualRAN combines the benefits of both recurrence-and Transformerbased methods in a simple way to capture local and global contextual information in the conversation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Machine Learning Methods",
      "text": "Our work mainly employs two machine learning methods: Recurrent Neural Network and Multi-Head Attention.\n\nOur goal is to combine these two networks in a simplistic way and achieve the best performance. We note that recently there have been new machine learning techniques such as self-distillation  (Xing et al., 2022)  and contrastive learning  (Xiao et al., 2023) . A number of tasks have achieved unprecedented success using these techniques. In future work, we will consider applying these technologies to our model to improve the performance of ERC.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Recurrent Neural Network",
      "text": "Recurrent Neural Networks (RNNs) are a class of neural network architectures for processing sequential data. The gating mechanism was introduced to solve the problem of gradient explosion or gradient disappearance in the traditional RNN  (Hochreiter and Schmidhuber, 1997) .  Hochreiter and Schmidhuber (1997)  proposed Long and Short Term Memory (LSTM) network to correctly deal with the problem of vanishing gradient. Gated Recurrent Unit (GRU) was proposed by  Chung et al. (2014)  in 2014 and is another classical RNN architecture. RNNs have been widely applied in the field of natural language processing due to their ability to process temporal data.  Bahdanau et al. (2015)  introduced an extension of encoder-decoder architecture to learn alignment and translation.  Johnson et al. (2017)  proposed an LSTM-based neural machine translation model to achieve translation between multiple languages in a simple solution. Recurrent neural networks such as LSTM and GRU can theoretically propagate both contextual and sequential information. There have been currently some ERC works modeling the context of discourse based on RNNs. Dia-logueRNN  (Majumder et al., 2019)  updates the status of the speaker and the global information of the conversation by employing multiple GRUs. DialogueCRN  (Hu et al., 2021)  was a cognitive theory-inspired approach that designed a cognitive inference module by exploiting LSTM to capture emotional cues contained in the context.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Head Attention Network",
      "text": "Multi-head ATtention network (MAT) was first proposed by  Vaswani et al. (2017) . It was powerful in feature dependency extraction, leading to remarkable achievements in many tasks. Contrary to RNNs which focus on local information, MAT can extract long-distance elemental dependencies. In recent years, MAT has been widely used in many research areas, such as automatic speech recognition  (Zhang et al., 2020) , natural language processing  (Vaswani et al., 2017) , and computer vision  (Dosovitskiy et al., 2021) . In addition, there exist some pre-trained models constructed with the help of MAT, such as BERT  (Devlin et al., 2019) , RoBERTa  (Liu et al., 2020) , and BART  (Lewis et al., 2020) . Assuming that the context and speaker information of the utterance is not considered, ERC can be regarded as a text classification task. In this case, each utterance can be finetuned with a pre-trained model to extract utterance-level feature. HiTrans  (Li et al., 2020)  adopted BERT to extract utterance-level features, while COSMIC  (Ghosal et al., 2020)   utterance. In this paper, we follow COSMIC's manner and extract utterance-level features by utilizing RoBERTa.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "We elaborate the proposed dual-stream network structure and its single-stream variants in this section. Our Dual-RAN is designed with the original intention of combining recurrence-based and attention-based methods to extract both local contextual information and global contextual information. As shown in Figure  2 , our DualRAN mainly consists of speaker-aware module, global-local-aware modeling, and emotion prediction. Among them, global-localaware modeling includes RNN-based local-aware module and MAT-based global-aware module.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Task Definition",
      "text": "There exists a conversation ğ‘ˆ which contains\n\n, where each utterance ğ‘¢ ğ‘– corresponds to a speaker ğ‘  ğ‘– . The utterances ğ‘¢ ğ‘– and ğ‘¢ ğ‘— may be spoken by the same speaker (i.e., ğ‘  ğ‘– = ğ‘  ğ‘— ) or different speakers (i.e., ğ‘  ğ‘– â‰  ğ‘  ğ‘— ). The task of ERC is to infer the emotion state ğ‘’ ğ‘– corresponding to the utterance ğ‘¢ ğ‘– based on the conversation content and speaker information. The emotion categories in distinct datasets may vary. For instance, in the IEMOCAP dataset, the emotion categories include happy, sad, neutral, angry, excited, and frustrated; while in the MELD dataset, the emotion categories include joy, anger, fear, disgust, sadness, surprise, and neutral. Table  1  shows the symbols and their definitions mentioned in this paper.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Speaker-Aware Module",
      "text": "Differences in the identity of speakers may have different effects on the semantics of utterances. To put it another The ğ‘–-th utterance in that conversation ğ‘ˆ ğ¶\n\nThe utterance-level feature matrix of ğ‘ˆ ğ‘†\n\nThe speaker sequence ğ‘  ğ‘–\n\nThe speaker corresponding to the ğ‘–-th utterance SPK\n\nThe speaker embedding matrix ğ¸\n\nThe set of emotion ğ‘’ ğ‘–\n\nThe emotion corresponding to the ğ‘–-th utterance î‰„\n\nThe input to the global-local-aware network ğ‘ ğ¿\n\nThe number of network layers for the local-aware module ğ‘ ğ»\n\nThe number of heads for the multi-head attention network ğ‘ ğº\n\nThe number of network layers for the global-aware module way, the current emotional state of a speaker is influenced not only by his or her own historical utterances but also by the historical utterances of other speakers. That is, there is emotional inertia and emotional contagion within and between speakers. In order to distinguish the influence of different speakers, we add the corresponding identity of the speaker to each utterance, thus implementing speaker-aware encoding. Specifically, we first encode word embedding for each speaker, then add the encoded speaker embedding to the utterance feature, and finally take the obtained new utterance feature as the input to the global-local-aware network. The above process can be formulated as follows:\n\nwhere ğ‘† denotes the speaker sequence corresponding to the utterance set ğ‘ˆ , while ğ™´ğ™¼ğ™± denotes the word embedding network; ğ¶ denotes the utterance-level feature matrix of ğ‘ˆ , which is extracted by the method of COSMIC  (Ghosal et al., 2020) .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Global-Local-Aware Modeling",
      "text": "The network structure of global-local-aware modeling is simple and effective, as the name suggests, it mainly consists of the local-aware module and global-aware module, which extract local contextual information and global contextual information, respectively. When performing backpropagation, the designed local-aware module and global-aware module are trained simultaneously to update the network parameters. In the following two parts, we describe their network structures respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Local-Aware Module",
      "text": "Numerous previous works have demonstrated that modeling the context of utterance is crucial for ERC. Therefore, we construct a local-aware module with a modified RNN. First, in order to extract temporal information of the utterance, we input the utterance feature to the vanilla RNN; then, inspired by the Transformer architecture, we adopt skip connection, i.e., the input and output of RNN are summed; finally, to enhance the expressiveness and stability of the network, we add a feedforward network layer consisting of two fully connected layers. The network structure of the local-aware module can be described by the following equation:\n\nwhere ğ‘‹ ğ‘™ indicates the ğ‘™-th layer feature matrix composed of all utterances, ğ‘™ = 0, 1, â€¦ , ğ‘ ğ¿ -1, and ğ‘‹ 0 = î‰„; ğ™½ğ™¾ğšğ™¼(â‹…) denotes the normalization function, and the layer normalization operation is used in our experiments. ğšğ™½ğ™½ â€² (â‹…) stands for the RNN layer with the addition of a fully connected layer, which can be formulated as,\n\nğšğ™½ğ™½(â‹…) denotes the bidirectional vanilla RNN such as LSTM and GRU; ğ™µğ™²(â‹…) means the fully connected layer, converting the feature dimension of the output to half of the input; ğ™³ğ™¿(â‹…) indicates the dropout operation. ğ™µğ™´ğ™´ğ™³(â‹…) is the feedforward network layer, which can be expressed as,\n\nğ›¼(â‹…) denotes the activation function, e.g., ReLU. In our experiments, we place ğ™½ğ™¾ğšğ™¼(â‹…) in front of ğšğ™½ğ™½ â€² (â‹…), i.e.,\n\n(5)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Global-Aware Module",
      "text": "The local-aware module possesses powerful temporal extraction capability, but it tends to capture local contextual information, while it is quite difficult to aggregate longdistance information. Therefore, we build a global-aware module with the help of Multi-head ATtention network (MAT) to capture global contextual information. Our globalaware module borrows the encoder structure of Transformer, and note that we do not incorporate position encoding because the local-aware module can capture temporal information in the conversation. The network structure of the globalaware module can be expressed as:\n\nwhere X ğ‘˜ denotes the ğ‘˜-th layer feature matrix composed of all utterances, ğ‘˜ = 0, 1, â€¦ , ğ‘ ğº -1, and X 0 = î‰„; ğ™°ğšƒğšƒ(â‹…) and ğ™µğ™´ğ™´ğ™³(â‹…) denote the attention network with multi-head setting and feedforward network layer, respectively. As with the local-aware module, ğ™½ğ™¾ğšğ™¼(â‹…) is placed ahead of ğ™°ğšƒğšƒ(â‹…), that is,\n\nAfter both local-aware modeling and global-aware modeling, we obtain the feature matrix ğ‘‹ ğ‘ ğ¿ with local information and X ğ‘ ğº with global information, respectively. Finally, to obtain the global-local-aware feature matrix, we concatenate ğ‘‹ ğ‘ ğ¿ and X ğ‘ ğº ,\n\nwhere ğ‘Š ğ‘”ğ‘™ is the trainable parameter, and X ğ‘”ğ‘™ denotes the feature matrix with global-local awareness.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Prediction",
      "text": "We make the obtained feature matrix X ğ‘”ğ‘™ as the input of the emotion prediction module. Specifically, the feature dimension of X ğ‘”ğ‘™ is converted to |ğ¸| (number of emotions) through a fully connected layer, and thus the predicted emotion ğ‘’ â€² ğ‘– (ğ‘’ â€² ğ‘– âˆˆ ğ¸) is obtained. The process can be formulated as follows:\n\nwhere x ğ‘”ğ‘™,ğ‘– âˆˆ X ğ‘”ğ‘™ , ğ‘Š ğ‘ ğ‘šğ‘ğ‘¥ is the learnable parameter, and ğ™°ğšğ™¶ğ™¼ğ™°ğš‡(â‹…) denotes the argmax function.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Training",
      "text": "To learn the network parameters of DualRAN, we define the loss function as follows:\n\nwhere ğ‘œ(ğ‘–) is the number of utterances of the ğ‘–-th dialog, and ğ‘‚ is the number of all dialogs in training set; ğ‘¦ â€² ğ‘–ğ‘— denotes the probability distribution of predicted emotion label of the ğ‘—-th utterance in the ğ‘–-th dialog, and ğ‘¦ ğ‘–ğ‘— denotes the ground truth label; ğœ‚ is the L2-regularizer weight, and ğ‘Š ğ‘ğ‘™ğ‘™ is the set of all learnable parameters.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Singlerans",
      "text": "We only change the network structure of global-localaware modeling in DualRAN to construct the Single-stream Recurrence-Attention Networks (SingleRANs). Like Dual-RAN, the global-local-aware modeling of SingleRAN contains two modules: local-aware module and global-aware module. The structure of the local-aware module and globalaware module itself remains unchanged, but they are combined in a single-stream and sequential manner, as shown in Figure  3 . According to the order of combining the localaware module and global-aware module, we divide Sin-gleRAN into two categories, i.e., SingleRANv1 and Sin-gleRANv2.\n\nIn SingleRANv1 (see Figure  3a ), the local-aware module is in the front and the global-aware module is in the back, that is:\n\nwhere ğ™»ğ™°ğ™¼(â‹…) and ğ™¶ğ™°ğ™¼(â‹…) denote the local-aware module and global-aware module, respectively. After the local-aware module of ğ‘ ğ¿ layers, we obtain the feature matrix ğ‘‹ ğ‘ ğ¿ .\n\nHere, ğ‘‹ ğ‘ ğ¿ is the output of the local-aware module and is also treated as the input to the global-aware module, i.e., X 0 = ğ‘‹ ğ‘ ğ¿ . We obtain the feature matrix X ğ‘”ğ‘™ after the global-aware module of ğ‘ ğº layers, which is treated as the input to the prediction module. In SingleRANv2 (see Figure  3b ), the global-aware module is in front and the local-aware module is in the back, that is:\n\nAfter the global-aware module of ğ‘ ğº layers, we obtain the feature matrix X ğ‘ ğº . Here, X ğ‘ ğº is the output of the globalaware module and is also treated as the input to the localaware module, i.e., ğ‘‹ 0 = X ğ‘ ğº . Similar to SingleRANv1, after being processed sequentially by the global-aware module of ğ‘ ğº layers and local-aware module of ğ‘ ğ¿ layers, We obtain the feature matrix X ğ‘”ğ‘™ , and it is treated as the input to the emotion prediction module.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "In order to evaluate the validity of our model, we conduct abundant experiments on four benchmark emotion datasets. These datasets include IEMOCAP 1    (Busso et al., 2008) , MELD 2    (Poria et al., 2019) , EmoryNLP 3    (Zahiri and Choi, 2018) , and DailyDialog 4    (Li et al., 2017) . The statistics of these datasets are reported in Table  2 , from which details of the data splitting can be observed.\n\nIEMOCAP is a dyadic conversational dataset containing 10 unique speakers, of which the first 8 speakers belong to the training set and the last two to the test set. The dataset consists of approximately 12 hours of multimodal dialog data, and we employ only text modality in this work. The dataset contains 152 conversations with a total of 7433 utterances, where these utterances are annotated with one of six emotions, namely happy, sad, neutral, anger, excited, and frustrated. MELD is a multi-party multimodal dialog dataset from the TV show \"Friends\", and we use only text modality in this work. The dataset contains 1433 dialogs with a total of 13708 utterances and has seven emotion categories: neutral, surprise, fear, sadness, joy, disgust, and anger. The utterances are labeled with sentiment categories, i.e., positive, negative, or neutral, in addition to being labeled as emotions. EmoryNLP collects multi-party conversations from the TV show \"Friends\". However, the selection of scenes and emotion labels differs from MELD. The dataset Figure  4  shows the percentage of each emotion in the four datasets. We can observe that all datasets exist the class-imbalanced problem, with DailyDialog being the most serious because of neutral accounting for 83.10%, which poses a high challenge to the ERC model. According to COSMIC 5  (Ghosal et al., 2020) , we use utterance-level text features which are fine-tuned adopting RoBERTa  (Liu et al., 2020)  to implement the ERC task.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Baselines And Evaluation Metrics",
      "text": "Baselines: The baseline used as comparisons in this work include COSMIC  (Ghosal et al., 2020) , HiTrans  (Li et al., 2020) , AGHMN (Wenxiang Jiao and King, 2020), DialogueCRN  (Hu et al., 2021) , SKAIG-ERC  (Li et al., 2021b) , DialogXL  (Shen et al., 2021) , I-GCN  (Nie et al., 2022) , LR-GCN  (Ren et al., 2022) , CauAIN  (Zhao et al., 2022) , CoG-BART  (Li et al., 2022a) , and GAR-Net  (Xu et al., 2022) . Of all these baselines, COSMIC, SKAIG-ERC, and CauAIN use commonsense knowledge, while the others do not.\n\nCOSMIC was a classic work that introduces external commonsense knowledge into emotion recognition in conversation, which leveraged multiple GRUs to integrate commonsense knowledge and extract complex interaction patterns. HiTrans extracted multidimensional contextual information with the use of two hierarchical Transformers and then captured speaker-aware information utilizing pairwise utterance speaker verification. AGHMN constructed 5 https://github.com/declare-lab/conv-emotion/tree/master/COSMIC the hierarchical memory network and attention gated recurrent units through multiple GRUs respectively to adequately model the context of the utterance. DialogueCRN employed multiple LSTMs to construct the perception phase module and cognition phase module respectively in order to simulate human cognitive behavior, thus enhancing the ability to extract and integrate emotional cues. SKAIG-ERC modeled the context of utterance adopting graph structure and commonsense knowledge, simulating the mental state of the speaker, and then it employed graph convolutional networks for information propagation, which enhanced the emotional representation of the utterance. DialogXL was a pioneering work that applied XLNet to emotion recognition in conversation, which focused on improvements to the recurrence and attention mechanisms to model conversational emotion data. I-GCN was a dialog emotion recognition method that modeled the dialog as a graph structure, and it extracted the semantic correlation information of utterances and temporal sequence information of the conversation with the help of graph convolutional networks. LR-GCN mainly consisted of two modules, latent relation exploration and information propagation, which adopted a multi-branch graph architecture in order to simultaneously capture the speaker information, contextual information of the utterance, and potential correlations between utterances. CauAIN was a cause-aware interaction based model that explicitly modeled speaker dependencies and contextual dependencies of utterance in combination with commonsense knowledge. CoG-BART was an approach that employed both contrast learning and generative models, which can capture the information of long-distance utterances while alleviating the problem of similar emotion. GAR-Net considered both word-level and utterance-level contexts, which constructed an utterancelevel graph reasoning network by treating the entire conversation as a fully connected graph.\n\nEvaluation Metrics: Our evaluation metrics include accuracy (%), weighted F1 (%), micro F1 (%), and macro F1 (%) scores. For the IEMOCAP and MELD datasets, we use accuracy and weighted F1 scores as evaluation metrics; for the EmoryNLP dataset, micro F1 and weighted F1 scores",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training Details",
      "text": "Our experiments are conducted on a single NVIDIA GeForce RTX 3090 and trained in an end-to-end fashion. The deep learning framework which we use is Pytorch with version 2.0.0, and the operating system is Ubuntu 20.04. We choose AdamW  (Loshchilov and Hutter, 2019)  as the optimizer, the L2 regularization factor is 3e-4, and the maximum epochs are set to 100. Other hyperparameter settings for different datasets are displayed in Table  3 . In our experiments, we utilize LSTM  (Hochreiter and Schmidhuber, 1997)  as recurrent neural network for the local-aware module.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison With Baselines",
      "text": "We report the results of comparative experiments on four emotion datasets in Table  4 , which allow the following conclusions to be drawn:\n\nâ€¢ Our proposed DualRAN achieves remarkable performance on all four emotion datasets, with the most significant improvements in scores on the IEMOCAP dataset. It indicates that DualRAN can adequately model the context and thus effectively extract both global dependency information and local dependency information.\n\nâ€¢ On the IEMOCAP dataset, DualRAN attains 69.62% accuracy and 69.73% weighted F1 score. Compared with DialogueCRN, the accuracy of our method is improved by 3.57%; compared with CoG-BART, the proposed DualRAN has a 3.55% improvement in the weight F1 score.\n\nâ€¢ On the MELD dataset, the weight F1 score of our Du-alRAN is 0.78% higher than that of CauAIN, reaching 66.24%. DualRAN achieves an accuracy of 67.70%, which is a 6.97% improvement relative to that of DialogueCRN. Without using external knowledge, the weighted F1 score of our method is still 1.03% higher than that of COSMIC.\n\nâ€¢ On the EmoryNLP dataset, the micro F1 score of the proposed DualRAN is 2.24% higher than that of CoG-BART, achieving 44.82%. Compared to DialogXL's weighted F1 score, the improvement of our model is 4.49%, achieving 39.22%.\n\nâ€¢ On the DailyDialog dataset, DualRAN obtains a micro F1 score of 60.07%, which is 1.86% higher than that of CauAIN. The macro F1 score of our model is 0.94% higher than that of SKAIG-ERC, achieving 52.89%. However, DualRAN's macro F1 score is 0.96% lower than CauAIN's and fails to achieve the best performance.\n\nOverall, DualRAN shows the most dramatic improvement on the IEMOCAP dataset compared to the results on the other datasets. By examining the dataset, it is found that  the number of utterances in a conversation is much higher in the IEMOCAP dataset than in any other dataset. In this case, IEMOCAP relies more on contextual modeling than other datasets. Therefore, our DualRAN shows a definite advantage over other baselines on the IEMOCAP dataset with the help of the global-local-aware network.\n\nWe also record the F1 scores of DualRAN for each emotion on the IEMOCAP and MELD datasets, as shown in Table  5 . It is evident that the proposed DualRAN achieves the best or second best F1 scores for each emotion. For a more intuitive representation, we draw bar charts based on Table  5  to show the comparisons between DualRAN with baselines for each emotion, as shown in Figure  5 . On the IEMOCAP dataset, DualRAN achieves the best results on happy, neutral, and frustrated, and the second-best F1 scores on sad, angry, and excited, ultimately achieving the best weighted F1 scores. Similar results are obtained on the MELD dataset. Notably, our model achieves 31.78% F1 scores on disgust, an extremely rare emotion category, which is far higher than the other baselines. The above results demonstrate that the proposed DualRAN provides powerful contextual modeling capabilities. In particular, on the MELD dataset, disgust can be identified better than other models with the aid of contextual modeling, and the classimbalanced problem is evaded to some extent.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Comparison Of Singleran With Dualran",
      "text": "In this subsection, we test the performance of two singlestream variants of DualRAN, namely SingleRANv1 and SingleRANv1, on four datasets. As shown in Table  6 , on the IEMOCAP dataset, both SingleRANv1 and SingleRANv2 have an accuracy of 68.27%, which is lower than the score of DualRAN 1.35%. On the MELD dataset, the weighted F1 score for SingleRANv1 is 0.22% lower than that of DualRAN, while the weighted F1 score for SingleRANv2 is 0.92% lower than that of DualRAN. Similar results appear on the EmoryNLP and DailyDialog datasets. The micro F1 score for SingleRANv1 decreases by 2.04% relative to that for DualRAN on the EmoryNLP dataset, while SingleR-ANv2's micro F1 score of 43.6% is 1.22% lower than Du-alRAN's. On the DailyDialog dataset, the micro F1 scores for SingleRANv1 and SingleRANv2 declined by 0.5% and 0.92% relative to those for DualRAN, respectively. Overall, the performance of two variants, i.e., SingleRANv1 and SingleRANv1, slightly lag behind those of DualRAN.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Validity Of Local-And Global-Aware Modules",
      "text": "In this subsection, we remove the local-aware and globalaware modules separately to explore their validity on the performance of DualRAN. From Table  7 , we can conclude that either removing the local-aware modules or removing the global-aware modules leads to the performance degradation of our model. On the IEMOCAP dataset, the weight F1 score of the model decreases from 69.73% to 64.22% when we remove the local-aware module, while that of the model drops to 65.06% when the global-aware module is removed. The magnitude of reduction suggests that the IEMOCAP dataset is more dependent on local-aware modeling compared to global-aware modeling, and similar patterns are observed for the other datasets (i.e., MELD and EmoryNLP) except for the DailyDialog dataset. Overall, the impact on the IEMOCAP dataset is more significant than that on the others when removing any module. This is due to the fact that a conversation in the IEMOCAP dataset contains more utterances and relies more on contextual modeling than the other datasets. To investigate the effect of the number of network layers for global-local-aware modeling on the performance of Du-alRAN, we conduct ablation studies related to the number of network layers in this subsection. We fix the number of network layers for the global-aware module, while adjusting those for the local-aware module and recording the experimental results. As shown in Figure  6 , the blue lines depict the effect of the number of network layers for the local-aware module on the accuracy and weight F1 scores. Note that these results are derived from experiments conducted on the IEMOCAP dataset. It can be found that as the number of network layers increases, both the accuracy score and the weight F1 score fluctuate around the optimal performance, i.e., roughly showing an increasing trend followed by a decreasing trend. Similarly, fixing the number of network layers for the local-aware module, we adjust the number of network layers for the global-aware module to explore its impact on the performance of our DualRAN. As shown by green lines in Figure  6 , the performance of the proposed model tends to increase and then decrease as the number of network layers for the global-aware module increases. We test the effect of different RNNs on DualRAN in this subsection. Figure  7  shows the experimental results using improved LSTM and improved GRU as the localaware module, respectively. We can see that the accuracy and weight F1 scores by adopting improved LSTM are higher relative to those by adopting improved GRU in all benchmark datasets. On the whole, better results can be obtained with improved LSTM, which indicates that improved LSTM can perform better local-aware modeling relative to improved GRU. Figure  8  shows the comparison between employing improved LSTM and vanilla LSTM. We can reveal that DualRAN utilizing improved LSTM achieves better performance relative to vanilla LSTM on the four datasets. This situation suggests that the inclusion of skip connections and feedforward layers in local-aware module is beneficial in enhancing the expressiveness of the model. To explore the effect of speaker identity on the proposed DualRAN, we conduct the ablation experiments on speaker information, and the results are displayed in Figure  9 . On the IEMOCAP dataset, the accuracy of our model decreases from 69.62% to 66.42% when speaker embedding is not employed, a decrease of 3.20%. On the MELD dataset, the weight F1 score drops to 65.35% when speaker information is removed. Similar performance decreases are found on the EmoryNLP and DailyDialog datasets. These phenomena suggest that speaker identity can effectively model emotional inertia and emotional contagion within and between speakers, which is beneficial to improve the performance of the model.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Effect Of Number Of Network Layers",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Impact Of Distinct Rnns",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Effect Of Speaker Identity",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Impact Of Skip Connection",
      "text": "Several studies  (He et al., 2016; Li et al., 2018 Li et al., , 2021a) )  have demonstrated that skip connection can improve the expressiveness and stability of the model, so we add skip connections to both the local-aware module and globalaware module. To demonstrate the effectiveness of skip connection, we conduct the ablation studies on skip connection in this subsection, and the results are depicted in Table  8 . As we can observe, the performance of the proposed model appears to degrade on all datasets whether skip connections are removed from the local-aware module or global-aware module. As expected, the degradation of our model is even more pronounced when we remove skip connections of both modules at the same time. These phenomena suggest that introducing skip connections in the global-local-aware network can effectively promote the performance of the model.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Results Of Sentiment Classification",
      "text": "In this subsection, we replace emotion with sentiment as the classified target. Accordingly, we transform DualRAN into a tri-classification (i.e., neutral, positive, and negative) model. Note that since the IEMOCAP, EmoryNLP, and DailyDialog datasets contain no sentiment labels, we require to merge the original emotion labels. The specific scheme of merging is shown in Table  9 .\n\nAs shown in Table  10 , the results of DualRAN are similar to COSMIC after the coarsening of emotion into sentiment, and the performance is improved on all datasets. For instance, the accuracy of DualRAN on the IEMOCAP dataset improves from 69.62% to 82.38%, an increase of 12.76%. Although, relative to COSMIC's results of sentiment classification, the weight F1 scores of our DualRAN on the MELD and EmoryNLP datasets are improved, the enhancements are limited. This situation is mainly due to the fact that most of the models can be easily classified after the fine-grained emotions is coarsened into sentiments. To put it in a nutshell, the dataset becomes relatively simple, so it is not necessary to model both local and global contexts to achieve better results.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Case Study",
      "text": "We extract raw utterances from the MELD dataset for the case study. As shown in Figure  10 , several baseline models (e.g., LR-GCN) tend to classify the utterances with true labels of disgust and fear as neutral. This is due to the problem of class imbalance in the MELD dataset, where disgust and fear belong to the minority class, while neutral belongs to the majority class. The baseline cannot adequately model the context and tends to predict the emotion",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Utterance",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Baseline Dualran",
      "text": "Ross: Four percent. Okay. I tip more than that when there's a bug in my food.\n\n[disgust]",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Rachel",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Limitations",
      "text": "As shown in Figure  11 , we depict the performance of DualRAN on four public emotion datasets with the confusion matrices. It can be seen that the proposed DualRAN achieves superior result on the IEMOCAP dataset. Similar to some previous models, DualRAN works less well on the DailyDialog dataset, as shown in Figure  11d . One main factor is that the DailyDialog dataset suffers from an extreme class imbalance, i.e., the utterances annotated as neutral account for a very large proportion of the dataset, causing DualRAN to be biased toward neutral during training. It is evident from Figure  11d  that most utterances tend to be predicted as neutral. It is assumed that the performance on the DailyDialog dataset will be improved if neutral is removed. The problem of class imbalance is also present in the MELD dataset. As shown in Figure  11b , the utterances with true labels of fear, sadness, and disgust incline to be classified as neutral. After examining the MELD dataset, it is found that these three emotions belong to the minority class. In addition, DualRAN suffers from the problem of similar emotion, i.e., some utterances are easily misidentified as another similar emotions. For example, as shown in Figure  11a , the utterances whose true labels are happy are easily predicted as excited, and the utterances with true  emotions of angry are easily classified as frustrated on the IEMOCAP dataset.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion And Prospect",
      "text": "In this paper, we propose a Dual-stream Recurrence-Attention Network (DualRAN) with global-local-aware capability to adequately capture both local and global contextual information of the utterance. The proposed Dual-RAN is a simple and effective dual-stream network consisting of local-and global-aware modules and focuses on the combination of recurrence-based and attention-based methods. In order to construct the local-aware module, we improve the structure of vanilla RNN referring to Transformer, that is adding the skip connection and feedforward network layer to enhance the expressiveness of the network. To explore the importance of speaker information for the ERC task, we encode the speaker identity and then add it to the corresponding utterance feature. Additionally, based on the local-and global-aware modules of DualRAN, we construct two single-stream recurrence-attention networks, i.e., SingleRANv1 and SingleRANv2. We conduct extensive comparison experiments and the results demonstrate that our proposed model outshines all baselines by an absolute margin. Meanwhile, we perform ablation experiments for each component and the empirical results prove the validity of these components.\n\nIn future research, we will work on addressing the class imbalance problem that is widespread in benchmark emotion datasets. Contrastive learning has gained great achievements in the field of computer vision in recent years, and extending it to the ERC task is a feasible solution to the imbalance problem. Another viable option is to generate some minority class samples with the help of the large language model in order to realize class balancing as much as possible. For the similar emotion problem, pushing away the distance of different class samples with the aid of contrastive learning is a promising scheme. With the potential of multimodal",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: is a general flow that embodies the",
      "page": 1
    },
    {
      "caption": "Figure 1: An example of emotion recognition in conversation.",
      "page": 1
    },
    {
      "caption": "Figure 2: The overall architecture of our proposed DualRAN. Here, ğ™½ğ™¾ğšğ™¼, ğ™µğ™², and ğ™³ğ™¿denote the normalization, fully connected,",
      "page": 5
    },
    {
      "caption": "Figure 2: , our DualRAN mainly",
      "page": 5
    },
    {
      "caption": "Figure 3: The network structure of globalâ€“local-aware modeling in SingleRAN.",
      "page": 7
    },
    {
      "caption": "Figure 3: According to the order of combining the local-",
      "page": 7
    },
    {
      "caption": "Figure 3: a), the local-aware module",
      "page": 7
    },
    {
      "caption": "Figure 3: b), the global-aware mod-",
      "page": 7
    },
    {
      "caption": "Figure 4: Percentage of each emotion in the dataset. Here, hap and neu denote the abbreviations of happy and neutral, respectively,",
      "page": 8
    },
    {
      "caption": "Figure 4: shows the percentage of each emotion in the",
      "page": 8
    },
    {
      "caption": "Figure 5: Comparison of DualRAN with baselines in each emotion on the IEMOCAP and MELD datasets.",
      "page": 10
    },
    {
      "caption": "Figure 6: The impact of the number of network layers on",
      "page": 11
    },
    {
      "caption": "Figure 6: , the blue lines depict",
      "page": 11
    },
    {
      "caption": "Figure 6: , the performance of the proposed",
      "page": 11
    },
    {
      "caption": "Figure 7: The impact of different RNNs on the performance",
      "page": 11
    },
    {
      "caption": "Figure 7: shows the experimental results",
      "page": 11
    },
    {
      "caption": "Figure 8: Comparison between the use of improved LSTM",
      "page": 12
    },
    {
      "caption": "Figure 8: shows the comparison between",
      "page": 12
    },
    {
      "caption": "Figure 9: The impact of speaker information on the perfor-",
      "page": 12
    },
    {
      "caption": "Figure 10: , several baseline",
      "page": 12
    },
    {
      "caption": "Figure 10: Case study from the MELD dataset. [emotion] at",
      "page": 13
    },
    {
      "caption": "Figure 10: , our proposed DualRAN, in",
      "page": 13
    },
    {
      "caption": "Figure 11: , we depict the performance of",
      "page": 13
    },
    {
      "caption": "Figure 11: d. One main",
      "page": 13
    },
    {
      "caption": "Figure 11: d that most utterances tend to be",
      "page": 13
    },
    {
      "caption": "Figure 11: b, the utterances",
      "page": 13
    },
    {
      "caption": "Figure 11: a, the utterances whose true labels are happy",
      "page": 13
    },
    {
      "caption": "Figure 11: Confusion matrices of the proposed method on the IEMOCAP, MELD, EmoryNLP, and DailyDialog datasets.",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Partialhyperparametersettingsfordistinctdatasets.Here,ğ‘ andğ‘ indicatethenumberoflayersforthelocal-awaremodule",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "accuracy\nweighted-F1",
          "MELD": "accuracy\nweighted-F1",
          "EmoryNLP": "micro-F1\nweighted-F1",
          "DailyDialog": "micro-F1\nmacro-F1"
        },
        {
          "Methods": "COSMIC\nHiTrans\nAGHMN\nDialogueCRN\nSKAIG-ERC\nDialogXL\nI-GCN\nLR-GCN\nCauAIN\nCoG-BART\nGAR-Net",
          "IEMOCAP": "-\n65.28\n-\n64.50\n63.50\n63.50\n66.05\n66.20\n-\n66.96\n-\n65.94\n65.50\n65.40\n68.50\n68.30\n-\n67.61\n-\n66.18\n-\n67.41",
          "MELD": "-\n65.21\n-\n61.94\n60.30\n58.10\n60.73\n58.39\n-\n65.18\n-\n62.41\n-\n60.80\n-\n65.60\n-\n65.46\n-\n64.81\n-\n62.11",
          "EmoryNLP": "-\n38.11\n-\n36.75\n-\n-\n-\n-\n-\n38.88\n-\n34.73\n-\n-\n-\n-\n-\n-\n42.58\n39.04\n-\n-",
          "DailyDialog": "58.48\n51.05\n-\n-\n-\n-\n-\n-\n59.75\n51.95\n54.93\n-\n-\n-\n-\n-\n58.21\n53.85\n56.29\n-\n56.97\n45.81"
        },
        {
          "Methods": "DualRAN",
          "IEMOCAP": "69.62â€ \n69.73â€ \n69.18Â±0.37â€¡\n69.17Â±0.41â€¡",
          "MELD": "67.70â€ \n66.24â€ \n67.32Â±0.31â€¡\n66.07Â±0.16â€¡",
          "EmoryNLP": "44.82â€ \n39.22â€ \n44.23Â±0.51â€¡\n39.18Â±0.26â€¡",
          "DailyDialog": "52.89â€ \n60.07â€ \n59.77Â±0.36â€¡\n52.44Â±0.37â€¡"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: Performance comparison of DualRAN with baselines in each emotion. Here, w-F1 denotes the weight F1 score. Results for all",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "hap\nsad\nneu\nang\nexc\nfru",
          "MELD": "neu\nsur\nfea\nsad\njoy\ndis\nang"
        },
        {
          "Methods": "",
          "IEMOCAP": "F1\nF1\nF1\nF1\nF1\nF1",
          "MELD": "F1\nF1\nF1\nF1\nF1\nF1\nF1"
        },
        {
          "Methods": "AGHMN\nI-GCN\nLR-GCN",
          "IEMOCAP": "52.1\n73.3\n58.4\n61.9\n69.7\n62.3\n83.8\n74.3\n50.0\n59.3\n64.6\n59.0\n69.0\n55.5\n79.1\n63.8\n74.0\n68.9",
          "MELD": "76.4\n49.7\n11.5\n27.0\n52.4\n14.0\n39.4\n38.5\n78.0\n51.6\n8.0\n54.7\n11.8\n43.5\n80.8\n65.8\n57.1\n0.0\n36.9\n11.0\n54.7"
        },
        {
          "Methods": "DualRAN 57.81",
          "IEMOCAP": "65.61\n69.94\n81.17\n65.23\n73.68",
          "MELD": "58.66\n14.71\n31.78\n54.95\n80.09\n38.39\n64.59"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 6: Performance comparison of SingleRAN with DualRAN.",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "accuracy\nweighted-F1",
          "MELD": "accuracy\nweighted-F1",
          "EmoryNLP": "micro-F1\nweighted-F1",
          "DailyDialog": "micro-F1\nmacro-F1"
        },
        {
          "Methods": "DualRAN",
          "IEMOCAP": "69.62\n69.73",
          "MELD": "67.70\n66.24",
          "EmoryNLP": "44.82\n39.22",
          "DailyDialog": "60.07\n52.89"
        },
        {
          "Methods": "SingleRANv1\nSingleRANv2",
          "IEMOCAP": "68.27\n68.41\n68.27\n68.28",
          "MELD": "67.55\n66.02\n66.36\n65.32",
          "EmoryNLP": "42.78\n38.60\n43.60\n39.22",
          "DailyDialog": "59.57\n52.14\n59.15\n51.90"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 6: Performance comparison of SingleRAN with DualRAN.",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "accuracy\nweighted-F1",
          "MELD": "accuracy\nweighted-F1",
          "EmoryNLP": "micro-F1\nweighted-F1",
          "DailyDialog": "micro-F1\nmacro-F1"
        },
        {
          "Methods": "DualRAN",
          "IEMOCAP": "69.62\n69.73",
          "MELD": "67.70\n66.24",
          "EmoryNLP": "44.82\n39.22",
          "DailyDialog": "60.07\n52.89"
        },
        {
          "Methods": "-w/o L\n-w/o G",
          "IEMOCAP": "64.14\n64.22\n65.06\n65.06",
          "MELD": "66.63\n65.25\n66.74\n65.74",
          "EmoryNLP": "42.28\n37.12\n42.99\n38.83",
          "DailyDialog": "58.71\n51.20\n58.26\n51.11"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 8: The impact of skip connection on the proposed DualRAN. Here, -w/o SC-L, -w/o SC-G, and -w/o SC-LG indicate the deletion",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "accuracy\nweighted-F1",
          "MELD": "accuracy\nweighted-F1",
          "EmoryNLP": "micro-F1\nweighted-F1",
          "DailyDialog": "micro-F1\nmacro-F1"
        },
        {
          "Methods": "DualRAN",
          "IEMOCAP": "69.62\n69.73",
          "MELD": "67.70\n66.24",
          "EmoryNLP": "44.82\n39.22",
          "DailyDialog": "60.07\n52.89"
        },
        {
          "Methods": "-w/o SC-L\n-w/o SC-G\n-w/o SC-LG",
          "IEMOCAP": "68.33\n68.45\n66.48\n66.48\n64.63\n64.82",
          "MELD": "66.59\n64.99\n67.09\n65.84\n64.64\n63.13",
          "EmoryNLP": "43.60\n37.23\n43.50\n38.58\n35.16\n33.32",
          "DailyDialog": "58.98\n49.52\n59.87\n52.28\n56.83\n49.70"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 8: The impact of skip connection on the proposed DualRAN. Here, -w/o SC-L, -w/o SC-G, and -w/o SC-LG indicate the deletion",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "accuracy\nweighted-F1",
          "MELD": "accuracy\nweighted-F1",
          "EmoryNLP": "micro-F1\nweighted-F1",
          "DailyDialog": "micro-F1\nmacro-F1"
        },
        {
          "Methods": "COSMIC-emo\nDualRAN-emo",
          "IEMOCAP": "-\n65.28\n69.62\n69.73",
          "MELD": "-\n65.21\n67.70\n66.24",
          "EmoryNLP": "-\n38.11\n44.82\n39.22",
          "DailyDialog": "58.48\n51.05\n60.07\n52.89"
        },
        {
          "Methods": "COSMIC-sen\nDualRAN-sen",
          "IEMOCAP": "-\n-\n82.38\n82.55",
          "MELD": "-\n73.20\n73.22\n73.21",
          "EmoryNLP": "-\n56.51\n57.42\n57.53",
          "DailyDialog": "-\n-\n60.66\n66.08"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Proceedings of the 3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "3",
      "title": "A novel dual attention-based blstm with hybrid features in speech emotion recognition",
      "authors": [
        "Q Chen",
        "G Huang"
      ],
      "year": "2021",
      "venue": "Engineering Applications of Artificial Intelligence",
      "doi": "10.1016/j.engappai.2021.104277"
    },
    {
      "citation_id": "4",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "NIPS 2014 Workshop on Deep Learning"
    },
    {
      "citation_id": "5",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "6",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "Proceedings of International Conference on Learning Representations"
    },
    {
      "citation_id": "7",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "8",
      "title": "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia",
      "doi": "10.1145/3394171.3413678"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/cvpr.2016.90"
    },
    {
      "citation_id": "10",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "11",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "12",
      "title": "Google's multilingual neural machine translation system: Enabling zeroshot translation",
      "authors": [
        "M Johnson",
        "M Schuster",
        "Q Le",
        "M Krikun",
        "Y Wu",
        "Z Chen",
        "N Thorat",
        "F ViÃ©gas",
        "M Wattenberg",
        "G Corrado",
        "M Hughes",
        "J Dean"
      ],
      "year": "2017",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": "10.1162/tacl_a_00065"
    },
    {
      "citation_id": "13",
      "title": "Understanding deep learning techniques for recognition of human emotions using facial expressions: A comprehensive survey",
      "authors": [
        "M Karnati",
        "A Seal",
        "D Bhattacharjee",
        "A Yazidi",
        "O Krejcar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "doi": "10.1109/TIM.2023.3243661"
    },
    {
      "citation_id": "14",
      "title": "BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.703"
    },
    {
      "citation_id": "15",
      "title": "2021a. DeepGCNs: Making GCNs go as deep as CNNs",
      "authors": [
        "G Li",
        "M Mueller",
        "G Qian",
        "I Perez",
        "A Abualshour",
        "A Thabet",
        "B Ghanem"
      ],
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/tpami.2021.3074057"
    },
    {
      "citation_id": "16",
      "title": "HiTrans: A transformerbased context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "2021b. Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": "10.18653/v1/2021.findings-emnlp.104"
    },
    {
      "citation_id": "18",
      "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
      "authors": [
        "Q Li",
        "Z Han",
        "X Wu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v32i1.11604"
    },
    {
      "citation_id": "19",
      "title": "2022a. Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "W Li",
        "W Shao",
        "S Ji",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2021.09.057"
    },
    {
      "citation_id": "21",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "22",
      "title": "S+PAGE: A speaker and position-aware graph neural network model for emotion recognition in conversation",
      "authors": [
        "C Liang",
        "J Xu",
        "Y Lin",
        "C Yang",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "24",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "25",
      "title": "DialogueRNN: An attentive RNN for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, Association for the Advancement of Artificial Intelligence (AAAI)",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "26",
      "title": "2022. I-gcn: Incremental graph convolution network for conversation emotion detection",
      "authors": [
        "W Nie",
        "R Chang",
        "M Ren",
        "Y Su",
        "A Liu"
      ],
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2021.3118881"
    },
    {
      "citation_id": "27",
      "title": "Human-machine dialogue modelling with the fusion of word-and sentence-level emotions",
      "authors": [
        "D Peng",
        "M Zhou",
        "C Liu",
        "J Ai"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2019.105319"
    },
    {
      "citation_id": "28",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/p19-1050"
    },
    {
      "citation_id": "29",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2021.3117062"
    },
    {
      "citation_id": "30",
      "title": "An eeg database and its initial benchmark emotion classification performance",
      "authors": [
        "A Seal",
        "P Reddy",
        "P Chaithanya",
        "A Meghana",
        "K Jahnavi",
        "O Krejcar",
        "R Hudak"
      ],
      "year": "2020",
      "venue": "Computational and Mathematical Methods in Medicine"
    },
    {
      "citation_id": "31",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v35i15.17625"
    },
    {
      "citation_id": "32",
      "title": "SUNET: Speakerutterance interaction graph neural network for emotion recognition in conversations",
      "authors": [
        "R Song",
        "F Giunchiglia",
        "L Shi",
        "Q Shen",
        "H Xu"
      ],
      "year": "2023",
      "venue": "Engineering Applications of Artificial Intelligence",
      "doi": "10.1016/j.engappai.2023.106315"
    },
    {
      "citation_id": "33",
      "title": "Emotionflow: Capture the dialogue level emotion transitions",
      "authors": [
        "X Song",
        "L Zang",
        "R Zhang",
        "S Hu",
        "L Huang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP43922.2022.9746464"
    },
    {
      "citation_id": "34",
      "title": "Improved deep cnn-based two stream super resolution and hybrid deep model-based facial emotion recognition",
      "authors": [
        "Z Ullah",
        "L Qi",
        "A Hasan",
        "M Asim"
      ],
      "year": "2022",
      "venue": "Engineering Applications of Artificial Intelligence",
      "doi": "10.1016/j.engappai.2022.105486"
    },
    {
      "citation_id": "35",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "36",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "M King"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Deep contrastive representation learning with self-distillation",
      "authors": [
        "Z Xiao",
        "H Xing",
        "B Zhao",
        "R Qu",
        "S Luo",
        "P Dai",
        "K Li",
        "Z Zhu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "doi": "10.1109/TETCI.2023.3304948"
    },
    {
      "citation_id": "38",
      "title": "Knowledge-interactive network with sentiment polarity intensity-aware multi-task learning for emotion recognition in conversations",
      "authors": [
        "Y Xie",
        "K Yang",
        "C Sun",
        "B Liu",
        "Z Ji"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics",
      "doi": "10.18653/v1/2021.findings-emnlp.245"
    },
    {
      "citation_id": "39",
      "title": "SelfMatch: Robust semisupervised time-series classification with self-distillation",
      "authors": [
        "H Xing",
        "Z Xiao",
        "D Zhan",
        "S Luo",
        "P Dai",
        "K Li"
      ],
      "year": "2022",
      "venue": "International Journal of Intelligent Systems",
      "doi": "10.1002/int.22957"
    },
    {
      "citation_id": "40",
      "title": "GAR-Net: A graph attention reasoning network for conversation understanding",
      "authors": [
        "H Xu",
        "Z Yuan",
        "K Zhao",
        "Y Xu",
        "J Zou",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "41",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "42",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Transformer transducer: A streamable speech recognition model with transformer encoders and RNN-t loss",
      "authors": [
        "Q Zhang",
        "H Lu",
        "H Sak",
        "A Tripathi",
        "E Mcdermott",
        "S Koo",
        "S Kumar"
      ],
      "year": "2020",
      "venue": "Proceedings of 2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/icassp40776.2020.9053896"
    },
    {
      "citation_id": "44",
      "title": "Cauain: Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, International Joint Conferences on Artificial Intelligence Organization",
      "doi": "10.24963/ijcai.2022/628"
    },
    {
      "citation_id": "45",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "L Zhu",
        "G Pergola",
        "L Gui",
        "D Zhou",
        "Y He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.125"
    }
  ]
}