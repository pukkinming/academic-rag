{
  "paper_id": "2402.13018v4",
  "title": "Emo-Superb: An In-Depth Look At Speech Emotion Recognition",
  "published": "2024-02-20T14:00:53Z",
  "authors": [
    "Haibin Wu",
    "Huang-Cheng Chou",
    "Kai-Wei Chang",
    "Lucas Goncalves",
    "Jiawei Du",
    "Jyh-Shing Roger Jang",
    "Chi-Chun Lee",
    "Hung-Yi Lee"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is a pivotal technology for human-computer interaction systems. However, 80.77% of SER papers yield results that cannot be reproduced  (Antoniou et al., 2023) . We develop EMO-SUPERB, shorted for EMOtion Speech Universal PERformance Benchmark, aims at enhancing open-source initiatives for SER. EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art speech selfsupervised learning models (SSLMs) for exhaustive evaluation across six open-source SER datasets. EMO-SUPERB streamlines result sharing via an online leaderboard, fostering collaboration within a community-driven benchmark and thereby enhancing the development of SER. On average, 2.58% annotations are annotated using natural language. SER relies on classification models and is unable to process natural languages, leading to the discarding of these valuable annotations. We prompt Chat-GPT to mimic annotators, comprehend natural language annotations, and subsequently relabel the data. By utilizing labels generated by ChatGPT, we consistently achieve an average relative gain of 3.08% across all settings. We make all resources open-source to facilitate future developments in SER. The source code and complete analysis are on the project website 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) aims to discern emotional cues from speech inputs, representing a pivotal technology for human-computer interaction systems. Recent years have witnessed significant advancements in SER. However, there are some unsolved problems in the SER domain: Issue 1: Devoted annotators prefer using natural language rather than traditional emotion labels * equal first contribution, † equal second contribution, ‡ equal corresponding author, order is random 1 EMO-SUPERB Website when annotating data, resulting in typed descriptions (e.g., \"Slightly Angry, calm\" to notify the intensity of emotion). While these descriptions are highly valuable, SER models, designed as classification models, cannot process natural languages and thus discard them. Notably, approximately 2.58% (on average) of the annotations across all datasets use typed descriptions.\n\nIssue 2: The author of SAIL-IEMOCAP  (Busso et al., 2008) , the most renowned SER dataset, has demonstrated that over 80.77% of SER papers produce results that cannot be reproduced  (Antoniou et al., 2023)  due to the absence of released codes. Issue 3: Official data partitioning guidelines are lacking in most SER datasets. Consequently, different papers adopt varying partitioning strategies, leading to potential data leakage problems: Typically, SER datasets comprise dialogues between two participants, denoted as Speaker A and Speaker B. In the process of segmenting these dialogues to isolate individual utterances, it is common to encounter scenarios where Speaker A's segments contain speech from Speaker B. This can cause issues because many studies adopt a straightforward approach to dividing the dataset. They possibly allocate utterances from Speaker A for training and those from Speaker B for testing. However, this approach inadvertently exposes the model to Speaker B's speech during training, leading to data leakage. Studies employing this cheating partition role, with data leakage, tend to achieve 4.011% performance improvements than those without it  (Antoniou et al., 2023) . However, comparing settings with data leakage to those without it is unfair. We introduce EMO-SUPERB to advance opensource initiatives in SER. Then, we detail how to address the above three issues individually.\n\n• For Issue 1, we employ ChatGPT to mimic annotators, comprehend typed descriptions, and re-label the data accordingly, as shown Figure  1 : Demonstration for the EMO-SUPERB platform: Developers design and evaluate SER models using our standardized dataset partition files and evaluation criteria. Developers then contribute these prediction results to the online leaderboard, enriching the benchmark database and enabling comparative analyses with other SER models.\n\nFinally, developers harness the visualization and statistical tools on the website to compare performance, gathering invaluable insights for future works. From the user's standpoint, they can upload datasets and select appropriate models tailored to their individual applications.\n\nin Section 2. With labels generated by Chat-GPT, we consistently achieve relatively 3.08% performance improvements across all settings.\n\n• For Issue 2, we develop a codebase to harness 15 SSLMs, renowned for enhancing state-ofthe-art performance in speech emotion recognition, for exhaustive evaluation across all open-source SER datasets in Section 3.2. Developers can utilize a single command line to execute both training and evaluation processes seamlessly, and we will release the easy-tofollow codebase.\n\n• For Issue 3, we partition six open-source SER datasets and address potential data leakage issues during the partitioning process, as shown in Section 3.3.\n\nFinally, we make all datasets labeled with Chat-GPT, data partition files, codes, and checkpoints open source to the community.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Empower By Chatgpt",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Importance Of Typed Descriptions",
      "text": "Emotion datasets  (Busso et al., 2008; Chou et al., 2017; Lotfian and Busso, 2017)  allow annotators to employ natural language to describe their perception of emotion corresponding to the given data if the provided label options are insufficient to capture their emotional perception fully. These descriptions, articulated in natural language, are called typed descriptions. Typed descriptions like \"Slightly Angry, calm\" serve to indicate the intensity of emotion, while \"Haaapy\" is used to emphasize happiness. Appendix D shows more examples of the typed descriptions.\n\nAlthough typed descriptions account for only approximately 2.58% of annotations across the four emotion databases that include them, they contain valuable information for emotion perception  (Lotfian and Busso, 2019; Chou et al., 2022) . Because typing down the natural language to describe the emotion perception takes more time than choosing labels from label options. Only motivated annotators will use typed descriptions. In fact, annotators are compensated based on their working hours rather than the volume of data they handle  (Lotfian and Busso, 2019) . Moreover, exemplary annotators receive additional bonuses. Some annotators invest extra time to ensure a comprehensive description of emotions to get more bonuses. However, SER, primarily based on classification models, cannot process natural language and consequently overlooks these valuable typed descriptions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Why Using Chatgpt For Relabeling",
      "text": "ChatGPT  (Achiam et al., 2023)  exhibits a remarkable ability to comprehend and analyze natural language.  Kheiri and Karimi (2023)    utilize ChatGPT to mimic annotators, summarizing their thoughts to re-label the data with typed descriptions. While GPT models have been previously utilized for data labeling tasks, our approach stands out due to its innovative application in generating a distribution of labels instead of assigning a single label. We show that this approach leads to consistent improvements across all experimental settings as shown in Table  3 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emo-Superb Platform",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Prompt Chatgpt",
      "text": "We design a carefully crafted prompt to transform the released version of GPT-4 Turbo, a variant of ChatGPT, into a knowledgeable assistant psychologist. Its primary function is to generate a distribution across emotion labels based on the input typed descriptions from annotators. As shown in Figure  2 , three inputs are provided to ChatGPT: the typed descriptions, reference distributions, and a well-designed prompt. When we prompt ChatGPT to refer to the distribution label, it fails to provide the distribution unless we supply the reference distribution. The format of the output emotion label is also a distribution. Guided by the prompt, the ChatGPT can adjust or maintain the reference distribution based on the typed descriptions. In the prompt, we also let ChatGPT explain why it changes or doesn't change the reference distributions. Without this, ChatGPT might default to laziness, consistently avoiding modifying the reference distributions. For detailed information and the final prompt, please refer to Table  11  in Appendix D.1 due to space limitations.\n\nWe choose the MSP-PODCAST (P) dataset to verify the efficacy of our proposed prompt method in utilizing typed descriptions to improve SER, as it is the largest dataset and has the highest percentage (6.08%) of typed descriptions among all other datasets. Figure  5  and the Appendix D.1.3 show the label distributions between original and re-label ones. ChatGPT can understand the typed distribution and output reasonable distributions.\n\nUltimately, we achieved an average performance improvement of approximately 3.08% across the 16 models on the MSP-PODCAST (P) as shown in Table  3  of Section 5.2. Designing an effective prompt can enhance the accuracy of the re-labeling process. This paper opens the door to utilizing large language models for comprehending typed descriptions. We welcome the community to use our user-friendly codebase to evaluate their datasets relabeled by large language models.\n\nAs shown in Figure  1 , our platform is designed to empower developers with seamless access to replicate our results, evaluate their custom SER models, compare model characteristics, and foster future SER development. This is facilitated by integrating three essential components: an easy-tofollow codebase, unified dataset partition files, and a community-driven leaderboard website. Users can select SER models for their own usage.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sslm-Based Codebase",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Framework",
      "text": "Self-supervised learning (SSL) is a promising direction for developing speech models. This approach entails training a large model with large-scale unlabeled data to obtain robust and general representations. Notably, after pre-training, one can achieve nearly SOTA performance on downstream tasks by employing the fixed SSLMs alongside task-specific lightweight prediction heads  (Yang et al., 2021) . Furthermore, SSLMs significantly enhance SER and demonstrate SOTA performance, as evidenced in  (Wagner et al., 2023) .\n\nWe develop a comprehensive codebase. The codebase depends on S3PRL 2    (Yang et al., 2021)  to leverage 15 speech-supervised learning models as feature extractors and trains lightweight heads for exhaustive evaluation across 6 open-source SER datasets with 9 common settings, as shown in Figure  3 . The six datasets adopted are SAIL-IEMOCAP, CREMA-D  (Cao et al., 2014) , MSP-IMPROV  (Busso et al., 2017) , MSP-PODCAST, BIIC-NNIME  (Chou et al., 2017) , and BIIC-PODCAST (Upadhyay et al., 2023).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Supervised Learning Models",
      "text": "We leverage two mainstream categories of SOTA SSLMs (in S3PRL), pre-trained using generative losses and discriminative losses. We summarize them in Table  1  and details can be found in Appendix B due to space limitation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pros Of The Codebase",
      "text": "The codebase has the following merits:\n\n• High-performance: Our choice to utilize SSLMs is based on their ability to consistently achieve SOTA results in speech emotion recognition, aligning with our goal to boost open-source efforts in this domain.\n\n• Affordability: The computing barrier is greatly diminished by leveraging pre-trained SSLMs and solely fine-tuning a lightweight head, enhancing affordability for researchers from diverse backgrounds.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Loss",
      "text": "Autoregressive Predictive Coding (APC)  (Chung et al., 2019)  Generative loss VQ-APC  (Chung et al., 2020)  Generative loss Non-autoregressive Predictive Coding (NPC)  (Liu et al., 2020a)  Generative loss Mockingjay  (Liu et al., 2020b))  Generative loss TERA  (Liu et al., 2021)  Generative loss DeCoAR 2  (Ling and Liu, 2020)  Generative loss WavLM  (Chen et al., 2022)  Discriminative loss Hubert  (Hsu et al., 2021)  Discriminative loss wav2vec 2.0 (W2V2)  (Baevski et al., 2020)  Discriminative loss Data2Vec  (Baevski et al., 2022)  Discriminative loss XLS-R  (Babu et al., 2021)  Discriminative loss VQ wav2vec (VQ-W2V)  (Baevski et al., 2019)    (Ling and Liu, 2020)  Discriminative loss wav2vec (W2V)  (Schneider et al., 2019)  Discriminative loss Contrastive Predictive Coding (CPC) (M CPC)  (Oord et al., 2018))  Discriminative loss  • Easy-to-follow: Developers can employ a single command line to execute all training and evaluation processes, making it exceptionally user-friendly.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Unified Dataset Partition Rules",
      "text": "Typically, emotion databases are collected from dialogues  (Busso et al., 2008 (Busso et al., , 2017;; Chou et al., 2017; Lotfian and Busso, 2017) . These dialogues often involve multiple speakers engaging in intensive turn-taking, overlap, and interruption. The segmented utterances for each speaker commonly include speech from their conversation partners. For example, consider the SAIL-IEMOCAP corpus, which comprises 5 dyadic interactions (dialogues between two speakers) involving a total of ten speakers. In 50% of previous studies, researchers randomly divide the recordings of these ten speakers into train and test sets  (Antoniou et al., 2023) . However, due to overlap often present across speaker's segments, this practice can lead to data leakage because speaker B's speech has already been used for the model training, mentioned in section 1 (Issue 3).\n\nIn this study, we establish partition rules that adhere to speaker-independent criteria to mitigate the risk of leakage. Specifically, we ensure that all utterances from both speakers involved in dialogues are assigned to either the training or testing set. Further details regarding partitioning the six emotion databases can be found in Appendix C. We provide the standardization of the training and testing splits and setups across the six public SER datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Community-Driven Leaderboard",
      "text": "The leaderboard website holds significant importance within EMO-SUPERB, continuously expanding and welcoming submissions worldwide, evolving it into a dynamic benchmark that goes beyond showcasing our own evaluation results. To mitigate the participation barrier, the website accepts submissions with participants' own models, especially when migrating their codes to the codebase in Section 3.2 is not straightforward. Participants simply need to adhere to the data partition files outlined in Section 3.3, evaluate their trained models, and submit the results. The website also offers useful visualization (e.g. radar chart Figure  9  in Appendix E) and statistical tools for comparing detailed characteristics of different models, thereby enhancing future model development.\n\nAdditionally, our platform encourages community contributions of prompts and datasets with newly re-labeled typed descriptions. Submitters can conveniently evaluate the quality of their labeled datasets using a single command line on our codebase introduced in Section 3.2.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Artifacts",
      "text": "Modern deep learning models present a reproducibility challenge, even with released codes, due to the potential impacts of minor hyperparameter change or package version disparities on performance. To assist users in debugging their training procedures, we offer Tensorboard files, hyperparameters, and pre-trained weights in our codebase. Furthermore, we provide downstream prediction files for several state-of-the-art models, enabling users to visualize and analyze results easily.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup 4.1 Datasets",
      "text": "We include the six public emotion datasets in the work. Some datasets use both primary emotions (denoted as (P)) and secondary emotions (marked as (S)) to allow annotators to choose single and multiple emotions, respectively. The Appendix A presents detailed information, and Table  5  summarizes statistical data regarding the six emotion databases. Appendix A.1 outlines the license terms and usage issues. We provide details of partitions in Appendix C to avoid issue 2 in Section 1, data leakage. The key information about these datasets is summarized as follows.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "The Iemocap",
      "text": "The SAIL-IEMOCAP  (Busso et al., 2008) , referred to as IEMOCAP, collects motion capture, audio, and video recordings from five dyadic conversations acted by ten professional actors in English. The recorded sessions were manually segmented into 10,039 utterances. The emotional annotations contain ten emotions and typed descriptions.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "The Crema-D",
      "text": "The CREMA-D  (Cao et al., 2014)  contains highquality audio-visual clips from 91 professional actors. There are 43 female and 48 male actors. There are 7,442 clips in English annotated via a crowdsourcing platform. The process of perceptual annotations has three scenarios: voice-only, face-only, and audio-visual. In this work, we only use voiceonly emotional annotations.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "The Improv",
      "text": "The MSP-IMPROV  (Busso et al., 2017) , referred to as IMPROV, consists of high-quality audio-video sessions acted by 12 actors in English. All sessions are manually segmented into 8,438 clips. The annotation process has two scenarios: primary (P) and secondary (S) emotions. The corpus collected the typed descriptions.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "The Pod",
      "text": "The MSP-PODCAST  (Lotfian and Busso, 2019) , referred to as POD, collected spontaneous and diverse emotional speech from various real-world podcast recordings with a commercial license. The labeling setting also contains primary and secondary scenarios. The major difference is the number of emotions in the given options. We use the release version 1.11 of the database, including 84,030 utterances in the train set, 19,815 in the development set, 30,647 in the test1 set, and 14,815 in the test2 set. We combine the test1 and test2 as the test set.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "The Nnime",
      "text": "The BIIC-NNIME  (Chou et al., 2017) , referred to as NNIME, consists of video, audio, and physiology recordings of dyadic conversations acted by 43 actors in Mandarin Chinese. All sessions are manually segmented into 5,596 clips. We exclude turns annotated by \"other\" from all annotators or by less than three annotators. The corpus also collects typed descriptions in Chinese.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "The B-Pod",
      "text": "The BIIC-PODCAST  (Upadhyay et al., 2023) , referred to as B-POD, is a variant of MSP-PODCAST in Mandarin Chinese. We use the release version 1.01. There are 48,815 utterances in the train set, 10,845 in the development set, and 10,340 in the test set. At least five annotators annotate each utterance, and the emotional annotators contain primary emotions (P) and secondary emotions (S), which is the same as MSP-PODCAST.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Selection Of Emotions",
      "text": "Most SER prior studies (Atmaja and Sasou, 2022;  Achiam et al., 2023)  only choose anger, happiness, sadness, and neutral state emotions as target emotions. In addition, they regard the excitement/joy annotations as happiness; however, excitement and happiness are not the same emotions  (Cowen and Keltner, 2017) , though those two emotions have correlations  (Mogilner et al., 2011) .\n\nIn contrast to previous approaches, we retain all original emotion labels and refrain from merging any emotions into others to balance the data (e.g., combining excitement with happiness). This strategy allows us to accurately assess performance and mirror natural emotion perceptions under realworld conditions.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Label Representation",
      "text": "Inspired by Semantics Space Theory  (Cowen and Keltner, 2021) , we gather numerous annotations and compute a distribution-like (soft label) representation, aiming to more accurately capture the high-dimensional nature of emotion perception. Notice that these distribution-like labels are the same as the reference distribution used for Chapt-GPT as the reference label in section 3.1.\n\nHere is one example: Let's assume we gather five annotations from five distinct raters for a single sample. These annotations comprise neutral (N), anger (A), anger (A), sadness (S), and sadness (S). Subsequently, we compute the label distributions, which in this instance are represented as (N, A, S, H) = (0.2, 0.4, 0.4, 0.0) for training SER systems. Additionally, in order to enhance SER performance, we employ the label smoothing technique proposed by  (Szegedy et al., 2016)  to refine the vector, utilizing a smoothing parameter of 0.05. This approach assigns a small probability to emotional classes with zero values.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Evaluation Metric",
      "text": "We use the macro-F1 score  (Opitz and Burst, 2019)  to evaluate the SER performance via the Scikitlearn  (Pedregosa et al., 2011) , considering recall and precision rates simultaneously. For the distribution-like multi-label training target, we select target classes by applying thresholds on the ground truth. A prediction is deemed successful if the proportion for a class surpasses 1/C, where C represents the number of emotional classes, aligning with the settings employed in prior research  (Riera et al., 2019) . The example is in Appendix E.2",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Objective Function And Training Details",
      "text": "Inspired by the study  (Cui et al., 2019) , we adopt the class-balanced cross-entropy loss, as our primary objective function due to the imbalanced label distributions across the six databases. Please refer to Appendix E.1 for detailed descriptions of classbalanced cross-entropy loss. We use the AdamW optimizer  (Loshchilov and Hutter, 2019 ) with a 0.0001 learning rate and the batch size is 32. We choose the best models according to the lowest value of the class-balanced cross-entropy loss on the development set. We use the Nvidia Tesla v100 GPUs with 32 GB memory for all results. The total of GPU hours is around 3,300 hours. According to  (Yang et al., 2021; Tsai et al., 2022; Feng et al., 2023) , SSLMs usually result in consistent results and consume large computations. All results in the work are single-run. We also verify it by running experiments for small SSLMs, and the standard deviation is only less than 1% on average.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Sslms For Ser",
      "text": "We mainly use SSLMs as our backbone models to train SER systems in the work.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Overall Results",
      "text": "Table  2  summarizes macro-F1 scores obtained by 16 SSLMs and FBANK across six datasets under nine conditions. FBANK, the most commonly used speech feature, is the baseline for comparison with SSLMs. We have the following observations: (1) All SSLMs exhibit significantly superior performance compared to FBANK. Notably, XLS-R-1B achieves a remarkable improvement of relatively 100.8% compared to FBANK. (2) The XLS-R-1B model demonstrates the highest average performance, surpassing WavLM, which typically achieves state-of-the-art results in most speech-  processing tasks. Despite this, WavLM still maintains considerable strength, achieving the highest performance in three out of nine conditions. (3) Surprisingly, despite its modest 90 million model parameters, the DeCoAR 2 model outperforms the W2V2 model, which has 317 million parameters. This finding suggests that DeCoAR 2 could be an attractive choice for developers of SER facing computational resource constraints.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Layer Analysis",
      "text": "Our training strategy involves extracting features from each layer of the SSLM, multiplying these features with layer-specific weights, and then aggregating the weighted features. These aggregated features are then fed into the downstream model. Only the layer weights and the downstream model are trainable. A large weight assigned to a specific layer suggests that the layer encodes rich emotional information. Additionally, we conduct a layer-wise analysis of the SSLMs. We select SSLMs with top-five performance, each with the same number of layers: WavLM, Hubert, W2V2 R, Data2Vec-A, and W2V2. We extract the layer weights from the best checkpoint of each model and normalize them using the softmax function to ensure values between 0 and 1. If emotion datasets contain multiple partitions (e.g., IEMOCAP and CREMA-D), we average the layer-wise weights. We show main results and additional layer-wise analysis can be found in Appendix F.1.\n\nFrom the model perspective (Figure  4a ), where we sum the layer weights across all datasets for each model and plot the resulting curves. We have the following observations: Different models have higher weights on different layers. For instance, the W2V2 R has the highest weight on the 17th layer, but the Data2Vec-A's is on the third layer. Also, the other three models have similar patterns in emphasizing all layers. Additionally, it's worth noting that the weights of W2V2 R in layers 22 to 24 are considerably lower compared to those in other layers. We observe that the SSLMs act differently and have no clear patterns in the work.\n\nFigure  4b  illustrates the layer weights for the state-of-the-art model, XLS-R-1B. Similar to other models, it exhibits a tendency to prioritize the shallow layers. However, there are two notable peak weights observed on the 30th layer, particularly trained on the POD (P) and POD (S) datasets.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results With Re-Labeled Data",
      "text": "Figure  5  shows a random selected example to compare the original distribution and relabeled distribution by ChatGPT (data sample \"POD-CAST_1631_0043_0001.wav\"). We observe that ChatGPT effectively comprehends the typed descriptions conveying positive emotions, thereby assigning greater weight to the \"happy\" emotion category. More examples can be found in Figure  8a  to Figure  8d  and Table  10  in Appendix D.1.3. Table  3  presents the macro-F1 scores of the experiment along with the effects of incorporating data labeled by ChatGPT. We denote \"w/o Chat-GPT labels\" and \"w/ ChatGPT labels\" to signify  results without and with ChatGPT labels, respectively, while maintaining all other settings the same. We note the following observations: (1) The experiments involved 16 models, resulting in an average relative performance gain of 3.08%. (2) Particularly noteworthy is the case of CPC, which exhibits a substantial 9.45% relative improvement.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Takeaways",
      "text": "Here are some takeaways: (1). DeCoAR 2 achieves considerate results on SER within a modest model size, making it an appealing option for developers constrained by computation resources. (2). SSL models trained for SER exhibit a tendency to assign greater weight to shallow layers, which are used to encode emotional information. (3). Leveraging ChatGPT for relabeling typed descriptions holds significant promise in boosting SER performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ethical Considerations And Limitations",
      "text": "In terms of limitations, our study solely focused on emotion datasets in English and Chinese, omitting datasets in other languages. Additionally, the absence of recordings featuring elderly and child speech, coupled with unknown annotator details, may hinder the representation of emotional perception across certain demographics. We do not address potential performance biases related to speaker gender within the SER systems. We only utilize ChatGPT for relabeling typed descriptions in the PODCAST dataset, given the considerable expense associated with utilizing the ChatGPT API. The task of designing improved prompts and labeling typed descriptions for other datasets remains for future investigation. We haven't attempted finetuning the entire SSLMs due to the significant computational resources required.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "We propose EMO-SUPERB, an ecosystem containing user-friendly codebases, ChatGPT re-labeled datasets, pre-trained models, fair data partition files, and a community-driven leaderboard for SER. We effectively address open questions in SER, including (1) boosting reproducibility, (2) addressing data leakage, and (3) leveraging unused typed descriptions. We encourage the community to use EMO-SUPERB to develop and evaluate the SER systems. We plan to expand our investigation in future work by incorporating additional evaluation angles, such as calibration error and gender bias.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A Details Of Emotion Dataset",
      "text": "Table  5  summarizes six public emotion datasets, detailing the number of citations for each, the total length of audio recordings (partitioned length), original recording sampling rates, and length statistics. Additionally, it provides statistics on segmented utterances, speaker and annotator information, and collection settings for all datasets.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.1 License Of Emotion Dataset",
      "text": "Table  4  offers a concise summary of the licensing details for six emotion datasets to facilitate their accessibility within the research community. It's important to note that the majority of these datasets are restricted to academic use. However, the POD-CAST and BIIC-PODCAST datasets stand out as they also offer the option for commercial licensing, albeit for a fee.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.2 The Sail-Iemocap",
      "text": "The SAIL-IEMOCAP dataset, referenced this document as IEMOCAP, was meticulously assembled from the motion capture, audio, and video recordings of dyadic conversations involving ten professional English-speaking actors  (Busso et al., 2008) . This dataset uniquely captures a blend of scripted and spontaneous dialogues, focusing primarily on scenarios that portray lovers in a relationship to elicit a rich spectrum of emotions. Each recording session featured a pair of speakers, one female and one male, engaging in interactions designed to provoke distinct emotional responses.\n\nTo ensure a diverse emotional range, the actors were provided with scripts curated to trigger specific feelings. The completed recordings were subsequently divided into 10,039 segments, each meticulously transcribed to facilitate further analysis.\n\nAnnotators then reviewed these segments, selecting emotions from a predefined list that encompassed ten distinct states: neutral, happiness, sadness, anger, surprise, fear, disgust, frustration, excitement, and an \"other\" category for emotions outside the listed spectrum. Additionally, a provision for typed descriptions allowed for a more nuanced annotation, accommodating the labeling of complex emotions that might not fit neatly into predefined categories. Notably, the annotation process permitted the selection of multiple emotions for a single utterance, with each segment being evaluated by at least three individuals from a pool of twelve annotators, comprised equally of actors and untrained (naive) annotators.\n\nA significant aspect of the IEMOCAP dataset is its focus on both self-perception and observed perception annotations, differentiating it from other emotional databases. This dual approach provides a more comprehensive understanding of the emotional landscape captured within the dataset.\n\nTo address challenges related to data reproducibility, highlighted in previous research  (Antoniou et al., 2023) , we have included detailed information on the dataset splits in Appendix refss:cviemocap. This is especially crucial considering the absence of standard split sets within the original corpus, a gap that our documentation aims to bridge.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A.3 The Crema-D",
      "text": "The CREMA-D dataset, introduced by  (Cao et al., 2014) , is a valuable resource comprising highquality audio-visual clips featuring performances from 91 professional actors, including 43 females and 48 males. These actors were tasked with recording one of twelve predetermined sentences, expressing six distinct emotions: anger, disgust, fear, happiness, sadness, and a neutral state.\n\nA notable aspect of this dataset is its extensive annotation process, involving 7,442 clips in English, evaluated by 2,443 unique annotators through a crowdsourcing platform. Each clip received feedback from at least six annotators, with each annotator attributing one of the six aforementioned emotions to the utterance. The perceptual annotation process unfolds across three distinct scenarios: voice-only, face-only, and audio-visual. In the voice-only scenario, annotators solely listen to the audio of the clips. Conversely, in the face-only scenario, annotators observe the facial expressions of the actors without accompanying audio. Finally, the audio-visual scenario allows annotators to assess both facial expressions and audio simultaneously. For the purpose of our study on Speech Emotion Recognition (SER), we focus exclusively on emotional annotations derived from the voice-only scenario. Notably, while many previous SER investigations utilized annotations from the audio-visual scenario as a learning target, or failed to specify annotation details altogether, we opt to leverage annotations solely from the voice-only setting for our analysis. Furthermore, we provide comprehensive details regarding the dataset splits employed in our paper, as outlined in Appendix C.3.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "A.4 The Msp-Improv",
      "text": "The MSP-IMPROV dataset, also known as IM-PROV  (Busso et al., 2017) , comprises high-quality audio-video recordings featuring interactions acted out by 12 actors in English. These sessions encompass four distinct emotions: anger, happiness, sadness, and a neutral state. Notably, each dyadic interaction involves one male and one female speaker and is recorded in four different scenarios: preparation, scripted, improvised, and improvised-scripted scenes.\n\nTo ensure comprehensive annotation, all sessions within the MSP-IMPROV dataset are manually segmented into 8,438 clips, each evaluated by at least five annotators via a crowdsourcing platform. Utilizing the quality control method proposed by  (Burmania et al., 2016) , the dataset employs mechanisms to identify and eliminate unreliable annota-tors.\n\nThe annotation process within MSP-IMPROV presents two scenarios: primary (P) and secondary (S) emotions. In the primary scenario, annotators select one emotion from a set of five options: anger, happiness, sadness, neutral state, or \"other.\" Secondary emotions, on the other hand, encompass a broader range, including frustration, depression, disgust, excitement, fear, and surprise. Notably, 53 utterances annotated as \"other\" by annotators are excluded from the dataset, although annotators have the option to provide textual descriptions when choosing this category.\n\nGiven that the dataset does not include predefined train, development, and test sets, we introduce our proposed split sets in Appendix C.2 for the sake of clarity and consistency in subsequent analyses.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "A.5 The Msp-Podcast",
      "text": "The MSP-PODCAST dataset, or PODCAST (Lotfian and Busso, 2019), offers a rich collection of spontaneous and diverse emotional speech extracted from real-world podcast recordings obtained under commercial licenses. Initially, the podcast recordings are segmented into individual utterances, which are then annotated via a crowdsourcing platform. Similar to MSP-IMPROV, the dataset implements quality control measures based on the methodology outlined by  (Burmania et al., 2016)  to ensure the reliability of annotators.\n\nThe annotation framework within MSP-PODCAST encompasses both primary (P) and secondary (S) scenarios. In the primary scenario, annotators select from a set of nine predefined emotions: anger, sadness, happiness, surprise, fear, disgust, contempt, neutral, and \"other,\" with the option to provide additional textual descriptions if necessary. The secondary scenario expands upon the primary emotions, incorporating an additional eight classes: amusement, frustration, depression, concern, disappointment, excitement, confusion, and annoyance, totaling 17 options.\n\nEach utterance within the dataset is evaluated by at least five unique annotators, ensuring robustness in the annotation process. The dataset version 1.11 consists of 84,030 utterances in the train set, 19,815 in the development set, 30,647 in the combined test set (test1 and test2), and 2,347 in the test3 set, which is excluded from analysis due to its private nature lacking annotations.\n\nIn total, the dataset encompasses contributions from over 2,172 distinct speakers and involves 14,363 annotators, providing a comprehensive resource for studying emotional speech.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "A.6 The Biic-Nnime",
      "text": "The BIIC-NNIME dataset, also known as NNIME  (Chou et al., 2017) , is a comprehensive resource featuring video, audio, and physiology recordings of dyadic conversations acted out by 43 actors in Mandarin Chinese. These sessions are characterized by spontaneous, unscripted interactions set in everyday home environments, encompassing six emotional scenes: anger, frustration, happiness, sadness, surprise, and neutral states.\n\nEach session within the NNIME dataset is meticulously segmented into 5,596 clips. To maintain annotation quality, utterances labeled as \"other\" by annotators or those annotated by fewer than three annotators are excluded from analysis. Notably, NNIME stands out from other emotion datasets due to its annotation of both speech and non-verbal behaviors, such as laughter, sighing, sobbing, and other vocal expressions.\n\nWith a total of 43 unique speakers and annotations from six different annotators, the labeling process within NNIME resembles that of the SAIL-IEMOCAP dataset. Annotators view clips sequentially and select emotions from a pool of 12 options: anger, frustration, disappointment, sadness, fear, surprise, excitement, happiness, relaxation, joy, neutral state, and \"other.\" Moreover, annotators have the flexibility to express emotional perceptions using Chinese words.\n\nGiven the absence of standard split sets for training deep-learning models within the corpus, we provide details of our proposed split sets in Appendix C.4 to ensure reproducibility and facilitate further research using the NNIME dataset.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "A.7 The Biic-Podcast",
      "text": "The BIIC-PODCAST dataset, or B-PODCAST  (Upadhyay et al., 2023) , presents a Mandarin Chinese variant of the MSP-PODCAST, featuring audio recordings sourced from real-world podcasts under commercial licenses. Notably, the dataset diverges from MSP-PODCAST in its labeling process, employing college students as annotators instead of utilizing a crowdsourcing platform. This approach aims to enhance quality control and ensure the reliability of annotations, a methodology similar to MSP-PODCAST's quality assessment standards.\n\nVersion 1.01 of the B-PODCAST dataset includes 48,815 utterances in the train set, 10,845 in the development set, and 10,340 in the test set. Each utterance undergoes evaluation by a minimum of five annotators. The emotional annotations within B-PODCAST encompass both primary (P) and secondary (S) emotions, mirroring the structure of MSP-PODCAST.\n\nThe primary emotions (P) include the same set of nine options as MSP-PODCAST, comprising anger, sadness, happiness, surprise, fear, disgust, contempt, neutral, and \"other.\" Similarly, the secondary emotions (S) expand upon the primary emotions with additional classes, maintaining consistency with MSP-PODCAST.\n\nOverall, B-PODCAST serves as a valuable resource for studying emotional speech in Mandarin Chinese, offering a curated dataset with robust annotations and quality control measures.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "B Sslm Introductions",
      "text": "In our codebase, we leverage two mainstream categories of SSLMs, pre-trained using generative loss, DeCoAR 2  (Ling and Liu, 2020) , Autoregressive Predictive Coding (APC)  (Chung et al., 2019) , VQ-APC  (Chung et al., 2020) , Nonautoregressive Predictive Coding (NPC)  (Liu et al., 2020a) , TERA  (Liu et al., 2021) , and Mockingjay  (Liu et al., 2020b) ), and discriminative loss (XLS-R-1B)  (Babu et al., 2021) , WavLM  (Chen et al., 2022) , Hubert  (Hsu et al., 2021) , wav2vec 2.0 (W2V2)  (Baevski et al., 2020) , VQ wav2vec (VQ-W2V)  (Baevski et al., 2019) , wav2vec (W2V)  (Schneider et al., 2019) , and Contrastive Predictive Coding (CPC) (M CPC)  (Oord et al., 2018) ).\n\nAPC employs a pretraining strategy similar to language models on a sequence of acoustic features (FBANK). It utilizes unidirectional RNNs to predict future FBANK frames based on past ones. VQ-APC improves APC's representation by integrating vector-quantization (VQ) layers. NPC boosts APC's efficiency by replacing RNNs with CNNs for faster inference. Mockingjay consists of Transformer encoders. It masks segments of input acoustic features along the time axis and reconstructs them during training. TERA builds upon Mockingjay's architecture by extending the masking strategy to frequency bins.\n\nDeCoAR 2.0 refines Mockingjay's design by incorporating a VQ layer just before final predictions, similar to VQ-APC's approach. Its training involves larger input masks, increased batch sizes, and the utilization of more unlabeled data to improve performance. Wav2vec introduced several architectural enhancements to refine CPC's performance. VQ-wav2vec integrates a VQ module into wav2vec, discretizing speech into tokens post after InfoNCE pretraining. These discrete tokens are used for training a BERT model, to get contextualized representations. Wav2vec 2.0 streamlines the vq-wav2vec pipeline into an end-to-end framework. This involves employing time masking in the latent space and substituting BERT's token prediction with InfoNCE's negative sampling. XLS-R builds upon wav2vec 2.0, expanding its capabilities to encompass multiple languages and augmenting the dataset size. HuBERT enables BERT's token prediction through offline clustering of representations. Predictions are made based on the clustered labels at masked locations. WavLM, based on Hubert, introduces noise during pretraining to enhance the robustness of SSL features.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "C Partition Setting",
      "text": "In the speaker-independent scenario, where the model is trained on data from certain speakers and tested on data from speakers not seen during training, ensuring fair and robust evaluation is crucial.\n\nHere are the details about data partitions for experiments on the SAIL-IEMOCAP, MSP-IMPROV, CREMA-D, and BIIC-NNIME datasets.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "C.1 The Iemocap",
      "text": "Table  6  summarizes the partitioning settings for the IEMOCAP corpus. Considering each session, we define five speaker-independent splits (i.e., Dyad 1 to Dyad 5). Each session consists of two speakers engaged in dyadic interactions. In our experiments, we conduct a 5-fold cross-validation as illustrated in Table  6 , where each fold includes a unique combination of training, development, and test sets to ensure comprehensive evaluation of the model's performance across different dyadic interactions within the IEMOCAP corpus.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "C.3 The Crema-D",
      "text": "In the speaker-independent scenario, the CREMA-D corpus is divided into five sets based on speaker IDs. Each set consists of a different combination of male and female speakers, as well as a distinct range of speaker IDs, as summarized in Table  8 . This partitioning strategy allows for a fair and balanced evaluation of models trained on CREMA-D data by ensuring that the test sets contain speakers not seen during training, thereby assessing the model's ability to generalize across different speaker characteristics and expressions. The standard partitions follow a similar methodology as the one mentioned for the IEMOCAP dataset in section C.1.   In this context, the word cloud provides insight into the types of words or phrases used by annotators to describe their emotional responses in the dataset.\n\nAnalyzing typed descriptions can be valuable for understanding the nuances of human emotion and improving the comprehensiveness of emotion recognition systems. By incorporating natural language processing techniques, researchers can extract valuable insights from typed descriptions to enhance the accuracy and granularity of emotion annotation in datasets like POD.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "D.1 Prompt For Chatgpt",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "D.1.1 Design Of Prompt",
      "text": "Table  11  shows the well-designed prompt, which contains five parts: objective, input format, example, output format, and refine and iterate. In the objective part, we clearly describe the goal of task. Then, we define the input format, including descriptions and reference emotion. Afterwards, we provide an example that provide the template the ChatGPT can follow. Finally, we ask the ChatGPT output the file in JSON format. Notice that the current version of prompt is the 14th version. In the refine and iterate part, we show the more rules that can enhance the accuracy of the output of the ChatGPT. We encourage the community to provide the designed prompt.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "D.1.2 Parameters And Cost Of Chatgpt Api",
      "text": "Listing shows the simple code of ChatGPT API. We setup the temperature as 0. and seed as 7 in the work to keep the reproducibility. We choose the version, \"gpt-4-0125-preview\", as the main model.\n\nThe average cost per data sample is $0.0045 USD. The target emotion is represented by an 8dimensional distribution vector. During training and development, ChatGPT examined 34.21% and 33.32% of the data, respectively. Out of the total      Figure  11 : The layerwise weights analysis across three models.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Demonstration for the EMO-SUPERB platform: Developers design and evaluate SER models using our",
      "page": 2
    },
    {
      "caption": "Figure 2: Labeling process using ChatGPT. Three in-",
      "page": 3
    },
    {
      "caption": "Figure 2: , three inputs are provided",
      "page": 3
    },
    {
      "caption": "Figure 5: and the Appendix D.1.3",
      "page": 3
    },
    {
      "caption": "Figure 1: , our platform is designed",
      "page": 3
    },
    {
      "caption": "Figure 3: Illustration of SSLM-based SER",
      "page": 4
    },
    {
      "caption": "Figure 3: The six datasets adopted are SAIL-",
      "page": 4
    },
    {
      "caption": "Figure 4: The layerwise weights analysis.",
      "page": 7
    },
    {
      "caption": "Figure 4: b illustrates the layer weights for the",
      "page": 7
    },
    {
      "caption": "Figure 5: shows a random selected example to",
      "page": 8
    },
    {
      "caption": "Figure 8: d and Table 10 in Appendix D.1.3.",
      "page": 8
    },
    {
      "caption": "Figure 5: Original and adjusted distributions. The orig-",
      "page": 8
    },
    {
      "caption": "Figure 6: displays a word cloud generated from",
      "page": 15
    },
    {
      "caption": "Figure 6: The figures shows typed descriptions of the",
      "page": 15
    },
    {
      "caption": "Figure 7: The figures shows comparisons of original and",
      "page": 16
    },
    {
      "caption": "Figure 7: and 8 shows the changes of label distribu-",
      "page": 16
    },
    {
      "caption": "Figure 10: and 11 show the weights across layers.",
      "page": 16
    },
    {
      "caption": "Figure 10: , the patterns of POD (P) and POD",
      "page": 16
    },
    {
      "caption": "Figure 11: , our observations align with those",
      "page": 16
    },
    {
      "caption": "Figure 8: The original distribution and adjusted distri-",
      "page": 18
    },
    {
      "caption": "Figure 9: Demonstration of radar chart to compare four",
      "page": 18
    },
    {
      "caption": "Figure 10: The layerwise weights analysis across 5",
      "page": 18
    },
    {
      "caption": "Figure 11: The layerwise weights analysis across three",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table 3: of Section 5.2. Designing an effective",
      "data": [
        {
          "Typed description": "inspired, proud,\ninspired, gratitude"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: of Section 5.2. Designing an effective",
      "data": [
        {
          "Reason": "Inspiration, pride,\nand gratitude are\nstrongly positive,\nincreasing 'happy'."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.38352": "",
          "Column_2": "0.559"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gpt-4 technical report",
      "authors": [
        "Josh Achiam"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Designing and Evaluating Speech Emotion Recognition Systems: A Reality Check Case Study with IEMOCAP",
      "authors": [
        "Nikolaos Antoniou",
        "Athanasios Katsamanis",
        "Theodoros Giannakopoulos",
        "Shrikanth Narayanan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP49357.2023.10096808"
    },
    {
      "citation_id": "3",
      "title": "Evaluating Self-Supervised Speech Representations for Speech Emotion Recognition",
      "authors": [
        "Bagus Tris",
        "Akira Sasou"
      ],
      "year": "2022",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2022.3225198"
    },
    {
      "citation_id": "4",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "Arun Babu",
        "Changhan Wang",
        "Andros Tjandra",
        "Kushal Lakhotia",
        "Qiantong Xu",
        "Naman Goyal",
        "Kritika Singh",
        "Yatharth Patrick Von Platen",
        "Juan Saraf",
        "Pino"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "5",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "6",
      "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "Alexei Baevski",
        "Steffen Schneider",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "arxiv": "arXiv:1910.05453"
    },
    {
      "citation_id": "7",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Increasing the Reliability of Crowdsourcing Evaluations Using Online Quality Assessment",
      "authors": [
        "Alec Burmania",
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2493525"
    },
    {
      "citation_id": "9",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "10",
      "title": "MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2515617"
    },
    {
      "citation_id": "11",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "12",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "NNIME: The NTHU-NTUA Chinese interactive multimodal emotion corpus",
      "authors": [
        "Huang-Cheng Chou",
        "Wei-Cheng Lin",
        "Lien-Chiang Chang",
        "Chyi-Chang Li",
        "Hsi-Pin",
        "Chi-Chun Ma",
        "Lee"
      ],
      "year": "2017",
      "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII.2017.8273615"
    },
    {
      "citation_id": "14",
      "title": "Exploiting Annotators' Typed Description of Emotion Perception to Maximize Utilization of Ratings for Speech Emotion Recognition",
      "authors": [
        "Huang-Cheng Chou",
        "Wei-Cheng Lin",
        "Chi-Chun Lee",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)",
      "doi": "10.1109/ICASSP43922.2022.9746990"
    },
    {
      "citation_id": "15",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Yu-An Chung",
        "Wei-Ning Hsu",
        "Hao Tang",
        "James Glass"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning",
      "arxiv": "arXiv:1904.03240"
    },
    {
      "citation_id": "16",
      "title": "Vector-quantized autoregressive predictive coding",
      "authors": [
        "Yu-An Chung",
        "Hao Tang",
        "James Glass"
      ],
      "year": "2020",
      "venue": "Vector-quantized autoregressive predictive coding",
      "arxiv": "arXiv:2005.08392"
    },
    {
      "citation_id": "17",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.1702247114"
    },
    {
      "citation_id": "18",
      "title": "Semantic Space Theory: A Computational Approach to Emotion",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2021",
      "venue": "Trends in Cognitive Sciences",
      "doi": "10.1016/j.tics.2020.11.004"
    },
    {
      "citation_id": "19",
      "title": "Class-Balanced Loss Based on Effective Number of Samples",
      "authors": [
        "Y Cui",
        "M Jia",
        "T.-Y Lin",
        "Y Song",
        "S Belongie"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2019.00949"
    },
    {
      "citation_id": "20",
      "title": "Superb@ slt 2022: Challenge on generalization and efficiency of self-supervised speech representation learning",
      "authors": [
        "Annie Tzu-Hsun Feng",
        "Ching-Feng Dong",
        "Shu-Wen Yeh",
        "Tzu-Quan Yang",
        "Jiatong Lin",
        "Kai-Wei Shi",
        "Zili Chang",
        "Haibin Huang",
        "Xuankai Wu",
        "Chang"
      ],
      "year": "2022",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "21",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "22",
      "title": "SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning",
      "authors": [
        "Kiana Kheiri",
        "Hamid Karimi"
      ],
      "year": "2023",
      "venue": "SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning"
    },
    {
      "citation_id": "23",
      "title": "Exploration of a Self-Supervised Speech Model: A Study on Emotional Corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
      "doi": "10.1109/SLT54892.2023.10023428"
    },
    {
      "citation_id": "24",
      "title": "Decoar 2.0: Deep contextualized acoustic representations with vector quantization",
      "authors": [
        "Shaoshi Ling",
        "Yuzong Liu"
      ],
      "year": "2020",
      "venue": "Decoar 2.0: Deep contextualized acoustic representations with vector quantization",
      "arxiv": "arXiv:2012.06659"
    },
    {
      "citation_id": "25",
      "title": "Non-autoregressive predictive coding for learning speech representations from local dependencies",
      "authors": [
        "Yu-An Alexander H Liu",
        "James Chung",
        "Glass"
      ],
      "year": "2020",
      "venue": "Non-autoregressive predictive coding for learning speech representations from local dependencies",
      "arxiv": "arXiv:2011.00406"
    },
    {
      "citation_id": "26",
      "title": "Tera: Self-supervised learning of transformer encoder representation for speech",
      "authors": [
        "Andy Liu",
        "Shang-Wen Li",
        "Hung-Yi Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "27",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "Andy Liu",
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Po-Chun Hsu",
        "Hung-Yi Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "28",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "29",
      "title": "Formulating Emotion Perception as a Probabilistic Model with Application to Categorical Emotion Classification",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII 2017)",
      "doi": "10.1109/ACII.2017.8273633"
    },
    {
      "citation_id": "30",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech From Existing Podcast Recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2736999"
    },
    {
      "citation_id": "31",
      "title": "The Shifting Meaning of Happiness",
      "authors": [
        "Cassie Mogilner",
        "D Sepandar",
        "Jennifer Kamvar",
        "Aaker"
      ],
      "year": "2011",
      "venue": "Social Psychological and Personality Science",
      "doi": "10.1177/1948550610393987"
    },
    {
      "citation_id": "32",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "33",
      "title": "Macro f1 and macro f1",
      "authors": [
        "Juri Opitz",
        "Sebastian Burst"
      ],
      "year": "2019",
      "venue": "Macro f1 and macro f1",
      "arxiv": "arXiv:1911.03347"
    },
    {
      "citation_id": "34",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "Fabian Pedregosa"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "35",
      "title": "No Sample Left Behind: Towards a Comprehensive Evaluation of Speech Emotion Recognition Systems",
      "authors": [
        "Pablo Riera",
        "Luciana Ferrer",
        "Agustín Gravano",
        "Lara Gauder"
      ],
      "year": "2019",
      "venue": "Proc. SMM",
      "doi": "10.21437/SMM.2019-3"
    },
    {
      "citation_id": "36",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "37",
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jon Shlens",
        "Zbigniew Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "Superb-sg: Enhanced speech processing universal performance benchmark for semantic and generative capabilities",
      "authors": [
        "Hsiang-Sheng Tsai",
        "Heng-Jui Chang",
        "Wen-Chin Huang",
        "Zili Huang",
        "Kushal Lakhotia",
        "Shu-Wen Yang",
        "Shuyan Dong",
        "Andy Liu",
        "Cheng-I Jeff Lai",
        "Jiatong Shi"
      ],
      "year": "2022",
      "venue": "Superb-sg: Enhanced speech processing universal performance benchmark for semantic and generative capabilities",
      "arxiv": "arXiv:2203.06849"
    },
    {
      "citation_id": "39",
      "title": "An Intelligent Infrastructure Toward Large Scale Naturalistic Affective Speech Corpora Collection",
      "authors": [
        "Woan-Shiuan Shreya G Upadhyay",
        "Bo-Hao Chien",
        "Lucas Su",
        "Ya-Tse Goncalves",
        "Ali Wu",
        "Carlos Salman",
        "Chi-Chun Busso",
        "Lee"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2023.3263585"
    },
    {
      "citation_id": "41",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    }
  ]
}