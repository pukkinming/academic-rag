{
  "paper_id": "2510.00725v1",
  "title": "Deap Dive: Dataset Investigation With Vision Transformers For Eeg Evaluation",
  "published": "2025-10-01T10:07:07Z",
  "authors": [
    "Annemarie Hoffsommer",
    "Helen Schneider",
    "Svetlana Pavlitska",
    "J. Marius Zöllner"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Accurately predicting emotions from brain signals has the potential to achieve goals such as improving mental health, human-computer interaction, and affective computing. Emotion prediction through neural signals offers a promising alternative to traditional methods, such as selfassessment and facial expression analysis, which can be subjective or ambiguous. Measurements of the brain activity via electroencephalogram (EEG) provides a more direct and unbiased data source. However, conducting a full EEG is a complex, resource-intensive process, leading to the rise of low-cost EEG devices with simplified measurement capabilities. This work examines how subsets of EEG channels from the DEAP dataset can be used for sufficiently accurate emotion prediction with low-cost EEG devices, rather than fully equipped EEG-measurements.Using Continuous Wavelet Transformation to convert EEG data into scaleograms, we trained a vision transformer (ViT) model for emotion classification. The model achieved over 91,57% accuracy in predicting 4 quadrants (high/low per arousal and valence) with only 12 measuring points (also referred to as channels). Our work shows clearly, that a significant reduction of input channels yields high results compared to state-of-the-art results of 96,9% with 32 channels. Training scripts to reproduce our code can be found here: https: //gitlab.kit.edu/kit/aifb/ATKS/public/ AutoSMiLeS/DEAP-DIVE.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The ability to predict human emotions accurately from neural signals holds potential for various fields, including (mental) health  [25] , human-computer interaction  [45] , and affective computing  [24] . Traditionally, emotion recognition has relied on subjective or ambiguous methods, such as self-assessment and facial expression analysis  [37] . While * These authors contributed equally to this work these methods provide valuable insights, they are limited by their dependence on interpretation, leading to inconsistencies and potential biases  [7] . Importantly, methods such as Electroencephalography (EEG) provide a direct and objective approach to analyzing neural activity and assessing emotional states by measuring the brain's electrical signals. This method mitigates the limitations of traditional techniques, such as subjective biases and inconsistencies.\n\nHowever, despite the advantages of EEG-based emotion prediction, a full-scale EEG setup is often resourceintensive. Medical-grade EEG devices, typically used in clinical settings, require specialized training, costly equipment, and extensive preparation due to the high number of channels involved (up to 60 electrodes). This complexity renders it impractical for application in remote settings, home healthcare environments, regions with limited access to medical resources, or by researchers from different disciplines lacking specialized medical expertise. To address this, there is an increasing interest in rendering portable, low-cost EEG devices usable in practical applications, particularly for emotion recognition  [21] ,  [9] . These devices, though limited in terms of the number of electrodes, offer a more accessible solution. An open research challenge involves the use of EEG-devices in delivering reliable predictions of emotions with fewer input channels  [20] . This work explores how subsets of EEG channels from the DEAP dataset  [23] , a widely used benchmark for emotion recognition  [4, 36, 40] , can be used to achieve accurate emotion prediction, even with low-cost, simplified EEG devices. We investigate the balance between minimizing the number of input channels and maintaining prediction accuracy, with a focus on making EEG-based emotion recognition more accessible and practical in real-world scenarios.\n\nA key element of our approach involves mapping EEG signals to emotions. Emotions are complex, dynamic processes that often defy simple categorization  [6] . However, by employing advanced signal processing techniques, such as Continuous Wavelet Transformation (CWT)  [30] , which captures temporal dependencies in EEG signals, we can transform these signals into images, known as scaleograms. Therefore, through the transformation of EEG data into CWTs we obtain a visual representation of the DEAP Dataset. Scaleograms are considered meaningful input for data with wavelet characteristics (e.g. EEGs) as they provide direct knowledge to relevant features of the data -the frequencies over time. These images are then used to train a vision transformer  [10] , a model architecture designed to capture both temporal and spatial dependencies in data. Our approach is displayed visually in an exemplary concept diagram in Figure  1 . By pairing CWT with the ViT, we aim to capture the underlying patterns in brain activity that correspond to emotional states. CWT has been used in recent studies combined with convolutional neural networks (CNNs)  [14] , support vector machines (SVMs)  [12] , or standard statistical methods  [29] .\n\nOur contributions are threefold. First, our work is the first to use CWT combined with vision transformers for emotion recognition and reaching SoTA results. Second, we offer insights into the minimal requirements for accurate brain-computer interfacing by analyzing different subsets of EEG-channels. We show that using only 12 channels achieves results of 91.57% accuracy close to SoTA-results of 96.90% with 32 channels. Third, we are the first, to the best of our knowledge, to provide a baseline for regression on valence and arousal for DEAP with an RMSE of 0.57.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Accurately labeling and classifying emotions, along with understanding their relationships and proximity, is a complex and often ambiguous task, as no clear metric exists. The Circumplex Model of Affect, proposed by Russell in the 1970s  [34]  and still widely used today, serves as a tool for defining and organizing emotions. This model classifies emotions on a continuous, two-dimensional scale. The horizontal axis -valence -represents the range from negative to positive emotions, while the vertical axis -arousal -mea-  sures the level of activation, from calmness or sleepiness to excitement or arousal (see Figure  2 ).\n\nClassifying emotions has been done among others with facial configurations  [43] ,  [47] , physiological signals (e.g. heartbeat  [38] , galvanic skin response  [32] , EEG  [13] ), and body posture  [35] . In our work, we analyze how EEG-data can give insights into what is happening in the brain during emotional occurrences and how new techniques such as vision transformers can help in making low-cost EEGrecordings more useful in affective research.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets For Emotion Recognition Using Eeg",
      "text": "Commonly used datasets for sentiment analysis with EEG data include SEED&SEED-IV  [46] ,  [11] , MAHNOB-HCI  [39]  and DEAP  [23] . The SEED dataset utilizes movie clips to evoke emotions labeled as positive, negative, and neutral (or happy, sad, fearful, neutral in SEED-IV). However, with only 15 participants, SEED has fewer subjects compared to DEAP, which is seen as a limitation for crosssubject validation. The MAHNOB-HCI dataset, which also uses movie clips to elicit emotions, includes 27 subjects and labels emotions based on liking and disliking. This labeling approach was not chosen for our work due to its misalignment with the intended Circumplex Model of Affect. The DEAP dataset comprises data from 32 participants, with 40 different measurements (also referred to as channels). These include 32 EEG channels (see Figure  3 ), along with additional physiological signals, e.g. electrooculography (eye movement), electromyography (muscle activity), galvanic skin response, blood volume pressure, respiration, and temperature. In DEAP, each participant watched 40 one-minute music videos, selected to evoke a range of emotional responses. After viewing each video, participants rated their emotional state in terms of valence, arousal, dominance, and liking, using a self-assessment method.\n\nFor this work, we used the preprocessed DEAP dataset, already prepared via data reordering, downsampling, artifact removal, band-pass filtering, and averaging to a common reference (see  [23]  for more details).\n\nThe number of selected videos in the DEAP Dataset is slightly unbalanced per quadrant (class), while the quadrants are Q1:  2 ). The following numbers of videos were available in the Dataset with each participant watching all of them: 8 in Q1, 12 in Q2, 10 in Q3 and 10 in Q4. Due to the insignificant low imbalance, we did not employ dataset balancing techniques.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "State Of The Art For Deap Dataset Classifications",
      "text": "Previous work for classification on the DEAP dataset can be seen in Table  1 . The four class accuracy refers to classification of affect into the four quadrants of Q1, Q2, Q3 and Q4 as mentioned above. Therefore predicting valence and arousal simultaneously. Best results are achieved by our vision transformer with 91.57% accuracy compared to 83.52% accuracy reached using a Multi-Layer Perceptron (MLP)  [29] . The binary classifications of valence is predicting either high or low and for the binary classifications of arousal respectively. For comparability to the four class approach, the combined binary accuracy is calculated as the probability of the model for binary valence and the model of binary arousal to classify the same data as a combined model correctly into the four quadrants. The accuracy is calculated by multiplying the accuracy of binary valence and binary arousal. The intention of the binary combined accuracy was solely to provide a theoretical reference for comparison within the 4-class classification task, therefore it was not computed for the proposed model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "State Of The Art Deap Classification Using Cwt",
      "text": "Previous work of classifying the DEAP dataset including wavelet transformations for preprocessing are shown in Table 2. Using CWT combined with a vision transformer results in 91.57% accuracy (only 12 channels) reaching near to state-of-the-art results of 96.9% accuracy (32 channels) with a majority voting of CNNs  [5] , despite significantly lower amount of input and utilizing only one architecture. The highest 4-class accuracy reported by  [5]  was achieved using a wavelet transformation. Building on this, the EEG data in this work was represented as scaleograms obtained through a wavelet transformation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "State Of The Art For Deap Dataset Regression",
      "text": "To our knowledge there is no work available that also predicts continuous values from the DEAP dataset.  [47]  predicts the continuous valence of the MAHNOB-HCI dataset by using facial expression and EEG Data. Using a subset of the DEAP Dataset only containing the EEG channels we achieved a Root Mean Squared Error (RMSE) of 0.57.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methods",
      "text": "A CWT was applied to each normalized measurement in order to generate a scaleogram. The continuous wavelet transformation  [30] ,  [15] , decomposes a given signal into the frequency components and their magnitude of the signal at each time. This 3D Data can be plotted as a 2D scaleogram with time and frequency as axis and the magnitude as color. Please note that in order to gain a result containing information of time and frequency the CWT looses resolution in difference to a fully reversible Fourier Transformation.\n\nEach resulting image corresponds to one electrode recording from a single participant while watching a oneminute video. The processed images were subsequently input into a vision transformer model. It was possible to choose different subsets of channels and build models using only specific single or groups of measurements. An exemplary approaches 4 channels can be seen in Figure  1 .\n\nAlthough the CWT introduces a slight loss of information into the system, this trade-off was considered acceptable. As the gain in time-domain information, combined with the transformer's ability to process temporal dependencies, was deemed to be beneficial.\n\nWe formulate the emotion recognition problem as a 4-class classification task, with the following labels: high arousal-low valence, high arousal-high valence, low arousal-low valence, low arousal-high valence.\n\nThe VAQ Estimate of the video (Valence Arousal Quadrant) was selected by the DEAP experimenters and was preferred as label over the self assessment manikin (SAM) labels (labeled by the participants) as it is less dependent on differing and inconsistent ratings of persons. If not otherwise stated in Chapter 5.3, the VAQ Estimate was chosen as label.\n\nA vision transformer (Linformer) model was employed. Vision transformers  [10]  apply the self-attention mechanism of transformers  [42] . ViTs partition images into fixedsize patches, linearly embedding them into sequences related to tokens used in language models  [10] . This approach allows the model to capture long-term dependencies as well as global and spectral context effectively  [42] .\n\nA linear classification layer was added to adapt the network for a 4-class classification task. The target classes were based on high/low arousal and high/low valence. Unless stated otherwise in 5.3, the VAQ Estimate was used as the label for classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiment Setup",
      "text": "For our experiments, the entire data of the DEAP-dataset was split into k=5 cross validation folds. The train-test split was conducted not dependent on participant ID. This means that part of the data stemming from a single participant could be in test-split, while the rest of the data from this participant could be in the train-split. This configuration allows for an evaluation of how well the models could generalize when baseline data from a person is available for training. The images obtained by the CWT were bi-linear resized to 224x224 pixels.\n\nFor the classification experiments different setups were chosen to evaluate the use of only one channel, subsets of the entire channels and differences between using SAMlabels and VAQ labels. Finally, a regression was conducted in order to have a more fine-grained evaluation of emotional experiences.\n\nFor all classification experiments, a learning rate scheduler and the Adam optimizer  [22]  as well as Cross Entropy Loss was employed for the training. Each experiment used 5-fold cross-validation (k=5). Early stopping was implemented, with training halted if the test loss did not improve after five consecutive epochs of exceeding the minimum observed test loss.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification -Single Channel Experiments",
      "text": "For the single channel experiments, each EEG channel was tested individually to assess its ability to predict emotions using the smallest available data subset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification -Channel Subsets Experiments",
      "text": "In order to evaluate how different numbers of EEG-channels result in different accuracies, the following subsets were employed for training:\n\nAll Data: All available data from the DEAP dataset was used for this experiment including EEG and other physiological signals.\n\nMuse-12/8/4 Electrodes and Emotiv Testing: To evaluate the performance of low-budget EEG devices with fewer electrodes, two commonly used devices were selected: the Muse S * (with 4 electrodes -AF7, AF8, TP9, TP10) and the Emotiv * (with 12 electrodes, (AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4)). Since the exact electrode placements of the Muse S are not available in the DEAP dataset, the closest corresponding electrodes were used. The experiments tested the three nearest (setup \"Muse-12\" -AF3, AF4, FP1, FP2, F7, F8, P7, P8, CP5, CP6, T7, T8), two nearest (setup \"Muse-8\" -AF3, AF4, F7, F8, P7,P8, T7, T8), and the single nearest (setup \"Muse-4\") electrodes to match the Muse S setup. As the nearest electrodes to the Muse were unclear to determine there are two Version 4a (AF3, AF4, P7, P8) and 4b (F7, F8, T7, T8).\n\nPCA-12/4 Testing: A Principal Component Analysis (PCA) was applied to the entire dataset as a standard method to identify the most relevant features. The 12 (setup \"PCA-12\") or 4 (setup \"PCA-4\") most relevant channels, determined based on the explained variance ratio, were grouped together and tested.\n\nEEG-Only Setup: This experiment used only channels 1-32, which correspond to the EEG electrodes, excluding other physiological measurements.\n\nNon-EEG-Only Setup: In this configuration, only channels 33-40 were used, corresponding to non-EEG physiological measurements.\n\nGrouped Channels: EEG channels were grouped by the brain regions the placements correspond to, and each group's performance was tested individually. See Figure  4  for the different regions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Classification -Differences Between Sam And Vaq Labels",
      "text": "In this setup, the SAM-labels were used instead of the VAQlabels. SAM labels were given individually by each participant, while the VAQ labels were given by the experimenters.\n\nThe objective was to investigate potential significant differences between the two labeling methods and gain deeper insights into the limitations of self-assessed data.\n\nFor the SAM labels, each video was classified by each participant individually based on the assigned valence and arousal scores of their self-assessment. SAM data ranges from 1 to 9 and was collected by participants selecting a position on a continuous horizontal scale between five manikins, with a distance of 1.6 units between each manikin. To compare the SAM and VAQ label data, the threshold for classifying a video as high or low valence/arousal was set at 5, as in the original DEAP study.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Regression Experiments",
      "text": "For the regression task only SAM labels were used, as they provide the only continuous values in the dataset. For train-* https://choosemuse.com/ * https://www.emotiv.com/ ing, the loss function was adjusted to the Huber Loss to account for outliers that are expected due to human factors and the Root-Mean-Squared-Error(RMSE) was used as metric. As it was observed that regressions tend to train longer, the early stopping threshold was adjusted to 10 consecutive epochs compared to the 5 in the classification settings.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "Defining what qualifies as relevant, significant, or sufficient for this study does not have a precise answer. Previous works (see Tables  1  and 2 ) do not contain a strict definition of a relevant result. To overcome this problem, we introduce the following new definition: in this work, the results for predicting four classes were considered relevant when the accuracy exceeded twice the expected random accuracy, effectively surpassing the 50% threshold. We are aware that due to the unbalanced dataset, this is not an exact calculation of chances. Serving the purpose of setting a lower bound, though, this calculation was considered sufficient.\n\nFor regression within the 9x9 area, the expected RMSE for the given data is 3.399. Accordingly, results were deemed relevant when the RMSE was lower than 1.6995, which is half of the expected RMSE value.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification -Single Channel Experiments",
      "text": "Only one channel, Channel 33 produced a sufficiently high classification accuracy when used as the sole input. This channel, representing horizontal eye movement (hEOG), is not typically part of an EEG setup. With an accuracy exceeding 70% for classifying four classes, it provides a partial answer to the question of how little input is needed to achieve meaningful predictions. Other channels did not yield an accuracy considered as significant.\n\nAs follow up we conducted a cross-person experiment for the single channel of horizontal eye movement. In this setup the test and train split was depending on the persons. Meaning that no data from a person selected in test was used for training of the model. The accuracy remained the same for the cross-person setting of channel 33, one fold even surpassed the previous experiments. This suggests that the eye movement could be a beneficial measurement for lowbudget EEG-devices as it provides a solid basis for classification. However, it is yet to be determined whether the eye-movement-model predicted an emotion or rather just Figure  5 . Classification experiments with VAQ labels -labels given by the experimenters. Results of Table  3 , 6 and 5 are presented visually by their 5 folds locality, spread and skewness through their quartiles. Experiments are ordered by descending mean of their 5 folds accuracy(↑) respectively. The mean (Ø) is shown after the experiment name in the labels of the horizontal axis. The dashed red line indicates the threshold of the double of random results. The data shows the first 6 experiments clearly over the threshold line in all 5 folds respectively. Outliers that differ significantly from the other folds are depicted as circles. Interestingly, the grouped \"O\"-channels perform only slightly below the threshold with a mean of 44.94% even though the group only contains 3 channels. The Emotiv-channels perform best with a mean of 90,3% in accuracy with only 12 channels as input.\n\nlearned which eye movements correspond to which videos and what label the video corresponded to. For detailed accuracy of each fold of the best single channel results, see Table  3 . A box-plot overview of all conducted experiments (including the best four single channel results) and their means over all 5 folds can be seen in Figure  5 . Channel 33 clearly lies in the top 7 results of all experiments.\n\nFor completeness, we experimented with cross-person setting by leaving 6 people out for each fold for validation. For channel 33 this resulted in a mean accuracy of 71.9%.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Classification -Channel Subsets Experiments",
      "text": "All Data -When all available channels were utilized, the classification accuracy reached approximately 80%.\n\nMuse-12/8/4 Electrodes -Using 12 electrodes resulted in one of the best-performing configurations with a maximum accuracy of 84%. Reducing the input to 8 electrodes caused an 8% drop in accuracy, reaching 76%. However, further reducing the input to 4 electrodes led to a more substantial drop in accuracy, falling by approximately 20% to 58-56%. This indicates that the model's performance is more resilient when reducing from 12 to 8 electrodes than when reducing from 8 to 4 electrodes. See Table  4  for comparison of accuracy across all folds and the corresponding number of channels for each channel-name. Emotiv -The channels corresponding to those used in the Emotiv device achieved the best classification accuracy at 91.50%. This suggests that this specific channel subset performs exceptionally well, even with significantly fewer (12 of 40) channels than the full dataset and shows one of the best trade Figure  6 . Explained variance for the best 12 channels obtained through PCA, with channels sorted in descending order of explained variance. The line shows the cumulative explained variance across the sorted channels. The most relevant features according to the explained variance is the hEOG (channel 33). It can be seen that the 12 channels only cumulate in a explained variance of 60%; more than half of the channels are needed to obtain a cumulative explained variance above the 80% mark.\n\noffs between accuracy and used channels.\n\nPCA -The explained variance for each channel obtained through PCA is shown in Figure  6 . The ordering of explained variance across channels does not align with the ranking obtained from single-channel testing. However, the highest-ranked channel (hEOG -channel 33) is consistent between both methods. Grouping the top 12 channels resulted in a maximum accuracy of 41.04%, while grouping the top 4 led to a slightly better performance with 43.16%. Consequently, no configuration based on PCA findings is considered relevant. These results are explainable since PCA is a linear feature reduction method  [2] , while the relationships between the features are not necessarily linear. EEG-only and Non-EEG only -The EEG-only configuration achieved 91.06% accuracy, ranking second-best with no significant difference to the top result. However, this setting required more than double the number of channels compared to the Emotiv configuration for a similar outcome. In contrast, the non-EEG channel setup did not produce sufficient accuracy, despite containing channel 33.\n\nGrouped Channels -Grouping the channels by location of the electrode led to ambivalent results, shown in Table  5 . The groups frontal (f), parietal (p), and between central and parietal (cp) showed at least one fold with sufficient accuracy. But there is at least one fold per group where no generalization is developed. This might show that EEG features are less obvious, which might lead to an outlierprone prediction when less training data is available. Furthermore, there are differences in the number of channels that are part of each group. All groups with relevant results are on the upper end of channel numbers, whereas the channels containing fewer channels tend to be less performing. For example, the comparison of group fc and cp shows that a group with more channels does not directly yield higher accuracy but seems more dependent on the input. Training data limitations could be a reason for more stable training with more channels; however, as the single-channel experiment showed, the used architecture is capable of performing a stable classification with only one channel.\n\nSee Figure  5  for a better insight into the experiments and a visual direct comparison between the experiments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Classification -Differences Between Sam And Vaq Labels",
      "text": "Using the SAM labels resulted in mostly comparable accuracy to the VAQ label results. Taking into account that the SAM labels are more dependent on each participant and therefore have a more volatile influence on the results, the results for all channels, EEG-only, Muse-12 and Muse-8 are viewed as similar to the results of the VAQ labels. In the Emotiv and Muse-4b setup there is a similar tendency of fold-performance in the SAM results and the VAQ results, respectively. However the overall difference of the Emotiv-Sam and Emotiv-VAQ is much larger than the overall difference of the Muse-4b-SAM and Muse-4b-VAQ. For the Muse-4b setup an explanation could be, that the training already seems to be unstable in the original setup (see fold 4 of Muse-4b in Table  4 ) and the differences and uncertainties of the labeling persons are disadvantageous. This is even more reasonable as 671 out of the 1280 total scaleograms were grouped into a different quadrant by the SAM labels than by the given VAQ.\n\nChannel 33 showed no generalization in this setup. It is possible, that the horizontal eye movement (channel 33) is corresponding to the dynamics of the presented videos more than to the experienced feelings. The drastic differing of results could support this hypothesis, if the SAM labels are assumed to be more accurate to the experienced emotions. However eye movement is used to predict emotional states in  [17]  ,  [41]  with high accuracy. The open question whether the experienced emotions or the video dynamics are crucial for the prediction is also relevant for the EEG channels. An interesting result regarding this question is the performance of the Occipital Lobe group 'o' in Table  5  with 56% accuracy for 3 channels. As the Occipital Lobe is associated as center of processing visual input  [19] .\n\nDetailed results are shown in Table  6 , a box-plot overview is provided in Figure  7 .   6  are presented visually as box-plots of their 5 folds and ordered by descending mean accuracy(↑). The mean (Ø) is shown after the experiment name in the labels of the horizontal axis. The dashed red line indicates the threshold of the double of random results. In contrast to the VAQ-label experiments seen in Figure  5  only four experiments clearly exceed the threshold line with all 5 folds.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Regression Experiments",
      "text": "In the regression experiments, the EEG-only setup produced the best performance, achieving the lowest RMSE of 0.57. Unlike the classification task, the Emotiv setup did not perform as well, reaching an RMSE of 0.88, which is still the second-best result.\n\nThe experiments with Muse-12 and Muse-8 showed similar tendencies comparing to the classification results, with a small performance gap between them. As observed in the classification task, the performance drop when reducing to 4 channels was more substantial, indicating a larger disparity in accuracy when fewer channels were used.\n\nFor channel 33 the results were not considered relevant but are still noticeably better than random. Here it should be taken into account that one channel only provides a limited amount of data for training.\n\nA detailed list of results is shown in Table  7 , a box-plot overview is provided in Figure  8 .   7  are presented visually as box-plots of their 5 folds and ordered by ascending mean RMSE(↓). The EEG-only and Emotiv experiments show best results, with Muse-12 close to the results of Emotiv.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We conducted a classification and regression with different subsets of the DEAP dataset, up to testing only single input EEG channels. In conclusion, our research underscores the effectiveness of pairing scaleograms and vision transformer as well as predicting emotional states with only subsets of a common EEG.\n\nWe evaluated the trade-off between minimal input and prediction performance, evaluating models trained on varying numbers of channels. Through these analyses, we showed different configurations that can be used for portable EEG devices, enabling more accessible, reliable emotion prediction technologies, while also offering significant contributions to the broader research community.\n\nThe performance of our model showed some ambiguities with different experiment setups. Pairing subsets that achieved a high accuracy with additional channels does not always lead to better performances. Most visible was this phenomenon in experiments where all available inputs were used but did not lead to best model performance. The underlying mechanisms of this non-superpositional behavior and the factors contributing to the amplification of accuracy across certain channels remain open questions for further investigation in the field of explainable AI. An interdisciplinary approach, particularly one integrating insights from neuroscience, will be highly beneficial in advancing our understanding of these phenomena.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Concept diagram for exemplary approach with 4 chan-",
      "page": 1
    },
    {
      "caption": "Figure 1: By pairing CWT with the ViT,",
      "page": 2
    },
    {
      "caption": "Figure 2: Russell’s Circumplex Model of Affect [8] showing four",
      "page": 2
    },
    {
      "caption": "Figure 3: International 10-20-System for EEG-channel placement",
      "page": 2
    },
    {
      "caption": "Figure 2: ). The fol-",
      "page": 3
    },
    {
      "caption": "Figure 1: Although the CWT introduces a slight loss of informa-",
      "page": 4
    },
    {
      "caption": "Figure 4: EEG Placement and the corresponding brain region [28].",
      "page": 4
    },
    {
      "caption": "Figure 4: for the different regions.",
      "page": 5
    },
    {
      "caption": "Figure 5: Classification experiments with VAQ labels - labels given by the experimenters. Results of Table 3, 6 and 5 are presented",
      "page": 6
    },
    {
      "caption": "Figure 6: Explained variance for the best 12 channels obtained",
      "page": 6
    },
    {
      "caption": "Figure 6: The ordering of ex-",
      "page": 6
    },
    {
      "caption": "Figure 5: for a better insight into the experiments and",
      "page": 7
    },
    {
      "caption": "Figure 7: Box-plot and mean of the classification experiments with",
      "page": 8
    },
    {
      "caption": "Figure 5: only four experiments clearly exceed the threshold line",
      "page": 8
    },
    {
      "caption": "Figure 8: Figure 8. Box-plot and mean RMSE of the best seven regression",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Channels": "channel 33",
          "Fold 1": "70.38",
          "Fold 2": "64.36",
          "Fold 3": "70.00",
          "Fold 4": "72.44",
          "Fold 5 Mean": "71.67"
        },
        {
          "Channels": "channel 34",
          "Fold 1": "37.05",
          "Fold 2": "38.33",
          "Fold 3": "31.41",
          "Fold 4": "34.62",
          "Fold 5 Mean": "37.31"
        },
        {
          "Channels": "channel 37",
          "Fold 1": "28.21",
          "Fold 2": "30.26",
          "Fold 3": "36.15",
          "Fold 4": "33.97",
          "Fold 5 Mean": "34.36"
        },
        {
          "Channels": "channel 33\ncross person",
          "Fold 1": "71.79",
          "Fold 2": "68.57",
          "Fold 3": "61.25",
          "Fold 4": "69.17",
          "Fold 5 Mean": "78.33"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 5: Accuracy (↑) for experiments, grouped by electrode",
      "data": [
        {
          "Channels": "all",
          "#": "40",
          "Fold 1": "78.12",
          "Fold 2": "78.44",
          "Fold 3": "78.39",
          "Fold 4": "80.23",
          "Fold 5 Mean": "79.61"
        },
        {
          "Channels": "eeg-only",
          "#": "32",
          "Fold 1": "88.94",
          "Fold 2": "89.18",
          "Fold 3": "91.06",
          "Fold 4": "90.37",
          "Fold 5 Mean": "89.99"
        },
        {
          "Channels": "non-eeg",
          "#": "8",
          "Fold 1": "35.51",
          "Fold 2": "36.13",
          "Fold 3": "35.09",
          "Fold 4": "34.10",
          "Fold 5 Mean": "34.48"
        },
        {
          "Channels": "Emotiv",
          "#": "12",
          "Fold 1": "89.70",
          "Fold 2": "89.56",
          "Fold 3": "91.57",
          "Fold 4": "91.28",
          "Fold 5 Mean": "89.37"
        },
        {
          "Channels": "Muse-12",
          "#": "12",
          "Fold 1": "83.25",
          "Fold 2": "81.33",
          "Fold 3": "84.25",
          "Fold 4": "79.77",
          "Fold 5 Mean": "81.10"
        },
        {
          "Channels": "Muse-8",
          "#": "8",
          "Fold 1": "73.35",
          "Fold 2": "74.30",
          "Fold 3": "76.44",
          "Fold 4": "66.01",
          "Fold 5 Mean": "76.38"
        },
        {
          "Channels": "Muse-4a",
          "#": "4",
          "Fold 1": "40.97",
          "Fold 2": "52.62",
          "Fold 3": "43.50",
          "Fold 4": "58.06",
          "Fold 5 Mean": "47.48"
        },
        {
          "Channels": "Muse-4b",
          "#": "4",
          "Fold 1": "47.23",
          "Fold 2": "53.45",
          "Fold 3": "56.65",
          "Fold 4": "24.85",
          "Fold 5 Mean": "49.37"
        },
        {
          "Channels": "PCA-12",
          "#": "12",
          "Fold 1": "38.90",
          "Fold 2": "36.88",
          "Fold 3": "41.04",
          "Fold 4": "36.98",
          "Fold 5 Mean": "36.67"
        },
        {
          "Channels": "PCA-4",
          "#": "4",
          "Fold 1": "41.09",
          "Fold 2": "42.23",
          "Fold 3": "43.16",
          "Fold 4": "42.28",
          "Fold 5 Mean": "41.21"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Accuracy (↑) for experiments, grouped by electrode",
      "data": [
        {
          "Channels": "t",
          "#": "2",
          "Fold 1": "24.62",
          "Fold 2": "27.89",
          "Fold 3": "31.15",
          "Fold 4": "31.15",
          "Fold 5 Mean": "31.15"
        },
        {
          "Channels": "f",
          "#": "5",
          "Fold 1": "25.00",
          "Fold 2": "66.88",
          "Fold 3": "67.66",
          "Fold 4": "66.02",
          "Fold 5 Mean": "60.70"
        },
        {
          "Channels": "c",
          "#": "3",
          "Fold 1": "28.51",
          "Fold 2": "46.33",
          "Fold 3": "37.11",
          "Fold 4": "30.26",
          "Fold 5 Mean": "39.58"
        },
        {
          "Channels": "fp",
          "#": "2",
          "Fold 1": "30.96",
          "Fold 2": "30.96",
          "Fold 3": "31.73",
          "Fold 4": "31.15",
          "Fold 5 Mean": "31.15"
        },
        {
          "Channels": "af",
          "#": "2",
          "Fold 1": "30.96",
          "Fold 2": "34.04",
          "Fold 3": "24.62",
          "Fold 4": "24.62",
          "Fold 5 Mean": "24.62"
        },
        {
          "Channels": "po",
          "#": "2",
          "Fold 1": "31.15",
          "Fold 2": "29.81",
          "Fold 3": "31.15",
          "Fold 4": "31.15",
          "Fold 5 Mean": "24.62"
        },
        {
          "Channels": "fc",
          "#": "5",
          "Fold 1": "40.92",
          "Fold 2": "52.82",
          "Fold 3": "51.65",
          "Fold 4": "60.87",
          "Fold 5 Mean": "46.85"
        },
        {
          "Channels": "cp",
          "#": "4",
          "Fold 1": "55.29",
          "Fold 2": "30.39",
          "Fold 3": "47.48",
          "Fold 4": "66.17",
          "Fold 5 Mean": "62.57"
        },
        {
          "Channels": "o",
          "#": "3",
          "Fold 1": "56.01",
          "Fold 2": "53.60",
          "Fold 3": "41.14",
          "Fold 4": "30.26",
          "Fold 5 Mean": "43.67"
        },
        {
          "Channels": "p",
          "#": "5",
          "Fold 1": "65.86",
          "Fold 2": "64.84",
          "Fold 3": "60.78",
          "Fold 4": "24.92",
          "Fold 5 Mean": "30.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Accuracy (↑) for experiments, grouped by electrode",
      "data": [
        {
          "Channels": "all",
          "#": "40",
          "Fold 1": "76.33",
          "Fold 2": "77.04",
          "Fold 3": "78.44",
          "Fold 4": "78.69",
          "Fold 5": "78.19",
          "Diff.": "-1.84"
        },
        {
          "Channels": "eeg-only",
          "#": "32",
          "Fold 1": "88.06",
          "Fold 2": "91.18",
          "Fold 3": "89.68",
          "Fold 4": "89.40",
          "Fold 5": "91.63",
          "Diff.": "+0.57"
        },
        {
          "Channels": "Emotiv",
          "#": "12",
          "Fold 1": "80.71",
          "Fold 2": "79.87",
          "Fold 3": "80.45",
          "Fold 4": "81.40",
          "Fold 5": "79.48",
          "Diff.": "-10,17"
        },
        {
          "Channels": "Muse-12",
          "#": "12",
          "Fold 1": "83.05",
          "Fold 2": "82.82",
          "Fold 3": "81.10",
          "Fold 4": "81.82",
          "Fold 5": "83.51",
          "Diff.": "-0.74"
        },
        {
          "Channels": "Muse-8",
          "#": "8",
          "Fold 1": "70.91",
          "Fold 2": "35.85",
          "Fold 3": "73.28",
          "Fold 4": "76.18",
          "Fold 5": "72.29",
          "Diff.": "-0.26"
        },
        {
          "Channels": "Muse-4b",
          "#": "4",
          "Fold 1": "36.12",
          "Fold 2": "36.12",
          "Fold 3": "36.12",
          "Fold 4": "60.34",
          "Fold 5": "36.21",
          "Diff.": "+3.69"
        },
        {
          "Channels": "Channel 33",
          "#": "1",
          "Fold 1": "40.13",
          "Fold 2": "43.21",
          "Fold 3": "45.90",
          "Fold 4": "45.51",
          "Fold 5": "42.56",
          "Diff.": "-26.54"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Channels": "all",
          "#": "40",
          "Fold 1": "1.074",
          "Fold 2": "1.140",
          "Fold 3": "1.119",
          "Fold 4": "1.190",
          "Fold 5 Mean": "1.107"
        },
        {
          "Channels": "eeg-only",
          "#": "32",
          "Fold 1": "0.575",
          "Fold 2": "0.611",
          "Fold 3": "0.624",
          "Fold 4": "0.612",
          "Fold 5 Mean": "0.604"
        },
        {
          "Channels": "Muse-12",
          "#": "12",
          "Fold 1": "1.156",
          "Fold 2": "0.926",
          "Fold 3": "1.020",
          "Fold 4": "0.913",
          "Fold 5 Mean": "1.005"
        },
        {
          "Channels": "Muse-8",
          "#": "8",
          "Fold 1": "1.124",
          "Fold 2": "1.147",
          "Fold 3": "1.296",
          "Fold 4": "1.434",
          "Fold 5 Mean": "1.223"
        },
        {
          "Channels": "Muse-4b",
          "#": "4",
          "Fold 1": "2.081",
          "Fold 2": "1.615",
          "Fold 3": "2.116",
          "Fold 4": "2.046",
          "Fold 5 Mean": "1.624"
        },
        {
          "Channels": "Emotiv",
          "#": "12",
          "Fold 1": "0.994",
          "Fold 2": "1.079",
          "Fold 3": "0.989",
          "Fold 4": "0.889",
          "Fold 5 Mean": "0.924"
        },
        {
          "Channels": "Channel 33",
          "#": "1",
          "Fold 1": "1.926",
          "Fold 2": "2.011",
          "Fold 3": "2.093",
          "Fold 4": "1.958",
          "Fold 5 Mean": "2.082"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in EEG signals using the continuous wavelet transform and CNNs",
      "authors": [
        "Oscar Almanza-Conejo",
        "Luz Almanza-Ojeda",
        "Jose Luis Contreras-Hernandez",
        "Mario Ibarra-Manzano"
      ],
      "year": "2023",
      "venue": "Emotion recognition in EEG signals using the continuous wavelet transform and CNNs"
    },
    {
      "citation_id": "2",
      "title": "Conceptual and empirical comparison of dimensionality reduction algorithms",
      "authors": [
        "Farzana Anowar",
        "Samira Sadaoui",
        "Bassant Selim"
      ],
      "year": "2021",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "3",
      "title": "Subject independent emotion recognition using EEG signals employing attention driven neural networks",
      "authors": [
        "Aniket Arjun",
        "Mahesh Singh Rajpoot",
        "Panicker"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition using effective connectivity and pre-trained convolutional neural networks in EEG signals",
      "authors": [
        "Sara Bagherzadeh",
        "Keivan Maghooli",
        "Ahmad Shalbaf",
        "Arash Maghsoudi"
      ],
      "year": "2022",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition using continuous wavelet transform and ensemble of convolutional neural networks through transfer learning from electroencephalogram signal",
      "authors": [
        "Sara Bagherzadeh",
        "Keivan Maghooli",
        "Ahmad Shalbaf",
        "Arash Maghsoudi"
      ],
      "year": "2023",
      "venue": "Frontiers in Biomedical Technologies"
    },
    {
      "citation_id": "6",
      "title": "What Is Complex/Emotional About Emotional Complexity?",
      "authors": [
        "Raul Berrios"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "7",
      "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition",
      "authors": [
        "Yunliang Chen",
        "Jungseock Joo"
      ],
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "8",
      "title": "Chirag Dua, Mohit Dalawat, and Divyashikha Sethia",
      "authors": [
        "Harsh Dabas",
        "Chaitanya Sethi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "EEGbased emotion recognition: Review of commercial EEG devices and machine learning techniques",
      "authors": [
        "Didar Dadebayev",
        "Wei Goh",
        "Ee Xion"
      ],
      "year": "2022",
      "venue": "EEGbased emotion recognition: Review of commercial EEG devices and machine learning techniques"
    },
    {
      "citation_id": "10",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. International Conference on Learning Representations",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. International Conference on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "12",
      "title": "SVM classification of CWT signal features for predicting sudden cardiac death",
      "authors": [
        "Towfeeq Fairooz",
        "Hedi Khammari"
      ],
      "year": "2016",
      "venue": "Biomedical Physics & Engineering Express"
    },
    {
      "citation_id": "13",
      "title": "EEG Emotion Classification Based on Graph Convolutional Network. Applied Sciences",
      "authors": [
        "Zhiqiang Fan",
        "Fangyue Chen",
        "Xiaokai Xia",
        "Yu Liu"
      ],
      "year": "2024",
      "venue": "EEG Emotion Classification Based on Graph Convolutional Network. Applied Sciences"
    },
    {
      "citation_id": "14",
      "title": "Emotion Recognition in Valence-Arousal Space from Multi-channel EEG data and Wavelet based Deep Learning Framework",
      "authors": [
        "Divya Garg",
        "Gyanendra Verma"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "15",
      "title": "Decomposition of Hardy Functions into Square Integrable Wavelets of Constant Shape",
      "authors": [
        "A Grossmann",
        "J Morlet"
      ],
      "year": "1984",
      "venue": "SIAM Journal on Mathematical Analysis"
    },
    {
      "citation_id": "16",
      "title": "Mayur Dahyabhai Chopda, and Ram Bilas Pachori. Cross-subject emotion recognition using flexible analytic wavelet transform from eeg signals",
      "authors": [
        "Vipin Gupta"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "17",
      "title": "Exploration of the effects of task-related fatigue on eye-motion features and its value in improving driver fatigue-related technology",
      "authors": [
        "Xinyun Hu",
        "Gabriel Lodewijks"
      ],
      "year": "2021",
      "venue": "Transportation Research Part F: Traffic Psychology and Behaviour"
    },
    {
      "citation_id": "18",
      "title": "Wavelet analysis based classification of emotion from eeg signal",
      "year": "2019",
      "venue": "International Conference on Electrical"
    },
    {
      "citation_id": "19",
      "title": "Neuroanatomy",
      "authors": [
        "Kinaan Javed",
        "Vamsi Reddy",
        "Forshing Lui"
      ],
      "year": "2024",
      "venue": "Cerebral Cortex. In StatPearls. StatPearls Publishing"
    },
    {
      "citation_id": "20",
      "title": "Removal of Artifacts from EEG Signals: A Review",
      "authors": [
        "Xiao Jiang",
        "Gui-Bin Bian",
        "Zean Tian"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "21",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "22",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "23",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Deep learning for affective computing: Text-based emotion recognition in decision support",
      "authors": [
        "Bernhard Kratzwald",
        "Suzana Ilić",
        "Mathias Kraus",
        "Stefan Feuerriegel",
        "Helmut Prendinger"
      ],
      "year": "2018",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "25",
      "title": "Correlation Analysis to Identify the Effective Data in Machine Learning: Prediction of Depressive Disorder and Emotion States",
      "authors": [
        "Sunil Kumar",
        "Ilyoung Chong"
      ],
      "year": "2018",
      "venue": "International Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition from multichannel EEG data through Convolutional Recurrent Neural Network",
      "authors": [
        "Xiang Li",
        "Dawei Song",
        "Peng Zhang",
        "Guangliang Yu",
        "Yuexian Hou",
        "Bin Hu"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Bioinformatics and Biomedicine"
    },
    {
      "citation_id": "27",
      "title": "Multi-channel EEG-based emotion recognition via a multi-level features guided capsule network",
      "authors": [
        "Yu Liu",
        "Yufeng Ding",
        "Chang Li",
        "Juan Cheng",
        "Rencheng Song",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "28",
      "title": "Cortical and autonomic responses during staged Taoist meditation: Two distinct meditation strategies",
      "authors": [
        "Nikolai Smetanin",
        "Maria Volodina"
      ],
      "year": "2021",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "29",
      "title": "EEG-Based Emotion Recognition Using Genetic Algorithm Optimized Multi-Layer Perceptron",
      "authors": [
        "Shyam Marjit",
        "Upasana Talukdar",
        "M Shyamanta",
        "Hazarika"
      ],
      "year": "2021",
      "venue": "International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation"
    },
    {
      "citation_id": "30",
      "title": "Wave propagation and sampling theory-Part I: Complex signal and scattering in multilayered media",
      "authors": [
        "J Morlet",
        "G Arens",
        "E Fourgeau",
        "D Glard"
      ],
      "year": "1982",
      "venue": "GEOPHYSICS"
    },
    {
      "citation_id": "31",
      "title": "The five percent electrode system for high-resolution EEG and ERP measurements",
      "authors": [
        "Robert Oostenveld",
        "Peter Praamstra"
      ],
      "year": "2001",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "32",
      "title": "Rutika Rajesh Bankar, Dhanashree Yende, and Aditya Kiran Patil. From face detection to emotion recognition on the framework of Raspberry pi and galvanic skin response sensor for visual and physiological biosignals",
      "authors": [
        "Varsha Kiran Patil",
        "R Vijaya",
        "Shreiya Pawar",
        "Randive"
      ],
      "year": "2023",
      "venue": "Journal of Electrical Systems and Information Technology"
    },
    {
      "citation_id": "33",
      "title": "EEG-based cross-subject emotion recognition using multi-source domain transfer learning",
      "authors": [
        "Jie Quan",
        "Ying Li",
        "Lingyue Wang",
        "Renjie He",
        "Shuo Yang",
        "Lei Guo"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "34",
      "title": "A Circumplex Model of Affect",
      "authors": [
        "James Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "35",
      "title": "Deep learning approach for emotion recognition from human body movements with feedforward deep convolution neural networks",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "36",
      "title": "Datasets for valence and arousal inference: A survey",
      "authors": [
        "Helen Schneider",
        "Svetlana Pavlitska",
        "Helen Gremmelmaier",
        "Marius Zöllner"
      ],
      "year": "2025",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "37",
      "title": "Dive into Ambiguity: Latent Distribution Mining and Pairwise Uncertainty Estimation for Facial Expression Recognition",
      "authors": [
        "Jiahui She",
        "Yibo Hu",
        "Hailin Shi",
        "Jun Wang",
        "Qiu Shen",
        "Tao Mei"
      ],
      "year": "2021",
      "venue": "Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Wearable Emotion Recognition Using Heart Rate Data from a Smart Bracelet",
      "authors": [
        "Lin Shu",
        "Yang Yu",
        "Wenzhuo Chen",
        "Haoqiang Hua",
        "Qin Li",
        "Jianxiu Jin",
        "Xiangmin Xu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "39",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "EEG-Based Emotion Recognition via Channel-Wise Attention and Self Attention",
      "authors": [
        "Wei Tao",
        "Chang Li",
        "Rencheng Song",
        "Juan Cheng",
        "Yu Liu",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Eye-Tracking Analysis for Emotion Recognition",
      "authors": [
        "Paweł Tarnowski",
        "Marcin Kołodziej",
        "Andrzej Majkowski",
        "Jan Remigiusz",
        "Rak"
      ],
      "year": "2020",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "42",
      "title": "Attention Is All You Need. 31st Conference on Neural Information Processing Systems",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2023",
      "venue": "Attention Is All You Need. 31st Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Cage: Circumplex affect guided expression inference",
      "authors": [
        "Niklas Wagner",
        "Felix Mätzler",
        "Samed Vossberg",
        "Helen Schneider",
        "Svetlana Pavlitska",
        "J Zöllner"
      ],
      "year": "2024",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "44",
      "title": "Transformers for EEG-Based Emotion Recognition: A Hierarchical Spatial Information Learning Model",
      "authors": [
        "Zhe Wang",
        "Yongxiong Wang",
        "Chuanfei Hu",
        "Zhong Yin",
        "Yu Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "45",
      "title": "Predicting Emotion Reactions for Human-Computer Conversation: A Variational Approach",
      "authors": [
        "Rui Zhang",
        "Zhenyu Wang",
        "Zhenhua Huang",
        "Li Li",
        "Mengdan Zheng"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Human-Machine Systems"
    },
    {
      "citation_id": "46",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "47",
      "title": "Continuous valence prediction using recurrent neural networks with facial expressions and EEG signals",
      "authors": [
        "S Dogancan",
        "Mustafa Sert"
      ],
      "year": "2018",
      "venue": "Signal Processing and Communications Applications Conference"
    }
  ]
}