{
  "paper_id": "2207.11900v6",
  "title": "Ga2Mif: Graph And Attention Based Two-Stage Multi-Source Information Fusion For Conversational Emotion Detection",
  "published": "2022-07-25T04:22:41Z",
  "authors": [
    "Jiang Li",
    "Xiaoping Wang",
    "Guoqing Lv",
    "Zhigang Zeng"
  ],
  "keywords": [
    "Emotion recognition in conversation",
    "crossmodal interactions",
    "multimodal fusion",
    "graph neural networks",
    "multi-head attention mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Emotion Recognition in Conversation (ERC) plays an influential role in the field of human-computer interaction and conversational robotics since it can motivate machines to provide empathetic services. Multimodal data modeling is an up-and-coming research area in recent years, which is inspired by human capability to integrate multiple senses. Several graph-based approaches claim to capture interactive information between modalities, but the heterogeneity of multimodal data makes these methods prohibit optimal solutions. In this work, we introduce a multimodal fusion approach named Graph and Attention based Two-stage Multi-source Information Fusion (GA2MIF) for emotion detection in conversation. Our proposed method circumvents the problem of taking heterogeneous graph as input to the model while eliminating complex redundant connections in the construction of graph. GA2MIF focuses on contextual modeling and cross-modal modeling through leveraging Multi-head Directed Graph ATtention networks (MDGATs) and Multi-head Pairwise Cross-modal ATtention networks (MP-CATs), respectively. Extensive experiments on two public datasets (i.e., IEMOCAP and MELD) demonstrate that the proposed GA2MIF has the capacity to validly capture intra-modal longrange contextual information and inter-modal complementary information, as well as outperforms the prevalent State-Of-The-Art (SOTA) models by a remarkable margin.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "I N recent years, human-computer interaction and intelligent robotics technologies are transforming science fiction into reality. However, there are numerous challenges to make machines interact with people in a natural manner. The ability to make machines empathize like humans, i.e., to recognize the emotional states of others and respond correspondingly, is particularly crucial in the field of social robotics  [1] . In addition, empathy can enhance the interaction between human and computer to provide superior artificial intelligence services to others. Accurately recognizing the emotional states of others The authors are with the School of Artificial Intelligence and Automation and the Key Laboratory of Image Processing and Intelligent Control of Education Ministry of China, Huazhong University of Science and Technology, Wuhan 430074, China (e-mail:lijfrank@hust.edu.cn; wangxiaoping@hust.edu.cn; guoqinglv@hust.edu.cn; zgzeng@hust.edu.cn).\n\nDigital Object Identifier 10.1109/TAFFC.2023.3261279 is a prerequisite for generating empathic responses, which is also a core research thrust in the field of cognition and behavior. Thus, emotion recognition plays an instrumental role in numerous domains and has attracted extensive attention from research scholars.\n\nEmotion Recognition in Conversation (ERC), also called conversational emotion detection, aims to detect emotional state of a speaker based on the signals that he or she expresses (e.g., the signals include text, audio, or facial expressions). ERC has potential applications in many fields, such as: (a) Disease Diagnosis  [2] , assisting the doctor in diagnosing disease by identifying the emotional state of the patient when talking to a psychologist. (b) Opinion Mining  [3] , improving the public's trust in government departments or institutions by analyzing online public opinion and measuring the public's experience of policies or services. (c) Conversation Generation  [4] , enhancing significantly the usability of dialogue systems and the satisfaction of customers through injecting emotions into the given model. (d) Recommender System  [5] , inferring the user's potential preferences by identifying his or her emotional states during historical chats with customer service. ERC systems can provide customized services to users and enhance the quality of empathetic interactions with users.\n\nMost of existing ERC models mainly employ text-modal data as input. DialogueGCN  [6]  constructs the conversation as a graph to extract long-distance contextual information, where each utterance is related to surrounding utterances. HiGRU  [7]  adopts lower-level and upper-level Gated Recurrent Units (GRUs) to tackle dilemmas in utterance-level conversational emotion recognition. COSMIC  [8]  leverages different elements of external commonsense knowledge such as mental states, events, and causal relations to detect utterance-level emotion in conversation. DialogueCRN  [9]  attempts to understand conversational context by exploring cognitive factors, which analogous to the unique cognitive thinking of human. Several efforts on modeling based on data from acoustic modalities are also available. Gat et al.  [10]  introduce a gradient-based adversary learning model that is effective in both speakerindependent and speaker-dependent situations for speech emotion recognition task. Jalal et al.  [11]  propose a speech emotion recognition approach based on both Long Short-Term Memory (LSTM) network and Convolutional Neural Network (CNN) to explore the impact of acoustic cues on recognition results. These techniques, anyway, only accept unimodal signal sources as inputs, which may limit the performance of the model. For instance, the model will have trouble properly recognizing emotion if the signal and emotional state of current modality do not match. No, it's all right. There's nothing wrong in that, you know.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Acoustics Acoustics Vision Vision Text",
      "text": "It's really lovely here. The air is sweet.\n\nNo. I'm not sorry. It's-\n\nWell, for one thing, your mother so much has told me to go.\n\nWell-What?\n\nWell, you've been embarrassed ever since I came.\n\nYeah. I know. It went out of style, didn't it?\n\nYou're not sorry you came? Humans are capable of multi-sensory integration, i.e., they can perceive surroundings from multiple senses. Intuitively, multimodal data from different sources can enhance the performance of machine learning models. Fig.  1  shows a multimodal conversation scene, where the conversational contents are derived from text, acoustic and visual modalities. Multimodal ERC is a burgeoning field of research and has recently been gaining momentum. There are, however, just a few multi-modality based conversational emotion recognition models. BC-LSTM  [12]  proposes a multimodal model based on a two-directional LSTM for extracting the contextual information of the current utterance. CMN  [13]  conducts temporal sequence modeling on utterance histories by GRUs and employs attention networks to select the most useful historical utterances. DialogueRNN  [14]  is a Recurrent Neural Network (RNN) based ERC framework that considers the characteristic of speaker for each utterance to provide more reliable contextual information. Nevertheless, these methods only concatenate multimodal features in a direct manner, which results in the inter-modal information not being able to interact. Hu et al.  [15]  propose a graph-based multimodal ERC model called MMGCN that claims to effectively not only exploit multimodal dependencies, but also model interand intra-speaker dependencies. Experimental results show that MMGCN achieves excellent performance on two public benchmark datasets. Although Graph Neural Networks (GNNs) show excellent performance in homogeneous graphs, they usually obtain suboptimal or even worse results in heterogeneous networks. MMGCN constructs a large graph by treating utterances from different modalities as nodes of the same type, which contradicts the premise that the input to GNN is a homogeneous graph. MMGCN, moreover, simply connects all utterances in each modality to build a complete graph, which undoubtedly brings too much noise (i.e., useless connections) to the model.\n\nIn order to address the above-mentioned problems, we propose a Graph and Attention based Two-stage Multi-source Information Fusion (GA2MIF) approach for conversational emotion recognition. The proposed GA2MIF is a new multimodal fusion framework whose network structure mainly consists of graph attention networks and multi-head attention networks. First, we adopt three Multi-head Directed Graph ATtention networks (MDGATs) to extract intra-modal local and long-range contextual information for each modality; then, we leverage three Multi-head Pairwise Cross-modal ATtention networks (MPCATs) to model cross-modal interactions in pairs and extract inter-modal complementary information. Our contributions are mainly as follows: show that GA2MIF achieves optimal performance with the accuracy of 69.75% and 61.65%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotion Recognition",
      "text": "Emotion recognition is an interdisciplinary realm that has attracted active research and attention in the areas of emotion understanding systems, opinion mining, and emotion generation. Broadly speaking, existing works in this field can be divided into two categories based on data sources: unimodalbased emotion recognition and multimodal-based emotion recognition.\n\n1) Unimodal-Based Emotion Recognition: Unimodal emotion recognition is to judge emotional state of object by encoding input data from single modality, which can be divided into primarily three types: text-based methods, audiobased methods and vision-based methods.\n\nText-based Methods have been the most prevalent unimodal emotion recognition since the development of natural language processing. DialogueGCN  [6]  models self-and inter-speaker dependencies between speakers to promote context understanding for utterance-level sentiment analysis in conversations. Ishiwatari et al.  [16]  not only models the dependency of the speaker, but also models the sequential information through relational position encoding. Aiming to model conversational data, DialogXL  [17]  replaces self-attention in XLNet with conversation-aware self-attention to extract the information of intra-and inter-speaker dependencies. DAG-ERC  [18]  designs a directed acyclic graph neural network to recognize emotion in conversation, where nodes represent utterances and edges represent connections between utterances. HiGRU  [7]  takes advantage of the lower-level GRU to learn individual utterance embeddings and the upper-level GRU to capture contexts of utterances. COSMIC  [8]  takes different elements of commonsense into account and makes use of them to learn interactions between speakers. DialogueCRN  [9]  designs multi-turn reasoning modules to perceive and combine clues, which can sufficiently extract speaker-level and situation-level context information. Audio-based Methods, often referred to as Speech Emotion Recognition (SER), estimate a speaker's emotional state by means of analyzing his/her speech. Gat et al.  [10]  introduce a general framework for normalizing the features of speakers while addressing the problem of small dataset settings. Jalal et al.  [11]  argue that smaller acoustic contexts are crucial in expressing emotion and propose a bidirectional LSTM-and CNN-based SER approach. Guo et al.  [19]  propose a spectro-temporal-channel attention module that offers different weights for frequency, time and channel-wise features to capture more expressive information. Vision-based Methods focus on emotion recognition based on facial expression, which is an essential field in affective computing. Jeong et al.  [20]  propose a fast facial emotion recognition method for recognizing a driver's emotion in realtime. Wang et al.  [21]  leverage stationary wavelet entropy to extract features and employ the Jaya algorithm to train facial emotion recognition model. Khaireddin et al.  [22]  fine-tune the hyperparameters of VGGNet architecture to achieve the highest single-network classification accuracy on the FER2013 dataset.\n\n2) Multimodal-Based Emotion Recognition: Unimodal information is insufficient and is easily affected by external factors, such as blocked facial expressions and disturbed voice. In view of complementarity between different modalities, research on multimodal emotion recognition has received increasing attention  [23] . MFN  [24]  makes use of a new neural structure based on multi-view sequence learning to consider both view-specific interactions and cross-view interactions. BC-LSTM  [12]  proposes a multimodal fusion method that captures contextual information of the utterance through a LSTM network. CMN  [13]  fuses audio, visual and textual features, and it leverages a GRU to model contextual information about historical conversations. ICON  [25]  models contextual information through GRU-based memory networks and considers the influence of both self-speaker and interspeaker. ConGCN  [26]  symbolizes the entire conversational corpus as a heterogeneous graph in which each node presents a speaker or an utterance, where each conversation includes textual and acoustic features. DialogueRNN  [14]  tracks the states of speakers throughout the conversation by utilizing multiple RNNs for multimodal emotion classification. Relational Tensor Network  [27]  considers relations and interactions of the context segment in a video and shows excellent performance. GME-LSTM(A)  [28]  fuses multimodal information at word-level and proposes a model suitable for complex speech structure with gating mechanism to select word-level fusion. MMGCN  [15]  constructs a big graph, which not only captures intra-and inter-speaker dependencies, but also models multimodal information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Machine Learning Methods",
      "text": "A great number of machine learning applications have made a surge of achievements in recent years relying on Graph Neural Networks (GNNs) and Multi-Head Attention mechanism. In this work, GNNs and multi-head attention mechanism are implemented for intra-modal contextual modeling and intermodal complementary modeling, respectively.\n\n1) Graph Neural Networks: Graph is a general data representation method describing complex relationship between entities in real scenarios, and has been applied broadly in the industry. However, deep learning has been incapable of effectively adapting to graph structured data. To this end, Graph Neural Networks (GNNs) are proposed to address the above-mentioned challenges. GNNs have been widely applied in numerous fields, including Computer Vision  [29] , Recommender System  [30] , Chemistry  [31] , Natural Language Processing  [32] . Given a graph G = (V, E), according to Message Passing Neural Network (MPNN)  [31] , and the information for the l-th layer G is updated as follows:\n\nwhere l = 0, 1, • • • , L, and L is the number of layers;\n\nis the l-th layer representation of node v, and x\n\nand u is a neighborhood of node v; PASS and UPDAT are the parameterized message passing function and state updating function, and READOUT is the readout function.\n\nThe widely used GNN models include 1stChebNet  [33] , GraphSAGE  [34]  and GAT  [35] . GAT supposes that contributions of neighboring nodes to current node are neither identical as GraphSage, nor predefined as 1stChebNet. GAT introduces attention mechanism to calculate the importance of neighboring nodes, which is defined as:\n\nwhere x\n\nis the (l + 1)-th layer representation of current node v; u are neighboring node of v; δ is nonlinear activation function, and W (l+1) denotes the trainable parameter. The attention weight µ (l+1) vu indicates the importance of u to v:\n\nwhere σ is the LeakyReLU function, and u, w are neighbors of v; both a and W (l+1) are the learnable parameters. In addition, multi-head GAT is executed to increase the expressiveness of the model  [35] .\n\n2) Multi-Head Attention Mechanism: Multi-Head Attention mechanism is first proposed in Transformer  [36]  architecture, which is inspired by attention model  [37] . Transformer is a new type of neural network that has been widely adopted in various fields, such as Natural Language Processing  [36] ,  [38] ,\n\nComputer Vision  [39] ,  [40] , and Speech Processing  [41] ,  [42] . Devlin et al.  [38]  propose a language representation model named BERT, which received enthusiastic attention once it was proposed due to its excellent performance. ViT  [39]  implements direct application of a standard Transformer to image classification tasks, and is a classic application that adapts Transformer to the field of computer vision. Dong et al.  [42]  extend Transformer to Automatic Speech Recognition (ASR), and proposed model is named Speech-Transformer. Speech-Transformer can achieve a reduced training cost compared to the majority of recurrence-based models.\n\nGiven packed feature representation query Q, key K, value V, the scaled dot-product attention is computed as:\n\nwhere\n\n) is called attention matrix; softmax denotes the softmax function, which is employed in a row-wise manner. Multi-Head Attention can be a mechanism that can enhance the stability and performance of the vanilla single attention. Specifically, different heads employ different query, key and value matrices. Multi-head attention can be formalized as follows:\n\nwhere MA, Att are the multi-head attention and single attention function, and ∥ denotes the concatenation operation; W Q,i , W K,i , W V,i are the learnable parameters, which can project Q, K, V into different representation subspaces, respectively; W ma is also the trainable parameter.\n\nIII. METHODOLOGY In this section, we introduce overall framework of the proposed method in detail. Our proposed framework is exhibited in Fig.  2 , which consists of unimodal pre-encoding, two-stage multi-source information fusion, and classification.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Problem Definitions",
      "text": "Before presenting overall framework of the proposed approach, several definitions covering the objective task, intramodal contextual information, and inter-modal complementary information will be given.\n\nObjective Task: A conversation consists of m utterances u 1 , u 2 , • • • , u m , and each utterance u i corresponds to an emotion label y i . Different emotion labels are available for different datasets, e.g., Happy, Sad, Neutral, Angry, Excited and Frustrated for the IEMOCAP dataset. There are more than two participants in a conversation, so u i and u j may be spoken by the same speaker or by different speakers. Each utterance u i has three modal expressions u t i , u a i , u v i in multimodal ERC, which corresponds to textual, acoustic and visual modalities, respectively. Given an utterance u i , the task of ERC is to take u i as input and detect the corresponding emotional state y i .\n\nIntra-modal Contextual Information: In a modality, the jth utterance u j before the utterance u i (j < i) is its context. Meanwhile, the context of u i also includes the k-th utterance u k (k > i) after it. Here, the information carried by u j and u k are contextual information of u i . Especially, if j = i -ϵ or k = i + ϵ and ϵ is a small integer, then u j or u k is local contextual utterance of u i . Conversely, if ϵ is a large integer, then u j or u k is long-range contextual utterance. Commonly, recurrence-based methods have difficulty catching long-range contextual information.\n\nInter-modal Complementary Information: In multiple modalities, the utterance u i can be represented as u t i , u a i , and u v i . The information carried by u a i can be regarded as the complementary information of u t i . Similarly, u v i can be also regarded as the complementary information of u t i , and the rest may be deduced by analogy.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Unimodal Pre-Encoding",
      "text": "To rigorously demonstrate the superiority of our model over MMGCN, we employ the identical unimodal encoding method as Hu et al.  [15] . Specifically, we adopt a Bi-directional LSTM (BiLSTM) network to extract the contextual information of textual modality; unlike textual modality, we adopt the fully connected networks to encode acoustic and visual modalities. The unimodal pre-encoding processes for three modalities can be formalized as:\n\nwhere",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Two-Stage Multi-Source Information Fusion",
      "text": "To adequately capture intra-modal contextual information and inter-modal complementary information, we present a Graph and Attention based Two-stage Multi-source Information Fusion (GA2MIF) technique. First, we model the intramodal contexts using three Multi-head Directed Graph ATtention networks (MDGATs) that fully capture the contextual information of textual, acoustic and visual modalities. Then, three Multi-head Pairwise Cross-modal ATtention network (MPCATs) are utilized for cross-modal modeling, which allow for inter-modal feature interactions and thus capture complementary information about inter-modality.  1) Characteristics of the Speaker: Different speakers exhibit distinct self-characteristics in a conversation, such as various personalities, timbres, and expressions. Therefore, we believe that the speaker's information is of importance for emotion recognition. In order to extract self-characteristics of speakers, we first encode the speaker to obtain embedding vector, i.e., speaker embedding; and then we add speaker embedding to the corresponding utterance. This process can be expressed as follows:\n\nwhere EMB represents Embedding function; S is the set of speakers, n is the number of speakers, and S e denotes speaker embedding; X τ denotes feature matrix adding speaker embeddings, τ ∈ {t, a, v}, and O τ is feature matrix from unimodal pre-encoding phase, o τ i ∈ O τ ; λ is trade-off parameter of speaker embedding.\n\n2) Creation of Directed Graph: As shown in Fig.  2 , we create three directed graphs G t , G a and G v for a conversation. G t , G a and G v can be represented as\n\n, where V τ denotes the set of nodes, E τ denotes the set of edges, i.e., the set of connections between nodes, and W τ denotes the set of edge weights. Specifically, in multimodal ERC, our graph is created as follows.\n\nNodes: In a conversation, each utterance u i is represented as three nodes v t i , v a i , and v v i . Here, t, a, v denote textual, acoustic, and visual modalities;\n\nGiven m utterances, we create 3×m nodes, i.e., |V τ | = 3×m, and m is the number of utterances in current conversation.\n\nEdges: Assuming that only unimodality is considered, we connect utterance node v i in the conversation with its past\n\nHere, J , K are defined as the window size of past and future contexts. Based on this, we construct edges using the above-mentioned strategy in the textual, acoustic and visual modalities respectively.\n\nEdge Weights: The edge weight can distinguish the importance of different neighboring nodes and is an important element in graph neural networks. Veličković et al.  [35]  propose Graph ATtention network (GAT) that claim to learn the edge weights of the graph. The attention scores are calculated as follows:\n\nwhere x τ i ∈ X τ denotes feature vector of node v τ i in the graph, and X τ is feature matrix of V τ , τ ∈ {t, a, v}; both x τ j and v τ k are neighbor node of v τ i , and\n\nis weight of the edge between node v τ i and v τ j , and also attention coefficient of GNN; ∥ denotes concatenation operation; σ represents non-linear activation function, e.g, LeakyReLU; a τ and W τ ew are the learnable parameters. We set the window size of context to get neighbors instead of taking all nodes in the conversation as neighbors, which not only improves computational efficiency but also increases the stability of the model (since the window size of context for each node v τ i is fixed in each experiment). In the standard GAT scoring function, however, W τ ew and a τ are applied consecutively. This way can be converted into a linear layer, limiting the expressiveness of attention function. In order to address the above dilemma, we employ an attention module based on GATv2 to set edge weights referring to the idea of Brody et al.  [43] . Specifically, our edge weights are calculated as follows:\n\nMGAT NM 3) Graph-Based Contextual Modeling: Graph Neural Networks (GNNs) have been proved to have excellent relational modeling capabilities. We employ three Multi-head Directed Graph ATtention networks (MDGATs) to model the contextual dependencies of utterances in the conversation. To alleviate the over-smoothing problem  [44]  of GNNs, we connect the previous layer input to the next layer input. Specifically, the input of the prior layer GNN and its output are first added together; then taking the obtained result as the input of the next layer GNN (see Fig.  3a ). Our designed MDGATs can be formalized as:\n\nwhere l = 0, 1, • • • , L, and L is the number of layers of MDGATs; X τ,l is the l-th layer feature matrix, X τ,0 = X τ , and E τ is edge set; NM and MGAT are the normalization layer and multi-head graph attention layer, respectively. Here, MGAT can be computed as:\n\nwhere ∥ is concatenation operation, and W l mg is the trainable parameter. We describe the process of a single GAT according to MPNN  [31] , i.e., GAT is divided into two phases: message passing and state updating.\n\nMessage Passing: The purpose of message passing is to aggregate the information of neighboring nodes. With the help of the calculation of edge weights in Equation  9 , we distinguish the importance of different nodes when aggregating contextual information. For multimodal ERC, our passing function is defined as follows:\n\nwhere µ τ,l+1 ij is the (l + 1)-th layer attention coefficient, as well as the (l + 1)-th layer edge weight between node v τ i and v τ j ; v τ j is neighboring node of v τ i , and i ̸ = j; x τ,l j ∈ X τ,l is the l-th layer feature vector of v τ j , and x τ,l+1 ps,i denotes the (l + 1)-th layer output of message passing; W τ,l+1 ps denotes the learnable parameter. x τ,0 j is initial input feature vector of v τ j , i.e., x τ,0 j = x τ j , X τ,0 = X τ , x τ,0 j ∈ X τ,0 , x τ j ∈ X τ . State Updating: In state updating phase, the updating function combines the (l + 1)-th layer output (x τ,l+1 ps,i ) of message passing with the l-th layer vector representation (x τ,l i ) of node v τ i to obtain the (l + 1)-th layer vector representation (x τ,l+1 i ) of v τ i . Here, τ ∈ {t, a, v}, l = 0, 1, • • • , L, and L is the number of layers of MDGATs. We adopt a variety of updating functions, and x τ,l+1 i can be computed as follows.\n\n• Sum Updating first applies the linear transformation to each of two vector representations, and then sums obtained results:\n\nx τ,l+1 sum,i = W τ,l+1 sum0 x τ,l+1 ps,i + W τ,l+1 sum1 x τ,l i ,\n\nwhere x τ,l+1 sum,i is the l-th layer output of state updating, as well as the (l + 1)-th layer vector representation of v i ; W τ,l+1 sum0 and W τ,l+1 sum1 are the trainable parameters. • Concat Updating applies the concatenation operation to the two vector representations, followed by the linear transformation:\n\nwhere ∥ is the concatenation operation, and W τ,l+1 cat is the learnable parameter.\n\n• Sum-Product Updating is our elaborate updating function.\n\nWe compute in Sum-Product not only the sum of two vectors, but also their element-wise product, thus considering two types of feature interactions between x τ,l ps,i and x τ,l i :\n\nx τ,l+1 sump,i = W τ,l+1 sump [(x τ,l+1 ps,i + x τ,l i ) ∥ (x τ,l+1 ps,i ⊙ x τ,l i )],\n\nwhere ⊙ denotes element-wise product, and W τ,l+1 sump is the learnable parameter. We hope that Sum-Product Updating function can update more messages from similar contextual neighbors to current node v τ i . 4) Attention-Based Cross-Modal Modeling: In this part, we introduce three Multi-head Pairwise Cross-modal ATtention networks (MPCATs) for cross-modal feature interaction and thus capture intra-modal complementary information. The structure of MPCAT can be seen in Fig.  3b . We can observe that the proposed MPCAT mainly consist of two multi-head attention layers (MA denotes the multi-head attention layer) and one feedforward layer (FF denotes the feedforward layer), where the input of each MPCAT contains the feature matrices of two modalities. Note that the inputs of MPCATs are based on the outputs of MDGATs. In the following, we describe the process of MPCATs in detail.\n\nFirstly, we treat acoustic feature matrix A as query Q of multi-head attention layer, and textual feature matrix T as key K and value V of multi-head attention layer; after the attention calculation, we obtain textual-acoustic interaction matrix T a . Similarly, in order to obtain textual-visual interaction matrix T v , we apply visual feature matrix V and textual feature matrix T to another multi-head attention layer; where query Q is replaced by visual feature matrix V, and key K and value V are replaced by textual feature matrix T . Secondly, we add T a and T v together; and the obtained result is input to the residual connection layer and normalization layer in turn to get textual feature matrix T av that are interacted by acoustic and visual modalities; where the residual connection and normalization layer are to ensure the stability of T av . Finally, with reference to network structure of Transformer  [36] , T av is sequentially applied to the feedforward layer, residual connection layer and normalization layer; where the residual connection and normalization layer to enhance the expressiveness of T av . After all above-mentioned steps, a new textual feature matrix (T av ) ′ encoded by MPCAT is acquired, which carries the information of acoustic and visual modality. We repeat above operation K times (K is the number of layers of MPCATs), then we get final output of MPCATs.\n\nThe k-th layer feature matrix T av,k (k = 0, 1, • • • , K) can be formulated as:\n\nwhere T k , A k , and V k are the k-th layer feature matrices of textual, acoustic, and visual modalities, respectively;\n\n; NM, DP, and MA denote the normalization, dropout and multihead attention layers, respectively; MPA includes a dropout layer and two multi-head attention layers (i.e., two MA). MA can be represented as:\n\nwhere head k i is the output of the i-th single attention layer, Att denotes the single attention layer; W k ma , W k Q,i , W k K,i , and W V,i are the trainable parameters.\n\nThe above-mentioned feature matrix (T av,k ) ′ is the output of the k-th layer MPCAT, as well as the input of the (k +1)-th layer MPCAT, i.e., (T av,k ) ′ = T k+1 , which can be computed as follows:\n\nwhere NM, FF, and DP is the normalization, feedforward and dropout layers, respectively; ρ denotes non-linear activation function, e.g., Relu; W k 0 , W k 1 , b k 0 , and b k 1 are the learnable parameters.\n\nLikewise, we can follow the processing of (T av,k ) ′ to obtain the k-th layer acoustic feature matrix (A tv,k ) ′ , a.k.a., A k+1 , and visual feature matrix (V ta,k ) ′ , a.k.a., V k+1 . A k+1 and V k+1 are encoded by two MPCATs. A k+1 and V k+1 are formulated as follows:\n\nwhere A tv,k is the k-th layer acoustic feature matrix that are interacted by textual and visual modalities; similarly, V ta,k is the k-th layer visual feature matrix that are interacted by textual and acoustic modalities. After T , A, V are encoded by the k-th layer MPCATs, we obtain three new feature matrices: T k+1 , A k+1 , V k+1 , which corresponding to textual, acoustic and visual modalities, respectively. Here, T 0 = X t,L , A 0 = X a,L , V 0 = X v,L , and L denotes the number of layers of MDGATs. These new feature matrices are results of the k-th layer cross-modal feature interactions. It is worth noting that T k+1 , A k+1 , and V k+1 are not only the outputs of the k-th layer MPCATs but also the inputs of the (k + 1)-th layer MPCATs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Multimodal Emotion Classification",
      "text": "After the K layers MPCATs encoding, we obtain the last layer feature matrices T (K) , A (K) , V (K) . We concatenate T (K) , A (K) , V (K) to obtain final feature matrix Z containing the information about three modalities. The concatenation operation can be formalized as follows:\n\nwhere ∥ is the concatenation operation; W u is the learnable parameter. Then Z is used as the input of multimodal classification module to predict emotional states of utterances. The emotion prediction can be formalized as follows:\n\nwhere z i ∈ Z is the feature vector of the i-th utterance u i ; p i denotes the probability distribution of predicted emotion label of u i , and ŷi denotes the predicted emotion; ReLU and Softmax denote the activation function and softmax function, respectively; W c , W ′ c , b c , and b ′ c are the trainable parameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Training Objective",
      "text": "Finally, we adopt cross-entropy with L2-regularization as objective function to train the proposed GA2MIF, which can be represented as follows:\n\nwhere n(i) is the number of utterances of the i-th conversation, and N is the number of all conversations in training set; p ij denotes the probability distribution of predicted emotion label of the j-th utterance in the i-th conversation, and y ij denotes the ground truth label of the j-th utterance in the i-th conversation; η is the L2-regularizer weight, and W l is the set of all learnable parameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental Settings A. Dataset Descriptions",
      "text": "We conduct experiments for the multimodal ERC task on the two extensively adopted datasets: Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [45]  and Multimodal Emotion Lines Dataset (MELD)  [46] . For the purpose of comparison, we follow the methods of Hu et al.  [15] : the textual features are extracted by adopting TextCNN  [47] ; the acoustic features are extracted through using OpenSmile toolkit  [48] ; and the visual features are extracted by employing DenseNet  [49] . The statistics of two datasets are shown in TABLE  I .\n\nIEMOCAP dataset is a dyadic multimodal ERC dataset and contains audio-video-text data of impromptu performances or scripted scenes of about ten participants. Each video includes a single conversation, and each conversation consists of multiple utterances. There are in total 151 conversations and 7433 utterances on the IEMOCAP dataset, and each utterance corresponds to an emotional label. In IEMOCAP dataset, there are six emotion labels in total, including Happy, Sad, Neutral, Angry Excited, and Frustrated. MELD dataset is a multi-party and multimodal conversational emotion recognition dataset collected from the TV show Friends. There are seven emotion categories containing Neutral, Surprise, Fear, Sadness, Joy, Disgust, and Angry. MELD includes the information of textual, acoustic and visual modalities with more than 1400 dialogues and 13000 utterances. Unlike the two-person conversation scenario on the IEMOCAP dataset, the MELD dataset has three or more participants in each conversation.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Comparison Models",
      "text": "In order to perform comprehensive evaluations of the proposed GA2MIF, we compare it to baseline models. These baselines include unimodal ERC model and multimodal ERC model, which are described as follows.\n\nMFN  [24]  performs multi-views information fusion and unifies the features of different modalities, but it does not consider either context-aware dependencies or speaker-aware dependencies. BC-LSTM  [12]  employs textual, visual and acoustic modalities for multimodal ERC task, and adopts an utterance level LSTM to capture multimodal information. CMN  [13]  leverages multimodal information by directly concatenating the features from three modalities, and uses GRU for contextual modeling. ICON  [25]  models the contextual knowledge of self-and inter-speaker influences through a GRU-based multi-hop memory network. DialogueRNN  [14]  recognizes current emotion by tracking the contextual information of utterance and taking the characteristic of speaker into account. DialogueGCN  [6]  is a graph-based model that regards the current conversation as a graph, where nodes represent utterances in that conversation. DialogueGCN can effectively capture long-range contextual information. In order to implement multimodal setting, we directly concatenate features of three modalities. DialogueCRN  [9]  is designed with multi-turn reasoning modules to extract and integrate affective cues and can sufficiently understand the contextual information from a cognitive perspective. We implement multimodal setting for DialogueCRN through directly concatenating features of three modalities. MMGCN  [15]  is a multimodal ERC model that simultaneously captures both intra-modal contextual information and inter-modal interactive information.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Implementation Details",
      "text": "All of our experiments are run on NVIDIA GeForce RTX 3080 Ti. All models are implemented via the PyTorch toolkit, as well as the maximum epoch is set to 100. The optimizer used for all models is AdamW, the L2 regularization factor is 0.00001, and the dropout rate is 0.1. Norm is replaced by the layer normalization in GA2MIF. The settings of partial hyperparameters are shown in TABLE II. For IEMOCAP dataset, the number of layers of MDGATs is 3, and that of MPCATs is 4; the trade-off parameter of speaker embedding λ is 1.6; the learning rate is 0.00001; the batch size is 8. For MELD dataset, the number of layers of MDGATs is 2, and that of MPCATs is 2; the trade-off parameter of speaker embedding λ is 0.6; the learning rate is 0.00001; the batch size is 32. For a more rigorous comparison with the optimal baseline MMGCN, our ratios of the training, validation, and test sets are aligned with those of MMGCN. Referring to previous works  [6] ,  [15] , we evaluate the performance of GA2MIF with weighted-average F1 score and average accuracy. V. COMPARISON AND ANALYSIS To illustrate the validity of our model, we conduct extensive experiments in this section. First of all, we intuitively compare the experimental results of our model with those of baseline models. Then, we discuss the effect of different settings on proposed GA2MIF; All experimental results are reported with the help of tables and figures. Finally, we provide two case studies in the last part of this section.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Comparison With Baseline Models",
      "text": "TABLE III reports the experimental results of GA2MIF with other baseline models. It can be visualized from TA-BLE III that our GA2MIF achieves optimal accuracy scores and weighted-average F1 scores on both IEMOCAP and MELD datasets. For the IEMOCAP dataset, the accuracy and F1 score of GA2MIF are 69.75% and 70.00%, which are 4.19% and 4.29% higher than those of the strongest baseline model, i.e., MMGCN. For the MELD dataset, likewise, the accuracy and F1 score of our GA2MIF respectively increase by 2.34% and 1.12% in comparison with those of MMGCN. We can conclude from above results that the improvements of GA2MIF on the MELD dataset are not as significant as that on the IEMOCAP dataset. After carefully comparing the differences between IEMOCAP and MELD, we notice that two neighboring utterances on the MELD dataset may not be consecutive sentences in real conversation scenarios. Therefore, GA2MIF is identical to most models on the MELD dataset, and it is challenging to take advantage of model itself in the absence of a powerful feature extractor.\n\nWe report F1 score corresponding to each emotion label in detail in TABLE III. Our GA2MIF obtain the highest F1 scores on the IEMOCAP dataset for most emotion labels except for Happy. We can observe that the results of GA2MIF show remarkably significant improvement relative to those of baseline models for Sad and Neutral. Fig.  4  depicts confusion matrix of GA2MIF and MMGCN on the IEMOCAP dataset. Fig.  4b  indicates that our GA2MIF recognizes Sad better than other emotion labels, with an accuracy score of 81.22%. By comparing Fig.  4a  and Fig.  4b , we can conclude that the accuracy scores of GA2MIF are higher than those of MMGCN except for Frustrated.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Impact Of Different Trade-Off Parameters",
      "text": "Fig.  5  shows the influence of different trade-off parameters of speaker embedding on the results. Fig.  5a  shows results on the IEMOCAP dataset, while the results on the MELD dataset are shown in Fig.  5b . We find from Fig.  5a  that the performance of GA2MIF gradually increases as the tradeoff parameter increases. This phenomenon suggests that the characteristic of speaker plays an essential role in emotion recognition task. It is noteworthy that the performance of GA2MIF starts to decrease when increasing to a certain threshold (i.e., λ = 1.6). As shown in Fig.  5b , the performance variation of GA2MIF on the MELD dataset is generally consistent with trend on the IEMOCAP dataset. In other words, the values of accuracy score and weighted-average F1 score increase with the increasing of trade-off parameter λ within a certain margin (i.e., λ < 0.6).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. Impact Of Different Window Sizes In Directed Graph",
      "text": "In this part, we discuss the effect of different window sizes on the performance of GA2MIF. Fig.  6  shows the results of GA2MIF corresponding to different window sizes on the IEMOCAP dataset. As shown in Fig.  6a , we set various window sizes (i.e., (2, 2), (4, 4), • • • ,  (30, 30) ) for GA2MIF on the IEMOCAP dataset, each of which is a combination of J and K. As we expected, the accuracy score and weighted-average F1 score increase with the increasing of the window size within a certain margin. When the window size increases to a certain threshold, i.e.,  (16, 16) , the performance of GA2MIF gradually starts to decrease. In the same way, we set different window sizes (i.e., (1, 1), (2, 2), • • • , (10, 10)) on the MELD dataset, and obtain conclusions that are similar to those on the IEMOCAP dataset. On the MELD dataset, however, the effect of window sizes on the performance of GA2MIF is relatively slight. The results on the MELD dataset are shown in Fig.  6b .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Impact Of Different Updating Functions",
      "text": "We design a novel updating function, Sum-Product, for MDGATs in Section III-C3. TABLE IV shows the results of proposed GA2MIF adopting Sum, Concat and Sum-Product functions. From the experimental results, we can observe that Sum-Product we designed has a slight improvement over the other two updating functions. In our future work, we hope to apply Sum-Product to feature interaction-sensitive",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Impact Of Different Number Of Network Layers",
      "text": "Our GA2MIF mainly consists of two sub-networks, namely MDGATs and MPCATs. We depict the effect of different number of layers in MDGATs or MPCATs on the performance of GA2MIF in Fig.  7 . The discussions and analyses in this subsection are based on the experimental results of the IEMOCAP dataset. Fig.  7a  illustrates the effect of different number of network layers on the accuracy scores. Notably, before discussing how the performance is affected by the number of layers in current sub-network, we fix the number of layers in the other sub-network. As shown by the dark blue line in Fig.  7a , we first set the number of layers in MPCATs to 4 and then plot the variation curve of accuracy score under different number of layers in MDGATs; conversely, the number of layers in MDGATs is first limited to 3, and then the change of accuracy score under different number of layers in MPCATs is recorded by the light blue line in Fig.  7a . It can be found that with the increase of layers in MDGATs, the accuracy score of GA2MIF rises first and then falls. The effect of different number of layers in MPCATs on accuracy score also follows this pattern, i.e., the accuracy score increases first and then decreases as the number of layers in MPCATs increases. The difference is that the number of layers in MDGATs is more sensitive to the performance of GA2MIF than that in MPCATs. Similarly, Fig.  7b  shows the effect of different number of network layers on the weightedaverage F1 scores. We can draw the analogous conclusion from Fig.  7b  as Fig.  7a . As shown in Fig.  7 , it is noteworthy that the performance of GA2MIF shows a significant degradation when we do not use either MDGATs (i.e., MDGATs with 0 layers) or MPCATs (i.e., MPCATs with 0 layers). Therefore, MDGATs and MPCATs can contribute to the performance of our model. In this subsection, we conduct experiments with two-and three-modality settings on two public datasets. Note that the proposed GA2MIF requires at least two modalities. These modality settings include the acoustic-visual setting, acoustictextual setting, visual-textual setting, and acoustic-visualtextual setting. As shown in TABLE V, we report accuracy and weighted-average F1 scores of all modality settings. It can be seen from TABLE V that the experimental results of three-modality setting outperform those of all two-modality settings. Among the two-modality settings, our model achieves the best results under the acoustic-textual setting and the worst performance under the acoustic-visual setting. On the IEMOCAP dataset, the accuracy and F1 scores reach 67.47% and 67.49%, respectively, under the acoustic-textual setting, which are 7.77% and 7.64% higher than those under the visual-textual setting. Moreover, the results of GA2MIF under the acoustic-textual setting are higher than those of MMGCN under three-modality setting, which fully demonstrates the effectiveness of the proposed GA2MIF.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "F. Performance Under Different Modality Settings",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "G. Case Studies",
      "text": "In the ERC task, several utterances with non-Neutral emotion labels such as \"Yes. On this, I would. [Sad]\", \"I do.\n\n[Angry]\", \"Actually, now that you mention it, no. I don't.\n\n[Excited]\", are difficult to be detected correctly by existing models. Most of existing text-modal ERC models tend to directly recognize these utterances as Neutral. The abovementioned scenario is shown in Fig.  8 . Intuitively, multimodal ERC models such as MMGCN can compensate the inadequacies of text-modal models by acoustic or visual modality. However, MMGCN treats utterances of all modalities as nodes of the same type, which clearly violates the assumption that the input of GNNs is a homogeneous graph and thus cannot effectively utilize multimodal information. Our GA2MIF inputs the information of each modality into MDGATs separately, and then employs MPCATs for inter-modal information interaction. Furthermore, our approach eliminates complex redundant connections by employing context window to connect edges instead of constructing fully connected graph, which allows for more efficient selection of useful contextual information. Experiments show that the proposed GA2MIF can detect emotional states more accurately relative to MMGCN.  Like most baseline methods, the proposed GA2MIF has challenges in distinguishing similar emotions, e.g., Happy vs Excited, Sad vs Frustrated. Fortunately, our GA2MIF alleviates the problem of similar emotions to a certain extent. The probability of predicting ground-truth emotion Sad as other emotions by MMGCN and GA2MIF is shown in Fig.  9 . We can derive from Fig.  9  that our GA2MIF identifies Sad as Frustrated on the IEMOCAP dataset with a probability of 7.76%, while MMGCN with a probability of 15.51%. Thus, the proposed GA2MIF achieves a more outstanding result with a 7.75% reduction relative to MMGCN.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Summary And Prospect",
      "text": "We propose a novel multimodal conversational emotion recognition model, i.e., Graph and Attention based Two-stage Multi-source Information Fusion (GA2MIF), in this paper. The proposed GA2MIF is mainly inspired by graph attention network and multi-head attention mechanism. Multi-head Directed Graph ATtention networks (MDGATs) and Multihead Pairwise Cross-modal ATtention networks (MPCATs) are designed to model intra-modal contexts and inter-modal interactions, respectively. The collaboration of MDGATs and MPCATs can effectively address the challenge of MMGCN in handling the heterogeneous graph, as well as, the complex redundant connections are eliminated through the context window. In addition, we design a new update function, Sum-Product, for MDGATs (arguably, Graph Neural Networks). We have demonstrated the effectiveness of GA2MIF on two extensively used datasets and achieved an impressive weightedaverage F1 score of 70.00% on the IEMOCAP dataset, which overwhelmingly outperforms all baseline models. Not only that, we also discuss and analyze the effect of different settings on the performance of GA2MIF.\n\nIn future work, we will continue to promote multimodal learning, and focus on how to enhance the expressiveness of acoustic and visual modalities. Furthermore, we hope to apply our model to more multimodal fusion scenarios, as well as tackle the notorious problems of similar-emotion and emotionshifting in conversational emotion recognition.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of a multimodal conversation scenario, in which the",
      "page": 2
    },
    {
      "caption": "Figure 2: , which consists of unimodal pre-encoding, two-stage",
      "page": 4
    },
    {
      "caption": "Figure 2: The overall framework of the proposed approach. Here, two-stage multi-source information fusion mainly involves the creation of directed graph,",
      "page": 5
    },
    {
      "caption": "Figure 3: The illustration of MDGAT and MPCAT, where MGAT and MPA de-",
      "page": 6
    },
    {
      "caption": "Figure 3: a). Our designed MDGATs can be",
      "page": 6
    },
    {
      "caption": "Figure 3: b. We can observe",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparison of confusion matrices between MMGCN and GA2MIF. Hap, Sad, Neu, Ang, Exc, Fru denote Happy, Sad, Neutral, Angry, Excited, and",
      "page": 9
    },
    {
      "caption": "Figure 4: depicts confusion",
      "page": 9
    },
    {
      "caption": "Figure 4: b indicates that our GA2MIF recognizes Sad better than",
      "page": 9
    },
    {
      "caption": "Figure 4: a and Fig. 4b, we can conclude that the",
      "page": 9
    },
    {
      "caption": "Figure 5: shows the influence of different trade-off parameters",
      "page": 9
    },
    {
      "caption": "Figure 5: a shows results",
      "page": 9
    },
    {
      "caption": "Figure 5: b. We find from Fig. 5a that the",
      "page": 9
    },
    {
      "caption": "Figure 5: The performance of our model with different trade-off parameters of speaker embedding. Acc and wa-F1 denote the accuracy and weighted-average",
      "page": 10
    },
    {
      "caption": "Figure 6: The accuracy and weighted-average F1 scores of our GA2MIF with different window sizes in directed graph.",
      "page": 10
    },
    {
      "caption": "Figure 5: b, the performance",
      "page": 10
    },
    {
      "caption": "Figure 6: shows the results",
      "page": 10
    },
    {
      "caption": "Figure 6: a, we set various win-",
      "page": 10
    },
    {
      "caption": "Figure 7: Effect of different number of network layers on the performance of GA2MIF. (a) The dark (or light) blue line denotes the effect of different layers in",
      "page": 11
    },
    {
      "caption": "Figure 7: The discussions and analyses",
      "page": 11
    },
    {
      "caption": "Figure 7: a illustrates the effect of different",
      "page": 11
    },
    {
      "caption": "Figure 7: a, we first set the number of layers in MPCATs",
      "page": 11
    },
    {
      "caption": "Figure 7: a. It can be found that with the increase of layers in",
      "page": 11
    },
    {
      "caption": "Figure 7: b shows the",
      "page": 11
    },
    {
      "caption": "Figure 7: b as Fig. 7a. As shown in Fig. 7, it is noteworthy that",
      "page": 11
    },
    {
      "caption": "Figure 8: Intuitively, multimodal",
      "page": 12
    },
    {
      "caption": "Figure 8: Example of emotion recognition in conversation on the IEMOCAP",
      "page": 12
    },
    {
      "caption": "Figure 9: The probability of predicting ground-truth emotion Sad as other",
      "page": 12
    },
    {
      "caption": "Figure 9: We can derive from Fig. 9 that our GA2MIF identifies Sad",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "IEMOCAP": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated",
          "MELD": "Neutral\nSurprise\nSadness\nJoy\nAnger"
        },
        {
          "Model": "",
          "IEMOCAP": "F1\nF1\nF1\nF1\nF1\nF1",
          "MELD": "F1\nF1\nF1\nF1\nF1"
        },
        {
          "Model": "MFN\nBC-LSTM\nCMN\nICON\nDialogueRNN\nDialogueGCN\nDialogueCRN\nMMGCN",
          "IEMOCAP": "47.19\n72.49\n55.38\n63.04\n64.52\n61.91\n32.63\n70.34\n51.14\n63.44\n67.91\n61.06\n30.38\n62.41\n52.39\n59.83\n60.25\n60.69\n29.91\n64.57\n57.38\n63.04\n63.42\n60.81\n33.18\n78.80\n59.21\n65.28\n71.86\n58.91\n47.10\n80.88\n58.71\n66.08\n70.97\n61.21\n51.59\n74.54\n62.38\n67.25\n73.96\n59.97\n45.45\n77.53\n61.99\n66.67\n72.04\n64.12",
          "MELD": "76.28\n48.29\n23.27\n52.23\n41.32\n75.66\n48.47\n22.06\n52.10\n44.39\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n76.79\n47.69\n20.41\n50.92\n45.52\n75.97\n46.05\n19.60\n51.20\n40.83\n76.13\n46.55\n11.43\n49.47\n44.92\n54.41\n75.16\n48.45\n25.71\n45.45"
        },
        {
          "Model": "GA2MIF",
          "IEMOCAP": "84.50\n68.38\n70.29\n75.99\n66.49\n46.15",
          "MELD": "76.92\n49.08\n27.18\n48.52\n51.87"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Updating Function": "",
          "IEMOCAP": "Acc\nwa-F1",
          "MELD": "Acc\nwa-F1"
        },
        {
          "Updating Function": "Sum\nConcat",
          "IEMOCAP": "68.94\n68.98\n68.91\n68.95",
          "MELD": "61.19\n58.70\n61.19\n58.68"
        },
        {
          "Updating Function": "Sum-Product",
          "IEMOCAP": "69.75\n70.00",
          "MELD": "61.65\n58.94"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality Setting": "",
          "IEMOCAP": "Acc\nwa-F1",
          "MELD": "Acc\nwa-F1"
        },
        {
          "Modality Setting": "a + v\na + t\nv + t",
          "IEMOCAP": "59.70\n59.85\n67.47\n67.49\n64.26\n64.39",
          "MELD": "48.12\n43.34\n60.04\n57.11\n60.00\n57.22"
        },
        {
          "Modality Setting": "a + v + t",
          "IEMOCAP": "69.75\n70.00",
          "MELD": "61.65\n58.94"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The influence of empathy in human-robot relations",
      "authors": [
        "I Leite",
        "A Pereira",
        "S Mascarenhas",
        "C Martinho",
        "R Prada",
        "A Paiva"
      ],
      "year": "2013",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "2",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "R Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "3",
      "title": "Over a decade of social opinion mining: a systematic review",
      "authors": [
        "K Cortis",
        "B Davis"
      ],
      "year": "2021",
      "venue": "Artificial intelligence review"
    },
    {
      "citation_id": "4",
      "title": "Infusing multi-source knowledge with heterogeneous graph neural network for emotional conversation generation",
      "authors": [
        "Y Liang",
        "F Meng",
        "Y Zhang",
        "Y Chen",
        "J Xu",
        "J Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "A knowledge-based recommendation system that includes sentiment analysis and deep learning",
      "authors": [
        "R Rosa",
        "G Schwartz",
        "W Ruggiero",
        "D Rodríguez"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "6",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "7",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "W Jiao",
        "H Yang",
        "I King",
        "M Lyu"
      ],
      "year": "2019",
      "venue": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "arxiv": "arXiv:1904.04446"
    },
    {
      "citation_id": "8",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "9",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "10",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "I Gat",
        "H Aronowitz",
        "W Zhu",
        "E Morais",
        "R Hoory"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition",
      "authors": [
        "M Jalal",
        "R Milner",
        "T Hain"
      ],
      "year": "2020",
      "venue": "Proceedings of Interspeech 2020. International Speech Communication Association"
    },
    {
      "citation_id": "12",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference on"
    },
    {
      "citation_id": "14",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "16",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Driver's facial expression recognition in realtime for safe driving",
      "authors": [
        "M Jeong",
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "21",
      "title": "Intelligent facial emotion recognition based on stationary wavelet entropy and jaya algorithm",
      "authors": [
        "S.-H Wang",
        "P Phillips",
        "Z.-C Dong",
        "Y.-D Zhang"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "22",
      "title": "Facial emotion recognition: State of the art performance on fer2013",
      "authors": [
        "Y Khaireddin",
        "Z Chen"
      ],
      "year": "2021",
      "venue": "Facial emotion recognition: State of the art performance on fer2013",
      "arxiv": "arXiv:2105.03588"
    },
    {
      "citation_id": "23",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "24",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "25",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "26",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "27",
      "title": "Multimodal relational tensor network for sentiment and emotion classification",
      "authors": [
        "S Sahay",
        "S Kumar",
        "R Xia",
        "J Huang",
        "L Nachman"
      ],
      "year": "2018",
      "venue": "Multimodal relational tensor network for sentiment and emotion classification",
      "arxiv": "arXiv:1806.02923"
    },
    {
      "citation_id": "28",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "M Chen",
        "S Wang",
        "P Liang",
        "T Baltrušaitis",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "29",
      "title": "Few-shot learning with graph neural networks",
      "authors": [
        "V Garcia",
        "J Bruna"
      ],
      "year": "2017",
      "venue": "Few-shot learning with graph neural networks",
      "arxiv": "arXiv:1711.04043"
    },
    {
      "citation_id": "30",
      "title": "Graph convolutional neural networks for web-scale recommender systems",
      "authors": [
        "R Ying",
        "R He",
        "K Chen",
        "P Eksombatchai",
        "W Hamilton",
        "J Leskovec"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "31",
      "title": "Neural message passing for quantum chemistry",
      "authors": [
        "J Gilmer",
        "S Schoenholz",
        "P Riley",
        "O Vinyals",
        "G Dahl"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "32",
      "title": "Encoding sentences with graph convolutional networks for semantic role labeling",
      "authors": [
        "D Marcheggiani",
        "I Titov"
      ],
      "year": "2017",
      "venue": "Encoding sentences with graph convolutional networks for semantic role labeling",
      "arxiv": "arXiv:1703.04826"
    },
    {
      "citation_id": "33",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "34",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "W Hamilton",
        "Z Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Graph attention networks",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "38",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "39",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "40",
      "title": "End-to-end object detection with transformers",
      "authors": [
        "N Carion",
        "F Massa",
        "G Synnaeve",
        "N Usunier",
        "A Kirillov",
        "S Zagoruyko"
      ],
      "year": "2020",
      "venue": "End-to-end object detection with transformers"
    },
    {
      "citation_id": "41",
      "title": "Developing realtime streaming transformer transducer for speech recognition on largescale dataset",
      "authors": [
        "X Chen",
        "Y Wu",
        "Z Wang",
        "S Liu",
        "J Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition",
      "authors": [
        "L Dong",
        "S Xu",
        "B Xu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "How attentive are graph attention networks",
      "authors": [
        "S Brody",
        "U Alon",
        "E Yahav"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "44",
      "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
      "authors": [
        "Q Li",
        "Z Han",
        "X.-M Wu"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "45",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "46",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "47",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "48",
      "title": "Recognizing realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "49",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    }
  ]
}