{
  "paper_id": "2407.12380v1",
  "title": "Pcq: Emotion Recognition In Speech Via Progressive Channel Querying",
  "published": "2024-07-17T07:58:16Z",
  "authors": [
    "Xincheng Wang",
    "Liejun Wang",
    "Yinfeng Yu",
    "Xinxin Jiao"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Channel querying",
    "Progressive"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In human-computer interaction (HCI), Speech Emotion Recognition (SER) is a key technology for understanding human intentions and emotions. Traditional SER methods struggle to effectively capture the long-term temporal correla-tions and dynamic variations in complex emotional expressions. To overcome these limitations, we introduce the PCQ method, a pioneering approach for SER via Progressive Channel Querying. This method can drill down layer by layer in the channel dimension through the channel query technique to achieve dynamic modeling of long-term contextual information of emotions. This mul-ti-level analysis gives the PCQ method an edge in capturing the nuances of hu-man emotions. Experimental results show that our model improves the weighted average (WA) accuracy by 3.98% and 3.45% and the unweighted av-erage (UA) accuracy by 5.67% and 5.83% on the IEMOCAP and EMODB emotion recognition datasets, respectively, significantly exceeding the baseline levels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition technology identifies and understands an individual's emotional state by analyzing multiple modalities  [1] , including speech, facial expressions, body movements, and language. This technology is essential in some fields, such as Face Forgeries  [2]  and health monitoring  [3] . Especially in specific scenarios, like call centres and telemedicine, speech becomes the preferred or the only feasible modal-ity for emotion recognition due to other modalities' unavailability or practical limita-tions, such as text and images. This modal dependence highlights speech emotion recognition's unique value and challenges in specific applications. Given this, this paper will investigate unimodal speech emotion recognition.\n\nMuch attention has been paid to the temporal nature of speech signals  [4] , a crucial property for understanding and processing speech data. Our research focuses on discrete speech emotion recognition (DSER)  [5] , i.e., in this framework, we assume that each sentence expresses a single emotion. However, the expression of human emo-tions is a dynamic process: emotions are not fully revealed instantly but gradually revealed over time. This dynamism means that although our task is to identify the dominant emotion in each sentence, we still need to pay attention to temporal varia-tions in the speech signal to capture subtle clues about how the emotion develops over time. The temporal character of emotional expression exacerbates the difficulty of accurately dynamic modelling emotional information in speech. To address this chal-lenge, current research approaches face several problems. Firstly, although the Transformer  [6]  based approach enables global modelling, the approach may not be appli-cable when dealing with relatively small unimodal speech emotion recognition data. Therefore, current research in SER focuses mainly on capturing contextual emotional information using self-attention or cross-attention mechanisms. Hu et al. in  [7]  applied different cross-attention modules to a joint speech emotion recognition network and achieved good results in a speaker-independent setting. In  [8] , Jiao et al. proposed a Hierarchical Cooperative Attention method, which combines features extracted by HuBERT with spectrogram features to enhance the accuracy of speech emotion recognition. In addition, Xu et al. proposed a head fusion strategy in  [9] , which solves the limitation that the multi-head attention mechanism can only focus on a single infor-mation point and makes the model able to concentrate on multiple important infor-mation points at the same time, which in turn optimizes the model performance. While these attention methods can achieve global attention to some extent, they inevitably require more parameters.\n\nMeanwhile, Convolutional Neural Networks (CNNs) are also prevalent in the re-search of speech emotion recognition. For example, Zhao et al. in  [10]  combined an LSTM network and a full convolutional network (FCN), the latter extracting features by concatenating multiple 2D convolutional layers. Aftab et al. in  [11]  designed a lightweight FCN to extract features in-depth by increasing the number of convolu-tional layers, and Mekruksavanich et al. in  [12]  explored the possibility of applying a 1D convolutional method applied to Thai sentiment recognition. Although these mul-tilayer convolutional network strategies are effective in feature extraction, they usually focus only on the output of the last layer of the CNN, which may ignore the fine-grained information in the shallow layers, which limits the model's ability to capture long-term contextual information.\n\nTo address the shortcomings in the existing methods mentioned above, our study proposes a novel approach to speech emotion recognition based on the progressive channel querying technique. By combining the dual perspectives of speech and spec-trogram and introducing the channel query module, this method can model speech emotion signals dynamically in the channel dimension in a progressive manner. Our main contributions are as follows:\n\n• We designed a Multilayer Lightweight CNN (MLCNN) branch to extract feature outputs from different layers. Further, we introduce the overall framework of the PCQ network. This framework merges the MLCNN branch, using spectrogram as input, with the WavLM pre-trained model branch, using original speech as input. • We designed a Channel Semantic Query (CSQ) module for querying and integrat-ing semantically similar sentiment features from neighbouring layers of an MLCNN in the channel dimension. This module exploits the temporal properties of speech signals to dynamically model speech emotion, thereby capturing the tem-poral variation of emotional information in the channel dimension. We achieve step-by-step acquisition of emotional information by integrating multiple CSQ modules in the PCQ framework. • Our method achieves good results on the IEMOCAP dataset and the EMODB da-taset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "This section provides a comprehensive overview of our proposed approach, as shown in Fig.  1 . Section 2.1 describes the structure of the sub-network MLCNN. Section 2.2 describes the WavLM pre-trained model and the processing of its output features. Section 2.3 introduces the CSQ module. Section 2.4 outlines the overall network structure of PCQ.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mlcnn Branch",
      "text": "As shown in Figure  1 (B), the MLCNN we developed consists of four layers with {16, 32, 48, 64} channels per layer. For the layer setting of the MLCNN, we have explained in detail as well as ablation experiments in Section 5. As the network depth increases, the MLCNN will output deeper semantic features layer by layer. Specifically, the network contains four layers whose outputs are x 1 , x 2 , x 3 , x 4 , and the feature of the mth pair of adjacent layers is denoted as f m . Thus, we define:\n\nwhere the [•, •] symbols are used to denote the selected two neighboring layer features. In addition, each layer contains a Parameter-efficient Depth Convolution (PDC) block, which consists of two point convolutions, a depth convolution, and a channel attention block, as shown in Figure  1 (C). The input feature is χ ∈ R C×W ×H , where C denotes the number of channels, W is the width, H is the height, and the cth channel of the input feature map is denoted as χ c . For each feature map, the global average pooled feature Gap(χ) is obtained by computing the average of all its elements. Gap(χ) is then fed into a fully connected layer Υ and processed by a sigmoid function δ to learn the importance of the sentiment information in each channel and generate the channel weights ω accordingly. Then, by multiplying the weights ω with the input feature χ, we obtain the output feature y. In Eq.(  1 ), (χ c ) i,j is the element with position (i, j) on the cth channel.  The attentional mechanism therefore enables the model to understand and emphasize key emotional information in each channel more accurately. The PDC module adopts an architecture that sequentially connects dot convolution, deep convolution, channel attention mechanism, and dot convolution. This design not only significantly improves the performance of the model in capturing emotionally relevant information, but also effectively reduces noise interference.\n\nThe PDC module maintains an equal number of input and output channels, denoted as C. Under equal conditions, traditional 3x3 convolutions require 9C 2 parameters, while the parameter count of the PDC module is (16/3)C 2 + 18C. From the analysis, it is found that the number of parameters of the PDC module will be lower than the traditional 3x3 convolution when C > 4.90. The relevant parameter count comparisons are detailed in Table  3  in Section 4. In addition, in Table  4  in Section 4, we also show that the PDC module improves in terms of accuracy compared to the traditional 3x3 convolution.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Wavlm Branch",
      "text": "As shown in (A) of Fig.  1 , the WavLM pre-trained model, serving as an encoder within a branching network that takes speech signals as inputs, effectively models long-term contextual sequences. Second, in the work  [13] , Zhao et al. revealed the layer-to-layer variability in the pre-trained model: the part closer to the output layer tends to contain rich task-specific information, whereas the underlying layers closer to the input capture more generic features. These bottom layers capture a wide range of features, while the top layer focuses on high-level abstract features that are closely related to a specific task. In this study, we utilized the WavLM pre-trained model for the speech emotion recognition task. Compared to the bottom layer of the model, the top layer of WavLM is more focused on modeling speech emotion information. Therefore, we selected features from the final output layer of the WavLM model and enhanced the network's overall recognition of emotions through multiple average pooling operations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Csq Module",
      "text": "In Figure  2 , we design the Channel Semantic Query (CSQ) Module. CSQ module takes three inputs, with the first two being shallow-level features X l ∈ R C l ×H l ×W l and deep-level features X h ∈ R C h ×H h ×W h . To aggregate the semantically similar information on these two different feature scales, we first employ convolution and bilinear interpolation to resize the deep-level speech features to match the size of the shallow-level speech features,\n\nNext, we evenly divide the channels of the two features into four groups (Group= 4) for positional encoding. We introduced a channel query token Q which queries and aggregates channel features with identical positional encodings, creating\n\nAs shown in Eq. (  4 ), in each ϖ i , Q efficiently synthesizes similar emotional information of the same positional encoding, utilizing its pre-trained knowledge. In high-dimensional channels, deeper semantic information exists. To further extract the deeper semantic feature η i ∈ R (Group/2+1)×H l ×W l , for four blocks from different channel dimensions, we use 3x3 dilation convolution with dilation rates of\n\nBased on the correlation between emotion and time, we concatenate η i to obtain ŷ ∈ R C l ×H l ×W l . Next, a convolution operation highlights the emotional information in η.\n\n2.  4  The Proposed Overall Framework (PCQ)\n\nIn Figure  1 (A), PCQ is composed of three main components: the MLCNN subbranch, the WavLM pre-training branch, and the CSQ module. Firstly, spectrogram features are input into the MLCNN, as illustrated in Figure  1 (B). The MLCNN produces three sets of outputs, namely, f 1 , f 2 , and f 3 . These outputs are then sequentially used as inputs for the three independent CSQ modules.\n\nAfter the speech signal passes through the WavLM encoder, it first obtains the channel query token Q 1 with channel number 1. Then, Q 1 undergoes two adaptive pooling operations to obtain the other two channel query tokens, Q 2 and Q 3 . These tokens are pre-trained features with global sentiment information.\n\nNext, we need to use this pre-training information to assist the PCQ network in achieving global sentiment perception. Therefore, Q 1 , Q 2 , and Q 3 are input as query tokens to the three CSQ modules sequentially. Next, the CSQ modules will sequentially generate three progressive features z 1 , z 2 , and z 3 , which are different degrees of perception of global emotion information. Q 1 obtains its self-attention features through an attention mechanism weighted by x 4 .\n\nTo enhance feature fusion, the Gap (Global Average Pooling) operation is applied to Q 4 , z 1 , z 2 , z 3 and x 4 . Subsequently, these fused features are fed into a classifier comprising multiple linear layers for emotion prediction. as shown in Eq. (  8 ).These fused features are fed into a classifier comprising multiple linear layers for emotion prediction, as shown in Eq.  (9) .\n\n3 Experiment",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "IEMOCAP : This dataset is an English corpus. In total, it contains about 12 hours of audiovisual data, of which audio data has been widely used in automatic emotion recognition re-search. We identify four main emotions: anger, sadness, happiness and neutrality. Considering the unbalanced distribution of emotion categories in the dataset, we merge \"happy\" and \"excited\" into \"happy\". EMODB : This dataset is a German language speech library recorded by 10 participants (5 males and 5 females) covering seven different emotional expressions: anger, disgust, fear, happiness, sadness, surprise, and neutrality. The entire database contains about 535 audio clips, each ranging from 1 to 10 seconds long, providing a rich sound sample for emotion recognition studies.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this study, we sampled the raw audio signal at 16 kHz and segmented it into 3-second segments, with underfilled segments using zero padding. The final prediction is based on the judgement of all the segments. Through a series of 40 ms Hamming windows, we generate spectrogram features. Each window was discrete Fourier trans-formed (DFT) as a frame to obtain an 800-point frequency domain signal, and the first 200 were taken as input features. In this way, we obtained a spectrogram of 300x200 size corresponding to each audio clip. To evaluate the model performance, we use both Weighted Accuracy (WA) and Unweighted Accuracy (UA) metrics and use 10-fold cross-validation to ensure that the results are reliable.Our emotion classi-fication task uses a cross-entropy loss function. Our system is implemented in PyTorch. The batch sizes are 16 and 32 for the IEMOCAP dataset and EMODB da-taset, respectively. The early stop is 20 epochs. We use the AdamW optimiser, and the learning rate is 1e-5. All experiments were conducted on an NVIDIA 4090 24G GPU.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Models",
      "text": "Parameters(↓) MFCC+Spectrogram+W2E(base)  [20]  174.62M PCQ(ours) 97.99M AlexNet(base)  [20]  2.47M MLCNN(ours) 0.092M",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "Fig.  3  shows the performance of MLCNN with the number of layers 2, 3 and 4 on the IEMOCAP dataset. The results show that both performance metrics improve signifi-cantly with the increasing number of layers, especially when the model reaches 4 lay-ers, both WA and UA metrics reach the highest, 75.62% and 76.15%, respectively. This trend demonstrates that increasing network depth effectively enhances model accuracy. However, to ensure comparability with the baseline AlexNet network, we decided against further increasing the number of layers, opting instead for the same network depth to facilitate effective model comparison under similar conditions. Therefore, a 4-layer MLCNN network was used for all subsequent experiments.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Comparisons",
      "text": "Table  1  shows the performance of our proposed PCQ network compared to the three-branch network used in the baseline study described in  [22]  on the IEMOCAP dataset. Our method achieves significant improvements of 3.98% and 3.45% in terms of WA and UA metrics, respectively. In addition, Table  1  also shows some current state-of-the-art speech emotion recognition models.\n\nCompared to these models, our PCQ network exhibits a superior performance.\n\nOn the EMODB dataset, as shown in Table  2 , the PCQ network achieved high accuracy. Compared with the baseline network  [20] , PCQ improved 5.67% and 5.83% on WA and UA, respectively, further demonstrat-ing the effectiveness and applicability of the methodology. In the upper part of Table  3 , we compare in detail the total number of parameters of the benchmark network used on the IEMOCAP dataset with our newly designed PCQ network. From the data, the number of parameters of the PCQ network is reduced by 43.98% compared to   the benchmark network. In the lower part of Table  3 , we compare the branching networks designed based on spectrograms. Compared to the baseline AlexNet, our designed MLCNN substantially reduces the number of parameters, which fully proves the lightweight feature of our proposed PCQ network and MLCNN network.\n\nTable  4  details the results of the ablation experiments for each component of the PCQ network on the IEMOCAP dataset. Replacing the PDC module with the 3x3 Conv2d resulted in a reduction of WA and UA by 0.42% and 0.34%, respectively, despite an increase in the number of network parameters. This clearly demonstrates the advantages of the PDC module in terms of lightweight and efficiency. Without the CSQ module, the WA and UA of the network decreased by 0.97% and 1.42%, re-spectively. As shown in the third-to-last row of Table  4 , without the CSQ module and the WavLM branch, the training accuracy of the network is significantly reduced. While the WA and UA when using Wav2Vec 2.0 as the pre-training encoder increase by 0.14% and 0.47% respectively compared to the baseline two-branch network, they are still 5.43% and 4.38% lower compared to our PCQ network. This further demon-strates that both our CSQ model and the WavLM pre-training model are indispensable key components in the PCQ backbone network. As shown in Fig.  4 , the t-SNE visual-isation results on the IEMOCAP dataset show that the PCQ method with the integrated CSQ module has clearer classification boundaries than the network without the CSQ module. Furthermore, in Fig.  5 , the normalised confusion matrix shows that the PCQ method significantly improves the recognition of \"happy\" and \"neutral\" emotions on the IEMOCAP dataset.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we propose a new framework for speech emotion recognition named Progressive Channel Querying(PCQ). The method mainly queries and integrates similar sentiment features in the channel dimension through the CSQ (Channel Semantic Query) module. Applying the CSQ module at different layers in the PCQ framework enables a gradual enhancement of the understanding of the sentiment information, thus allowing the model to acquire sentiment features in a progressive manner. Experimental results on the IEMOCAP dataset and the EMODB dataset show that our method achieves significant improvements on the SER task compared to existing techniques. SER tasks are used in a particular scenario. For future research, we will work on multi-scene SER research, i.e., multimodal emotion recognition, where the input is speech, image, text, and other modalities.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Section 2.1 describes the structure of the sub-network MLCNN.",
      "page": 3
    },
    {
      "caption": "Figure 1: (B), the MLCNN we developed consists of four layers with",
      "page": 3
    },
    {
      "caption": "Figure 1: (C). The input feature is χ ∈RC×W ×H, where C denotes",
      "page": 3
    },
    {
      "caption": "Figure 1: (A) The overall PCQ framework. (B) The MLCNN network. (C) The PDC",
      "page": 4
    },
    {
      "caption": "Figure 1: , the WavLM pre-trained model, serving as an encoder",
      "page": 4
    },
    {
      "caption": "Figure 2: CSQ module. The letters on the arrows in the diagram represent the following:",
      "page": 5
    },
    {
      "caption": "Figure 2: , we design the Channel Semantic Query (CSQ) Module. CSQ mod-",
      "page": 5
    },
    {
      "caption": "Figure 1: (A), PCQ is composed of three main components: the MLCNN sub-",
      "page": 6
    },
    {
      "caption": "Figure 3: Visualisation of MLCNN results for different layers on the IEMOCAP dataset:",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the performance of MLCNN with the number of layers 2, 3 and",
      "page": 8
    },
    {
      "caption": "Figure 4: (Top) The t-SNE visualization of feature distribution on the IEMOCAP",
      "page": 9
    },
    {
      "caption": "Figure 4: , the t-SNE visual-isation results",
      "page": 10
    },
    {
      "caption": "Figure 5: , the normalised confusion matrix shows that the",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Comparison of baseline and state-of-the-art methods on the EMO-DB",
      "data": [
        {
          "Models": "TSP+INCA[21] †\nGM-TCN[22]†\nLightSER[11] †\nTIM-Net[23]†\nMFCC+Spectrogram+W2E(base)[20]⋆",
          "WA(↑)": "90.09\n91.39\n94.21\n95.70\n90.17",
          "UA(↑)": "89.47\n90.48\n94.16\n95.17\n89.65"
        },
        {
          "Models": "PCQ(ours)",
          "WA(↑)": "95.84",
          "UA(↑)": "95.48"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Weavenet: End-to-end audiovisual sentiment analysis",
      "authors": [
        "Y Yu",
        "Z Jia",
        "F Shi",
        "M Zhu",
        "W Wang",
        "X Li"
      ],
      "year": "2021",
      "venue": "International Conference on Cognitive Systems and Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Rethinking gradient operator for exposing ai-enabled face forgeries",
      "authors": [
        "Z Guo",
        "G Yang",
        "D Zhang",
        "M Xia"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "Two birds with one stone: Knowledge-embedded temporal convolutional transformer for depression detection and emotion recognition",
      "authors": [
        "W Zheng",
        "L Yan",
        "F Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Cm-tcn: Channel-aware multi-scale temporal convolutional networks for speech emotion recognition",
      "authors": [
        "T Wu",
        "L Wang",
        "J Zhang"
      ],
      "year": "2023",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "5",
      "title": "Combining a parallel 2d cnn with a self-attention dilated residual network for ctcbased discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Q Li",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "J Tao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "6",
      "title": "Srt: Improved transformer-based model for classification of 2d heartbeat images",
      "authors": [
        "W Wu",
        "Y Huang",
        "X Wu"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "7",
      "title": "A joint network based on interactive attention for speech emotion recognition",
      "authors": [
        "Y Hu",
        "S Hou",
        "H Yang",
        "H Huang",
        "L He"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "8",
      "title": "Mfhca: Enhancing speech emotion recognition via multi-spatial fusion and hierarchical cooperative attention",
      "authors": [
        "X Jiao",
        "L Wang",
        "Y Yu"
      ],
      "year": "2024",
      "venue": "Mfhca: Enhancing speech emotion recognition via multi-spatial fusion and hierarchical cooperative attention",
      "arxiv": "arXiv:2404.13509"
    },
    {
      "citation_id": "9",
      "title": "Head fusion: Improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset",
      "authors": [
        "M Xu",
        "F Zhang",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Exploring deep spectrum representations via attention-based recurrent and convolutional neural networks for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Y Zhao",
        "Z Zhang",
        "N Cummins",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "11",
      "title": "Light-sernet: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "A Aftab",
        "A Morsali",
        "S Ghaemmaghami",
        "B Champagne"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Negative emotion recognition using deep learning for thai language",
      "authors": [
        "S Mekruksavanich",
        "A Jitpattanakul",
        "N Hnoohom"
      ],
      "year": "2020",
      "venue": "2020 joint international conference on digital arts, media and technology with ECTI northern section conference on electrical, electronics, computer and telecommunications engineering"
    },
    {
      "citation_id": "13",
      "title": "Improving automatic speech recognition performance for low-resource languages with self-supervised models",
      "authors": [
        "J Zhao",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Q Cao",
        "M Hou",
        "B Chen",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition with global-aware fusion on multi-scale feature representation",
      "authors": [
        "W Zhu",
        "X Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Multiple acoustic features speech emotion recognition using cross-attention transformer",
      "authors": [
        "Y He",
        "N Minematsu",
        "D Saito"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition based on graphlstm neural network",
      "authors": [
        "Y Li",
        "Y Wang",
        "X Yang",
        "S Im"
      ],
      "year": "2023",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "18",
      "title": "Learning multi-scale features for speech emotion recognition with connection attention mechanism",
      "authors": [
        "Z Chen",
        "J Li",
        "H Liu",
        "X Wang",
        "H Wang",
        "Q Zheng"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "19",
      "title": "Knowledge enhancement for speech emotion recognition via multi-level acoustic feature",
      "authors": [
        "H Zhao",
        "N Huang",
        "H Chen"
      ],
      "year": "2024",
      "venue": "Connection Science"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T Tuncer",
        "S Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "22",
      "title": "Gm-tcnet: Gated multi-scale temporal convolutional network using emotion causality for speech emotion recognition",
      "authors": [
        "J Ye",
        "X Wen",
        "X Wang",
        "Y Xu",
        "Y Luo",
        "C Wu",
        "L Chen",
        "K Liu"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "23",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}