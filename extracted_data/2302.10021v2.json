{
  "paper_id": "2302.10021v2",
  "title": "Medical Face Masks And Emotion Recognition From The Body: Insights From A Deep Learning Perspective",
  "published": "2023-02-20T15:07:24Z",
  "authors": [
    "Nikolaos Kegkeroglou",
    "Panagiotis P. Filntisis",
    "Petros Maragos"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The COVID-19 pandemic has undoubtedly changed the standards and affected all aspects of our lives, especially social communication. It has forced people to extensively wear medical face masks, in order to prevent transmission. This face occlusion can strongly irritate emotional reading from the face and urges us to incorporate the whole body as an emotional cue. In this paper, we conduct insightful studies about the effect of face occlusion on emotion recognition performance, and showcase the superiority of full body input over the plain masked face. We utilize a deep learning model based on the Temporal Segment Network framework, and aspire to fully overcome the face mask consequences. Although facial and bodily features can be learned from a single input, this may lead to irrelevant information confusion. By processing those features separately and fusing their prediction scores, we are more effectively taking advantage of both modalities. This framework also naturally supports temporal modeling, by mingling information among neighboring frames. In combination, these techniques form an effective system capable of tackling emotion recognition difficulties, caused by safety protocols applied in crucial areas.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The possible applications of an interface capable of assessing human emotional states are numerous. Humans generally treat computer agents as they might treat other people  [32] . Robots and systems that are able to recognize, interpret and process human affect  [5] , are arguably well suited to this, making the interaction more effective and pleasant. They find fertile ground in the area of computer-assisted education, as learning is the quintessential emotional experience. A learning episode might begin with curiosity and fascination. But as its difficulty increases, one may experience confusion, frustration or anxiety, and thus, may abandon learning  [29] . A tutoring agent, who is able to estimate the learner's affective state, can respond appropriately and give encouraging suggestions. Existing work has shown that robot tutors enhance learning, by personalizing their motivational strategies to the student's emotional behavior  [13]    [20] . Another crucial area is health care, as mental health disorders, like depression and psychosis, are on the rise across the world. Emotion recognition systems can be an effective strategy for preventing and monitoring such disorders  [38] .\n\nWhile works based on facial expressions abound in the area, recognizing affect from the body remains a less explored topic. A study in neurobiology has shown that body movement and posture contain useful features for recognizing human affect  [8] . In other experiments, it was shown that facial and bodily expressions work complementary for visual perception of emotion, and in some cases humans perceive bodily expressed emotional information as more diagnostic than facial  [2] . Furthermore, the visibility of facial cues is not guaranteed. Bodily expression recognition becomes crucial when facial features are occluded. Medical face masks, which are extensively used nowadays due to the COVID-19 pandemic  [6] , are the epitome of face occlusion. Because bodies are more expressive than faces in those situations, social information can be detected from the body instead.\n\nAlthough there has been a considerable amount of research on automatic emotion recognition in adults, the topic regarding children has been understudied. Children go through a critical development process and applications involving them require special attention  [31] . They also tend to fidget and move around more than adults, leading to more self-occlusions and non-frontal head poses  [4] . This becomes even more challenging, considering the current health and safety protocols that demand the use of face masks. Robots can no longer rely only on facial expressions to recognize emotion, but also have to take into account body expressions that can stay visible and detectable, even when the face is unobservable. Children's behavior and natural characteristics differ from adults, so perception systems need to be specifically trained, to be able to tackle Child-Robot Interaction (CRI) problems.\n\nThe rest of the paper is organized as follows: Section 2 discusses related work on emotion recognition mainly from the body. Section 3 describes the adopted deep learning-based visual emotion recognition model in detail, as well as the tools and methods used for the experiments. Section 4 presents the experimental results, and lastly Section 5 provides our conclusions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "In recent years, deep learning methods have been very popular due to the massive amounts of digital data in combination with powerful processing hardware. Deep extracted features have yielded excellent results and on most cases outperformed non-deep state-ofthe-art methods for the emotion recognition task. When processing a video with emotional expressions, an essential component is capturing temporal information to complement the prediction from still images. Two-stream Convolutional Neural Network (CNN) architectures use multi-frame optical flow to handle complex actions like emotional expressions  [34] .\n\nThe most common modality used by the research community for identifying emotion is facial expressions  [21] . Some works have proposed an audiovisual approach  [12] , where the system takes speech as an additional input to the face, in order to tackle occlusions and increase robustness. In  [26] , they utilize 3D CNNs to extract spatiotemporal features both from face videos and audio signals, and deep belief nets  [17]  for emotion recognition. However, the COVID-19 pandemic has fostered a pervasive use of medical face masks all around the world, making a serious impact on social communication. Several studies investigated how the presence of a face mask affects emotion recognition accuracy and revealed that it diminishes the people's ability to accurately categorize a facial expression  [6]    [14] . On top of that, the mask impairs re-identification of the same face by people  [24] , which suggests a need for mask-specific model training. In  [35] , they also explored how masks influence the perceived emotional profile of facial expressions. It was shown, that it not only led to a decrease in perceived intensity of the intended emotions, but it also resulted in an overall increase in the perceived intensity of non-intended emotions. In  [28] , even super-recognizers, people who are highly skilled and superior in recognition tasks, were impaired by the face occlusion caused by the face mask. This negative effect in emotional reading is not limited to adults, as it also concerns interaction with children  [7] .\n\nMotivated by all the above, we move towards incorporating bodily expressed information as a major cue in the emotion recognition task. An early work  [15]  combined handcrafted face and body features at feature and decision-level for emotion classification. In  [10] , a hierarchical multi-label annotation method was proposed, which fused body skeleton with facial expressions for automatic recognition of emotion of children during CRI scenarios. In  [23] , they experimented with two bodily expression pipelines, one of which implemented a two-stream-based CNN. The other one relied solely on the human skeleton and utilized a spatial-temporal Graph Convolutional Network (GCN)  [37] , which constructs a graph from the human body landmarks with their natural spatial connectivity, as well as temporally neighboring landmarks.\n\nAlong with body, context has been an additional modality involved in the task of emotion recognition. In  [11] , RGB and flow body streams were accompanied with a context RGB stream and a visual-semantic embedding loss based on word embedding representations. In  [18] , they proposed a network structure composed of a GCN processing skeleton landmarks, and two 3D CNNs for RGB body and context input. A network ensemble, including streams that processed the body in RGB, flow and skeleton form was proposed in  [30] . This variety of bodily expressed cues have also been involved in CRI emotion recognition systems  [9]    [25] .\n\nOur work focuses on the effect of the face occlusion on emotion recognition performance. We adopt a proven related work model and process only RGB input, despite the diversity of body cues that can be conveyed. We sense that this approach suits best to our purpose, regarding the medical face mask effect study.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Visual Emotion Recognition Model",
      "text": "In this chapter, we present the model, that will be used to tackle the visual emotion recognition task. We discuss its structure and benefits and also address the occuring challenges, which are taken into account in the model's various configurations. Furthermore, we describe the tools utilized to conduct the upcoming experiments and some techniques to enhance model performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Capturing",
      "text": "Complex actions, like emotional expressions, comprise multiple stages spanning over a period of time and it would be quite a loss failing to utilize them. On the other hand, each expressed emotion is not present throughout a whole input video. These facts, indicate that we are in need of effective general feature capturing. While the plain CNN architecture considers the whole input sequence, as well as each frame in the video separately, the Temporal Segment Network (TSN) framework  [36]  operates on a sequence of short snippets sparsely sampled from the entire video. Each snippet in this sequence will produce its own preliminary prediction of the emotion classes and then, a consensus among the snippets will be derived as the video-level prediction. Therefore, it allows the network to access several parts of the video, but also tackles the inability of the former to model long-range temporal structure, thus, being more likely to observe the corresponding expression.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "The overall architecture of our model is shown in Fig.  1 . Formally, given a video ùëâ , we divide it into ùêæ non-overlapping segments {ùëÜ 1 , ùëÜ 2 , ..., ùëÜ ùêæ }, to access several parts of the video, and transform them into a sequence of snippets {ùëá 1 ,ùëá 2 , ...,ùëá ùêæ }. Each snippet ùëá ùëò is produced, by randomly sampling 3 consecutive frames from its corresponding segment ùëÜ ùëò , to tackle frame redundancy. Finally, a segmental consensus function H is applied on the snippet-level predictions produced by the backbone, to obtain the final scores ùëÜ:\n\nHere F (ùëá ùëò ; W) denotes the function representing the application of a CNN with parameters W on the snippet ùëá ùëò . The CNN is equipped with a ResNet-50 backbone architecture  [16] . The consensus function H we use is average pooling and the obtained video-level scores ùëÜ are fed to a loss function L to perform the training step. This framework offers several benefits to emotion recognition. Compared to processing the entire video, the sampling process ignores redundant information in consecutive video frames, helping avoid overfitting, and offers a type of data augmentation, valuable for children emotion databases of small size.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Database",
      "text": "We perform our experiments on the EmoReact dataset  [27] , which contains 1102 videos of 63 children, aged between 4 and 14, expressing emotions while discussing different topics, collected from the YouTube channel React. Each video is annotated with one or more emotions, from a total of 8 emotion labels: Curiosity, Uncertainty, Excitement, Happiness, Surprise, Disgust, Fear, and Frustration. Therefore, we are dealing with a CRI binary multi-label classification problem. In Fig.  2 , we show the imbalance of EmoReact's training set, which means it includes an unequal number of videos for each emotion label, and is something that we must address in our upcoming model configuration choices. We can also argue that some emotions (Fear, Frustration, Disgust) are expressed in a relatively low number of samples, which results in possible lack of diversity and less ease to generalize well across unseen individuals, introducing an extra degree of difficulty to our problem.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Medical Face Mask Effect Study",
      "text": "The COVID-19 pandemic has forced people to extensively wear medical face masks, in order to prevent transmission. Motivated by this fact, we want to conduct an experimental study about the effect of medical face masks on emotion recognition, by applying a relevant mask on the EmoReact children's faces, as an attempt to simulate the face occlusion consequence.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mask Application.",
      "text": "To apply the mask, we detect the facial surface geometry using Google's MediaPipe Face Mesh  [19] , an end-to-end CNN-based model for inferring an approximate 3D mesh representation of a human face from a single image. It uses a dense mesh model of 468 vertices and is well-suited for face-based augmented reality effects. We track the 2D coordinates of the right and left jawline vertices, starting from just below the eyes until the chin, and one extra vertex for the nose, in order to form a polygon that is finally filled to represent the mask (Fig.  3 ). The jawlines for the mask are created by tracking the edge x-axis vertices and accordingly selecting among several jawline candidates, that we manually created for this particular face mesh model 1  . In Fig.  4 , we display several samples of EmoReact after the application of the mask and showcase our tool's robustness to face orientation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Body Detection.",
      "text": "In order to incorporate bodily expressions, we need a way to detect the human body. Google's MediaPipe also provides human body and hand skeleton tracking tools  [3] [39] . We combine keypoints tracked by both tools and create a bounding box with the edge points, expanded by a factor of 10% at each respective dimension, which is then cropped as the input image. This process is demonstrated in Fig.  5 , where most background noise is removed and full body information dominates the cropped image.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modality Fusion",
      "text": "We are looking to take advantage of the face and body information separately, by fusing the individual modality prediction scores with a late fusion scheme. The full body crop includes the masked face, and processing it as a single RGB input image can lead to irrelevant information confusion. The proposed method is to separate the face and body features, in order to avoid the aforementioned issue. The core model remains as is, but now processes the face crop, and the plain body crop with the corresponding face area blacked out, in two separate forward passes (Fig.  6 ). After producing the scores ùëÜ ùëì and ùëÜ ùëè from face and plain body respectively, we use a late fusion scheme to obtain the final scores ùëÜ. Finally, the overall loss L is simply the summation of the individual modality losses: L = ùêø ùëì + ùêø ùëè .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Temporal Modeling",
      "text": "The current TSN-based model processes only one of the ùëÅ consecutive frames of each snippet, being heavily based on spatial structure. This architecture naturally supports temporal modeling, by mingling information among neighboring snippet frames with the Temporal Shift Module (TSM)  [22] . TSM can be inserted into CNNs, to exploit temporality at zero computation and parameters. It shifts part of the channels of the input frames and the latent representations of each snippet along the temporal dimension, both forward and backward, thus facilitate information exchange among neighboring frames. Because information contained in the channels is no longer accessible for the current frame, the spatial modeling ability of the backbone can be harmed. To address this problem, the module is placed inside the residual branches of the ResNet, so the information in the original activation is still accessible after temporal shift, through the identity mappings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model & Training Configurations",
      "text": "The model is pretrained on AffectNet, the largest facial expression dataset. We obtain the weights of the network as provided by the PyTorch framework, achieving 59.47% accuracy on the validation set. Before feeding the input to the network, we rescale sampled RGB images from full resolution to 224 √ó 224. We train our models for 60 epochs, with stochastic gradient descent with momentum 0.9 and a batch size of 8, L2 regularization with weight decay 5e-4, and start with a learning rate of 1e-2, which is then reduced by",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we present our experimental procedure and results. First, an ablation study on the number of TSN segments is performed to explore possible trade-offs. Then, we study the medical face mask effect by comparing emotion recognition results of masked input to when the faces are visible. We examine the case of when the mask is applied to the image of the full body, as well as only the face, to compare performance between input modalities. Furthermore, visual explanation techniques are utilized to display expressive features for different modalities and emotion categories. Lastly, we report results given with the enhancement techniques, both when individually utilized and when combined.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Performance Vs Speed Trade-Off",
      "text": "In Table  1 , we perform an ablation study on the number of segments and consequently the number of snippets, which are used during the TSN training, by considering 4 different values: 1, 3, 5 and 10. By increasing the number of segments, we significantly increase computational load, and therefore inference and training time. On the other hand, when we provide the model with multiple parts of the video, it might help achieve better performance. The numbers",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mask Effect Results",
      "text": "We compare emotion recognition results between default and masked input, for face and full body crops. For face cropping, we extract the visual face features using OpenFace  [1] , an open source facial behavior analysis toolkit.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mask Effect On Face Input.",
      "text": "In Table  2 , we report results on face input. At first sight, performance drops considerably (‚âà 3-4%). This is a result we expected, as the mask covers the majority of the face, including one of the most expressive facial features, the mouth. Intuitively, if one would try to predict the emotions expressed in the two images of Table  2 , we sense that they would have a better chance without the presence of the mask. Regarding the number of segments used, performance peaks at 10, but increasing it above 3 does not result in significant performance difference. This means that the model does not necessarily create stronger temporal structure when provided with more than 3 parts of the video.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Mask Effect On Full Body",
      "text": "Input. Looking at Table  3 , which shows results on full body input, the first and most important observation we make, is that performance decrease is very little to none (0-0.4%). These results suggest that the model can exploit body information in such a way, that even with the application of a face mask, and consequently face information loss, it only suffers minimal performance drop. We also note the same pattern of performance with the masked face input results, which is better performance as complexity goes up. However, performance increase from 3 to 10 segments is minimal (0.1%), which again suggests working towards the speed side of the trade-off.  input gives superior results over face crop. With black we highlight the best overall result, whereas with blue we highlight the result of the suggested optimal model, regarding the performance vs speed trade-off discussed earlier. The obvious conclusion is that moving towards bodily expression recognition is our best option, when the face is occluded. However, this is only a baseline result, which we could build on and pursue improvements by enhancing our model.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Per Emotion Performance",
      "text": "In Fig.  7 , we report per emotion ROC AUC and compare masked face versus masked full body input performance. Full body outperforms face in all emotions, except for Excitement and Frustration. This could be translated as these two emotions being expressed more by facial than bodily features from the children involved and  incorporating the body in this case misleads the network. For Fear, performance is a lot higher with full body compared to face, which intuitively makes sense as children tend to utilize their body more to express fear  [10] . Happiness is not conventionally an emotion with intense expressions, as most people think of just a simple smile, which is obstructed by the mask, but the model manages to recognize it at a decent level. Another conclusion we could come up to is that for some emotion pairs, like Curiosity-Uncertainty or Excitement-Surprise, which intuitively are quite similar to each other, performance might be lower for each emotion individually, because it is harder for the model to distinguish one from the other.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Visual Explanation",
      "text": "To have a better understanding of the mask effect on performance, we utilize a technique for producing visual explanations for predictions. We wish to explore where our model focuses in the input image and how its behaviour varies for the different emotion category targets. The method we choose is Grad-CAM  [33] , which uses the gradients of an emotion target flowing into the final convolutional layer, to produce a coarse localization map that highlights the important regions in the image for predicting that particular emotion. We provide some example frames of the activation mapping, where model focus increases from blue to red. Starting from the face examples (Fig.  8 ), we can see that the model focuses on the upper part of the face. The facial features Regarding the body examples (Fig.  9 ), the arms become visible and provide information that is utilized by the model. The bodily features that could be utilized are the hands (calm for Happiness, aroused for Excitement, investigating for Curiosity, fist for Frustration), the arms (wide open for Excitement), and the shoulders (shrugged for Curiosity).\n\nIn Fig.  10 , we present several examples where the model focuses not only the body, but also on the face, fusing different modality information to make predictions. This suggests that it is able to learn both facial and bodily features in a single RGB stream.\n\nOverall, the model has learned to ignore noisy features, like the mask and the background. It is crucial to note, that the background is considered noise in this dataset, as the videos were recorded in a directed setup and it can be the same for different reaction topics. There is also large variability introduced by differences in the children's appearance due to clothing, body shape, size and hairstyles. These examples specify that the model is able to overcome these difficulties and focus on the expressive features.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Enhancement Results",
      "text": "We enhance the TSN-based model with the modality fusion and temporal modeling techniques and aspire to fully overcome the consequences of the face mask, by achieving performance as high as with the unmasked input.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Formally,",
      "page": 2
    },
    {
      "caption": "Figure 2: , we show the imbalance of EmoReact‚Äôs",
      "page": 2
    },
    {
      "caption": "Figure 1: TSN-Based Model Architecture",
      "page": 3
    },
    {
      "caption": "Figure 2: EmoReact Training Set Imbalance",
      "page": 3
    },
    {
      "caption": "Figure 3: Mask Application Steps",
      "page": 3
    },
    {
      "caption": "Figure 3: ). The jawlines",
      "page": 3
    },
    {
      "caption": "Figure 5: , where most background noise is removed",
      "page": 3
    },
    {
      "caption": "Figure 4: EmoReact Masked Samples",
      "page": 4
    },
    {
      "caption": "Figure 6: ). After producing the",
      "page": 4
    },
    {
      "caption": "Figure 5: Body Detection",
      "page": 4
    },
    {
      "caption": "Figure 6: Modality Score Late Fusion Scheme",
      "page": 5
    },
    {
      "caption": "Figure 7: , we report per emotion ROC AUC and compare masked",
      "page": 5
    },
    {
      "caption": "Figure 7: Masked Input Modality per Emotion Performance",
      "page": 6
    },
    {
      "caption": "Figure 8: ), we can see that the",
      "page": 6
    },
    {
      "caption": "Figure 8: Face Decision Regions",
      "page": 6
    },
    {
      "caption": "Figure 9: Body Decision Regions",
      "page": 6
    },
    {
      "caption": "Figure 10: Mixed Decision Regions",
      "page": 6
    },
    {
      "caption": "Figure 9: ), the arms become visible",
      "page": 6
    },
    {
      "caption": "Figure 10: , we present several examples where the model focuses",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Segments": "",
          "Time per Epoch (sec.)": "Training"
        },
        {
          "Segments": "1",
          "Time per Epoch (sec.)": "6"
        },
        {
          "Segments": "3",
          "Time per Epoch (sec.)": "14"
        },
        {
          "Segments": "5",
          "Time per Epoch (sec.)": "23"
        },
        {
          "Segments": "10",
          "Time per Epoch (sec.)": "32"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Segments": "",
          "ROC AUC": "Default",
          "Performance": ""
        },
        {
          "Segments": "1",
          "ROC AUC": "0.755",
          "Performance": "‚àí2.7%"
        },
        {
          "Segments": "3",
          "ROC AUC": "0.769",
          "Performance": "‚àí3.6%"
        },
        {
          "Segments": "5",
          "ROC AUC": "0.767",
          "Performance": "‚àí3.5%"
        },
        {
          "Segments": "10",
          "ROC AUC": "0.770",
          "Performance": "‚àí2.9%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Segments": "",
          "ROC AUC": "Default",
          "Performance": ""
        },
        {
          "Segments": "1",
          "ROC AUC": "0.752",
          "Performance": "-"
        },
        {
          "Segments": "3",
          "ROC AUC": "0.759",
          "Performance": "‚àí0.1%"
        },
        {
          "Segments": "5",
          "ROC AUC": "0.758",
          "Performance": "‚àí0.4%"
        },
        {
          "Segments": "10",
          "ROC AUC": "0.761",
          "Performance": "‚àí0.2%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Segments": "",
          "ROC AUC": "Masked Face",
          "Performance": ""
        },
        {
          "Segments": "1",
          "ROC AUC": "0.728",
          "Performance": "+2.4%"
        },
        {
          "Segments": "3",
          "ROC AUC": "0.733",
          "Performance": "+2.5%"
        },
        {
          "Segments": "5",
          "ROC AUC": "0.732",
          "Performance": "+2.2%"
        },
        {
          "Segments": "10",
          "ROC AUC": "0.741",
          "Performance": "+1.8%"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Open-Face: A general-purpose face recognition library with mobile applications",
      "authors": [
        "Brandon Amos",
        "Bartosz Ludwiczuk",
        "Mahadev Satyanarayanan"
      ],
      "year": "2016",
      "venue": "Open-Face: A general-purpose face recognition library with mobile applications"
    },
    {
      "citation_id": "2",
      "title": "Body Cues, Not Facial Expressions, Discriminate Between Intense Positive and Negative Emotions",
      "authors": [
        "Hillel Aviezer",
        "Yaacov Trope",
        "Alexander Todorov"
      ],
      "year": "2012",
      "venue": "Science",
      "doi": "10.1126/science.1224313"
    },
    {
      "citation_id": "3",
      "title": "BlazePose: On-device Real-time Body Pose tracking",
      "authors": [
        "Valentin Bazarevsky",
        "Ivan Grishchenko",
        "Karthik Raveendran",
        "Tyler Zhu",
        "Fan Zhang",
        "Matthias Grundmann"
      ],
      "year": "2020",
      "venue": "BlazePose: On-device Real-time Body Pose tracking",
      "doi": "10.48550/ARXIV.2006.10204"
    },
    {
      "citation_id": "4",
      "title": "Child-Robot Interaction: Perspectives and Challenges",
      "authors": [
        "Tony Belpaeme",
        "Paul Baxter",
        "Joachim De Greeff",
        "James Kennedy",
        "Robin Read",
        "Rosemarijn Looije",
        "Mark Neerincx",
        "Ilaria Baroni",
        "Mattia Coti"
      ],
      "year": "2013",
      "venue": "Social Robotics",
      "doi": "10.1007/978-3-319-02675-6_45"
    },
    {
      "citation_id": "5",
      "title": "Emotion and sociable humanoid robots",
      "authors": [
        "Cynthia Breazeal"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies",
      "doi": "10.1016/s1071-5819(03)00018-1"
    },
    {
      "citation_id": "6",
      "title": "Wearing Face Masks Strongly Confuses Counterparts in Reading Emotions",
      "year": "2020",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2020.566886"
    },
    {
      "citation_id": "7",
      "title": "The Impact of Face Masks on the Emotional Reading Abilities of Children-A Lesson From a",
      "authors": [
        "Claus-Christian Carbon",
        "Martin Serrano"
      ],
      "year": "2021",
      "venue": "Joint School-University Project. i-Perception",
      "doi": "10.1177/20416695211038265"
    },
    {
      "citation_id": "8",
      "title": "Why bodies? Twelve reasons for including bodily expressions in affective neuroscience",
      "authors": [
        "Beatrice De"
      ],
      "year": "2009",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences",
      "doi": "10.1098/rstb.2009.0190"
    },
    {
      "citation_id": "9",
      "title": "Gerasimos Potamianos, and Petros Maragos. 2021. A Robotic Edutainment Framework for Designing Child-Robot Interaction Scenarios",
      "authors": [
        "Niki Efthymiou",
        "P Panagiotis",
        "Filntisis"
      ],
      "venue": "The 14th PErvasive Technologies Related to Assistive Environments Conference",
      "doi": "10.1145/3453892.3458048"
    },
    {
      "citation_id": "10",
      "title": "Fusing Body Posture With Facial Expressions for Joint Recognition of Affect in Child-Robot Interaction",
      "authors": [
        "P Panagiotis",
        "Niki Filntisis",
        "Petros Efthymiou",
        "Koutras"
      ],
      "year": "2019",
      "venue": "Gerasimos Potamianos, and Petros Maragos",
      "doi": "10.1109/lra.2019.2930434"
    },
    {
      "citation_id": "11",
      "title": "Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss",
      "authors": [
        "P Panagiotis",
        "Niki Filntisis",
        "Efthymiou"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020 Workshops"
    },
    {
      "citation_id": "12",
      "title": "Gerasimos Potamianos, and Petros Maragos. 2021. An Audiovisual Child Emotion Recognition System for Child-Robot Interaction Applications",
      "authors": [
        "P Panagiotis",
        "Niki Filntisis",
        "Efthymiou"
      ],
      "venue": "2021 29th European Signal Processing Conference (EUSIPCO)",
      "doi": "10.23919/EUSIPCO54536.2021.9616106"
    },
    {
      "citation_id": "13",
      "title": "Affective Personalization of a Social Robot Tutor for Children's Second Language Skills",
      "authors": [
        "Goren Gordon",
        "Samuel Spaulding",
        "Jacqueline Westlund",
        "Jin Lee",
        "Luke Plummer",
        "Marayna Martinez",
        "Madhurima Das",
        "Cynthia Breazeal"
      ],
      "year": "2016",
      "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Face masks reduce emotion-recognition accuracy and perceived closeness",
      "authors": [
        "Felix Grundmann",
        "Kai Epstude",
        "Susanne Scheibe"
      ],
      "year": "2021",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0249792"
    },
    {
      "citation_id": "15",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "Hatice Gunes",
        "Massimo Piccardi"
      ],
      "year": "2007",
      "venue": "Journal of Network and Computer Applications",
      "doi": "10.1016/j.jnca.2006.09.007"
    },
    {
      "citation_id": "16",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/cvpr.2016.90"
    },
    {
      "citation_id": "17",
      "title": "A Fast Learning Algorithm for Deep Belief Nets",
      "authors": [
        "Geoffrey Hinton",
        "Simon Osindero",
        "Yee-Whye Teh"
      ],
      "year": "2006",
      "venue": "Neural Comput",
      "doi": "10.1162/neco.2006.18.7.1527"
    },
    {
      "citation_id": "18",
      "title": "Emotion Recognition Based on Body and Context Fusion in the Wild",
      "authors": [
        "Yibo Huang",
        "Hongqian Wen",
        "Linbo Qing",
        "Rulong Jin",
        "Leiming Xiao"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "doi": "10.1109/iccvw54120.2021.00403"
    },
    {
      "citation_id": "19",
      "title": "Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs",
      "authors": [
        "Yury Kartynnik",
        "Artsiom Ablavatski",
        "Ivan Grishchenko",
        "Matthias Grundmann"
      ],
      "year": "2019",
      "venue": "Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs",
      "doi": "10.48550/ARXIV.1907.06724"
    },
    {
      "citation_id": "20",
      "title": "Modelling Empathic Behaviour in a Robotic Game Companion for Children: An Ethnographic Study in Real-World Settings",
      "authors": [
        "Iolanda Leite",
        "Ginevra Castellano",
        "Andr√© Pereira",
        "Carlos Martinho",
        "Ana Paiva"
      ],
      "year": "2012",
      "venue": "Proceedings of the Seventh Annual ACM/IEEE International Conference on Human-Robot Interaction",
      "doi": "10.1145/2157689.2157811"
    },
    {
      "citation_id": "21",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2020.2981446"
    },
    {
      "citation_id": "22",
      "title": "TSM: Temporal Shift Module for Efficient Video Understanding",
      "authors": [
        "Ji Lin",
        "Chuang Gan",
        "Song Han"
      ],
      "year": "2018",
      "venue": "TSM: Temporal Shift Module for Efficient Video Understanding",
      "doi": "10.48550/ARXIV.1811.08383"
    },
    {
      "citation_id": "23",
      "title": "ARBEE: Towards Automated Recognition of Bodily Expression of Emotion In the Wild",
      "authors": [
        "Yu Luo",
        "Jianbo Ye",
        "Reginald Adams",
        "Jia Li",
        "Michelle Newman",
        "James Wang"
      ],
      "year": "2018",
      "venue": "ARBEE: Towards Automated Recognition of Bodily Expression of Emotion In the Wild",
      "doi": "10.48550/ARXIV.1808.09568"
    },
    {
      "citation_id": "24",
      "title": "The impact of facemasks on emotion recognition, trust attribution and re-identification",
      "authors": [
        "Marco Marini",
        "Alessandro Ansani",
        "Fabio Paglieri",
        "Fausto Caruana",
        "Marco Viola"
      ],
      "year": "2021",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-021-84806-5"
    },
    {
      "citation_id": "25",
      "title": "3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism",
      "authors": [
        "Elisabeta Marinoiu",
        "Mihai Zanfir",
        "Vlad Olaru",
        "Cristian Sminchisescu"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/cvpr.2018.00230"
    },
    {
      "citation_id": "26",
      "title": "Deep Spatio-Temporal Features for Multimodal Emotion Recognition",
      "authors": [
        "Dung Nguyen",
        "Kien Nguyen",
        "Sridha Sridharan",
        "Afsane Ghasemi",
        "David Dean",
        "Clinton Fookes"
      ],
      "year": "2017",
      "venue": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV.2017.140"
    },
    {
      "citation_id": "27",
      "title": "EmoReact: A Multimodal Approach and Dataset for Recognizing Emotional Responses in Children",
      "authors": [
        "Behnaz Nojavanasghari",
        "Tadas Baltru≈°aitis",
        "Charles Hughes",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction",
      "doi": "10.1145/2993148.2993168"
    },
    {
      "citation_id": "28",
      "title": "The effect of face masks and sunglasses on identity and expression recognition with super-recognizers and typical observers",
      "authors": [
        "Eilidh Noyes",
        "Josh Davis",
        "Nikolay Petrov",
        "Katie Gray",
        "Kay Ritchie"
      ],
      "year": "2021",
      "venue": "Royal Society Open Science",
      "doi": "10.1098/rsos.201169"
    },
    {
      "citation_id": "29",
      "title": "Academic Emotions in Students' Self-Regulated Learning and Achievement: A Program of Qualitative and Quantitative Research",
      "authors": [
        "Reinhard Pekrun",
        "Thomas Goetz",
        "Wolfram Titz",
        "Raymond Perry"
      ],
      "year": "2002",
      "venue": "Educational Psychologist",
      "doi": "10.1207/s15326985ep3702_4"
    },
    {
      "citation_id": "30",
      "title": "Leveraging Semantic Scene Characteristics and Multi-Stream Convolutional Architectures in a Contextual Approach for Video-Based Visual Emotion Recognition in the Wild",
      "authors": [
        "Ioannis Pikoulis",
        "P Panagiotis",
        "Petros Filntisis",
        "Maragos"
      ],
      "year": "2021",
      "venue": "Leveraging Semantic Scene Characteristics and Multi-Stream Convolutional Architectures in a Contextual Approach for Video-Based Visual Emotion Recognition in the Wild",
      "doi": "10.48550/ARXIV.2105.07484"
    },
    {
      "citation_id": "31",
      "title": "Child-computer interaction",
      "authors": [
        "J Read",
        "P Markopoulos"
      ],
      "year": "2013",
      "venue": "International Journal of Child-Computer Interaction",
      "doi": "10.1016/j.ijcci.2012.09.001"
    },
    {
      "citation_id": "32",
      "title": "The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Pla",
      "authors": [
        "Byron Reeves",
        "Clifford Nass"
      ],
      "year": "1996",
      "venue": "The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Pla"
    },
    {
      "citation_id": "33",
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "authors": [
        "R Ramprasaath",
        "Michael Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2016",
      "venue": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "doi": "10.48550/ARXIV.1610.02391"
    },
    {
      "citation_id": "34",
      "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "How does the presence of a surgical face mask impair the perceived intensity of facial emotions?",
      "authors": [
        "Maria Tsantani",
        "Vita Podgajecka",
        "Katie Gray",
        "Richard Cook"
      ],
      "year": "2022",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0262344"
    },
    {
      "citation_id": "36",
      "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
      "authors": [
        "Limin Wang",
        "Yuanjun Xiong",
        "Zhe Wang",
        "Yu Qiao",
        "Dahua Lin",
        "Xiaoou Tang",
        "Luc Van Gool"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016"
    },
    {
      "citation_id": "37",
      "title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition",
      "authors": [
        "Sijie Yan",
        "Yuanjun Xiong",
        "Dahua Lin"
      ],
      "year": "2018",
      "venue": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition",
      "doi": "10.48550/ARXIV.1801.07455"
    },
    {
      "citation_id": "38",
      "title": "Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation",
      "authors": [
        "Zhengyuan Yang",
        "Amanda Kay",
        "Yuncheng Li",
        "Wendi Cross",
        "Jiebo Luo"
      ],
      "year": "2020",
      "venue": "Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation",
      "arxiv": "arXiv:2011.00043"
    },
    {
      "citation_id": "39",
      "title": "MediaPipe Hands: On-device Real-time Hand Tracking",
      "authors": [
        "Fan Zhang",
        "Valentin Bazarevsky",
        "Andrey Vakunov",
        "Andrei Tkachenka",
        "George Sung",
        "Chuo-Ling Chang",
        "Matthias Grundmann"
      ],
      "year": "2020",
      "venue": "MediaPipe Hands: On-device Real-time Hand Tracking",
      "doi": "10.48550/ARXIV.2006.10214"
    }
  ]
}