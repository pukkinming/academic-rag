{
  "paper_id": "2403.13659v4",
  "title": "Recursive Joint Cross-Modal Attention For Multimodal Fusion In Dimensional Emotion Recognition",
  "published": "2024-03-20T15:08:43Z",
  "authors": [
    "R. Gnana Praveen",
    "Jahangir Alam"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Though multimodal emotion recognition has achieved significant progress over recent years, the potential of rich synergic relationships across the modalities is not fully exploited. In this paper, we introduce Recursive Joint Cross-Modal Attention (RJCMA) to effectively capture both intra-and inter-modal relationships across audio, visual, and text modalities for dimensional emotion recognition. In particular, we compute the attention weights based on cross-correlation between the joint audio-visual-text feature representations and the feature representations of individual modalities to simultaneously capture intra-and intermodal relationships across the modalities. The attended features of the individual modalities are again fed as input to the fusion model in a recursive mechanism to obtain more refined feature representations. We have also explored Temporal Convolutional Networks (TCNs) to improve the temporal modeling of the feature representations of individual modalities. Extensive experiments are conducted to evaluate the performance of the proposed fusion model on the challenging Affwild2 dataset. By effectively capturing the synergic intra-and inter-modal relationships across audio, visual, and text modalities, the proposed fusion model achieves a Concordance Correlation Coefficient (CCC) of 0.585 (0.542) and 0.674 (0.619) for valence and arousal respectively on the validation set (test set). This shows a significant improvement over the baseline of 0.240 (0.211) and 0.200 (0.191) for valence and arousal, respectively, in the validation set (test set), achieving second place in the valence-arousal challenge of the 6th Affective Behavior Analysis in-the-Wild (ABAW) competition. The code is available on GitHub: https: //github.com/praveena2j/RJCMA.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is a challenging problem due to the diverse nature of expressions associated with emotional states across individuals and cultures  [1] . It has a wide range of applications in various fields such as health care (assessing pain, fatigue, depression)  [2, 3] , autonomous driving (assessing driver emotional states)  [4] , robotics (for realistic human-computer interaction)  [5] , etc. Emotion recognition is often explored in the literature as a classification problem of categorizing emotions into seven classes: anger, disgust, fear, happiness, sadness, surprise, and contempt  [6] . Recently, compound expression datasets have also been introduced to capture human emotions beyond seven classes  [7] . However, they also do not capture the diverse range of emotions expressed by humans. Therefore, a dimensional model of emotions has been introduced to capture the diverse range of emotions, predominantly using valence and arousal. Valence denotes the range of emotions from very sad (negative) to very happy (positive), whereas arousal represents the intensities of emotions from being very passive (sleepiness) to very active (high excitement)  [8] . Dimensional Emotion Recognition (DER) is more challenging than categorical emotion recognition, as they are highly prone to label noise due to the complex process of obtaining dimensional annotations.\n\nMultimodal learning has been recently gaining a lot of attention as it can offer rich complementary information across multiple modalities, which can play a crucial role in outperforming unimodal approaches  [9] . Human emotions are communicated in complex ways through various modalities such as face, voice, and language. Multimodal emotion recognition aims to effectively fuse audio, visual, and text by capturing the rich intra-and inter-modal complementary relationships across the modalities. Early approaches of multimodal fusion for DER either rely on Long Short Term Memory (LSTM)-based fusion  [10, 11]  or early feature concatenation  [12, 13] . With the advent of transformers  [14] , attention models using multimodal transformers have attained much interest to effectively combine multiple modalities for DER  [15] [16] [17] . Recently, cross-modal attention was found to be quite promising in capturing the rich inter-modal complementary relationships across the modalities, which has been successfully applied for several appli-arXiv:2403.13659v4 [cs.CV] 13 Apr 2024 cations such as action localization  [18] , emotion recognition  [19] , and person verification  [20] . Praveen et al.  [21, 22]  explored joint cross-attention by introducing joint feature representation in the cross-attentional framework and achieved significant improvement over vanilla cross-attention  [19] . They have further improved the performance of the model by introducing recursive fusion, and LSTMs for temporal modeling of individual feature representations as well as the audio-visual representations  [23] . Recursive fusion of the cross-attention models has also been successfully explored for other audio-visual tasks such as event localization  [24]  and person verification  [25] . However, most of the existing cross-attention based models are focused on audio-visual fusion for DER.\n\nIn this work, we have investigated the prospect of efficiently capturing the synergic intra-and inter-modal relationships across audio, visual, and text modalities for DER. By deploying cross-correlation between the joint audiovisual-text feature representation and feature representations of individual modalities, we can simultaneously capture intra-and inter-modal relationships across the modalities. Inspired by the performance of recursive attention models  [23, 24] , we have also incorporated a recursive fusion of audio, visual, and text modalities in the context of joint cross-attentional fusion to obtain more refined feature representations. The major contributions of the paper can be summarized as follows.\n\n• Joint cross-modal attention is explored among audio, visual, and text modalities using a joint audio-visual-text feature representation to simultaneously capture intraand inter-modal relationships across the modalities. • Recursive fusion is used to further improve the feature representations of individual modalities. TCNs are also used to improve the temporal modeling of individual feature representations. • Extensive experiments are conducted to evaluate the robustness of the proposed approach on the challenging Af-fwild2 dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "One of the early approaches using deep learning architectures for DER was proposed by Tzirakis et al.  [10] , where they explored 1D Convolutional Neural Networks (CNN) for audio and Resnet-50  [26]  for visual modality. The deep features are then fused using LSTMs for estimating predictions of valence and arousal. With the advancement of 3D CNN models, Kuhnke et al.  [12]  showed performance improvement using R3D  [27]  for visual modality and Resnet-18  [26]  for audio modality with simple feature concatenation. Kollias et al.  [28] [29] [30]  explored DER along with other tasks of classification of expressions and action units in a unified framework. Another widely explored line of research for DER is based on knowledge distillation (KD)  [31] . Schoneveld et al.  [11]  explored KD for obtaining robust visual representations, while Wang et al.  [32]  and Deng et al.  [33]  attempted to leverage KD to deal with label uncertainties. Recently, KD is also explored along with the paradigm of Learning Under Privileged Information (LUPI) to efficiently exploit multiple modalities for DER  [34] . Li et al.  [35]  proposed Decoupled Multimodal Distillation (DMD) to mitigate the issues of multimodal heterogeneities by dynamically distilling the modality-relevant information across the modalities. Although these methods have shown promising performance by exploiting multiple modalities, they do not focus on capturing the synergic relationships pertaining to intra-and inter-modal relationships across the modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Attention Models For Multimodal Emotion Recognition",
      "text": "Inspired by the performance of transformers  [14] , several approaches have been proposed to investigate the potential of transformers for DER. Most of the existing works explored transformers to encode the concatenated version of the feature representations of individual modalities  [16, 17, 36, 37] . Tran et al.  [38]  showed that fine-tuning transformers with cross-modal attention trained on a large-scale voxceleb2  [39]  dataset helps to improve the performance of multimodal emotion recognition. Huang et al.  [40]  explored multimodal transformers along with self-attention modules, where audio modality is used to attend to visual modality to produce robust multimodal feature representations. Parthasarathy et al.  [15]  further extended their idea by employing cross-modal attention in a bidirectional fashion, where audio modality is used to attend to visual modality and vice-versa. Karas et al.  [41]  provided a comprehensive evaluation of the fusion models based on selfattention, cross-attention and LSTMs for DER. Zhang et al.  [42]  proposed leader-follower attention, where audio, visual, and text modalities are combined to obtain the attended feature representations using the modality-wise attention scores, which is further concatenated to visual features for final predictions. Praveen et al.  [19]  explored cross-attention based on cross-correlation across the feature representations of audio and visual modalities. They further extended their approach by employing joint audio-visual feature representation in the cross-attentional framework and showed significant performance improvement  [21] . Praveen et al.  [23]  improved the performance by introducing recursive fusion and LSTMs for temporal modeling of individual modalities. In this work, we further extend the idea of  [23]  by incorporating text modality, and TCNs to effectively capture the intra-modal relationships. The proposed approach primar-ily differs from  [23]  in three aspects :  (1)  In this work, we employ text modality in the framework of recursive joint cross-attention in addition to audio and visual modalities, whereas  [23]  uses only audio and visual modalities. (2) In  [23] , LSTMs are used, whereas we have deployed TCNs as they are found to be effective in improving the temporal modeling of individual modalities. (3) In  [23] , R3D  [27]  is used for visual backbone and Resnet 18 for audio modality, whereas we have used Resnet-50 fine-tuned on FER+  [43]  for visual and VGG  [44]  pretrained on Audioset for audio modality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Approach",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual Network",
      "text": "Facial expressions play a significant role in conveying the emotional state of a person. In videos, spatial information provide the semantic regions of face pertinent to the expressions, whereas temporal dynamics convey the evolution of expressions in videos across the frames. Therefore, effectively modeling the spatial and temporal dynamics of facial expressions in videos is crucial to obtain robust visual feature representations. Several approaches have been explored using 2D CNNs in conjunction with LSTMs, where 2D CNNs are used to encode the spatial information and LSTMs for temporal dynamics of facial expressions  [45, 46] . With the advent of 3D CNN models  [27] , they are successfully explored for DER  [12, 21, 22]  and showed improvement over 2D CNNs with LSTMs. It has also been shown that 3DCNNs in combination with LSTMs are effective in capturing the temporal dynamics  [23] , where 3DCNNs are efficient in capturing short-term dynamics and LSTMs are robust in modeling long-term dynamics. Recently, TCNs are found to be promising to effectively capture the temporal dynamics for DER  [36, 42] . In this work, we have used Resnet-50  [26]  pretrained on MS-CELEB-1M dataset  [47] , which is further fine-tuned on FER+  [43]  dataset similar to that of  [42] . We have further used TCNs to effectively capture the temporal dynamics of facial expressions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Network",
      "text": "Speech-based emotion recognition is another promising research area due to the rich emotion-relevant information in the vocal expressions. With the advancement of deep learning models, vocal expressions are encoded using 1D CNNs with raw speech signals  [10]  or 2D CNNs with spectrograms  [11, 12] . Some of the works also explored deep features in combination with conventional hand-crafted features to encode the vocal expressions  [16, 48] . Recently, spectrograms have been widely explored as they are found to be efficient in capturing the affective states of the vocal expressions  [21, 42] . Therefore, we have also explored spectrograms with 2D CNNs to encode the vocal expressions. Specifically, we have used VGG-Net architecture pretrained on large-scale audioset dataset  [49] . Similar to visual modality, we have also used TCNs to encode the temporal dynamics of frame-level vocal embeddings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Text Network",
      "text": "Text modality is another predominantly explored modality for emotion detection, which carries semantic emotionrelevant information in the text data  [50] . Effectively leveraging the textual data can boost the performance of multimodal fusion as they can offer significant emotion-relevant information and complement audio and visual modalities. Based on transformers, BERT features are predominantly explored text encoders for emotion recognition in the literature  [51] . Therefore, we also used BERT as text encoder, followed by TCNs to encode the temporal information across the word embeddings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Recursive Joint Cross-Modal Attention",
      "text": "Given the input video sub-sequence S of K frames, the audio, visual and text data are preprocessed and fed to the corresponding encoders, followed by TCNs to obtain the feature representations of the respective modalities as shown in Fig 1 . The feature representations of the audio, visual and text modalities are denoted by Given the audio (A), Visual (V), and Text (T) feature representations, X a , X v , and X t the joint feature representation (J ) of audio, visual, and text modalities is obtained by concatenating the feature vectors of all modalities, followed by fully connected layer as\n\nwhere d = d a + d v + d t denotes the dimensionality of J and F C denote fully connected layer. Now J is fed to the joint cross-attentional framework of the respective modalities as shown in Fig  2  to attend to the feature representations of individual modalities. This helps to simultaneously encode the intra-and inter-modal relationships within the same modalities as well as across the modalities in obtaining the attention weights. The crosscorrelation between the J and X a are obtained as joint cross-correlation matrix C a , which is given by\n\nwhere W ja ∈ R da×d represents learnable weight matrix across the X a and J . Similarly, the cross-correlation matrices of other two modalities C v and C t are obtained as\n\nwhere W jv ∈ R dv×d , W jt ∈ R dt×d represents learnable weight matrices for visual and text modalities respectively. The obtained joint cross-correlation matrices of the individual modalities are used to compute the attention weights, thereby capturing the semantic relevance of both across and within the same modalities. Higher correlation coefficient of the joint cross-correlation matrices denote higher semantic relevance pertinent to the intra-and inter-modal relationships of the corresponding feature vectors. Now the joint cross-correlation matrices are used to compute the attention maps of the individual modalities. For the audio modality, the joint correlation matrix C a and the corresponding audio features X a are combined using the learnable weight matrix W ca , followed by ReLU activation function to compute the attention maps H a , which is given by\n\nwhere W ca ∈ R K×K denote learnable weight matrix for audio modality. Similarly the attention maps of visual and text modalities are obtained as\n\nwhere W cv ∈ R K×K , W ct ∈ R K×K are the learnable weight matrices of visual and text modalities respectively. Now the attention maps are used to compute the attended features of the individual modalities as:\n\nX att,t = H t W ht + X t  (10)  where W ha ∈ R K×K , W hv ∈ R K×K , and W ht ∈ R K×K denote the learnable weight matrices for audio, visual and text modalities respectively.\n\nIn order to obtain more refined feature representations, the attended features of each modality are again fed as input to the respective joint cross-modal attention module, which is given by\n\nwhere\n\nht ∈ R K×K , and\n\nht ∈ R K×K denote the learnable weight matrices of audio, visual and text modalities respectively and l refers to the recursive step.\n\nThe attended features of the individual modalities after l iterations are concatenated to obtain the multimodal feature representation X (l) att , shown as: att is fed to regression layers (Multi Layer Perceptron) for the final prediction of valence or arousal.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "Affwild is the largest database in the field of affective computing, consisting of 298 videos captured under extremely challenging conditions from YouTube videos  [52] . The database is extended substantially to foster the development of robust models for estimating valence and arousal by introducing 260 more videos, resulting in a total of 558 videos with 1.4M frames  [53] . The database has been further expanded by adding new videos in the subsequent series of ABAW challenges  [54] [55] [56] [57] [58] [59] . In the 6th ABAW challenge  [60]  for the valence-arousal track, the dataset is provided with 594 videos of around 2, 993, 081 frames from 584 subjects. Sixteen of these videos display two subjects, both of which have been annotated. The final annotations are obtained by taking an average of annotations provided by four experts, using a joystick. The annotations for valence and arousal are provided continuously in the range of [-1, 1]. The dataset is partitioned into the training, validation, and test sets in a subject-independent manner to ensure that each subject appears exclusively in one partition. The partitioning resulted in 356, 76, and 162 videos for train, validation, and test partitions respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preprocessing",
      "text": "For the visual modality, we have used the cropped and aligned images provided by the challenge organizers  [60] .\n\nFor the missing faces in the video frames, we have considered black frames (i.e., zero pixels), and the video frames with no annotations of valence and arousal i.e., with annotations of -5 are excluded. The given video sequences are divided into sub-sequences of 300 (i.e.,K = 300) with a stride of 200 frames and the facial images are resized to 48 × 48.\n\nFor audio modality, the speech signals are extracted from the corresponding videos with a sampling rate of 16KHz. The log melspectrograms are then obtained using the preprocessing code provided by the Vggish repository 1  . To ensure that the audio modality is properly synchronized with the sub-sequences of other modalities, we have used a hop length of 1/f ps of the raw videos to extract the spectrograms.\n\nFor the text modality, the extracted speech signals from audio preprocessing are fed to the pretrained speech recognition model of Vosk toolkit 2  to obtain the recognized words along with word-level timestamps. Next, a pretrained punctuation restoration and capitalization model 3  is used to restore the punctuations of the recognized words, which carries semantic information pertinent to emotional states. Now BERT features are extracted at word-level using a pre-trained BERT model  4  . The word-level features are computed by taking a summation of the last four layers of the BERT model  [61] . The recognized words may usually span a larger time window of multiple frames. In order to synchronize the word-level BERT features of text modality with audio and visual modalities, the word-level text embedding is populated as per the timestamp of each word by reassigning the same word-level feature to all the frames within the time span of the corresponding word.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Training Details",
      "text": "For visual modality, random flipping, and random crop with a size of 40 are used for data augmentation in training, while only center crop is used for validation. For audio and visual features, the input data is normalized in order to have a mean and standard deviation of 0.5. For text modality, the BERT features are normalized to ensure the mean as 0 and standard deviation as 1. Adam optimizer is used with a weight decay of 0.001 and batch size is set to be 12. The models are trained separately for valence and arousal. The maximum number of epochs is set to be 100 and early stopping is employed to avoid over-fitting. The hyperparameters of the initial learning rate and minimum learning rate are set to be 1e -5 and 1e -8 respectively. In our training strategy, we have deployed a warm-up scheme using ReduceLROnP lateau scheduler with patience of 5 and a factor of 0.1 based on the CCC score of the validation partition. It has been shown that gradual training of the backbones of individual modalities along with the fusion model by gradually fine-tuning the layers of the backbones helps to improve the performance of the system  [42] . Therefore, we have deployed a similar strategy in our training framework, where three groups of layers for visual (Resnet-50) and audio (VGG) backbones are progressively selected for fine-tuning. Initially at epoch 0, the first group is unfrozen and the learning rate is linearly warmed up to 1e -5 within an epoch. Then repetitive warm-up is employed until epoch 5, after which ReduceLROnP lateau is used to update the learning rate. The learning rate is gradually dropped with a factor of 0.1 until validation CCC does not improve over 5 consecutive epochs. After which the second group is unfrozen and the learning rate is reset to 1e -5, followed by a warm-up scheme with ReduceLROnP lateau. The procedure is repeated till all the layers are fine-tuned for audio and visual backbones. Also, note that the best model state dictionary over prior epochs is loaded at the end of each epoch to mitigate the issues of over-fitting. To further control the problem of over-fitting, we have employed cross-validation with 6 folds, where the fold 0 partition is the same as the original partition provided by the organizers  [60] . The results obtained from the 6-fold cross-validation are shown in Table  1 . In all these experiments, we have used 3 iterations in the fusion model (i.e., l=3).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Loss Function",
      "text": "The concordance correlation coefficient (ρ c ) is the widelyused evaluation metric in the literature for DER to measure the level of agreement between the predictions (x) and ground truth (y) annotations  [10] . Let µ x , and µ y represent the mean of predictions and ground truth, respectively. Similarly, σ 2 x and σ 2 y denote the variance of predictions and ground truth, respectively, then ρ c between the predictions and ground truth can be obtained as:\n\nwhere σ 2 xy denotes the covariance between predictions and ground truth. Though Mean Square Error (MSE) is the commonly used loss function for regression models, we use the CCC-based loss function as it is a standard loss function in the literature for DER  [10, 21] , which is given by",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "In order to understand the impact of the recursive mechanism, we have conducted a series of experiments by varying the number of recursions as shown in Table  3 . First, we did an experiment with a single recursion, which is the same as joint cross attention  [21]  for audio, visual, and text modalities. Now we executed multiple experiments by slowly increasing the number of recursions and found that the performance of the system gradually increases with multiple recursions. This shows that the recursive mechanism helps in obtaining more robust feature representations by progressively refining the features. We have achieved the best results at 3 iterations, beyond that the performance of the system declines. We hypothesize that this can be attributed to the fact that though recursive fusion initially helps to improve the performance, it may result in over-fitting with more iterations, resulting in a performance decline on the validation set. A similar trend of performance improvement with multiple recursions is also observed for other audiovisual tasks such as person verification  [25]  and event localization  [24] , thereby underscoring our hypothesis.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison To State-Of-The-Art",
      "text": "Most of the approaches evaluated on the Affwild2 dataset have been submitted to previous ABAW challenges. Therefore, we have compared the performance of the proposed approach with some of the relevant state-of-the-art models of previous ABAW challenges as shown in Table  2 . Several approaches explored ensemble-based methods using multiple encoders for each modality, followed by transformers to encode the concatenated multimodal feature representation  [16, 17, 36] . By exploiting multiple backbones and large-scale training with external datasets, Meng et al.  [16]  significantly improved the test set performance on both valence and arousal. Similarly, Zhou et al.  [36]  also explored multiple backbones and showed better improvement in the arousal performance of the test set than  [16] . Even though ensemble-based methods show better performance on test set, it is often cumbersome and computationally expensive. Zhang et al.  [37]  showed that exploring Masked Auto-Encoders (MAE) is a promising line of research to achieve better generalization and consistently improved performance in both valence and arousal. Zhang et al.  [42]  explored the leader-follower attention model for multimodal fusion, where audio and text modalities are leveraged to attend to the visual modality, and showed good performance on the test set without the need for multiple backbones for each modality.\n\nPraveen et al.  [21]  proposed Joint Cross Attention (JCA) by introducing a joint feature representation in the crossattention framework and showed significant improvement in the validation set, especially for valence. They further improved the performance of their approach by deploying a recursive mechanism and LSTMs for the temporal modeling of individual modalities and multimodal feature representations  [23] . However, they do not seem to have a better generalization ability as they fail to show improvement in the test set. We hypothesize that this may be due to naive audio and visual backbones, fine-tuned on Affwild2 dataset. Therefore, to have a fair comparison with  [23] , we have reimplemented their fusion model by replacing their audio and visual backbones with Vggish  [44]  and Resnet-50 pre-trained on MSCELEB-1M and FER+  [42] , followed by TCNs. By replacing the visual and audio backbones of  [23]  and progressive fine-tuning of the backbones similar to that of  [42] , the problem of over-fitting has been mitigated and showed improvement in test set performance also. We can observe that performance has been improved in both validation and test sets by introducing text modality into the RJCA framework  [23] , especially for arousal. Note that even though the performance of the valence for  [37, 42]  as well as our approach on the official validation set is lower, they achieved better performance on other folds of crossvalidation, thus improving the performance of the test set.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results On Test Set",
      "text": "Multimodal learning is gaining a lot of attention for affective behavior analysis as most of the methods submitted for the valence-arousal challenge of the 6th ABAW competition  [60]  employed multimodal fusion  [62, 64, 65, 67] . Some of these methods explored ensemble-based fusion for better generalization ability  [62, 64] . Another widely explored strategy to improve the test set performance is to exploit pre-training with multiple large-scale datasets using Masked Auto-Encoders (MAE)  [62, 63]  or multiple backbones for each modality  [64] [65] [66] . Netease Fuxi AI Lab  [62]  used MAEs pretrained with 5 external data sets of around 262M images and Vggish model  [44]  for the audio modality, followed by ensemble-based fusion, and showed a significant improvement for both valence and arousal, achieving first place in the challenge. Similarly, CtyunAI  [63]  also used MAE for visual modality pre-trained with 4 external datasets, followed by TCNs for temporal modeling, and achieved very good performance for arousal than valence. Sun-CE  [64]  explored multiple backbones for both audio and visual modalities, followed by ensemble-based fusion, while USTC-IAT-United  [65]  used multiple backbones only for the audio modality and achieved decent performance for both valence and arousal. KBS-DGU and ETS-LIVIA  [67]  exploited combining features from multiple fusion models of self-attention and cross-attention models for improving the performance of audio-visual fusion. KBS-DGU explored both self-attention and cross-attention models, while ETS-LIVIA  [67]  used joint feature representation with cross-attention models across audio and visual modalities. Unlike other approaches, we have explored the prospect of simultaneously capturing the intra-and intermodal relationships across audio, visual, and text modal-ities recursively and achieved very good performance for both valence and arousal without using multiple backbones or pre-training with large-scale external datasets. Therefore, the performance of our approach can be solely attributed to the robustness of the sophisticated fusion model, providing a cost-effective solution for affective behavior analysis inthe-wild.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we showed that effectively capturing the synergic relationships pertinent to the intra-and inter-modal characteristics among the audio, visual, and text modalities can significantly improve the performance of the system. By introducing the joint representation in the crossattentional framework, we can simultaneously capture both intra-and inter-modal relationships across the audio, visual, and text modalities. The performance of the system is further enhanced using recursive fusion by progressively refining the features to obtain robust multimodal feature representations. Experimental results in the challenging Affwild2 dataset indicate that the proposed model can achieve a better multimodal fusion performance, outperforming most methods for both valence and arousal. The performance of the system can also be improved by leveraging advanced text encoders and sophisticated backbones for the individual modalities.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The feature representations of the audio, visual and",
      "page": 3
    },
    {
      "caption": "Figure 2: to attend to the",
      "page": 3
    },
    {
      "caption": "Figure 1: Illustration of the proposed framework with the recursive joint cross-modal attention. Best viewed in color.",
      "page": 4
    },
    {
      "caption": "Figure 2: Joint Cross-Modal Attention blocks of audio, visual and text modalities",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold 0": "Fold 1",
          "0.455": "0.585",
          "0.652": "0.674",
          "0.553": "0.629"
        },
        {
          "Fold 0": "Fold 2",
          "0.455": "0.467",
          "0.652": "0.631",
          "0.553": "0.549"
        },
        {
          "Fold 0": "Fold 3",
          "0.455": "0.536",
          "0.652": "0.647",
          "0.553": "0.591"
        },
        {
          "Fold 0": "Fold 4",
          "0.455": "0.432",
          "0.652": "0.629",
          "0.553": "0.526"
        },
        {
          "Fold 0": "Fold 5",
          "0.455": "0.463",
          "0.652": "0.615",
          "0.553": "0.539"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zhang et al. [37]": "Zhang et al. [42]",
          "Transformers": "Leader-Follower",
          "0.464": "0.441",
          "0.640": "0.645",
          "0.648": "0.552",
          "0.625": "0.629"
        },
        {
          "Zhang et al. [37]": "Zhou et al. [36]",
          "Transformers": "Transformers",
          "0.464": "0.550",
          "0.640": "0.681",
          "0.648": "0.500",
          "0.625": "0.632"
        },
        {
          "Zhang et al. [37]": "Zhang et al. [17]",
          "Transformers": "Transformers",
          "0.464": "0.554",
          "0.640": "0.659",
          "0.648": "0.523",
          "0.625": "0.545"
        },
        {
          "Zhang et al. [37]": "Meng et al. [16]",
          "Transformers": "Transformers",
          "0.464": "0.588",
          "0.640": "0.668",
          "0.648": "0.606",
          "0.625": "0.596"
        },
        {
          "Zhang et al. [37]": "Praveen et al [21]",
          "Transformers": "JCA",
          "0.464": "0.663",
          "0.640": "0.584",
          "0.648": "0.374",
          "0.625": "0.363"
        },
        {
          "Zhang et al. [37]": "Praveen et al [23]",
          "Transformers": "RJCA",
          "0.464": "0.703",
          "0.640": "0.623",
          "0.648": "0.467",
          "0.625": "0.405"
        },
        {
          "Zhang et al. [37]": "Praveen et al [23]⋆",
          "Transformers": "RJCA",
          "0.464": "0.443",
          "0.640": "0.639",
          "0.648": "0.537",
          "0.625": "0.576"
        },
        {
          "Zhang et al. [37]": "RJCMA (Ours)",
          "Transformers": "RJCMA",
          "0.464": "0.455",
          "0.640": "0.652",
          "0.648": "0.542",
          "0.625": "0.619"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "l = 1": "l = 2",
          "0.568": "0.576",
          "0.656": "0.669",
          "0.607": "0.618"
        },
        {
          "l = 1": "l = 3",
          "0.568": "0.585",
          "0.656": "0.674",
          "0.607": "0.629"
        },
        {
          "l = 1": "l = 4",
          "0.568": "0.579",
          "0.656": "0.652",
          "0.607": "0.615"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: CCC of the proposed approach on Affwild2 test set compared to other methods submitted to 6th ABAW competition. Bold",
      "data": [
        {
          "Netease Fuxi AI Lab [62]": "RJCMA (Ours)",
          "Audio, Visual": "Audio, Visual, Text",
          "Yes": "No",
          "0.6873": "0.5418",
          "0.6569": "0.6196",
          "0.6721": "0.5807"
        },
        {
          "Netease Fuxi AI Lab [62]": "CtyunAI [63]",
          "Audio, Visual": "Visual",
          "Yes": "No",
          "0.6873": "0.5223",
          "0.6569": "0.6057",
          "0.6721": "0.5640"
        },
        {
          "Netease Fuxi AI Lab [62]": "SUN-CE [64]",
          "Audio, Visual": "Audio, Visual",
          "Yes": "Yes",
          "0.6873": "0.5355",
          "0.6569": "0.5861",
          "0.6721": "0.5608"
        },
        {
          "Netease Fuxi AI Lab [62]": "USTC-IAT-United [65]",
          "Audio, Visual": "Audio, Visual",
          "Yes": "No",
          "0.6873": "0.5208",
          "0.6569": "0.5748",
          "0.6721": "0.5478"
        },
        {
          "Netease Fuxi AI Lab [62]": "HSEmotion [66]",
          "Audio, Visual": "Visual",
          "Yes": "No",
          "0.6873": "0.4925",
          "0.6569": "0.5461",
          "0.6721": "0.5193"
        },
        {
          "Netease Fuxi AI Lab [62]": "KBS-DGU∗",
          "Audio, Visual": "Audio, Visual",
          "Yes": "No",
          "0.6873": "0.4836",
          "0.6569": "0.5318",
          "0.6721": "0.5077"
        },
        {
          "Netease Fuxi AI Lab [62]": "ETS-LIVIA [67]",
          "Audio, Visual": "Audio, Visual",
          "Yes": "No",
          "0.6873": "0.4198",
          "0.6569": "0.4669",
          "0.6721": "0.4434"
        },
        {
          "Netease Fuxi AI Lab [62]": "CAS-MAIS∗",
          "Audio, Visual": "Audio, Visual, Text",
          "Yes": "No",
          "0.6873": "0.4245",
          "0.6569": "0.3414",
          "0.6721": "0.3830"
        },
        {
          "Netease Fuxi AI Lab [62]": "IMLAB [68]",
          "Audio, Visual": "Visual",
          "Yes": "No",
          "0.6873": "0.2912",
          "0.6569": "0.2456",
          "0.6721": "00.2684"
        },
        {
          "Netease Fuxi AI Lab [62]": "Baseline [60]",
          "Audio, Visual": "Visual",
          "Yes": "No",
          "0.6873": "0.2110",
          "0.6569": "0.1910",
          "0.6721": "0.2010"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective image content analysis: Two decades review and new perspectives",
      "authors": [
        "Sicheng Zhao",
        "Xingxu Yao",
        "Jufeng Yang",
        "Guoli Jia",
        "Guiguang Ding",
        "Tat-Seng Chua",
        "Björn Schuller",
        "Kurt Keutzer"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Deep weakly supervised domain adaptation for pain localization in videos",
      "authors": [
        "Eric Gnana Praveen",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "3",
      "title": "Deep domain adaptation with ordinal regression for pain assessment using weakly-labeled videos",
      "authors": [
        "Eric Gnana Praveen Rajasekhar",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2021",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "4",
      "title": "Emotion detection and face recognition of drivers in autonomous vehicles in iot platform. Image and Vision Computing",
      "authors": [
        "Zhongshan Chen",
        "Xinning Feng",
        "Shengwei Zhang"
      ],
      "year": "2022",
      "venue": "Emotion detection and face recognition of drivers in autonomous vehicles in iot platform. Image and Vision Computing"
    },
    {
      "citation_id": "5",
      "title": "Affective multimodal human-computer interaction",
      "authors": [
        "Maja Pantic",
        "Nicu Sebe",
        "Jeffrey Cohn",
        "Thomas Huang"
      ],
      "year": "2005",
      "venue": "Proceedings of the 13th Annual ACM International Conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "7",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "9",
      "title": "Deep multimodal complementarity learning",
      "authors": [
        "Daheng Wang",
        "Tong Zhao",
        "Wenhao Yu",
        "Nitesh Chawla",
        "Meng Jiang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "10",
      "title": "End-to-end multimodal er using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "Mihalis Nicolaou",
        "Björn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2006",
      "venue": "IEEE J. of Selected Topics in Signal Proc"
    },
    {
      "citation_id": "11",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "Schoneveld",
        "H Othmani",
        "Abdelkawy"
      ],
      "year": "2021",
      "venue": "Pattern Rec. Letters"
    },
    {
      "citation_id": "12",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "Felix Kuhnke",
        "Lars Rumberg",
        "Jörn Ostermann"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "D Juan",
        "Patrick Ortega",
        "Alessandro Cardinal",
        "Koerich"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)"
    },
    {
      "citation_id": "14",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spo-ken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "16",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Wenqiang Jiang",
        "Tenggan Zhang",
        "Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "17",
      "title": "Abaw5 challenge: A facial affect recognition approach utilizing transformer encoder and audiovisual fusion",
      "authors": [
        "Ziyang Zhang",
        "Liuwei An",
        "Zishun Cui",
        "Ao Xu",
        "Tengteng Dong",
        "Yueqi Jiang",
        "Jingyi Shi",
        "Xin Liu",
        "Xiao Sun",
        "Meng Wang"
      ],
      "year": "2007",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "18",
      "title": "Cross-attentional audio-visual fusion for weaklysupervised action localization",
      "authors": [
        "Jun-Tae Lee",
        "Mihir Jain",
        "Hyoungwoo Park",
        "Sungrack Yun"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "R Gnana Praveen",
        "Eric Granger",
        "Patrick Cardinal"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "20",
      "title": "Audio-visual speaker verification via joint cross-attention",
      "authors": [
        "Gnana Praveen",
        "Jahangir Alam"
      ],
      "year": "2023",
      "venue": "Speech and Computer"
    },
    {
      "citation_id": "21",
      "title": "A joint cross-attention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "Wheidima Gnana Praveen",
        "Nasib Carneiro De Melo",
        "Haseeb Ullah",
        "Osama Aslam",
        "Théo Zeeshan",
        "Marco Denorme",
        "Alessandro Pedersoli",
        "Simon Koerich",
        "Patrick Bacon",
        "Eric Cardinal",
        "Granger"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "22",
      "title": "Audio-visual fusion for emotion recognition in the valence-arousal space using joint cross-attention",
      "authors": [
        "R Gnana Praveen",
        "Patrick Cardinal",
        "Eric Granger"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "23",
      "title": "Recursive joint attention for audio-visual fusion in regression based emotion recognition",
      "authors": [
        "Eric Gnana Praveen",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "24",
      "title": "Audio-visual event localization via recursive fusion by joint co-attention",
      "authors": [
        "Bin Duan",
        "Hao Tang",
        "Wei Wang",
        "Ziliang Zong",
        "Guowei Yang",
        "Yan Yan"
      ],
      "year": "2021",
      "venue": "2021 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "25",
      "title": "Audio-visual person verification based on recursive fusion of joint cross-attention",
      "authors": [
        "Praveen Gnana",
        "Jahangir Alam"
      ],
      "year": "2024",
      "venue": "Audio-visual person verification based on recursive fusion of joint cross-attention",
      "arxiv": "arXiv:2403.04654"
    },
    {
      "citation_id": "26",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "27",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "28",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "29",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "30",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "31",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "32",
      "title": "A multi-task mean teacher for semi-supervised facial affective behavior analysis",
      "authors": [
        "Lingfeng Wang",
        "Shisen Wang",
        "Jin Qi",
        "Kenji Suzuki"
      ],
      "year": "2021",
      "venue": "ICCV Workshop"
    },
    {
      "citation_id": "33",
      "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
      "authors": [
        "Didan Deng",
        "Liang Wu",
        "Bertram Shi"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "34",
      "title": "Privileged knowledge distillation for dimensional emotion recognition in the wild",
      "authors": [
        "Muhammad Haseeb",
        "Muhammad Osama Zeeshan",
        "Marco Pedersoli",
        "Alessandro Koerich",
        "Simon Bacon",
        "Eric Granger"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "35",
      "title": "Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Yong Li",
        "Yuanzhi Wang",
        "Zhen Cui"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "36",
      "title": "Leveraging tcn and transformer for effective visualaudio fusion in continuous emotion recognition",
      "authors": [
        "Weiwei Zhou",
        "Jiada Lu",
        "Zhaolong Xiong",
        "Weifeng Wang"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "37",
      "title": "Multimodal facial affective analysis based on masked autoencoder",
      "authors": [
        "Wei Zhang",
        "Bowen Ma",
        "Feng Qiu",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "38",
      "title": "A pre-trained audiovisual transformer for emotion recognition",
      "authors": [
        "Minh Tran",
        "Mohammad Soleymani"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "39",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "40",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "41",
      "title": "Timecontinuous audiovisual fusion with recurrence vs attention for in-the-wild affect recognition",
      "authors": [
        "Vincent Karas",
        "Mani Kumar Tellamekala",
        "Adria Mallol-Ragolta",
        "Michel Valstar",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "42",
      "title": "Multimodal continuous emotion recognition: A technical report for abaw5",
      "authors": [
        "Su Zhang",
        "Ziyuan Zhao",
        "Cuntai Guan"
      ],
      "year": "2007",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "43",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "44",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2008",
      "venue": "2017 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "45",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "A Mihalis",
        "Hatice Nicolaou",
        "Maja Gunes",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Lstm-modeling of continuous emotions in an a-v affect recognition framework",
      "authors": [
        "M Wöllmer",
        "F Kaiser",
        "B Eyben",
        "G Schuller",
        "Rigoll"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "47",
      "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Yandong Guo",
        "Lei Zhang",
        "Yuxiao Hu",
        "Xiaodong He",
        "Jianfeng Gao"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016"
    },
    {
      "citation_id": "48",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "Y Zhang",
        "Z Ding",
        "C Wei",
        "Guan"
      ],
      "year": "2021",
      "venue": "ICCV Workshop"
    },
    {
      "citation_id": "49",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "Hershey",
        "D Chaudhuri",
        "J F Ellis",
        "Gemmeke",
        "C Jansen",
        "M Moore",
        "D Plakal",
        "R A Platt",
        "B Saurous",
        "M Seybold",
        "R Slaney",
        "Weiss",
        "Wilson"
      ],
      "venue": "Cnn architectures for large-scale audio classification"
    },
    {
      "citation_id": "50",
      "title": "Challenges and opportunities of text-based emotion detection: A survey",
      "authors": [
        "Abdullah Al Maruf",
        "Fahima Khanam",
        "Md Haque",
        "Zakaria Masud Jiyad",
        "M Mridha",
        "Zeyar Aung"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "51",
      "title": "Transformer models for text-based emotion detection: a review of bert-based approaches",
      "authors": [
        "Francisca Adoma",
        "Henry Nunoo-Mensah",
        "Wenyu Chen"
      ],
      "year": "2021",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "52",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Kollias",
        "M A Tzirakis",
        "A Nicolaou",
        "G Papaioannou",
        "B Zhao",
        "I Schuller",
        "Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "IJCV"
    },
    {
      "citation_id": "53",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "54",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "55",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Dimitrios Kollias",
        "Attila Schulc",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "56",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "57",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection and multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection and multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "58",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "59",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "61",
      "title": "Multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st international on multimodal sentiment analysis in real-life media challenge and workshop"
    },
    {
      "citation_id": "62",
      "title": "Affective behaviour analysis via integrating multi-modal knowledge",
      "authors": [
        "Wei Zhang",
        "Feng Qiu",
        "Chen Liu",
        "Lincheng Li",
        "Heming Du",
        "Tiancheng Guo",
        "Xin Yu"
      ],
      "year": "2024",
      "venue": "Affective behaviour analysis via integrating multi-modal knowledge",
      "arxiv": "arXiv:2403.10825"
    },
    {
      "citation_id": "63",
      "title": "Boosting continuous emotion recognition with self-pretraining using masked autoencoders, temporal convolutional networks, and transformers",
      "authors": [
        "Weiwei Zhou",
        "Jiada Lu",
        "Chenkun Ling",
        "Weifeng Wang",
        "Shaowei Liu"
      ],
      "year": "2024",
      "venue": "Boosting continuous emotion recognition with self-pretraining using masked autoencoders, temporal convolutional networks, and transformers",
      "arxiv": "arXiv:2403.11440"
    },
    {
      "citation_id": "64",
      "title": "Sun team's contribution to abaw 2024 competition: Audiovisual valence-arousal estimation and expression recognition",
      "authors": [
        "Denis Dresvyanskiy",
        "Maxim Markitantov",
        "Jiawei Yu",
        "Peitong Li",
        "Heysem Kaya",
        "Alexey Karpov"
      ],
      "year": "2024",
      "venue": "Sun team's contribution to abaw 2024 competition: Audiovisual valence-arousal estimation and expression recognition",
      "arxiv": "arXiv:2403.12609"
    },
    {
      "citation_id": "65",
      "title": "Multimodal fusion method with spatiotemporal sequences and relationship learning for valence-arousal estimation",
      "authors": [
        "Jun Yu",
        "Gongpeng Zhao",
        "Yongqi Wan",
        "Zhihong Wei",
        "Yang Zheng",
        "Zerui Zhang",
        "Zhongpeng Cai",
        "Guochen Xie",
        "Jichao Zhu",
        "Wangyuan Zhu"
      ],
      "year": "2024",
      "venue": "Multimodal fusion method with spatiotemporal sequences and relationship learning for valence-arousal estimation",
      "arxiv": "arXiv:2403.12425"
    },
    {
      "citation_id": "66",
      "title": "Hsemotion team at the 6th abaw competition: Facial expressions, valence-arousal and emotion intensity prediction",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2024",
      "venue": "Hsemotion team at the 6th abaw competition: Facial expressions, valence-arousal and emotion intensity prediction",
      "arxiv": "arXiv:2403.11590"
    },
    {
      "citation_id": "67",
      "title": "Joint multimodal transformer for dimensional emotional recognition in the wild",
      "authors": [
        "Paul Waligora",
        "Osama Zeeshan",
        "Haseeb Aslam",
        "Soufiane Belharbi",
        "Alessandro Koerich",
        "Marco Pedersoli",
        "Simon Bacon",
        "Eric Granger"
      ],
      "year": "2024",
      "venue": "Joint multimodal transformer for dimensional emotional recognition in the wild",
      "arxiv": "arXiv:2403.10488"
    },
    {
      "citation_id": "68",
      "title": "Emotion recognition using transformers with masked learning",
      "authors": [
        "Seongjae Min",
        "Junseok Yang",
        "Sangjun Lim",
        "Junyong Lee",
        "Sangwon Lee",
        "Sejoon Lim"
      ],
      "year": "2024",
      "venue": "Emotion recognition using transformers with masked learning",
      "arxiv": "arXiv:2403.13731"
    }
  ]
}