{
  "paper_id": "2509.16760v1",
  "title": "Feature Selection Via Graph Topology Inference For Soundscape Emotion Recognition",
  "published": "2025-09-20T17:52:20Z",
  "authors": [
    "Samuel Rey",
    "Luca Martino",
    "Roberto San Millan",
    "Eduardo Morgado"
  ],
  "keywords": [
    "Feature selection",
    "soundscape emotion recognition",
    "network topology inference",
    "graph learning",
    "structural equation models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Research on soundscapes has shifted the focus of environmental acoustics from noise levels to the perception of sounds, incorporating contextual factors. Soundscape emotion recognition (SER) models perception using a set of features, with arousal and valence commonly regarded as sufficient descriptors of affect. In this work, we blend graph learning techniques with a novel information criterion to develop a feature selection framework for SER. Specifically, we estimate a sparse graph representation of feature relations using linear structural equation models (SEM) tailored to the widely used Emo-Soundscapes dataset. The resulting graph captures the relations between input features and the two emotional outputs. To determine the appropriate level of sparsity, we propose a novel generalized elbow detector, which provides both a point estimate and an uncertainty interval. We conduct an extensive evaluation of our methods, including visualizations of the inferred relations. While several of our findings align with previous studies, the graph representation also reveals a strong connection between arousal and valence, challenging common SER assumptions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "I N recent years, soundscape research has shifted its focus in environmental acoustics from noise levels to how communities and individuals perceive sounds, incorporating contextual factors. The term soundscape, coined by Southworth and later popularized by Schafer  [1] ,  [2] , encompasses a broad range of applications, including urban sound planning  [3] , acoustic monitoring  [4] , sound design  [5] , and sonification  [6] . Indeed, soundscape emotion recognition (SER) is an active research area addressing challenges such as the definition of perceptual descriptors  [7] , probabilistic modelling approaches  [8] , and the influence of psychological well-being and demographic factors  [9] . Compared to emotions elicited by music or speech, those associated with soundscapes are subtler and influenced by a complex interplay of physical, social, cultural, and psychological factors. Among the various models used to S. Rey, R. San Millán-Castillo, and E. Morgado are with the Department of Signal Theory and Communications, Universidad Rey Juan Carlos, Madrid, Spain. Emails: {samuel.rey.escudero, roberto.sanmillan, eduardo.morgado}@urjc.es. L. Martino is with the Department of Economics and Business, Universitá degli Studi di Catania, Catania, Italy. Email: luca.martino@unict.it This work was partially supported by Agencia Estatal de Investigación-AEI, Grant PID2022-136887NB-I00 (POLIGRAPH) funded by MCIN/AEI/10.13039/501100011033, by the PIAno di inCEntivi per la RIcerca di Ateneo 2024/2026 (UPB 28722052159) and PIACERI Starting Grant BA-GRAPH (UPB 28722052144) of the University of Catania, and by the Community of Madrid within the ELLIS Unit Madrid framework and the grant CAM-URJC F1180 (CP2301). represent this complexity, Russell's circumplex model  [10] -  [13]  has been widely adopted, focusing on two affective dimensions, arousal (i.e., eventfulness) and valence (i.e., pleasantness), that are considered sufficient descriptors of perceived affect in soundscape studies. Modelling SER offers an efficient alternative to perceptual surveys and supports a deeper understanding of soundscape perception. While nonlinear models often outperform linear ones, the latter remain attractive due to their simplicity  [14] . However, SER predictive frameworks vary considerably in their choice of datasets and descriptors. In the last decade, the scientific community has made an effort to offer valuable open-source databases, including general-purpose collections such as Emo-Soundscapes  [13] , urban soundscape datasets such as ATHUS  [15]  and ISD  [8] , and indoor collections such as HSDD  [16] . In this work, we focus on the Emo-Soundscapes (EMO) database, the most widely explored dataset in SER research  [6] ,  [13] ,  [17] -  [25] . Identifying the features that most strongly influence perceptual descriptors remains an open research problem, which motivates our focus on feature selection for the EMO database. Existing approaches to feature selection fall into three main categories: (a) wrapper methods, which sequentially evaluate subsets of variables by optimizing a cost function; (b) filter-based methods, which rank variables based on statistical criteria independently of the model; and (c) embedded methods, which integrate selection into model training.\n\nIn this context, graph-based representations offer an alternative framework for capturing pairwise relationships among variables. By encoding interactions as edges between nodes, graphs can represent a variety of underlying structures in the data, ranging from statistical dependencies to perceptual associations  [26] ,  [27] . Relating the properties of observed signals with the topology of the graph lies at the core of graph signal processing (GSP), a field devoted to learning from data defined over irregular domains modeled by graphs  [28] -  [30] . A central problem in GSP is graph learning, which seeks to infer a graph structure from observed signals that captures the underlying relations among variables  [31] -  [34] . The key to this task is leveraging signal models that connect the signal properties with the graph topology. Notable approaches include partial correlations and Gaussian graphical models  [32] ,  [35] ,  [36] , sparse structural equation models (SEM)  [37] -  [40] , smooth models  [41] -  [43] , and graph-stationary models  [44] -  [47] .\n\nIn this work, we integrate techniques from graph learning and information criteria to develop a novel variable selection approach for SER. More specifically, we assume that every variable can be expressed as a linear combination of its neighbors, following a linear SEM  [48] ,  [49] . In contrast to previous methods, the two outputs (arousal and valence) are considered jointly, alongside the inputs. This modeling approach leads to a convex optimization problem for inferring the underlying graph topology, where we include a regularization term to promote sparsity. While most graph learning methods treat the sparsity regularization parameter λ as a tunable hyperparameter, we propose a novel and systematic alternative for selecting its optimal value. We introduce a generalized version of the automatic elbow detector, which can be interpreted as a modern information criterion  [50] . In addition to estimating the optimal λ, this generalization yields an interval-based solution that provides an uncertainty estimate. As a brief summary, the proposed approach can be interpreted as a combination between wrapper and filter methods for feature selection. Specifically, we adopt a graph topology learning framework that enables the joint analysis of dependencies and connections among all variables, including both inputs and outputs. This allows us to quantify the strength of connections with the outputs (as in a wrapper method, facilitating the construction of rankings) and to identify and discard variables based on their correlations (as in a filter method). The remainder of this work is organized as follows. Section II describes the EMO database. Section III presents the proposed graph learning framework, and Section IV provides insights about the tuning of the regularization parameter. Section V presents and discusses the results of several experiments. Finally, Section VI summarizes the main findings and conclusions, and outlines future research directions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Emosoundscapes Database And Scope",
      "text": "This work explores the EMO dataset 1  , a large, publicly available collection of soundscapes with emotion annotations, which, to the best of our knowledge, is the most extensively benchmarked dataset of soundscapes  [13] . It contains 1213 audio files released under a Creative Commons license, created by mixing sounds from the Freesound collaborative platform  [51] . For each clip, EMO provides 122 input variables, assorted and meaningful from an acoustic signal perspective, and two output variables representing emotional labels. This variety of soundscapes and features gives EMO a dimensionality well suited for evaluating feature selection methods. The EMO dataset classifies soundscapes within six of Schafer's taxonomy categories  [2] : human sounds (e.g., shouts, whispers), natural sounds (e.g., rain, birds), quiet and silence (e.g., silent forest, quiet park), sounds and society (e.g., concert, store), mechanical sounds (e.g., factory, engine), and sounds as indicators (e.g., clock, church bells). These categories involve both the sound source and the listening context, providing a more general and straightforward approach than other taxonomies, such as Brown's  [52] . The dataset includes 600 clips with 100 examples per category, plus an additional 613 clips created by manually mixing (and gain-adjusting) sounds from two or three categories, resulting in 1213 soundscapes in total. Perceived emotions (arousal and valence) were obtained through a crowd-sourcing procedure involving 1182 trusted annotators, who completed a parallel, ranking-based questionnaire consisting of pairwise comparisons between two audio clips. The collected ratings achieved adequate inter-subject reliability.\n\nThe EMO files are 6 s long, monophonic, and sampled at 44100 Hz. Previous studies  [53] ,  [54]  have shown that this format is adequate for assessing soundscape affective dimensions such as eventfulness (i.e., arousal) and pleasantness (i.e., valence); see Figure  1  for a graphical representation. From each file, EMO extracts up to 122 normalized features using a 23 ms Hanning window with 50% overlap, computed with generalpurpose tools such as YAAFE  [55]  and MIRToolbox  [56] . These features can be categorized as follows:\n\n• Psychoacoustic variables: perceptual (i.e., subjective) attributes of sounds, such as level (loudness for overall level and MFCCs for band-limited levels), spectrum (sharpness for high frequencies), and temporal and spectral modulation (fluctuation strength). In the EMO dataset, these features correspond to the indices 4, from 24 to 49, 113, 114, and from 117 to 119. • Time-domain variables: signal dynamics, such as classical estimators based on samples of the audio signal (i.e., energy, entropy of energy, root mean square (RMS), zerocrossing), or the ratio between the magnitude difference at the beginning and end of a decay period (i.e., decrease slope), to name a few. They correspond to indices from 1 to 7, 22, 23, 52, 115, 116. • Frequency-domain variables: spectral shape and harmonic structure of sounds, including pitch (fundamental frequency of the audio signal), chromagrams (spectral representation based on the 12 equal-tempered pitches of Western music), inharmonicity (proportion of frequencies that are not multiples of the fundamental frequency), the first three spectral statistical moments (centroid, spread, skewness), and high-frequency content (roll-off ). These correspond to the remaining indices.\n\nIII. SOUNDSCAPE REPRESENTATION VIA GRAPH LEARNING We aim to learn a graph representation that models the relations among the acoustic features in the EMO dataset. The resulting graph captures the underlying structure of dependencies between features, offering new insights into their interactions. We further leverage this learned structure to identify which features are most influential in predicting the perceived emotional responses, namely arousal and valence. To this end, we first present the fundamentals of graph learning and then tailor this framework to the specific characteristics of the EMO dataset. Finally, while our analysis and methods focus on the EMO dataset, the proposed methods are general and can be applied to other soundscape datasets as well.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Fundamentals Of Graph Learning",
      "text": "A graph is a mathematical entity that encodes pairwise interactions. Formally, a graph G = (V, E) is composed of N nodes collected in the set V, and a set of edges E ⊆ V × V.\n\nThe connectivity of G is encoded in a sparse adjacency matrix A ∈ R N ×N , where A ij = 0 if and only if (i, j) / ∈ E, and each nonzero entry A ij represents the weight of the edge between nodes i and j  [27] . An essential component in this setting are graph signals, which are signals defined over the set of nodes V and represented as a vector x ∈ R N , where x i denotes the signal value at node i.\n\nGraph learning is an inverse problem aiming to infer the topology of a graph from a set of nodal observations  [33] ,  [58] . Let the matrix X := [x 1 , . . . , x M ] ∈ R N ×M be a collection of M graph signals whose (statistical) properties are assumed to be intimately related to the unknown graph G. Then, we can infer the topology of the graph encoded in the adjacency matrix A by solving the following optimization problem\n\nwhere L(A, X) is a loss function that captures the relationship between A and the observed signals X, while ∥A∥ 1 denotes the ℓ 1 norm of the vectorized matrix A, included to promote sparsity. The parameter λ ≥ 0 controls the level of sparsity, and the constraint set A ensures that A corresponds to a valid adjacency matrix. The success of graph learning critically depends on the nature of the dependency between the observed signals X and the unknown topology encoded in A, which is encoded in the function L(A, X). Different approaches leverage different types of signal-structure relationships, with prominent examples including graphical models  [35] ,  [59] , graph smoothness  [41] , and graph stationarity  [44] . Next, we focus on estimating the graph topology under the assumption that the observed signals follow a SEM, which constitutes the setting of interest for this work.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Learning A Graph Representation For The Emo Dataset",
      "text": "According to the description of the EMO dataset in Section II, modeling the relations between the different soundscape features as a graph entails learning the topology of a graph with N = 124 nodes, where each node corresponds to a particular feature, including the output features, arousal and valence. To that end, we have access to M = 1213 different soundscapes, collected in the matrix X ∈ R 124×1213 .\n\nIn the context of soundscape emotions, there is a growing interest in predicting a subset of features, particularly those related to perceived emotions such as arousal and valence, from the remaining ones. Based on this observation, we postulate that the observed features follow a linear SEM  [40] ,  [48] , meaning that the matrix X is given by the following autoregressive model\n\nwhere W denotes the exogenous input matrix, whose columns are independent realizations of a multivariate distribution with diagonal covariance. Here, A denotes the unknown adjacency matrix encoding the relations between the different features, which is assumed to be sparse and with zeros on its diagonal. Intuitively, equation (  2 ) implies that the value of the i-th feature in a given observation can be expressed as a linear combination of the values of its neighbors, as dictated by the adjacency matrix. Moreover, the estimation error is determined by the (unknown) covariance of the exogenous input W.\n\nBuilding on the SEM in (  2 ), a common approach to estimate the adjacency matrix A from the signals X is to solve the following convex regularized least-squares problem\n\nThe first term in the objective promotes fidelity to the data, while the second term, weighted by λ, encourages sparsity in the adjacency matrix via the ℓ 1 norm, similar to Lasso regression settings. It should be noted that the matrix A is subject to two constraints: 1) zeros on its diagonal, i.e., A ii = 0.\n\n2) and it is symmetric 2  , i.e., A = A ⊤ . Consequently, the set of valid adjacency matrices A is defined as the set of symmetric matrices with no self-loops. The problem in (3) is convex, and can therefore be efficiently solved using a variety of algorithms that guarantee convergence to the global minimum  [60] -  [62] .\n\nSolving the graph learning problem in (3) yields an estimate A λ that encodes how each feature can be predicted from its neighbors, including those related to perceived emotions such as arousal and valence. Although these two outputs are often the main focus in SER, the proposed model allows any feature to be predicted from other features in the graph, providing a general and interpretable framework for analyzing interfeature dependencies. Unlike alternative methods, we explicitly constrain A λ to be symmetric, reflecting the assumption of an undirected feature graph in which influences between pairs of features are bidirectional. Moreover, prior studies suggest that arousal and valence are statistically independent or only weakly related, with large variations depending on context  [63] . This property can be naturally incorporated into the model via additional structural constraints, and its implications are examined in detail in the numerical evaluation in Section V.\n\nFinally, the regularization parameter λ plays a central role in the feature selection process, as it directly controls the sparsity of the learned graph. Larger values of λ promote sparser structures, often yielding more interpretable relationships at the cost of higher reconstruction error, whereas smaller values produce denser graphs with greater fidelity to the data. Thus, selecting λ involves a trade-off between interpretability and prediction accuracy. Developing an strategy for choosing λ is a non-trivial task, which we address in the next section.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Choosing Λ By Automatic Elbow Detectors",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Preliminaries",
      "text": "In several applications, we can construct a non-increasing error curve (e.g., the mean squared error) can be computed,\n\nthat is function of a discrete or continuous input z ∈ R. the input variable z can represent the number of parameters or the order of a model, and in these framework, z would be a discrete variable. Moreover, z can be related to the regularization coefficient in regularized regression problem and, in this case, it would be a continuous variable. A graphical example is depicted in Figure  2 . The function V (z) can be any error metric that characterizes the performance of the system. An example of application is when we have a parameter λ of a regularizer. We can define a parameter z = 1/λ that is the inverse of the regularization parameter. In this scenario, as z → ∞ (i.e., λ = 0), we have V (z) approaches the minimum possible error (i.e.,maximum overfitting in the system). In the minimum error is 0, we have V (z) = MSE → 0 as z → ∞. However, using a regularizer, we usually desire to find a trade-off between fitting and ability in generalization. Namely, there exists an optimal value z * . The definition and computation of this optimal value z * is still object of active research  [50] ,  [64] -  [66] . In practice, we can compute V (z) only in a finite number of points that are (generally) non-uniformly sampled at z 1 < z 2 < ... < z K . Thus, we finally observe V (z) in a finite number of points,\n\nWithout loosing generality, we assume that V (z K ) = 0. Recall that generally the difference z k+1 -z k varies with k, and generally |z k+1 -\n\nHere, we are interested in finding an interval of z values denoted [k * 1 , k * 2 ] that possibly contains the optimal value z * and, more generally, contains reasonable values of z from a practical point of view. With this purpose, we generalize the universal automatic elbow detector (UAED) in  [50]  in three distinct directions: The curve is samples in K = 6 points at z 0 , z 1 ..., z K , depicted with blue circles. For simplicity and without loss of generality, we have considered z 0 = 0 and V (z K ) = 0.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Interval Solution By The Generalized Uaed (G-Uaed)",
      "text": "In this section, we present the extended UAED technique for obtaining an interval of indices in the context of an elbow detection problem, which encodes the uncertainty in model selection. Below, we introduce the generalized UAED (G-UAED). Let define\n\nso that the two optimal points forming the interval solution would be denoted as\n\n. Note that if V (z) has a great drop at the beginning, for instance, reaching zero already at the first point z 1 (i.e., V (z 1 ) = 0), the 'elbow' will be at z 1 . The other extreme scenario, is when V (z) has linear drop and reach zero in the last point, i.e., V (z K ) = 0. In this case, there is not any elbow, or the unique possible elbow is z K . In the general case, a smaller area under the curve V (z), implies that the elbow is located closer to z = 0. Hence, for our purpose we desire to approximate the area under V (z) (recall that V (z) is known only in certain points z 1 , ..., z K ) and then find a rule to decide the elbow point. Thus, we build a piece-linear approximation of the curve V (z). The underlying idea is based on the construction of three straight lines: the first one passing through the points\n\n), the second one passing through the points (k 1 , V (k 1 )) to (k 2 , V (k 2 )) and the last one passing through the points (k 2 , V (k 2 )) to (z K , 0) (recall that we have assumed V (z K ) = 0). See Figure  2  for a graphical example (clearly, k 2 > k 1 ). Thus, \"inspired\" by the concept of the maximum area under the curve in ROC, the goal is to minimize the area under this piece-linear approximation of the curve V (z). The total area under this approximation is the sum of the two trapezoidal areas (A 1 and A 2 ) and a triangular area (A 3 ), as depicted in Figure  2 ,\n\nIt is worth mentioning that the expressions above differ from the equations in  [50] . The final solution will be given by\n\nwhere A 1 , A 2 and A 3 are defined in Eq. (  4 ). The solution can be readily obtained numerically. It is possible to show that the optimal interval [k * 1 , k * 2 ] contains the elbow solution given by the standard UAED in  [50] . The optimization problem in Eq. (  5 ) can be easily solved by exhaustive search if K, the total number of z points, is not a huge number.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Numerical Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. First Experiment: Selection Of The Lasso-Shrinkage Parameter",
      "text": "In this section, we test the ability of G-UAED to suggest a properly sn interval of λ values. Consider a dataset {x i , y i } N i=1 where the x i = [x i,1 , ..., x i,R ] ⊤ , X = [x 1 , ..., x N ] are input vectors and R × N input matrix, whereas y = [y 1 , ..., y N ] ⊤ is the output vector. We also define the vector of unknown parameters β = [β 1 , ..., β R ] ⊤ , and assume the observation model\n\nwhere ϵ is a noise perturbation that we consider Gaussian with zero mean and variance σ 2 = 1. We set R = 30 and generate artificially N = 200 data pairs following model considering x i,r ∼ N (x|0, 100) (randomly drawn for each i, r at each run). We consider two scenarios. In the first one, we assume the vector\n\n0, 0, 0, 0, ..,\n\nhence the last 10 features do not affect the outputs y i . In a second scenario, we assume the vector\n\n0, 0, 0, 0, ..,\n\ni.e., we have more zeros (the 20 last positions).\n\nAfter data generation, we apply a LASSO technique  [67] ,  [68]  in order to obtain and estimation of the coefficient vector β * λ . Namely, we follow the minimization of a square error with an L 1 regularizer,\n\nWe use a thin grid of λ values: as λ grows, the number of zero in β * λ , until a maximum value of λ sun that β * λ has only null entries. We denote as MSE(λ) = ||y -β * λ X|| 2 . Clearly, with λ = 0, we have the smallest MSE and if 0 < λ 1 < λ 2 we have MSE(λ 1 ) < MSE(λ 2 ), i.e., MSE(0) < MSE(λ 1 ) < MSE(λ 2 ). The optimal selection of λ, which balances model fitting, generalization, and other desirable properties, remains an active area of research  [67] ,  [68] . In the literature, It seems to have an agreement that for a practical point of view, there exists a range of values that are acceptable in terms of performance, rather than a single value. This is particularly clear for LASSO regularizers applied for variable selection: most than one value of λ can yield the true numbers of zeros in β * λ . Note that if we set z = 1/λ and define\n\nwe have a non-increasing error function of z. Therefore, we can apply G-UAED to provide an optimal choice of z * = 1/λ * , and also G-UAED to provide an interval solution\n\n, in order to quantify the uncertainty in the selection. However, instead of defining the variable as z = 1/λ, we consider z to be the number of links in the graph (for a given λ). When λ = 0 the graph has the maximum possible number of links, whereas for λ = λ max it has none (by definition). The motivation behind this choice is that it tends to produce more convex curves, thereby facilitating the application of G-UAED  [50] .\n\nWe denote as λ * 1 the smallest value of λ such that we have only 10 zeros (located at the last ten positions of β * λ ) and denote s λ * 2 the biggest value of λ such that we have only 10 zeros (located at the last ten positions of β * λ ). Hence, reasonable choices of λ, for our problem, is any λ inside\n\nFor each realization of the data, the groundtruth interval [λ * 1 , λ * 2 ] is shown in Figure  3  with a pink shaded areas. We apply the procedures in Section IV, to find the interval solutions based on the G-UAED. We can observe in Fig.  3 , all the λ values suggested by the intervals based the G-UAED are completely contained in the groundtruth intervals\n\n, with the exception of 3 runs (over a total number of 100 independent runs), where a small portions of the first parts of the intervals are slight out the groundtruth intervals (that, in these 3 scenarios, are highlighted with solid red lines). Hence, it appears clear that the interval solutions, provided by G-UAED, contain safe and useful λ values to use in a LASSO regression problem.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Feature Selection On The Emo Database (Experiments)",
      "text": "Let us consider the soundscape emotion dataset described in Section II. The 122 × 1024 matrix X contains all the 122 input features of the dataset and the last two lines correspond to the two outputs (a) arousal and (b) valence. Note that inputs and outputs are stored within the same matrix X, making our approach agnostic to the distinction between inputs and outputs. We use the SEM in (3) to obtain the 122×122 matrix A λ , i.e.,\n\nfor different values of λ ≥ 0. Note that if λ = 0 we have the best matrix A λ in terms of fitting (i.e., minimum error ∥X -A λ X∥ 2 F ), and the corresponding graph includes the maximum number of links among the nodes. As λ grows, the error ∥X-A λ X∥ 2\n\nF grows as well. The matrix A λ becomes more sparse containing more zeros, and the corresponding graph includes less links. At same lambda value λ max , the matrix A λ becomes the null matrix (containing only zeros). Then we try different values of λ considering a tiny grid from 0 to λ max . Note that as λ increases, the number of links in the graph decrease. Hence, For each value of λ ∈ [0, λ max ] we obtain A λ , and compute the normalized (Frobenius) MSE error:\n\nfor each matrix A λ we compute the number of connections (links) in the corresponding graph. Clearly, with λ = 0, we have the smallest N-MSE and if 0 < λ 1 < λ 2 we have N-MSE(λ 1 ) < N-MSE(λ 2 ), i.e., N-MSE(0) < N-MSE(λ 1 ) < N-MSE(λ 2 ). The optimal selection of λ, which balances model fitting, generalization, and other desirable properties, remains an active area of research  [67] ,  [68] .\n\nIn the literature, it seems to have an agreement that for a practical point of view, there exists a range of values that are acceptable in terms of performance, rather than a single value. This is particularly clear for LASSO regularizers applied for variable selection: most than one value of λ can yield good solutions, as we have shown in Section V-A. Note that if we set z = 1/λ and define\n\nwe have a non-increasing error function of z and we can apply G-UAED. Other possible definition (similar to other information criteria) is V (z) = log(N-MSE(z)). Therefore, we can apply G-UAED to provide an optimal choice of z * = 1/λ * , and also G-UAED to provide an interval solution\n\n, in order to quantify the uncertainty in the selection. However, here, instead of using the variable z = 1 λ , we consider as z the number of links in the graph (given a λ). If λ = 0, we have the maximum of possible links, whereas with λ = λ max we have zero links (by definition). THe reason, is that this choice of the variable seems to generate more convex curves, helping the application of G-UAED. In this section, we consider four different scenarios:\n\n• Scenario 1. We allow the connection between the outputs, arousal and valence (last two lines in X), and compute the normalized error in Eq. (  9 ) considering all the matrix X (he reconstruction of the all matrix X). • Scenario 2. We do not allow the connection between the outputs, and compute the normalized error in Eq. (  9 ) considering all the matrix X. • Scenario 3: We allow the connection between the outputs, but compute the normalized error (9) considering only the last two lines of X (corresponding to the two outputs), i.e., we consider only the reconstruction of the outputs. • Scenario 4: We do not allow the connection between the outputs, and compute the normalized error (9) considering only the last two lines of X (corresponding to the outputs), i.e., we consider only the reconstruction of the outputs. The curves showing the normalized error versus the number of links in the graph are depicted in Figure  4 , for all the four scenarios. Clearly, the normalized error decreases as the number of links grows. We have moved vertically the curves such that the minimum error is zero, without losing generality. Recall that at each point in the figures corresponds to a value of λ. As remarked above, as the number of connections grows, the corresponding λ is smaller. Then, we apply the G-UAED scheme in Section IV obtaining the elbows and the estimated intervals, represented by the red points and red lines in Figure  4 . Table  I  summarizes these results in terms of links and values of λ. Recall that Scenarios 2 and 4 consider only the two lines f the outputs for building the error curve, so that the values of the links are much smaller.\n\nLooking Table  I  we can make some interesting observations:\n\n• The values of λ are the same in Scenarios 1-2 and in Scenarios 3-4. Moreover, there are all similar values in the 4 scenarios: the extreme value of the intervals is 472.59, exactly the same for all the four cases.\n\n• Scenarios 1 and 3 present a slightly smaller values of the elbows in terms of links. This is due to the fact that Scenarios 1-3 allow the inter-link between the outputs, whereas Scenarios 2-4 do not allow it. This shows already a very interesting result: the outputs arousal and valence present a relevant partial correlation/connection. This verifies the theory on psychological phenomena. The graphs corresponding to the elbows of the Scenarios 3 and 4 are depicted in Figures  5(a )-6(a). The nodes 123 and 124 represent the two outputs, arousal and valence, respectively. We decide to show these graphs since they correspond to the bigger elbow λ = 159.78 and allow a more clear representation. The other graphs in Figures  5 6 correspond to bigger values of λ ∈ {300, 360, 472.59}. The last value, λ = 472.59, is the extreme value of the intervals found by the G-UAED. We show these different graphs, to see the evaluation of the graphs within the intervals obtained by G-UAED. Figures 5 are devoted to the case \"with inter-link\" between the outputs. Whereas, Figures 6 are devoted to the case \"without interlink\" between the outputs. The information provided by the graphs is also summarized in Tables  II  and III , the first one considering the possible inter-link between outputs, whereas in the latter considers the results without the possible interlink between outputs. The following key points arise from a careful analysis:\n\n• The most important features seems to be 113 (\"loudness mean\") and 114 (\"loudness standard deviation\"), that is completely in line with the results in the literature. As remarked in other works, 113 is more related to Arousal whereas 114 seems to related to Valence. However, in several graphs in Figures  5 6 , they result connected to each other. • When we allow the inter-link between outputs, less variables are required to explain the outputs. This means that there is a clear connection/correlation between Arousal and Valence that, is in some sense, with the theoretical point of view/discussion. • Valence always seems to require less variables than Arousal. This is a result completely in contrast with the other results in the literature. in  [20] , a linear (regression) method was also applied to these dataset jointly with wrapper methods and a Gibbs analysis (for feature selection), finding that (see  [20, Section 6] ) that the most relevant variables in common to the two outputs were 114, 113, 14, 115 (\"energy mean\"), and 3. The frequencydomain features 14 (\"spread mean\") and 3 (\"decrease slope mean\") also appear in our graphs and Tables II-III. Thus, with the exception of the variable 115, we have an agreement, in this sense, between these two studies. Note that variable 4 (\"maximum fluctuation\") is quite connected to 3, according our graphs. Furthermore, in  [20]  The results with bigger λ are in line with the results obtained in the literature using nonlinear (regression) methods which, in general, suggest the use of less number of features in the model. For instance, the analysis with regression trees in  [69]  suggest the use of 2 variables for Arousal, that are 113 (\"Loudness mean\") and 4 (\"Fluctuation max\"), that also appear in our graphs and Tables II-III. Regarding Valence, the study in  [69]  suggest a non-linear regression model based on 4 variable 114 (\"Loudness standard deviation\") , 113 (\"Loudness mean\"), 3 (\"Decrease Slope mean\"), and 115 (\"energy mean\"). With the exception of the latter (feature 115), we have a perfect agreement with the result in Table  III  and λ = 472.59 (that is one of the extreme values obtained by G-UAED).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Previous Results In The Literature",
      "text": "The results in this work are in line with the results in  [20]  where a simple linear model was considered. With nonlinear models (as decision trees  [69] ) smaller number of features have been suggested with respect to  [20] . The need for more variables for valence is shown also in  [6] ,  [17] . Tables IV summarize the results in other works considering different regression methods. Moreover, Table  IV  presents the prediction model, the number of variables for the two outputs, and the more relevant variables, reported in the literature. Note that, in some previous works, the names of selected variables have not been provided. From Table  IV , we can observe that psychoacoustic variables become relevant in previous work with the EMO dataset, such as MFCCs, Roughness, Fluctuation Strength, and Loudness. Predictive models based on recent soundscape databases also remark the key role of psychoacoustic features. ARAUS showed that the maximum Loudness was the most important variable for ISO Pleasantness (i.e., the equivalent to valence in this work)  [70] , and then frequencydomain variables. On the other hand, ISD proposed a model for ISO Pleasantness with Loudness (fifth percentile) as an extremely important variable, along with time-domain and frequency-domain variables (i.e., based on L Aeq , L Ceq , and sound pressure level percentiles). Moreover, ISO Eventfulness (i.e., the equivalent to arousal in this work), was modelled considering other psychoacoustic variables (e.g., Fluctuation strength, Roughness). All these ideas are in line with the conclusions in a recent review on soundscape modelling  [14] . The soundscape models presented in this study result in a similar, and parsimonious, set of features that have been used in the literature with other methods and datasets. Our proposal remarks psychoacoustic variables as central (i.e., Loudness), and then, a fine-tuning with other frequency-domain and timedomain features.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Vi. Final Discussion",
      "text": "In last decades, soundscape research hsa become one of the most active topics in acoustics. SER modelling benefits from feature selection by leading to more parsimonious and interpretable solutions. and surveys. Simpler models and detection of prominent features save time and resources in practical and research problems.\n\nThe proposed approach can be viewed as a combination between a wrapper method and a filter method for feature selection. Specifically, we employ a graph topology learning framework that allows the dependencies and connections among all variables, both inputs and outputs, to be analyzed jointly. We can obtained the intensity on the connections with the outputs (and then build rankings, as with a wrapper method) and observe and discard variables according to their correlations (as in a filter method). The topology of the graph is obtained by considering a lineal SEM model with a LASSO penalization. Furthermore, we have proposed an innovative procedure for handling the parameter λ in LASSO. We have introduced the G-UAED technique for obtaining elbow points and intervals for λ. In this way we control the sparsity of the model. Benchmarking with previous studies with the EMO dataset confirms a clear agreement on the most prominent features for SER modelling. This work insists on highlighting psychoacoustic variables (e.g., 113 and 114) as the most relevant, but refining the metrics with time-domain and frequency-domain fetaures (e.g., 14 and 3). Nevertheless, the employed graph approach captures the underlying structure between all features and provides new insights:\n\n• The two analyzed outputs, arousal and valence, seems to be very correlated, unlike in the assumptions in SER theory.\n\n• Valence seems to require less variables (to be modeled) than arousal. This is in contrast with many other previous studies.\n\n• The suggested number of features requires to arousal (i.e., 7) is in line with the results in recent studies in the literature. • Relevant variables as 20 and 14, or 4 and 3, 14 and 13, or 113 and 114, highly correlated and, in many cases, can be considered interchangeable. Some results in the literature differ only with respect to these variables, but are, in fact, in agreement. Clearly, the proposed approach is applicable to other soundscape databases (even entirely different datasets and/or applications). As future research, we also plan to properly analyze the signs of the coefficients associated to the links of the graph (for the ranking, we just use the module). Furthermore, directed graphs, allowing edges with arrows instead of simple links, could also be considered. One possibility is to relax the symmetry condition related to the matrix A, i.e., removing the condition A = A ⊤ .   signal processing for machine learning: A review and new perspectives,\" IEEE Signal Process. Mag., vol. 37, no. 6, pp. 117-127, 2020.\n\n[30] G. Leus, A. G. Marques, J. M. F. Moura, A. Ortega, and D. I. Shuman, \"Graph signal processing: History, development, impact, and outlook,\"",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: for a graphical representation. From each",
      "page": 2
    },
    {
      "caption": "Figure 1: Circle of emotion with Arousal and Valence in two cardinal",
      "page": 3
    },
    {
      "caption": "Figure 2: The function",
      "page": 4
    },
    {
      "caption": "Figure 2: Example of error curve V (z) (shown blue solid line) and the",
      "page": 4
    },
    {
      "caption": "Figure 2: for a graphical example",
      "page": 5
    },
    {
      "caption": "Figure 3: with a pink shaded",
      "page": 5
    },
    {
      "caption": "Figure 3: , all the λ values suggested by the intervals based the G-",
      "page": 5
    },
    {
      "caption": "Figure 3: The groundtruth intervals [λ∗",
      "page": 6
    },
    {
      "caption": "Figure 4: , for all the",
      "page": 6
    },
    {
      "caption": "Figure 4: Table I summarizes these results in terms of links and values",
      "page": 7
    },
    {
      "caption": "Figure 4: The normalized error as function of the number of links in the graph, in the 4 different scenarios. Note that the points with zero links",
      "page": 10
    },
    {
      "caption": "Figure 5: Graph representation and associated adjacency matrix of edges involving the output variables (valence and arousal). Connectivity between output",
      "page": 11
    },
    {
      "caption": "Figure 6: Graph representation and associated adjacency matrix of edges involving the output variables (valence and arousal). Connectivity between output",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[13]\n[18]\n[17]\n[6]\n[19]\n[20]\n[69]\n[24]\nHere": "",
          "Support Vector Regression\nSeveral Deep Learning methods\nRandom Forest and other methods\nRandom Forest\nConvolutional Neuronal Networks\nMultiple Linear Regression\nDecision Tree Regression\nConvolutional Neuronal Networks\nSEM with LASSO": "",
          "39\n54\n26\n15\n23\n7\n2\n64\n7": "",
          "39\n54\n29\n14\n23\n16\n4\n64\n6": "",
          "—\n—\n—\n—": "1, 2, 4"
        },
        {
          "[13]\n[18]\n[17]\n[6]\n[19]\n[20]\n[69]\n[24]\nHere": "",
          "Support Vector Regression\nSeveral Deep Learning methods\nRandom Forest and other methods\nRandom Forest\nConvolutional Neuronal Networks\nMultiple Linear Regression\nDecision Tree Regression\nConvolutional Neuronal Networks\nSEM with LASSO": "",
          "39\n54\n26\n15\n23\n7\n2\n64\n7": "",
          "39\n54\n29\n14\n23\n16\n4\n64\n6": "",
          "—\n—\n—\n—": "Roughness M, 50\nRoughness M, 1"
        },
        {
          "[13]\n[18]\n[17]\n[6]\n[19]\n[20]\n[69]\n[24]\nHere": "",
          "Support Vector Regression\nSeveral Deep Learning methods\nRandom Forest and other methods\nRandom Forest\nConvolutional Neuronal Networks\nMultiple Linear Regression\nDecision Tree Regression\nConvolutional Neuronal Networks\nSEM with LASSO": "",
          "39\n54\n26\n15\n23\n7\n2\n64\n7": "",
          "39\n54\n29\n14\n23\n16\n4\n64\n6": "",
          "—\n—\n—\n—": "∼ 24-36, 37-49"
        },
        {
          "[13]\n[18]\n[17]\n[6]\n[19]\n[20]\n[69]\n[24]\nHere": "",
          "Support Vector Regression\nSeveral Deep Learning methods\nRandom Forest and other methods\nRandom Forest\nConvolutional Neuronal Networks\nMultiple Linear Regression\nDecision Tree Regression\nConvolutional Neuronal Networks\nSEM with LASSO": "",
          "39\n54\n26\n15\n23\n7\n2\n64\n7": "",
          "39\n54\n29\n14\n23\n16\n4\n64\n6": "",
          "—\n—\n—\n—": "113, 114\n114, 113"
        },
        {
          "[13]\n[18]\n[17]\n[6]\n[19]\n[20]\n[69]\n[24]\nHere": "",
          "Support Vector Regression\nSeveral Deep Learning methods\nRandom Forest and other methods\nRandom Forest\nConvolutional Neuronal Networks\nMultiple Linear Regression\nDecision Tree Regression\nConvolutional Neuronal Networks\nSEM with LASSO": "",
          "39\n54\n26\n15\n23\n7\n2\n64\n7": "",
          "39\n54\n29\n14\n23\n16\n4\n64\n6": "",
          "—\n—\n—\n—": "113, 4\n114, 102"
        },
        {
          "[13]\n[18]\n[17]\n[6]\n[19]\n[20]\n[69]\n[24]\nHere": "",
          "Support Vector Regression\nSeveral Deep Learning methods\nRandom Forest and other methods\nRandom Forest\nConvolutional Neuronal Networks\nMultiple Linear Regression\nDecision Tree Regression\nConvolutional Neuronal Networks\nSEM with LASSO": "",
          "39\n54\n26\n15\n23\n7\n2\n64\n7": "",
          "39\n54\n29\n14\n23\n16\n4\n64\n6": "",
          "—\n—\n—\n—": "audio spectrograms as images"
        },
        {
          "[13]\n[18]\n[17]\n[6]\n[19]\n[20]\n[69]\n[24]\nHere": "",
          "Support Vector Regression\nSeveral Deep Learning methods\nRandom Forest and other methods\nRandom Forest\nConvolutional Neuronal Networks\nMultiple Linear Regression\nDecision Tree Regression\nConvolutional Neuronal Networks\nSEM with LASSO": "",
          "39\n54\n26\n15\n23\n7\n2\n64\n7": "",
          "39\n54\n29\n14\n23\n16\n4\n64\n6": "",
          "—\n—\n—\n—": "113, 14\n114, 3 (and Arousal)"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The sonic environment of cities",
      "authors": [
        "M Southworth"
      ],
      "year": "1967",
      "venue": "The sonic environment of cities"
    },
    {
      "citation_id": "2",
      "title": "The soundscape: Our sonic environment and the tuning of the world",
      "authors": [
        "R Schafer"
      ],
      "year": "1993",
      "venue": "The soundscape: Our sonic environment and the tuning of the world"
    },
    {
      "citation_id": "3",
      "title": "Multi-stage sound planning methodology for urban redevelopment",
      "authors": [
        "T Van Renterghem",
        "L Dekoninck",
        "D Botteldooren"
      ],
      "year": "2020",
      "venue": "Sustainable Cities and Society"
    },
    {
      "citation_id": "4",
      "title": "5g iot system for real-time psycho-acoustic soundscape monitoring in smart cities with dynamic computational offloading to the edge",
      "authors": [
        "J Segura-Garcia",
        "J Calero",
        "A Pastor-Aparicio",
        "R Marco-Alaez",
        "S Felici-Castell",
        "Q Wang"
      ],
      "year": "2021",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "5",
      "title": "The emotional impact of sound: A short theory of film sound design",
      "authors": [
        "T Görne"
      ],
      "year": "2019",
      "venue": "EPiC Series in Technology"
    },
    {
      "citation_id": "6",
      "title": "A comparative analysis of modeling and predicting perceived and induced emotions in sonification",
      "authors": [
        "F Abri",
        "L Gutiérrez",
        "P Datta",
        "D Sears",
        "A Siami Namin",
        "K Jones"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "7",
      "title": "Soundscape perception indices (spis): Developing context-dependent single value scores of multidimensional soundscape perceptual quality",
      "authors": [
        "A Mitchell",
        "F Aletta",
        "T Oberman",
        "J Kang"
      ],
      "year": "2024",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "8",
      "title": "How to analyse and represent quantitative soundscape data",
      "authors": [
        "A Mitchell",
        "F Aletta",
        "J Kang"
      ],
      "year": "2022",
      "venue": "JASA Express Letters"
    },
    {
      "citation_id": "9",
      "title": "Psychological wellbeing and demographic factors can mediate soundscape pleasantness and eventfulness: A large sample study",
      "authors": [
        "M Erfanian",
        "A Mitchell",
        "F Aletta",
        "J Kang"
      ],
      "year": "2021",
      "venue": "Journal of Environmental Psychology"
    },
    {
      "citation_id": "10",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "11",
      "title": "A principal components model of soundscape perception",
      "authors": [
        "Ö Axelsson",
        "M Nilsson",
        "B Berglund"
      ],
      "year": "2010",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "12",
      "title": "Soundscape reproduction and synthesis",
      "authors": [
        "W Davies",
        "N Bruce",
        "J Murphy"
      ],
      "year": "2014",
      "venue": "Acta Acustica United with Acustica"
    },
    {
      "citation_id": "13",
      "title": "Emo-soundscapes: A dataset for soundscape emotion recognition",
      "authors": [
        "J Fan",
        "M Thorogood",
        "P Pasquier"
      ],
      "year": "2017",
      "venue": "2017 Seventh international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "14",
      "title": "A systematic review of prediction models for the experience of urban soundscapes",
      "authors": [
        "M Lionello",
        "F Aletta",
        "J Kang"
      ],
      "year": "2020",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "15",
      "title": "Athens urban soundscape (athus): a dataset for urban soundscape quality recognition",
      "authors": [
        "T Giannakopoulos",
        "M Orfanidi",
        "S Perantonis"
      ],
      "year": "2019",
      "venue": "MultiMedia Modeling: 25th International Conference"
    },
    {
      "citation_id": "16",
      "title": "Extensive crowdsourced dataset of in-situ evaluated binaural soundscapes of private dwellings containing subjective sound-related and situational ratings along with person factors to study time-varying influences on sound perception-research data",
      "authors": [
        "S Versümer",
        "J Steffens",
        "F Rosenthal"
      ],
      "year": "2023",
      "venue": "Extensive crowdsourced dataset of in-situ evaluated binaural soundscapes of private dwellings containing subjective sound-related and situational ratings along with person factors to study time-varying influences on sound perception-research data"
    },
    {
      "citation_id": "17",
      "title": "Predicting emotions perceived from sounds",
      "authors": [
        "F Abri",
        "L Gutiérrez",
        "A Namin",
        "D Sears",
        "K Jones"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "18",
      "title": "Soundscape emotion recognition via deep learning",
      "authors": [
        "J Fan",
        "F Tung",
        "W Li",
        "P Pasquier"
      ],
      "year": "2018",
      "venue": "Proceedings of the Sound and Music Computing"
    },
    {
      "citation_id": "19",
      "title": "Emotional quantification of soundscapes by learning between samples",
      "authors": [
        "S Ntalampiras"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "20",
      "title": "An exhaustive variable selection study for linear models of soundscape emotions: Rankings and Gibbs analysis",
      "authors": [
        "R San Millán-Castillo",
        "L Martino",
        "E Morgado",
        "F Llorente"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Classifying perceived emotions based on polarity of arousal and valence from sound events",
      "authors": [
        "P Krishan",
        "F Abri"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "22",
      "title": "Variable selection analysis for decision tree regression models of soundscapes emotions",
      "authors": [
        "R San Millán-Castillo",
        "L Martino",
        "E Morgado"
      ],
      "year": "2022",
      "venue": "Proceedings of the 53º Congreso Español de Acústica -XII Congreso Ibérico de Acústica. Sociedad Española de Acústica"
    },
    {
      "citation_id": "23",
      "title": "A variable selection analysis for soundscape emotion modelling using decision tree regression and modern information criteria",
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "Emotional parameter estimation from emo-soundscapes dataset using deep convolutional autoencoders",
      "authors": [
        "F Serradilla",
        "Á Juan",
        "D Martínez-Iñigo"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "25",
      "title": "Emotioncaps: Enhancing audio captioning through emotion-augmented data generation",
      "authors": [
        "M Manivannan",
        "V Nethrapalli",
        "M Cartwright"
      ],
      "year": "2024",
      "venue": "Emotioncaps: Enhancing audio captioning through emotion-augmented data generation",
      "arxiv": "arXiv:2410.12028"
    },
    {
      "citation_id": "26",
      "title": "Statistical Analysis of Network Data: Methods and Models",
      "authors": [
        "E Kolaczyk"
      ],
      "year": "2009",
      "venue": "Statistical Analysis of Network Data: Methods and Models"
    },
    {
      "citation_id": "27",
      "title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
      "authors": [
        "D Shuman",
        "S Narang",
        "P Frossard",
        "A Ortega",
        "P Vandergheynst"
      ],
      "year": "2013",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "28",
      "title": "Graph signal processing: Overview, challenges, and applications",
      "authors": [
        "A Ortega",
        "P Frossard",
        "J Kovačević",
        "J Moura",
        "P Vandergheynst"
      ],
      "year": "2018",
      "venue": "Proc. IEEE"
    },
    {
      "citation_id": "29",
      "title": "Graph IEEE Signal Process. Mag",
      "authors": [
        "X Dong",
        "D Thanou",
        "L Toni",
        "M Bronstein",
        "P Frossard"
      ],
      "year": "2023",
      "venue": "Graph IEEE Signal Process. Mag"
    },
    {
      "citation_id": "30",
      "title": "Discovering structure by learning sparse graphs",
      "authors": [
        "B Lake",
        "J Tenenbaum"
      ],
      "year": "2010",
      "venue": "Proc. Annu. Meet"
    },
    {
      "citation_id": "31",
      "title": "Graph learning from data under Laplacian and structural constraints",
      "authors": [
        "H Egilmez",
        "E Pavez",
        "A Ortega"
      ],
      "year": "2017",
      "venue": "IEEE J. Select. Topics Signal Process"
    },
    {
      "citation_id": "32",
      "title": "Connecting the dots: Identifying network structure via graph signal processing",
      "authors": [
        "G Mateos",
        "S Segarra",
        "A Marques",
        "A Ribeiro"
      ],
      "year": "2019",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "33",
      "title": "Joint network topology inference via a shared graphon model",
      "authors": [
        "M Navarro",
        "S Segarra"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Signal Process"
    },
    {
      "citation_id": "34",
      "title": "Sparse inverse covariance estimation with the graphical lasso",
      "authors": [
        "J Friedman",
        "T Hastie",
        "R Tibshirani"
      ],
      "year": "2008",
      "venue": "Biostatistics"
    },
    {
      "citation_id": "35",
      "title": "Enhanced graph-learning schemes driven by similar distributions of motifs",
      "authors": [
        "S Rey",
        "T Roddenberry",
        "S Segarra",
        "A Marques"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Signal Process"
    },
    {
      "citation_id": "36",
      "title": "Sparse structural equation modeling for inference of gene regulatory networks exploiting genetic perturbations",
      "authors": [
        "X Cai",
        "J Bazerque",
        "G Giannakis"
      ],
      "year": "2013",
      "venue": "PLoS, Comput. Biology"
    },
    {
      "citation_id": "37",
      "title": "Proximal-gradient algorithms for tracking cascades over social networks",
      "authors": [
        "B Baingana",
        "G Mateos",
        "G Giannakis"
      ],
      "year": "2014",
      "venue": "IEEE J. Select. Topics Signal Process"
    },
    {
      "citation_id": "38",
      "title": "Dags with no tears: Continuous optimization for structure learning",
      "authors": [
        "X Zheng",
        "B Aragam",
        "P Ravikumar",
        "E Xing"
      ],
      "year": "2018",
      "venue": "Conf. Neural Inform. Process. Syst."
    },
    {
      "citation_id": "39",
      "title": "Non-negative weighted dag structure learning",
      "authors": [
        "S Rey",
        "S Saboksayr",
        "G Mateos"
      ],
      "year": "2025",
      "venue": "IEEE Int. Conf. Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "40",
      "title": "How to learn a graph from smooth signals",
      "authors": [
        "V Kalofolias"
      ],
      "year": "2016",
      "venue": "Intl. Conf. Artif. Intel. Statist. (AISTATS). J. Mach. Learning Res"
    },
    {
      "citation_id": "41",
      "title": "Learning Laplacian matrix in smooth graph signal representations",
      "authors": [
        "X Dong",
        "D Thanou",
        "P Frossard",
        "P Vandergheynst"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Signal Process"
    },
    {
      "citation_id": "42",
      "title": "Learning graphs from smooth and graph-stationary signals with hidden variables",
      "authors": [
        "A Buciulea",
        "S Rey",
        "A Marques"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Signal, Inform. Process. Networks"
    },
    {
      "citation_id": "43",
      "title": "Network topology inference from spectral templates",
      "authors": [
        "S Segarra",
        "A Marques",
        "G Mateos",
        "A Ribeiro"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Signal, Inform. Process. Networks"
    },
    {
      "citation_id": "44",
      "title": "Online topology inference from streaming stationary graph signals with partial connectivity information",
      "authors": [
        "R Shafipour",
        "G Mateos"
      ],
      "year": "2020",
      "venue": "Algorithms"
    },
    {
      "citation_id": "45",
      "title": "Joint network topology inference in the presence of hidden nodes",
      "authors": [
        "M Navarro",
        "S Rey",
        "A Buciulea",
        "A Marques",
        "S Segarra"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Signal Process"
    },
    {
      "citation_id": "46",
      "title": "Polynomial graphical lasso: Learning edges from gaussian graph-stationary signals",
      "authors": [
        "A Buciulea",
        "J Ying",
        "A Marques",
        "D Palomar"
      ],
      "year": "2025",
      "venue": "IEEE Trans. Signal Process"
    },
    {
      "citation_id": "47",
      "title": "Structural equation modeling: Foundations and extensions",
      "authors": [
        "D Kaplan"
      ],
      "year": "2008",
      "venue": "Structural equation modeling: Foundations and extensions"
    },
    {
      "citation_id": "48",
      "title": "Directlingam: A direct method for learning a linear non-gaussian structural equation model",
      "authors": [
        "S Shimizu",
        "T Inazumi",
        "Y Sogawa",
        "A Hyvarinen",
        "Y Kawahara",
        "T Washio",
        "P Hoyer",
        "K Bollen",
        "P Hoyer"
      ],
      "year": "2011",
      "venue": "J. Mach. Learning Res"
    },
    {
      "citation_id": "49",
      "title": "Universal and automatic elbow detection for learning the effective number of components in model selection problems",
      "authors": [
        "E Morgado",
        "L Martino",
        "R San Millán-Castillo"
      ],
      "year": "2023",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "50",
      "title": "Freesound datasets: a platform for the creation of open audio datasets",
      "authors": [
        "E Fonseca",
        "J Pons Puig",
        "X Favory",
        "F Font",
        "D Corbera",
        "A Bogdanov",
        "S Ferraro",
        "A Oramas",
        "X Porter",
        "Serra"
      ],
      "year": "2017",
      "venue": "Proceedings of the 18th ISMIR Conference"
    },
    {
      "citation_id": "51",
      "title": "Soundscapes and environmental noise management",
      "authors": [
        "A Brown"
      ],
      "year": "2010",
      "venue": "Noise Control Engineering Journal"
    },
    {
      "citation_id": "52",
      "title": "Soundscape evaluation: Binaural or monaural?",
      "authors": [
        "C Xu",
        "J Kang"
      ],
      "year": "2019",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "53",
      "title": "Automatic soundscape affect recognition using a dimensional approach",
      "authors": [
        "J Fan",
        "M Thorogood",
        "P Pasquier"
      ],
      "year": "2016",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "54",
      "title": "Yaafe, an easy to use and efficient audio feature extraction software",
      "authors": [
        "B Mathieu",
        "S Essid",
        "T Fillon",
        "J Prado",
        "G Richard"
      ],
      "year": "2010",
      "venue": "ISMIR. Citeseer"
    },
    {
      "citation_id": "55",
      "title": "in Data analysis, machine learning and applications",
      "authors": [
        "O Lartillot",
        "P Toiviainen",
        "T Eerola"
      ],
      "year": "2008",
      "venue": "in Data analysis, machine learning and applications"
    },
    {
      "citation_id": "56",
      "title": "Fuzzy cognitive maps for artificial emotions forecasting",
      "authors": [
        "J Salmeron"
      ],
      "year": "2012",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "57",
      "title": "Online learning of expanding graphs",
      "authors": [
        "S Rey",
        "B Das",
        "E Isufi"
      ],
      "year": "2025",
      "venue": "EEE Open J. Signal Signal Process"
    },
    {
      "citation_id": "58",
      "title": "Highdimensional covariance estimation by minimizing ℓ 1 -penalized logdeterminant divergence",
      "authors": [
        "P Ravikumar",
        "M Wainwright",
        "G Raskutti",
        "B Yu"
      ],
      "year": "2011",
      "venue": "Electron. J. Statist"
    },
    {
      "citation_id": "59",
      "title": "Convex Optimization",
      "authors": [
        "S Boyd",
        "L Vandenberghe"
      ],
      "year": "2004",
      "venue": "Convex Optimization"
    },
    {
      "citation_id": "60",
      "title": "First-order methods in optimization",
      "authors": [
        "A Beck"
      ],
      "year": "2017",
      "venue": "SIAM"
    },
    {
      "citation_id": "61",
      "title": "Fair glasso: Estimating fair graphical models with unbiased statistical behavior",
      "authors": [
        "M Navarro",
        "S Rey",
        "A Buciulea",
        "A Marques",
        "S Segarra"
      ],
      "year": "2024",
      "venue": "Conf. Neural Inform. Process. Syst."
    },
    {
      "citation_id": "62",
      "title": "The relation between valence and arousal in subjective experience",
      "authors": [
        "P Kuppens",
        "F Tuerlinckx",
        "J Russell",
        "L Barrett"
      ],
      "year": "2013",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "63",
      "title": "Spectral information criterion for automatic elbow detection",
      "authors": [
        "L Martino",
        "R San Millán-Castillo",
        "E Morgado"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "64",
      "title": "Model selection for ecologists: the worldviews of AIC and BIC",
      "authors": [
        "K Aho",
        "D Derryberry",
        "T Peterson"
      ],
      "year": "2014",
      "venue": "Ecology"
    },
    {
      "citation_id": "65",
      "title": "Who belongs in the family?",
      "authors": [
        "R Thorndike"
      ],
      "year": "1953",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "66",
      "title": "A critical review of LASSO and its derivatives for variable selection under dependence among covariates",
      "authors": [
        "L Freijeiro-González",
        "M Febrero-Bande",
        "W González-Manteiga"
      ],
      "year": "2022",
      "venue": "International Statistical Review"
    },
    {
      "citation_id": "67",
      "title": "Regression shrinkage and selection via the Lasso",
      "authors": [
        "R Tibshirani"
      ],
      "year": "1996",
      "venue": "Journal of the Royal Statistical Society. Series B (Methodological)"
    },
    {
      "citation_id": "68",
      "title": "A variable selection analysis for soundscape emotion modelling using decision tree regression and modern information criteria",
      "authors": [
        "R San Millán-Castillo",
        "L Martino",
        "E Morgado"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "69",
      "title": "Araus: A large-scale dataset and baseline models of affective responses to augmented urban soundscapes",
      "authors": [
        "K Ooi",
        "Z.-T Ong",
        "K Watcharasupat",
        "B Lam",
        "J Hong",
        "W.-S Gan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}