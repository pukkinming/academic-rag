{
  "paper_id": "2110.00385v2",
  "title": "Neural Dependency Coding Inspired Multimodal Fusion",
  "published": "2021-09-28T17:52:09Z",
  "authors": [
    "Shiv Shankar"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Information integration from different modalities is an active area of research. Human beings and, in general, biological neural systems are quite adept at using a multitude of signals from different sensory perceptive fields to interact with the environment and each other. Recent work in deep fusion models via neural networks has led to substantial improvements over unimodal approaches in areas like speech recognition, emotion recognition and analysis, captioning and image description. However, such research has mostly focused on architectural changes allowing for fusion of different modalities while keeping the model complexity manageable. Inspired by recent neuroscience ideas about multisensory integration and processing, we investigate the effect of introducing neural dependencies in the loss functions. Experiments on multimodal sentiment analysis tasks: CMU-MOSI and CMU-MOSEI with different models show that our approach provides a consistent performance boost.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human beings perceive the world as a unified whole, not in individual sensory modalities. While traditionally different sensory models have been studied in isolation, it has been well recognized that perception operates via the integration of information from multiple sensory modalities. Research in multimodal fusion aims to achieve a similar goal in artificial models: integrate different unimodal representations into a unified common representation.\n\nHeterogeneities across different modalities mean that learning multimodal representations face challenges like feature shifts, distributional effects, nuisance variation and other related challenges  (Baltrušaitis, Ahuja, and Morency 2018) . Effective fusion still is an open and challenging task. While in some areas like opinion analysis  (Garcia et al. 2019; Soleymani et al. 2017) , speech recognition, image processing  (Xu, Tao, and Xu 2015, 2013)  and trait analysis  (Park et al. 2014 ) fusion models have achieved substantial improvements over their unimodal counterparts ;in other areas like sentiment analysis  (Rahman et al. 2020 ) the improvements have been uninspiring.\n\nCurrent research in deep multimodal fusion primarily deals with architectural improvements to create complex feature rich yet efficient representations  (Zadeh et al. 2017; Liu et al. 2018; Hazarika, Zimmermann, and Poria 2020) . Recently  (Rahman et al. 2020 ) used pre-trained transformer  (Tsai et al. 2019; Siriwardhana et al. 2020 ) based models to achieve state-of the-art results on multimodal sentiment benchmark MOSI  (Wöllmer et al. 2013)  and MOSEI  (Zadeh et al. 2018c) .\n\nUnlike these prior works, we do not focus not on architectural changes. Inspiring from work in multisensory neural processing, we utilize a concept known as synergy or dependency to train these models. Synergy is an informationtheoretic concept related to mutual information (MI). The synergy between random variables X and Y refers to the unique mutual information that X provides about Y . As it is based on conditional mutual information, synergy is able to capture relations between modalities that are difficult to capture via techniques like subspace alignment which implicitly assume linear dependencies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Preliminaries",
      "text": "In this section, we give an overview of mutual information and existing work on deep multimodal fusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Fusion",
      "text": "The problem in the most abstract terms is a supervised learning problem. We are provided with a dataset of N observations D = (x i , y i ) N i=1 . All x i come from a space X and y i from Y. We are provided a loss function L : Y × Y → R which is the task loss. Our goal is to learn a model\n\nIn multimodal fusion the space of inputs X naturally decomposes into K different modalities X = K j=1 X j . We use X j to represent random variables which form the individual modality specific components of the input random variable X.\n\nThe standard way to learn such a multimodal function is to decompose it into two components: a) an embedding component E which fuses information into a high dimensional vector in R d and b) a predictive component P which maps vector from R d to Y. Furthermore since the different modalities are often no directly compatible with each other (for eg text and image), E itself is decomposed into a) modality specific readers F i X i → R di which are specifically designed for each individual modality X i and b) a fusion component F : i R di → R d which fuses information from eah individual modality embedding. F is provided with uni-modal representations of the inputs X i = (X 1 , X 2 , . . . X K ) obtained through embedding networks f i . F has to retain both unimodal dependencies (i.e relations between features that span only one modality) and multi-modal dependency (i.e relationships between features across multiple modalities).\n\nThis decomposition has two advantages a) the individual modality reader can be pre-trained on the task at hand or even from a larger dataset (for example BERT  (Devlin et al. 2018)  for language, Resnet (?) for images ) which allows us to leverage wider modality specific information and b) often but not always each individual modality is in principle enough to correctly predict the output Plethora of neural architectures have been proposed to learn multimodal representations for sentiment classification. Models often rely on a fusion mechanism  (Khan et al. 2012) , tensor factorisation  (Liu et al. 2018; Zadeh et al. 2019)  or complex attention mechanisms  (Zadeh et al. 2018a ) that is fed with modality specific representations.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Mutual Information",
      "text": "Mutual information (MI) measures the dependence between two random variable X and Y and is able to incorporate all forms of relationships between the two. MI between X and Y , is given by\n\nwhere p XY is the joint probability density of the pair (X, Y ), and p X , p Y are the marginal probability densities of X, Y respectively  (Cover 1999) . It is also clear from the above expression that it equals the KL divergence of the joint density of (X, Y ) relative to the product of marginals of X and Y .\n\nSince under the assumption of independence, the distribution of the pair (X, Y ) is given by the product of their individual distributions; the MI between X, Y is 0 if and only if X, Y are independent. As such mutual information can be understood as a measure of the inherent dependence expressed in the joint distribution of X, Y .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Estimation Of Mutual Information",
      "text": "The computation of MI and other mutual dependency measures based only on samples is difficult. \"Reliably estimating mutual information from finite continuous data remains a significant and unresolved problem\"  (Kinney and Atwal 2014) . Recently several estimators have been proposed based on optimization of variational bounds with flexible neural networks as proposed witness functions. For example  Belghazi et al. (2018)  maximize the Donsker-Varadhan bound  (Donsker and Varadhan 1985)  to estimate KL divergence.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Alternative Dependence Measures",
      "text": "Given the interpretation of the mutual information between variables as a divergence between their joint distribution and the independence implied distribution, other measures of mutual dependence can and have been proposed by replacing the KL divergence in with other proper discrepancy measures  (Lopez and Jog 2018; Belghazi et al. 2018; Griffith et al. 2014)  We will focus on the Hilbert-Schmidt criteria (also knowns as MMD criteria) which is also used in our experiments\n\n• Hilbert-Schmidt Dependence  (Gretton et al. 2005 ) is obtained by using the Maximum Mean Discrepancy or MMD  (Gretton et al. 2012 ) instead of KL divergence in (1). Further extensions to MMD have been developed based on neural networks which provide non-universal but more powerful kernel based tests  (Liu et al. 2020) .\n\nNeural Synergy\n\nThe aforementioned measures have been developed for the case of two random variables. Extension of mutual information to the multivariate case is an active area of research in information theory  (Griffith and",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Neural Dependency Coding In Multisensory Processing",
      "text": "A common and vital feature of nervous systems is the integration of information arriving simultaneously from multiple sensory pathways. The underlying neural structures have been found to be related in both vertebrates and invertebrates. The classic understanding of this process is that different sensory modalities are processed individually and then combined in various multimodal convergence zones, including cortical and subcortical regions  (Ghazanfar and Schroeder 2006) , as well as multimodal association areas  (Rauschecker, Tian, and Hauser 1995) . Studies in the superior colliculus  (Meredith, Nemitz, and Stein 1987)  showed that multiple sensory modalities are processed in this brain stem region, with some neurons being exclusively unimodal and others being multimodal. Hypotheses of encoding of multimodal information include changes in neuronal firing rates  (Pennartz 2009)  or a combinatorial code in population of neurons  (Osborne et al. 2008; Rohe and Noppeney 2016) .\n\nStudies of multisensory collicular neurons suggest that their crossmodal receptive fields (RF) often overlap  (Spence, Driver, and Driver 2004) . This pattern is also found in multisensory neurons present in other brain regions. As such, a spatiotemporal hypothesis of multisensory integration has been suggested: superadditive multimodal processing is observed when information from different modalities comes from spatiotemporally overlapping receptive fields  (Recanzone 2003; Wallace et al. 2004; Stanford, Quessy, and Stein 2005) . Since multimodal cortical neurons are generally downstream of modality-specific regions, the information about RF overlap is present in their input unimodal neural representations. Recent observations have led to the discovery of multimodal neurons in the generally modality-specific regions suggesting that a non-trivial part of the process happens in distributed circuits  (Schroeder and Foxe 2005; Stein and Stanford 2008) . Tyll, Budinger, and Noesselt (2011) provide evidence that that cortical multimodal processing is influenced via corticothalamic connections. Moreover, the sensory-specific nuclei of the thalamus have been shown to feed multisensory information to primary sensory specific-cortices  (Kayser, Petkov, and Logothetis 2008)  Other evidence also shows that while multimodal representations are distinct from unimodal ones, there is sufficient overlap between the set of neurons that process different sensory modalities. For example, Follmann, Goldsmith, and  Stein (2018)  show that even in a simple crustacean organism, more than half the neurons in the commissural ganglion are multimodal. Moreover, they show that in 30% of these multimodal neurons, responses to one modality were predictive of responses to other modalities. Both these facts suggest that the neural representations across different modalities have high information about each other.\n\nCortical and subcortical networks often contain clusters of strongly connected neurons. Functionally the existence of such cliques imply highly integrated pyramidal cells that handle a disproportionately large amount of traffic  (Harriger, Van Den Heuvel, and Sporns 2012) . In cortical circuits, around 20% of the neurons account for 80% of the information propagation  (Nigam et al. 2016; Van Den Heuvel and Sporns 2011) .  Timme et al. (2016) ;  Faber et al. (2019)  demonstrate that multimodal computation tends to concentrate in such local cortical clusters. They also found significantly elevated synergy in such clusters and that the amount of synergy was proportional to the amount of information flow, suggesting that neural synergy emerges where there is more significant cognitive processing and information flow.\n\nHigh synergy has also been hypothesized to facilitate inter-circuit communication and intra-circuit processing. Correlated neural representation, especially synergic activity, are indicative of the recurrent oscillations that are believed to underlie multisensory cognition  (Hasselmo, Bodelón, and Wyble 2002;   Hernández-Pérez, Cooper, and Newman 2020; Honey, Newman, and Schapiro 2017).  Fries (2015) ;  Yu et al. (2008)  show the importance of synergy for orga-nizing information in cortical circuits. The synchronization of neuronal circuits is related to higher-order processing  (Vinck et al. 2015)  especially in thalamic and cortical circuits. Moreover,  Sherrill et al. (2021)  recently have shown that recurrent information flow in cortical circuits leads to an increase in neural synergy and neural complexity.  Sherrill et al. (2020)  show that correlated activity and neural synergy were positively related when multiple external correlated stimuli are provided. Thus, synergy in neural firings is a representation of the similarity between inputs. However, synergy is also low when the stimuli are very highly correlated. This suggests that correlated neural firings are a means of combining both redundancy and complementarity when faced with multiple inputs. This also points to the possibility that multimodal perception is most efficient when inputs produce maximal synergy.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model",
      "text": "For our purposes we will limit ourselves to talk about tasks similar to the MOSI dataset. In this setting the input has three modalities viz audio (a), visal (v) and text (t) The fusion problem involves learning a representation E θ that combined the uni-modal representations of the inputs X a,v,t = (X a , X v , X t ).\n\nTo train our neural architecture we need to estimate the previously defined synergy measures. Multivarite synergy is defined in terms of mutual information between different random variables in the combined distribution. We extend synergy to also include an MMD based measure of dependence as well as defined earlier. For estimation of these dependence measures, we use estimators of the following distribution divergences for this purpose in our experiments.\n\n• Kullback Liebler Dependence/Mutual Information -We use the method of  Belghazi et al. (2018)  who use the Donsker Varadhan bound  (Donsker and Varadhan 1985; Belghazi et al. 2018)  to estimate the KL divergence between the requisite distribution. This version corresponds to the standard definition of synergy.\n\n• Neural Hilbert Schmidt Dependence -We use neural kernel augmented MMD  (Liu et al. 2020)  to estimate the divergence between the requisite joint distributions to estimate synergy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments Datasets",
      "text": "We empirically evaluate our methods on two commonly used datasets for multimodal training viz CMU-MOSI and CMU-MOSEI. CMU-MOSI  (Wöllmer et al. 2013 ) is sentiment prediction tasks on a set of short youtube video clips. CMU-MOSEI  (Zadeh et al. 2018b ) is a similar dataset consisting of around 23k review videos taken from YouTube. The output in both cases is a sentiment score in  [-3, 3] . For each dataset, three modalities are available; audio, video, and language text.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Models",
      "text": "We run our experiments with the following architectures:\n\n• Tensor Fusion Network or TFN  (Zadeh et al. 2017 ) combined information via pooling of a high dimensional tensor representation of multimodal features. More specifically it does a multimodal Hadamard product of the aggregated features with RNN based language features. • Memory Fusion Network or MFN  (Zadeh et al. 2018a)  incorporate gated memory-units to store multiview representations. It then performs an attention augmented readout over the memory units to combine information into a single representation. • MAGBERT  (Rahman et al. 2020 ) is a transformer based architecture that uses the Wang gate  (Wang et al. 2019) .\n\nThe multimodal information is send to the multimodal gate to compute modified embeddings which are passed to a BERT  (Devlin et al. 2018)  based model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation",
      "text": "For our experiments we evaluate and report both the Mean Absolute Error (MAE) and the correlation of model predictions with true labels. Existing works  (Rahman et al. 2020 ) also use the output of the regression model, to predict a positive or negative sentiment on the data, using it as binary classifier. Using the same approach we report the accuracy or our models. In the results Accuracy Acc 7 denotes accuracy on 7 classes and Acc 2 denotes the binary accuracy. We also report the correlation of model intensity predictions with true values.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Results on MOSI are presented in Table  1  while Table  2  present results for MOSEI dataset. We trained each of the models with the standard cross-entropy loss (labeled as MLE) and with cross-entropy loss regularized with the synergy penalty discussed earlier. On both datasets our model performs improve on their respective baselines; sometimes by more than 3 points. Also, note that just changing the training loss provides a reasonable improvement over the stateof-the-art model MAGBERT  (Rahman et al. 2020) . This shows that our approach is generalizable across architectures.\n\nOur results also show that in general MMD based models tends to be better than the KL divergence based models. The greater efficacy of MMD based synergy might be due to the inherent behavior of the MMD dependency, which is always well defined, or it might reflect the hardness of information estimation. For example, it is well known that reasonable bounds on standard mutual information are challenging to obtain  (Kinney and Atwal 2014) ; while MMD estimators do not suffer from the entropy estimation issue and are consistent  (Gretton et al. 2012)  Recently  Colombo et al. (2020)  has conducted experiments on similar lines. The main differences between the our method and their method are a) our method focuses on synergy terms whereas their proposal is optimizing joint mutual information between different unimodal representations; and b) they experiment with variational measures of information whereas we use exact measures in the MMD criteria. We replicate our experiments with their best performing model and present the results in our Tables ?? with the label MI W as . It is clear that our proposal is better than the Waserstein model used by  Colombo et al. (2020) . Such an approach was also used by  Han et al. (2020) ; however their proposal includes architectural changes as well which makes the exact comparison unclear. However these results present further evidence of the utility of some of form of dependence driver optimization for multimodal fusion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we introduced the idea of synergy maximization. We experimented with different measures of synergy based on discrepancy measures such as KL divergene and MMD distance. We show that training with synergy can produce benefit on even state-of-the-art architectures. Similar experiments have been performed recently by  Han et al. (2020)  and  Colombo et al. (2020)  suggesting mutual information based approaches can be used to improve multimodal fusion.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table 1: Results on sentiment analysis on CMU-MOSI.",
      "data": [
        {
          "MFN": "31.3\n76.6\n1.01\n0.62\n34.5\n76.9\n0.94\n0.65\n35.9\n77.4\n0.95\n0.66\n35.1\n77.1\n0.97\n0.63"
        },
        {
          "MFN": "LFN"
        },
        {
          "MFN": "31.9\n76.9\n1.01\n0.64\n32.6\n77.6\n0.97\n0.64\n35.4\n77.9\n0.97\n0.67\n32.4\n77.6\n0.97\n0.64"
        },
        {
          "MFN": "MAGBERT"
        },
        {
          "MFN": "40.2\n83.7\n0.79\n0.80\n41.9\n84.3\n0.76\n0.82\n41.9\n85.6\n0.76\n0.82\n41.8\n84.2\n0.76\n0.82"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Results on sentiment analysis on CMU-MOSI.",
      "data": [
        {
          "MFN": "44.3\n74.7\n0.72\n0.52\n45.3\n74.8\n0.72\n0.56\n46.2\n75.1\n0.69\n0.56\n45.1\n75.2\n0.72\n0.54"
        },
        {
          "MFN": "LFN"
        },
        {
          "MFN": "45.2\n74.3\n0.70\n0.54\n46.1\n75.3\n0.69\n0.56\n46.3\n75.3\n0.67\n0.56\n45.9\n75.1\n0.69\n0.55"
        },
        {
          "MFN": "MAGBERT"
        },
        {
          "MFN": "46.9\n84.8\n0.59\n0.77\n47.4\n85.3\n0.59\n0.79\n47.9\n85.4\n0.59\n0.79\n47.2\n85.0\n0.59\n0.78"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "2",
      "title": "Mine: mutual information neural estimation",
      "authors": [
        "M Belghazi",
        "A Baratin",
        "S Rajeswar",
        "S Ozair",
        "Y Bengio",
        "A Courville",
        "R Hjelm"
      ],
      "year": "2018",
      "venue": "Mine: mutual information neural estimation",
      "arxiv": "arXiv:1801.04062"
    },
    {
      "citation_id": "3",
      "title": "Elements of information theory",
      "authors": [
        "T Cover"
      ],
      "year": "1999",
      "venue": "Elements of information theory"
    },
    {
      "citation_id": "4",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "5",
      "title": "Large deviations for stationary Gaussian processes",
      "authors": [
        "M Donsker",
        "S Varadhan"
      ],
      "year": "1985",
      "venue": "Communications in Mathematical Physics"
    },
    {
      "citation_id": "6",
      "title": "Computation is concentrated in rich clubs of local cortical networks",
      "authors": [
        "S Faber",
        "N Timme",
        "J Beggs",
        "E Newman"
      ],
      "year": "2019",
      "venue": "Network Neuroscience"
    },
    {
      "citation_id": "7",
      "title": "Multimodal sensory information is represented by a combinatorial code in a sensorimotor system",
      "authors": [
        "R Follmann",
        "C Goldsmith",
        "W Stein"
      ],
      "year": "2018",
      "venue": "PLoS biology"
    },
    {
      "citation_id": "8",
      "title": "Rhythms for cognition: communication through coherence",
      "authors": [
        "P Fries"
      ],
      "year": "2015",
      "venue": "Neuron"
    },
    {
      "citation_id": "9",
      "title": "Common information is far less than mutual information",
      "authors": [
        "P Gács",
        "J Körner"
      ],
      "year": "1973",
      "venue": "Problems of Control and Information Theory"
    },
    {
      "citation_id": "10",
      "title": "A multimodal movie review corpus for fine-grained opinion mining",
      "authors": [
        "A Garcia",
        "S Essid",
        "F Buc",
        "C Clavel"
      ],
      "year": "2019",
      "venue": "A multimodal movie review corpus for fine-grained opinion mining",
      "arxiv": "arXiv:1902.10102"
    },
    {
      "citation_id": "11",
      "title": "Is neocortex essentially multisensory?",
      "authors": [
        "A Ghazanfar",
        "C Schroeder"
      ],
      "year": "2006",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "12",
      "title": "A Kernel Two-Sample Test",
      "authors": [
        "A Gretton",
        "K Borgwardt",
        "M Rasch",
        "B Schölkopf",
        "A Smola"
      ],
      "year": "2012",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "13",
      "title": "Measuring statistical dependence with Hilbert-Schmidt norms",
      "authors": [
        "A Gretton",
        "O Bousquet",
        "A Smola",
        "B Schölkopf"
      ],
      "year": "2005",
      "venue": "International conference on algorithmic learning theory"
    },
    {
      "citation_id": "14",
      "title": "Intersection information based on common randomness",
      "authors": [
        "V Griffith",
        "E Chong",
        "R James",
        "C Ellison",
        "J Crutchfield"
      ],
      "year": "2014",
      "venue": "Entropy"
    },
    {
      "citation_id": "15",
      "title": "Quantifying synergistic mutual information",
      "authors": [
        "V Griffith",
        "C Koch"
      ],
      "year": "2014",
      "venue": "Guided self-organization: inception"
    },
    {
      "citation_id": "16",
      "title": "Rich club organization of macaque cerebral cortex and its role in network communication",
      "authors": [
        "L Harriger",
        "Van Den",
        "M Heuvel",
        "O Sporns"
      ],
      "year": "2012",
      "venue": "Rich club organization of macaque cerebral cortex and its role in network communication"
    },
    {
      "citation_id": "17",
      "title": "A proposed function for hippocampal theta rhythm: separate phases of encoding and retrieval enhance reversal of prior learning",
      "authors": [
        "M Hasselmo",
        "C Bodelón",
        "B Wyble"
      ],
      "year": "2002",
      "venue": "Neural computation"
    },
    {
      "citation_id": "18",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Medial entorhinal cortex activates in a traveling wave in the rat",
      "authors": [
        "J Hernández-Pérez",
        "K Cooper",
        "E Newman"
      ],
      "year": "2020",
      "venue": "Elife"
    },
    {
      "citation_id": "20",
      "title": "Switching between internal and external modes: a multiscale learning principle",
      "authors": [
        "C Honey",
        "E Newman",
        "A Schapiro"
      ],
      "year": "2017",
      "venue": "Network Neuroscience"
    },
    {
      "citation_id": "21",
      "title": "Measuring multivariate redundant information with pointwise common change in surprisal",
      "authors": [
        "R Ince"
      ],
      "year": "2017",
      "venue": "Entropy"
    },
    {
      "citation_id": "22",
      "title": "Multivariate dependence beyond Shannon information",
      "authors": [
        "R James",
        "J Crutchfield",
        "C Kayser",
        "C Petkov",
        "N Logothetis"
      ],
      "year": "2008",
      "venue": "Cerebral Cortex"
    },
    {
      "citation_id": "23",
      "title": "Color attributes for object detection",
      "authors": [
        "F Khan",
        "R Anwer",
        "J Van De Weijer",
        "A Bagdanov",
        "M Vanrell",
        "A Lopez"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Equitability, mutual information, and the maximal information coefficient",
      "authors": [
        "J Kinney",
        "G Atwal"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "25",
      "title": "A novel approach to multivariate redundancy and synergy",
      "authors": [
        "A Kolchinsky"
      ],
      "year": "2019",
      "venue": "A novel approach to multivariate redundancy and synergy",
      "arxiv": "arXiv:1908.08642"
    },
    {
      "citation_id": "26",
      "title": "Learning deep kernels for non-parametric two-sample tests",
      "authors": [
        "F Liu",
        "W Xu",
        "J Lu",
        "G Zhang",
        "A Gretton",
        "D Sutherland"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "27",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "28",
      "title": "Generalization error bounds using Wasserstein distances",
      "authors": [
        "A Lopez",
        "V Jog"
      ],
      "year": "2018",
      "venue": "IEEE Information Theory Workshop (ITW)"
    },
    {
      "citation_id": "29",
      "title": "Determinants of multisensory integration in superior colliculus neurons. I. Temporal factors",
      "authors": [
        "M Meredith",
        "J Nemitz",
        "B Stein"
      ],
      "year": "1987",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "30",
      "title": "Temporal Multimodal Fusion for Driver Behavior Prediction Tasks using Gated Recurrent Fusion Units",
      "authors": [
        "A Narayanan",
        "A Siravuru",
        "B Dariush"
      ],
      "year": "2019",
      "venue": "Temporal Multimodal Fusion for Driver Behavior Prediction Tasks using Gated Recurrent Fusion Units"
    },
    {
      "citation_id": "31",
      "title": "Rich-club organization in effective connectivity among cortical neurons",
      "authors": [
        "S Nigam",
        "M Shimono",
        "S Ito",
        "F.-C Yeh",
        "N Timme",
        "M Myroshnychenko",
        "C Lapish",
        "Z Tosi",
        "P Hottowy",
        "W Smith"
      ],
      "year": "2016",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "32",
      "title": "The neural basis for combinatorial coding in a cortical population response",
      "authors": [
        "L Osborne",
        "S Palmer",
        "S Lisberger",
        "W Bialek"
      ],
      "year": "2008",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "33",
      "title": "Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach",
      "authors": [
        "S Ozair",
        "C Lynch",
        "Y Bengio",
        "A Oord",
        "S Levine",
        "P Sermanet",
        "S Park",
        "H Shim",
        "M Chatterjee",
        "K Sagae",
        "L.-P Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction",
      "arxiv": "arXiv:1903.11780"
    },
    {
      "citation_id": "34",
      "title": "Identification and integration of sensory modalities: neural basis and relation to consciousness",
      "authors": [
        "C Pennartz"
      ],
      "year": "2009",
      "venue": "Consciousness and cognition"
    },
    {
      "citation_id": "35",
      "title": "Probability metrics and the stability of stochastic models",
      "authors": [
        "S Rachev"
      ],
      "year": "1991",
      "venue": "Probability metrics and the stability of stochastic models"
    },
    {
      "citation_id": "36",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Zadeh",
        "C Mao",
        "L.-P Morency",
        "E Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "37",
      "title": "Processing of complex sounds in the macaque nonprimary auditory cortex",
      "authors": [
        "J Rauschecker",
        "B Tian",
        "M Hauser"
      ],
      "year": "1995",
      "venue": "Science"
    },
    {
      "citation_id": "38",
      "title": "Auditory influences on visual temporal rate perception",
      "authors": [
        "G Recanzone"
      ],
      "year": "2003",
      "venue": "Journal of neurophysiology"
    },
    {
      "citation_id": "39",
      "title": "Distinct computational principles govern multisensory integration in primary sensory and association cortices",
      "authors": [
        "T Rohe",
        "U Noppeney",
        "F Rosas",
        "P Mediano",
        "M Gastpar",
        "H Jensen"
      ],
      "year": "2016",
      "venue": "Physical Review E"
    },
    {
      "citation_id": "40",
      "title": "Multisensory contributions to low-level,'unisensory'processing",
      "authors": [
        "C Schroeder",
        "J Foxe"
      ],
      "year": "2005",
      "venue": "Current opinion in neurobiology"
    },
    {
      "citation_id": "41",
      "title": "Correlated activity favors synergistic processing in local cortical networks in vitro at synaptically relevant timescales",
      "authors": [
        "S Sherrill",
        "N Timme",
        "J Beggs",
        "E Newman"
      ],
      "year": "2020",
      "venue": "Network Neuroscience"
    },
    {
      "citation_id": "42",
      "title": "Synergistic neural integration is greater downstream of recurrent information flow in organotypic cortical cultures",
      "authors": [
        "S Sherrill",
        "N Timme",
        "J Beggs",
        "E Newman"
      ],
      "year": "2021",
      "venue": "bioRxiv"
    },
    {
      "citation_id": "43",
      "title": "Jointly Fine-Tuning\" BERT-like",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "44",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S.-F Chang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "45",
      "title": "Understanding the limitations of variational mutual information estimators",
      "authors": [
        "J Song",
        "S Ermon"
      ],
      "year": "2019",
      "venue": "Understanding the limitations of variational mutual information estimators",
      "arxiv": "arXiv:1910.06222"
    },
    {
      "citation_id": "46",
      "title": "Crossmodal space and crossmodal attention",
      "authors": [
        "C Spence",
        "J Driver",
        "J Driver"
      ],
      "year": "2004",
      "venue": "Crossmodal space and crossmodal attention"
    },
    {
      "citation_id": "47",
      "title": "Evaluating the operations underlying multisensory integration in the cat superior colliculus",
      "authors": [
        "T Stanford",
        "S Quessy",
        "B Stein"
      ],
      "year": "2005",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "48",
      "title": "Multisensory integration: current issues from the perspective of the single neuron",
      "authors": [
        "B Stein",
        "T Stanford"
      ],
      "year": "2008",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "49",
      "title": "High-degree neurons feed cortical computations",
      "authors": [
        "N Timme",
        "S Ito",
        "M Myroshnychenko",
        "S Nigam",
        "M Shimono",
        "F.-C Yeh",
        "P Hottowy",
        "A Litke",
        "J Beggs"
      ],
      "year": "2016",
      "venue": "PLoS computational biology"
    },
    {
      "citation_id": "50",
      "title": "A measure for brain complexity: relating functional segregation and integration in the nervous system",
      "authors": [
        "G Tononi",
        "O Sporns",
        "G Edelman"
      ],
      "year": "1994",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "51",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov",
        "S Tyll",
        "E Budinger",
        "T Noesselt"
      ],
      "year": "2011",
      "venue": "Thalamic influences on multisensory integration. Communicative & integrative biology"
    },
    {
      "citation_id": "52",
      "title": "Rich-club organization of the human connectome",
      "authors": [
        "Van Den",
        "M Heuvel",
        "O Sporns"
      ],
      "year": "2011",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "53",
      "title": "Arousal and locomotion make distinct contributions to cortical activity patterns and visual encoding",
      "authors": [
        "M Vinck",
        "R Batista-Brito",
        "U Knoblich",
        "J Cardin"
      ],
      "year": "2015",
      "venue": "Neuron"
    },
    {
      "citation_id": "54",
      "title": "Visual experience is necessary for the development of multisensory integration",
      "authors": [
        "M Wallace",
        "T Perrault",
        "W Hairston",
        "B Stein"
      ],
      "year": "2004",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "55",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Y Wang",
        "Y Shen",
        "Z Liu",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "56",
      "title": "Nonnegative decomposition of multivariate information. arXiv 2010",
      "authors": [
        "P Williams",
        "R Beer"
      ],
      "year": "2010",
      "venue": "Nonnegative decomposition of multivariate information. arXiv 2010",
      "arxiv": "arXiv:1004.2515"
    },
    {
      "citation_id": "57",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "M Wöllmer",
        "F Weninger",
        "T Knaup",
        "B Schuller",
        "C Sun",
        "K Sagae",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "58",
      "title": "A survey on multi-view learning",
      "authors": [
        "C Xu",
        "D Tao",
        "C Xu"
      ],
      "year": "2013",
      "venue": "A survey on multi-view learning",
      "arxiv": "arXiv:1304.5634"
    },
    {
      "citation_id": "59",
      "title": "Multi-view learning with incomplete views",
      "authors": [
        "C Xu",
        "D Tao",
        "C Xu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "60",
      "title": "A small world of neuronal synchrony",
      "authors": [
        "S Yu",
        "D Huang",
        "W Singer",
        "D Nikolić"
      ],
      "year": "2008",
      "venue": "Cerebral cortex"
    },
    {
      "citation_id": "61",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "62",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "63",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "64",
      "title": "Factorized multimodal transformer for multimodal sequential learning",
      "authors": [
        "A Zadeh",
        "C Mao",
        "K Shi",
        "Y Zhang",
        "P Liang",
        "S Poria",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Factorized multimodal transformer for multimodal sequential learning",
      "arxiv": "arXiv:1911.09826"
    },
    {
      "citation_id": "65",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "66",
      "title": "Improving Multimodal fusion via Dependency Optimization",
      "authors": [
        "P Colombo",
        "E Chapuis",
        "C Clavel"
      ],
      "year": "2020",
      "venue": "Improving Multimodal fusion via Dependency Optimization"
    },
    {
      "citation_id": "67",
      "title": "Factorized multimodal transformer for multimodal sequential learning",
      "authors": [
        "W Han",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Factorized multimodal transformer for multimodal sequential learning"
    }
  ]
}