{
  "paper_id": "2301.00985v2",
  "title": "Dfme: A New Benchmark For Dynamic Facial Micro-Expression Recognition",
  "published": "2023-01-03T07:33:33Z",
  "authors": [
    "Sirui Zhao",
    "Huaying Tang",
    "Xinglong Mao",
    "Shifeng Liu",
    "Yiming Zhang",
    "Hao Wang",
    "Tong Xu",
    "Enhong Chen"
  ],
  "keywords": [
    "Emotion recognition",
    "facial micro-expression",
    "facial action units",
    "micro-expression recognition",
    "databases"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "One of the most important subconscious reactions, micro-expression (ME), is a spontaneous, subtle, and transient facial expression that reveals human beings' genuine emotion. Therefore, automatically recognizing ME (MER) is becoming increasingly crucial in the field of affective computing, providing essential technical support for lie detection, clinical psychological diagnosis, and public safety. However, the ME data scarcity has severely hindered the development of advanced data-driven MER models. Despite the recent efforts by several spontaneous ME databases to alleviate this problem, there is still a lack of sufficient data. Hence, in this paper, we overcome the ME data scarcity problem by collecting and annotating a dynamic spontaneous ME database with the largest current ME data scale called DFME (Dynamic Facial Micro-expressions). Specifically, the DFME database contains 7,526 well-labeled ME videos spanning multiple high frame rates, elicited by 671 participants and annotated by more than 20 professional annotators over three years. Furthermore, we comprehensively verify the created DFME, including using influential spatiotemporal video feature learning models and MER models as baselines, and conduct emotion classification and ME action unit classification experiments. The experimental results demonstrate that the DFME database can facilitate research in automatic MER, and provide a new benchmark for this field. DFME will be published via https://mea-lab-421.github.io.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we first review the existing public spontaneous ME databases related to MER. Then, we mainly summarize some representative MER studies based on deep learning technologies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Micro-Expression Databases",
      "text": "The premise of obtaining an automatic MER algorithm with excellent performance is to hold a database with sufficient ME samples whose labels are credible and whose visual features are distinguishable. As an emerging field of affective computing, the number of ME databases is still relatively limited. Nevertheless, since more and more researchers have begun to pay attention to ME analysis, some high-quality databases are gradually springing up. Table  1  clearly summarizes the characteristics of these databases.\n\nAs the two earliest proposed ME databases, samples in the USF-HD  [34]  and Polikovsky  [35]  databases are all posed MEs. The participants were first required to watch video clips containing ME samples and then posed them by imitation. However, naturally generated MEs strongly correlate with emotions, while the posed ones are deliberately displayed and have nothing to do with the current emotional state of the participants. Consequently, these two databases are rarely used by researchers for ME analysis.\n\nThe subsequent researchers proposed to induce spontaneous MEs with the neutralization paradigm. Under this paradigm, several strong emotional stimuli were employed to elicit expressions. Participants were endowed with a certain degree of high-pressure mechanism, and instructed to keep a neutral face as much as possible. Databases adopting the neutralization paradigm include SMIC  [27] , CASME  [36] , CASME II  [28] , CAS(ME) 2  [37] , SAMM  [29] , MMEW  [24] , and 4DME  [38] , which are to be introduced in turn.\n\nSMIC database  [27]  is the first published spontaneous ME database, which consists of three parts: HS, VIS, and NIR. The HS part includes 164 ME samples from 16 participants, recorded by a high-speed camera with a frame rate of 100 frames per second (fps) and a resolution of 640×480. Both the VIS and NIR parts contain 71 ME samples from 8 individuals, while the former part was recorded using a standard visual camera and the latter using a near-infrared camera. Two annotators classified each ME into three emotion categories (positive, negative, and surprise) based on the participants' self-reports about the elicitation videos. Facial AUs were not annotated in SMIC.\n\nCASME series databases are released by the Institute of Psychology, Chinese Academy of Sciences. As the earliest database in this series, CASME  [36]  contains a total of 195 ME samples from 19 participants with a frame rate of 60fps. Two annotators labeled the facial AUs, together with the corresponding onset, apex, and offset frames of each ME sample frame by frame. According to the facial AUs, participants' self-reports, and the relevant video content, MEs were divided into eight emotion categories: amusement, sadness, disgust, surprise, contempt, fear, repression, and tense. CASME II  [28]  is an advanced version of CASME. First, the number of ME samples in CASME II has been expanded to 247 samples from 26 participants. Besides, CASME II provides a higher frame rate of 200fps and facial area resolution of 280×340 to capture more subtle changes in expressions. Five emotion categories were labeled in CASME II: happiness, disgust, surprise, repression, and others. The CAS(ME) 2 database  [37]  embodies two parts, both of which were collected at 30fps and 640×480 pixels. Different from all the other databases above, there are 87 long video clips containing both MaEs and MEs in the first part of CAS(ME) 2 , which can be used to promote the research of ME detection. The other part consists of 300 MaEs and 57 MEs, which were labeled with four emotion tags, including positive, negative, surprise, and others.\n\nSAMM database  [29]  has the highest resolution of all published spontaneous ME databases, which includes 159 ME samples generated by 32 participants, with a frame rate of 200fps and a resolution of 2040×1088. Different from other databases, to achieve a better elicitation effect, participants were asked to fill in a scale before the formal start of the collection, and then a series of stimulus videos were customized for each participant according to the scale. SAMM contains seven emotion categories: happiness, disgust, surprise, fear, anger, sadness, and others. Three coders annotated the AUs and key-frames in detail for each ME sample.\n\nMMEW database  [24]  consists of 300 ME and 900 MaE samples from 36 participants, which were collected with 90 fps and 1920×1080 resolution. Each expression sample is marked with seven emotion labels (the same as SAMM), AUs, and three key-frames. Compared with the previous databases, MMEW is more conducive to the models using the MaE samples under the same parameter setting and elicitated environment to assist in learning ME features.\n\nTo comprehensively capture the movement information of ME in all directions as much as possible, 4DME database  [38]  has made significant innovations in the recording method. Each ME sample in this database has multimodality video data, including 4D facial data reconstructed by 3D facial meshes sequences, traditional 2D frontal facial grayscale, RGB and depth videos. 4DME contains 267 MEs and 123 MaEs from 41 participants, thus 1,068 ME videos of four forms and 492 MaE videos in total. In addition, five emotion labels (positive, negative, surprise, repression, and others) were annotated based on facial AUs only, noting that each sample may have multiple emotion labels (up to two).\n\nUnlike databases with the neutralization paradigm, the MEVIEW database  [39]  consists of video clips of two real high-pressure scenes downloaded from the Internet. There The CAS(ME) 3  database  [30]  adopted the mock crime paradigm to elicit MEs with high ecological validity. However, unlike MEVIEW, the collection was still controlled in the laboratory environment, yielding 166 MEs and 347 MaEs. CAS(ME) 3 also contains two other parts: one consists of 943 MEs and 3,143 MaEs collected using the neutralization paradigm, respectively marked with AUs, key-frames, and seven emotion labels (the same as SAMM) for each sample; the other part contains 1,508 unlabeled long video clips, which can be used for the self-supervised learning task of ME detection and recognition. This database was collected at a frame rate of 30fps with a resolution of 1280×720.\n\nDespite more and more databases striving to record the movement characteristics of MEs more detailedly and comprehensively through various methods, these databases are still small-scale databases. In automatic ME analysis, models based on deep learning have become mainstream.\n\nHowever, due to the insufficient sample size, the complexity of the model can easily lead to overfitting in the training process  [40] ,  [41] ,  [42] . Though we can alleviate this problem by using data augmentation to increase the number of samples, many uncontrollable noises might be introduced. Some work has proposed using composite databases to train the model  [43] ,  [44] ,  [45] , but different databases have different parameter settings, and thus such a simple fusion is not reasonable. In addition, due to the short duration and low intensity of MEs, a higher frame rate may contribute to capturing more details. Nevertheless, the highest frame rate of all above databases is only 200fps, and most are less than 100fps. Therefore, it is necessary to establish a larger-scale ME database with a higher frame rate.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Micro-Expression Recognition Approaches",
      "text": "In the past decade, MER has gained increasing attention from researchers in affective computing and computer vision. The first attempt at automatic, spontaneous MER dates back to 2011, Pfister et al.  [16]  utilized a local binary pattern from three orthogonal planes (LBP-TOP) to explore MER on the first spontaneous ME database SMIC. Since then, there have been numerous efforts dedicated to developing automatic MER techniques. Generally, current MER methods can be broadly categorized into hand-crafted and deep learning methods. Typical hand-crafted ME features include LBP-TOP  [17] , HOOF  [46] , 3DHOG  [35] , and their variants  [18] ,  [19] ,  [47] ,  [48] ,  [49] . However, the hand-crafted methods heavily rely on complex expert knowledge, and the extracted ME features have limited discrimination. Current MER methods mainly employ deep neural networks for high-level expression feature learning and emotion classification, with a focus on addressing the challenges of subtle ME and insufficient ME data for model training. Furthermore, current deep learning MER methods can be divided into single frame-based and video sequence-based approaches based on whether they fully consider the temporal information of ME. In the following sections, we will categorize and summarize these two types of MER methods.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Single Frame-Based Mer Methods.",
      "text": "The single frame-based MER method typically utilizes only the highest intensity frame, i.e., the apex frame in terms of RGB or optical-flow format, from the ME video as input for neural networks to learn the ME features. After considering the challenge of lacking sufficient ME samples, Peng et al.  [50]  first chose ResNet-10  [51] , which was pre-trained on a large-scale image database, as the backbone, and then continued to fine-tune the classification network on large MaE samples for MER using only apex frames. Encouragingly, the recognition accuracy exceeds the hand-crafted methods based on LBP-TOP, HOOF, and 3DHOG. Inspired by the success of capsule models on image recognition, Quang et al.  [52]  proposed a CapsuleNet for MER using only apex frames. Recently, Xia et al.  [53]  proposed an expressionidentity disentangle network for MER by leveraging MaE databases as guidance. Li et al.  [54]  first spotted the apex frame by estimating pixel-level change rates in the frequency domain, and then proposed a joint feature learning architecture coupling local and global information from the detected apex frames to recognize MEs.\n\nAt the same time, Liong et al.  [55]  explored the effectiveness and superiority of using the optical flow of the apex frame in ME video. Inspired by this work, Liu et al.  [56]  first calculated the optical-flow image of the apex frame to the onset frame in the ME clips and then used the pre-trained ResNet-18 network to encode the opticalflow image for MER. In particular, they introduced domain adversarial training strategies to address the challenge of lacking large-scale ME data for training and won first place for MEGC2019. Furthermore, Zhou et al.  [57]  proposed a novel Feature Refinement (FR) with expression-specific feature learning and fusion for MER based on optical-flow information of apex frames. Gong et al.  [58]  proposed a meta-learning based multi-model fusion network for MER. Liu et al.  [59]  proposed a novel MER framework with a SqueezeNet  [60]  for spotting the apex frame and a 3D-CNN for recognition.\n\nOverall, single frame-based MER investigations are conducted on apex frames of ME videos, which can reduce the complexity of the used deep neural networks. Additionally, this method has the benefit of utilizing large-scale images for transfer learning to effectively address model overfitting due to insufficient ME data. However, it should be noted that single frame-based MER disregards the temporal information present in ME videos, which contains valuable clues and is a crucial feature that distinguishes MEs from MaEs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Video Sequence-Based Mer Methods.",
      "text": "Unlike the single frame-based MER, video sequence-based MER has the ability to learn spatiotemporal ME feature from the entire ME video or its subsequences. As a result, video sequence-based MER is often preferred over single framebased MER for capturing more detailed information about MEs. After fully considering the significant expression states in the ME video, Kim et al.  [61]  first used CNN to encode the spatial feature of each expression state (i.e., onset, onset to apex transition, apex, apex to offset transition and offset), then utilized LSTM to learn the temporal features based on the extracted spatial ME features. Wang et al.  [21]  proposed Transferring Long-term Convolutional Nerual Network (TLCNN) to solve the learning of spatial-temporal ME feature under small sample ME data. The TLCNN was also based on the CNN-LSTM structure and transferred knowledge from large-scale expression data and single frames of ME video clips. Khor et al.  [62]  proposed an Enriched Longterm Recurrent Convolutional Network (ELRCN) which made spatial and temporal enrichment by stacking different input data and features. Unlike the CNN-LSTM architecture, 3D convolution neural network (3D-CNN)  [63]  can simultaneously learn the spatial and temporal ME features. Based on 3D-CNN, Peng et al.  [64]  proposed a Dual Temporal Scale Convolutional Neural Network (DTSCNN), which used the optical-flow sequences of ME videos as model input to obtain high-level ME features and can adapt to a different frame rate of ME video clips. Wang et al.  [65]  proposed an MER framework based on Eulerian motion based 3D-CNN (EM-CED), which used the pre-extracted Eulerian motion feature maps as input and with a global attention module to encode rich spatiotemporal information. Xia et al.  [66]  proposed a deep recurrent convolutional networks based MER approach, which modeled the spatiotemporal ME deformations in views of facial appearance and geometry separately. To solve the challenge of extracting high-level ME features from the training model lacking sufficient and class-balanced ME samples, Zhao et al.  [22]  extracted the ME optical-flow sequence to express the original ME video and proposed a novel two-stage learning (i.e., prior learning and target learning) method based on a siamese 3D-CNN for MER. Sun et al.  [67]  proposed a knowledge transfer technique that distilled and transferred knowledge from AUs for MER based on crucial temporal sequences, where knowledge from a pre-trained deep teacher neural network was distilled and transferred to a shallow student neural network. Zhao et al.  [68]  proposed a deep prototypical learning framework on RGB key-frame sequences, namely ME-PLAN, based on a 3D residual prototypical network and a local-wise attention module for MER. Recently, with the advancement of deep learning technology, some excellent neural networks, such as GCN  [40] ,  [69] ,  [70] ,  [71]  and Transformers  [72] ,  [73] , have also been used for MER.\n\nAlthough video sequence-based MER leverages the spatial-temporal information of ME, the corresponding models tend to have higher structural complexity and are prone to overfit on current small-scale ME databases  [2] ,  [22] ,  [25] . Consequently, building a large-scale ME database remains a crucial task in developing an automatic MER system, as it serves as a fundamental component.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Dfme Database Profile",
      "text": "As the old saying goes, 'One cannot make bricks without straw'. To address the problem of ME data hunger, we construct a database of spontaneous ME with the largest sample size at present, called DFME. In the following subsections, we will elaborate on the building details and statistical properties of our DFME database.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Participant And Equipment",
      "text": "In our DFME, 671 participants were recruited (381 males and 290 females), mainly for college students and teaching staff. Participants were age-distributed between 17 and 40 years, with a mean age of 22.43 years (standard deviation = 2.54), and all from China. Before starting the formal experiment, the participants were informed about the purpose, experimental procedure, possible benefits and risks of our research. All studies involving human participants (which are all ordinary people not involving patients) adhered to the Declaration of Helsinki. Everybody participating in the experiment signed informed consent and chose whether to allow their facial images and videos used for the academic paper, ensuring ethical and responsible research practices.\n\nGiven the subtle nature and brief duration of MEs, the recording process is susceptible to disturbances from external factors. Therefore, we conducted the recording in a wellcontrolled laboratory environment, as depicted in Fig.  2 . Three LED lights equipped with reflector umbrellas were strategically positioned to ensure a consistently bright and stable light source illuminating the participants' faces during experiments. In addition, we employed a self-developed high-speed camera (1024×768, freely configurable frame rates) for capturing MEs, which was connected via a 10 Gigabit optical fiber transmission line to a 4T-sized highspeed acquisition memory, facilitating real-time storage of the collected ME video clips.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Elicitation Process",
      "text": "So far, there are three generations of ME-eliciting paradigms as outlined in  [30] . Although the third generation has the highest ecological validity, it is inevitable to interact and have conversations with the participants when simulating the natural scenes. These irrelevant body and mouth movements caused by speaking are also a kind of noise for MEs. Hence, we still employ the neutralization paradigm for ME elicitation to minimize noise interference and focus more on the movement characteristics of MEs. Specific details of the elicitation process will be introduced below.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Elicitation Materials",
      "text": "The effectiveness of elicitation materials determines the quantity and quality of MEs, so selecting the materials with high emotional valence is very crucial  [28] . The stimuli we used were all video clips from the Internet, ranging in length from 46 seconds to 258 seconds. In order to find more effective stimulus materials, we recruited 50 volunteers to evaluate 30 video clips collected previously. The evaluation process was as follows: after watching each video, volunteers were asked to choose only one emotion from happiness, anger, contempt, disgust, fear, sadness and surprise as the main emotion evoked by this video, and score the stimulus level on an integer scale of 1 to 5, corresponding to the intensity from mildest (but not None) to strongest. Finally, we took the emotion selected by more than half of the volunteers as the emotional class of each video. By ranking the average stimulus intensity values, we obtained the optimal 15 video clips as elicitation materials adopted in our experiment. Specific statistical details are shown in Table  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Elicitation And Collection Procedure",
      "text": "The collection took place in a meticulously arranged laboratory setting. Prior to start, each participant was seated at an assigned location. Through adjustments to the seat's height, the camera's focal length and the LED lamps' brightness, we ensured that the participant's face appeared utterly, clearly, and brightly in the centre of the screen. The monitor in front of the participant would play ten randomly selected elicitation videos (EVs) covering all seven discrete emotional types that had been previously verified effective in turn. The collector synchronously managed the recording of the participant's facial region through the high-speed camera to capture facial videos (FVs) containing ME fragments. While watching EVs, participants were required to maintain a neutral face as far as possible and control the occurrence of their facial expressions, alongside keeping an upright sitting posture, avoiding excessive head movements, and dedicating complete attention to the played EV. If they failed and repeatedly showed obvious expressions, they would have to complete a lengthy and tedious questionnaire as punishment.\n\nAfter watching each EV, participants would have a period of rest to ease their emotions. Meanwhile, they were instructed to fill in an affective grade scale according to the emotional experience generated just now, and form a selfreport detailing the timestamp of any observed expressions, emotion category and intensity. Specifically, the collector replayed from the beginning of the participant's FV latest collected and confirmed the timestamp t in the corresponding EV which coincided with the appearance of a facial movement segment. Once t was determined, the replay of FV was paused. Participants were inquired about their genuine psychological responses related to the EV within a 3-second time window centered around t, and noted down the timestamp t together with their corresponding emotions on the designated areas of the self-report scale. Then, the collector continued to replay the FV and repeated aforementioned steps, until the entire FV playback concluded, so that participants were allowed to view the next EV.\n\nDue to the existence of cognitive differences, the emotional orientation of the elicitation materials and the internal emotional experience of participants are sometimes not exactly consistent. What's more, external expressions of the same emotion are also diverse on account of individual differences. Therefore, it is worth noting that we specifically required participants to clarify their true internal emotions in their self-reports whenever facial expressions appear, which is vital in aiding subsequent annotators to comprehend the nuances of their MEs.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Me Annotation",
      "text": "Building the DFME database required a two-stage annotation: the sample selection stage as well as the coding and category labeling stage. In the first stage, we clipped short fragments containing valid expression samples from the collected long video sequences. The second stage included three successive rounds of fine-grained annotation, through which we confirmed all MEs and labeled their three keyframes (i.e., onset, apex, and offset frame), facial AUs, and emotion categories. Furthermore, we performed annotation agreement test to verify the reliability of emotion labels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sample Selection",
      "text": "In the sample selection stage, participants' FV sequences were manually segmented into several shorter video fragments, each capturing at least one ME or MaE.Using the self-developed video annotation software, an experienced annotator checked through the collected original FVs frame by frame to locate the fragments of facial muscle movements. With the guidance of the self-reports from participants, the annotator was able to effectively distinguish the facial expressions definitely related to emotion, and abandon interference data unrelated to emotion (such as violent blinking caused by dry eyelids, habitual mouth opening, etc.). Besides, we reserved some fragments with blinking or eye movements if they contained MaE or ME data.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Coding And Category Labeling",
      "text": "The apex frame corresponds to the moment when facial expression changes most dramatically. In the first round of the fine-grained annotation, five annotators independently marked out the onset, apex, and offset frame of each expression clip, and the median value of their annotation  In the second round of fine-grained annotation, we mainly annotated the AUs that occurred in MEs using the Facial Action Coding System (FACS)  [74] . There may exist a single AU such as AU4, or a combination of more different AUs like AU6+AU12 in an ME. When multiple categories of AUs appear, some obscure ones are easily overlooked. To enhance the reliability and integrity of the AU labels, two experienced FACS-certified annotators independently labeled the AUs for all the MEs identified previously. According to the actual induction of the participants during the experiments, and also referring to the AUs mainly involved in the previously published ME databases, we totally included 24 different categories of AUs for annotation. Of these AUs, six categories appear in the upper face, 13 in the lower face, and the other five belong to miscellaneous actions. Table  3  lists the specific AU numbers and their corresponding face actions. Since the manually annotated AU intensity is highly subjective, annotators merely indicated whether each AU appeared during the annotation rather than defining the intensity of its occurrence.\n\nAfter labeling the AUs, the two annotators determined the final AU label through crosscheck and discussion. The reliability R between the two annotators was 0.83, which was calculated as\n\nwhere AU (A 1 ) ∩ AU (A 2 ) means the number of AUs both annotators agreed, and All AU is the total number of AUs in an ME labeled out by the two annotators.\n\nIn the third round of fine-grained labeling, we performed the emotion labeling of MEs taking eight categories into account: happiness, anger, contempt, disgust, fear, sadness, surprise, and others. 'Others' represents MEs that are difficult to divide into the former seven prototypical emotion categories. Seven annotators independently gave the emotion labels of all MEs. When disagreements arised, a '50% majority voting' approach was employed, where a sample was assigned a specific emotion label if at least four annotators agreed on that label. For samples with unresolved disagreements, a second round of voting was conducted through collective discussions among all annotators to determine the emotion label. If a consensus still cannot be reached, the sample was categorized as 'Others'.\n\nIn previous spontaneous ME databases, the reference basis of emotion labeling was not precisely the same. In some databases, as represented by SMIC, emotion labels were determined based on self-reports provided by participants. Some other studies believed that seeing is believing, so their annotation was based on the correspondence between AUs and emotions. However, on the one hand, unlike MaEs, only part of the AUs can appear simultaneously in MEs due to their low intensity, and some AUs are shared by different emotion categories, which may lead to category confusion. On the other hand, we should not ignore the differences in self-emotional cognition of different participants, which means that the self-reports given for the whole piece of elicitation materials may be rough and inaccurate. Therefore, in DFME, the emotion labels were determined through a comprehensive analysis of facial AUs, self-reports of participants, and elicitation material contents, which is consistent with the method adopted by the CASME series. It is worth mentioning that we obtained the participants' fine-grained self-reports in the data collection process, and this is also the information that we recommend as a priority for reference when determining emotion labels. We matched the corresponding timestamps of MEs and elicitation materials through playback, enabling participants to report their emotions for each time of successful ME induction, which significantly improved the confidence of self-reports in emotion labeling. Fig.  3  shows some representative ME samples of seven discrete emotion categories in DFME.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "Annotation Agreement",
      "text": "Having reliable emotion categories of MEs is of vital significance for a database. In this section, we utilized Fleiss's Kappa test  [75]  to evaluate the quality of our emotion annotation encouraged by work  [76] . Fleiss's Kappa is a measure of the agreement among three or more annotators, testing the consistency of annotation results. Therefore, we consider Fleiss's Kappa as an excellent indicator to evaluate the reliability of emotion annotation.\n\nIn DFME, seven annotators independently labeled each ME sample based on facial AUs, an accurate self-report, and the corresponding elicitation material content. The samples were divided into eight emotion categories: {1: happiness, 2: anger, 3: contempt, 4: disgust, 5: fear, 6: sadness, 7: surprise, 8: others}. Let n = 7 represent the total number of annotation personnel, N indicate the total number of ME video clips, K = 8 represent the number of emotion categories. n ij is the number of annotators who assigned the i-th ME video clip to the j-th category, so we can calculate p j , the proportion of all assignments which were to the j-th emotion:\n\nThen, the extent of agreement among the n annotators for the i-th ME video clip indicated by P i is calculated. In other words, it can be indexed by the proportion of pairs agreeing in their evaluation of the i-th ME out of all the n(n -1) possible pairs of agreement:\n\nThe mean of P i is therefore:\n\nAnd we also have P e :\n\nFinally, we can calculate κ by:\n\nThus, we obtained κ = 0.72 through performing Fleiss's Kappa test in DFME. According to Table  4 , we know that all of our emotion annotators achieve substantial agreement, meaning that our emotion labels are quite reliable.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Statistical Properties Of Dfme",
      "text": "The DFME database consists of three parts: PART A, PART B, and PART C. The only difference between these three parts is the frame rate setting of the high-speed camera in the experiment. In PART A, all 1,118 ME samples from 72 participants have a frame rate of 500fps. The frame rate of PART B is 300fps with 969 ME samples from 92 participants. PART C has the most data size with 5,439 ME samples from 492 participants, whose frame rate is 200fps. Although we recruited a total of 671 participants, 15 of them had strong control over their facial expressions, from whom we could not collect any ME sample. Therefore, the final DFME database contains 7,526 ME samples from 656 participants, and we gave each sample an emotion category label as well as AU labels annotated according to FACS. Fig.  4  describes the distribution of ME samples in detail. According to the specific distribution of AU labels shown in Table  5 , the average number of AU's per ME sample occurrence can be calculated as 12928/7526 = 1.718.\n\nGiven that we have collected the fine-grained selfreports and the AU labels with considerable reliability, this fosters the exploration of the emotion-AU correspondence rule in MEs. Therefore, we counted the ratio of highoccurrence AUs in each emotion (Table  6 ), which reflects the existence preference of AU in MEs with different emotions, not affected by the emotional category imbalance problem in the database. We also matched the emotion and AU combinations according to the statistical results, and the conclusions are shown as Table  7 .\n\nBased on the statistical results presented in Table  6 , we have some findings to discuss:\n\n• In MaEs, AU9 (nose wrinkler) is highly associated with disgust, and AU20 (lip stretcher) is related to fear. These two AUs frequently appear in MaEs but are not easily induced in MEs. We ought not to conclude that these AUs' association with their corresponding emotions no longer exists in MEs. Instead, when participants tried to restrain their emotions, it was easier to control the movement of certain facial muscles such as AU9 and AU20 than others.\n\n• AU4 (brow lowerer), AU7 (lid tightener), and AU24 (lip presser) simultaneously occur at high frequency in different negative emotions (disgust, anger, fear, sadness, etc.). Without the assistance of participants' fine-grained self-reports, it is definitely challenging to distinguish MEs of negative emotions merely relying on these common AUs, which is also one of the reasons why some models excessively confuse the disgust MEs with those of other negative emotions in the seven-class classification automatic MER task.    AU4, AU4+AU7, AU7, AU24\n\n1 Shared: the AU combinations commonly appearing in Anger, Disgust, Fear and Sadness with high frequency.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "•",
      "text": "In the positive emotion (i.e., happiness), some AUs related to negative emotions can occur together with AU6 or AU12, specifically, including AU10 (associated with disgust), AU24 (associated with negative emotions), and Left/Right-AU12 (associated with contempt). The appearance of these extra AUs is a sign of participants trying to suppress their positive feelings, hide their smiles and twist their expressions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Database Evaluation",
      "text": "In this section, we conducted comprehensive experiments to verify the effectiveness of our DFME database for automatic MER and AU classification tasks, leveraging influential spatiotemporal feature learning and MER models. More specifically, MER involves assigning an emotion class label to a given ME video sample, which is a multi-classification task. On the other hand, AU classification aims to predict whether an AU exists in a video clip, corresponding to a single binary multi-label problem  [77] . These experiments can serve as a valuable reference for future research on ME analysis using the DFME database.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Evaluation Database",
      "text": "The DFME database is described in detail in Section 3. For the subsequent MER and AU classification verification, we combined 7, 275 samples with clear emotion and AU labels in PART A, B and C of DFME as our experimental database. The emotion labels include happiness, anger, contempt, disgust, fear, sadness and surprise. By drawing inspiration from CD6ME  [77] , we also selected the following 12 frequently occurring AUs: AU1, AU2, AU4, AU5, AU6, AU7, AU9, AU10, AU12, AU14, AU15, and AU17 as the AU labels for our experiment. In fact, these 12 AUs account for about 88.48% (11439/12928) of the DFME database, covering most AUs in MEs. Among the remaining AUs, most AUs are small in number, such as AU16 and AU18, while others like AU23 (Lip Tightener) are usually unconscious actions unrelated to emotions. For these reasons, we chose these 12 AUs for the experiments.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Data Preprocessing",
      "text": "In facial expression recognition, many variables, such as backgrounds and head poses, can affect the final recognition results. Therefore, before formally conducting automatic MER experiments, we need to preprocess all ME videos in the following steps (i.e., face alignment and face cropping) to minimize the influence of irrelevant variables.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Face Alignment",
      "text": "To eliminate variations in pose and angle among all ME samples, we need to perform face alignment. In this step, we took the following operations for each ME sample. Firstly, we selected a frontal face image as a reference and applied Style Aggregated Network (SAN)  [78]  to extract its facial landmarks. Subsequently, we employed Procrustes analysis  [79]  to compute an affine transformation based on the landmarks of the onset frame and those of the reference image. The rationale behind not using landmarks from all frames in the ME video is to avoid errors introduced by the calculation of landmarks and transformations that could significantly impact real MEs. Finally, the transformation was applied to each frame to align the faces. Additionally, some landmarks were situated in regions where MEs may appear, which may not be stable enough for alignment. Therefore, we excluded such landmarks during the alignment process.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Face Cropping",
      "text": "Since the movement of MEs is mainly in the facial area, face cropping is essential to eliminate biases caused by varying backgrounds. Following face alignment, we employed Reti-naFace  [80]  to crop the faces. Similar to face alignment, face cropping was based on the onset frame rather than each frame of a sample.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Evaluation Protocols And Metrics",
      "text": "Due to the small sample size of previous databases such as CASME II  [28] , SAMM  [29] , and SMIC  [27] , most MER studies employed the leave-one-subject-out strategy for evaluation. However, given the relatively large number of ME clips in DFME, this paper utilized a simpler and more efficient subject-independent 10-fold cross-validation strategy.\n\nIn each fold, data from 10% of subjects were selected as the test set, while the remaining 90% were used for training. Inline with CD6ME  [77] , we employed the F1-score as the evaluation metric for AU classification. Furthermore, three widely recognized ME classification metrics, namely Accuracy (ACC), Unweighted F1-Score (UF1), and Unweighted Average Recall (UAR), were utilized to assess the MER performance. Finally, we computed the average of ten folds' outcomes as the final result in the MER.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Accuracy (Acc)",
      "text": "ACC is one of the most common metrics, which can evaluate the overall performance of the recognition method on the database. It is calculated as follows:\n\nwhere K represents the number of the classes, N i stands for the sample number of the i-th class and T P i is the number of true positive samples of the i-th class.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "F1-Score (F1)",
      "text": "When the problem of class imbalance in the database is pronounced, ACC may not accurately reflect the model's true performance. As a result, F1-score is often employed as the evaluation metric in most classification tasks to address this challenge. The F1-score is defined as shown below:\n\nwhere T P , F P , F N refer to true positives, false positives and false negatives, respectively. Notably, T P , F P , F N are calculated based on all ten folds in the AU classification task.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Unweighted F1-Score (Uf1)",
      "text": "UF1, also known as macro-averaged F1-score, is defined as:\n\nClass imbalance is an intractable problem in the MER task, so introducing UF1 as an evaluation metric can better measure the method's performance in all classes rather than in some major classes.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Unweighted Average Recall (Uar)",
      "text": "UAR is also a more suitable metric than ACC when dealing with class imbalance.\n\nBoth UF1 and UAR can effectively assess whether MER methods provide accurate predictions across all classes.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Evaluation Baseline Models",
      "text": "To comprehensively validate our database, we specifically selected three different groups of baseline methods for MER and AU classification, including: 3D-CNN Methods, Handcrafted MER Methods and Deep learning MER Methods.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "3D-Cnn Methods",
      "text": "In recent years, many influential 3D-CNN methods have emerged in the field of video classification. Due to their excellent ability to represent spatiotemporal features, they are often used in dynamic ME analysis. Here, we selected 3D-ResNet (R3D)  [81]  and Inflated 3D ConvNet (I3D)  [82]  as two baseline methods. Hara et al. proposed R3D for tasks such as video classification and recognition. Since then, R3D is often used as the backbone in approaches to videorelated tasks. The basic idea of this model is to replace the 2D convolutional kernels with spatiotemporal 3D kernels according to the 2D-ResNet  [51]  network structure. I3D  [82]  is based on 2D ConvNet inflation. It utilizes convolutional kernels with different sizes to extract features, and the key idea behind the I3D model is to inflate the 2D filters and pooling kernels into 3D, allowing it to understand and process the spatial and temporal information of video data.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Hand-Crafted Mer Methods",
      "text": "The hand-crafted methods in MER are typically based on traditional machine learning, extracting elaborately designed manual features from ME videos. Hand-crafted methods were popular and achieved SOTA results on small ME databases in the early days. We selected LBP-TOP  [17]  and MDMO  [20]  as two baseline methods. LBP-TOP extends LBP from 2D to 3D, which extracts features from three orthogonal planes (X-Y, X-T, and Y-T) and connects them together. MDMO refers to the main directional mean optical flow features, which is a typical optical flow operator feature. It divides 36 regions of interest on the face and has good performance for subtle facial changes.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Deep Learning Mer Methods",
      "text": "In addition to hand-crafted methods, deep neural networks have been widely used in MER. Due to the scarcity of ME data, unlike general neural networks which have a deep and complex structure to better extract features, networks in MER prefer to reduce the number of parameters to prevent overfitting. We selected OffApexNet  [83] , STSTNet  [84] , RCN-A  [85] , MERSiam  [22]  and FR  [57]  as four baseline methods. Off-ApexNet calculates optical flow maps between two frames and uses them to represent the entire video. STSTNet designs a shallow triple stream three-dimensional CNN that is computationally light whilst capable of extracting discriminative high-level features and details of MEs. RCN-A is a recurrent convolutional network (RCN) to explore the shallower-architecture and lowerresolution input data, shrinking model and input complexities simultaneously. MERSiam  [22]  selects the optical flow sequences of ME videos as the model input and the method introduces a unique two-stage learning strategy, which includes prior-learning and target-learning stages. The main structure of MERSiam is built upon a Siamese 3D-CNN. FR aims to obtain salient and discriminative features for specific expressions and also predict expression by fusing the expression-specific features. It consists of an expression proposal module with the attention mechanism and a classification branch.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Evaluation Implementation Settings",
      "text": "All experiments were conducted on 2 NVIDIA GeForce RTX 3090 GPUs with 2 × 24 GB memory. Following the original settings, the length of ME clips was 16 frames for 3D-CNN methods and the length of optical flow sequences was 10 for MERSiam. The spatial sizes of each input image or optical feature map were 224×224 for R3D and I3D models, 28×28 for OffApexNet, STSTNet and FR, 60×60 for RCN-A and 112×112 for MERSiam.\n\nDuring training, cross-entropy loss and stochastic gradient descent (SGD) with a momentum of 0.9 were used to optimize the model parameters, and the batch size was set to 32 for 3D-CNN methods, 8 for MERSiam and 256 for other deep learning MER methods. Besides, other hyperparameters such as learning rate followed the settings in corresponding papers.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Evaluation Baseline Results",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Mer Results",
      "text": "To demonstrate the effectiveness of our DFME database for automatic MER tasks, we conducted a comprehensive MER experiment based on the above baseline methods. The results are shown in Table  8 , and the recognition confusion matrix of each baseline model is shown in Fig.  5 . Comparing the MER results among the three groups of baseline methods, it's evident that most deep learning MER approaches exhibit better recognition performance compared to the others. This is consistent with our expectations because deep learning MER methods not only leverage the strengths of deep learning but also specifically design modules and introduce prior knowledge for ME features. Surprisingly, typical video understanding models like I3D have also achieved competitive recognition performance, surpassing two influential hand-crafted MER approaches. This reflects that by utilizing the DFME database we built, it's possible to train general video understanding models to acquire the ability for perceiving ME features.\n\nFurthermore, by observing the confusion matrices in Fig.  5 , we also discovered significant variations in the models' perception abilities for different categories of MEs. Taking FR as an example, it demonstrated higher recognition accuracy for disgust, happiness, and surprise MEs, reaching 76.58%, 64.92%, and 79.97%, respectively. However, the recognition accuracy for the remaining four ME categories were all below 15%. The primary reasons contributing to this discrepancy can be identified as follows: 1) Negative emotions like anger, contempt, disgust, fear, and sadness exhibit significant similarities in ME. Referring to",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Au Classification Results",
      "text": "AU classification is crucial for analyzing facial behaviors.\n\nIn this experiment, we conducted AU classification experiments on the DFME database using the aforementioned baseline methods. The experimental results are shown in Table  9 . It is evident that deep learning methods still achieve competitive results, especially FR method which performs best on this task. In addition, due to the large differences in the distribution of each AU in DFME, there are also significant differences in the classification performance of models for them. Overall, these models exhibit the best classification performance on common AUs such as AU1, AU2, and AU4. However, they tend to have a lower classification accuracy for low-frequency AU9 and AU15.   [81]  0.6061 0.5903 0.6967 0.4931 0.1878 0.3305 0.0000 0.0000 0.3102 0.0159 0.0000 0.0000 0.2692 I3D  [82]  0.7517 0.7372 0.8082 0.6103 0.3221 0.4156 0.0000 0.1561 0.4697 0.3880 0.0000 0.3935 0.4210 LBP-TOP  [17]  0.6282 0.5845 0.8297 0.3931 0.0722 0.1171 0.0000 0.0000 0.4007 0.0166 0.0000 0.0000 0.2535 MDMO  [20]  0.5864 0.5783 0.7510 0.3736 0.0770 0.2079 0.0000 0.0082 0.2800 0.0897 0.0000 0.1675 0.2600 OffApexNet  [83]  0.2973 0.0201 0.4367 0.0000 0.0000 0.0326 0.0229 0.1051 0.2805 0.0254 0.0410 0.0098 0.1060 STSTNet  [84]  0.7256 0.6389 0.8833 0.3934 0.0567 0.1664 0.0000 0.0000 0.2654 0.0316 0.0000 0.0309 0.2660 RCN-A  [85]  0.8120 0.8046 0.9076 0.5990 0.3478 0.4946 0.0588 0.1281 0.5157 0.1855 0.0000 0.2725 0.4272 MERSiam  [22]  0.7710 0.7554 0.7306 0.5615 0.1383 0.2059 0.0000 0.1491 0.1774 0.0283 0.0000 0.3621 0.3233 FR  [57]  0.8792 0.8837 0.9367 0.6971 0.4352 0.6217 0.0678 0.3373 0.7163 0.4638 0.3979 0.6605 0.5914",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: compares an ME and an MaE with the same emotion",
      "page": 1
    },
    {
      "caption": "Figure 1: Examples of MaE and ME from the same person with a timeline in seconds, both belong to the “Happiness” emotion",
      "page": 2
    },
    {
      "caption": "Figure 2: Three LED lights equipped with reflector umbrellas were",
      "page": 6
    },
    {
      "caption": "Figure 2: Experimental environment for eliciting MEs",
      "page": 7
    },
    {
      "caption": "Figure 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME",
      "page": 8
    },
    {
      "caption": "Figure 3: shows some representative ME",
      "page": 9
    },
    {
      "caption": "Figure 4: describes",
      "page": 9
    },
    {
      "caption": "Figure 4: Distribution of ME Samples in DFME. Each column represents the total sample number of an emotion category, and",
      "page": 10
    },
    {
      "caption": "Figure 5: Comparing",
      "page": 12
    },
    {
      "caption": "Figure 5: , we also discovered significant variations in the mod-",
      "page": 12
    },
    {
      "caption": "Figure 5: Confusion matrices of baseline methods including 3D-CNN, hand-crafted MER and deep learning MER methods.",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "and Enhong Chen, Fellow,\nIEEE"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "the most\nimportant subconscious reactions, micro-expression (ME),\nis a spontaneous, subtle, and transient facial\nAbstract—One of"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "expression that reveals human beings’ genuine emotion. Therefore, automatically recognizing ME (MER) is becoming increasingly"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "crucial\nin the field of affective computing, providing essential\ntechnical support\nfor lie detection, clinical psychological diagnosis, and"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "public safety. However, the ME data scarcity has severely hindered the development of advanced data-driven MER models. Despite the"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "recent efforts by several spontaneous ME databases to alleviate this problem, there is still a lack of sufficient data. Hence, in this paper,"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "we overcome the ME data scarcity problem by collecting and annotating a dynamic spontaneous ME database with the largest current"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "ME data scale called DFME (Dynamic Facial Micro-expressions). Specifically,\nthe DFME database contains 7,526 well-labeled ME"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "videos spanning multiple high frame rates, elicited by 671 participants and annotated by more than 20 professional annotators over"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "three years. Furthermore, we comprehensively verify the created DFME,\nincluding using influential spatiotemporal video feature"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "learning models and MER models as baselines, and conduct emotion classification and ME action unit classification experiments. The"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "experimental results demonstrate that\nthe DFME database can facilitate research in automatic MER, and provide a new benchmark for"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "this field. DFME will be published via https://mea-lab-421.github.io."
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "facial micro-expression,\nfacial action units, micro-expression recognition, databases\nIndex Terms—Emotion recognition,"
        },
        {
          "Sirui Zhao, Huaying Tang, Xinglong Mao, Shifeng Liu, Yiming Zhang, Hao Wang, Tong Xu, Member,\nIEEE,": "✦"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "second": "0.36"
        },
        {
          "second": "(b) An example of ME with “Happiness” emotion."
        },
        {
          "second": ""
        },
        {
          "second": "frame denote the start and end time of an expression respectively,"
        },
        {
          "second": ""
        },
        {
          "second": ""
        },
        {
          "second": ""
        },
        {
          "second": "dundant, so labeling ME clips is extremely time-consuming"
        },
        {
          "second": "and labor-intensive. Meanwhile, some studies [31], [32], [32],"
        },
        {
          "second": "have also attempted to expand the existing ME sample size"
        },
        {
          "second": "by using generative methods. Specifically, by utilizing gen-"
        },
        {
          "second": "erative models and existing ME videos, researchers induce"
        },
        {
          "second": "deformations in easily obtainable facial images to create new"
        },
        {
          "second": "ME videos. For instance, Xu et al. [32] introduced Generative"
        },
        {
          "second": "Adversarial Network based on fine-grained facial\naction"
        },
        {
          "second": "units (AUs) modulation to generate ME sequences. Zhao et"
        },
        {
          "second": "al.\n[33]\ncombined a motion estimation network based on"
        },
        {
          "second": "Thin-Plate Spline with a generation network constrained"
        },
        {
          "second": "by relative AUs\nto accurately generate fine-grained MEs."
        },
        {
          "second": "Though ME generation techniques partially alleviate\nthe"
        },
        {
          "second": "scarcity of existing ME samples, the quality of generated ME"
        },
        {
          "second": "videos and the diversity of movements still cannot replace"
        },
        {
          "second": "authentically captured MEs."
        },
        {
          "second": ""
        },
        {
          "second": "In order to overcome the ME data shortage bottleneck,"
        },
        {
          "second": ""
        },
        {
          "second": "this paper collects and annotates the currently largest ME"
        },
        {
          "second": "database called DFME (Dynamic Facial Micro-expressions)"
        },
        {
          "second": "to\nadvance\nthe\ndevelopment\nof MER.\nSpecifically,\nthe"
        },
        {
          "second": "proposed DFME database contains 7,526 well-labeled ME"
        },
        {
          "second": "200fps,\nvideos\nspanning multiple\nhigh\nframe\nrates,\ni.e.,"
        },
        {
          "second": "300fps, 500fps, primarily annotated with seven discrete emo-"
        },
        {
          "second": "tion labels, i.e., happiness, anger, contempt, disgust, fear, sadness"
        },
        {
          "second": "and surprise, along with 24 facial AU labels listed in Table 3."
        },
        {
          "second": "In particular, our ME videos are produced by conducting"
        },
        {
          "second": "video emotional stimulation on 671 participants while sup-"
        },
        {
          "second": "pressing emotions as much as possible, and are repeatedly"
        },
        {
          "second": "annotated by more\nthan 20 professional\nannotators over"
        },
        {
          "second": "three years. The well-labeled ME samples with cropped"
        },
        {
          "second": "regions of the facial\nimages will be publicly available to the"
        },
        {
          "second": "MER research community. In addition, we comprehensively"
        },
        {
          "second": "validate the created DFME database,\nincluding developing"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: clearly sum- from other databases, to achieve a better elicitation effect,",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "a new database validation strategy and reproducing several",
          "3": "participants’ self-reports about the elicitation videos. Facial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "influential\nspatiotemporal video\nfeature\nlearning models",
          "3": "AUs were not annotated in SMIC."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "and MER models to conduct basic emotion classification and",
          "3": "CASME series databases are released by the Institute of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME action unit classification experiments. The experimental",
          "3": "Psychology, Chinese Academy of Sciences. As the earliest"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "results demonstrate that\nthe DFME database can facilitate",
          "3": "database\nin this\nseries, CASME [36]\ncontains\na\ntotal\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "research in automatic MER, and provide a new benchmark",
          "3": "195 ME samples from 19 participants with a frame rate of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "for this field.",
          "3": "60fps. Two annotators labeled the facial AUs, together with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "The\nrest of\nthis paper\nis organized as\nfollows. First,",
          "3": "the corresponding onset, apex, and offset\nframes of each"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "we summarize currently existing ME databases and review",
          "3": "ME sample frame by frame. According to the facial AUs,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "related work on MER in the next section.\nIn section 3, we",
          "3": "participants’\nself-reports,\nand the\nrelevant video content,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "elaborate on the building details and statistical properties",
          "3": "amuse-\nMEs were divided into eight\nemotion categories:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "of our DFME database. Then the comprehensive database",
          "3": "ment, sadness, disgust, surprise, contempt,\nfear, repression, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "evaluation is developed and discussed in Section 4. Finally,",
          "3": "tense. CASME II\n[28]\nis an advanced version of CASME."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "research\nconclusions\nand future work\nare\naddressed in",
          "3": "First,\nthe number of ME samples\nin CASME II has been"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Section 5.",
          "3": "expanded to\n247\nsamples\nfrom 26 participants. Besides,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "CASME II provides a higher frame rate of 200fps and facial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "area resolution of 280×340 to capture more subtle changes"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2\nRELATED WORK",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "in expressions.\nFive\nemotion categories were\nlabeled in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "In this\nsection, we first\nreview the\nexisting public\nspon-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "CASME II: happiness, disgust, surprise, repression, and others."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "taneous ME databases\nrelated to MER. Then, we mainly",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "The CAS(ME)2 database [37] embodies two parts, both of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "summarize some representative MER studies based on deep",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "which were collected at 30fps and 640×480 pixels. Different"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "learning technologies.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "from all\nthe other databases above,\nthere are 87 long video"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "clips\ncontaining both MaEs and MEs\nin the first part of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2.1\nMicro-expression Databases",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "3": "CAS(ME)2, which can be used to promote the research of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "The premise of obtaining an automatic MER algorithm with",
          "3": "ME detection. The other part consists of 300 MaEs and 57"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "excellent performance is to hold a database with sufficient",
          "3": "MEs, which were labeled with four emotion tags, including"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME samples whose labels are credible and whose visual fea-",
          "3": "positive, negative, surprise, and others."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "tures are distinguishable. As an emerging field of affective",
          "3": "SAMM database [29] has\nthe highest\nresolution of all"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "computing,\nthe number of ME databases is still\nrelatively",
          "3": "published spontaneous ME databases, which includes 159"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "limited. Nevertheless, since more and more researchers have",
          "3": "ME samples generated by 32 participants, with a\nframe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "begun to pay attention to ME analysis, some high-quality",
          "3": "rate\nof\n200fps\nand a\nresolution of\n2040×1088. Different"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "databases are gradually springing up. Table 1 clearly sum-",
          "3": "from other databases,\nto achieve a better elicitation effect,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "marizes the characteristics of these databases.",
          "3": "participants were asked to fill\nin a scale before the formal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "As\nthe\ntwo earliest proposed ME databases,\nsamples",
          "3": "start of\nthe collection, and then a series of stimulus videos"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "in the USF-HD [34] and Polikovsky [35] databases are all",
          "3": "were customized for each participant according to the scale."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "posed MEs. The participants were first\nrequired to watch",
          "3": "SAMM contains seven emotion categories: happiness, disgust,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "video clips containing ME samples and then posed them",
          "3": "surprise,\nfear, anger, sadness, and others. Three coders anno-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "by imitation. However, naturally generated MEs\nstrongly",
          "3": "tated the AUs and key-frames in detail for each ME sample."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "correlate with emotions, while the posed ones are deliber-",
          "3": "MMEW database [24] consists of 300 ME and 900 MaE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ately displayed and have nothing to do with the current",
          "3": "samples from 36 participants, which were collected with 90"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "emotional state of the participants. Consequently, these two",
          "3": "fps and 1920×1080 resolution. Each expression sample is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "databases are rarely used by researchers for ME analysis.",
          "3": "marked with seven emotion labels\n(the same as SAMM),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "The subsequent researchers proposed to induce sponta-",
          "3": "AUs, and three key-frames. Compared with the previous"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "neous MEs with the neutralization paradigm. Under\nthis",
          "3": "databases, MMEW is more conducive to the models using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "paradigm, several strong emotional stimuli were employed",
          "3": "the MaE samples under\nthe\nsame parameter\nsetting and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "to elicit expressions. Participants were endowed with a cer-",
          "3": "elicitated environment to assist in learning ME features."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "tain degree of high-pressure mechanism, and instructed to",
          "3": "To\ncomprehensively\ncapture\nthe movement\ninforma-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "keep a neutral face as much as possible. Databases adopting",
          "3": "tion of ME in all directions as much as possible, 4DME"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the neutralization paradigm include\nSMIC [27], CASME",
          "3": "database [38] has made significant innovations in the record-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[36], CASME II\n[28], CAS(ME)2\n[37], SAMM [29], MMEW",
          "3": "ing method. Each ME sample in this database has multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[24], and 4DME [38], which are to be introduced in turn.",
          "3": "modality video data, including 4D facial data reconstructed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "SMIC database [27]\nis\nthe first published spontaneous",
          "3": "by 3D facial meshes sequences, traditional 2D frontal facial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME database, which consists of\nthree parts: HS, VIS, and",
          "3": "grayscale, RGB and depth videos. 4DME contains 267 MEs"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "NIR. The HS part includes 164 ME samples from 16 partic-",
          "3": "and 123 MaEs from 41 participants,\nthus 1,068 ME videos"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ipants, recorded by a high-speed camera with a frame rate",
          "3": "of\nfour\nforms and 492 MaE videos\nin total.\nIn addition,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "of 100 frames per second (fps) and a resolution of 640×480.",
          "3": "five emotion labels (positive, negative, surprise, repression, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Both the VIS and NIR parts contain 71 ME samples from",
          "3": "others) were annotated based on facial AUs only, noting that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "8 individuals, while the former part was recorded using a",
          "3": "each sample may have multiple emotion labels (up to two)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "standard visual camera and the latter using a near-infrared",
          "3": "Unlike databases with the neutralization paradigm,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "camera. Two annotators classified each ME into three emo-",
          "3": "MEVIEW database [39] consists of video clips of\ntwo real"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "tion categories (positive, negative, and surprise) based on the",
          "3": "high-pressure scenes downloaded from the Internet. There"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "Participants"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "Gender"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "(Male/Female)"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "10/6"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "22/13"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "/"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "9/13"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "16/16"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "/"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "/"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "50/50"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "9/22"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "38/27"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "31/41"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "61/31"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": "282/210"
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        },
        {
          "TABLE 1: Statistical Information of Current Spontaneous ME Databases": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": ""
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "of 943 MEs and 3,143 MaEs collected using the neutraliza-"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "tion paradigm, respectively marked with AUs, key-frames,"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "and seven emotion labels (the same as SAMM) for each sam-"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "ple; the other part contains 1,508 unlabeled long video clips,"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "which can be used for the self-supervised learning task of"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "ME detection and recognition. This database was collected"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "at a frame rate of 30fps with a resolution of 1280×720."
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "Despite more\nand more databases\nstriving\nto\nrecord"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "the movement characteristics of MEs more detailedly and"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "comprehensively through various methods, these databases"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "are\nstill\nsmall-scale databases.\nIn automatic ME analysis,"
        },
        {
          "Con: Contempt; Unc: Unclear; Oth: Others; PosSur: Positively surprise; NegSur: Negatively surprise; RepSur: Repressively surprise; PosRep:": "models based on deep learning have become mainstream."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "However, due to the insufficient sample size, the complexity",
          "5": "At\nthe same time, Liong et al.\n[55] explored the effec-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "of\nthe model can easily lead to overfitting in the training",
          "5": "tiveness and superiority of using the optical flow of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "process [40], [41], [42]. Though we can alleviate this problem",
          "5": "apex\nframe\nin ME video.\nInspired by this work, Liu et"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "by using data\naugmentation to\nincrease\nthe number\nof",
          "5": "al.\n[56] first calculated the optical-flow image of\nthe apex"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "samples, many uncontrollable noises might be introduced.",
          "5": "frame to the onset\nframe in the ME clips and then used"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Some work has proposed using composite databases\nto",
          "5": "the pre-trained ResNet-18 network to encode the optical-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "train the model [43], [44], [45], but different databases have",
          "5": "flow image for MER. In particular, they introduced domain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "different parameter settings, and thus such a simple fusion",
          "5": "adversarial\ntraining strategies\nto address\nthe challenge of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "is not reasonable. In addition, due to the short duration and",
          "5": "lacking large-scale ME data for training and won first place"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "low intensity of MEs, a higher frame rate may contribute to",
          "5": "for MEGC2019.\nFurthermore, Zhou et\nal.\n[57] proposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "capturing more details. Nevertheless, the highest frame rate",
          "5": "a novel Feature Refinement\n(FR) with expression-specific"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "of all above databases is only 200fps, and most are less than",
          "5": "feature learning and fusion for MER based on optical-flow"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "100fps. Therefore,\nit\nis necessary to establish a larger-scale",
          "5": "information of apex frames. Gong et al.\n[58] proposed a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME database with a higher frame rate.",
          "5": "meta-learning based multi-model fusion network for MER."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "Liu et al.\n[59] proposed a novel MER framework with a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2.2\nMicro-expression Recognition Approaches",
          "5": "SqueezeNet [60] for spotting the apex frame and a 3D-CNN"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "In the past decade, MER has gained increasing attention",
          "5": "for recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "Overall, single frame-based MER investigations are con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "from researchers in affective computing and computer vi-",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "ducted on apex frames of ME videos, which can reduce the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "sion. The first attempt at automatic, spontaneous MER dates",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "complexity of the used deep neural networks. Additionally,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "back to 2011, Pfister et al. [16] utilized a local binary pattern",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "this method has the benefit of utilizing large-scale images"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "from three orthogonal planes\n(LBP-TOP)\nto explore MER",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "for transfer learning to effectively address model overfitting"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "on the first\nspontaneous ME database SMIC. Since\nthen,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "due to insufficient ME data. However,\nit should be noted"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "there have been numerous\nefforts dedicated to develop-",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "that single frame-based MER disregards the temporal infor-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ing\nautomatic MER techniques. Generally,\ncurrent MER",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "mation present in ME videos, which contains valuable clues"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "methods can be broadly categorized into hand-crafted and",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "and is a crucial feature that distinguishes MEs from MaEs."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "deep learning methods. Typical hand-crafted ME features",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "include LBP-TOP [17], HOOF [46], 3DHOG [35], and their",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "2.2.2\nVideo sequence-based MER methods."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "variants [18], [19], [47], [48], [49]. However, the hand-crafted",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "methods heavily rely on complex expert knowledge, and the",
          "5": "Unlike the single frame-based MER, video sequence-based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "extracted ME features have limited discrimination. Current",
          "5": "MER has the ability to learn spatiotemporal ME feature from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "MER methods mainly employ deep neural networks\nfor",
          "5": "the entire ME video or its subsequences. As a result, video"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "high-level\nexpression feature\nlearning and emotion clas-",
          "5": "sequence-based MER is often preferred over single frame-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "sification, with a\nfocus\non addressing\nthe\nchallenges\nof",
          "5": "based MER for capturing more detailed information about"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "subtle ME and insufficient ME data\nfor model\ntraining.",
          "5": "MEs. After fully considering the significant expression states"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Furthermore, current deep learning MER methods can be",
          "5": "in the ME video, Kim et al. [61] first used CNN to encode the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "divided into single frame-based and video sequence-based",
          "5": "spatial\nfeature of each expression state (i.e., onset, onset\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "approaches based on whether they fully consider the tem-",
          "5": "apex transition, apex, apex to offset\ntransition and offset),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "poral\ninformation of ME. In the following sections, we will",
          "5": "then utilized LSTM to learn the\ntemporal\nfeatures based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "categorize and summarize these two types of MER methods.",
          "5": "on the extracted spatial ME features. Wang et al.\n[21] pro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "posed Transferring Long-term Convolutional Nerual Net-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2.2.1\nSingle frame-based MER methods.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "5": "work (TLCNN) to solve the learning of spatial-temporal ME"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "The single frame-based MER method typically utilizes only",
          "5": "feature under small sample ME data. The TLCNN was also"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the highest\nintensity frame,\ni.e.,\nthe apex frame in terms of",
          "5": "based on the CNN-LSTM structure and transferred knowl-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "RGB or optical-flow format, from the ME video as input for",
          "5": "edge from large-scale expression data and single frames of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "neural networks to learn the ME features. After considering",
          "5": "ME video clips. Khor et al. [62] proposed an Enriched Long-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the\nchallenge of\nlacking sufficient ME samples, Peng et",
          "5": "term Recurrent Convolutional Network\n(ELRCN) which"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "al. [50] first chose ResNet-10 [51], which was pre-trained on",
          "5": "made spatial and temporal enrichment by stacking different"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "a large-scale image database, as the backbone, and then con-",
          "5": "input data and features. Unlike the CNN-LSTM architecture,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "tinued to fine-tune the classification network on large MaE",
          "5": "3D convolution neural network (3D-CNN) [63] can simulta-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "samples\nfor MER using only apex frames. Encouragingly,",
          "5": "neously learn the spatial and temporal ME features. Based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the recognition accuracy exceeds the hand-crafted methods",
          "5": "on 3D-CNN, Peng et al. [64] proposed a Dual Temporal Scale"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "based on LBP-TOP, HOOF, and 3DHOG.\nInspired by the",
          "5": "Convolutional Neural Network (DTSCNN), which used the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "success of capsule models on image recognition, Quang et",
          "5": "optical-flow sequences\nof ME videos\nas model\ninput\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "al.\n[52] proposed a CapsuleNet\nfor MER using only apex",
          "5": "obtain high-level ME features and can adapt\nto a different"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "frames. Recently, Xia\net\nal.\n[53] proposed an expression-",
          "5": "frame rate of ME video clips. Wang et al.\n[65] proposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "identity disentangle network for MER by leveraging MaE",
          "5": "an MER framework based on Eulerian motion based 3D-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "databases as guidance. Li et al.\n[54] first spotted the apex",
          "5": "CNN (EM-CED), which used the pre-extracted Eulerian"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "frame\nby\nestimating pixel-level\nchange\nrates\nin the\nfre-",
          "5": "motion feature maps as input and with a global attention"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "quency domain, and then proposed a joint feature learning",
          "5": "module to encode rich spatiotemporal\ninformation. Xia et"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "architecture coupling local and global\ninformation from the",
          "5": "al.\n[66] proposed a deep recurrent convolutional networks"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "detected apex frames to recognize MEs.",
          "5": "based MER approach, which modeled the spatiotemporal"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "TABLE 2: Video clips for eliciting MEs"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME deformations in views of facial appearance and geome-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "try separately. To solve the challenge of extracting high-level",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "Video ID\nDuring Time\nEmotion Category\nMean Score(1-5)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME features from the training model\nlacking sufficient and",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "02sa\n3’44”\nSadness\n4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "class-balanced ME samples, Zhao et al.\n[22] extracted the",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "03sa\n4’18”\nSadness\n3.36"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME optical-flow sequence to express the original ME video",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "06c\n2’01”\nContempt\n2.83"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "and proposed a novel two-stage learning (i.e., prior learning",
          "6": "07a\n1’26”\nAnger\n3.49"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "and target\nlearning) method based on a siamese 3D-CNN",
          "6": "08su\n1’26”\nSurprise\n2.16"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "09f\n2’22”\nFear\n3.72"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "for MER. Sun et\nal.\n[67] proposed a knowledge\ntransfer",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "10a\n2’58”\nAnger\n4.33"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "technique\nthat distilled and transferred knowledge\nfrom",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "11d\n1’24”\nDisgust\n3.95"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AUs for MER based on crucial\ntemporal sequences, where",
          "6": "13f\n2’14”\nFear\n3.36"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "14d\n1’22”\nDisgust\n3.23"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "knowledge from a pre-trained deep teacher neural network",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "17h\n1’17”\nHappiness\n2.81"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "was distilled and transferred to a shallow student neural",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "18h\n1’58”\nHappiness\n3.08"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "network. Zhao\net\nal.\n[68] proposed a deep prototypical",
          "6": "20d\n0’46”\nDisgust\n2.87"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "learning framework on RGB key-frame sequences, namely",
          "6": "21c\n1’44”\nContempt\n2.11"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "23sa\n1’44”\nSadness\n3.25"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME-PLAN, based on a 3D residual prototypical network and",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "a local-wise attention module for MER. Recently, with the",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "advancement of deep learning technology,\nsome excellent",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "Gigabit optical fiber\ntransmission line to a 4T-sized high-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "neural networks,\nsuch as GCN [40],\n[69],\n[70],\n[71]\nand",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "speed acquisition memory,\nfacilitating real-time storage of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Transformers\n[72],\n[73], have also been used for MER.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "the collected ME video clips."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Although\nvideo\nsequence-based MER\nleverages\nthe",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "spatial-temporal\ninformation\nof ME,\nthe\ncorresponding",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "models tend to have higher structural complexity and are",
          "6": "3.2\nElicitation Process"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "prone to overfit on current\nsmall-scale ME databases\n[2],",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "So far, there are three generations of ME-eliciting paradigms"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[22], [25]. Consequently, building a large-scale ME database",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "as outlined in [30]. Although the third generation has the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "remains\na\ncrucial\ntask in developing an automatic MER",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "highest ecological validity,\nit\nis\ninevitable to interact and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "system, as it serves as a fundamental component.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "have conversations with the participants when simulating"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "the natural scenes. These irrelevant body and mouth move-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "ments caused by speaking are also a kind of noise for MEs."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "3\nDFME DATABASE PROFILE",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "Hence, we still employ the neutralization paradigm for ME"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "As the old saying goes,\n’One cannot make bricks without",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "elicitation to minimize noise interference and focus more on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "straw’. To address the problem of ME data hunger, we con-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "the movement characteristics of MEs. Specific details of the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "struct a database of spontaneous ME with the largest sample",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "elicitation process will be introduced below."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "size at present, called DFME.\nIn the following subsections,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "we will\nelaborate\non the building details\nand statistical",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "3.2.1\nElicitation Materials"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "properties of our DFME database.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "The\neffectiveness\nof\nelicitation materials determines\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "quantity and quality of MEs, so selecting the materials with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "3.1\nParticipant and Equipment",
          "6": "high emotional valence is very crucial\n[28]. The stimuli we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "used were all video clips from the Internet, ranging in length"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "In our DFME, 671 participants were recruited (381 males",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "from 46\nseconds\nto\n258\nseconds.\nIn order\nto find more"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "and 290 females), mainly for college students and teaching",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "effective stimulus materials, we recruited 50 volunteers to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "staff. Participants were age-distributed between 17 and 40",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "evaluate 30 video clips collected previously. The evaluation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "years, with a mean age of 22.43 years (standard deviation =",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "process was as follows: after watching each video, volun-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2.54), and all\nfrom China. Before starting the formal exper-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "teers were asked to choose only one emotion from happiness,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "iment,\nthe participants were informed about\nthe purpose,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "anger, contempt, disgust,\nfear, sadness and surprise as the main"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "experimental procedure, possible benefits and risks of our",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "emotion evoked by this video, and score the stimulus level"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "research. All studies involving human participants (which",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "on an integer scale of 1 to 5, corresponding to the intensity"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "are all ordinary people not\ninvolving patients) adhered to",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "from mildest\n(but not None)\nto strongest. Finally, we took"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the Declaration of Helsinki. Everybody participating in the",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "the emotion selected by more than half of the volunteers as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "experiment signed informed consent and chose whether to",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "the emotional class of each video. By ranking the average"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "allow their facial\nimages and videos used for the academic",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "stimulus intensity values, we obtained the optimal 15 video"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "paper, ensuring ethical and responsible research practices.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "clips\nas\nelicitation materials\nadopted in our\nexperiment."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Given the subtle nature and brief duration of MEs,\nthe",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "Specific statistical details are shown in Table 2."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "recording process is susceptible to disturbances from exter-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "nal factors. Therefore, we conducted the recording in a well-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "6": "3.2.2\nElicitation and Collection Procedure"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "controlled laboratory environment, as depicted in Fig. 2.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Three LED lights equipped with reflector umbrellas were",
          "6": "The collection took place in a meticulously arranged labora-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "strategically positioned to ensure a consistently bright and",
          "6": "tory setting. Prior to start, each participant was seated at an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "stable light source illuminating the participants’\nfaces dur-",
          "6": "assigned location. Through adjustments to the seat’s height,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ing experiments. In addition, we employed a self-developed",
          "6": "the camera’s focal length and the LED lamps’ brightness, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "high-speed camera\n(1024×768,\nfreely\nconfigurable\nframe",
          "6": "ensured that the participant’s face appeared utterly, clearly,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "rates)\nfor\ncapturing MEs, which was\nconnected via a 10",
          "6": "and brightly in the centre of the screen."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2: Experimental environment for eliciting MEs": "specifically required participants to clarify their true internal"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "emotions\nin their\nself-reports whenever\nfacial expressions"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "appear, which is vital\nin aiding subsequent annotators\nto"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "comprehend the nuances of their MEs."
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "3.3\nME Annotation"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "Building the DFME database required a two-stage annota-"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "tion:\nthe sample selection stage as well as the coding and"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "category labeling stage.\nIn the first stage, we clipped short"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "fragments\ncontaining valid expression samples\nfrom the"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "collected long video sequences. The second stage included"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "three successive rounds of fine-grained annotation, through"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "which we confirmed all MEs and labeled their\nthree key-"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "frames (i.e., onset, apex, and offset\nframe),\nfacial AUs, and"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "emotion categories. Furthermore, we performed annotation"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "agreement test to verify the reliability of emotion labels."
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "3.3.1\nSample Selection"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "In the sample selection stage, participants’ FV sequences"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "were manually segmented into several shorter video frag-"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "ments, each capturing at\nleast one ME or MaE.Using the"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "self-developed video annotation software, an experienced"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "annotator checked through the collected original FVs frame"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "by frame\nto locate\nthe\nfragments of\nfacial muscle move-"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "ments. With the guidance of\nthe self-reports from partici-"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "pants,\nthe annotator was able to effectively distinguish the"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "facial expressions definitely related to emotion, and aban-"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "don interference data unrelated to emotion (such as violent"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "blinking caused by dry eyelids, habitual mouth opening,"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "etc.). Besides, we reserved some fragments with blinking or"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "eye movements if they contained MaE or ME data."
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "3.3.2\nCoding and Category Labeling"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": ""
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "The apex frame\ncorresponds\nto the moment when facial"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "expression changes most dramatically. In the first round of"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "the fine-grained annotation, five annotators independently"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "marked out\nthe onset, apex, and offset\nframe of each ex-"
        },
        {
          "Fig. 2: Experimental environment for eliciting MEs": "pression clip,\nand the median value of\ntheir\nannotation"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU6"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "an ME labeled out by the two annotators."
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "In the third round of fine-grained labeling, we performed"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "the\nemotion labeling of MEs\ntaking eight\ncategories\ninto"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "account: happiness, anger, contempt, disgust,\nfear, sadness, sur-"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "prise, and others.\n’Others’\nrepresents MEs\nthat are difficult"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "to divide into the former\nseven prototypical emotion cat-"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "egories. Seven annotators independently gave the emotion"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "labels of all MEs. When disagreements arised, a ’50% ma-"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "jority voting’ approach was employed, where a sample was"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "assigned a specific emotion label\nif at\nleast\nfour annotators"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "agreed on that label. For samples with unresolved disagree-"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "ments, a second round of voting was\nconducted through"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "collective discussions among all annotators to determine the"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "emotion label.\nIf a consensus\nstill\ncannot be reached,\nthe"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "sample was categorized as ’Others’."
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "In previous spontaneous ME databases, the reference ba-"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "sis of emotion labeling was not precisely the same. In some"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "databases,\nas\nrepresented by SMIC,\nemotion labels were"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "determined based on self-reports provided by participants."
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "Some other studies believed that seeing is believing, so their"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "annotation was based on the correspondence between AUs"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "and emotions. However, on the one hand, unlike MaEs,"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "only part of\nthe AUs\ncan appear\nsimultaneously in MEs"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "due to their\nlow intensity, and some AUs are shared by"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "different emotion categories, which may lead to category"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "confusion. On the other hand, we should not\nignore the"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "differences in self-emotional cognition of different partici-"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "pants, which means that the self-reports given for the whole"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "piece of elicitation materials may be rough and inaccurate."
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "Therefore,\nin DFME,\nthe emotion labels were determined"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "through a comprehensive analysis of facial AUs, self-reports"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "of participants, and elicitation material contents, which is"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "consistent with the method adopted by the CASME series."
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "It\nis worth mentioning that we obtained the participants’"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": ""
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "fine-grained self-reports in the data collection process, and"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "this is also the information that we recommend as a priority"
        },
        {
          "Fig. 3: Representative ME Samples of Seven Discrete Emotion Categories in DFME": "for reference when determining emotion labels. We matched"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "TABLE 4: Interpretation of κ for Fleiss’Kappa Test"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the corresponding timestamps of MEs and elicitation ma-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "terials\nthrough playback,\nenabling participants\nto\nreport",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "κ\nInterpretation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "their emotions\nfor each time of\nsuccessful ME induction,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "≤ 0\nPoor agreement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "which significantly improved the confidence of self-reports",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "0.01-0.20\nSlight agreement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "in emotion labeling. Fig. 3 shows some representative ME",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "0.21-0.40\nFair agreement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "samples of seven discrete emotion categories in DFME.",
          "9": "0.41-0.60\nModerate agreement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "0.61-0.80\nSubstantial agreement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "0.81-1.00\nAlmost perfect agreement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "3.3.3\nAnnotation Agreement",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Having reliable emotion categories of MEs is of vital signif-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "icance for a database.\nIn this\nsection, we utilized Fleiss’s",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "3.4\nStatistical Properties of DFME"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Kappa\ntest\n[75]\nto\nevaluate\nthe\nquality\nof\nour\nemotion",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "annotation encouraged by work [76]. Fleiss’s Kappa is a",
          "9": "The DFME database consists of three parts: PART A, PART"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "measure of the agreement among three or more annotators,",
          "9": "B, and PART C. The only difference between these three"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "testing the consistency of annotation results. Therefore, we",
          "9": "parts is the frame rate setting of\nthe high-speed camera in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "consider Fleiss’s Kappa as an excellent indicator to evaluate",
          "9": "the experiment.\nIn PART A, all 1,118 ME samples from 72"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the reliability of emotion annotation.",
          "9": "participants have a frame rate of 500fps. The frame rate of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "In DFME, seven annotators independently labeled each",
          "9": "PART B is 300fps with 969 ME samples from 92 participants."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME sample based on facial AUs, an accurate self-report, and",
          "9": "PART C has\nthe most data\nsize with 5,439 ME samples"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the corresponding elicitation material content. The samples",
          "9": "from 492 participants, whose frame rate is 200fps. Although"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "were divided into eight emotion categories: {1: happiness,",
          "9": "we\nrecruited a total of 671 participants, 15 of\nthem had"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2:\nanger,\n3:\ncontempt,\n4: disgust,\n5:\nfear,\n6:\nsadness,\n7:",
          "9": "strong control over their facial expressions, from whom we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "surprise, 8: others}. Let n = 7 represent\nthe total number",
          "9": "could not collect any ME sample. Therefore, the final DFME"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "of\nannotation personnel, N indicate\nthe\ntotal number of",
          "9": "database contains 7,526 ME samples from 656 participants,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME video clips, K = 8 represent\nthe number of emotion",
          "9": "and we gave each sample an emotion category label as well"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "categories. nij is the number of annotators who assigned the",
          "9": "as AU labels annotated according to FACS. Fig. 4 describes"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "i-th ME video clip to the j-th category, so we can calculate",
          "9": "the distribution of ME samples in detail. According to the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "pj, the proportion of all assignments which were to the j-th",
          "9": "specific distribution of AU labels\nshown in Table\n5,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "emotion:",
          "9": "average number of AU’s per ME sample occurrence can be"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "1",
          "9": "calculated as 12928/7526 = 1.718."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "N(cid:88) i\n(2)\npj =\nnij,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "N × n",
          "9": "Given\nthat we\nhave\ncollected\nthe\nfine-grained\nself-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "=1",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "reports and the AU labels with considerable reliability, this"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "fosters the exploration of\nthe emotion-AU correspondence"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "K(cid:88) j\n(3)\npj = 1.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "rule\nin MEs.\nTherefore, we\ncounted\nthe\nratio\nof\nhigh-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "=1",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "occurrence AUs in each emotion (Table 6), which reflects the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "existence preference of AU in MEs with different emotions,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Then,\nthe extent of agreement among the n annotators",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "not affected by the emotional category imbalance problem"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "is calculated.\nIn\nfor the i-th ME video clip indicated by Pi",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "in the database. We\nalso matched the\nemotion and AU"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "other words,\nit can be indexed by the proportion of pairs",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "combinations\naccording to the\nstatistical\nresults,\nand the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "agreeing in their evaluation of\nthe i-th ME out of all\nthe",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "conclusions are shown as Table 7."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "n(n − 1) possible pairs of agreement:",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "9": "Based on the statistical results presented in Table 6, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "1",
          "9": "have some findings to discuss:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "K(cid:88) j\n[(\nn2\n(4)\nPi =\nij) − n].",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "n × (n − 1)",
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "n(n − 1) possible pairs of agreement:"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "1"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "K(cid:88) j\n[(\nn2\n(4)\nPi =\nij) − n]."
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "n × (n − 1)"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "=1"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "is therefore:\nThe mean of Pi"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "1 N\nN(cid:88) i\nP =\n(5)\nPi."
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "=1"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "And we also have Pe:"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "K(cid:88) j\np2\n(6)\nPe =\nj ."
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "=1"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "Finally, we can calculate κ by:"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "P − Pe"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ".\nκ =\n(7)"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "1 − Pe"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": ""
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "Thus, we obtained κ = 0.72 through performing Fleiss’s"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "Kappa test in DFME. According to Table 4, we know that all"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "of our emotion annotators achieve substantial agreement,"
        },
        {
          "agreeing in their evaluation of\nthe i-th ME out of all\nthe": "meaning that our emotion labels are quite reliable."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU6"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU7"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1500": ""
        },
        {
          "1500": ""
        },
        {
          "1500": "1000"
        },
        {
          "1500": ""
        },
        {
          "1500": ""
        },
        {
          "1500": "500"
        },
        {
          "1500": ""
        },
        {
          "1500": ""
        },
        {
          "1500": ""
        },
        {
          "1500": "0"
        },
        {
          "1500": "PART A"
        },
        {
          "1500": "PART B"
        },
        {
          "1500": ""
        },
        {
          "1500": "PART C"
        },
        {
          "1500": ""
        },
        {
          "1500": "Combined"
        },
        {
          "1500": ""
        },
        {
          "1500": ""
        },
        {
          "1500": "Fig. 4: Distribution of ME Samples in DFME. Each column represents the total sample number of an emotion category, and"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "Anger"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "pct(%)"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "72.5"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "29.1"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "16.3"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "7.6"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "5.6"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "5.6"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "5.2"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": "4.8"
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": ""
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": ""
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": ""
        },
        {
          "TABLE 6: AUs of High Occurrence in MEs of Seven Discrete Emotion Categories": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": ""
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": ""
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Emotion Categories"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": ""
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Happiness"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": ""
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Anger"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": ""
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Contempt"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Disgust"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Fear"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Sadness"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Surprise"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "Shared1"
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": ""
        },
        {
          "TABLE 7: Matching Emotion and AU Combinations in MEs": "1 Shared:\nthe AU combinations"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "task. On the other hand, AU classification aims to predict",
          "11": "4.3\nEvaluation Protocols and Metrics"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "whether an AU exists in a video clip, corresponding to a",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "Due to the small sample size of previous databases such as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "single binary multi-label problem [77]. These experiments",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "CASME II [28], SAMM [29], and SMIC [27], most MER stud-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "can serve as a valuable reference for future research on ME",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "ies employed the leave-one-subject-out strategy for evalu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "analysis using the DFME database.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "ation. However, given the relatively large number of ME"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "clips in DFME,\nthis paper utilized a simpler and more effi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "cient subject-independent 10-fold cross-validation strategy."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "4.1\nEvaluation Database",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "In each fold, data from 10% of subjects were selected as the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "The DFME database is described in detail\nin Section 3. For",
          "11": "test set, while the remaining 90% were used for training."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the subsequent MER and AU classification verification, we",
          "11": "Inline with CD6ME [77], we employed the F1-score as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "combined 7, 275 samples with clear emotion and AU labels",
          "11": "the\nevaluation metric\nfor AU classification. Furthermore,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "in PART A, B and C of DFME as our experimental database.",
          "11": "three widely recognized ME classification metrics, namely"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "anger,\ncontempt, dis-\nThe emotion labels\ninclude happiness,",
          "11": "Accuracy\n(ACC), Unweighted\nF1-Score\n(UF1),\nand Un-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "gust,\nfear, sadness and surprise. By drawing inspiration from",
          "11": "weighted Average Recall (UAR), were utilized to assess the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "CD6ME [77], we also selected the following 12 frequently",
          "11": "MER performance. Finally, we computed the average of ten"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "occurring AUs: AU1, AU2, AU4, AU5, AU6, AU7, AU9,",
          "11": "folds’ outcomes as the final result in the MER."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AU10, AU12, AU14, AU15, and AU17 as the AU labels for",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "4.3.1\nAccuracy (ACC)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "our\nexperiment.\nIn fact,\nthese 12 AUs account\nfor about",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "88.48% (11439/12928) of the DFME database, covering most",
          "11": "ACC is one of the most common metrics, which can evaluate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AUs\nin MEs. Among the\nremaining AUs, most AUs are",
          "11": "the overall performance of\nthe recognition method on the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "small\nin number,\nsuch as AU16 and AU18, while others",
          "11": "database. It is calculated as follows:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "like AU23 (Lip Tightener) are usually unconscious actions",
          "11": "(cid:80)K"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "i=1 T Pi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "ACC =\n,\n(8)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "unrelated to emotions. For these reasons, we chose these 12",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "(cid:80)K"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "i=1 Ni"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "AUs for the experiments.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "where K represents the number of the classes, Ni stands for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "is the number\nthe sample number of the i-th class and T Pi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "4.2\nData Preprocessing",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "of true positive samples of the i-th class."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "In facial\nexpression recognition, many variables,\nsuch as",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "4.3.2\nF1-score (F1)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "backgrounds and head poses, can affect the final recognition",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "When the problem of\nclass\nimbalance\nin the database\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "results. Therefore,\nbefore\nformally\nconducting\nautomatic",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "pronounced, ACC may not accurately reflect\nthe model’s"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "MER experiments, we need to preprocess all ME videos in",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "true performance. As a result, F1-score is often employed as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the following steps (i.e.,\nface alignment and face cropping)",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "the evaluation metric in most classification tasks to address"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "to minimize the influence of irrelevant variables.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "this challenge. The F1-score is defined as shown below:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "2 · T P"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "4.2.1\nFace Alignment",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": ".\nF 1 =\n(9)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "2 · T P + F P + F N"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "To eliminate variations\nin pose and angle among all ME",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "where T P , F P , F N refer to true positives,\nfalse positives"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "samples, we need to perform face alignment. In this step, we",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "and false negatives, respectively. Notably, T P , F P , F N are"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "took the following operations for each ME sample. Firstly,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "calculated based on all ten folds in the AU classification task."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "we selected a frontal face image as a reference and applied",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Style Aggregated Network (SAN)\n[78]\nto extract\nits\nfacial",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "4.3.3\nUnweighted F1-score (UF1)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "landmarks. Subsequently, we employed Procrustes analysis",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "UF1, also known as macro-averaged F1-score, is defined as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[79] to compute an affine transformation based on the land-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "marks of the onset frame and those of the reference image.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "2 · T Pi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "The rationale behind not using landmarks from all frames in",
          "11": "1 K\nK(cid:88) i\n.\nU F 1 =\n(10)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "2 · T Pi + F Pi + F Ni"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the ME video is to avoid errors introduced by the calculation",
          "11": "=1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "of\nlandmarks and transformations that could significantly",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "Class\nimbalance is an intractable problem in the MER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "impact real MEs. Finally, the transformation was applied to",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "task, so introducing UF1 as an evaluation metric can better"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "each frame to align the faces. Additionally, some landmarks",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "measure the method’s performance in all classes rather than"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "were situated in regions where MEs may appear, which may",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "in some major classes."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "not be stable enough for alignment. Therefore, we excluded",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "such landmarks during the alignment process.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "4.3.4\nUnweighted Average Recall\n(UAR)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "UAR is also a more suitable metric than ACC when dealing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "4.2.2\nFace Cropping",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "with class imbalance."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Since the movement of MEs is mainly in the facial area, face",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "cropping is essential\nto eliminate biases caused by varying",
          "11": "T Pi"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "1 K\nK(cid:88) i\nU AR =\n.\n(11)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "backgrounds. Following face alignment, we employed Reti-",
          "11": "Ni"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "11": "=1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "naFace [80] to crop the faces. Similar to face alignment, face",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "cropping was based on the onset\nframe rather\nthan each",
          "11": "Both UF1 and UAR can effectively assess whether MER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "frame of a sample.",
          "11": "methods provide accurate predictions across all classes."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 8: , and the recognition confusion",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "4.4\nEvaluation Baseline Models",
          "12": "specific expressions and also predict expression by fusing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "the expression-specific features. It consists of an expression"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "To comprehensively validate our database, we specifically",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "proposal module with the attention mechanism and a clas-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "selected three different groups of baseline methods for MER",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "sification branch."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "and AU classification,\nincluding: 3D-CNN Methods, Hand-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "crafted MER Methods and Deep learning MER Methods.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "4.5\nEvaluation Implementation Settings"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "4.4.1\n3D-CNN Methods",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "All experiments were conducted on 2 NVIDIA GeForce RTX"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "In recent years, many influential 3D-CNN methods have",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "3090 GPUs with 2 × 24 GB memory. Following the original"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "emerged in the field of video classification. Due\nto their",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "settings, the length of ME clips was 16 frames for 3D-CNN"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "excellent ability to represent spatiotemporal\nfeatures,\nthey",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "methods and the length of optical flow sequences was 10 for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "are often used in dynamic ME analysis. Here, we selected",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "MERSiam. The spatial sizes of each input\nimage or optical"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "3D-ResNet\n(R3D)\n[81] and Inflated 3D ConvNet\n(I3D)\n[82]",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "feature map were 224×224 for R3D and I3D models, 28×28"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "as two baseline methods. Hara et al. proposed R3D for tasks",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "for OffApexNet, STSTNet and FR, 60×60 for RCN-A and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "such as video\nclassification and recognition.\nSince\nthen,",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "112×112 for MERSiam."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "R3D is often used as the backbone in approaches to video-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "During training, cross-entropy loss and stochastic gra-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "related tasks. The basic idea of\nthis model\nis to replace the",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "dient descent\n(SGD) with a momentum of 0.9 were used"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2D convolutional kernels with spatiotemporal 3D kernels",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "to optimize the model parameters, and the batch size was"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "according to the 2D-ResNet [51] network structure. I3D [82]",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "set\nto 32 for 3D-CNN methods, 8 for MERSiam and 256"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "is based on 2D ConvNet\ninflation.\nIt utilizes convolutional",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "for other deep learning MER methods. Besides, other hy-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "kernels with different sizes to extract\nfeatures, and the key",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "perparameters such as learning rate followed the settings in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "idea behind the I3D model\nis to inflate the 2D filters and",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "corresponding papers."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "pooling\nkernels\ninto\n3D,\nallowing\nit\nto understand and",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "process the spatial and temporal information of video data.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "4.6\nEvaluation Baseline Results"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "4.4.2\nHand-crafted MER Methods",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "4.6.1\nMER Results"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "The\nhand-crafted methods\nin MER are\ntypically\nbased",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "To demonstrate\nthe\neffectiveness of our DFME database"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "on traditional machine learning, extracting elaborately de-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "for automatic MER tasks, we conducted a comprehensive"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "signed manual\nfeatures\nfrom ME\nvideos. Hand-crafted",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "MER experiment based on the above baseline methods. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "methods were popular and achieved SOTA results on small",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "results are shown in Table 8, and the recognition confusion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME databases\nin\nthe\nearly days. We\nselected LBP-TOP",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "matrix of each baseline model is shown in Fig. 5. Comparing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[17] and MDMO [20] as\ntwo baseline methods. LBP-TOP",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "the MER results among the three groups of baseline meth-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "extends LBP from 2D to 3D, which extracts features from",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "ods,\nit’s evident\nthat most deep learning MER approaches"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "three orthogonal planes\n(X-Y, X-T, and Y-T) and connects",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "exhibit\nbetter\nrecognition performance\ncompared to\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "them together. MDMO refers to the main directional mean",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "others. This\nis\nconsistent with our\nexpectations\nbecause"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "optical flow features, which is a typical optical flow operator",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "deep learning MER methods not only leverage the strengths"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "feature. It divides 36 regions of interest on the face and has",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "of deep learning but also specifically design modules and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "good performance for subtle facial changes.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "introduce prior knowledge for ME features. Surprisingly,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "typical\nvideo understanding models\nlike\nI3D have\nalso"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "4.4.3\nDeep learning MER methods",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "12": "achieved competitive recognition performance, surpassing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "In addition to hand-crafted methods, deep neural networks",
          "12": "two influential hand-crafted MER approaches. This reflects"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "have been widely used in MER. Due\nto the\nscarcity of",
          "12": "that by utilizing the DFME database we built,\nit’s possible"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ME data, unlike general neural networks which have\na",
          "12": "to train general video understanding models to acquire the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "deep and complex structure to better extract\nfeatures, net-",
          "12": "ability for perceiving ME features."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "works\nin MER prefer\nto reduce\nthe number of parame-",
          "12": "Furthermore, by observing the\nconfusion matrices\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ters\nto prevent overfitting. We\nselected OffApexNet\n[83],",
          "12": "Fig. 5, we also discovered significant variations in the mod-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "STSTNet\n[84], RCN-A [85], MERSiam [22] and FR [57] as",
          "12": "els’ perception abilities for different categories of MEs. Tak-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "four baseline methods. Off-ApexNet calculates optical flow",
          "12": "ing FR as an example,\nit demonstrated higher\nrecognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "maps\nbetween\ntwo\nframes\nand uses\nthem to\nrepresent",
          "12": "accuracy for disgust, happiness, and surprise MEs,\nreaching"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the entire video. STSTNet designs a shallow triple stream",
          "12": "76.58%,\n64.92%,\nand\n79.97%,\nrespectively. However,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "three-dimensional CNN that is computationally light whilst",
          "12": "recognition accuracy for the remaining four ME categories"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "capable of extracting discriminative high-level features and",
          "12": "were all below 15%. The primary reasons contributing to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "details of MEs. RCN-A is a recurrent convolutional network",
          "12": "this discrepancy can be identified as\nfollows: 1) Negative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "(RCN)\nto\nexplore\nthe\nshallower-architecture\nand\nlower-",
          "12": "contempt, disgust,\nemotions like anger,\nfear, and sadness ex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "resolution input data, shrinking model and input complex-",
          "12": "hibit significant similarities in ME. Referring to Table 6,\nit"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ities simultaneously. MERSiam [22] selects the optical flow",
          "12": "becomes apparent\nthat MEs of anger, disgust and fear share"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "sequences of ME videos as the model input and the method",
          "12": "common facial movements,\nincluding AU4, AU7, AU10,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "introduces a unique two-stage learning strategy, which in-",
          "12": "AU14, and AU24. 2) The problem of class imbalance persists"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "cludes prior-learning and target-learning stages. The main",
          "12": "within our DFME database. The number of disgust samples"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "structure of MERSiam is built upon a Siamese 3D-CNN.",
          "12": "(2528) is six times greater than contempt (401) which contains"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "FR aims\nto obtain salient and discriminative features\nfor",
          "12": "the least samples. To address these aspects, exploring more"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "5.82"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "7.98"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "4.00"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "True label",
          "13": "10.31"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "5.14"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "6.61"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "53.48"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "e"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "s\ni"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "r\np"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "r"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "u\ns"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "4.85"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "5.74"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "2.73"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "True label",
          "13": "10.54"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "4.74"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "6.93"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "72.52"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "e"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "s\ni"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "r\np"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "r"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "u\ns"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "8.40"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "9.23"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "4.51"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "True label",
          "13": "13.90"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "5.75"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "9.45"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "79.97"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "e"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "s\ni"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "r\np"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "r"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": "u\ns"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "13": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "ACC(%)": "36.62",
          "UAR": "0.2313",
          "UF1": "0.2164"
        },
        {
          "Methods": "3D-CNN Methods",
          "ACC(%)": "",
          "UAR": "",
          "UF1": ""
        },
        {
          "Methods": "",
          "ACC(%)": "39.29",
          "UAR": "0.3058",
          "UF1": "0.2923"
        },
        {
          "Methods": "",
          "ACC(%)": "46.82",
          "UAR": "0.2653",
          "UF1": "0.2336"
        },
        {
          "Methods": "Hand-crafted MER",
          "ACC(%)": "",
          "UAR": "",
          "UF1": ""
        },
        {
          "Methods": "",
          "ACC(%)": "49.34",
          "UAR": "0.2939",
          "UF1": "0.2489"
        },
        {
          "Methods": "",
          "ACC(%)": "48.06",
          "UAR": "0.2806",
          "UF1": "0.2386"
        },
        {
          "Methods": "",
          "ACC(%)": "50.90",
          "UAR": "0.3108",
          "UF1": "0.2714"
        },
        {
          "Methods": "Deep Learning MER",
          "ACC(%)": "",
          "UAR": "",
          "UF1": ""
        },
        {
          "Methods": "",
          "ACC(%)": "50.98",
          "UAR": "0.3123",
          "UF1": "0.2751"
        },
        {
          "Methods": "",
          "ACC(%)": "52.18",
          "UAR": "0.3532",
          "UF1": "0.3184"
        },
        {
          "Methods": "",
          "ACC(%)": "",
          "UAR": "",
          "UF1": ""
        },
        {
          "Methods": "",
          "ACC(%)": "52.59",
          "UAR": "0.3814",
          "UF1": "0.3559"
        },
        {
          "Methods": "",
          "ACC(%)": "",
          "UAR": "",
          "UF1": ""
        },
        {
          "Methods": "No data augmentation was used in any of the experiments.",
          "ACC(%)": "",
          "UAR": "",
          "UF1": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Methods",
          "14": "Average"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "R3D [81]",
          "14": "0.2692"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "I3D [82]",
          "14": "0.4210"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "LBP-TOP [17]",
          "14": "0.2535"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "MDMO [20]",
          "14": "0.2600"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "OffApexNet [83]",
          "14": "0.1060"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "STSTNet [84]",
          "14": "0.2660"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "RCN-A [85]",
          "14": "0.4272"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "MERSiam [22]",
          "14": "0.3233"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "FR [57]",
          "14": "0.5914"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "ACKNOWLEDGMENTS",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[16] T. Pfister, X. Li, G. Zhao, and M. Pietik¨ainen, “Recognising sponta-"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "neous facial micro-expressions,” in 2011 international conference on"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "This work has received a lot of guidance and help from the",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "computer vision.\nIEEE, 2011, pp. 1449–1456."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[17] G. Zhao and M. Pietikainen, “Dynamic texture recognition using"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "teachers in the Micro-expression Laboratory of\nInstitute of",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "local binary patterns with an application to facial expressions,”"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "Psychology, Chinese Academy of Sciences. We would like to",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "IEEE Transactions on Pattern Analysis & Machine Intelligence, no. 6,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "express our special thanks to them.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "pp. 915–928, 2007."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[18] Y. Wang,\nJ.\nSee, R. C.-W. Phan,\nand Y.-H. Oh,\n“Lbp with six"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "intersection points: Reducing redundant\ninformation in lbp-top"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "for micro-expression recognition,” in Computer Vision–ACCV 2014:"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "REFERENCES",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "12th Asian Conference\non Computer Vision,\nSingapore,\nSingapore,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "November 1-5, 2014, Revised Selected Papers, Part\nI 12.\nSpringer,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[1]\nA. Mehrabian, “Communication without words,” in Communica-",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "2015, pp. 525–537."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "tion theory.\nRoutledge, 2017, pp. 193–200.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[19] X. Huang, G. Zhao, X. Hong, W. Zheng,\nand M. Pietik¨ainen,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[2]\nG.\nZhao,\nX.\nLi,\nY.\nLi,\nand M.\nPietik¨ainen,\n“Facial micro-",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "“Spontaneous facial micro-expression analysis using spatiotempo-"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "expressions: An overview,” Proceedings of the IEEE, 2023. [Online].",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "ral completed local quantized patterns,” Neurocomputing, vol. 175,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "Available: https://api.semanticscholar.org/CorpusID:259823625",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "pp. 564–578, 2016."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[3]\nE. A. Haggard and K.\nS.\nIsaacs,\n“Micromomentary\nfacial\nex-",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[20] Y.-J. Liu, J.-K. Zhang, W.-J. Yan, S.-J. Wang, G. Zhao, and X. Fu, “A"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "pressions as indicators of ego mechanisms in psychotherapy,” in",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "main directional mean optical flow feature for spontaneous micro-"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "Methods of research in psychotherapy.\nSpringer, 1966, pp. 154–165.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "expression recognition,” IEEE Transactions on Affective Computing,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[4]\nP\n. Ekman and W. V. Friesen, “Nonverbal\nleakage and clues\nto",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "vol. 7, no. 4, pp. 299–310, 2015."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "deception,” Psychiatry, vol. 32, no. 1, pp. 88–106, 1969.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[21]\nS.-J. Wang, B.-J. Li, Y.-J. Liu, W.-J. Yan, X. Ou, X. Huang, F. Xu,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[5]\nS. Porter and L. Ten Brinke, “Reading between the lies: Identifying",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "and X. Fu, “Micro-expression recognition with small sample size"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "concealed and falsified emotions in universal facial expressions,”",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "by transferring long-term convolutional neural network,” Neuro-"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "Psychological science, vol. 19, no. 5, pp. 508–514, 2008.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "computing, vol. 312, pp. 251–262, 2018."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "in the marketplace, politics, and\n[6]\nP\n. Ekman, Telling lies: Clues to deceit",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[22]\nS. Zhao, H. Tao, Y. Zhang, T. Xu, K. Zhang, Z. Hao, and E. Chen, “A"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "marriage (revised edition).\nWW Norton & Company, 2009.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "two-stage 3d cnn based learning method for spontaneous micro-"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[7]\nS. Weinberger, “Intent\nto deceive?\ncan the science of deception",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "expression recognition,” Neurocomputing, vol.\n448, pp.\n276–289,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "detection help to catch terrorists? sharon weinberger takes a close",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "2021."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "look at the evidence for it,” Nature, vol. 465, no. 7297, pp. 412–416,",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[23] Q. Mao, L. Zhou, W. Zheng, X. Shao, and X. Huang, “Objective"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "2010.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "class-based micro-expression recognition under partial occlusion"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[8]\nL. Hunter, L. Roland, and A. Ferozpuri, “Emotional expression",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "via region-inspired relation reasoning network,” IEEE Transactions"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "processing and depressive symptomatology: Eye-tracking reveals",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "on Affective Computing, vol. 13, no. 4, pp. 1998–2016, 2022."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "differential\nimportance of\nlower and middle facial areas of\ninter-",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[24] X. Ben, Y. Ren,\nJ. Zhang, S.-J. Wang, K. Kpalma, W. Meng, and"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "est,” Depression Research and Treatment, vol. 2020, 2020.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "Y.-J. Liu, “Video-based facial micro-expression analysis: A survey"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[9]\nJ. Zhenyu, Micro-reactions.\nBeijing: China Friendship Publishing",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "of datasets,\nfeatures and algorithms,” IEEE transactions on pattern"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "Company, 2020.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "analysis and machine intelligence, 2021."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[10] L. Lombardi and F. Marcolin, “Psychological stress detection by",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[25] Y. Li,\nJ. Wei, Y. Liu,\nJ. Kauttonen, and G. Zhao, “Deep learning"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "Intelligent\nInformation\n2d and 3d facial\nimage\nprocessing,”\nin",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "IEEE Transactions\nfor micro-expression recognition: A survey,”"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "Hiding and Multimedia Signal Processing, 2017. [Online]. Available:",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "on Affective Computing,\nvol.\n13, pp.\n2028–2046,\n2021.\n[Online]."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "https://api.semanticscholar.org/CorpusID:222112061",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "Available: https://api.semanticscholar.org/CorpusID:237532789"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[11]\nJ. Wang, X. Pan, X. Li, G. Wei,\nand Y. Zhou,\n“Single\ntrunk",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[26] B.\nXia,\nW.\nWang,\nS.\nWang,\nand\nE.\nChen,\n“Learning"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "multi-scale network for micro-expression recognition,” Graphics",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "from\nmacro-expression:\na\nmicro-expression\nrecognition"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "and Visual Computing, vol. 4, p. 200026, 2021.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "Proceedings\nof\nthe\n28th\nACM\nInternational\nframework,”"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[12] P.\nEkman,\n“Micro\nexpressions\ntraining\ntool,”",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "Conference\non\nMultimedia,\n2020.\n[Online].\nAvailable:"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "=Emotionsrevealed.com, 2003.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "https://api.semanticscholar.org/CorpusID:222278587"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[13] T. A. Russell, E. M.-Y. Chu, and M. L. Phillips, “A pilot study to",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[27] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietik¨ainen, “A spon-"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "investigate the effectiveness of emotion recognition remediation",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "taneous micro-expression database:\nInducement,\ncollection and"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "in schizophrenia using the micro-expression training tool.” The",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "baseline,” in 2013 10th IEEE International Conference and Workshops"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "British journal of clinical psychology, vol. 45 Pt 4, pp. 579–83, 2006.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "on Automatic face and gesture recognition (fg).\nIEEE, 2013, pp. 1–6."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": ""
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "23119588",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[28] W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y.-J. Liu, Y.-H. Chen, and"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[14]\nJ.\nEndres\nand A. H.\nLaidlaw,\n“Micro-expression\nrecognition",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "X. Fu,\n“Casme\nii: An improved spontaneous micro-expression"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "BMC Medical\ntraining\nin medical\nstudents:\na\npilot\nstudy,”",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "database and the baseline evaluation,” PloS one, vol. 9, no. 1, p."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "Education,\nvol.\n9,\npp.\n47\n–\n47,\n2009.\n[Online].\nAvailable:",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "e86041, 2014."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "https://api.semanticscholar.org/CorpusID:18933437",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[29] A. K. Davison, C. Lansley, N. Costen, K. Tan, and M. H. Yap,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "[15] M. Frank, M. Herbasz, K. Sinuk, A. Keller, and C. Nolan, “I see",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "IEEE\n“Samm: A spontaneous micro-facial movement dataset,”"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "how you feel: Training laypeople and professionals to recognize",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "transactions on affective computing, vol. 9, no. 1, pp. 116–129, 2016."
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "the international com-\nfleeting emotions,” in The annual meeting of",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "[30]\nJ. Li, Z. Dong, S. Lu, S.-J. Wang, W.-J. Yan, Y. Ma, Y. Liu, C. Huang,"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "munication association. Sheraton New York, New York City, 2009, pp.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "and X. Fu, “Cas\n(me) 3: A third generation facial\nspontaneous"
        },
        {
          "0.8792\n0.8837\n0.9367\n0.6971\n0.4352\n0.6217\nFR [57]": "1–35.",
          "0.0678\n0.3373\n0.7163\n0.4638\n0.3979\n0.6605\n0.5914": "micro-expression database with depth information and high eco-"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "logical validity,” IEEE Transactions on Pattern Analysis and Machine",
          "15": "tion methods,” IEEE transactions on affective computing, vol. 9, no. 4,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Intelligence, 2022.",
          "15": "pp. 563–577, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[31]\nJ.\nLi, M. H.\nYap, W.-H.\nCheng,\nJ.\nSee,\nX. Hong,\nX.\nLi,",
          "15": "[49]\nF. Xu,\nJ. Zhang, and J. Z. Wang, “Microexpression identification"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "S.-J. Wang, A. K. Davison, Y.\nLi,\nand Z. Dong,\n“Megc2022:",
          "15": "and categorization using a facial dynamics map,” IEEE Transac-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Acm\nmultimedia\n2022\nmicro-expression\ngrand\nchallenge,”",
          "15": "tions on Affective Computing, vol. 8, no. 2, pp. 254–267, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Proceedings\nof\nthe\n30th\nACM International\nConference\non\nin",
          "15": "[50] M. Peng, Z. Wu, Z. Zhang, and T. Chen, “From macro to micro"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Multimedia, ser. MM ’22.\nNew York, NY, USA: Association for",
          "15": "expression recognition: Deep learning on small datasets using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Computing Machinery,\n2022, p.\n7170–7174.\n[Online]. Available:",
          "15": "on\ntransfer\nlearning,” in 2018 13th IEEE International Conference"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "https://doi.org/10.1145/3503161.3551601",
          "15": "Automatic Face & Gesture Recognition (FG 2018).\nIEEE, 2018, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[32] Y. Xu, S. Zhao, H. Tang, X. Mao, T. Xu, and E. Chen, “Famgan:",
          "15": "657–661."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Fine-grained aus modulation based generative adversarial net-",
          "15": "[51] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the 29th\nwork for micro-expression generation,” in Proceedings of",
          "15": "image recognition,” in Proceedings of the IEEE conference on computer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ACM International Conference on Multimedia, 2021, pp. 4813–4817.",
          "15": "vision and pattern recognition, 2016, pp. 770–778."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[33]\nS. Zhao,\nS. Yin, H. Tang, R.\nJin, Y. Xu, T. Xu,\nand E. Chen,",
          "15": "[52] N. Van Quang, J. Chun, and T. Tokuyama, “Capsulenet for micro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "“Fine-grained micro-expression generation based on thin-plate",
          "15": "expression recognition,” in 2019 14th IEEE International Conference"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the 30th ACM\nspline and relative au constraint,” in Proceedings of",
          "15": "on Automatic Face & Gesture Recognition (FG 2019).\nIEEE, 2019, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "International Conference on Multimedia, 2022, pp. 7150–7154.",
          "15": "1–7."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[34] M. Shreve, S. Godavarthy, D. Goldgof, and S. Sarkar, “Macro-and",
          "15": "[53] B. Xia, W. Wang, S. Wang, and E. Chen, “Learning from macro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "micro-expression spotting in long videos using spatio-temporal",
          "15": "expression: a micro-expression recognition framework,” in Pro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "strain,” in 2011 IEEE International Conference on Automatic Face &",
          "15": "ceedings\nof\nthe 28th ACM International Conference\non Multimedia,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Gesture Recognition (FG).\nIEEE, 2011, pp. 51–56.",
          "15": "2020, pp. 2936–2944."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[35]\nS. Polikovsky, Y. Kameda, and Y. Ohta, “Facial micro-expressions",
          "15": "[54] Y. Li, X. Huang, and G. Zhao, “Joint local and global\ninformation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "recognition using high speed camera and 3d-gradient descriptor,”",
          "15": "learning with single apex frame detection for micro-expression"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "in 3rd International Conference on Imaging for Crime Detection and",
          "15": "recognition,” IEEE Transactions\non Image Processing, vol. 30, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Prevention (ICDP), 2009, pp. 1–6.",
          "15": "249–263, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[36] W.-J. Yan, Q. Wu, Y.-J.\nLiu,\nS.-J. Wang,\nand X.\nFu,\n“Casme",
          "15": "[55]\nS.-T. Liong,\nJ. See, K. Wong, and R. C.-W. Phan, “Less\nis more:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "database: A dataset of\nspontaneous micro-expressions\ncollected",
          "15": "Micro-expression recognition from video using apex frame,” Sig-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "from neutralized faces,” in 2013 10th IEEE international conference",
          "15": "nal Processing: Image Communication, vol. 62, pp. 82–92, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "and workshops on automatic face and gesture recognition (FG).\nIEEE,",
          "15": "[56] Y.\nLiu, H. Du,\nL. Zheng,\nand\nT. Gedeon,\n“A neural micro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2013, pp. 1–7.",
          "15": "expression recognizer,” in 2019 14th IEEE International Conference"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[37]\nF. Qu, S.-J. Wang, W.-J. Yan, H. Li, S. Wu, and X. Fu, “Cas(me)2: a",
          "15": "on Automatic Face & Gesture Recognition (FG 2019).\nIEEE, 2019, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "database for spontaneous macro-expression and micro-expression",
          "15": "1–4."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "spotting and recognition,” IEEE Transactions on Affective Comput-",
          "15": "[57] L. Zhou, Q. Mao, X. Huang, F. Zhang, and Z. Zhang, “Feature"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ing, vol. 9, no. 4, pp. 424–436, 2017.",
          "15": "refinement: An expression-specific\nfeature\nlearning and fusion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[38] X. Li, S. Cheng, Y. Li, M. Behzad, J. Shen, S. Zafeiriou, M. Pantic,",
          "15": "method for micro-expression recognition,” Pattern Recognition, vol."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "and G. Zhao, “4dme: A spontaneous 4d micro-expression dataset",
          "15": "122, p. 108275, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "with multimodalities,” IEEE Transactions\non Affective Computing,",
          "15": "[58] W. Gong, Y. Zhang, W. Wang, P. Cheng, and J. Gonz`alez, “Meta-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2022.",
          "15": "mmfnet: Meta-learning\nbased multi-model\nfusion network\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[39] P. Hus´ak, J. Cech, and J. Matas, “Spotting facial micro-expressions",
          "15": "on Multimedia\nmicro-expression recognition,” ACM Transactions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "“in the wild”,” in 22nd Computer Vision Winter Workshop (Retz),",
          "15": "Computing, Communications, and Applications (TOMM), 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2017, pp. 1–9.",
          "15": "[59]\nS. Liu, Y. Ren, L. Li, X. Sun, Y. Song, and C.-C. Hung, “Micro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[40] A.\nJ. R. Kumar\nand B. Bhanu,\n“Micro-expression classification",
          "15": "expression recognition based on squeezenet and c3d,” Multimedia"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "based on landmark relations with graph attention convolutional",
          "15": "Systems, vol. 28, pp. 2227 – 2236, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "network,” 2021 IEEE/CVF Conference on Computer Vision and Pattern",
          "15": "[60]\nF. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Recognition Workshops (CVPRW), pp. 1511–1520, 2021.",
          "15": "K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[41] M. A. Takalkar, S. Thuseethan, S. Rajasegarar, Z. Chaczko, M. Xu,",
          "15": "parameters\nand ¡1mb model\nsize,” ArXiv, vol.\nabs/1602.07360,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "and J. Yearwood, “Lgattnet: Automatic micro-expression detection",
          "15": "2016."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "using dual-stream local and global attentions,” Knowledge-Based",
          "15": "[61] D. H. Kim, W.\nJ. Baddar, and Y. M. Ro, “Micro-expression recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Systems, vol. 212, p. 106566, 2021.",
          "15": "nition with expression-state constrained spatio-temporal\nfeature"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[42] H. Pan, L. Xie, J. Li, Z. Lv, and Z. Wang, “Micro-expression recog-",
          "15": "the 24th ACM international con-\nrepresentations,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "nition by two-stream difference network,” IET Computer Vision,",
          "15": "ference on Multimedia.\nACM, 2016, pp. 382–386."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "vol. 15, no. 6, pp. 440–448, 2021.",
          "15": "[62] H.-Q. Khor, J. See, R. C. W. Phan, and W. Lin, “Enriched long-term"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[43] M. H. Yap,\nJ.\nSee, X. Hong,\nand\nS.-J. Wang,\n“Facial micro-",
          "15": "recurrent convolutional network for facial micro-expression recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "expressions grand challenge 2018 summary,” in 2018 13th IEEE",
          "15": "nition,” in 2018 13th IEEE International Conference on Automatic Face"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "International Conference on Automatic Face & Gesture Recognition (FG",
          "15": "& Gesture Recognition (FG 2018).\nIEEE, 2018, pp. 667–674."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2018).\nIEEE, 2018, pp. 675–678.",
          "15": "[63]\nS.\nJi, W. Xu, M. Yang, and K. Yu, “3d convolutional neural net-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[44]\nJ. See, M. H. Yap,\nJ. Li, X. Hong, and S.-J. Wang, “Megc 2019–",
          "15": "works for human action recognition,” IEEE transactions on pattern"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the\nsecond facial micro-expressions\ngrand challenge,”\nin 2019",
          "15": "analysis and machine intelligence, vol. 35, no. 1, pp. 221–231, 2012."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "14th\nIEEE International Conference\non Automatic Face & Gesture",
          "15": "[64] M. Peng, C. Wang, T. Chen, G. Liu, and X. Fu, “Dual temporal scale"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Recognition (FG 2019).\nIEEE, 2019, pp. 1–5.",
          "15": "convolutional neural network for micro-expression recognition,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[45] Y. Zong, W. Zheng, X. Hong, C. Tang, Z. Cui,\nand G. Zhao,",
          "15": "Frontiers in psychology, vol. 8, p. 1745, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "“Cross-database micro-expression recognition: A benchmark,” in",
          "15": "[65] Y. Wang, H. Ma, X. Xing, and Z. Pan, “Eulerian motion based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Proceedings\nof\nthe 2019 on International Conference\non Multimedia",
          "15": "3dcnn architecture\nfor\nfacial micro-expression recognition,”\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Retrieval, 2019, pp. 354–363.",
          "15": "International Conference on Multimedia Modeling.\nSpringer, 2020,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[46] R. Chaudhry, A. Ravichandran, G. Hager,\nand R. Vidal,\n“His-",
          "15": "pp. 266–277."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "tograms of oriented optical flow and binet-cauchy kernels on non-",
          "15": "[66] Z. Xia, X. Hong, X. Gao, X. Feng, and G. Zhao, “Spatiotemporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "linear dynamical systems for the recognition of human actions,”",
          "15": "recurrent\nconvolutional networks\nfor\nrecognizing\nspontaneous"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "in 2009 IEEE Conference on Computer Vision and Pattern Recognition.",
          "15": "micro-expressions,” IEEE Transactions on Multimedia, vol. 22, no. 3,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "IEEE, 2009, pp. 1932–1939.",
          "15": "pp. 626–640, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[47] X. Huang, S.-J. Wang, X. Liu, G. Zhao, X. Feng, and M. Pietik¨ainen,",
          "15": "[67] B. Sun, S. Cao, D. Li, J. He, and L. Yu, “Dynamic micro-expression"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "“Discriminative spatiotemporal local binary pattern with revisited",
          "15": "IEEE Transactions\non\nrecognition using knowledge distillation,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "integral projection for spontaneous facial micro-expression recog-",
          "15": "Affective Computing, vol. 13, no. 2, pp. 1037–1043, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "nition,” IEEE Transactions on Affective Computing, vol. 10, no. 1, pp.",
          "15": "[68]\nS. Zhao, H. Tang, S. Liu, Y. Zhang, H. Wang, T. Xu, E. Chen, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "32–47, 2019.",
          "15": "C. Guan, “Me-plan: A deep prototypical learning with local atten-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[48] X. Li, X. Hong, A. Moilanen, X. Huang, T. Pfister, G. Zhao, and",
          "15": "tion network for dynamic micro-expression recognition,” Neural"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "M. Pietik¨ainen, “Towards reading hidden emotions: A compara-",
          "15": "networks\n:\nthe\nofficial\njournal\nof\nthe\nInternational Neural Network"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "tive study of spontaneous micro-expression spotting and recogni-",
          "15": "Society, vol. 153, pp. 427–443, 2022."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[69] H.-X. Xie, L. Lo, H.-H. Shuai,\nand W.-H. Cheng,\n“Au-assisted",
          "16": "Huaying Tang received the B.S. degree in the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "graph\nattention\nconvolutional\nnetwork\nfor micro-expression",
          "16": "School of Computer Science and Technology"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "the 28th ACM International Conference\nrecognition,” Proceedings of",
          "16": "from University of Science and Technology of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "on Multimedia, 2020.",
          "16": "China\n(USTC), Hefei, China,\nin\n2021. He\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[70] L. Lei, T. Chen, S. Li, and J. Li, “Micro-expression recognition",
          "16": "currently pursuing the M.S. degree in computer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "based on facial graph representation learning and facial action unit",
          "16": "science and technology in USTC. His research"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "fusion,” 2021 IEEE/CVF Conference on Computer Vision and Pattern",
          "16": "interests lie around automatic micro-expressions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Recognition Workshops (CVPRW), pp. 1571–1580, 2021.",
          "16": "analysis\nand\naffect\ncomputing. He\nhas\npub-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[71] Y. Zhang, H. Wang, Y. Xu, X. Mao, T. Xu, S. Zhao, and E. Chen,",
          "16": "lished several papers\nin refereed conferences"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "“Adaptive\ngraph\nattention\nnetwork with\ntemporal\nfusion\nfor",
          "16": "and journals,\nincluding ACM Multimedia, Neural"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "micro-expressions recognition,” in 2023 IEEE International Confer-",
          "16": "Networks, etc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ence on Multimedia and Expo (ICME), 2023, pp. 1391–1396.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[72]\nJ. Hong, C. Lee, and H. Jung, “Late fusion-based video transformer",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "for facial micro-expression recognition,” Applied Sciences, 2022.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "received\nthe\nB.S\ndegree\nin\nXinglong Mao"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[73] X.-B. Nguyen, C. N. Duong, X. Li, S. Gauch, H.-S. Seo, and K. Luu,",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "the School\nof Data Science\nfrom University"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "“Micron-bert: Bert-based facial micro-expression recognition,” in",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "of Science and Technology of China (USTC),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Proceedings\nof\nthe\nIEEE/CVF Conference\non Computer Vision\nand",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "Hefei, China. He\nis\ncurrently working\ntoward"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Pattern Recognition, 2023, pp. 1482–1492.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "the M.S. degree from the School of Data Sci-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[74] P. Ekman and W. V. Friesen, “Facial action coding system,” Envi-",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "ence. His research interests include automatic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ronmental Psychology & Nonverbal Behavior, 1978.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "micro-expressions analysis and affect comput-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[75]\nJ. L. Fleiss, “Measuring nominal\nscale agreement among many",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "ing. He has published several conference papers"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "raters.” Psychological bulletin, vol. 76, no. 5, p. 378, 1971.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "in ACM Multimedia Conference,\nICME, etc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[76] X.\nJiang, Y. Zong, W. Zheng, C. Tang, W. Xia, C. Lu, and J. Liu,",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "“Dfew: A large-scale database for recognizing dynamic facial ex-",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "pressions in the wild,” in Proceedings of the 28th ACM International",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Conference on Multimedia, 2020, pp. 2881–2889.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[77] T. Varanka, Y. Li, W. Peng,\nand G. Zhao,\n“Data\nleakage\nand",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "the B.S degree\nin\nthe\nShifeng Liu received"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "evaluation issues in micro-expression analysis,” IEEE Transactions",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "School of Gifted Young from University of Sci-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "on Affective Computing, 2023.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "ence and Technology of China (USTC), Hefei,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[78] X. Dong, Y. Yan, W. Ouyang,\nand Y. Yang,\n“Style\naggregated",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "China.\nShe\nis\ncurrently working\ntoward\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "network for facial landmark detection,” 2018 IEEE/CVF Conference",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "M.S. degree from the School of Data Science."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "on Computer Vision and Pattern Recognition, pp. 379–388, 2018.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "Her\nresearch interests include automatic micro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[79]\nJ. C. Gower,\n“Generalized procrustes\nanalysis,” Psychometrika,",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "expressions analysis, human-computer\ninterac-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "vol. 40, pp. 33–51, 1975.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "tion (HCI) and affect computing. She has pub-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[80]\nJ. Deng,\nJ. Guo, Y. Zhou,\nJ. Yu,\nI. Kotsia, and S. Zafeiriou, “Reti-",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "lished several papers\nin refereed conferences"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "naface: Single-stage dense face localisation in the wild,” ArXiv,",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "and journals,\nincluding ACM Multimedia Confer-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "vol. abs/1905.00641, 2019.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "ence,\nICME, Neural Networks, etc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[81] K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3d cnns",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2018\nIEEE/CVF\nretrace\nthe history of\n2d cnns\nand imagenet?”",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Conference on Computer Vision and Pattern Recognition, pp. 6546–",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "6555, 2018.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "Yiming Zhang is currently working toward the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[82]\nJ. Carreira and A. Zisserman, “Quo vadis, action recognition? a",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "Ph.D.\ndegree\nin\nthe School\nof Data Science"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2017\nIEEE Conference\non\nnew model\nand the kinetics dataset,”",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "from University of Science and Technology of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Computer Vision and Pattern Recognition (CVPR), pp. 4724–4733,",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "China (USTC). His\nresearch interests\ninclude"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2017.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "automatic micro-expressions analysis and affect"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[83]\nS. Liong, Y. S. Gan, W.-C. Yau, Y.-C. Huang, and T. Ken, “Off-",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "computing. He has published several papers in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "apexnet\non micro-expression\nrecognition\nsystem,” ArXiv,\nvol.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "conference proceedings,\nincluding ACM Multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "abs/1805.08699, 2018.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "media and ICME."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[84]\nS. Liong, Y. S. Gan, J. See, and H.-Q. Khor, “Shallow triple stream",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "three-dimensional cnn (ststnet) for micro-expression recognition,”",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "2019 14th IEEE International Conference on Automatic Face & Gesture",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Recognition (FG 2019), pp. 1–5, 2019.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "[85] Z. Xia, W. Peng, H.-Q. Khor, X. Feng, and G. Zhao, “Revealing the",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "invisible with model and data shrinking for composite-database",
          "16": "Hao Wang received the PhD degree in computer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "micro-expression recognition,” IEEE Transactions on Image Process-",
          "16": "science from USTC. He is currently an associate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ing, vol. 29, pp. 8590–8605, 2020.",
          "16": "researcher with the School of Computer Science"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "",
          "16": "and Technology, USTC. His main research inter-"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ests include data mining,\nrepresentation learn-": "ing, network embedding and recommender sys-"
        },
        {
          "ests include data mining,\nrepresentation learn-": "tems. He has published several papers in re-"
        },
        {
          "ests include data mining,\nrepresentation learn-": "ferred conference proceedings, such as TKDE,"
        },
        {
          "ests include data mining,\nrepresentation learn-": "TOIS, NeuriPS, and AAAI."
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": "Tong Xu received the Ph.D. degree in University"
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": "of Science and Technology of China (USTC),"
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": "Hefei, China, in 2016. He is currently working as"
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": "a Professor of\nthe Anhui Province Key Labora-"
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": "tory of Big Data Analysis and Application, USTC."
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": "He has authored 100+ journal and conference"
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": "papers in the fields of social network and so-"
        },
        {
          "ests include data mining,\nrepresentation learn-": ""
        },
        {
          "ests include data mining,\nrepresentation learn-": "cial media analysis,\nincluding IEEE TKDE,\nIEEE"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "IEEE) received the PhD\nEnhong Chen (Fellow,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "degree from USTC. He is a professor and ex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ecutive dean of School of Data Science, USTC."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "His general area of research includes data min-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ing and machine learning, social network anal-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ysis, and recommender systems. He has pub-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "lished more than 200 papers in refereed con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "ferences and journals,\nincluding IEEE Transac-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "tions on Knowledge and Data Engineering, IEEE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "Transactions on Mobile Computing, KDD, ICDM,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "NeurIPS, and CIKM. He was on program com-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "mittees of numerous conferences including KDD,\nICDM, and SDM. His"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "research is supported by the National Science Foundation for Distin-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2022": "guished Young Scholars of China."
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Communication without words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2017",
      "venue": "Communication theory"
    },
    {
      "citation_id": "2",
      "title": "Facial microexpressions: An overview",
      "authors": [
        "G Zhao",
        "X Li",
        "Y Li",
        "M Pietikäinen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "3",
      "title": "Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy",
      "authors": [
        "E Haggard",
        "K Isaacs"
      ],
      "year": "1966",
      "venue": "Methods of research in psychotherapy"
    },
    {
      "citation_id": "4",
      "title": "Nonverbal leakage and clues to deception",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "Psychiatry"
    },
    {
      "citation_id": "5",
      "title": "Reading between the lies: Identifying concealed and falsified emotions in universal facial expressions",
      "authors": [
        "S Porter",
        "L Brinke"
      ],
      "year": "2008",
      "venue": "Psychological science"
    },
    {
      "citation_id": "6",
      "title": "Telling lies: Clues to deceit in the marketplace, politics, and marriage",
      "authors": [
        "P Ekman"
      ],
      "year": "2009",
      "venue": "Telling lies: Clues to deceit in the marketplace, politics, and marriage"
    },
    {
      "citation_id": "7",
      "title": "Intent to deceive? can the science of deception detection help to catch terrorists? sharon weinberger takes a close look at the evidence for it",
      "authors": [
        "S Weinberger"
      ],
      "year": "2010",
      "venue": "Nature"
    },
    {
      "citation_id": "8",
      "title": "Emotional expression processing and depressive symptomatology: Eye-tracking reveals differential importance of lower and middle facial areas of interest",
      "authors": [
        "L Hunter",
        "L Roland",
        "A Ferozpuri"
      ],
      "year": "2020",
      "venue": "Depression Research and Treatment"
    },
    {
      "citation_id": "9",
      "title": "Micro-reactions",
      "authors": [
        "J Zhenyu"
      ],
      "year": "2020",
      "venue": "Micro-reactions"
    },
    {
      "citation_id": "10",
      "title": "Psychological stress detection by 2d and 3d facial image processing",
      "authors": [
        "L Lombardi",
        "F Marcolin"
      ],
      "year": "2017",
      "venue": "Intelligent Information Hiding and Multimedia Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Single trunk multi-scale network for micro-expression recognition",
      "authors": [
        "J Wang",
        "X Pan",
        "X Li",
        "G Wei",
        "Y Zhou"
      ],
      "year": "2021",
      "venue": "Graphics and Visual Computing"
    },
    {
      "citation_id": "12",
      "title": "Micro expressions training tool",
      "authors": [
        "P Ekman"
      ],
      "year": "2003",
      "venue": "Micro expressions training tool"
    },
    {
      "citation_id": "13",
      "title": "A pilot study to investigate the effectiveness of emotion recognition remediation in schizophrenia using the micro-expression training tool",
      "authors": [
        "T Russell",
        "-Y Chu",
        "M Phillips"
      ],
      "year": "2006",
      "venue": "The British journal of clinical psychology"
    },
    {
      "citation_id": "14",
      "title": "Micro-expression recognition training in medical students: a pilot study",
      "authors": [
        "J Endres",
        "A Laidlaw"
      ],
      "year": "2009",
      "venue": "BMC Medical Education"
    },
    {
      "citation_id": "15",
      "title": "I see how you feel: Training laypeople and professionals to recognize fleeting emotions",
      "authors": [
        "M Frank",
        "M Herbasz",
        "K Sinuk",
        "A Keller",
        "C Nolan"
      ],
      "year": "2009",
      "venue": "The annual meeting of the international communication association"
    },
    {
      "citation_id": "16",
      "title": "Recognising spontaneous facial micro-expressions",
      "authors": [
        "T Pfister",
        "X Li",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Recognising spontaneous facial micro-expressions"
    },
    {
      "citation_id": "17",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Lbp with six intersection points: Reducing redundant information in lbp-top for micro-expression recognition",
      "authors": [
        "Y Wang",
        "J See",
        "R -W. Phan",
        "Y.-H Oh"
      ],
      "year": "2014",
      "venue": "Computer Vision-ACCV 2014: 12th Asian Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Spontaneous facial micro-expression analysis using spatiotemporal completed local quantized patterns",
      "authors": [
        "X Huang",
        "G Zhao",
        "X Hong",
        "W Zheng",
        "M Pietikäinen"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "20",
      "title": "A main directional mean optical flow feature for spontaneous microexpression recognition",
      "authors": [
        "Y.-J Liu",
        "J.-K Zhang",
        "W.-J Yan",
        "S.-J Wang",
        "G Zhao",
        "X Fu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Micro-expression recognition with small sample size by transferring long-term convolutional neural network",
      "authors": [
        "S.-J Wang",
        "B.-J Li",
        "Y.-J Liu",
        "W.-J Yan",
        "X Ou",
        "X Huang",
        "F Xu",
        "X Fu"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "22",
      "title": "A two-stage 3d cnn based learning method for spontaneous microexpression recognition",
      "authors": [
        "S Zhao",
        "H Tao",
        "Y Zhang",
        "T Xu",
        "K Zhang",
        "Z Hao",
        "E Chen"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "23",
      "title": "Objective class-based micro-expression recognition under partial occlusion via region-inspired relation reasoning network",
      "authors": [
        "Q Mao",
        "L Zhou",
        "W Zheng",
        "X Shao",
        "X Huang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Video-based facial micro-expression analysis: A survey of datasets, features and algorithms",
      "authors": [
        "X Ben",
        "Y Ren",
        "J Zhang",
        "S.-J Wang",
        "K Kpalma",
        "W Meng",
        "Y.-J Liu"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "25",
      "title": "Deep learning for micro-expression recognition: A survey",
      "authors": [
        "Y Li",
        "J Wei",
        "Y Liu",
        "J Kauttonen",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Learning from macro-expression: a micro-expression recognition framework",
      "authors": [
        "B Xia",
        "W Wang",
        "S Wang",
        "E Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "A spontaneous micro-expression database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic face and gesture recognition (fg)"
    },
    {
      "citation_id": "28",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2016",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "30",
      "title": "Cas (me) 3: A third generation facial spontaneous micro-expression database with depth information and high eco-logical validity",
      "authors": [
        "J Li",
        "Z Dong",
        "S Lu",
        "S.-J Wang",
        "W.-J Yan",
        "Y Ma",
        "Y Liu",
        "C Huang",
        "X Fu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Megc2022: Acm multimedia 2022 micro-expression grand challenge",
      "authors": [
        "J Li",
        "M Yap",
        "W.-H Cheng",
        "J See",
        "X Hong",
        "X Li",
        "S.-J Wang",
        "A Davison",
        "Y Li",
        "Z Dong"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia, ser. MM '22",
      "doi": "10.1145/3503161.3551601"
    },
    {
      "citation_id": "32",
      "title": "Famgan: Fine-grained aus modulation based generative adversarial network for micro-expression generation",
      "authors": [
        "Y Xu",
        "S Zhao",
        "H Tang",
        "X Mao",
        "T Xu",
        "E Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Fine-grained micro-expression generation based on thin-plate spline and relative au constraint",
      "authors": [
        "S Zhao",
        "S Yin",
        "H Tang",
        "R Jin",
        "Y Xu",
        "T Xu",
        "E Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "Macro-and micro-expression spotting in long videos using spatio-temporal strain",
      "authors": [
        "M Shreve",
        "S Godavarthy",
        "D Goldgof",
        "S Sarkar"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "35",
      "title": "Facial micro-expressions recognition using high speed camera and 3d-gradient descriptor",
      "authors": [
        "S Polikovsky",
        "Y Kameda",
        "Y Ohta"
      ],
      "year": "2009",
      "venue": "3rd International Conference on Imaging for Crime Detection and Prevention"
    },
    {
      "citation_id": "36",
      "title": "Casme database: A dataset of spontaneous micro-expressions collected from neutralized faces",
      "authors": [
        "W.-J Yan",
        "Q Wu",
        "Y.-J Liu",
        "S.-J Wang",
        "X Fu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "37",
      "title": "Cas(me) 2 : a database for spontaneous macro-expression and micro-expression spotting and recognition",
      "authors": [
        "F Qu",
        "S.-J Wang",
        "W.-J Yan",
        "H Li",
        "S Wu",
        "X Fu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "4dme: A spontaneous 4d micro-expression dataset with multimodalities",
      "authors": [
        "X Li",
        "S Cheng",
        "Y Li",
        "M Behzad",
        "J Shen",
        "S Zafeiriou",
        "M Pantic",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Spotting facial micro-expressions \"in the wild",
      "authors": [
        "P Husák",
        "J Cech",
        "J Matas"
      ],
      "year": "2017",
      "venue": "22nd Computer Vision Winter Workshop"
    },
    {
      "citation_id": "40",
      "title": "Micro-expression classification based on landmark relations with graph attention convolutional network",
      "authors": [
        "A Kumar",
        "B Bhanu"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "41",
      "title": "Lgattnet: Automatic micro-expression detection using dual-stream local and global attentions",
      "authors": [
        "M Takalkar",
        "S Thuseethan",
        "S Rajasegarar",
        "Z Chaczko",
        "M Xu",
        "J Yearwood"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "42",
      "title": "Micro-expression recognition by two-stream difference network",
      "authors": [
        "H Pan",
        "L Xie",
        "J Li",
        "Z Lv",
        "Z Wang"
      ],
      "year": "2021",
      "venue": "IET Computer Vision"
    },
    {
      "citation_id": "43",
      "title": "Facial microexpressions grand challenge 2018 summary",
      "authors": [
        "M Yap",
        "J See",
        "X Hong",
        "S.-J Wang"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "44",
      "title": "Megc 2019the second facial micro-expressions grand challenge",
      "authors": [
        "J See",
        "M Yap",
        "J Li",
        "X Hong",
        "S.-J Wang"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "45",
      "title": "Cross-database micro-expression recognition: A benchmark",
      "authors": [
        "Y Zong",
        "W Zheng",
        "X Hong",
        "C Tang",
        "Z Cui",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "46",
      "title": "Histograms of oriented optical flow and binet-cauchy kernels on nonlinear dynamical systems for the recognition of human actions",
      "authors": [
        "R Chaudhry",
        "A Ravichandran",
        "G Hager",
        "R Vidal"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Discriminative spatiotemporal local binary pattern with revisited integral projection for spontaneous facial micro-expression recognition",
      "authors": [
        "X Huang",
        "S.-J Wang",
        "X Liu",
        "G Zhao",
        "X Feng",
        "M Pietikäinen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Towards reading hidden emotions: A comparative study of spontaneous micro-expression spotting and recogni-tion methods",
      "authors": [
        "X Li",
        "X Hong",
        "A Moilanen",
        "X Huang",
        "T Pfister",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "49",
      "title": "Microexpression identification and categorization using a facial dynamics map",
      "authors": [
        "F Xu",
        "J Zhang",
        "J Wang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "From macro to micro expression recognition: Deep learning on small datasets using transfer learning",
      "authors": [
        "M Peng",
        "Z Wu",
        "Z Zhang",
        "T Chen"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "51",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "52",
      "title": "Capsulenet for microexpression recognition",
      "authors": [
        "N Van Quang",
        "J Chun",
        "T Tokuyama"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "53",
      "title": "Learning from macroexpression: a micro-expression recognition framework",
      "authors": [
        "B Xia",
        "W Wang",
        "S Wang",
        "E Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "54",
      "title": "Joint local and global information learning with single apex frame detection for micro-expression recognition",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "55",
      "title": "Less is more: Micro-expression recognition from video using apex frame",
      "authors": [
        "S.-T Liong",
        "J See",
        "K Wong",
        "-W Phan"
      ],
      "year": "2018",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "56",
      "title": "A neural microexpression recognizer",
      "authors": [
        "Y Liu",
        "H Du",
        "L Zheng",
        "T Gedeon"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "57",
      "title": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "X Huang",
        "F Zhang",
        "Z Zhang"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "58",
      "title": "Metammfnet: Meta-learning based multi-model fusion network for micro-expression recognition",
      "authors": [
        "W Gong",
        "Y Zhang",
        "W Wang",
        "P Cheng",
        "J Gonzàlez"
      ],
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "59",
      "title": "Microexpression recognition based on squeezenet and c3d",
      "authors": [
        "S Liu",
        "Y Ren",
        "L Li",
        "X Sun",
        "Y Song",
        "C.-C Hung"
      ],
      "year": "2022",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "60",
      "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡1mb model size",
      "authors": [
        "F Iandola",
        "M Moskewicz",
        "K Ashraf",
        "S Han",
        "W Dally",
        "K Keutzer"
      ],
      "year": "2016",
      "venue": "ArXiv"
    },
    {
      "citation_id": "61",
      "title": "Micro-expression recognition with expression-state constrained spatio-temporal feature representations",
      "authors": [
        "D Kim",
        "W Baddar",
        "Y Ro"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM international conference on Multimedia"
    },
    {
      "citation_id": "62",
      "title": "Enriched long-term recurrent convolutional network for facial micro-expression recognition",
      "authors": [
        "H.-Q Khor",
        "J See",
        "R Phan",
        "W Lin"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "63",
      "title": "3d convolutional neural networks for human action recognition",
      "authors": [
        "S Ji",
        "W Xu",
        "M Yang",
        "K Yu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "64",
      "title": "Dual temporal scale convolutional neural network for micro-expression recognition",
      "authors": [
        "M Peng",
        "C Wang",
        "T Chen",
        "G Liu",
        "X Fu"
      ],
      "year": "2017",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "65",
      "title": "Eulerian motion based 3dcnn architecture for facial micro-expression recognition",
      "authors": [
        "Y Wang",
        "H Ma",
        "X Xing",
        "Z Pan"
      ],
      "year": "2020",
      "venue": "International Conference on Multimedia Modeling"
    },
    {
      "citation_id": "66",
      "title": "Spatiotemporal recurrent convolutional networks for recognizing spontaneous micro-expressions",
      "authors": [
        "Z Xia",
        "X Hong",
        "X Gao",
        "X Feng",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "67",
      "title": "Dynamic micro-expression recognition using knowledge distillation",
      "authors": [
        "B Sun",
        "S Cao",
        "D Li",
        "J He",
        "L Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "68",
      "title": "Me-plan: A deep prototypical learning with local attention network for dynamic micro-expression recognition",
      "authors": [
        "S Zhao",
        "H Tang",
        "S Liu",
        "Y Zhang",
        "H Wang",
        "T Xu",
        "E Chen",
        "C Guan"
      ],
      "year": "2022",
      "venue": "Neural networks : the official journal of the International Neural Network Society"
    },
    {
      "citation_id": "69",
      "title": "Au-assisted graph attention convolutional network for micro-expression recognition",
      "authors": [
        "H.-X Xie",
        "L Lo",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "70",
      "title": "Micro-expression recognition based on facial graph representation learning and facial action unit fusion",
      "authors": [
        "L Lei",
        "T Chen",
        "S Li",
        "J Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "71",
      "title": "Adaptive graph attention network with temporal fusion for micro-expressions recognition",
      "authors": [
        "Y Zhang",
        "H Wang",
        "Y Xu",
        "X Mao",
        "T Xu",
        "S Zhao",
        "E Chen"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "72",
      "title": "Late fusion-based video transformer for facial micro-expression recognition",
      "authors": [
        "J Hong",
        "C Lee",
        "H Jung"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "73",
      "title": "Micron-bert: Bert-based facial micro-expression recognition",
      "authors": [
        "X.-B Nguyen",
        "C Duong",
        "X Li",
        "S Gauch",
        "H.-S Seo",
        "K Luu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "74",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "75",
      "title": "Measuring nominal scale agreement among many raters",
      "authors": [
        "J Fleiss"
      ],
      "year": "1971",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "76",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "77",
      "title": "Data leakage and evaluation issues in micro-expression analysis",
      "authors": [
        "T Varanka",
        "Y Li",
        "W Peng",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "78",
      "title": "Style aggregated network for facial landmark detection",
      "authors": [
        "X Dong",
        "Y Yan",
        "W Ouyang",
        "Y Yang"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "79",
      "title": "Generalized procrustes analysis",
      "authors": [
        "J Gower"
      ],
      "year": "1975",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "80",
      "title": "Retinaface: Single-stage dense face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "Y Zhou",
        "J Yu",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "81",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "82",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "83",
      "title": "Offapexnet on micro-expression recognition system",
      "authors": [
        "S Liong",
        "Y Gan",
        "W.-C Yau",
        "Y.-C Huang",
        "T Ken"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "84",
      "title": "Shallow triple stream three-dimensional cnn (ststnet) for micro-expression recognition",
      "authors": [
        "S Liong",
        "Y Gan",
        "J See",
        "H.-Q Khor"
      ],
      "year": "2019",
      "venue": "Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "85",
      "title": "Revealing the invisible with model and data shrinking for composite-database micro-expression recognition",
      "authors": [
        "Z Xia",
        "W Peng",
        "H.-Q Khor",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    }
  ]
}