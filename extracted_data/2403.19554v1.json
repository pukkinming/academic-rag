{
  "paper_id": "2403.19554v1",
  "title": "Cross-Attention Is Not Always Needed: Dynamic Cross-Attention For Audio-Visual Dimensional Emotion Recognition",
  "published": "2024-03-28T16:38:04Z",
  "authors": [
    "R. Gnana Praveen",
    "Jahangir Alam"
  ],
  "keywords": [
    "Audio-Visual Fusion",
    "Emotion Recognition",
    "Cross-Attention",
    "Weak complementary relationships"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In video-based emotion recognition, audio and visual modalities are often expected to have a complementary relationship, which is widely explored using cross-attention. However, they may also exhibit weak complementary relationships, resulting in poor representations of audio-visual features, thus degrading the performance of the system. To address this issue, we propose Dynamic Cross-Attention (DCA) that can dynamically select cross-attended or unattended features on the fly based on their strong or weak complementary relationship with each other, respectively. Specifically, a simple yet efficient gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose crossattended features only when they exhibit a strong complementary relationship, otherwise unattended features. We evaluate the performance of the proposed approach on the challenging RECOLA and Aff-Wild2 datasets. We also compare the proposed approach with other variants of cross-attention and show that the proposed model consistently improves the performance on both datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic recognition of human emotions is a challenging problem, typically formulated as the classification of emotions. However, humans can express a wide range of more subtle and complex emotions beyond them. In recent years, regression of expressions has gained a lot of attention because it has the potential to capture a wide range of expressions. Depending on the granularity of labels, the regression of emotions can be formulated as ordinal regression or continuous regression. Compared to ordinal regression, continuous (or dimensional) regression is even more challenging due to the highly complex process of obtaining annotations on continuous dimensions. Valence and arousal are widely used dimensions to estimate emotion intensities in a continuous domain. Valence reflects the wide range of emotions in the dimension of pleasantness, from being negative (sad) to positive (happy). In contrast, arousal spans a range of intensities, from passive (sleepiness) to active (high excitement). Here, both audio and visual modalities strongly complement each other, thereby assigning higher attention scores for face and voice with significant expressions (b) Attention scores based on cross-attention for the subject \"21-24-1920x1080\" from the validation set of the Aff-Wild2 dataset. In this case, the facial modality is corrupted due to extreme blur and pose, however, it has rich vocal expressions. Attending the corrupted face to rich vocal expressions fails to assign higher attention scores for vocal expressions.\n\nRecently, multimodal learning has achieved remarkable success by leveraging complementary relationships across the modalities using cross-modal attention for Emotion Recognition (ER)  [1, 2] . The idea of cross-modal attention, often termed as Cross-Attention (CA) or co-attention, is to leverage one modality to attend to another modality based on crossmodal interactions  [3] . However, audio and visual modalities may not always strongly complement each other, they may also exhibit weak complementary relationships  [4] . It has been shown that the audio and visual modalities may also demonstrate conflicting (when one of the modalities is noisy or paradoxical) or dominating (when one of the modalities is restrained or unemotional) relationships for ER  [5] . When one of the modalities is noisy or restrained (weak complementary relationship), leveraging the noisy modality to attend to a good modality can deteriorate the fused Audio-Visual (A-V) feature representations  [4] . To better understand the problem of weak complementary relationships for ER, we provided an interpretability analysis by visualizing the attention scores (normalized between 0 and 1) of CA as shown in Fig.  1 . We can observe that the audio and visual modalities demonstrate higher attention scores of CA for samples with strong complementary relationships (top image), where intense facial expressions are strongly complemented by intense vocal expressions. For weak complementary relationships, where intense vocal expressions are associated with noisy facial expressions (bottom image), we can see that both audio and visual modalities exhibit lower CA scores. Even though audio modality conveys strong emotional expressions, attending the audio with noisy visual lowers the attention scores for the audio modality. Therefore, CA fails to retain rich information of intense vocal expressions, resulting in poor A-V feature representations. Motivated by this insight, we have investigated the prospect of developing a robust model that can dynamically choose when to integrate the cross-attended or unattended features depending on the strong or weak complementary relationships, respectively.\n\nIn this work, we propose a Dynamic Cross Attention (DCA) model that can dynamically adapt to both strong and weak complementary relationships by selecting the semantic features to deal with the problem of weak complementary relationships. Specifically, we introduce a gating layer to evaluate the strength of the complementary relationships and select the cross-attended or unattended features dynamically based on strong or weak complementary relationships, respectively. Therefore, the proposed DCA model adds more flexibility to the CA framework and improves the fusion performance even when the modalities exhibit weak complementary relationships. The proposed model is a simple, yet efficient way of dynamically selecting the most relevant features, which can also be adapted to any variant of the CA model. To our knowledge, this is the first work to investigate incompatibility issues (weak complementary relationships) between audio and visual modalities for ER.\n\nThe major contributions of the paper can be summarized as follows. (i) We investigate the potential of the CA model in leveraging complementary relationships and show that weak complementary relationships degrade the fusion performance. (ii) We propose a DCA model to dynamically select the crossattended or unattended features based on the strength of their complementary relationships across audio and visual modalities. (iii) The proposed model is further evaluated on different variants of CA and demonstrated that the proposed model consistently improves the performance of the system on both RECOLA and Aff-Wild2 datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Attention models for ER: Recently, multimodal transformers with CA showed significant improvement for ER  [6, 7, 8] . Parthasarathy et al.  [9]  explored multimodal transformers, where the CA module is integrated with the self-attention module to obtain the A-V cross-modal feature representa-tions. Zhang et al.  [10]  proposed a leader-follower attention mechanism by considering the visual modality as the primary channel, while the audio modality is used as a supplementary channel to boost visual performance. Karas et al.  [6]  and Meng et al.  [8]  showed improvement in fusion performance by exploring a set of fusion models based on LSTMs and transformers. Zhou et al.  [7]  explored temporal convolutional networks (TCNs) for individual modalities, whereas Zhang et al  [11]  exploited masked auto-encoders for visual modality. However, most of these methods  [7, 11, 6, 8]  rely on a naive fusion approach or ensemble-based fusion using transformers and LSTMs. Unlike these approaches, Praveen et al.  [2]  proposed a CA model to effectively leverage complementary relationships by allowing the modalities to interact with each other. They have extended the approach by introducing joint feature representation in the CA framework to capture intermodal and intramodal relationships  [12]  and recursive fusion  [13] . Although these methods have shown impressive performance with CA, they rely on the assumption that audio and visual modalities always exhibit strong complementary relationships. When audio and visual modalities exhibit a weak complementary relationship due to restrained or noisy modalities, these methods will result in poor performance. Gating-Based Attention: Conventionally gating mechanisms have been explored for multimodal fusion to control the flow of modalities to reduce redundancy  [14]  or to mitigate the impact of noisy modalities  [15, 16, 17] . Aspandi et al.  [18]  proposed a gated-sequence neural network for dimensional ER, where the gating mechanism is explored for temporal modeling to adaptively fuse the modalities based on their relative importance. Kumar et al.  [15]  explored the conditional gating mechanism using a nonlinear transformation by modulating the cross-modal interactions to learn the relative importance of modalities. Liu et al.  [19]  used a twostage gating mechanism to control the alignment and contribution of the modalities for speech and text. Unlike these approaches, we have focused on handling the problem of weak complementary relationships using a conditional gating mechanism by dynamically selecting the most relevant features based on strong or weak complementary relationships, respectively.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Model",
      "text": "A) Notations: For an input video sub-sequence S, L nonoverlapping video clips are uniformly sampled, and the corresponding deep feature vectors are obtained from the pretrained models of audio and visual modalities. Let X a and X v denote the deep feature vectors of audio and visual modalities, respectively for the given input video subsequence S of fixed size, which is expressed as\n\nwhere d a and d v represent the dimensions of the audio and visual feature vectors, respectively, and x l a and x l v de- notes the audio and visual feature vectors of the video clips, respectively, for l = 1, 2, ..., L clips.\n\nB) Preliminary-Cross Attention: In this section, we briefly introduce the cross-attention (CA)  [2]  (baseline fusion model) as a preliminary to the proposed model. Given the audio and visual feature vectors X a and X v for a video subsequence S, the cross-correlation across the modalities are computed as Z = X ⊤ a W X v where W ∈ R da×dv represents cross-correlation weights among the audio and visual features. Next, we compute the CA weights of audio and visual features, A a and A v by applying column-wise softmax of Z and Z ⊤ , respectively. After obtaining the CA weights, they are used to obtain the attention maps of the audio and visual features as\n\nThe attention maps are added to the corresponding features to obtain the final cross-attended features X att,a and X att,v .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C) Dynamic Cross-Attention Model:",
      "text": "The proposed DCA model is depicted in Fig.  2  with vanilla CA as the baseline.\n\nGiven the cross-attended and unattended features of audio and visual modalities, we design a gating layer using a fully connected layer for each modality separately to obtain the attention weights for the attended and unattended features, which are given by\n\nwhere W gl,a ∈ da×2 , W gl,v ∈ R dv×2 are the learnable weights of the gating layers and Y go,a ∈ R L×2 , Y go,v ∈ R L×2 are outputs of gating layers of audio and visual modalities, respectively. To obtain probabilistic attention scores, the output of the gating layers is activated using a softmax function with a small temperature T , as given by\n\nwhere G a ∈ R L×2 , G v ∈ R L×2 denotes the probabilistic attention scores of audio and visual modalities. K denotes the number of output units of the gating layer, which is 2, one for cross-attended features and one for unattended features. These gating weights allow the selection of attended or unattended features dynamically based on the strong or weak complementary relationships, respectively. The parameter T acts as a softmax temperature on the output of the gating layer to provide a regularization effect. In our experiments, we have empirically set the value of T to 0.1 (see supplementary material for ablation study on T ). Ideally, the gating output for unattended features is 0 for samples with strong complementary relationships (vice-versa), allowing only the crossattended features, which is the same as the CA model  [2] . By using a small temperature T , the non-selected unattended feature, which acts as a noisy signal gets a small weightage, thereby providing a regularization effect  [20] . The probabilistic attention scores of the gating layer help to estimate the relevance of attended and unattended features for the accurate prediction of valence and arousal. The two columns of G a correspond to the probabilistic attention scores of unattended features (first column) and cross-attended features (second column). To multiply with the corresponding feature vectors, each column is replicated to match the dimension of the corresponding feature vectors, which is denoted by G a0 , G a1 and G v0 , G v1 for audio and visual modalities respectively. Now, the replicated attention scores are multiplied with the corresponding cross-attended and unattended features of the respective modalities, which is further fed to the ReLU activation function as:\n\nwhere ⊗ denotes element-wise multiplication. X att,ga and X att,gv denote the final attended feature vectors of audio and visual modalities, respectively obtained from the DCA model, which is further concatenated to obtain the A-V feature representation X att,g , followed by regression layers to get final predictions.   1 . More specifically, we have considered four baseline models: Cross-Attention (CA)  [2] , Joint Cross-Attention (JCA)  [12] , Recursive Joint Cross-Attention (RJCA)  [13] , and Transformerbased Cross-Attention (TCA)  [9] . First, we have implemented a simple baseline of the CA model to capture the complementary relationship between audio and visual modalities. Then we added the proposed DCA model to analyze the impact on the CA model, which has significantly improved the performance of the system. Similarly, we have also analyzed the impact of the proposed DCA model on other baseline models JCA, RJCA, and TCA. In all of these baselines, the fused A-V feature representations are influenced by the cross-modal interactions by using one modality to attend to the other modality (assuming strong complementary relationships). Since the proposed DCA model addresses the problem of weak complementary relationships, we can observe that the proposed model consistently improves the performance over all baselines. In particular, the proposed DCA model shows a significant improvement with CA and TCA over that of JCA and RJCA. This can be due to the fact that CA and TCA models rely only on cross-modal attention, where audio modality is used to attend to visual modality and vice-versa. In the case of JCA and RJCA, the attention weights for each modality are based on both intra-and inter-modal relationships using the joint feature representation, thereby reducing the impact of noisy or restrained modality. We have also observed a similar trend of performance improvement for the ablation study on the RECOLA dataset, which is provided in supplementary material. E) Comparison to state-of-the-art: In all these experiments, we performed multiple runs and took an average of three runs to have statistically stable results. RECOLA: Table  2  shows the performance comparison of the proposed approach with the relevant approaches on the development set of the RECOLA dataset. Since the test set is no longer supported for RECOLA, we have evaluated only the development set. Although Tzirakis et al.  [24]  explored deep features with LSTM-based fusion on the concatenated features, the fusion performance is not better than that of individual modalities. The performance has been improved by Ortega et al.  [25]  using pre-trained models on FER2013 dataset for visual, and Low-Level Descriptors (LLD) for audio. Schoneveld et al.  [23]  further improved the fusion performance using knowledge distillation for visual features, and a VGG network for audio features. However, these methods fail to effectively capture the complementary intermodal relationships. Unlike these approaches, Praveen et al.  [2, 12, 13]  explored CA to capture the complementary relationship across audio and visual modalities and showed improvement over prior methods. Although  [2, 12, 13]  achieved  superior performance, they failed to deal with weak complementary relationships. By deploying the proposed DCA model, we have achieved a further improvement in the fusion performance for both valence and arousal.\n\nAff-Wild2: Table  3  shows the performance of the proposed approach against the relevant state-of-the-art A-V fusion models on both development and test set partitions of the Aff-Wild2 dataset. Most of the related work on this dataset has been submitted to the ABAW challenges  [22] . Therefore we compare the proposed approach with the relevant stateof-the-art models of ABAW challenges for A-V fusion in dimensional ER. Zhang et al.  [10]  exploited audio as a supplementary channel to boost the performance of visual modality. Meng et al.  [8]  showed significant improvement in both development and test sets by leveraging three external datasets and multiple backbones for audio and visual modalities using an ensemble of LSTMs and transformers. Similarly, Zhou et al.  [7]  and Zhang et al.  [11]  also explored multiple backbones to achieve better generalization and improved performance on the test set. However, both of these methods  [7]  and  [11]  rely on a naive fusion approach based on transformers. Most approaches  [11, 7, 8]  explored multiple sophisticated backbones with ensemble models and external datasets to attain better generalization ability and improve test set performance. Unlike these approaches, Praveen et al.  [12]  focused on improving the fusion performance by effectively captur-ing both intra-and inter-modal relationships across the audio and visual modalities by deploying a joint representation in the CA framework. They further improved the fusion performance by introducing LSTMs and recursive fusion to obtain robust A-V feature representations  [13] . Though  [12]  and  [13]  do not outperform the performance on the test set compared to that of  [7, 11, 8] , their performance can be solely attributed by the sophisticated fusion models based on CA as they use only single backbones for each modality without any external datasets. Since the primary focus of our work is to improve the fusion performance by handling weak complementary relationships, we have evaluated the proposed model on these sophisticated CA-based fusion models  [13] . By deploying the proposed DCA model on the RJCA model  [13] , we can observe that the fusion performance has been further improved on both development and test sets. The performance improvement is more emphasized in the test set, which can be attributed to the regularization effect provided by the variable T in Eq. 3. To have a fair comparison, we also compared the fusion models (ensemble of transformers and LSTMs) of other state-of-the-art methods  [7, 11, 8]  using our backbones (see supplementary material). Since the Aff-Wild2 dataset is obtained from extremely challenging environments, the videos are highly vulnerable to exhibiting weak complementary relationships. Therefore, the improvement in fusion performance of the proposed DCA model is relatively better than that of the RECOLA dataset. F) Qualitative Evaluation: We also provided interpretability analysis for a better understanding of the proposed model using the visualization of the predictions of valence and arousal of proposed model along with the considered baselines (as shown in Fig.  3 ). We have chosen the subject named \"21-24-1920x1080\" from the validation set of the Aff-Wild2 dataset to demonstrate the superiority of the proposed model in handling weak complementary relationships. All the baseline models can track the ground truth of the dimensional emotions by leveraging the complementary relationships across the modalities. Since JCA and RJCA exploit both intra-and inter-modal relationships using joint representation, they are able to perform relatively better than CA and TCA by closely tracking the ground truth. However, all these baseline models do not deal with the problem of weak complementary relationships between the audio and visual modalities. Since the proposed DCA model handles the problem of weak complementary relationships while retaining the potential of strong complementary relationships, the proposed DCA model is able to effectively track the ground truth better than all the baseline models. As we can observe in Fig.  3 , even though the facial expressions of some of the frames (2600 to 3000) are corrupted due to extreme pose and blur, the proposed DCA model is still able to closely track the variations of the ground truth by leveraging the rich vocal expressions without being contaminated. All the baseline models fail to track the variations in the ground truth as the rich vocal expressions are contaminated by the noisy visual modality, especially for valence (see more visualizations in supplementary material). Since we perform smoothening operations on predictions, they do not follow sudden fluctuations in ground truth.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we investigated the issues with weak complementary relationships across audio and visual modalities in the framework of CA for ER. To address this issue, we proposed a simple, yet efficient DCA model to effectively capture the inter-modal relationships by handling the problem of weak complementary relationships while retaining the benefit of strong complementary relationships. By adaptively choosing the most relevant features of the individual modalities based on the gated attention scores, the proposed model is able to adapt to both strong and weak complementary relationships. Experimental results demonstrate the superiority of the proposed model on the considered baseline models on two datasets. By levering advanced backbones for audio and visual modalities, we can also expect significant improvement in fusion performance on test sets.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Attention scores based on cross-attention for the sub-",
      "page": 1
    },
    {
      "caption": "Figure 1: We can observe that the audio and visual modalities demon-",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of the proposed Dynamic Cross-Attention (DCA) model with vanilla Cross-Attention (CA) as the baseline.",
      "page": 3
    },
    {
      "caption": "Figure 2: with vanilla CA as the baseline.",
      "page": 3
    },
    {
      "caption": "Figure 3: Visualization of the predictions of valence and arousal",
      "page": 5
    },
    {
      "caption": "Figure 3: ). We have chosen the subject named ”21-24-",
      "page": 5
    },
    {
      "caption": "Figure 3: , even though the",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: More dio. Schoneveld et al. [23] further improved the fusion",
      "data": [
        {
          "Method": "Tzirakis et al. [24]",
          "Type of Fusion": "LSTM",
          "Valence": "0.502",
          "Arousal": "0.731"
        },
        {
          "Method": "Ortega et al. [25]",
          "Type of Fusion": "Feature Concat",
          "Valence": "0.565",
          "Arousal": "0.749"
        },
        {
          "Method": "Schoneveld et al. [23]",
          "Type of Fusion": "LSTM",
          "Valence": "0.630",
          "Arousal": "0.810"
        },
        {
          "Method": "Praveen et al [2]",
          "Type of Fusion": "CA",
          "Valence": "0.687",
          "Arousal": "0.831"
        },
        {
          "Method": "Praveen et al.[12]",
          "Type of Fusion": "JCA",
          "Valence": "0.728",
          "Arousal": "0.842"
        },
        {
          "Method": "Praveen et al.[13]",
          "Type of Fusion": "RJCA",
          "Valence": "0.736",
          "Arousal": "0.855"
        },
        {
          "Method": "Ours",
          "Type of Fusion": "RJCA + DCA",
          "Valence": "0.743",
          "Arousal": "0.867"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: More dio. Schoneveld et al. [23] further improved the fusion",
      "data": [
        {
          "Method": "CA [2] w/o DCA",
          "Valence": "0.541",
          "Arousal": "0.517"
        },
        {
          "Method": "CA [2] w/ DCA",
          "Valence": "0.624",
          "Arousal": "0.582"
        },
        {
          "Method": "TCA [9] w/o DCA",
          "Valence": "0.564",
          "Arousal": "0.543"
        },
        {
          "Method": "TCA [9] w/ DCA",
          "Valence": "0.635",
          "Arousal": "0.621"
        },
        {
          "Method": "JCA [12] w/o DCA",
          "Valence": "0.657",
          "Arousal": "0.580"
        },
        {
          "Method": "JCA [12] w/ DCA",
          "Valence": "0.679",
          "Arousal": "0.612"
        },
        {
          "Method": "RJCA [13] w/o DCA",
          "Valence": "0.721",
          "Arousal": "0.694"
        },
        {
          "Method": "RJCA [13] w/ DCA",
          "Valence": "0.742",
          "Arousal": "0.718"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: shows the performance of the pro-",
      "data": [
        {
          "Method": "",
          "Type of\nFusion": "",
          "Validation Set": "Valence",
          "Test Set": "Valence"
        },
        {
          "Method": "Zhang et al. [10]",
          "Type of\nFusion": "Leader Follower Attention",
          "Validation Set": "0.469",
          "Test Set": "0.463"
        },
        {
          "Method": "Karas et al. [6]",
          "Type of\nFusion": "LSTM + Transformers",
          "Validation Set": "0.388",
          "Test Set": "0.418"
        },
        {
          "Method": "Meng et al. [8]",
          "Type of\nFusion": "LSTM + Transformers",
          "Validation Set": "0.605",
          "Test Set": "0.606"
        },
        {
          "Method": "Zhou et al. [7]",
          "Type of\nFusion": "TCN + Transformers",
          "Validation Set": "0.550",
          "Test Set": "0.566"
        },
        {
          "Method": "Zhang et al [11]",
          "Type of\nFusion": "Transformers",
          "Validation Set": "0.648",
          "Test Set": "0.523"
        },
        {
          "Method": "Praveen et al.[12]",
          "Type of\nFusion": "JCA",
          "Validation Set": "0.657",
          "Test Set": "0.451"
        },
        {
          "Method": "Praveen et al.[13]",
          "Type of\nFusion": "RJCA",
          "Validation Set": "0.721",
          "Test Set": "0.467"
        },
        {
          "Method": "Ours",
          "Type of\nFusion": "RJCA + DCA",
          "Validation Set": "0.742",
          "Test Set": "0.507"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "D Krishna",
        "Ankita Patil"
      ],
      "year": "2020",
      "venue": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks"
    },
    {
      "citation_id": "3",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "R Gnana Praveen",
        "Eric Granger",
        "Patrick Cardinal"
      ],
      "year": "2021",
      "venue": "FG"
    },
    {
      "citation_id": "4",
      "title": "Multimodal learning with transformers: A survey",
      "authors": [
        "Peng Xu",
        "Xiatian Zhu",
        "David Clifton"
      ],
      "year": "2023",
      "venue": "TPAMI"
    },
    {
      "citation_id": "5",
      "title": "Leaky gated cross-attention for weakly supervised multi-modal temporal action localization",
      "authors": [
        "Jun-Tae Lee",
        "Sungrack Yun",
        "Mihir Jain"
      ],
      "year": "2022",
      "venue": "Leaky gated cross-attention for weakly supervised multi-modal temporal action localization"
    },
    {
      "citation_id": "6",
      "title": "M2lens: Visualizing and explaining multimodal models for sentiment analysis",
      "authors": [
        "Xingbo Wang",
        "Jianben He",
        "Zhihua Jin",
        "Muqiao Yang",
        "Yong Wang",
        "Huamin Qu"
      ],
      "year": "2022",
      "venue": "TVCG"
    },
    {
      "citation_id": "7",
      "title": "Timecontinuous audiovisual fusion with recurrence vs attention for in-the-wild affect recognition",
      "authors": [
        "Vincent Karas",
        "Mani Kumar Tellamekala",
        "Adria Mallol-Ragolta",
        "Michel Valstar",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "Timecontinuous audiovisual fusion with recurrence vs attention for in-the-wild affect recognition"
    },
    {
      "citation_id": "8",
      "title": "Leveraging tcn and transformer for effective visual-audio fusion in continuous emotion recognition",
      "authors": [
        "Weiwei Zhou",
        "Jiada Lu",
        "Zhaolong Xiong",
        "Weifeng Wang"
      ],
      "year": "2023",
      "venue": "Leveraging tcn and transformer for effective visual-audio fusion in continuous emotion recognition"
    },
    {
      "citation_id": "9",
      "title": "Valence and arousal estimation based on multimodal temporalaware features for videos in the wild",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Wenqiang Jiang",
        "Tenggan Zhang",
        "Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "Valence and arousal estimation based on multimodal temporalaware features for videos in the wild"
    },
    {
      "citation_id": "10",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "IEEE SLT Workshop"
    },
    {
      "citation_id": "11",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "Su Zhang",
        "Yi Ding",
        "Ziquan Wei",
        "Cuntai Guan"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "12",
      "title": "Multi-modal facial affective analysis based on masked autoencoder",
      "authors": [
        "Wei Zhang",
        "Bowen Ma",
        "Feng Qiu",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "IEEE CVPRW"
    },
    {
      "citation_id": "13",
      "title": "Audiovisual fusion for emotion recognition in the valence-arousal space using joint cross-attention",
      "authors": [
        "Patrick Gnana Praveen",
        "Eric Cardinal",
        "Granger"
      ],
      "year": "2023",
      "venue": "TBIOM"
    },
    {
      "citation_id": "14",
      "title": "Recursive joint attention for audio-visual fusion in regression based emotion recognition",
      "authors": [
        "Eric Gnana Praveen",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Not all attention is needed: Gated attention network for sequence data",
      "authors": [
        "Lanqing Xue",
        "Xiaopeng Li",
        "Nevin Zhang"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "16",
      "title": "Gated mechanism for attention based multi modal sentiment analysis",
      "authors": [
        "Ayush Kumar",
        "Jithendra Vepa"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Embrace smaller attention: Efficient cross-modal matching with dual gated attention fusion",
      "authors": [
        "Weikuo Guo",
        "Xiangwei Kong"
      ],
      "year": "2023",
      "venue": "Proc. of IEEE ICASSP"
    },
    {
      "citation_id": "18",
      "title": "Gated attention fusion network for multimodal sentiment classification",
      "authors": [
        "Yongping Du",
        "Yang Liu",
        "Zhi Peng",
        "Xingnan Jin"
      ],
      "year": "2022",
      "venue": "Gated attention fusion network for multimodal sentiment classification"
    },
    {
      "citation_id": "19",
      "title": "Audio-visual gated-sequenced neural networks for affect recognition",
      "authors": [
        "Decky Aspandi",
        "Federico Sukno",
        "Bjorn Schuller",
        "Xavier Binefa"
      ],
      "year": "2022",
      "venue": "Audio-visual gated-sequenced neural networks for affect recognition"
    },
    {
      "citation_id": "20",
      "title": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "Pengfei Liu",
        "Kun Li",
        "Helen Meng"
      ],
      "year": "2020",
      "venue": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition"
    },
    {
      "citation_id": "21",
      "title": "Regularizing deep neural networks by noise: Its interpretation and optimization",
      "authors": [
        "Hyeonwoo Noh",
        "Tackgeun You",
        "Jonghwan Mun",
        "Bohyung Han"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "22",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "FG"
    },
    {
      "citation_id": "23",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges"
    },
    {
      "citation_id": "24",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "Schoneveld Liam",
        "Alice Othmani",
        "Abdelkawy Hazem"
      ],
      "year": "2021",
      "venue": "PR Letters"
    },
    {
      "citation_id": "25",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "Mihalis Nicolaou",
        "Björn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2017",
      "venue": "JSTSP"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "D Juan",
        "Patrick Ortega",
        "Alessandro Cardinal",
        "Koerich"
      ],
      "year": "2019",
      "venue": "SMC"
    }
  ]
}