{
  "paper_id": "2506.18716v1",
  "title": "Multi-Modal Anchor Gated Transformer With Knowledge Distillation For Emotion Recognition In Conversation",
  "published": "2025-06-23T14:53:22Z",
  "authors": [
    "Jie Li",
    "Shifei Ding",
    "Lili Guo",
    "Xuan Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) aims to detect the emotions of individual utterances within a conversation. Generating efficient and modality-specific representations for each utterance remains a significant challenge. Previous studies have proposed various models to integrate features extracted using different modality-specific encoders. However, they neglect the varying contributions of modalities to this task and introduce high complexity by aligning modalities at the frame level. To address these challenges, we propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation (MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance textual modality representations, while knowledge distillation is utilized to strengthen representations of weaker modalities. Furthermore, we introduce a multi-modal anchor gated transformer to effectively integrate utterance-level representations across modalities. Extensive experiments on the IEMOCAP and MELD datasets demonstrate the effectiveness of knowledge distillation in enhancing modality representations and achieve stateof-the-art performance in emotion recognition. Our code is available at: https://github.com/JieLi-dd/ MAGTKD.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion plays a pivotal role in human communication, influencing not only the content but also the tone and context of interactions. Emotion Recognition in Conversation (ERC) aims to identify the emotional states expressed in each utterance within a dialogue. This task is essential for applications in areas such as conversational agents, healthcare systems, and recommendation engines. While emotions are traditionally expressed through text, they are also richly conveyed in the audio and visual modalities  [Poria et al., 2017; Wu et al., 2025] . Figure  1  provides an illustrative example of multi-modal ERC, showcasing how information from various modalities can be integrated to improve emotion recognition.\n\nOk, Paulo, why don't you just go get dressed, and then you be on your way, ok, bye-bye.\n\n[joy]\n\nRachel, how did this happen?  [neutral]  I don't know, I just kinda ran into him last night.\n\n[fear]\n\nWhere?  [neutral]  Figure  1 : Multi-modal conversation example from the MELD dataset.\n\nRecent research in ERC has primarily focused on individual modalities. Text-based models often leverage context modeling  [Majumder et al., 2019; Song et al., 2022; Hu et al., 2023; Yang et al., 2024]  or incorporate external knowledge  [Zhong et al., 2019; Zhu et al., 2021; Lee and Lee, 2022; Wang et al., 2025] . Audio-based models make use of multi-task learning, attention mechanisms, and data augmentation strategies  [Latif et al., 2023; He et al., 2025; Guo et al., 2025] , while video-based models extract key frames to enhance emotion recognition  [Wei et al., 2021; Poria et al., 2017] . However, relying solely on a single modality for emotion recognition can overlook crucial emotional cues embedded in other modalities, leading to suboptimal performance. This limitation has spurred increasing interest in multi-modal ERC. Existing multi-modal models typically extract frame-level features for each modality, align these features across modalities, and fuse them for emotion classification  [Tsai et al., 2019; Guo et al., 2022; Zheng et al., 2023] . While effective, these approaches often treat all modalities equally, disregarding the varying significance of each modality in emotion recognition. Furthermore, the complex alignment process increases the computational burden, making these models less suitable for deployment in resource-constrained environments. To address these challenges, a more efficient and adaptive approach to modality representation and integration is needed.\n\nPrompt-based learning has recently gained attention for its success in both natural language processing (NLP) and multimodal tasks. In ERC tasks focusing on textual data, welldesigned prompts can effectively guide models to extract relevant contextual information, thereby improving the quality of utterance-level features  [Song et al., 2022; Son et al., 2022; Yun et al., 2024] . Additionally, knowledge distillation techniques have been widely explored to enhance the performance of student models by enabling them to mimic better the representations learned by teacher models  [Lin et al., 2022; Li et al., 2023; Yun et al., 2024; Ma et al., 2024] . To overcome the challenges in multi-modal ERC, we introduce the Multi-modal Anchor Gated Transformer with Knowledge Distillation (MAGTKD), a framework designed to improve the integration of multi-modal information for emotion recognition tasks. Specifically, MAGTKD leverages context-aware prompts to extract high-quality utterance-level textual representations. These robust textual features are then used within a knowledge distillation framework to enhance the representation capacity of weaker modalities (e.g., audio and video), ultimately improving the fusion of multi-modal features for emotion recognition. In contrast to existing methods, which rely on frame-level feature interactions before fusion-thereby increasing computational complexity  [Tsai et al., 2019; Guo et al., 2022; Zheng et al., 2023] -MAGTKD directly fuses utterance-level features post-interaction, addressing the computational burden while maintaining or even improving performance. We evaluate MAGTKD on two widely-used benchmark datasets, IEMOCAP and MELD. Experimental results demonstrate that MAGTKD achieves state-of-the-art performance on both datasets, surpassing existing methods in both accuracy and efficiency.\n\nThe key contributions of this work are as follows:\n\n• We propose MAGTKD, a novel framework for ERC that effectively integrates multi-modal features, taking into account the varying contributions of different modalities to emotion classification.\n\n• MAGTKD significantly reduces model complexity compared to traditional frame-level feature fusion methods.\n\n• MAGTKD sets new benchmarks for ERC, achieving superior performance on the IEMOCAP and MELD datasets.\n\n2 Related Works",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Prompt Learning",
      "text": "Prompt Learning has emerged as an effective approach for leveraging pre-trained models by designing task-specific prompts to fine-tune and integrate them for downstream tasks, enabling improved modality representations. It has been widely adopted across various NLP tasks  [Gao et al., 2021; Heinzerling and Inui, 2021; Xu et al., 2023] . Recently, researchers have begun exploring the application of prompt learning in multi-modal settings  [Tsimpoukelli et al., 2021; Khattak et al., 2023; Zhu et al., 2023] .  [Tsimpoukelli et al., 2021]  presents a simple, yet effective, approach for transferring this few-shot learning ability to a multi-modal setting (vision and language).  [Khattak et al., 2023]  proposes Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations.  [Zhu et al., 2023]  develop Visual Prompt multi-modal Tracking (ViPT), which learns the modal-relevant prompts to adapt the frozen pre-trained foundation model to various downstream multi-modal tracking tasks. With success in diverse NLP and multi-modal learning applications, we extend prompt learning to the emotion recognition task, aiming to harness its potential for enhancing emotional feature extraction and representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Konwledge Distillation",
      "text": "Knowledge Distillation (KD) aims to transfer knowledge from a large teacher network to a smaller student network. This knowledge transfer typically occurs at three levels: soft labels of the final layer  [Hinton et al., 2015] , intermediatelayer features  [Romero et al., 2015] , and the relationships between features across layers  [Yim et al., 2017] . Based on the learning strategy, KD can be categorized into offline  [Passalis and Tefas, 2018; Li et al., 2020]  and online  [Zhang et al., 2018; Chung et al., 2020]  distillation. In offline distillation, the teacher model is pre-trained to guide the student model's learning. In contrast, online distillation involves simultaneous training of the teacher and student models with joint parameter updates. KD has demonstrated its effectiveness in transferring knowledge across modalities in multi-modal research  [Albanie et al., 2018] . Motivated by this, we adapt KD techniques to the multi-modal ERC task, enabling efficient knowledge transfer between modalities to enhance emotion recognition performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modal Fusion",
      "text": "In the domain of modality fusion, existing works predominantly focus on extracting frame-level features and performing feature interactions at this granularity.  [Tsai et al., 2019]  introduces the Multi-modal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data.  [Zheng et al., 2023]  extracts three modal frame-level features and uses an attention mechanism to perform alignment operations on the three modal features. However, frame-level feature alignment often introduces significant computational complexity. Unlike these approaches, our work adopts utterance-level feature extraction and designs a novel model for multi-modal feature fusion, effectively reducing complexity while maintaining strong performance in emotion recognition tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "To enhance the representation of each modality and achieve effective multi-modal fusion, we propose the MAGTKD model for the ERC task. Figure  2  illustrates the overall architecture of the proposed framework, with detailed descriptions provided in the following subsections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Task Definition",
      "text": "Given a set of speakers S, utterances U , and emotion labels Y , a conversation consisting of k utterances is represented as  speakers and y m , y n ∈ Y are one of the predefined emotion categories. If i = j, s i and s j represent the same speaker. Moreover, u k ∈ U represents the k-th utterance. Each utterance u k = {t k , a k , v k } contains three modalities, where t, a, v represent text, audio and video, respectively. The goal of ERC is to predict to which emotion label y m the utterance u k belongs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "Figure  2  illustrates the extraction of modality-specific features using dedicated encoders for each input modality. In this section, we explain the feature generation process in detail:\n\nText: Following prior work [Lee and Lee, 2022; Song et al., 2022], we utilize prompt learning to model both the context and speaker information. For the text encoder, we adopt RoBERTa  [Liu et al., 2019] . We construct the contextual representation C k and the prompt P k as follows.\n\n(3) where < mask > represents a special token, F t k ∈ R 1×d is embedding of < mask >, representing the aggregated emotion feature, and d is the hidden dimension of the < mask > token.\n\nAudio: Self-supervised learning has achieved remarkable success not only in natural language processing but also in audio and video domains  [Baevski et al., 2020; Baevski et al., 2022] . For the audio encoder, we employ Date2vec  [Baevski et al., 2022] , with the audio segment a k of the k-th utterance as input. The process of extracting audio features is formalized as follows.\n\nwhere F a k ∈ R 1×d is the embedding of a k , and d is the hidden dimension of audio features.\n\nVideo: Similar to the audio feature extraction process, we utilize Timesformer  [Bertasius et al., 2021]  as the video encoder. The process of extracting video features is formalized as follows.\n\n) where v k is the video input, and F v k ∈ R 1×d is the embedding of v k , and d is the hidden dimension of video features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "Unlike traditional knowledge distillation methods that utilize KL divergence, the multi-modal ERC task involves crossmodal knowledge distillation. We adopt a collaborative distillation strategy based on soft labels and intermediate-layer features, using Pearson correlation coefficients as our crossmodal measurement approach.\n\n) where p(u, v) is the Pearson correlation coefficient between two logit vectors u and v.\n\nSoft Label-Based Distillation leverages the soft label outputs from the last layer of each modality encoder to compute the knowledge divergence across modalities at both the sample and feature levels. Pearson correlation coefficients are employed to measure the degree of knowledge disparity between different modalities. By reducing this disparity, the textual features transfer knowledge to the audio features. The process is formalized as:\n\nY a i,: = sof tmax(P a i,: /τ ) (8)\n\nwhere B is a training batch, C is the emotion categories, P t , P a ∈ R B×C are the prediction matrix of text and audio modality, respectively. τ is a temperature parameter to control the softness of logits. Intermediate-Layer Feature-Based Distillation computes similarity matrices within a batch for the textual modality as the target matrix (via dot product between text modality features and their transpose). Similarly, a source matrix is computed for the audio and text modalities. Using the softmax function, we derive the target and source distributions. The KL divergence between these distributions is minimized to enable the transfer of knowledge from textual features to audio features. The process is defined as:\n\n, ∀i, j ∈ B (10)\n\n, ∀i, j ∈ B (11)\n\nwhere F i,j , F ′ i,j ∈ R B×B are the text-modal similarity matrix and the text-audio modal similarity matrix, respectively. T i , S i are target and source distributions.\n\nThe overall loss function includes the above two losses and the cross-entropy loss:\n\n) where y i is true labels and p i is predict labels.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Modal Anchor Gated Transformer",
      "text": "In the first stage, we utilize prompt learning and knowledge distillation to extract utterance-level features for each modality. However, directly concatenating these features for emotion recognition, in figure  3 , results in degraded model performance. To address this issue, we propose a second stage that employs a Multi-modal Anchor Gated Transformer (MAGT) to effectively integrate features across the three modalities. Specifically, each modality serves as an anchor to aggregate complementary information from other modalities. Specifically, we first use the audio and video features as anchors to aggregate information from the other modalities. Given the strong performance of the text modality, the raw text features are used as an anchor to aggregate the audio and video features enriched by other modalities.\n\nWe construct the dataset at the conversation level, where utterance-level features from different modalities are arranged sequentially based on temporal order. For each utterance, speaker and positional embeddings are added. Speaker embedding uniquely maps each speaker in the dataset to a sequence ID, which is then embedded using an embedding layer.\n\nwhere P E is positional embedding and SE is speaker embedding. s u k represents speaker of utterance u k . F m = (F m1 , F m2 , . . . , F m k ) and m ∈ {t, a, v}. H m is the positional-and speaker-aware utterance sequence representation for m modality.\n\nGiven the remarkable success of Transformers in NLP, the effectiveness of gating mechanisms for preserving salient features, and the superior performance of the text modality compared to audio and video in ERC tasks, we introduce an Anchor-Gated Transformer. This structure leverages a Transformer encoder with three inputs (Query Q ∈ R lq×d k , Key K ∈ R l k ×d k , and Value V ∈ R l k ×d k ) to integrate features across modalities.\n\nwhere m ∈ {t, a, v} and n ∈ {t, a, v} -{m}. H m→m represents anchor modality aggregating its own information.\n\nH n→m represents an anchor modality aggregating information from other modalities.\n\nTo enhance the emotional representation of each modality, we incorporate a gating mechanism to filter out irrelevant information and retain the most effective emotional features.\n\nwhere W n→m is a weight matrix, b n→m is a bias parameter, α n→m represents gate, and ⊗ is the element-wise product.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Classifier",
      "text": "The emotion classification task is performed using a linear layer. The features t ′ , extracted from the Multi-modal Attention and Graph-based Transformer (MAGT), are transformed into the emotion label p i corresponding to each utterance u i .\n\nThe classification process can be formalized as follows:\n\nwhere W ∈ R C×d and b ∈ R C are the weight matrix and bias vector of the linear layer, respectively. Here, C denotes the number of emotion classes, and d represents the dimension of the feature vector t ′ . The softmax function ensures that the outputs are normalized probabilities across all emotion classes, and argmax selects the class with the highest probability as the predicted label p i .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training",
      "text": "In the first stage, the utterance-level feature extraction is optimized using the loss function defined in Equation  14 . In the second stage, the multi-modal fusion is performed using the knowledge distillation (KD) loss functions defined in Equations 9 and 12. These can be formalized as:\n\nwhere i refers to either the audio or video modality. The total loss function in the second stage is the sum of the cross-entropy loss and the two distillation loss functions, weighted by their respective coefficients. This can be expressed as:\n\nwhere α and β are the coefficients for the distillation losses of the audio and video modalities, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "In this section, we introduce two widely adopted benchmark datasets: MELD and IEMOCAP. Following descriptions for specific details of these two datasets: MELD  [Poria et al., 2019]  is a multiparty conversation dataset containing over 1,400 dialogues and more than 13,000 utterances extracted from the TV show \"Friends.\" This dataset includes seven emotion categories: neutral, surprise, fear, sadness, joy, disgust, and anger.\n\nIEMOCAP  [Busso et al., 2008]  consists of 7,433 utterances and 151 dialogues, divided into five sessions, each involving two speakers. Each utterance is labeled with one of six emotion categories: happiness, sadness, anger, excitement, frustration, and neutral. The training and development datasets are randomly split from the first four sessions in a 9:1 ratio. The test dataset comprises the last session.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We evaluate the performance of our model on two datasets using Accuracy (Acc) and Weighted F1-score (W F1) as metrics. We designed a two-stage experimental process. The first stage focuses on extracting utterance-level features from different modalities, while the second stage involves multimodal feature fusion using a dataset constructed at the conversation level. The hyperparameter settings are shown in Table  1 . All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Stage",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines",
      "text": "We compare our proposed model, MAGTKD, against classic baselines, including DialogueRNN  [Majumder et al., 2019] ,  DialogueGCN [Ghosal et al., 2019] , MMGCN  [Hu et al., 2021b] , and DialogueCRN  [Hu et al., 2021a] , as well as stateof-the-art models such as A-DMN  [Xing et al., 2022] , Dia-logueINAB  [Ding et al., 2023] , SACCMA  [Guo et al., 2024] , Ada2I  [Nguyen et al., 2024] , and GraphCPC  [Li et al., 2024] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparative Experiments",
      "text": "Table  2  compares our model with prior works on IEMO-CAP and MELD. Our model achieves the best performance on both datasets, setting new state-of-the-art (SOTA) results.\n\nOn IEMOCAP, we achieve 69.38% accuracy and 69.59% weighted F1 (W F1), outperforming GraphCFC by 0.99% and 1.56%, respectively. On MELD, our model achieves 66.36% accuracy and 65.32% W F1, with improvements of 5.17% and 6.83% over Ada2I, the previous SOTA. These results demonstrate our model's ability to effectively integrate multi-modal features and handle challenges in conversational emotion recognition through prompt learning, knowledge distillation, and advanced fusion techniques.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Visualization And Analysis",
      "text": "Figure  4  shows t-SNE visualizations of the feature representations from the IEMOCAP and MELD datasets. We visualize single-modal features (text, audio, and video) as well as features enhanced by knowledge distillation, where the text modality guides the audio and video modalities.\n\nThe visualizations indicate that the text modality has the strongest discriminative power across both datasets, followed by audio, while video shows the least discriminative ability. After applying knowledge distillation, the audio modality improves significantly, benefiting from the text modality's guid-  ance. However, the video modality struggles to learn effectively, highlighting the challenge of transferring knowledge to modalities with weaker feature representations. Furthermore, the audio modality in IEMOCAP shows stronger discriminative power compared to MELD, which results in better performance when learning from the text modality. In the next sections, we will provide quantitative results to further explore this trend.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "Table  3  shows the results of ablation studies on feature extraction and multi-modal fusion. We evaluate individual modalities and the effects of knowledge distillation from the text modality to audio and video modalities.\n\nThe results highlight that knowledge distillation improves the performance of the audio modality but has little effect on the video modality. This aligns with the observations in Figure  4 , where the video modality struggles to learn effectively from the text modality due to its weaker feature representation. On the other hand, the audio modality benefits more from the distillation process, particularly in the IEMOCAP dataset where it has a stronger feature representation compared to MELD.\n\nFor multi-modal fusion, we first tested the Concat method. The best performance was achieved by fusing the strongest modality features (text, distilled audio, and undistilled video). However, adding the distilled video modality degraded performance. We then tested the MAGT fusion method, which also performed best when fusing the strongest modalities. Notably, MAGT maintained stable performance even when the weakest modality, the distilled video, was included.\n\nThese results demonstrate that MAGT effectively integrates emotional cues from different modalities, even when some modalities contribute less useful information.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Complexity Analysis",
      "text": "whereas the utterance-level model's complexity is:\n\nThus, the relative temporal complexity is:\n\nFor shorter dialogues (U ≪ L), the proposed model has a significant reduction in complexity. Additionally, spatial complexity is reduced by a factor of L. In summary, the proposed model is more efficient, with linear spatial and quadratic temporal complexity in U , compared to the frame-level model's quadratic temporal complexity in L.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Module",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Hyper-Parametric Analysis",
      "text": "Figure  5  shows the effect of varying the hyperparameters α (audio distillation coefficient) and β (video distillation coefficient) on model performance for IEMOCAP and MELD. For IEMOCAP, changing α significantly affects performance when β is fixed, while adjusting β with a fixed α results in smaller variations. This suggests that the audio modality better benefits from knowledge distillation, whereas the video modality shows weaker learning. This aligns with the observations in Figure  4 , where the audio modality has better feature discriminability than the video modality. In MELD, setting α = 0 and increasing β initially improves performance, but further increases lead to a decline. Similarly, setting β = 0 and varying α shows an initial performance boost  followed by a decrease, confirming that the KD loss improves model performance but requires careful tuning.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Convergence Analysis",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "The proposed MAGTKD model effectively addresses the challenges of multi-modal ERC by leveraging prompt learning to extract robust textual representations and employing knowledge distillation to enhance weaker modalities. The subsequent use of MAGT enables efficient aggregation of emotional information across modalities, resulting in stateof-the-art performance on both the MELD and IEMOCAP datasets. Future work will explore extending MAGTKD to handle more complex multi-modal scenarios, such as incorporating dynamic contextual information in real-time conversations or addressing challenges posed by highly imbalanced datasets.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: provides an illustrative example of",
      "page": 1
    },
    {
      "caption": "Figure 1: Multi-modal conversation example from the MELD",
      "page": 1
    },
    {
      "caption": "Figure 2: illustrates the overall archi-",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of the architecture of MAGTKD.",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the extraction of modality-specific fea-",
      "page": 3
    },
    {
      "caption": "Figure 3: , results in degraded model perfor-",
      "page": 4
    },
    {
      "caption": "Figure 3: Architecture of the Concat model. Each modality passes",
      "page": 4
    },
    {
      "caption": "Figure 4: shows t-SNE visualizations of the feature represen-",
      "page": 5
    },
    {
      "caption": "Figure 4: t-SNE visualization of feature representations for IEMOCAP (top row) and MELD (bottom row) datasets. “kd” refers to knowledge",
      "page": 6
    },
    {
      "caption": "Figure 5: shows the effect of varying the hyperparameters α",
      "page": 7
    },
    {
      "caption": "Figure 4: , where the audio modality has better",
      "page": 7
    },
    {
      "caption": "Figure 5: (a) Hyperparametric analysis for IEMOCAP, (b) Hyper-",
      "page": 7
    },
    {
      "caption": "Figure 6: (a)-(b) show the variations in Total Loss and Knowledge",
      "page": 7
    },
    {
      "caption": "Figure 6: presents the convergence behavior of the model on",
      "page": 7
    },
    {
      "caption": "Figure 4: Similar patterns are observed for the",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 2: compares our model with prior works on IEMO-",
      "data": [
        {
          "Stage": "Feature\nExtraction",
          "Parameter": "Dimensions",
          "IEMOCAP": "768",
          "MELD": "768"
        },
        {
          "Stage": "",
          "Parameter": "Learning rate",
          "IEMOCAP": "1e-5",
          "MELD": "1e-5"
        },
        {
          "Stage": "",
          "Parameter": "Batch",
          "IEMOCAP": "4",
          "MELD": "4"
        },
        {
          "Stage": "",
          "Parameter": "Epochs",
          "IEMOCAP": "10",
          "MELD": "10"
        },
        {
          "Stage": "Multi-modal\nFusion",
          "Parameter": "Learning rate",
          "IEMOCAP": "1e-5",
          "MELD": "1e-4"
        },
        {
          "Stage": "",
          "Parameter": "Batch",
          "IEMOCAP": "16",
          "MELD": "16"
        },
        {
          "Stage": "",
          "Parameter": "Epochs",
          "IEMOCAP": "30",
          "MELD": "30"
        },
        {
          "Stage": "",
          "Parameter": "α, β",
          "IEMOCAP": "0.7, 0.8",
          "MELD": "0.01, 0.09"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "Albanie"
      ],
      "year": "2018",
      "venue": "ACM MM"
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Baevski"
      ],
      "year": "2020",
      "venue": "NIPS"
    },
    {
      "citation_id": "3",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Baevski"
      ],
      "year": "2022",
      "venue": "ICML"
    },
    {
      "citation_id": "4",
      "title": "Is space-time attention all you need for video understanding?",
      "authors": [
        "Bertasius"
      ],
      "year": "2008",
      "venue": "ICML"
    },
    {
      "citation_id": "5",
      "title": "Dialogueinab: an interaction neural network based on attitudes and behaviors of interlocutors for dialogue emotion recognition",
      "authors": [
        "Ding"
      ],
      "year": "2019",
      "venue": "EMNLP/IJCNLP"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition with multimodal transformer fusion framework based on acoustic and lexical information",
      "authors": [
        "Guo"
      ],
      "year": "2022",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "7",
      "title": "Heinzerling and Inui, 2021] Benjamin Heinzerling and Kentaro Inui. Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries",
      "authors": [
        "Guo"
      ],
      "year": "2015",
      "venue": "EACL"
    },
    {
      "citation_id": "8",
      "title": "DialogueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Hu"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "9",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Hu"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "10",
      "title": "Multi-modal prompt learning",
      "authors": [
        "Khattak"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "11",
      "title": "CoMPM: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Latif"
      ],
      "year": "2022",
      "venue": "NAACL"
    },
    {
      "citation_id": "12",
      "title": "Few sample knowledge distillation for efficient network compression",
      "authors": [
        "Li"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Curriculum temperature for knowledge distillation",
      "authors": [
        "Li"
      ],
      "year": "2023",
      "venue": "AAAI"
    },
    {
      "citation_id": "14",
      "title": "Graphcfc: A directed graph based crossmodal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "Li"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multimed"
    },
    {
      "citation_id": "15",
      "title": "Knowledge distillation via the target-aware transformer",
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "16",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "Ma"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multimed"
    },
    {
      "citation_id": "17",
      "title": "Alexander Gelbukh, and Erik Cambria. Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Majumder"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "18",
      "title": "Ada2i: Enhancing modality balance for multimodal conversational emotion recognition",
      "authors": [
        "Nguyen"
      ],
      "year": "2017",
      "venue": "ACM MM"
    },
    {
      "citation_id": "19",
      "title": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Poria"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "20",
      "title": "GRASP: Guiding model with RelAtional semantics using prompt for dialogue relation extraction",
      "authors": [
        "Romero"
      ],
      "year": "2015",
      "venue": "COLING"
    },
    {
      "citation_id": "21",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Tsai"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "22",
      "title": "Enriching multimodal sentiment analysis through textual emotional descriptions of visual-audio content",
      "authors": [
        "Tsimpoukelli"
      ],
      "year": "2021",
      "venue": "NIPS"
    },
    {
      "citation_id": "23",
      "title": "Efficient cross-task prompt tuning for few-shot conversational emotion recognition",
      "authors": [
        "Xu"
      ],
      "year": "2023",
      "venue": "EMNLP"
    },
    {
      "citation_id": "24",
      "title": "Disentangled variational autoencoder for emotion recognition in conversations",
      "year": "2024",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "25",
      "title": "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning",
      "authors": [
        "Yim"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "26",
      "title": "TelME: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "year": "2024",
      "venue": "NAACL"
    },
    {
      "citation_id": "27",
      "title": "Deep mutual learning",
      "authors": [
        "Zhang"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "28",
      "title": "A facial expression-aware multimodal multitask learning framework for emotion recognition in multiparty conversations",
      "authors": [
        "Zheng"
      ],
      "year": "2019",
      "venue": "EMNLP/IJCNLP"
    },
    {
      "citation_id": "29",
      "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "authors": [
        "Zhu"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "30",
      "title": "Visual prompt multi-modal tracking",
      "authors": [
        "Zhu"
      ],
      "year": "2023",
      "venue": "CVPR"
    }
  ]
}