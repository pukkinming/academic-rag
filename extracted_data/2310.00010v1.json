{
  "paper_id": "2310.00010v1",
  "title": "Artificial Empathy Classification: A Survey Of Deep Learning Techniques, Datasets, Scales And Evaluation",
  "published": "2023-09-04T16:02:59Z",
  "authors": [
    "Sharjeel Tahir",
    "Syed Afaq Shah",
    "Jumana Abu-Khalaf"
  ],
  "keywords": [
    "(2) an analysis module that parses conversation text"
  ],
  "sections": [
    {
      "section_name": "I. Introduction",
      "text": "The first use of empathy dates back to ancient Greece, when Aristotle was studying how people interact with each other and came up with the idea that we are all connected through a shared consciousness  [1] . This theory was later expanded upon by Plato in his book \"The Republic\", where he described how each person has their own world inside their head, which they can only see through the eyes of others  [1] . It can be simply portrayed using the words of the psychologist Theodore Lipps, that states empathy as \"feeling one's way into the experience of another\"  [2] .\n\nIdentify applicable funding agency here. If none, delete this. Fig.  1 . Levels/Aspects of empathy inspired by  Asada et al. [13]  emotional reactions associated with human empathy. Overall, the use of artificial empathy in literature is often diverse and imaginative, indicating how this subject has caught the imagination of scholars. While these representations are not necessarily scientifically accurate or anchored in real-world technology, they may be a good method to investigate the implications artificial comprehension and reaction to human emotions.\n\nWhile there are several reasons to endow artificial agents with empathy, it is especially vital because we want our robotic companions to learn what it is like to be human. Building robots with strong empathy skills can benefit the society by allowing machines to comprehend and comfort people in times of distress by picking up on cues about how they are feeling.  [17] . This may also assist them in learning how to behave correctly while interacting with people, allowing them to become more effective companions over time  [18] . Hence, there has been a lot of interest from the past couple of decades in using technology to make robots more emotionally intelligentfor example, by training them to recognize when someone is upset and then communicate or interact with them in such a way that makes them feel better (or at least less upset, as a human companion would do)  [18] .\n\nFrom medical advice tools to companion/social robots, empathy is a crucial aspect of a wide variety of platforms. For instance, people who live in isolation, such as the elderly population or healthcare users who are confined because of infectious diseases such as Covid-19, may benefit from the emotional support that can be provided by online platforms like CRECA (Context Representative Counseling Agent) and emotionallyintelligent companion robots like ARI  [19] . CRECA is a classic example of the several online chatbot-based assistants, that have largely been offered as an effective option to aid individuals with mental health difficulties. On the other hand, companion robots offer a wide range of uses, for instance, domestic chores, health monitoring in facilities such as aged care, and providing social assistance to autistic children. While the bulk of these uses require an emotionally competent robot, it is more vital that agents that participate in the delivery of basic medical care, be emotionally capable  [19] . Numerous studies have proven how emotional and empathic capabilities of a robot can enhance the experience of humanrobot interaction and collaboration  [9] . This is mainly because agents with empathy skills are seen as more compassionate and trustworthy than those without, and because they may inspire empathy in their users  [20] .\n\nThe distorted realities of AE: 1) Falsely claiming that current technologies are capable of artificial empathy when they are not.: Existing technologies, such as chatbots or collaborative robots, have been claimed to have \"artificial empathy\" by some academics, even if they do not entail the understanding or reaction to \"real human emotions\". For instance, \"emotionally intelligent\" robots are trained to perform a set number of actions in response to certain human emotions or actions  [21] . This may be deceptive since it may lead the user to believe that the technologies are more sophisticated and competent than they really are.\n\n2) Overselling the abilities of artificial agents.: Researchers have asserted that certain cutting-edge technology, such as affective computing systems and companion robots, can read and react to human emotions with remarkable resemblance to the way humans do (anthropomorphism). However, the accuracy and dependability of these systems are generally lacking, and they often rely on a cycle of discourse that may not incorporate the genuine concept of empathy  [22] .\n\n3) Not thinking about the moral consequences of AE.: The ethical and sociological ramifications of such technologies, i.e., purposely delivering damaging reactions to users of online Artificial empathy has the potential to improve people's lives, but it's important to look at the issue from all angles to ensure we do not overlook crucial aspects.\n\nThe surveys that have been previously carried out in the field of AE concentrate their attention, for the most part, on the description of empathy and experimental settings of the works that study implementation and evaluation of AE. For example, they explain the particular aspects, such as the cohort of engaged participants and how their responses were compiled and used  [6] . The goal of this paper is to review recent trends in the field of AE, especially the works that employ deep learning (DL) classification methods. Therefore, the studies considered in this survey are predominantly from after the advent of DL. Moreover, we analyse the existing metrics for evaluating empathy and highlight the desiderata for future AE evaluation metrics and benchmarks.\n\nTo the best of our knowledge, this is the first work to:\n\n• Review existing DL methods used to implement (detect and elicit) AE. • Analyse the existing scales of empathy and compare their performance with respect to Artificial Empathy. • Examine the extent to which the existing datasets can be used to train and evaluate AE models. We have grouped the existing works into sections as per the processes involved in the implementation of AE. Section II provides a review of the state-of-the-art techniques used to model empathy and how each of those studies evaluated their performance. A detailed analysis and overview of the existing scales used for human-based evaluation of AE is presented in Section III. Datasets that have been used to train AE models are analyzed in Section 4. Section V concludes the paper and discusses the possible future research directions.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Ii. Existing Classification Techniques Of Artificial Empathy A. Observations",
      "text": "From what we can gather in the literature, most of the stateof-the-art DL-based techniques have dealt with textual data. It's also the case that only the textual works include the use of more recent models like transformers. When compared to other DL approaches like autoencoders and deep CNNs, transformer models such as GPT, GPT-2, BERT, and its variants have shown encouraging results. Therefore, it is reasonable to assume that vision transformers can boost efficiency while dealing with visual data  [24] . Furthermore, when using multimodal features for predicting user empathic reactions, works like  [25] and [26]  have demonstrated good results, supporting the fusion of different modalities for this task.\n\nIn addition, Reinforcement learning (RL), a common learning approach for artificial agents, has been used in various works  [27] [28] , leading to more realistic performance from the agents  [29] . Deep reinforcement learning can improve this, particularly when intrinsic motivation (as opposed to the commonly used extrinsic motivation) is included as a learning objective  [30] . Intrinsic motivation could be characterized, in the context of AE agents, as the sense of programming that prioritizes establishing artificial empathy as the agent's primary objective. Extrinsic motivation, on the other hand, involves concentrating on external outcomes, such as evaluation metrics and user experience. By contrasting intrinsic motivation and extrinsic motivation, as tailored by  Bagheri et al. [28] , the Figure  2  is a depiction of how the function of intrinsic motivation in the development of artificial empathic agents differs from extrinsic motivation. Developers may be better able to construct agents capable of true emotional connection and great user experiences if they prioritise intrinsic motivation and value empathy as a primary function of the system. In order to demonstrate inclusion of intrinsic goals on a state-of-the-art deep reinforcement learning model (Q-learning model), we modify the architecture presented by  Sorrentino et al. [27]  in Figure  3 . The rationale for abstaining from employing solely emotional cues of the user as a means of providing rewards is multifaceted, with the primary objective of enhancing the quality of the human-robot relationship.\n\nFinally, assessment metrics are crucial, especially in the creation of the event response. Many works have employed grammar and language structure-specific metrics including BLEU,  BERT, and PPL [31] . Obviously, such approaches fail when evaluating empathy, since the intensity of feelings is more important than the grammar used to express them. As a result, there is a need for autonomous measures that can give more precise task-specific assessment for AE. A good example is the empathy-based autonomous measure proposed by  Lee et al. [32]  inspired by the EPITOME framework  [33] . It is also apparent that approaches using both types of evaluation, i.e., human and automated, have a more robust and comprehensive examination of their outcomes, highlighting the significance of employing both types of evaluation measures. Furthermore, the comparison between human and automated evaluation methods, interestingly, shows variation in results for many works, raising concern over the reliability of automatic metrics. To exemplify this, as depicted in Figure  4 , the models presented by  [34]  report contradicting scores of relevance and fluency, as compared to empathy.\n\nThe methodologies which are currently being used for the  Fig.  4 . Conflicting scores of human vs autonomous AE metrics as reported by  [34] .\n\nclassification and modelling of AE are analysed in this section.\n\nBased on the type of data used, we classify the research into two major groups i.e., textual and visual/multi-modal. The papers are further classified based on the technique they have implemented, i.e., transformer networks, DNNs, and other algorithms. A summary of the AE detection and elicitation techniques is given in Table  2 .\n\nIn a work by  Lee et al. [38] , to improve the empathic response generation of ECAs (Embodied Conversational Agents), the Uncertainty-Aware Conditional Variational Auto-Encoder (UACVAE) framework was introduced. A metric called Utterance Entailment (UE) was also included to measure how well an agent's responses fit its circumstances. In their proposed CVAE-based dialogue agent, an approximation of the aleatoric uncertainty of the generated dialogue response is derived from variance of the latent Gaussian distribution. A GPT-2 pretrained model was used to get the sequence embeddings of the dialogue context, the external information, and the generated response. The final loss function integrates the KL divergence between the Gaussian distributions produced by the prior and recognition network. Two variants of the framework have been implemented i.e., UA-CVAE(M) that has several linear layers based combination network and UA-CVAE(C) that has a single layer based combination network. From the combination network, the outputs are fed to the response decoder, which is based on a GPT-2 pretrained language model. In order to train the UA-CVAE, the stochastic gradient variational bayes (SGVB) algorithm is used. The model is implemented on the Empathetic Dialogues dataset  [34] . Both automatic and human evaluation were performed, using automatic metrics including PPL,  ROGUE [39] , METEOR  [40] , intra-response Distinct-n [41], and their proposed UE score. Results show a strong correlation between the proposed UE score and participant responses. This suggests that UE scores can be used for assessing AE in future studies, however, further experimentation with different datasets and participants would be useful for a complete evaluation of its performance.\n\nUsing the newly created psychotherapeutic intervention known as self-attachment technique (SAT),  Alazraki et al. [42]  offered a computational architecture that augments a rule-based agent for delivery. They compiled 1,181 crowdsourced emotional utterances and 2,143 sympathetic rewrites of neutral phrases to form a new dataset called Empatheticpersonas. They've implemented a tree-like dialogue flowchart, and they generate innovative yet secure utterances at each node in the chart, with the goal of reducing ambiguity as much as possible. They achieve this by parsing the rewritten utterances in their proposed dataset and removing fragments at important punctuation points to create concise sentences. For the empathy score, they employ a T5 model [43] that has been trained on a labelled subset of the proposed dataset and for the fluency score, they also deduct a penalty for each repeated word inside an utterance from the inverse of its perplexity given by a GPT-2 language model  [44] . In addition, they adopt a RoBERTa model  [45]  for the task of emotion recognition that is trained on an existing affective empathy dataset  [46]  and further fine-tuned on the expressions of emotion on the Empatheticpersonas dataset. To evaluate the application, human trials with 16 subjects from a nonclinical population, and two medical professionals specialised in mental health, was performed, where subjects were asked to fill out a questionnaire based on questions regarding their interaction with the chatbot. It is worth noting that out of the B. State-of-the-art Research in Textual Data 1) Transformer based Techniques: Rashkin et al.  [34]  proposed a benchmark to evaluate empathetic dialogues/conversation. They presented a dataset that can improve the existing empathetic dialogue generation systems, consisting of 24,850 conversations. The dialogue generation system consists of two modules; Retrieval and Generative. The retrieval-based design is made up of two different transformer encoders; one for the context, and the other for the candidate.  BERT [35]  serves as the baseline architecture for the encoding of both the context (setting of the dialogue) and the candidates (suitable responses). The whole Transformer architecture  [36]  is made use of in the generative set-up, where an encoder and a decoder are used. The output of the encoder is used by the Transformer decoder in order to make a prediction about a sequence of words. The Transformer networks employed in their experiments all have the same fundamental architectural makeup (four layers and six attention heads). Performance of the model is evaluated against other techniques using BLEU (Bilingual Evaluation Understudy) and PPL (perplexity) scores. Scores show an improvement when candidate responses are selected from the ED dataset compared to others.  Hosseini et al. [37]  took the process of identifying empathy in online chat platforms one step further by introducing a direction of empathy feature -seeking empathy or providing emapathy. They also created a dataset i.e., IEMPATHIZE that consists of 5000 sentences from a cancer platform. The Lexicon-based model consists of emotion lexicon and subjectivity lexicon to establish the baseline's feature set. Strong and weak subjective words were extracted from the conversations and used as features to train a logistic regression model. Extracted TF-IDF feature vectors at the word level are used to train Naive Bayes, Support Vector Machine, and Random Forest, three popular machine learning techniques. Next, they employed a concatenation of CNN, Conv-LSTM, LSTM and Bi-LSTM networks, and fed the outputs of these networks into a fully connected layer for predictions. BERT is also fine-tuned and used as a pre-trained language model. In contrast to the majority of prior text-based empathy methods, they have employed F1 scores to assess the empathy prediction of various models, which is more of an intrinsic measure given that it is not related to language rules, unlike metrics such as BLEU. Results indicate that BERT outperforms other models both with and without pre-training on domain-specific data (more than 5 percent improvement of F1 scores). post (the patient or the person who has the enquiry on the web platform), while R-Encoder is in charge of comprehending empathy from the response post. For domain adaptive pretraining of the encoders, they use the Talklife platform  [47] . At the last stage of the framework, an empathy identifier module is implemented. It uses the final representation of the seeker's token and compares it with the response's token by passing it through a linear layer to get the predictions. Performance of the model's response is benchmarked using BLEU and BERT scores. In our view, the distinctiveness of their work is the unique approach in which they have labelled the dialogues, i.e., ER, IR, and EX, allowing for improved classification techniques.\n\nIn a recent work, Ayshabi et al.  [31]  proposed a multiresolution system based on disparate decoders for recognition and processing of emotions and to accumulate the feedback to generate an empathetic response. They used an Emotion-aware Transformer Encoder unit to extract semantic and emotional context from the dialogue. This is then processed and dispatched to the Emotion Expressive Transformer Decoder unit. This results in the identification of the emotion expressed in the dialogue and the generation of a congruent empathetic response. Next, the reaction emotion is learned with the use of separately parameterized decoders, and an empathic response is then generated by a meta decoder that aggregates the weights taken from the decoders. Similar to  [38] , results are tested on the Empathetic dialogues dataset  [48] . Their\n\n23 participants originally selected, only 16 returned completed evaluation surveys, showing the unreliability of human-based evaluation.  Sharma et al. [33]  introduced a conceptual framework (EPITOME) for modelling empathy from online discussions and chats on mental health platforms while introducing a new dataset as well. The framework consisted of three communication mechanisms of empathy: Emotional Reactions (ER), Interpretations (IR), and Explorations (EX). For this purpose, they suggested a RoBERTa-based bi-encoder model to detect empathy-inducing language patterns in conversations. They propose a model based on two independently pre-trained transformer encoders from RoBERTaBASE (S-Encoder & R-Encoder) to encode seeker post and response post, respectively. S-Encoder is responsible for encoding context from the seeker and it proved to perform the best. In addition, to evaluate the model's performance, they used two approaches, i.e., BLEU scores and human evaluation. For human evaluation, participants were asked to evaluate 100 random dialogues against three aspects: empathy, relevancy and fluency.  Lee et al. [32]  conducted an empirical study that demonstrated the promising capabilities of GPT-3 in generating empathic responses through in-context learning. Specifically, they trained the model on the Empathetic Dialogues dataset, leveraging emotion information to select in-context examples for training and testing. Their proposed model was benchmarked against two state-of-the-art models, Blender 90M and Emp-GPT3, and was evaluated using a novel autonomous evaluation metric that directly measures empathy. This metric, which is based on the EPITOME framework previously proposed by  Sharma et al. [33] , measures the difference between empathy scores generated by the model and \"human golden responses\". In addition, the study used other automatic evaluation metrics, such as PPL and Distinct, to assess the fluency and diversity of the generated responses. Overall, this work provides evidence that GPT-3 can effectively generate empathic responses, and demonstrates the usefulness of an automatic evaluation metric that focuses on the key aspect of empathy, unlike majority of the evaluation works in the area of AE.  Roller et al. [51]  used poly-encoder transformer architecture exploiting GPT-3 as the base (using the publicly available ParlAI framework). They highlight the importance of domainspecific training to generate empathic responses. Their proposed model has two disparate parts: retriever and generator, where the retriever selects next dialogues based on candidate scores. The generator, on the other hand, is responsible for generating rather than selecting response from a limited set. The proposed model is then evaluated on the Empathetic Dialogues dataset. Two novel autonomous evaluation metrics for AE are proposed namely TF-IDF and ACUTE-Eval. However, implementation is limited to TF-IDF, as the latter is too costly to implement.\n\n2) Other DNN-based Techniques:  Harilal et al. [52]  presented CARO, an empathetic online chatbot which provides support for people suffering of mental health problems. Their proposed technique is an ensemble of two models: one that produces medical advice, and another that generates natural, empathetic dialogue. They introduce an intent feature that decides whether a user should be directed through medical advice model or the empathetic conversation generator. They use separate baseline models i.e., Seq2seq network and an LSTM for empathetic dialog generation and medical advice generation, respectively. Facebook AI Empathetic Dialogue dataset and Medical Q/A dataset are used to train the baseline models. To elicit an empathic reaction, the emotions retrieved by the emotion classifier were attached to the beginning of the context phrase. The model is composed of Encoder-Decoder architecture, where each Encoder and Decoder block is composed of one or more LSTM/GRU units.\n\nBoth intent and emotion classification tasks were performed by using an LSTM unit that generates a decoded sequence to be passed on to a dense layer, where Softmax activation is applied for prediction. Performance of the model's response is evaluated using BLEU and BERT scores.  Li et al. [50]  proposed a multi-resolution adversarial neural network based model called EmpDG, whose major characteristics are its empathetic generator and its interactive discriminators. The empathic generator uses Transformer-based encoder-decoder design. Semantic context and multi-resolution emotional context are encoded in the encoder; and the decoder combines these to create responses. Two CNN-based discriminators were designed to increase the generator's empathy (i.e., the semantic discriminator and the emotional discriminator). In order to encode the multi-resolution emotional context, they make use of a separate transformer encoder that has a unique set of parameters. As for the discriminators module, the semantic discriminator calculates the semantic distance between the produced and gold responses, the emotional discriminator; determines if the produced reactions are sufficiently empathetic. Both of the discriminators are based on a CNN classifier. Empathetic dialogues dataset is used for experimentation of the model. In contrast to many other studies, they avoid using the BLEU scoring metric for its unreliability as explained by  Liu et al. [53]    [34] . Five different algorithms-kNN, a Random Forest Classifier (RFC), a Multi-Layer Perceptron (MLP), a Gaussian Naive Bayes (GNB), and a Decision Tree Classifier-were used to evaluate their proposed network's performance. The Closeness Assessment Measure (CEM) and the Area Under the ROC Curve (AUC)  [64]  were proposed as evaluation measures. Although AUC and CEM are not often used to evaluate empathy, they were helpful here because of the linear trend shown in the scores of empathy.\n\n3) Other Techniques:  Kurashige et al. [65]  proposed a virtual counselling agent \"CRECA\" (also implemented on an actual robot) that could help deal with psychological problems of clients through empathic interactions. The architecture of their proposed system consists of two phases namely, problem-discovery and problem-solving. During the phase of problem discovery, the goals are to better understand the nature of the client's challenges and to earn the client's confidence. The client's problem is grouped into 5 or 6 categories after the problem-discovery phase. Thus, problem-solving context is established. During the problem-solving phase, as the client's dialog on specific issues progress, similar keywords are matched single or several times, and replies to deepen the client's contemplation are maintained. The text input from the user is analyzed by a context-based reasoning (CBR) module  [66] . There are three main parts to language or dialogue processing: (1) an init/exit module that acts as a bridge between humans and CA; (2) an analysis module that parses conversation text; and (3) an input/output module that takes in new dialogue and returns processed text  [67] . The input/output module operates by exploiting the counselling knowledge dictionaries in order to provide replies that are appropriate for the present setting. To enhance the empathic abilities of the robot, the Japanese concept of \"unazuki\" (nodding), which is considered to be a higher level of empathy representation, is employed on  Tan et al. [26]  proposed a multi-modal LSTM with featurelevel fusion and local attention that predicts empathic responses from audio, text, and visual features. They use the OMG-empathy dataset from the OMG-Empathy challenge to evaluate their model's ability to recognize empathic responses. For feature extraction from text, YouTube's automated subtitling was utilised to get transcripts and start/end times of each utterances. The GloVe word embeddings  [72]  were used to extract features per utterance by averaging across the embeddings of the words comprising the speech. For feature extraction from audio data, they extracted 990 low-level acoustic characteristics using openSMILE v2.3.0 [73] and the accompanying emobase configuration file. As for the video features, they retrieved fully-connected feature embeddings from the pretrained VGG Face CNN models  [74] . The features were retrieved from the listener's face for each frame, as they observed that adding the speaker/actor's facial features was not helping the model's recognition performance. They adopted various models and parameters when working with different modalities i.e., text only, text and visual only. For evaluation, they utilise the Concordance Correlation Coefficient (CCC) to compare the model's predictions with participants' own reports of their levels of empathy.\n\nA virtual empathic robot assistant was presented by  Fung et al. [25] . They used facial, speech and audio data to perform emotion recognition followed by empathic responses by the proposed virtual agent. They used a CNN for the classification task, where each word was treated as a vector input, while for audio, each frame was used as a vector. A Deep Neural Network (DNN) was suggested for computing the emission probabilities of speech data. At the same time, a Long Short-Term Memory (LSTM) was presented to manage the previous context of each given conversation. They make use of the Kaldi speech recognition tools in order to train acoustic models. A dataset was created from the TED-LIUM corpus release 2 [75] for testing. Six different emotional categories were chosen for the purpose of the experiment. As a baseline for feature extraction, a linear-kernel SVM model from the LibSVM package  [76]  was utilized in conjunction with the INTERSPEECH 2009 emotion feature set [77] retrieved using openSMILE  [78] . For additional experimentation (testing), a new corpus was created using scenes from the popular English sitcoms -Friends and Seinfeld. After this, cross-evaluation was performed using two databases for training and one for testing.  Mathur et al. [79]  proposed an automated method for determining whether or not a user has empathized with a robot storyteller, based on the user's eye gaze, facial action units, facial landmarks, head altitude, and point distribution parameters. They also provide a dataset of visual information gleaned from human-robot narrative exchanges in order to promote empathy. Details about the dataset are given in Section 4 (Storyteller robot dataset). Moreover, they experimented with ten different ML models (8 classic ML models and 2 DL models) to detect empathy, where detection of empathy is referred to as predicting the participants' empathic/non-empathic responses. The 8 classical machine learning models were, adaptive boosting a Raspberry pi based robot. The system is then evaluated given a 15 item Likert scale on 12 participants, who questionnaire.\n\nIn a follow-up study (AdaBoost), bagging, decision trees, linear-kernel support vector machine (Linear SVM), logistic regression, random forest, rbf-kernel support vector machine (RBF SVM), and XGBoost. The two deep learning techniques used were LSTMs and Temporal Convolutional Networks (TCN). Information from eye gaze, face attributes, and head movement at each visual frame was extracted to record participants' visual actions while they listened to the robot storyteller. Eye-gaze directions, the strength and existence of 17 facial action units (FAUs), facial landmarks, head position coordinates, and PDM parameters for face location, scale, rotation, and deformation were all retrieved using the OpenFace 2.2.0 toolkit. To understand and exploit temporal patterns in visual signals that are predictive of empathy, they employed raw sequences of visual data, for deep learning models. 5-fold stratified cross validation was repeated 10 times giving a total of 50 folds. Evaluation was performed using four metrics, namely ACC, AUC, Precision and recall.\n\nCarolis et al.\n\n[21] implemented affective reasoning on the NAO robot for simulating empathic behaviors in the context of Ambient Assistive Living (AAL). In their emotion detection architecture, firstly, facial feature extraction is performed using the famous Viola-Jones detector. After this, the Staked Active Shape Model (STASM) method is used to find facial key points. This method uses the Active Shape Model with a simplified version of SIFT descriptors and Multivariate Adaptive Regression Splines (MARS) to match descriptors. For speech recognition, an online service called VOCE (Voice Classifier of Emotions) is employed using a kNN classifier. This tool identifies the valence and arousal values of the given audio. For collection of data, two caregivers, who were looking after two elderly people were asked to record their experience for a period of 9 months. The caregivers were asked to keep a paper journal in which they recorded the day's activities and any noteworthy occurrences, with a focus on how they made them feel i.e., sad, depressed, excited. Empathy was modelled in the robot using Deep Belief Networks (DBN). The robot has a predetermined set of empathic goals, such as comforting the subject to make them feel cared for/loved. Constant monitoring of the audio and facial cues coming from the user are processed through the DBN and a certain empathic goal (e.g. comforting the subject through dialogue) is activated. For evaluation, two methods were adapted: expert-based evalua-tion and user study. No computational/autonomous evaluation techniques were used.\n\nIn order to attain more richly-detailed HRI engagements,  Bagheri et al. [80]  proposed the Automatic Cognitive Empathy Model (ACEM). A stacked autoencoder network, trained and tested on the RAVDESS dataset, is used to identify users' emotional states  [81] . The proposed model consists of three distinct modules: an emotion detection module, a perspective taking module, and an empathic behaviour provider module. The implementation of a stacked autoencoder with a softmax activation function is utilised for the purpose of extracting and classifying facial features. While the perspective taking module has been introduced, its implementation has not yet been carried out. The module responsible for providing empathic behaviour encompasses two distinct categories of empathetic responses, precisely parallel and reactive. The parallel empathy approach is a technique employed to mirror the emotional state of of comprehension and technique instructs the the user, thereby fostering a sense validation. In contrast, the reactive robot to respond favourably to the user's emotional condition by making upbeat comments or providing other positive feedback. Classification Rate (CR), False Alarm Rate (FAR), and Confusion Matrix (CM), inspired by  [82] , are utilised to evaluate the performance of the proposed emotion classification technique. For the evaluation of empathic behavior provider module, 40 participants of mixed gender and personality types were selected. Each participant viewed six videos (representing various emotion classes) while being interacted with by a Pepper robot in order to test the effectiveness of the proposed ACEM. After the experiment was over, each participant filled out three surveys to share their thoughts: the UTAUT questionnaire [83], the friendship questionnaire [9], and the engagement parameter  [84] . All of them reflect various facets of the robot's character and are measured on likert scales. For further validation of the results obtained from the evaluation surveys, the Cronbach's Alpha and Wilcoxon Test are applied. A detailed analysis from the results is discussed against various aspects of the robot's empathic and personality characteristics.\n\n2) Other Techniques:  Filho et al. [85]  proposed RegressionWiSARD and Clus-RegressionWiSARD n-tuple regressors and their ensembles in order to predict empathy. They perform experiments on visual and audio data from the OMG-Empathy dataset. For preprocessing of the image data, Adaptive Gaussian filter [86], Sauvola method [87], Canny border detector  [88] , and Otsu's binarization  [89]  were adapted. On the other hand, for preprocessing the audio data, mel-frequency cepstral coefficient extraction  [90]  was implemented, that converts audio to text. A combination of Regression WiSARD (ReW)  [91]  and  ClusWiSARD [92]  i.e., ClusRegression WiSARD (CReW) was employed. Following this, ensemble of ReWs and CReWs was formed using three different techniques (Bagging; where each weak learner is trained using a portion of the training data, with replacement. Boost; where weak learners are trained the same as bagging method except for replacement. Naive; where all the weak learners are trained on the whole dataset.) OMG-Empathy Dataset was used to train and test their proposed technique, while CCC was used for validation of each model's performance.  Rasool et al. [10]  proposed an HRI based computational emotion model where the internal emotions are defined using psychological studies and generated on 2D (pleasure-arousal) scaling model, whereas, fuzzy logic is used to calculate the intensity of each emotion. The process can be broken down into three primary stages: perception, assessment, and the expression of empathy. Facial expression recognition algorithms [93], based on Constrained Local Model (CLM) with LeaderP clustering algorithms [94] and topological Gaussian Adaptive Resonance theory algorithm (TGART) are exploited. Face detection is performed using the famous Viola-Jones algorithm. Additionally, Point Distribution Model (PDM) is applied to generate the 2D feature point positions of each patch. While empathy is typically a reflection of two factors i.e., personality and mood, in this work, only the mood is used as the deciding factor while personality remained constant. To express empathy, a virtual facial expression simulator called Grimace is used. Evaluation was performed using facial data to detect the level of empathy as '0' for no-empathy and '1' for empathy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Reinforcement Learning For Ae",
      "text": "As previously mentioned, Reinforcement Learning (RL) is a modern technique that is increasingly used to train artificial agents on unfamiliar data and infuse them with the motivation to explore alternative input and output combinations. Following are some of the works that have incorporated RL for AE.  Leite et al. [95]  proposed a multi-modal system for modelling empathy that combines visual and task-related features. A robot i.e., iCat was employed to play a game of chess with children and display empathic responses, such as facial cues and motivating comments during the game. The robot is based on the Open Platform for Personal Robotics (OPPR) software that allows the iCat to be programmed as per the needs of the task. They follow an approach based on RL, so that the robot can learn by reward and penalty the best strategies for a particular user, and adapt to its empathic behaviour accordingly. A questionnaire with three subscales -help, engagement, and self-validation was used to get evaluation from the participants.\n\nIn a later work,  Bagheri et al. [28]  proposed implementation of cognitive aspect of empathy on a robot using interpersonal goals, such as aggression and social behaviour, as well as intrapersonal goals, such as parallel emotion and empathic concern. Three modules comprise their proposed framework: Emotion Detection, Reinforcement Learning, and Empathic Behavior Provider. The Emotion Detection module uses a facial emotion detection model to categorise the user's facial expressions into six categories, and the Reinforcement Learning module uses contextual bandit to learn the 'optimal action-selection policy' that enables the robot to select the most appropriate empathic behaviour. After each action, the Q-table is updated and initialized with zeros. The Empathic Behavior Provider module applies the specified behaviours to the robot so that it may respond to the emotions of the user. The proposed framework was implemented on a real-world robot, Pepper. The robot engaged with human participants in a game-playing scenario, and the model was evaluated by asking individuals to report their interactions with the robot on three different scales: friendship, UTAUT, and engagement questionnaires.  Sorrentino et al. [27]  used an online platform to train a DRL algorithm -DQN, that used rewards from online participants against several generated facial expressions (emotions) as motivation, in an effort to implement 'affective' empathy. The network used a standard E-greedy Q-learning approach.\n\nExpressions generated by the network were rewarded as 'coherent/incoherent' by the users. The trained network was then implemented on a real robot called CloudIA. A setting of three modes of conversations that featured small conversations, watching a video, and playing a guessing game, was used for the robot to engage with participants. Towards the end of each interactions, Godspeed questionnaire was filled out by each participant to rate the robot's abilities against the 5 sub-scales of the questionnaire. The experiment concluded higher emotion expression capabilities of the robot, but not the 'empathetic behaviour'. Moreover, all the participants involved in the experiment were from the same age group.\n\nStudies that use DRL to implement AE are rather limited in number, at the moment. Since empathy is a complex emotion, it is safe to say that Q-learning is the favourable technique to go with, as it allows exploration of hidden states.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Reflection",
      "text": "One problem with the current research is that most of the experimental settings are monotonous and controlled, making them highly unlikely to be generalized. Also, different types of scales have been used to report user's evaluation making it hard to draw a statistical comparison between these techniques. This is to say that the evaluation should be performed on a general scale that allows for a performance comparison against other works allowing the researchers to improve their future efforts.\n\nSimilar is the case with datasets. In addition to scarcity of visual empathy datasets, description of empathy varies in each corpus. It is needless to say that benchmarking requires a consistent modelling of the aspect being evaluated, in order to draw a reliable evaluation. For instance, EPITOME by Sharma et al. uses emotional reactions, interpretations and explorations as three different mechanisms to recognize empathy, whereas, IEMPATHIZE by Hosseini et al. presents two directions of empathy i..e., seeking or providing as the classifying mechanism.\n\nSince  Liu et al. [96]  proved the inability of BLEU score to validate the performance of dialogue generation tasks, as it does not strongly associate with human judgement, it seems illogical to use this metric for evaluating empathy. This questions evaluation methods of works such as Sharma et al.\n\n[33] and  Harilal et al. [52] . Table  III  compares the most widely used autonomous metrics for evaluation of AE.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Iii. Review Of Evaluation Methods For Empathy",
      "text": "WITH RESPECT TO ARTIFICIAL AGENTS In this section, we start by reporting our observations on the various empathy assessment scales used in human-robot interactions. We outline the primary features of each scale and their respective application in studies. Additionally, we assess the distinctions between these metrics, particularly in terms of empathy type and other relevant characteristics.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Observations",
      "text": "When it comes to the adaption of a generalised scale for the evaluation of empathy, there are several factors that might lead to ambiguities and variations. One of the reasons is the variations in the interaction levels and application goals of the various artificial agents. For example, a companion robot in an aged care setting may require more compassion, whereas an industrial robot may concentrate on getting the maximum performance. One further thing to be concerned about is how the agents might differ in their features and non-functional aspects including aesthetics, physicality, language skills, and reaction time  [97] . The vast majority of the currently available measures are concerned with empathy rather than AE. Because of this, it is more challenging to use them in situations involving HRI-based empathetic exchanges. In addition, the few quantifiers that are applicable to AE need for an assessment based on human subjects. This results in issues, such as the possibility of participants having a bias, the expense in terms of both time and money, and variations in how different people perceive different events.\n\nSeveral aspects, including user-related factors, contextrelated factors, and system-related factors, might influence the evaluation of empathy in artificial agents and must be evaluated and addressed. Examples of these factors include, personality, user satisfaction and acceptability of the agent. For an empathy scale to accurately measure the empathetic abilities of an artificial agent, it is of the high significance that it takes into account these aspects  [98] [99] .\n\nWhile there are recent works that have tried to adapt human-human interaction (HHI) empathy metrics into the HRI domain, such as RoPE [100] and QMAE [99], validation of these scales is required to ensure the effectiveness and intercorrelation of the items included. This is only achievable if future research includes a validation of the current AE evaluation scales. Another idea is to bring together the characteristics of autonomous and human evaluation metrics together and design a framework that can incorporate the strengths of both. A summary of the scales for empathy evaluation is provided in Table  3 .",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "B. Godspeed Questionnaire",
      "text": "This popular scale was presented by  Bartneck et al. [110]  as a series of questionnaires to measure the user's perception of robots. It combines five consistent and validated questionnaires based on 5-point semantic differential scales as a standardized metric for the five key concepts in HRI.\n\nC.  Barrett-Lennard Relationship Inventory (BLRI)  This scale was proposed by , initially as an instrument to measure the emotional relation between humans. However, it can be used to indicate humanrobot relations as well. The BLRI consists of 64 items (16 to measure each of the four dimensions). The dimensions/subscales were as follows: empathic understanding, level of regard, un-conditionality of regard, and congruence. Item wording reflects either positive (e.g., \"She or he understands me.\") or negative ways of responding to a person. Subjects used 6-point scales (3 = strongly feel that it is true to -3 = strongly feel that it is not true) to respond to items.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D. Davis' Iri",
      "text": "Davis  ' IRI [4]  for evaluating empathic relations is one of the most popular empathy scales of all time. It is a 28-item selfreport questionnaire with four 7-item subscales, each assessing a different component of empathy. These subscales represent characteristics that are important in interpreting the aspects of empathy in any individual. Following are the subscales of the Davis' scale with a brief overview. 1) Perspective Taking (PT) scale -It evaluates the capacity to see ordinary events through the perspective of the attitudes held by other individuals. Fantasy (FS) scale -It assesses a person's propensity to project their own thoughts and feelings onto the emotions and actions of fictional characters that they encounter in works of fiction such as novels, films, and plays. Empathic Concern (EC) scale -It assesses the likelihood of having sentiments of warmth, compassion, and care for other people in everyday life. Personal Distress (PD) scale -It examines usual emotional responses, but instead of worry for others, it delves into one's own sentiments of personal disquiet and discomfort in response to the feelings of somebody else.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "2)",
      "text": "3)",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "4) E. Robot'S Perceived Empathy (Rope) Scale",
      "text": "There are a total of 18 questions that make up the Robot's Perceived Empathy (RoPE) scale  [116] , with two sub-categories, measuring either \"empathetic understanding or empathic response\". Each sub-scale's items were chosen with consideration for research on the efficacy of empathy in HRI settings  [117] . Not all of the questions on the original empathy survey applied to robots, hence some were removed. Implementation of the RoPe scale can be seen in the work by  Daher et al. [118] .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "F. State Empathy Questionnaire",
      "text": "The self-report Emotion Awareness Questionnaire (EAQ)  [109]  was made to reflect the key ideas of emotional awareness. It has six scales: the ability to tell the difference between emotions and find out where they came from (Differentiating Emotions); paying attention to the physical aspects of the emotion experience (Bodily Awareness, i.e. being aware that 1) Anthropomorphism: rates the user's impression of the robot on five semantic differentials. Animacy: rates the user's impression of the robot on six semantic differentials. Likeability: rates the user's impression of the robot on five semantic differentials. Perceived Intelligence: rates the user's impression of the robot on five semantic differentials 2)\n\n3) 4) 5) Perceived Safety: rates the user's three semantic differentials.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emotional State On",
      "text": "Implementation of the Godspeed questionnaire can be seen by  Johanson et al. [115] , who implement expressions of humor on a robot in healthcare setting to gauge its impact on users. emotions are accompanied by physical symptoms; the ability to talk about emotions (Verbal Sharing); the blunt expression of emotions (Acting Out); and others' feelings (Attend to Others' Emotions and Emotional Analyses, respectively). The initial scale of 40 points has been decreased to 30, and irrelevant items have been excluded from the examination of cognitive and emotional empathy.\n\nnot at all) to 5 (extremely). The level of participants' ability to empathise with others was evaluated by calculating the absolute difference in PANAS emotion scores between those reported by the targets in the clips and those reported by the participants themselves. The PANAS scales are valid, reliable, and independent measures of both positive and negative affect, regardless of the population investigated, the time period examined, or the answer format used. G. Emotion Awareness Questionnaire\n\nThe self-report Emotion Awareness Questionnaire (EAQ)  [109]  was made to reflect the key ideas of emotional awareness. It has six scales: the ability to tell the difference between emotions and find out where they came from (Differentiating Emotions); paying attention to the physical aspects of the emotion experience (Bodily Awareness, i.e. being aware that emotions are accompanied by physical symptoms; the ability to talk about emotions (Verbal Sharing); the blunt expression of emotions (Acting Out); and others' feelings (Attend to Others' Emotions and Emotional Analyses, respectively). The initial scale of 40 points has been decreased to 30, and irrelevant items have been excluded from the examination of cognitive and emotional empathy.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "K. Multidimensional Emotional Empathy Scale (Mdees)",
      "text": "Proposed by  Caruso et al. [106] , the MDEE scale has a total of 30 items (questions), and it was originally administered to a sample size of 793 adults and children. Principal Component Analysis (PCA) was used to get a total of six significant variables out of the data. On the basis of these variables, sub-scales are formed. This scale assesses the emotional components of empathy and can be used to evaluate emotional empathy. Additionally, it provides specific sub-scales for further analysing the results.\n\nL. Hogan's Empathy Scale (HES) R  Hogan [102] devised the HES in 1969. After assigning a criteria for rating empathy, the first step in developing Hogan's empathy scale was to compare the answers of 57 men who had high ratings for empathy against 57 men who had low ratings for empathy across the combined-item pools of the California Psychological Inventory (CPI). Each rating scale has four questions that follow the Likert scale and have seven levels, with opposing adjectives serving as anchors at each end. One of the items that makes up the Adjustment scale, for example, is anxious-calm.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "H. Emotional Response Questionnaire (Erq)",
      "text": "There are various versions and types of Emotional Response Questionnaires used by different researchers from time-totime. It has been updated and changed as per the requirement of each study where it was deployed to, which is why there is no single description for this certain empathy scale, however, the one most frequently used structure of ERQ was proposed by Batson et al.  [104] . Following are some of its features. The ERQ was a list of 28 adjectives that described how people felt. Eight of these adjectives (alarmed, grieved, upset, worried, disturbed, distressed, troubled, and perturbed) were found in previous research to describe feelings of personal distress, and six others were found to describe feelings of empathy  (sympathetic, moved, compassionate, warm, softhearted, tender) .\n\nRespondents were asked to rate how much of each emotion they were feeling while watching the worker, on a 7-point scale (1 = not at all, 7 = extremely).\n\nM. Questionnaire Measure of Emotional Empathy (QMEE) & Emotional Empathic Tendency Scale (EETS) QMEE and Emotional Empathic Tendency Scale are more or less the same, and are both presented in the same study by  Mehrabian and Epstein [103] . The QMEE is a questionnaire that consists of 33 items, each of which the candidate rates on a scale ranging from very strong disagreement (-4) to very strong agreement (+4). The signs that came before negative items are changed, and the sum of their scores on all 33 questions is used to get the respondent's overall score. Therefore, a high score indicates a high level of empathy. The EETS, same as the QMEE, has 33 questions on your propensity to feel other people's emotions. For instance, \"it's hard for me to understand why certain things bother people so much, and it saddens me when I see a lone stranger within a bunch\". There is a 9-point scale from -4 (very strong disagreement) to +4 (strong agreement) that respondents use to score each statement (very strong agreement).",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "I. Questionnaire Of Cognitive And Affective Empathy (Qcae)",
      "text": "QCAE was developed and validated by  Reniers et al. [114] . The research compiled a total of 65 items for the purpose of gauging cognitive (29 items) and affective (36 items) empathy. These items were derived from the Empathy Quotient, the Hogan Empathy Scale, the Empathy subscale of the Impulsiveness-Venturesomeness-Empathy Inventory, and the IRI. The Hogan Empathy Scale was used to measure cognitive empathy. The QCAE is a reliable instrument for evaluating both cognitive and emotional aspects of empathy.\n\nN. Empathy Quotient (EQ) Empathy Quotient by Cohen et al.\n\n[107] measures empathy on a 40-item Likert scale for empathy and a 20-item scale for controls. The greatest possible score on the (EQ) is 80, with a minimum score of 0. Scores on the individual empathy items J. Positive and Negative Affect Schedule (PANAS) PANAS [105] is a likert scale of five points that has been devised to capture the respondents' sentiments as they reported their experiences. The scale ranges from 1 (very little or may be 2, 1, or 0, giving the scale a total range of 20 to 80. The filler questions are not related to empathy and are there just to make sure that a participant does not get overwhelmed by intense focus on empathy. The EQ was designed to be short, easy to use, and easy to score. An example item from the scale is \"It is hard for me to see why some things upset people so much.\" Participants have the option to slightly/strongly agree/disagree, scoring 1/2 points, respectively. information systems. It's a tweak to the RoPE scale that takes into account some of the components of AE that were left out of the RoPE scale. It incorporates a few aspects from some of the popular questionnaires, such as the BLRI scale [101], Godspeed questionnaire [110], Companionship Scale for Artificial Pets  [119] , and AttrakDiff questionnaire  [120] . It is a 6 point likert scale with values ranging from -3 to 3. The degree to which a user feels connected to a digital assistant is quantified here. The relationship depends on how the user and artificial agent interact, how the user responds to the artificial agent, how accurate the agent's predictions are, and how the user interprets the results.\n\nO. Empathy Questionnaire for Children and (EmQue-CA)",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Adolescents",
      "text": "The EmQue-CA consists of 21 items generated by the Rieffe et al.  [112] . There were three scales: (1) Affective empathy (nine items, like \"When a friend is upset, I feel upset too\") measures how much one shares another person's feelings.\n\n(2) Cognitive empathy (six items, like \"If a friend cries, I often understand what has happened\") measures how much one understands why another person is upset. (3) Intention to comfort (six items, like \"If a friend is sad, I want to do something to make it better\") measures how much one wants to help someone who is upset On a scale from one to three, participants were asked to indicate whether or not they found the description to be true for them: (1) not true, (2) somewhat true, and (3) true. Re-scoring was performed on all of the questions such that higher scores would imply a stronger level of empathy.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "S. Empathy Assessment Index (Eai)",
      "text": "Proposed by  Lietz et al. [121] , the EAI is a 20-item likert scale-based self report empathy questionnaire. It has four sub-scales that are namely, affective response, self-other awareness, perspective taking and emotion regulation. The scale was developed to measure human empathy, but it may be modified for use with artificial agents if required; it already has components like self-other awareness and perspective taking that are important for medical support robots.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "T. Emote Questionnaire",
      "text": "Emote questionnaire is a part of the EMOTE project  [122] . It is a 14 item self-report scale, whose items are mostly inspired by the IRI  [4] . At the same time, it is also one of the few questionnaires that have been used to evaluate behaviour in robots i.e., social assistive robots (SARs). However, their are studies that have used the EMOTE questionnaire with a few modifications (removing irrelevant items) for evaluation of emotion in robots  [123] .\n\nP. The Toronto Empathy Questionnaire (TEQ) Proposed by  Spreng et al. [111] , the TEQ contains 16 questions that encompass a wide range of attributes associated with the theoretical facets of empathy. The affective aspect of empathic responding is thought to be related to such phenomena as emotional contagion, emotion comprehension, sympathetic physiological arousal and con-specific altruism, all of which are represented in TEQ items. The TEQ correlates highly with Davis' IRI, however, it is an amalgamation of several empathy scales, i.e, Hogan's Empathy Scale, QMEE, Balanced Emotional Empathy Scale , Jefferson Scale of Physician Empathy and a few more.\n\nU. Friendship Questionnaire Inspired by  Leite et al. [95] , the friendship questionnaire is a five-point likert scale used to evaluate the friendly characteristics of an artificial agent. It is important to include this in the empathy related works because investigating friendship functions including closeness, emotional security, and social presence can provide some indications of improvement in human-robot relationship  [80] . Q. Basic Empathy Scale (BES) Presenting the BES,  Jolliffe et al. [108]  also mentioned that the definitions of empathy used for the development of the QMEE and IRI, and the items on these scales, may be failing to measure empathy adequately. To clarify the definition of empathy and segregate its types on a better level, BES was presented, with an updated concept of empathy, especially cognitive empathy, since the previous questionnaires were not able to measure cognitive empathy. The identification and development of affective empathy questions, which measured emotional congruence, is a primary focus of this scale.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "V. Reflection",
      "text": "Human-based evaluation methods are widely regarded as the most reliable and accurate. These methods enable evaluators to make nuanced and context-dependent judgements of empathy, which is essential for understanding the complex nature of human social interactions. As such, human-based evaluation methods are often considered the gold standard for empathy evaluation  [114] [100] .\n\nOn the other hand, the current research indicates an increasing inclination towards the creation of autonomous techniques for assessing empathy, specifically in the domain of natural language processing (NLP) and affective computing  [33] . These techniques exhibit the capability to automate the evalu-ation of empathy on a large scale and to furnish awareness into R. Questionnaire to Evaluate Empathy (QMAE)\n\nThe  QMAE [99]  is one of the few in Artificial Agents empathy evaluation methods specifically designed for use in human resources the affective aspects of large datasets. Moreover, utilization of traditional scales such as the EAI [121] can prove useful in formation of an automated system for empathy evaluation in empathy in artificial agents since they already include elements such as self-other awareness.\n\nWhile there are fewer studies that have shown interest in development of autonomous empathy evaluation metrics, there has not been any works conducted to authenticate these metrics. Further research is required to establish the efficacy of these metrics and to ascertain their constraints for AE evaluation. A visual depiction of the ratio of usage among different types of evaluation metrics can be seen in Figures  5,  6  and 7 . The objective behind this is to emphasise the prevalent scales utilised in contemporary research on AE.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Iv. Review Of Datasets For Artificial Empathy",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A. Observations",
      "text": "Most of the visual datasets, such as OMG-Empathy, use a controlled environment, where listeners and speakers are given a set of dialogues. There are drawbacks to maintaining a controlled setting, such as a disconnect between the speakers' scripted narratives and the way they would communicate genuine narratives. Furthermore, these datasets do not include information about the effect of the annotated behavior as to how the listeners perceived it. On the other hand, due to the fact that the text-based datasets have often been taken from random talks on various online platforms such as Facebook and other online blogs, their raw nature is superior to that of the visual ones in this context. It will be useful to have visual datasets collected in their natural settings since this will make it possible to train artificial agents, particularly robots, how to interact and interpret dialogue in a manner that is more natural.\n\nFurthermore, classification of empathy is binary in the existing visual datasets, which hinders developing an understanding of extent/degree of empathy in the artificial agents. Empathy is a complex emotion, and it should not be limited to a set of defined emotions. For instance, a human analyses the situation by verbal and other cues, takes into account the overall context and valence of the dialogue, connects it with previous experiences, and then responds.\n\nAnother significant challenge with some of the existing empathy datasets is their underutilization, primarily due to their relatively small data size. Researchers often prefer to employ customized datasets, which may introduce uncertainties, as empathy is multifaceted and diverse in nature. Moreover, several of these datasets exclusively capture an individual's emotional state, without considering the accompanying reactions of the other person and appropriate responses. In the following we discuss empathy evaluation datasets. conversation is rooted in a particular circumstance where a speaker was experiencing a certain emotion and a listener was reacting to their experiences. The dataset is comprised of crowdsourced one-on-one chats, and it addresses a diverse range of emotions while maintaining a sense of equilibrium. The talks are collected from 810 different individuals and are made accessible to the public under the framework of ParlAI3. Every interaction has been partitioned into around 80% train, 10% validation, and 10% test. The data is divided in such a manner that all sets of conversation in which the same speaker delivers the initial scenario description will be included inside the same partition. This is done to avoid the discussion of the same situation being repeated in multiple partitions. The total number of chats for the final training, validation, and testing splits are 19533, 2770, and 2547, respectively. Adaptation of the dataset can be seen in  [55] .",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "C. Omg-Empathy",
      "text": "The OMG-empathy [124] dataset was created by capturing the audio and visual data from a live conversational encounter between two speakers and a listener, in which the speakers and the listener were sitting in front of each other. The speakers and the listener were facing each other. In each of the scenarios, there are two different speakers, and each of them tells two different tales. After each recording, the participants were given the opportunity to rewatch the interactions on a computer screen and were prompted to make notes regarding the manner in which the interaction influenced their affective state in terms of valence using a continuous scale with values ranging from positive one (1) to negative one (-1). There are two distinct protocol settings: personalised and generic. The dataset includes three pre-defined types of separation sets: B. Facebook Empathic Dialogues (FED) FED  [34]  is a public dataset consisting of 25,000 discussions that are based on different emotional scenarios. Each\n\ntraining, validation, and testing. These sets are applicable to both methods. The self-assessment annotations are used to divide the samples into training and testing sets, and these sets are then balanced against one another. Out of all the tales, four of them are used for training, one for validation, and the other three for testing. There are ten videos connected to each story, and each listener watched one of them. Adaptation of the dataset can be seen in  [125] . the internet. These discussion pairs consist of medical advice taken from a variety of online medical consulting forums, including eHealth Forum, HealthTap, and WebMD, amongst others. There are 35,294 questions and answers included inside the dataset.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "G. Iempathize Iempathize",
      "text": "[37] is a publicly accessible dataset consisting of 5,007 phrases that were taken from an online cancer network and categorised as either seeking empathy, delivering empathy, or having none. The breast and lung cancer discussion forums provide the source material for the chosen sentences. The sentences employed in the dataset all have a maximum length of five words to ensure that the annotation process is as accurate and efficient as possible. The annotation of the sentences into the aforementioned three categories was the responsibility of two graduate students. This dataset is one of a kind because, in addition to just identifying empathy, it also indicates whether a person is seeking or offering empathy to another.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "D. Neural Image Commenting With Empathy (Nice)",
      "text": "The Neural Image Commenting Evaluation (NICE) dataset  [126]  includes approximately two million pictures, 7 million human-generated comments related to those images, and over 28,000 human annotated examples. Following the application of the filters, the dataset now contains a total of 2,150,528 photos as well as 6,720,542 comment dialogue threads. According to the study by Chen et al., the NICE dataset utilises a substantially less number of abstract words than the other datasets, while having the biggest vocabulary size. This indicates that the dataset is capable of producing words and remarks that are easier to comprehend and more cohesive than those produced by any previous empathy datasets.\n\nH. Daily Dialogue DailyDialogue [128] is a vast dataset consisting of everyday interactions that have been labelled as belonging to one of four distinct categories: inform, questions, directives, or comply. It has 13,118 conversations that are broken up into three sets: a training set with 11,118 dialogues, a validation set with 1000 dialogues, and a test set with 1000 dialogues. There are around 8 speaker turns involved in each debate, and each turn is of 15 tokens. The dataset is annotated based on a classification system that consists of seven basic emotion categories. Three professionals from the domain of dialogue and communication theory performed the annotation of the dataset.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "E. Epitome",
      "text": "Epitome  [33]  is a publicly available dataset that contains 1.6 million posts and 8 million interactions. These are derived from discussions posted on 55 sub-reddits that are focused on mental health (by reddit.com). A subset of 10 thousand exchanges within these threads have been annotated with regards to empathy. Crowd-workers were provided a pair of posts (seeker post and response post) and asked to identify the existence of the three communication channels in EPITOME (emotional reactions, interpretations, and explorations), one at a time. This was done as part of the annotation process. Adaptation of the dataset can be seen in  [127] .\n\nI. Emotional Dialogues in OpenSubtitles (EDOS)\n\nEmotionalDialogues in OpenSubtitles (EDOS)  [48] , is a large-scale public dataset that contains 1 million emotional dialogues that have been extracted from the subtitles of movies.  Open Subtitles (OS). In this dataset, each dialogue turn is automatically annotated with 32 fine-grained emotions, eight empathic response categories, and a Neutral category. They annotate a portion of the dataset (9k) using a semi-automated manual annotation in conjunction with a low-quality classifier i.e., BERT. After that, this subset is then utilised train an emotion classifier that will be used to automatically label the remaining dataset. It is one of the only two datasets that have all 32 emotion labels, the other being the NICE dataset.\n\nJ. Storyteller Robot Dataset  Mathur et al [79]  introduced storyteller robot dataset. To generate this dataset, they have a desktop robot named QT read three distinct stories to a group of people, who then fill out a likert-scale questionnaire to describe their degree of empathy at the conclusion of each story. Subsequently, depending on the participant's answers to the questionnaire, an empathy score (ES) is assigned to each story. K. Reflection online mental health platforms, the ability to empathise with their users is more crucial than ever. Many academics have suggested state-of-the-art DL approaches, like Transformers for multi-modal data, to implement artificial empathy, with the hope of evoking empathy in humans by means of artificial agents or vice versa. However, approaches such as reinforcement learning, which have proven quite beneficial with unsupervised and unseen data, have not yet been investigated in this field.\n\nUnlike the seven fundamental emotions such as anger or sadness, empathy is a complex human emotion; thus, it cannot be recognised simply from image or audio data without additional evaluation. In this regard, recent research has shown the use of questionnaires designed specifically to assess the empathic capacities of an artificial agent. It is important to note that, to yet, only two assessment instruments have been established exclusively to evaluate AE. Moreover, none of these two scales have been used by other studies in the field to prove their effectiveness.\n\nLast but not least, in contrast to other tasks using computer vision, artificial empathy is negatively impacted by having a smaller number of visual datasets. Because the majority of contemporary methods depend on a higher quantity of data, this further complicates the process of developing state-of-theart classification and assessment approaches. Currently, there is a single video-based data set for AE, that too with a restricted degree of empathy. Conversely, text-based systems have made significant progress in recent years because of the availability of more publicly accessible data and methods such as Transformers.\n\nBased on our findings, we propose the following future research directions, which are discussed in detail:\n\n• Is my empathy, your empathy? -Generalized idea of AE • The more the merrier. -Call for datasets Despite the variety of datasets available in the domain, research shows utilization of only a few (including both text and visual). There are several reasons as to why these datasets are favoured over the others. For instance, the Empathetic Dialogues dataset comprises a substantial quantity of conversational interactions that have been annotated with empathyrelated labels denoting the presence or absence of empathy. The utilisation of such data enables researchers to effectively train machine learning models in the identification of patterns and characteristics that are linked to empathetic behaviour. Figure  8  shows the ratio between the popularly used AE datasets.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "V. Conclusion And Future Research Propositions",
      "text": "As more and more artificial agents are being used to assist healthcare workers in settings like aged care homes and Explainable AI (XAI) has lately gained popularity due to its capacity to analyse and explain the reasoning behind the behaviour of various ML techniques. Creating explainable AI models that can offer a clear explanation for their empathic judgements, to assist create trust with users and guarantee that they are making decisions that line with human values, might thus be one of the promising topics to pursue.\n\nA. Generalized idea of AE When designing \"empathetic\" agents, we are frequently faced with the challenge of deciding how to assess their performance (particularly involving user studies). In the available literature, it can be seen that the idea of artificial empathy differs from one study to another depending on the experimental circumstances. This creates misunderstanding when analyzing the performance of various strategies. Therefore, there ought to be a uniform concept of AE that the scientific community should adhere to for future work.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "F. Evaluation Metrics",
      "text": "To assess a system's empathic capabilities, many characteristics of empathy must be analysed. However, most studies that employ autonomous evaluation metrics such as F1 or BLEU scores miss out on most elements of empathy and are instead focused on the system's language generation ability. Human evaluation, on the other hand, has a better grasp on empathic evaluation, but at the expense of complications associated with human valuation, such as time constraints. As a result, evolving metrics are required to address additional features of artificial empathy while being cost and time efficient.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "B. Call For Large-Scale Datasets",
      "text": "Empathy is the ability to comprehend and react to the emotional states of others, which is difficult for artificial agents to mimic. Large and diverse datasets are required to train DL models in order to increase their capacity to recognise and react to emotions. These datasets should include a diverse variety of samples of various emotional states, as well as information about the context in which they are manifested. Furthermore, additional data in several modalities i.e., video, text and images, is required to increase agents' capacity to react to emotions in an appropriate and human-like way.\n\nC. Reinforcement Learning RL methods have been used in a number of experiments examining artificial empathy, however the findings are not conclusive. This is because they have not been evaluated with existing datasets. Therefore, further study is required to comprehend how RL may be utilised successfully to train models for artificial empathy and how to enhance the capacity of RL agents to comprehend and react to emotions in a humanlike way. Different motivation/goal settings, as to how the actions/responses of an artificial agent impact the empathy evaluation can help further improve the performance.\n\nNotably, RL is a complex area, and merging it with natural language generation and empathy elicitation in robots, which The majority of the current works that deal with visual data concentrate on face expressions. As the emotion recognition task helps analyze, other forms of cues, such as tone of voice and body gestures, may prove to be particularly effective in providing more information about a person's mood and, therefore, the degree of empathy required.\n\n[12]\n\n[13]  [14]",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Levels/Aspects of empathy inspired by Asada et al. [13]",
      "page": 2
    },
    {
      "caption": "Figure 2: is a depiction of how the function of intrinsic",
      "page": 3
    },
    {
      "caption": "Figure 3: The rationale for abstaining from employing solely",
      "page": 3
    },
    {
      "caption": "Figure 4: , the models presented by [34] report",
      "page": 3
    },
    {
      "caption": "Figure 2: Examples of intrinsic goals vs extrinsic goals for an artificial empathic agent.",
      "page": 4
    },
    {
      "caption": "Figure 3: Modifying the architecture of a Q-learning based RL model originally given by Sorrentino et al. [27] to include intrinsic goals, eliminating human",
      "page": 4
    },
    {
      "caption": "Figure 4: Conflicting scores of human vs autonomous AE metrics as reported by [34].",
      "page": 4
    },
    {
      "caption": "Figure 5: Comparison of frequency of different types of evaluation metrics used for AE [13]",
      "page": 15
    },
    {
      "caption": "Figure 6: Comparison of frequency of various human evaluation metrics used for AE [13]",
      "page": 15
    },
    {
      "caption": "Figure 7: Comparison of frequency of various autonomous evaluation metrics used for AE [13]",
      "page": 16
    },
    {
      "caption": "Figure 8: Comparison of frequently used AE datasets.",
      "page": 18
    },
    {
      "caption": "Figure 8: shows the ratio between the popularly used AE",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Study": "Rashkin et  al.",
          "Technique(s)": "BERT",
          "Modality": "Textual",
          "Evaluation metric": "BLEU",
          "Year": "2018"
        },
        {
          "Study": "Hosseini et  al.",
          "Technique(s)": "CNN,  LSTM,  BERT",
          "Modality": "Textual",
          "Evaluation metric": "F1 scores",
          "Year": "2021"
        },
        {
          "Study": "Lee et  al.",
          "Technique(s)": "CVAE,  GPT-2",
          "Modality": "Textual",
          "Evaluation metric": "PPL,  ROGUE,  METEOR, \nDitinct-n,  UE",
          "Year": "2022"
        },
        {
          "Study": "Alazraki et  al.",
          "Technique(s)": "RoBERTa,  GPT-2",
          "Modality": "Textual",
          "Evaluation metric": "Human  evaluation",
          "Year": "2021"
        },
        {
          "Study": "Sharma  et  al.",
          "Technique(s)": "RoBERTa,  Bi-encoder",
          "Modality": "Textual",
          "Evaluation metric": "BLEU,  BERT",
          "Year": "2020"
        },
        {
          "Study": "Harilal  et  al.",
          "Technique(s)": "Seq2seq,  LSTM",
          "Modality": "Textual",
          "Evaluation metric": "BLEU,  BERT",
          "Year": "2020"
        },
        {
          "Study": "Li  et  al.",
          "Technique(s)": "Adversarial  NN,  CNNs",
          "Modality": "Textual",
          "Evaluation metric": "PPL,  Distinct1  and  Distinct2",
          "Year": "2020"
        },
        {
          "Study": "Montiel-Vasquez  et  al.",
          "Technique(s)": "PBC4cip,  RFM",
          "Modality": "Textual",
          "Evaluation metric": "CEM,  AUC",
          "Year": "2022"
        },
        {
          "Study": "Ayshabi  et  al.",
          "Technique(s)": "Transformer-Encoder, \nDecoder",
          "Modality": "Textual",
          "Evaluation metric": "BLEU,  Human  evaluation",
          "Year": "2021"
        },
        {
          "Study": "Kurashige et  al.",
          "Technique(s)": "RNN",
          "Modality": "Textual",
          "Evaluation metric": "Human  evaluation",
          "Year": "2018"
        },
        {
          "Study": "Rasool et  al.",
          "Technique(s)": "CLM,  LeaderP clustering, \nTGART",
          "Modality": "Textual",
          "Evaluation metric": "Self  evaluation",
          "Year": "2015"
        },
        {
          "Study": "Lee et  al.",
          "Technique(s)": "GPT-3,  RoBERTa",
          "Modality": "Textual",
          "Evaluation metric": "PPL,  Distinct,  EPITOME,  Human evaluation",
          "Year": "2022"
        },
        {
          "Study": "Xie  et  al.",
          "Technique(s)": "GPT, SVR",
          "Modality": "Textual, Speech",
          "Evaluation metric": "PPL,  EMO  ACC,  A.MSE,  V.MSE",
          "Year": "2021"
        },
        {
          "Study": "Tan  et  al.",
          "Technique(s)": "CNN,  LSTM",
          "Modality": "Audio, Textual and  Video",
          "Evaluation metric": "Human  evaluation,  CCC",
          "Year": "2019"
        },
        {
          "Study": "Fung  et  al.",
          "Technique(s)": "CNN,  LSTM",
          "Modality": "Speech,  Audio and  Video",
          "Evaluation metric": "Cross-evaluation  on  datasets",
          "Year": "2018"
        },
        {
          "Study": "Mathur  et  al.",
          "Technique(s)": "8  ML  models,  LSTM, TCN",
          "Modality": "Video",
          "Evaluation metric": "ACC,  AUC,  Precision,  Recall",
          "Year": "2021"
        },
        {
          "Study": "Carolis  et  al.",
          "Technique(s)": "STASM, kNN,  DBN",
          "Modality": "Speech  and  Video",
          "Evaluation metric": "Human  evaluation",
          "Year": "2017"
        },
        {
          "Study": "Bagheri et  al.",
          "Technique(s)": "SAE",
          "Modality": "Image",
          "Evaluation metric": "UTAUT, Friendship questionnaire, \nEngagement  parameter",
          "Year": "2020"
        },
        {
          "Study": "Filho  et  al.",
          "Technique(s)": "RegressionWiSARD",
          "Modality": "Audio and  Video",
          "Evaluation metric": "CCC",
          "Year": "2020"
        },
        {
          "Study": "Leite  et  al.",
          "Technique(s)": "OPPR,  RL",
          "Modality": "Image",
          "Evaluation metric": "Human  evaluation",
          "Year": "2013"
        },
        {
          "Study": "Sorrentino et  al.",
          "Technique(s)": "Reinforcement  Learning",
          "Modality": "Video and  Audio",
          "Evaluation metric": "UTAUT",
          "Year": "2022"
        },
        {
          "Study": "Bagheri et  al.",
          "Technique(s)": "Reinforcement  Learning",
          "Modality": "Video",
          "Evaluation metric": "Godspeed  Questionnaire",
          "Year": "2022"
        },
        {
          "Study": "Daher et  al.",
          "Technique(s)": "NLTK  (Py  package)",
          "Modality": "Textual",
          "Evaluation metric": "RoPe  scale",
          "Year": "2022"
        },
        {
          "Study": "Roller et  al.",
          "Technique(s)": "GPT3",
          "Modality": "Textual",
          "Evaluation metric": "ACUTE-Eval",
          "Year": "2020"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Study": "Rashkin et al.",
          "Dataset": "Empathetic Dialogues",
          "PPL": "16.89",
          "Distinct": "2.5"
        },
        {
          "Study": "Lee et al.",
          "Dataset": "Empathetic Dialogues",
          "PPL": "17.16",
          "Distinct": "3.1"
        },
        {
          "Study": "Sharma et al.",
          "Dataset": "Empathetic Dialogues",
          "PPL": "18.32",
          "Distinct": "-"
        },
        {
          "Study": "Li et al.",
          "Dataset": "Empathetic Dialogues",
          "PPL": "19.09",
          "Distinct": "3.0"
        },
        {
          "Study": "Harilal et al.",
          "Dataset": "Empathetic Dialogues",
          "PPL": "23.60",
          "Distinct": "3.3"
        },
        {
          "Study": "Ayshabi et al.",
          "Dataset": "Empathetic Dialogues",
          "PPL": "23.60",
          "Distinct": "3.3"
        },
        {
          "Study": "Lee et al.",
          "Dataset": "Empathetic Dialogues",
          "PPL": "23.60",
          "Distinct": "3.3"
        },
        {
          "Study": "Xie et al.",
          "Dataset": "Empathetic Dialogues",
          "PPL": "23.60",
          "Distinct": "3.3"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Study": "Barrett-Lennard  et  al.  [101]",
          "Method": "Barrett-Lennard  Relationship \nInventory  (BLRI)",
          "Components/Subscales": "Empathic  understanding,  Level  of \nregard,  Un-conditionality  of  regard, \nCongruence.",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Partially  (cogni- \ntive \naspects)",
          "Year": "1962"
        },
        {
          "Study": "R  Hogan  [102]",
          "Method": "Hogan’s  Empathy  Scale",
          "Components/Subscales": "California  Psychlogical  Inventory \nMinnesota  Multiphasic  Personality \nInventory,  Chapin  Social  Insight \ntest",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \nCognitive  (role- \ntaking)",
          "Year": "1969"
        },
        {
          "Study": "Mehrabian  &  Epstein  [103]",
          "Method": "QMEE/EETS",
          "Components/Subscales": "Susceptibility  to  Emotional  Contagion, \nAppreciation  of  the  Feelings \nof  Unfamiliar  and  Distant \nOthers,  Extreme  Emotional \nResponsiveness,  Tendency  to \nBe  Moved  by  Others’  Positive \nEmotional  Experiences, \nTendency  To  Be  Moved \nby  Others’  Negative  Emotional \nExperiences,  Sympathetic  Tendency, \nand  Willingness  to  Be  in  Contact \nwith  Others  Who  Have  Problems.",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional)",
          "Year": "1972"
        },
        {
          "Study": "Batson  et  al.  [104]",
          "Method": "Emotional  Response  Questionnaire \n(ERQ)",
          "Components/Subscales": "Personal  distress, \nEmpathy/Sympathy",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional)",
          "Year": "1982"
        },
        {
          "Study": "M  H  Davis  [4]",
          "Method": "Interpersonal  Reactivity  Index  (IRI)",
          "Components/Subscales": "Perspective  taking,  Fantasy \nscale,  Empathic  concern, \nPersonal  distress",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \nEmotional, \nCognitive, \n(compassion)",
          "Year": "1983"
        },
        {
          "Study": "Biggam  et  al.  [105]",
          "Method": "Positive  and  Negative  Affect \nSchedule  (PANAS)",
          "Components/Subscales": "Positvie  affect,  Negative  affect",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional)",
          "Year": "1996"
        },
        {
          "Study": "Caruso  et  al.  [106]",
          "Method": "Multidimensional  Emotional \nEmpathy  Scale  (MDEES)",
          "Components/Subscales": "Empathic  suffering,  Positive \nsharing,  Responsive  Crying, \nEmotional  attention,  Feeling \nfor  others,  Emotional  contagion",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional)",
          "Year": "1998"
        },
        {
          "Study": "Baron-Cohen  et  al.  [107]",
          "Method": "Empathy  Quotient  (EQ)",
          "Components/Subscales": "Clinical  Empathy,  Social  distress",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes  (clinical)",
          "Year": "2004"
        },
        {
          "Study": "Jolliffe  et  al.  [108]",
          "Method": "Basic  Empathy  Scale  (BES)",
          "Components/Subscales": "Emotional  congruence,  Cognitive \naspects",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional, \nCognitive)",
          "Year": "2006"
        },
        {
          "Study": "Rieffe  et  al.  [109]",
          "Method": "Emotion  Awareness  Questionnaire",
          "Components/Subscales": "Differentiating  emotions,  Bodily \nawareness,  Verbal  sharing, \nActing  out  emotions,  Attending \nto  other’s  emotions  and \nanalysis  of  own  emotions",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional, \nCognitive)",
          "Year": "2007"
        },
        {
          "Study": "Bartneck  et  al.  [110]",
          "Method": "Godspeed  Questionnaire",
          "Components/Subscales": "Anthropomorphism, \nAnimacy,  Likeability, \nPerceived  Intelligence, \nPerceived  Safety",
          "Associated \n(originally) \nwith  HRI?": "Yes",
          "Addresses Em- \npathy \nDirectly?": "No",
          "Year": "2009"
        },
        {
          "Study": "Spreng  et  al.  [111]",
          "Method": "Toronto  Empathy  Questionnaire \n(TEQ)",
          "Components/Subscales": "Emotional  contagion,  Emotional \ncomprehension,  Sympathetic \nphysological  arousal,  Con-specific \naltruism",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional, \nCognitive)",
          "Year": "2009"
        },
        {
          "Study": "Rieffe  et  al.  [112]",
          "Method": "Empathy  Questionnaire  for \nChildren   and   Adolescents   (EmQue- \nCA)",
          "Components/Subscales": "Affective  empathy,  Cognitive  empathy, \nIntention  to  comfort",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional, \nCognitive)",
          "Year": "2010"
        },
        {
          "Study": "Shen  et  al.  [113]",
          "Method": "State  Empathy  Questionnaire",
          "Components/Subscales": "Emotive,  Cognitive, \nAssociative  (ability  to  relate)",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional, \nCognitive)",
          "Year": "2010"
        },
        {
          "Study": "Reniers  et  al.  [114]",
          "Method": "Questionnaire  of  Cognitive  and \nAffective  Empathy  (QCAE)",
          "Components/Subscales": "Cognitive  empthy,  Emotional \nempathy",
          "Associated \n(originally) \nwith  HRI?": "No",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(Emotional, \nCognitive)",
          "Year": "2011"
        },
        {
          "Study": "Charrier  et  al.  [100]",
          "Method": "Robot’s  Perceived  Empathy  (RoPE)",
          "Components/Subscales": "Empathic  understanding,  Empathic \nresponse,  Filler  items  (for  HRI)",
          "Associated \n(originally) \nwith  HRI?": "Yes",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(perceived)",
          "Year": "2019"
        },
        {
          "Study": "Putta  et  al.  [99]",
          "Method": "QMAE",
          "Components/Subscales": "Empathic  Understanding,  Empathic \nresponse,  Empathic  relationship",
          "Associated \n(originally) \nwith  HRI?": "Yes",
          "Addresses Em- \npathy \nDirectly?": "Yes \n(perceived)",
          "Year": "2022"
        },
        {
          "Study": "Leite  et  al.  [95]",
          "Method": "Friendship  Questionnaire",
          "Components/Subscales": "Intimacy,  Emotional  security, \nSocial  presence",
          "Associated \n(originally) \nwith  HRI?": "Yes",
          "Addresses Em- \npathy \nDirectly?": "No",
          "Year": "2006"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Study": "Rashkin et al.",
          "Dataset": "Empathetic Dialogues",
          "Modality": "Textual",
          "Source": "Facebook conversations",
          "Year": "2018"
        },
        {
          "Study": "Sharma et al.",
          "Dataset": "EPITOME",
          "Modality": "Textual",
          "Source": "Reddit (mental health)",
          "Year": "2020"
        },
        {
          "Study": "Amanova et al.",
          "Dataset": "Daily Dialogue",
          "Modality": "Textual",
          "Source": "AMI, MapTask, SWBD",
          "Year": "2016"
        },
        {
          "Study": "Welivita et al.",
          "Dataset": "EmotionalDialogues   in   OpenSubti- \ntles (EDOS)",
          "Modality": "Textual",
          "Source": "Open Subtitles",
          "Year": "2021"
        },
        {
          "Study": "Mathur et al.",
          "Dataset": "Storyteller Robot Dataset",
          "Modality": "Textual",
          "Source": "Stories",
          "Year": "2021"
        },
        {
          "Study": "Hosseini et al.",
          "Dataset": "IEMPATHIZE",
          "Modality": "Textual",
          "Source": "Online (cancer discussion) \nforums",
          "Year": "2021"
        },
        {
          "Study": "Harilal et al.",
          "Dataset": "Medical Question Answering (MQA)",
          "Modality": "Textual",
          "Source": "eHealth forums",
          "Year": "2020"
        },
        {
          "Study": "Barros et al.",
          "Dataset": "OMG-Empathy",
          "Modality": "Video",
          "Source": "Participant recordings",
          "Year": "2019"
        },
        {
          "Study": "Chen et al.",
          "Dataset": "Neural Image Commenting with Empathy (NICE)",
          "Modality": "Image",
          "Source": "Images   (websites),   Com- \nments   user generated",
          "Year": "2021"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The arts, empathy, and aristotle",
      "authors": [
        "David Swanger"
      ],
      "year": "1993",
      "venue": "Journal of Aesthetic Education"
    },
    {
      "citation_id": "2",
      "title": "The science of empathy",
      "authors": [
        "Helen Riess"
      ],
      "year": "2017",
      "venue": "Journal of patient experience"
    },
    {
      "citation_id": "3",
      "title": "The origins and social significance of empathy-related responding. a review of empathy and moral development: implications for caring and justice by ml hoffman",
      "authors": [
        "Nancy Eisenberg",
        "Amanda Sheffield"
      ],
      "year": "2001",
      "venue": "Social Justice Research"
    },
    {
      "citation_id": "4",
      "title": "Measuring individual differences in empathy: Evidence for a multidimensional approach",
      "authors": [
        "H Mark",
        "Davis"
      ],
      "year": "1983",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "5",
      "title": "Empathy: Its ultimate and proximate bases",
      "authors": [
        "D Stephanie",
        "Frans Bm De Preston",
        "Waal"
      ],
      "year": "2002",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "6",
      "title": "Empathy in virtual agents and robots: A survey",
      "authors": [
        "Ana Paiva",
        "Iolanda Leite",
        "Hana Boukricha",
        "Ipke Wachsmuth"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "7",
      "title": "Comparison of cognitive empathy, emotional empathy, and social functioning in different age groups",
      "authors": [
        "Zeinab Khanjani",
        "Mosanezhad Elnaz",
        "Issa Jeddi",
        "Saeede Hekmati",
        "Mahin Khalilzade",
        "Morteza Etemadi Nia",
        "Parvaneh Andalib",
        "Ashrafian"
      ],
      "year": "2015",
      "venue": "Australian Psychologist"
    },
    {
      "citation_id": "8",
      "title": "The cognitive, affective, and somatic empathy scales (cases) for children",
      "authors": [
        "Adrian Raine",
        "Frances Chen"
      ],
      "year": "2018",
      "venue": "Journal of Clinical Child & Adolescent Psychology"
    },
    {
      "citation_id": "9",
      "title": "The influence of empathy in human-robot relations",
      "authors": [
        "Iolanda Leite",
        "Andre Pereira",
        "Samuel Mascarenhas",
        "Carlos Martinho",
        "Rui Prada",
        "Ana Paiva"
      ],
      "year": "2013",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "10",
      "title": "Empathic interaction using the computational emotion model",
      "authors": [
        "Zeeshan Rasool",
        "Naoki Masuyama",
        "Md Nazrul Islam",
        "Chu Loo"
      ],
      "year": "2015",
      "venue": "2015 IEEE Symposium Series on Computational Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Socially assistive robots: The link between personality, empathy, physiological signals, and task performance",
      "authors": [
        "Adriana Tapus",
        "J Maja",
        "Mataric"
      ],
      "year": "2008",
      "venue": "AAAI spring symposium: emotion, personality, and social behavior"
    },
    {
      "citation_id": "12",
      "title": "The cognitive, affective, and somatic empathy scales (cases) for children",
      "authors": [
        "Adrian Raine",
        "Frances Chen"
      ],
      "year": "2018",
      "venue": "Journal of Clinical Child & Adolescent Psychology"
    },
    {
      "citation_id": "13",
      "title": "Towards artificial empathy",
      "authors": [
        "Minoru Asada"
      ],
      "year": "2015",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "14",
      "title": "Empathic computing and human robot interaction",
      "authors": [
        "Mark Billinghurst"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "15",
      "title": "Affective computing and sentiment analysis. A practical guide to sentiment analysis",
      "authors": [
        "Erik Cambria",
        "Dipankar Das",
        "Sivaji Bandyopadhyay",
        "Antonio Feraco"
      ],
      "year": "2017",
      "venue": "Affective computing and sentiment analysis. A practical guide to sentiment analysis"
    },
    {
      "citation_id": "16",
      "title": "Toward artificial emotional intelligence for cooperative social human-machine interaction",
      "authors": [
        "Abhijit Berat A Erol",
        "Patrick Majumdar",
        "Paul Benavidez",
        "Kim-Kwang Raymond Rad",
        "Mo Choo",
        "Jamshidi"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "17",
      "title": "Can i feel your pain? the biological and socio-cognitive factors shaping people's empathy with social robots",
      "authors": [
        "Joanna Malinowska"
      ],
      "year": "2022",
      "venue": "International Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "18",
      "title": "The social assistive robot and companion",
      "authors": [
        "Sara Cooper",
        "Alessandro Di Fava",
        "Carlos Vivas",
        "Luca Marchionni",
        "Francesco Ferro",
        "Ari"
      ],
      "year": "2020",
      "venue": "2020 29th IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "19",
      "title": "Computers that care: investigating the effects of orientation of emotion exhibited by an embodied computer agent",
      "authors": [
        "Scott Brave",
        "Clifford Nass",
        "Kevin Hutchinson"
      ],
      "year": "2005",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "20",
      "title": "Simulating empathic behavior in a social assistive robot",
      "authors": [
        "Berardina De Carolis",
        "Stefano Ferilli",
        "Giuseppe Palestra"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "21",
      "title": "Towards human-robot affective co-evolution overcoming oppositions in constructing emotions and empathy",
      "authors": [
        "Luisa Damiano",
        "Paul Dumouchel",
        "Hagen Lehmann"
      ],
      "year": "2015",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "22",
      "title": "Perceptions and opinions of patients about mental health chatbots: scoping review",
      "authors": [
        "Alaa Abd-Alrazaq",
        "Mohannad Alajlani",
        "Nashva Ali",
        "Kerstin Denecke",
        "Bridgette Bewick",
        "Mowafa Househ"
      ],
      "year": "2021",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "23",
      "title": "Transformers in vision: A survey",
      "authors": [
        "Salman Khan",
        "Muzammal Naseer",
        "Munawar Hayat",
        "Fahad Syed Waqas Zamir",
        "Mubarak Shahbaz Khan",
        "Shah"
      ],
      "year": "2022",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "24",
      "title": "A multimodal lstm for predicting listener empathic responses over time",
      "authors": [
        "Pascale Fung",
        "Dario Bertero",
        "Yan Wan",
        "Anik Dey",
        "Ricky Ho Yin Chan",
        "Farhad Bin Siddique",
        "Yang Yang",
        "Chien-Sheng Wu",
        "Ruixi Lin",
        "; Zhi-Xuan Tan",
        "Arushi Goel",
        "Thanh-Son Nguyen",
        "Desmond Ong"
      ],
      "year": "2018",
      "venue": "Computational Linguistics and Intelligent Text Processing"
    },
    {
      "citation_id": "25",
      "title": "Elahe Bagheri, Oliver Roesler, Hoang-Long Cao, and Bram Vanderborght. A reinforcement learning based cognitive empathy framework for social robots",
      "authors": [
        "Alessandra Sorrentino",
        "Gustavo Assunc ¸a ˜o",
        "Filippo Cavallo",
        "Laura Fiorini",
        "Paulo Menezes"
      ],
      "year": "2021",
      "venue": "Social Robotics: 14th International Conference, ICSR 2022"
    },
    {
      "citation_id": "26",
      "title": "A modular data-driven architecture for empathetic conversational agents",
      "authors": [
        "Vincenzo Scotti",
        "Roberto Tedesco",
        "Licia Sbattella"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Big Data and Smart Computing (BigComp)"
    },
    {
      "citation_id": "27",
      "title": "Intrinsically motivated reinforcement learning for human-robot interaction in the real-world",
      "authors": [
        "Ahmed Hussain Qureshi",
        "Yutaka Nakamura",
        "Yuichiro Yoshikawa",
        "Hiroshi Ishiguro"
      ],
      "year": "2018",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "28",
      "title": "A multi-resolution mechanism with multiple decoders for empathetic dialogue generation",
      "authors": [
        "Mk Ayshabi",
        "Mary Sumam",
        "Idicula"
      ],
      "year": "2021",
      "venue": "2021 8th International Conference on Smart Computing and Communications (ICSCC)"
    },
    {
      "citation_id": "29",
      "title": "Does gpt-3 generate empathetic dialogues? a novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation",
      "authors": [
        "Young-Jun Lee",
        "Chae-Gyun Lim",
        "Ho-Jin Choi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "A computational approach to understanding empathy expressed in textbased mental health support",
      "authors": [
        "Ashish Sharma",
        "Adam Miner",
        "David Atkins",
        "Tim Althoff"
      ],
      "year": "2020",
      "venue": "A computational approach to understanding empathy expressed in textbased mental health support",
      "arxiv": "arXiv:2009.08441"
    },
    {
      "citation_id": "31",
      "title": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "Eric Hannah Rashkin",
        "Margaret Smith",
        "Y-Lan Li",
        "Boureau"
      ],
      "year": "2018",
      "venue": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "arxiv": "arXiv:1811.00207"
    },
    {
      "citation_id": "32",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "33",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "34",
      "title": "Jing Yang Lee, Kong Aik Lee, and Woon Seng Gan. Improving contextual coherence in variational personalized and empathetic dialogue agents",
      "authors": [
        "Mahshid Hosseini",
        "Cornelia Caragea"
      ],
      "year": "2021",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "35",
      "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "authors": [
        "Chin-Yew Lin"
      ],
      "year": "2004",
      "venue": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models",
      "arxiv": "arXiv:1510.03055"
    },
    {
      "citation_id": "36",
      "title": "An empathetic ai coach for self-attachment therapy",
      "authors": [
        "Lisa Alazraki",
        "Ali Ghachem"
      ],
      "year": "2021",
      "venue": "2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI)"
    },
    {
      "citation_id": "37",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "38",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "39",
      "title": "Soundnet: Learning sound representations from unlabeled video",
      "authors": [
        "Yusuf Aytar",
        "Carl Vondrick",
        "Antonio Torralba"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "40",
      "title": "Contextualized affect representations for emotion recognition",
      "authors": [
        "Elvis Saravia",
        "Hsien-Chi Toby Liu",
        "Yen-Hao Huang",
        "Junlin Wu",
        "Yi-Shin Chen",
        "Ana Carer ; Suchin Gururangan",
        "´ Marasovic",
        "Swabha Swayamdipta",
        "Kyle Lo",
        "Iz Beltagy",
        "Doug Downey",
        "Noah Smith"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing",
      "arxiv": "arXiv:2004.10964"
    },
    {
      "citation_id": "41",
      "title": "A large-scale dataset for empathetic response generation",
      "authors": [
        "Anuradha Welivita",
        "Yubo Xie",
        "Pearl Pu"
      ],
      "year": "2019",
      "venue": "Zhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu, and Pascale Fung. Moel: Mixture of empathetic listeners",
      "arxiv": "arXiv:1908.07687"
    },
    {
      "citation_id": "42",
      "title": "Empdg: Multi-resolution interactive empathetic dialogue generation",
      "authors": [
        "Qintong Li",
        "Hongshen Chen",
        "Zhaochun Ren",
        "Pengjie Ren",
        "Zhaopeng Tu",
        "Zhumin Chen"
      ],
      "venue": "Empdg: Multi-resolution interactive empathetic dialogue generation"
    },
    {
      "citation_id": "43",
      "title": "Recipes for building an open-domain chatbot",
      "authors": [
        "Stephen Roller",
        "Emily Dinan",
        "Naman Goyal",
        "Da Ju",
        "Mary Williamson",
        "Yinhan Liu",
        "Jing Xu",
        "Myle Ott",
        "Kurt Shuster",
        "Eric Smith",
        "Y-Lan Boureau",
        "Jason Weston"
      ],
      "year": "2020",
      "venue": "Recipes for building an open-domain chatbot"
    },
    {
      "citation_id": "44",
      "title": "Caro: an empathetic health conversational chatbot for people with major depression",
      "authors": [
        "Nidhin Harilal",
        "Rushil Shah",
        "Saumitra Sharma",
        "Vedanta Bhutani"
      ],
      "year": "2020",
      "venue": "Proceedings of the 7th ACM IKDD CoDS and 25th COMAD"
    },
    {
      "citation_id": "45",
      "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "authors": [
        "Chia-Wei Liu",
        "Ryan Lowe",
        "Michael Iulian V Serban",
        "Laurent Noseworthy",
        "Joelle Charlin",
        "Pineau"
      ],
      "year": "2016",
      "venue": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "arxiv": "arXiv:1603.08023"
    },
    {
      "citation_id": "46",
      "title": "Hierarchical neural network generative models",
      "authors": [
        "Alessandro Iulian V Serban",
        "Yoshua Sordoni",
        "Aaron Bengio",
        "Joelle Courville",
        "Pineau"
      ],
      "venue": "Hierarchical neural network generative models"
    },
    {
      "citation_id": "47",
      "title": "Edwin Carlos Montiel-Va ´zquez, Jorge Adolfo Ramírez Uresti, and Octavio Loyola-Gonza ´lez. An explainable artificial intelligence approach for detecting empathy in textual communication",
      "year": "2015",
      "venue": "] for movie dialogues",
      "arxiv": "arXiv:1507.04808"
    },
    {
      "citation_id": "48",
      "title": "Pattern-based classification: A unifying perspective",
      "authors": [
        "Siegfried Bjo ¨ Rn Bringmann",
        "Albrecht Nijssen",
        "Zimmermann"
      ],
      "year": "2011",
      "venue": "Pattern-based classification: A unifying perspective",
      "arxiv": "arXiv:1111.6191"
    },
    {
      "citation_id": "49",
      "title": "Aspectbased sentiment analysis via affective knowledge enhanced graph convolutional networks",
      "authors": [
        "Bin Liang",
        "Hang Su",
        "Lin Gui",
        "Erik Cambria",
        "Ruifeng Xu"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "50",
      "title": "Alfredo Cuzzocrea and Giovanni Pilato. Taxonomy-based detection of user emotions for advanced artificial intelligent applications",
      "authors": [
        "Ondrej Bruna",
        "Hakob Avetisyan",
        "Jan Holub"
      ],
      "year": "2016",
      "venue": "International Conference on Hybrid Artificial Intelligence Systems"
    },
    {
      "citation_id": "51",
      "title": "Gabriel Ichcanziho Pe ´rez-Landa, Octavio Loyola-Gonza ´lez, and Miguel Angel Medina-Pe ´rez. An explainable artificial intelligence model for detecting xenophobic tweets",
      "authors": [
        "Jetze Schuurmans",
        "Flavius Frasincar"
      ],
      "year": "2020",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "52",
      "title": "Dr Alassane Ndiaye, and Dr Dominik Heckmann. Weka: Practical machine learning tools and techniques with java implementations",
      "authors": [
        "Rossen Dimov",
        "Michael Feld",
        "Dr Michael"
      ],
      "year": "2007",
      "venue": "WS"
    },
    {
      "citation_id": "53",
      "title": "Multi-class auc metrics and weighted alternatives",
      "authors": [
        "Mark Hall",
        "Eibe Frank",
        "Geoffrey Holmes",
        "Bernhard Pfahringer",
        "Peter Reutemann",
        "Ian Witten"
      ],
      "year": "2008",
      "venue": "2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)"
    },
    {
      "citation_id": "54",
      "title": "Context respectful counseling agent integrated with robot nodding for dialog promotion",
      "authors": [
        "Kentarou Kurashige",
        "Setsuo Tsuruta",
        "Eriko Sakurai",
        "Yoshitaka Sakurai",
        "Rainer Knauf",
        "Ernesto Damiani"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "55",
      "title": "Using contexts to supervise a collaborative process",
      "authors": [
        "J Avelino",
        "Setsuo Gonzalez",
        "Yoshitaka Tsuruta",
        "Johann Sakurai",
        "Kouhei Nguyen",
        "Ken Takada",
        "Uchida"
      ],
      "year": "2011",
      "venue": "AI EDAM"
    },
    {
      "citation_id": "56",
      "title": "Eliza-a computer program for the study of natural language communication between man and machine",
      "authors": [
        "Joseph Weizenbaum"
      ],
      "year": "1966",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "57",
      "title": "Baijun Xie and Chung Hyuk Park. Empathetic robot with transformerbased dialogue agent",
      "authors": [
        "Kentarou Kurashige",
        "Setsuo Tsuruta",
        "Eriko Sakurai",
        "Yoshitaka Sakurai",
        "Rainer Knauf",
        "Ernesto Damiani",
        "Andrea Kutics"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "58",
      "title": "Georgios Paltoglou and Michael Thelwall. Seeing stars of valence and arousal in blog posts",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "59",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "60",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "Florian Eyben",
        "Felix Weninger",
        "Florian Gross",
        "Bjo ¨ Rn Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "61",
      "title": "Deep face recognition",
      "authors": [
        "Andrea Omkar M Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "Deep face recognition"
    },
    {
      "citation_id": "62",
      "title": "Enhancing the ted-lium corpus with selected data for language modeling and more ted talks",
      "authors": [
        "Anthony Rousseau",
        "Paul Dele ´glise",
        "Yannick Esteve"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "63",
      "title": "Libsvm: a library for support vector machines",
      "authors": [
        "Chih-Chung Chang",
        "Chih-Jen Lin"
      ],
      "year": "2011",
      "venue": "ACM transactions on intelligent systems and technology (TIST)"
    },
    {
      "citation_id": "64",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "Stefan Bjo ¨ Rn Schuller",
        "Anton Steidl",
        "Batliner"
      ],
      "year": "2009",
      "venue": "The interspeech 2009 emotion challenge"
    },
    {
      "citation_id": "65",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wo ¨ Llmer",
        "Bjo ¨ Rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "66",
      "title": "Modeling user empathy elicited by a robot storyteller",
      "authors": [
        "Leena Mathur",
        "Micol Spitale",
        "Hao Xi",
        "Jieyun Li",
        "Maja Mataric"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "67",
      "title": "Marcel Heerink, Ben Kro ¨ se, Vanessa Evers, and Bob Wielinga. Assessing acceptance of assistive social agent technology by older adults: the almere model",
      "authors": [
        "Elahe Bagheri",
        "Pablo Esteban",
        "Hoang-Long Cao",
        "Albert De Beir",
        "Dirk Lefeber",
        "Bram Vanderborght",
        "; Shuo Yang",
        "Li-Fang Chen",
        "Tao Yan",
        "Yun-Hao Zhao",
        "Ye-Jia Fan",
        "; Candace",
        "L Sidner",
        "Cory Kidd",
        "Christopher Lee",
        "Neal Lesh"
      ],
      "year": "2004",
      "venue": "2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)"
    },
    {
      "citation_id": "68",
      "title": "A weightless regression system for predicting multi-modal empathy",
      "authors": [
        "Leopoldo Ad Lusquino Filho",
        "F Luiz",
        "Oliveira",
        "C Hugo",
        "Carneiro",
        "Aluízio Gabriel P Guarisa",
        "Felipe Mg Franc Lima Filho",
        "Priscila Mv ¸a",
        "Lima"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "69",
      "title": "Local smoothness in terms of variance: the adaptive gaussian filter",
      "year": "2000",
      "venue": "Citeseer, 2000. Jaakko Sauvola and Matti Pietika ¨inen. Adaptive document image binarization"
    },
    {
      "citation_id": "70",
      "title": "A computational approach to edge detection",
      "authors": [
        "John Canny"
      ],
      "year": "1986",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "71",
      "title": "A threshold selection method from gray-level histograms",
      "authors": [
        "Nobuyuki Otsu"
      ],
      "year": "1979",
      "venue": "IEEE transactions on systems, man, and cybernetics"
    },
    {
      "citation_id": "72",
      "title": "Hmm-based audio keyword generation",
      "authors": [
        "Min Xu",
        "Ling-Yu Duan",
        "Jianfei Cai",
        "Liang-Tien Chia",
        "Changsheng Xu",
        "Qi Tian"
      ],
      "year": "2004",
      "venue": "Pacific-Rim Conference on Multimedia"
    },
    {
      "citation_id": "73",
      "title": "Prediction of palm oil production with an enhanced n-tuple regression network",
      "authors": [
        "Leopoldo Ad Lusquino Filho",
        "F Luiz",
        "Oliveira",
        "L Aluizio",
        "Filho",
        "Priscila Gabriel P Guarisa",
        "Felipe Mg Franc Lima",
        "¸a"
      ],
      "venue": "Prediction of palm oil production with an enhanced n-tuple regression network"
    },
    {
      "citation_id": "74",
      "title": "Clustering data streams with weightless neural networks",
      "authors": [
        "Priscila Douglas De O Cardoso",
        "Massimo Lima",
        "Joa Gregorio",
        "Felipe Mg Franc ˜o Gama",
        "M ¸a",
        "J Gregorio",
        "Gama"
      ],
      "year": "2011",
      "venue": "Clustering data streams with weightless neural networks"
    },
    {
      "citation_id": "75",
      "title": "Geometric feature-based facial emotion recognition using two-stage fuzzy reasoning model",
      "authors": [
        "Md Islam",
        "Chu Loo"
      ],
      "year": "2014",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "76",
      "title": "Rsmat: Robust simultaneous modeling and tracking",
      "authors": [
        "Jesus Nuevo",
        "Luis Bergasa",
        "Pedro Jime ´nez"
      ],
      "year": "2010",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "77",
      "title": "Modelling empathy in social robotic companions",
      "authors": [
        "Iolanda Leite",
        "Andre Pereira",
        "Ginevra Castellano",
        "Samuel Mascarenhas",
        "Carlos Martinho",
        "Ana Paiva"
      ],
      "year": "2011",
      "venue": "International conference on user modeling, adaptation, and personalization"
    },
    {
      "citation_id": "78",
      "title": "",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin"
      ],
      "venue": ""
    },
    {
      "citation_id": "79",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "year": "2018",
      "venue": "Evaluating empathy in artificial agents. arXiv Conference on Intelligent Robots and Systems (IROS 2018)"
    },
    {
      "citation_id": "80",
      "title": "The development of a companionship scale for artificial pets",
      "authors": [
        "Ding-Bang Luh",
        "Carolina Li",
        "Yu-Jung Kao"
      ],
      "year": "2015",
      "venue": "Interacting with Computers"
    },
    {
      "citation_id": "81",
      "title": "Axe ux: Exploring long-term user experience with iscale and attrakdiff",
      "authors": [
        "Tanja Walsh",
        "Jari Varsaluoma",
        "Sari Kujala",
        "Piia Nurkka",
        "Helen Petrie",
        "Chris Power"
      ],
      "year": "2014",
      "venue": "Proceedings of the 18th international academic mindtrek conference: Media business, management, content & services"
    },
    {
      "citation_id": "82",
      "title": "The empathy assessment index (eai): A confirmatory factor analysis of a multidimensional model of empathy",
      "authors": [
        "Cynthia Lietz",
        "Karen Gerdes",
        "Fei Sun",
        "Jennifer Geiger",
        "M Alex Wagaman",
        "Elizabeth Segal"
      ],
      "year": "2011",
      "venue": "Journal of the Society for Social Work and Research"
    },
    {
      "citation_id": "83",
      "title": "Towards empathic artificial tutors",
      "authors": [
        "Amol Deshmukh",
        "Ginevra Castellano",
        "Arvid Kappas",
        "Wolmet Barendregt",
        "Fernando Nabais",
        "Ana Paiva",
        "Tiago Ribeiro",
        "Iolanda Leite",
        "Ruth Aylett"
      ],
      "year": "2013",
      "venue": "2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "84",
      "title": "Studying effects of incorporating automated affect perception with spoken dialog in social robots",
      "authors": [
        "Ali Mollahosseini",
        "Hojjat Abdollahi",
        "Mohammad Mahoor"
      ],
      "year": "2018",
      "venue": "2018 27th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "85",
      "title": "The omg-empathy dataset: Evaluating the impact of affective behavior in storytelling",
      "authors": [
        "Pablo Barros",
        "Nikhil Churamani",
        "Angelica Lim",
        "Stefan Wermter"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "86",
      "title": "Caisa at wassa 2022: Adapter-tuning for empathy prediction",
      "authors": [
        "Allison Lahnala",
        "Charles Welch",
        "Lucie Flek"
      ],
      "year": "2022",
      "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis"
    },
    {
      "citation_id": "87",
      "title": "Creating annotated dialogue resources: Cross-domain dialogue act classification",
      "authors": [
        "Dilafruz Amanova",
        "Volha Petukhova",
        "Dietrich Klakow"
      ],
      "year": "2016",
      "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)"
    },
    {
      "citation_id": "88",
      "title": "",
      "authors": [
        "O ¨ Zge Nilay Yalc ¸ın"
      ],
      "venue": ""
    },
    {
      "citation_id": "89",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "Matteo Spezialetti",
        "Giuseppe Placidi",
        "Silvia Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "90",
      "title": "Empathy scale adaptation for artificial agents: a review with a new subscale proposal",
      "authors": [
        "Harika Putta",
        "Karl Daher",
        "Mira Kamali",
        "Omar Khaled",
        "Denis Lalanne",
        "Elena Mugellini"
      ],
      "year": "2022",
      "venue": "2022 8th International Conference on Control, Decision and Information Technologies (CoDIT)"
    },
    {
      "citation_id": "91",
      "title": "Ame ´lie Cordier",
      "authors": [
        "Laurianne Charrier",
        "Alisa Rieger",
        "Alexandre Galdeano"
      ],
      "venue": "Ame ´lie Cordier"
    },
    {
      "citation_id": "92",
      "title": "Dimensions of therapist response as causal factors in therapeutic change",
      "authors": [
        "Salima Mathieu Lefort",
        "Hassas"
      ],
      "year": "1962",
      "venue": "2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "93",
      "title": "Development of an empathy scale",
      "authors": [
        "Robert Hogan"
      ],
      "year": "1969",
      "venue": "Journal of consulting and clinical psychology"
    },
    {
      "citation_id": "94",
      "title": "A measure of emotional",
      "authors": [
        "Albert Mehrabian",
        "Norman Epstein",
        "Empathy"
      ],
      "year": "1972",
      "venue": "A measure of emotional"
    },
    {
      "citation_id": "95",
      "title": "More evidence that empathy is a source of altruistic motivation",
      "authors": [
        "Miho Toi",
        "C Daniel Batson"
      ],
      "year": "1982",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "96",
      "title": "The personality of the scottish police officer: The issue of positive and negative affectivity",
      "authors": [
        "H Fiona",
        "Kevin Biggam",
        "Power"
      ],
      "year": "1996",
      "venue": "Personality and Individual Differences"
    },
    {
      "citation_id": "97",
      "title": "Simon Baron-Cohen and Sally Wheelwright. The empathy quotient: an investigation of adults with asperger syndrome or high functioning autism, and normal sex differences",
      "authors": [
        "R David",
        "John Caruso",
        "Mayer"
      ],
      "year": "1998",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "98",
      "title": "Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots",
      "authors": [
        "Darrick Jolliffe",
        "David Farrington ; Carolien Rieffe",
        "Mark Meerum Terwogt",
        "Konstantinos Petrides",
        "Richard Cowan",
        "Anne Miers",
        "Abigail Tolland"
      ],
      "year": "2006",
      "venue": "International journal of social robotics"
    },
    {
      "citation_id": "99",
      "title": "The toronto empathy questionnaire: Scale development and initial validation of a factor-analytic solution to multiple empathy measures",
      "authors": [
        "* Nathan Spreng",
        "Margaret Mckinnon",
        "Raymond Mar",
        "Brian Levine ; Carolien Rieffe",
        "Lizet Ketelaar",
        "Carin Wiefferink"
      ],
      "year": "2009",
      "venue": "Personality and individual differences"
    },
    {
      "citation_id": "100",
      "title": "On a scale of state empathy during message processing",
      "authors": [
        "Lijiang Shen"
      ],
      "year": "2010",
      "venue": "Western Journal of Communication"
    },
    {
      "citation_id": "101",
      "title": "Use of humor by a healthcare robot positively affects user perceptions and behavior",
      "authors": [
        "Renate Lep Reniers",
        "Rhiannon Corcoran",
        "Richard Drake",
        "Nick Shryane",
        "Birgit Vo ¨ Llm ; Deborah L Johanson",
        "Ho Seok Ahn",
        "J Lim",
        "Christopher Lee",
        "Gabrielle Sebaratnam",
        "Bruce Macdonald",
        "Elizabeth Broadbent"
      ],
      "year": "2011",
      "venue": "Journal of personality assessment"
    },
    {
      "citation_id": "102",
      "title": "Empathy display influence on human-robot interactions: a pilot study",
      "authors": [
        "Laurianne Charrier",
        "Alisa Rieger",
        "Alexandre Galdeano",
        "Ame ´lie Cordier",
        "Mathieu Lefort",
        "Salima Hassas"
      ],
      "year": "2019",
      "venue": "2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    }
  ]
}