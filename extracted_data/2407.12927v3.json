{
  "paper_id": "2407.12927v3",
  "title": "Textualized And Feature-Based Models For Compound Multimodal Emotion Recognition In The Wild",
  "published": "2024-07-17T18:01:25Z",
  "authors": [
    "Nicolas Richet",
    "Soufiane Belharbi",
    "Haseeb Aslam",
    "Meike Emilie Schadt",
    "Manuela González-González",
    "Gustave Cortal",
    "Alessandro Lameiras Koerich",
    "Marco Pedersoli",
    "Alain Finkel",
    "Simon Bacon",
    "Eric Granger"
  ],
  "keywords": [
    "Emotion Recognition",
    "Compound Expressions",
    "Multimodal Learning",
    "Multimodal Textualization",
    "Large Language Models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Systems for multimodal emotion recognition (ER) are commonly trained to extract features from different modalities (e.g., visual, audio, and textual) that are combined to predict individual basic emotions. However, compound emotions often occur in real-world scenarios, and the uncertainty of recognizing such complex emotions over diverse modalities is challenging for feature-based models. As an alternative, emerging large language models (LLMs) like BERT and LLaMA can rely on explicit non-verbal cues that may be translated from different non-textual modalities (e.g., audio and visual) into text. Textualization of modalities augments data with emotional cues to help the LLM encode the interconnections between all modalities in a shared text space. In such text-based models, prior knowledge of ER tasks is leveraged to textualize relevant non-verbal cues such as audio tone from vocal expressions, and action unit intensity from facial expressions. Since the pre-trained weights are publicly available for many LLMs, training on large-scale datasets is unnecessary, allowing to fine-tune for downstream tasks such as compound ER (CER). This paper compares the potential of text-and feature-based approaches for compound multimodal ER in videos. Experiments were conducted on the challenging C-EXPR-DB dataset in the wild for CER, and contrasted with results on the MELD dataset for basic ER. Our results indicate that multimodal textualization provides lower accuracy than feature-based models on C-EXPR-DB, where text transcripts are captured in the wild. However, higher accuracy can be achieved when the video data has rich transcripts. Our code is available at: github.com/nicolas-richet/featurevs-text-compound-emotion.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition (ER) plays a critical role in human behavior analysis, human-computer interaction, and affective computing  [13, 60] . Research in ER mainly focuses on recognizing the seven basic emotions -anger, surprise, disgust, enjoyment, fear, sadness, and contempt  [4, 46] . Recently, there has been growing interest in recognizing complex emotions commonly occurring in real-world scenarios, such as compound emotions, where a mixture of emotions is exhibited  [26] . ER is more challenging for complex emotions because they are often ambiguous and subtle, and can be easily confused with basic emotions. Despite the availability of multiple modalities that can potentially help to recognize these complex emotions  [70] , they can introduce additional uncertainty and conflict  [23, 63] .\n\nMultimodal information, e.g., faces, voice, and text extracted from videos, has been used extensively to develop robust ER models  [1, 60, 62] . Convolutional and transformer-based backbones are commonly trained to extract discriminant features from each modality. These include vision backbones such as ResNet  [18]  and audio backbones such as VGGish  [19] . A fusion model is required to combine features from verbal (spoken text) and nonverbal cues (visual and audio), thereby producing contextualized features to predict accurate emotion classes  [39] . Multimodal learning allows for building complex joint feature representations that can achieve high accuracy for ER. This feature-based approach has driven much progress in ER  [60] . Indeed, it requires simple and minimal emotionclass annotations -typically enough to allow these models to learn to automatically extract and combine relevant features from different modalities for prediction. However, using only a single emotion class for CER in real-world scenarios and without any other guidance, such as output supervision or input cues, is challenging, especially using videos captured in the wild  [30] . Recently, another multimodal learning approach called TextMI has been proposed for sentiment analysis  [17]  in videos. In contrast with the feature-based approach, input modalities like audio and visual are textualized. Based on prior knowledge of the task, the authors propose extracting nonverbal cues deemed relevant to the task in the form of a descriptive text. This can include a textual description of action unit (AU) intensity from visual  [12, 14]  and the tone of the audio. This conversion of modalities to text could be seen as an expert-based data augmentation with emotion-related cues used as inputs for the model during training and evaluation. This augmentation provides the models with direct guidance and emotional context to learn the task at hand. Since all modalities are formatted as text, a language model is required to better understand the interconnections between words and modalities and their relations to emotions. However, stateof-the-art language models are typically large, and training them requires a considerable amount of text data, which is not always available. The recent surge of large language models (LLMs)  [72]  made their use possible  [17] . Powerful LLMs such as BERT  [10]  and LLaMA  [56]  have been pretrained, and their weights have been made public, allowing us to fine-tune these models for downstream tasks  [22] . Their application in multimodal ER provides a simple method for multimodal fusion, and results shown in  [17]  are promising. Multimodal textualization remains largely unexplored in the literature. In a very recent work  [5] , modality descriptions are employed to model output supervision for emotional reasoning and recognition. This differs from TextMI  [17] , which uses these cues as inputs. In this paper, we analyze text-based approaches like TextMI for multimodal CER. This paper focuses on the following question: how does textualized modeling perform against feature-based modeling for CER in videos? As shown in Fig.  1 (a), feature-based methods employ different backbone models to automatically extract features from each modality. Feature-level fusion models also allow us to automatically learn joint feature representations for ER, although combining diverse modalities over videos remains a challenge  [39] . The textualized approach  [17]  simplifies fusion since all nonverbal modalities (visual and audio) are converted into a single textual modality (see Fig.  1(b) ). However, a bottleneck of the approach is that textualizing nonverbal modalities requires the choice of textual descriptions. For instance, the same audio segment can be textualized into the high/low valence/arousal spectrum, or the vocal intonation can be textualized. Similarly, for the facial modality, the choice of AUs and the granularity and context window are some of the design choices that can heavily affect the overall performance. The manual selection and design of textual descriptions require the intervention of domain experts to construct relevant nonverbal cues, making them application-dependent. This is similar to the contrast between using learned versus handcrafted features and the challenges of the latter. In this paper, we compare the performance of state-of-theart deep learning models that follow standard feature-based vs. text-based modeling approaches. An extensive set of experiments was conducted on the challenging C-EXPR-DB video dataset for CER in the context of the 7th Workshop and Competition on Affective Behavior Analysis in-the-Wild (ABAW)  [28, 33, 30, 29, 25, 24, 32, 31, 27] . To further assess the benefits of using a textualized approach in basic ER, experiments were also conducted on the MELD video dataset.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Feature-Based Modeling",
      "text": "This approach extracts features from audio (vocal) and video (facial) modalities and text transcripts for multimodal CER in videos. Feature embedding is combined for feature-level classification (see Fig.  2 ). The rest of this section provides more details on this approach.\n\nFeature Representation. In ER applications, the de facto strategy to leverage different modalities extracted from videos is to extract their features  [1, 34, 54, 62, 75] . These modalities typically include vision and audio. Textual modality, such as audio transcripts, is also included when available. Other ER applications such as pain estimation leverage biosignals such as physiological modalities  [62, 64] : electrodermal activity (EDA), electromyograph (EMG), and electrocar-diogram (ECG). The general motivation behind combining these modalities is to leverage their complementary information over a video sequence.\n\nEach modality typically employs a dedicated pre-trained feature extractor, which can be pre-trained on different largescale datasets. In addition, public weights pre-trained on related datasets can be employed. For visual modality, ResNet  [18]  backbone is commonly used, which is followed by a module that leverages temporal information such as temporal convolutional network (TCN)  [2] . 3D models such as R3D-CNN  [57]  can better leverage spatio-temporal dependency between frames directly at feature extraction. For audio modality, a variety of public pre-trained feature extractors are available, such as VGGish  [19] , Wav2Vec 2.0  [61] , and HuBERT  [21] . In addition, traditional audio features can be easily computed, such as spectrograms and MFCCs  [65] . Multiple text feature extractors are available for the text modality, such as BERT  [10]  and RoBERTa  [43] . Feature extractors are typically kept frozen while the subsequent modules are fine-tuned to avoid expensive computational costs.\n\nFeature-Level Fusion. A bottleneck in feature-based models is fusion (Fig.  1 ). Different methods rely on temporal models to combine features from over a video, like LSTMs  [7, 54, 48] , or rely on simple concatenation  [34, 73] . Recent works focus more on self-and cross-modal attention and transformers  [59]  Figure  3:  A common text-based approach used for multimodal CER, where non-verbal modalities are textualized. to perform attention-based fusion  [35, 44, 51, 58, 62, 69, 74] . This has shown promising results as they can capture interand intra-modality relationships. Note that aligning modalities in a video setup is challenging as well. For example, what part of audio or text could be assigned to a single frame remains unclear. Usually, an empirical sliding window is employed to align other modalities with a frame. This paper follows the recent work  [69]  to experiment with a feature-based approach for the ER task (Fig.  2 ). Their method achieved good results for continuous ER (valence/arousal) in videos in a recent ABAW challenge. In our case, it was adapted to perform emotion classification in videos as well. In particular, we use ResNet50  [18] , pre-trained on MS-CELEB1M  [16]  and FER + [3] datasets for visual modality. For audio modality, we employ VGGish  [19] , while BERT  [10]  is used over text modality. Temporal information is further exploited by using temporal convolutional network (TCN)  [2]  after each feature extractor. We employ the co-attention block  [69]  to attend to features from different modalities. This builds a single embedding per frame while leveraging a contextual window. The per-frame feature is then fed to a classifier head to predict emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Text-Based Modeling",
      "text": "This approach extracts textual descriptions from audio (vocal) and visual (facial) modalities for multimodal CER in videos, and combines them with text transcripts. This joint textual description is processed by an LLM for classification (see Fig.  3 ). The rest of this section provides more details on this approach.\n\nAudio and Visual Text Description. The API of Hume Inc.  1 is employed over a sliding window to analyze the tone of the audio. Their model is trained on millions of human interactions  2  . The API scores each tone characteristic, allowing us to sort and select the top 10. Examples of tone characteristics include confusion, anxiety, disappointment, distress, and even basic emotions. The name of each tone characteristic and a \"Low\" or \"High\" prefix, determined using a threshold on the score, are used to describe the tone textually. We also use a fine-tuned Wav2Vec 2.0 model  [61]   3  to predict scores for arousal, valence and dominance for each audio clip. Those scores are then categorized as \"Low\" or \"High\" using a threshold. The textual description produced from audio concatenates all tone and arousal-valence-dominance textual descriptions.\n\nFace cropping and alignment are first performed at each frame using RetinaFace  [8] . Py-feat library  [6]  is then used to extract action units (AUs) intensity  [12, 14]  along with basic emotion probabilities. AUs codebook  [4]  maps each facial expression to a set of action units. For instance, the expression \"Happy\" is associated with \"AU6\" (Cheek raise), \"AU12\" (Lip corner puller), and \"AU25\" (lips part). Typically, a set of these facial AUs activate at once. Over a sequence of frames, we select the maximum intensity of each action unit over time. Then, we use a threshold to determine which AUs are activated. In this case, the text description will be the concatenation of the name of each selected action unit as depicted in Fig.  3 . The same procedure is repeated with basic emotions, where the top 3 emotions are selected, and their textual name is used as a description.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Combination Of Transcripts With Audio And Visual",
      "text": "Texts. Some multimodal datasets like MELD  [49]  and CMU-MOSEI  [68]  provide text transcriptions. We use Whisper-large-v2  [52]  to generate transcripts when they are unavailable. Once all textual descriptions are acquired, we combine them into a single prompt and feed it to an LLM such as LLaMA-3  [47] , in our case. Our used prompts follow this template: A fully connected layer (FC) follows LLaMA-3 to perform classification. Both the LLM and the FC layer are fine-tuned using ground-truth class labels.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Discussion",
      "text": "This paper compares feature-and text-based approaches for multimodal CER in videos in the wild. For a fair comparison, our experimental setup is constrained to be as similar as possible for the two approaches. This section provides the experimental methodology, results, and discussion. We also include results on basic emotions in videos. The full C-EXPR-DB  [26]  contains 400 videos with 200,000 frames in total. However, we do not have access to the 400, but only to the 56 test videos of the challenge. Each frame has an annotation with twelve compound emotions. For the 7th ABAW Challenge, only seven compound emotions are considered from the original twelve, which are: Fearfully Surprised, Happily Surprised, Sadly Surprised, Disgustedly Surprised, Angrily Surprised, Sadly Fearful, and Sadly Angry. In this work, these 56 videos used for the test are referred to as C-EXPR-DB. The challenge organizers provide it without annotation. To perform experiments earlier than the challenge deadline, we annotated these 56 videos by our internal expert team. Each video may have different parts where there are compound emotions. The annotation is done at frame level with the same seven compound emotions of the challenge in addition to the class \"Other\" that represents any other emotion, compound or basic, that is different from the considered seven ones. Once annotated, we cut each original video into segments using the annotation timeline. Each segment contains only one compound emotion. We obtained 125 segments. We refer to this dataset as C-EXPR-DB * . Its class distribution is presented in Table .1. We split C-EXPR-DB * into 5-cross validation. Performance is reported on the validation set of each fold. The evaluation is done only on compound emotions while discarding the class \"Other\". However, the training may or may not include this extra class. More details about our annotation are provided later. b) MELD (basic emotions)  [49] : This multi-party dataset was created from video clipping of the TV show \"Friends\" utterances. The train, validation, and test sets consist of 9988, 1108, and 2610 utterances, respectively. Each utterance has one global label from seven basic emotions: anger, sadness, joy, neutral, fear, surprise, or disgust. In addition, a transcript of each utterance is provided. This dataset is unbalanced, where neutral is the most dominant label with 4710 utterances, while disgust is the least frequent label with 271 utterances.\n\n(2) Annotation of Compound Emotions in C-EXPR-DB * . The 56 test videos of C-EXPR-DB for the 7th ABAW CER Challenge were annotated by two expert annotators, both with a psychology background and one with extensive experience with emotion recognition. Each video was annotated by both annotators and subsequently triangulated to create one unified annotation file. The triangulation included a discussion to create agreement on the compound emotions found in each video, the segment of the video where it could be found (time stamps), and the reasoning behind the choice of compound emotion identified.\n\nAnnotators followed a codebook (Table .2), created specifically for the challenge, where the seven compound emotions (Fearfully Surprised, Happily Surprised, Sadly Surprised, Disgustedly Surprised, Angrily Surprised, Sadly Fearful, Sadly Angry) were properly described. In addition, the individual emotions (Happiness, Sadness, Anger, Fear, Disgust, and Surprise) were described and broken down in specific cues, including behavioral responses and facial, language, audio, and body language markers for each of the basic emotions. This allowed for the use of multi-modalities to properly identify the presence of the emotions. Compound emotions were annotated when both emotions occurred simultaneously or closely followed each other. Additionally, other emotions not related to the compound emotions were tagged as \"Other\". Videos were annotated using the software ELAN 6.8  4   Experiencing a sudden and unexpected event that brings sadness or disappointment. This type of surprise is unpleasant and upsetting. Must include at least one cue related to sadness and at least one cue related to surprise. They should occur simultaneously or closely following each other. Disgustedly Surprised Experiencing a sudden shock or surprise that also causes feelings of disgust or revulsion. This might happen if something unexpected occurs that is also repulsive. Must include at least one cue related to disgust and at least one cue related to surprise. They should occur simultaneously or closely following each other.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Angrily Surprised",
      "text": "Experiencing a sudden shock or surprise that provokes anger. This might happen if something unexpected occurs that is also infuriating. Must include at least one cue related to anger and at least one cue related to surprise. They should occur simultaneously or closely following each other.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sadly Fearful",
      "text": "Feeling both sadness and fear simultaneously. This can occur when facing a situation that is both threatening and sorrowful. Must include at least one cue related to sadness and at least one cue related to fear. They should occur simultaneously or closely following each other.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sadly Angry",
      "text": "Feeling both sadness and anger simultaneously. This can happen when dealing with a situation that evokes both sorrow and frustration or rage. Must include at least one cue related to sadness and at least one cue related to anger. They should occur simultaneously or closely following each other.\n\nTable  2 : Codebook used by our experts to annotate compound C-EXPR-DB * emotions.\n\nboth annotators had previous experience with this annotation tool.\n\n(3) Baseline Models. For a fair comparison, recent models are considered. For the feature-based approach, we follow the work in  [69]  where we used ResNet50  [18]  for feature extraction over visual modality. It is pre-trained on MS-CELEB1M  [16]  dataset as a facial recognition task. Then, it is fine-tuned on the FER + [3] dataset. VGGish  [19]  is used for audio modality. Over text modality, the BERT Base Uncased model is used  [10] . To leverage temporal dependency from videos, we used temporal convolutional network (TCN)  [2] , which has shown to yield good results in previous ABAW challenges  [50, 69] . For the fusion module, we used the coattention method (LFAN) proposed in  [69] , followed by a classification head. All three feature extractors are frozen, and every subsequent module is fine-tuned. For the text-based approach, once the text of all modalities is acquired, it is fed to a language model, which is followed by a dense layer to perform classification. In our experiments, we used LLaMA-3 8B  [47] , a recent open LLM that can fit in an average GPU.\n\nBoth modules are fine-tuned using emotion labels as supervision.\n\n(4) Training Protocol. The following learning strategies are used: a) 7th ABAW CER Challenge: We train our model on MELD over the seven basic emotions. We then evaluate the final model over the 56 unlabeled videos of the test set C-EXPR-DB, following the challenge protocol. Since the model is trained over basic emotions, for each pair of the seven compound emotions, we sum their corresponding probabilities and pick the pair with the highest score to predict the compound emotion. We submitted different cases of feature and text approaches based on the training size of MELD: with 1% and 100% of total training samples. We alsp explored zero-shot predictions of a multimodal large language model (MLLM) using the LLaVA-NeXT-Video  [71] , which uses visual modality. b) Feature vs. text comparison: Over C-EXPR-DB * and MELD, we perform supervised learning on each dataset separately. For C-EXPR-DB * , we report the performance on the validation set of each of the 5-cross validation, while we report test performance on MELD.\n\nThe next presents our experimental details of each approach: 1) Feature-based approach: The pre-processing of videos is carried out as follows. For the visual modality, RetinaFace  [8]  is used to crop and align faces from each frame, which are then resized to 48 × 48. A sliding window over frames is used for training. The size is estimated using validation among 16 * n where n spans from 2 to 18. Windows overlaps with the size of 16. A window of frames is fed to a pre-trained ResNet50  [18] . The audio of a video is initially converted to WAV format with a sampling rate of 16k. To synchronize with frames, we set the hop length to be 1/frame rate.\n\nFor audio feature extraction, we used VGGish  [19] . For text modality, we used the transcripts provided by MELD. However, for C-EXPR-DB and C-EXPR-DB * , we used Whisper-large-v2  [52]  to generate each video transcript. BERT  [10]  is then used to extract features aligned with frames. The three feature extractors are frozen. Only the subsequent modules are fine-tuned. The model  [69]  is fine-tuned on frame-level. When only a video-level emotion label is available, this same label is transferred to each frame in the video. Stochastic gradient descent (SGD) is used for optimization with a batch size estimated by validation from 2, 4, 8, 16, 18 and weight decay of 10 -4 . For MELD dataset with 1%, we trained for 1000 epochs, while 300 epochs are used for the case of full data (100%) due to long computation time (37 hours on an NVIDIA A100 GPU).\n\nFor C-EXPR-DB * , we trained for 100 epochs. Standard cross-entropy loss is used for training.\n\n2) Text-based approach: The pre-processing of each modality is performed as follows. For the visual modality, the Py-Feat 5  library is used to extract the intensity of 20 Face Action Units for each frame of a video. A maximum is then applied over a sliding window of frames to summarize the action unit scores for each frame. This sliding window is used as the context for each frame. The audio parts of the video are used as input to the prosody model using the API of hume.ai, and the top-10 tone characteristics are then used. We also used a fine-tuned Wav2Vec 2.0 model  [61]   6  to predict scores for arousal, valence, and dominance for each audio clip. We use QLoRA  [9]  in order to efficiently fine-tune LLaMA-3 8B  [47] . The training is done at the frame level, and in the case where only video-level emotion is available, this same label is transferred to each frame in the video.\n\nTo reduce the computation time on the MELD dataset and avoid too many prompt duplicates, we only use a few frames with their context from each video during the training while the test is conducted over all frames. SGD is also used for optimization with a batch size estimated by validation from {8, 10, 14} and weight decay of 10 -3 . For MELD dataset with 1%, we use a learning rate estimated by validation from {2 × 10 -3 , 5 × 10 -3 , 7 × 10 -3 } while learning rates from {7 × 10 -3 , 10 -2 , 3 × 10 -2 } are used for the case of full data.\n\nFor the C-EXPR-DB * dataset, we set a window hop size to decrease the number of redundant prompts used in training.\n\nWe estimate the batch size from {6, 8} and the learning rate from {2 × 10 -3 , 5 × 10 -3 , 7 × 10 -3 }. The window size is also selected from {10, 15, 20, 30}, and we use a hop size of 10. Standard cross-entropy loss is used for training. This limitation affects the MLLM's performance, as it was designed to provide a rich description for understanding video content rather than making decisions on a single category. As a result, for some short videos, the MLLM returned non-valid emotions, such as descriptions of scenes or other text. We post-processed the outputs in these cases, replacing them with the \"neutral\" category.\n\n(5) Performance Measures. To assess model performance, we use the average F 1 score required in the 7th ABAW Challenge. It is defined as follows,\n\n(1) where c represents the class ID, T P represents True Positives, F P represents False Positives, and F N represents False Negatives. Average F 1 is computed with w c = 1/7 while weighted F 1 is computed with w c set to be the proportion of each class.\n\nFollowing the literature, on the MELD dataset, we use the weighted F 1 score, which accounts for unbalanced classes, similarly to C-EXPR-DB * . We note that evaluation over C-EXPR-DB * is done over frame level similarly to C-EXPR-DB.\n\nWith the MELD dataset, only global video-level labels are available where a class is assigned to the entire video. Evaluation must also be performed at the video level, not the frame level. For video-level prediction, we post-process the frame-level predictions using three different strategies:\n\n1) Majority voting: Majority voting is performed over the predicted classes of all the frames. The winning class becomes the prediction for the video.\n\n2) Average logits: For each class, the average its logits are assesses across frames. This yields an average logit vector. The video class is the class with the maximum logit.\n\n3) Average probabilities: This is similar to average logits, but it is perfromed over probabilities instead.\n\n(6) Frame-based Ensembling. For the ABAW CER challenge, we perform a submission with ensembling over prediction labels of different models at the frame level. Given a set of models, we first perform single model prediction at frame level. Then, we perform label prediction aggregation at each frame to have a final frame label. To do so, at a time t, we consider a window of length 10 that covers the frame t and its previous frames. Then, we perform majority voting over the prediction label across all frames and models within that window. The label result of the vote is assigned as the final label prediction for the frame t. Ablation studies were conducted on text-and feature-based approaches over single and multimodal cases (see Table  3 ). Both datasets were considered -MELD with relatively controlled setup, and C-EXPR-DB * , an extreme case of in-the-wild. The first observation is the contribution of a single modality, which depends on the dataset. On MELD, where there are rich transcripts, both approaches achieved their highest F 1 score on the text modality with 59.78% for feature-based and 62.55% for text-based. However, other modalities contributed less. We note that visual textualization achieved a poor F 1 score of 25.18% compared to audio with 43.97%. This suggests that either there is less information in the visual modality or the textual cues used are less efficient in capturing the emotion. This pattern is consistent with the feature-based approach but with less decline in performance between the two modalities.\n\nOn C-EXPR-DB * , the text transcript is very limited. In most videos, people shout, scream, talk very briefly, or do not talk and only express compound emotions visually. Such characteristics seem to be tied to compound emotions, making their prediction difficult. Interestingly, each approach leverages a different modality to deal with compound emotion prediction.\n\nIn the feature-based method, visual modality seems to be the strongest, yielding an F 1 score of 53.46%. However, in the text-based method, audio seems to be the strongest modality with an F 1 score of 50.96%, while text modality ranks second with 46.11%, and visual modality yields a poor score of 19.82%. This very low score of visual modality may indicate that its textualization yields inadequate information since this result contradicts the feature-based approach which scores the highest with 53.46% suggesting that visual modality holds rich information. The choice of converting a modality into a set of textual descriptions can lead to drastic and irreversible loss of information. Handcrafting these textual cues requires expertise, which may lead to poor performance. Modality textualization is very difficult to apply in real-world applications. An additional observation that can be drawn from these results is that finetuned LLMs, as in our case where we used a fully connected layer for classification on top of it, are pow-erful over single text modality when the text is rich. This can be observed over MELD dataset where the text-based method yields a score of 62.55% over transcript only. However, the performance drops when using a poorer transcript over the compound emotion dataset (C-EXPR-DB * ) with a score of 46.11%.\n\nAnother observation from Table  3  is the impact of multiple modalities on performance. On MELD, using multimodal seems to improve performance compared to a single modality but leads to a considerable performance drop on compound emotion for both approaches. This may be explained by the conflicting emotions seen simultaneously through each modality. For example, the transcript \"OK\" can carry out the emotion 'Neutral'. However, the way it is said, and the facial appearance of saying it can change the emotion to suggest the dual emotions of 'Surprise' and 'Happy'. In other cases, multi-conflicting instances of the same modality can be an issue, like in audio, where it is difficult to separate the sound source. For example, the case where a person is commenting on an event where we are interested in the emotion of the commentator. The overlay of the event and a person's audio signals could easily reflect conflicting emotions. Videos in the wild are extremely challenging, and there are many different cases to consider. Focusing on the right instance in a modality remains a challenge  [40] , but it may be easier with the visual modality than with audio and text.\n\nA comparison between text-and feature-based approaches for basic (MELD) and compound (C-EXPR-DB * ) ER is presented in Tables  4  and 5 , respectively. On MELD, the text-based approach yields the best performance with ≈ 4% above the feature-based approach. As discussed earlier, this is mainly due to the high quality of transcripts and the LLM. However, on C-EXPR-DB * , the feature-based approach is ahead of the text-based approach. Given the poor transcript quality, textualization does not bridge the gap with the feature-based approach. Based on our experiments, one should attempt to use textualization over feature-based methods only when rich transcripts are available.\n\nExperiments on C-EXPR-DB * with ensembling at the frame level (see Table  6 ) indicate that combining the prediction of relatively good models yields better performance. Re-  garding the performance of Zero-shot MLLM LLaVa-NeXT-Video  [71] , we obtained low performance over both datasets.\n\nAcknowledging that we have constrained its output to be a single label is important. This may have limited its performance since a single label may not be enough to express an emotion. Despite this, the model produced decent performance over MELD with an F 1 score of 36.25%. However, limited performance is reported over compound emotion dataset C-EXPR-DB * . We observed that the model predicts the class 'Fearfully Surprised' on this specific dataset almost at every frame. Predicting two emotions simultaneously seems more challenging than a single emotion for this model.\n\nWe note that our obtained results over MELD are competitive with state-of-the-art performance as presented in Table  7 . The text-based method over the case without contextual information achieved a new state-of-the-art F 1 score of 65.50%.\n\nWe conducted additional experiments over C-EXPR-DB * where we investigated the impact of weights initialization by comparing random vs MELD pretrained weights over both textand feature-based method. Results are reported in Table  8 . These results convey a mixed message where in some cases pretrained weights can help and sometimes, it is better to start from random initialization depending on the fold. However, we note that feature-based method is more robust to initialization as we observe only slight performance shift (around 2%) between random and pretrained case. However, for the case of text-based method, in some folds the performance shift is large an can go up to 8% and 14%.     on C-EXPR-DB. Over these four submissions, the featurebased method leads with an F 1 score of 22.64%, followed by the text-based method with a score of 19.86%. Our final submission, which performs frame-level prediction fusion across the four submissions, achieved the highest score of 25.90%, leading to rank three in this challenge.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Computation Time",
      "text": "We present in This approach also requires a lot of preprocessing by large models to extract additional textual information such as action units, emotions, and audio cues from visual and audio modalities. Note that all measurements reported here depend on the sequence length and account only for the forward computations while excluding all preprocessing steps such as face detection.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Challenges Of Textualizing Modalities",
      "text": "Using deep feature-based models for multimodal ER is the most common approach. It is easy to use and it requires less effort and expertise from the user to implement. Adequate features can be automatically extracted and combined from different modalities by a feature extractor without the need for manual intervention. Since we provide full data to the model, we can assume that there is no loss of information at the model input. In addition, it is easily transferable to other tasks without much change. The publicly available pretrained feature extractors make it more attractive. However, a well-known bottleneck of this approach is the multimodal and spatio-temporal fusion of diverse modalities  [39] .\n\nModality textualization is a very recent topic  [17] . While it can leverage the very recent progress of LLMs, it still faces several limitations to be a practical and competitive approach to the feature-based method. We can mention two main limitations. The first one is the need for domain experts to select the cues to be extracted from each modality and how they should be textualized. For example, cues other than AUs could be extracted in the visual modality, and there could also be dif-ferent ways to convert them into text. Handcrafting of cues is challenging as they are not guaranteed to provide optimum textualization. Moreover, each modality requires its specific textualization, which depends on the task. Changing the application or task requires new domain experts and new textual cues that are most suitable. Models are less transferable to other applications, which limits their usage.\n\nAnother issue is the loss of information. The process of textualization performs a discretization of a modality from row data to textual descriptions. This mapping will most likely lead to information loss and poor performance. This has been observed on C-EXPR-DB * where the feature-based approach outperforms on visual, whereas its counterpart yields the lowest performance, indicating a potential loss of information during our choice for textualization.\n\nA main benefit of using a text-based approach is to leverage the potential of LLMs when dealing with a dataset with rich transcripts. Combining rich transcripts with textualized modalities may lead to higher ER accuracy than a featurebased method, as observed over MELD. However, conflicting modalities could lead to poor performance over compound emotions. Feature-based methods leverage fusion, especially late fusion, which may balance this conflict. However, textbased approaches lack such explicit modules as all modalities are treated indistinguishably as they are all processed by the same module. Although the authors of  [17]  motivated textualization as an easy way to perform multimodal fusion, such said fusion could be its limitation. Performing very early fusion by manually converting all modalities into text to be processed by the same module, such as an LLM, could hinder the benefit of a modality and make it less efficient, as most of the information could already be lost. Most of the work needs to be done by the LLM to recover the missing information. However, in the feature-based approach, the specialized feature extractor per modality does a lot of work leading to reliable features. This eases the late fusion. We note that text-based approach could be computationally expensive with 597.52 TFLOPs for inference over a sequence of 224 frames while feature-based method yields 1.87 TFLOPs.\n\nThe choice of the best approach for CER or basic ER remains an open question. Similarly, the choice and design of textual cues of different modalities are still in their early stages. The feature-based models are easier to apply, yet text-based models can yield better results when dealing with rich text.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "Multimodal ER is a challenging task, especially when recognizing the complex compound emotions that are captured in real-world unconstrained videos. The central question in this study is: how textualized modeling performs com-pared to feature-based modeling for CER in videos?. We performed several experiments to compare the performance of deep feature-based and text-based models on CER and basic ER datasets. Our results have uncovered several challenges related to modality textualization on C-EXPR-DB, where text transcripts are captured in the wild. Feature-based methods may still provide better accuracy in this case. However, a textbased approach may yield better results when videos have high-quality transcripts.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Models for compound multimodal ER in videos. (a) In the feature-based approach, a dedicated feature extractor",
      "page": 2
    },
    {
      "caption": "Figure 1: (a), feature-based",
      "page": 2
    },
    {
      "caption": "Figure 1: (b)). However, a bottleneck of the approach is",
      "page": 2
    },
    {
      "caption": "Figure 2: A common feature-based approach used for multimodal CER.",
      "page": 3
    },
    {
      "caption": "Figure 2: ). The rest of this section provides",
      "page": 3
    },
    {
      "caption": "Figure 1: ). Different methods rely on temporal models",
      "page": 3
    },
    {
      "caption": "Figure 3: A common text-based approach used for multimodal CER, where non-verbal modalities are textualized.",
      "page": 4
    },
    {
      "caption": "Figure 2: ). Their method",
      "page": 4
    },
    {
      "caption": "Figure 3: ). The rest of this section provides more details on this",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Angrily Surprised\nSadly Angry\nFearfully Surprised\nHappily Surprised\nSadly Fearful\nDisgustedly Surprised\nSadly Surprised\nOther∗": "Total",
          "8\n16\n24\n15\n19\n10\n13\n20": "125",
          "53.92\n100.60\n98.85\n150.57\n107.56\n82.45\n182.17\n71.00": "847.15"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fearfully Surprised": "Happily Surprised",
          "Experiencing a sudden shock or surprise accompanied by fear. This might happen if something\nunexpected occurs that also appears threatening or dangerous. Must include at least one cue related\nto fear and a cue related to surprise, either occurring simultaneously or closely following each other.": "Experiencing a sudden and unexpected event that brings joy or happiness. This type of surprise\nis pleasant and delightful. Must include at least one cue related to happiness and at least one cue\nrelated to surprise. They should occur simultaneously or closely following each other."
        },
        {
          "Fearfully Surprised": "Sadly Surprised",
          "Experiencing a sudden shock or surprise accompanied by fear. This might happen if something\nunexpected occurs that also appears threatening or dangerous. Must include at least one cue related\nto fear and a cue related to surprise, either occurring simultaneously or closely following each other.": "Experiencing a sudden and unexpected event that brings sadness or disappointment. This type of\nsurprise is unpleasant and upsetting. Must include at least one cue related to sadness and at least\none cue related to surprise. They should occur simultaneously or closely following each other."
        },
        {
          "Fearfully Surprised": "Disgustedly Surprised",
          "Experiencing a sudden shock or surprise accompanied by fear. This might happen if something\nunexpected occurs that also appears threatening or dangerous. Must include at least one cue related\nto fear and a cue related to surprise, either occurring simultaneously or closely following each other.": "Experiencing a sudden shock or surprise that also causes feelings of disgust or revulsion. This\nmight happen if something unexpected occurs that is also repulsive. Must include at least one cue\nrelated to disgust and at\nleast one cue related to surprise. They should occur simultaneously or\nclosely following each other."
        },
        {
          "Fearfully Surprised": "Angrily Surprised",
          "Experiencing a sudden shock or surprise accompanied by fear. This might happen if something\nunexpected occurs that also appears threatening or dangerous. Must include at least one cue related\nto fear and a cue related to surprise, either occurring simultaneously or closely following each other.": "Experiencing a sudden shock or surprise that provokes anger. This might happen if something\nunexpected occurs that is also infuriating. Must include at least one cue related to anger and at least\none cue related to surprise. They should occur simultaneously or closely following each other."
        },
        {
          "Fearfully Surprised": "Sadly Fearful",
          "Experiencing a sudden shock or surprise accompanied by fear. This might happen if something\nunexpected occurs that also appears threatening or dangerous. Must include at least one cue related\nto fear and a cue related to surprise, either occurring simultaneously or closely following each other.": "Feeling both sadness and fear simultaneously. This can occur when facing a situation that is both\nthreatening and sorrowful. Must include at least one cue related to sadness and at least one cue\nrelated to fear. They should occur simultaneously or closely following each other."
        },
        {
          "Fearfully Surprised": "Sadly Angry",
          "Experiencing a sudden shock or surprise accompanied by fear. This might happen if something\nunexpected occurs that also appears threatening or dangerous. Must include at least one cue related\nto fear and a cue related to surprise, either occurring simultaneously or closely following each other.": "Feeling both sadness and anger simultaneously. This can happen when dealing with a situation that\nevokes both sorrow and frustration or rage. Must include at least one cue related to sadness and at\nleast one cue related to anger. They should occur simultaneously or closely following each other."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 8: Impact of pre-training: Frame-level weighted F1",
      "data": [
        {
          "Folds / Case": "",
          "Zero-shot LMM\nTraining w/ \"Other\"\nTraining w/o \"Other\"\nLLaVa-NeXT-Video [71] (Visual)": "Text-based\nFeature-based\nText-based\nFeature-based"
        },
        {
          "Folds / Case": "Majority voting",
          "Zero-shot LMM\nTraining w/ \"Other\"\nTraining w/o \"Other\"\nLLaVa-NeXT-Video [71] (Visual)": ""
        },
        {
          "Folds / Case": "Fold 0\nFold 1\nFold 2\nFold 3\nFold 4",
          "Zero-shot LMM\nTraining w/ \"Other\"\nTraining w/o \"Other\"\nLLaVa-NeXT-Video [71] (Visual)": "17.96\n32.83\n32.91\n42.74\n44.46\n46.59\n30.08\n49.47\n18.61\n45.45\n31.87\n45.67\n35.10\n30.15\n34.06\n40.73\n28.15\n54.32\n26.05\n53.96"
        },
        {
          "Folds / Case": "Average logits",
          "Zero-shot LMM\nTraining w/ \"Other\"\nTraining w/o \"Other\"\nLLaVa-NeXT-Video [71] (Visual)": ""
        },
        {
          "Folds / Case": "Fold 0\nFold 1\nFold 2\nFold 3\nFold 4",
          "Zero-shot LMM\nTraining w/ \"Other\"\nTraining w/o \"Other\"\nLLaVa-NeXT-Video [71] (Visual)": "20.76\n36.56\n32.91\n46.52\n12.02\n42.12\n49.01\n26.92\n43.98\n34.20\n18.61\n45.45\n31.87\n44.66\n25.06\n35.10\n29.90\n39.21\n37.14\n29.23\n28.15\n54.32\n16.50\n53.96\n26.45"
        },
        {
          "Folds / Case": "Average probabilities",
          "Zero-shot LMM\nTraining w/ \"Other\"\nTraining w/o \"Other\"\nLLaVa-NeXT-Video [71] (Visual)": ""
        },
        {
          "Folds / Case": "Fold 0\nFold 1\nFold 2\nFold 3\nFold 4",
          "Zero-shot LMM\nTraining w/ \"Other\"\nTraining w/o \"Other\"\nLLaVa-NeXT-Video [71] (Visual)": "17.96\n36.56\n32.91\n42.74\n44.46\n40.98\n26.92\n40.38\n18.61\n36.55\n31.87\n34.80\n35.10\n29.90\n34.06\n40.73\n28.15\n54.32\n22.35\n53.96"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 8: Impact of pre-training: Frame-level weighted F1",
      "data": [
        {
          "Netease Fuxi AI Lab, Liu et al [41]\nHSEmotion, Savchenko [53]\nHFUT-MAC2, Liu et al [42]\nAIPL-BME-SEU, Li et al [38]": "ETS-LIVIA (Ours methods)\n· Text-based (100% of MELD)\n· Feature-based (100% of MELD)\n· Feature-based (1% of MELD)\n· Zero-shot MLLM LLaVa-NeXT-Video [71]\n· Frame-based ensembling of our 4 submissions",
          "60.63\n32.43\n22.81\n16.44": "19.86\n22.64\n16.20\n17.67\n25.90"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 8: Impact of pre-training: Frame-level weighted F1",
      "data": [
        {
          "Case/Approach": "Train time 1 epoch",
          "Zero-shot MLLM\nText-based\nFeature-based\nLLaVa-NeXT-Video [71]": ""
        },
        {
          "Case/Approach": "MELD\nC-EXPR-DB∗",
          "Zero-shot MLLM\nText-based\nFeature-based\nLLaVa-NeXT-Video [71]": "9.3min\n6.5min\n–\n2.1min\n23sec\n–"
        },
        {
          "Case/Approach": "Inference time per-frame",
          "Zero-shot MLLM\nText-based\nFeature-based\nLLaVa-NeXT-Video [71]": "35ms\n0.12ms\n715ms"
        },
        {
          "Case/Approach": "Total n. params.",
          "Zero-shot MLLM\nText-based\nFeature-based\nLLaVa-NeXT-Video [71]": "7.51B\n223.91M\n7.06B"
        },
        {
          "Case/Approach": "N. learnable params.",
          "Zero-shot MLLM\nText-based\nFeature-based\nLLaVa-NeXT-Video [71]": "3.44M\n5.00M\n–"
        },
        {
          "Case/Approach": "N. FLOPs (TFLOPs)",
          "Zero-shot MLLM\nText-based\nFeature-based\nLLaVa-NeXT-Video [71]": "597.52\n1.87\n–"
        },
        {
          "Case/Approach": "N. MACs",
          "Zero-shot MLLM\nText-based\nFeature-based\nLLaVa-NeXT-Video [71]": "298.75TMACs\n938.76 GMACs\n–"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Distilling privileged multimodal information for expression recognition using optimal transport",
      "authors": [
        "M Aslam",
        "M Zeeshan",
        "S Belharbi",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2024",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "2",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling"
    },
    {
      "citation_id": "3",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Canton-Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "ICLM"
    },
    {
      "citation_id": "4",
      "title": "Guided interpretable facial expression recognition via spatial action unit cues",
      "authors": [
        "S Belharbi",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2024",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "5",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Z Cheng",
        "Z Cheng",
        "J He",
        "J Sun",
        "K Wang",
        "Y Lin",
        "Z Lian",
        "X Peng",
        "A Hauptmann"
      ],
      "year": "2024",
      "venue": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning"
    },
    {
      "citation_id": "6",
      "title": "Py-feat: Python facial expression analysis toolbox",
      "authors": [
        "J Cheong",
        "E Jolly",
        "T Xie",
        "S Byrne",
        "M Kenney",
        "L Chang"
      ],
      "year": "2023",
      "venue": "Affective Science"
    },
    {
      "citation_id": "7",
      "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
      "authors": [
        "D Deng",
        "L Wu",
        "B Shi"
      ],
      "year": "2021",
      "venue": "Iterative distillation for better uncertainty estimates in multitask emotion recognition"
    },
    {
      "citation_id": "8",
      "title": "Retinaface: Single-stage dense face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "Y Zhou",
        "J Yu",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Retinaface: Single-stage dense face localisation in the wild"
    },
    {
      "citation_id": "9",
      "title": "Qlora: Efficient finetuning of quantized llms",
      "authors": [
        "T Dettmers",
        "A Pagnoni",
        "A Holtzman",
        "L Zettlemoyer"
      ],
      "year": "2023",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "10",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "11",
      "title": "Hcam -hierarchical cross attention model for multi-modal emotion recognition",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2024",
      "venue": "Hcam -hierarchical cross attention model for multi-modal emotion recognition"
    },
    {
      "citation_id": "12",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "14",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "E Friesen",
        "P Ekman"
      ],
      "year": "1978",
      "venue": "Facial action coding system: a technique for the measurement of facial movement"
    },
    {
      "citation_id": "15",
      "title": "Ckerc : Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "authors": [
        "Y Fu"
      ],
      "year": "2024",
      "venue": "Ckerc : Joint large language models with commonsense knowledge for emotion recognition in conversation"
    },
    {
      "citation_id": "16",
      "title": "Msceleb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "17",
      "title": "Textmi: Textualize multimodal information for integrating non-verbal cues in pre-trained language models",
      "authors": [
        "M Hasan",
        "M Islam",
        "S Lee",
        "W Rahman",
        "I Naim",
        "M Khan",
        "E Hoque"
      ],
      "year": "2023",
      "venue": "Textmi: Textualize multimodal information for integrating non-verbal cues in pre-trained language models"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "19",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold",
        "M Slaney",
        "R Weiss",
        "K Wilson"
      ],
      "year": "2017",
      "venue": "Cnn architectures for large-scale audio classification"
    },
    {
      "citation_id": "20",
      "title": "Multi-modal emotion recognition with self-guided modality calibration",
      "authors": [
        "M Hou",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2022",
      "venue": "Multi-modal emotion recognition with self-guided modality calibration"
    },
    {
      "citation_id": "21",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Fine tuning LLM for enterprise: Practical guidelines and recommendations",
      "authors": [
        "M Vm",
        "K Warrier",
        "H Gupta"
      ],
      "year": "2024",
      "venue": "Fine tuning LLM for enterprise: Practical guidelines and recommendations"
    },
    {
      "citation_id": "23",
      "title": "MAP: multimodal uncertaintyaware vision-language pre-training model",
      "authors": [
        "Y Ji",
        "J Wang",
        "Y Gong",
        "L Zhang",
        "Y Zhu",
        "H Wang",
        "J Zhang",
        "T Sakai",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "24",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "25",
      "title": "Abaw: Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "ECCV"
    },
    {
      "citation_id": "26",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "27",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "28",
      "title": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2024",
      "venue": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond"
    },
    {
      "citation_id": "29",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "30",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Cowen",
        "S Zafeiriou",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition"
    },
    {
      "citation_id": "31",
      "title": "Affect analysis in-thewild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-thewild: Valence-arousal, expressions, action units and a unified framework"
    },
    {
      "citation_id": "32",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "33",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou",
        "I Kotsia",
        "A Dhall",
        "S Ghosh",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition"
    },
    {
      "citation_id": "34",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "35",
      "title": "Multi-label multimodal emotion recognition with transformer-based fusion and emotion-level representation learning",
      "authors": [
        "H.-D Le",
        "G.-S Lee",
        "S.-H Kim",
        "S Kim",
        "H.-J Yang"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "36",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "S Lei",
        "G Dong",
        "X Wang",
        "K Wang",
        "S Wang"
      ],
      "year": "2024",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework"
    },
    {
      "citation_id": "37",
      "title": "Revisiting disentanglement and fusion on modality and context in conversational multimodal emotion recognition",
      "authors": [
        "B Li",
        "H Fei",
        "L Liao",
        "Y Zhao",
        "C Teng",
        "T.-S Chua",
        "D Ji",
        "F Li"
      ],
      "year": "2023",
      "venue": "International Conference on Multimedia"
    },
    {
      "citation_id": "38",
      "title": "Temporal label hierachical network for compound emotion recognition",
      "authors": [
        "S Li",
        "H Lian",
        "C Lu",
        "Y Zhao",
        "T Qi",
        "H Yang",
        "Y Zong",
        "W Zheng"
      ],
      "year": "2024",
      "venue": "Temporal label hierachical network for compound emotion recognition"
    },
    {
      "citation_id": "39",
      "title": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "P Liang",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "40",
      "title": "A light weight model for active speaker detection",
      "authors": [
        "J Liao",
        "H Duan",
        "K Feng",
        "W Zhao",
        "Y Yang",
        "L Chen"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "41",
      "title": "Affective behaviour analysis via progressive learning",
      "authors": [
        "C Liu",
        "W Zhang",
        "F Li",
        "L Yu"
      ],
      "year": "2024",
      "venue": "Affective behaviour analysis via progressive learning"
    },
    {
      "citation_id": "42",
      "title": "Compound expression recognition via multi model ensemble for the abaw7 challenge",
      "authors": [
        "X Liu",
        "K Shen",
        "J Yao",
        "B Wang",
        "M Liu",
        "L An",
        "Z Cui",
        "W Feng",
        "X Sun"
      ],
      "year": "2024",
      "venue": "Compound expression recognition via multi model ensemble for the abaw7 challenge"
    },
    {
      "citation_id": "43",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "44",
      "title": "Transformer encoder with multiscale deep learning for pain classification using physiological signals",
      "authors": [
        "Z Lu",
        "B Ozek",
        "S Kamarthi"
      ],
      "year": "2023",
      "venue": "Frontiers in Physiology"
    },
    {
      "citation_id": "45",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "More evidence for the universality of a contempt expression",
      "authors": [
        "D Matsumoto"
      ],
      "year": "1992",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "47",
      "title": "Introducing meta llama 3: The most capable openly available llm to date",
      "year": "2024",
      "venue": "Introducing meta llama 3: The most capable openly available llm to date"
    },
    {
      "citation_id": "48",
      "title": "Deep autoencoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "D Nguyen",
        "D Nguyen",
        "R Zeng",
        "T Nguyen",
        "S Tran",
        "T Nguyen",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "50",
      "title": "Recursive joint cross-modal attention for multimodal fusion in dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "J Alam"
      ],
      "year": "2024",
      "venue": "CVPRw"
    },
    {
      "citation_id": "51",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "E Granger",
        "P Cardinal"
      ],
      "year": "2021",
      "venue": "International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "52",
      "title": "Robust speech recognition via largescale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "ICLM"
    },
    {
      "citation_id": "53",
      "title": "Hsemotion team at the 7th abaw challenge: Multi-task learning and compound facial expression recognition",
      "authors": [
        "A Savchenko"
      ],
      "year": "2024",
      "venue": "Hsemotion team at the 7th abaw challenge: Multi-task learning and compound facial expression recognition"
    },
    {
      "citation_id": "54",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "L Schoneveld",
        "A Othmani",
        "H Abdelkawy"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "55",
      "title": "Multimodal emotion recognition with transformer-based self supervised feature fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "56",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar",
        "A Rodriguez",
        "A Joulin",
        "E Grave",
        "G Lample"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models"
    },
    {
      "citation_id": "57",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "58",
      "title": "A pre-trained audio-visual transformer for emotion recognition",
      "authors": [
        "M Tran",
        "M Soleymani"
      ],
      "year": "2022",
      "venue": "A pre-trained audio-visual transformer for emotion recognition"
    },
    {
      "citation_id": "59",
      "title": "",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "60",
      "title": "Multimodal emotion recognition with deep learning: Advancements, challenges, and future directions",
      "authors": [
        "G Vijayaraghavan",
        "M Dhanasekaran"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "61",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "TPAMI"
    },
    {
      "citation_id": "62",
      "title": "Joint multimodal transformer for emotion recognition in the wild",
      "authors": [
        "P Waligora",
        "H Aslam",
        "O Zeeshan",
        "S Belharbi",
        "A Koerich",
        "M Pedersoli",
        "S Bacon",
        "E Granger"
      ],
      "year": "2024",
      "venue": "Joint multimodal transformer for emotion recognition in the wild"
    },
    {
      "citation_id": "63",
      "title": "Uncertainty-aware multi-modal learning via cross-modal random network prediction",
      "authors": [
        "H Wang",
        "J Zhang",
        "Y Chen",
        "C Ma",
        "J Avery",
        "L Hull",
        "G Carneiro"
      ],
      "year": "2022",
      "venue": "ECCV"
    },
    {
      "citation_id": "64",
      "title": "Automatic pain recognition from video and biomedical signals",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "R Niese",
        "S Walter",
        "S Gruss",
        "H Traue"
      ],
      "year": "2014",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "65",
      "title": "Hmm-based audio keyword generation",
      "authors": [
        "M Xu",
        "L.-Y Duan",
        "J Cai",
        "L.-T Chia",
        "C Xu",
        "Q Tian"
      ],
      "year": "2004",
      "venue": "Pacific-Rim Conference on Multimedia"
    },
    {
      "citation_id": "66",
      "title": "Emotion-anchored contrastive learning framework for emotion recognition in conversation",
      "authors": [
        "F Yu",
        "J Guo",
        "Z Wu",
        "X Dai"
      ],
      "year": "2024",
      "venue": "Emotion-anchored contrastive learning framework for emotion recognition in conversation"
    },
    {
      "citation_id": "67",
      "title": "Telme: Teacherleading multimodal fusion network for emotion recognition in conversation",
      "authors": [
        "T Yun",
        "H Lim",
        "J Lee",
        "M Song"
      ],
      "year": "2024",
      "venue": "Telme: Teacherleading multimodal fusion network for emotion recognition in conversation"
    },
    {
      "citation_id": "68",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph"
    },
    {
      "citation_id": "69",
      "title": "Continuous emotion recognition using visual-audio-linguistic information: A technical report for ABAW3",
      "authors": [
        "S Zhang",
        "R An",
        "Y Ding",
        "C Guan"
      ],
      "year": "2022",
      "venue": "CVPRw"
    },
    {
      "citation_id": "70",
      "title": "Affective behaviour analysis via integrating multi-modal knowledge",
      "authors": [
        "W Zhang",
        "F Qiu",
        "C Liu",
        "L Li",
        "H Du",
        "T Guo",
        "X Yu"
      ],
      "year": "2024",
      "venue": "CVPRw"
    },
    {
      "citation_id": "71",
      "title": "Llava-next: A strong zero-shot video understanding model",
      "authors": [
        "Y Zhang",
        "B Li",
        "H Liu",
        "Y Lee",
        "L Gui",
        "D Fu",
        "J Feng",
        "Z Liu",
        "C Li"
      ],
      "year": "2024",
      "venue": "Llava-next: A strong zero-shot video understanding model"
    },
    {
      "citation_id": "72",
      "title": "A survey of large language models",
      "authors": [
        "W Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong",
        "Y Du",
        "C Yang",
        "Y Chen",
        "Z Chen",
        "J Jiang",
        "R Ren",
        "Y Li",
        "X Tang",
        "Z Liu",
        "P Liu",
        "J Nie",
        "J Wen"
      ],
      "year": "2023",
      "venue": "A survey of large language models"
    },
    {
      "citation_id": "73",
      "title": "Multimodal-based stream integrated neural networks for pain assessment",
      "authors": [
        "R Zhi",
        "C Zhou",
        "J Yu",
        "T Li",
        "G Zamzmi"
      ],
      "year": "2021",
      "venue": "IEICE Transactions on Information and Systems"
    },
    {
      "citation_id": "74",
      "title": "Leveraging TCN and transformer for effective visual-audio fusion in continuous emotion recognition",
      "authors": [
        "W Zhou",
        "J Lu",
        "Z Xiong",
        "W Wang"
      ],
      "year": "2023",
      "venue": "CVPRw"
    },
    {
      "citation_id": "75",
      "title": "Self-supervised multimodal learning: A survey",
      "authors": [
        "Y Zong",
        "O Aodha",
        "T Hospedales"
      ],
      "year": "2023",
      "venue": "Self-supervised multimodal learning: A survey"
    },
    {
      "citation_id": "76",
      "title": "",
      "authors": [
        "Corr"
      ],
      "venue": ""
    }
  ]
}