{
  "paper_id": "2509.04938v1",
  "title": "An Emotion Recognition Framework Via Cross-Modal Alignment Of Eeg And Eye Movement Data",
  "published": "2025-09-05T09:00:44Z",
  "authors": [
    "Jianlu Wang",
    "Yanan Wang",
    "Tong Liu"
  ],
  "keywords": [
    "Emotion Recognition",
    "Cross-modal Attention",
    "Multimodal Learning",
    "Hybrid Feature Selection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is essential for applications in affective computing and behavioral prediction, but conventional systems relying on single-modality data often fail to capture the complexity of affective states. To address this limitation, we propose an emotion recognition framework that achieves accurate multimodal alignment of Electroencephalogram (EEG) and eye movement data through a hybrid architecture based on cross-modal attention mechanism. Experiments on the SEED-IV dataset demonstrate that our method achieve 90.62% accuracy. This work provides a promising foundation for leveraging multimodal data in emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human emotions play a crucial role in cognition and behavior, guiding decisionmaking and shaping social interactions. Automated emotion recognition, the computational identification of affective states from physiological or behavioral signals, has therefore attracted considerable attention in fields ranging from affective computing and healthcare analysis to driver monitoring and adaptive learning systems  [1] . By accurately decoding a user's emotional state in real time, emotion recognition systems can enable personalized feedback, enhance user engagement, and predict subsequent behaviors  [4] .\n\nMost existing emotion recognition systems rely solely on unimodal cues such as speech  [6]  or facial expressions  [9] , which may be insufficient under challenging conditions. Among physiological modalities, EEG and eye movement data offer distinct advantages. EEG captures high-temporal-resolution cortical dynamics associated with emotional valence and arousal, while eye movements reflect attentional shifts and autonomic nervous system activity  [13] . Unlike speech or facial expressions, these modalities are less susceptible to social masking and provide complementary perspectives on emotional states. Nevertheless, integrating these modalities remains challenging due to noise, high dimensionality, and temporal misalignment  [8] .\n\nTo address these challenges, we propose an emotion recognition framework comprising three core components: (1) a hybrid feature selection module (PCA-RFE) for noise reduction and discriminative feature extraction, (2) a temporallyaware cross-modal attention mechanism to dynamically align EEG and eye movement sequences, and (3) a residual multi-layer perceptron (MLP) classifier for hierarchical emotion pattern refinement.\n\nThe remainder of this paper is organized as follows. Section 2 reviews related work on emotion recognition and multimodal learning. Section 3 introduces the details of the proposed method. Section 4 presents experimental results and analysis. Section 5 concludes the paper and outlines future research directions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Emotion recognition aims to identify human emotional states by analyzing behavioral and physiological signals, such as speech, facial expressions, or neurophysiological data  [8] . Traditional methods primarily relied on handcrafted feature engineering combined with Machine Learning (ML) methods, including Support Vector Machines (SVM) and Random Forests (RF)  [8] . However, with the advancement of Deep Learning (DL), models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Multilayer Perceptrons (MLPs) have demonstrated superior performance by automatically learning hierarchical feature representations from raw data  [11] . These Deep Learningbased approaches have become fundamental in advancing emotion recognition task by effectively capturing complex patterns in behavioral and physiological data  [5] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Learning",
      "text": "Multimodal learning involves integrating information from multiple modalities to enhance learning accuracy and robustness  [11] . Compared to unimodal approaches, multimodal methods help mitigate noise and ambiguity inherent in single-modality data  [7] . Common techniques include early fusion (concatenating features from different modalities) and late fusion (combining modality-specific predictions)  [2] . Multimodal learning has achieved significant success in emotion recognition, particularly when integrating physiological and behavioral signals such as EEG and eye-tracking features  [12] .\n\nIn the context of emotion recognition, cross-modal approaches such as transformerbased architectures have gained traction due to their ability to capture longrange dependencies  [10] . However, MLPs often outperform transformers in scenarios with limited training data or high-dimensional multimodal inputs  [7] . Therefore, MLP-based cross-modal method is particularly suitable for the SEED-IV  [12]  dataset, which integrates EEG and eye-tracking data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mlp Emotion Classifier",
      "text": "Cross-modal attention ( )\n\nArchitecture of the proposed framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Preprocessing And Feature Extraction",
      "text": "We utilize the publicly available SEED-IV dataset 3  , which contains simultaneous EEG and eye-movement recordings from 15 users while watching emotioneliciting videos labeled as happy, sad, fearful and neutral. We extract Differential Entropy (DE) features at five different frequency bands from EEG signals. The DE of a one-dimensional signal X following a Gaussian distribution N µ, σ 2 is determined:\n\nThe resulting EEG feature vector has a dimension of 310 (62 channels x 5 frequency bands). The dataset provides 31 computational features related to eye movement, including pupil diameter, dispersion, fixation, and blink.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Selection",
      "text": "To optimize multimodal emotion recognition using EEG and eye movement features, we employ a hybrid feature selection pipeline  [3]  that integrates normalization, dimensionality reduction, and discriminative feature selection. The specific steps for feature selection are as follows. (1) The process begins with temporal normalization via Min-Max scaling applied to the raw EEG and eye-movement features.  (2)  The main selection mechanism utilizes Recursive Feature Elimination (RFE) with a linear SVM. For EEG, 200 discriminative features are selected by progressively eliminating redundant channels. For eye-movement data, 20 optimal features are retained based on their relevance to emotion classification. (3) At the same time, Principal Component Analysis (PCA) is performed to reduce feature dimensionality while preserving the global feature structure. Specifically, 80 principal components are retained for EEG. (4) The final feature representation is constructed by concatenating the PCA-reduced components and the RFE-selected raw features, resulting in fused feature dimensions of 280 for EEG and 51 for eye movement data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Modal Attention",
      "text": "The cross-modal attention mechanism dynamically aligns complementary EEG (denoted as X α ∈ R Tα×dα ) and eye movement (X β ∈ R T β ×d β ) sequences by computing attention weights between their temporal positions. Specifically, queries (Q α = X α W Qα ), keys (K β = X β W K β ), and values (V β = X β W V β ) are derived from learnable projections, where W Qα , W K β , W V β are weight matrices. The output Y α , sharing the temporal length T α of EEG, is computed as:\n\nwhere the (i, j)-th entry in the attention matrix quantifies the relevance of the j-th eye feature to the i-th EEG timestep. This mechanism adaptively fuses EEG and eye features into a unified representation by summarizing eye movement contributions weighted by EEG-guided attention.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mlp-Based Emotion Recognition",
      "text": "The MLP classifier adopts a deep residual network comprising four fully connected layers to map the fused cross-modal features to corresponding emotion labels. Prior to entering the MLP, the fused feature vector is first projected into a 128-dimensional space. Then the MLP processes the fused features through hidden layers of 256, 256, and 128 units, culminating in a 4-dimensional output for emotion classification. To ensure stable training and mitigate overfitting, batch normalization, a 30% dropout rate, and the GELU activation function are employed throughout the layers.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Settings And Baseline Methods",
      "text": "The SEED-IV dataset was utilized for emotion recognition. The dataset was partitioned into train (70%), validation (15%), and test (15%) sets using stratified sampling to preserve the class distribution. We used the PyTorch framework for model training and evaluation, employing the Adam optimizer with a learning rate of 0.0005 and 100 iterations. The batch size was set to 16, and 5-fold cross validation was conducted for robust evaluation. Model performance was evaluated using four complementary metrics: accuracy, precision, recall, and F1-score.\n\nTo validate the effectiveness of the proposed framework in emotion recognition, two baseline methods were implemented for comparative analysis. The first baseline was early feature fusion with an SVM  [12] . The second baseline was an ablated version of our framework without feature selection, where the hybrid feature selection pipeline (PCA and RFE) was excluded. Table  1  presents the average and standard deviation of accuracy, precision, recall and F1-score for three methods on the SEED-IV dataset. The proposed framework achieves state-of-the-art performance with 90.62% accuracy and 90.56% F1-score, significantly outperforming both baselines. Compared to the early feature fusion with SVM baseline, our framework achieves improvements of 13.68% in accuracy and 13.76% in F1-score. Even without feature selection, our framework still improves accuracy by 10.34% compared to the SVM-based baseline method, demonstrating the strong representational power of deep fusion and MLP classification. Further incorporating PCA and RFE feature selection leads to an additional 3.34% gain in accuracy, underlining the effectiveness of removing noisy or redundant features.  Fig.  2  shows the per-class confusion matrices for the different methods. All three methods achieve the highest accuracy rate in identifying the \"happy\" emotion among the four categories. Notably, our framework achieves recognition rates exceeding 85% for all four emotions, with 91.71% for \"neutral\" and an impressive 96.59% for \"happy\". Moreover, our framework improves the accuracy of neutral from 71.11% to 91.71%, highlighting the significance of feature selection, cross-modal attention and MLP classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Analysis Result Of The Proposed Method",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a cross-modal emotion recognition framework to address noise in raw multimodal data and limited temporal interaction modeling. Experiments demonstrated its superiority, achieving state-of-the-art performance with 90.62% accuracy and 90.56% F1-score. In future work, we will focus on advanced feature selection strategies and temporal-aware modeling to further improve accuracy and generalizability in real-world applications.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: displays the architecture of the proposed framework. The framework com-",
      "page": 3
    },
    {
      "caption": "Figure 1: Architecture of the proposed framework.",
      "page": 3
    },
    {
      "caption": "Figure 2: Confusion matrices for diﬀerent methods. (a) Feature fusion with SVM [12].",
      "page": 5
    },
    {
      "caption": "Figure 2: shows the per-class confusion matrices for the diﬀerent methods. All",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning · Hybrid Feature Selection.": "1\nIntroduction"
        },
        {
          "Learning · Hybrid Feature Selection.": "Human emotions play a crucial role in cognition and behavior, guiding decision-"
        },
        {
          "Learning · Hybrid Feature Selection.": "making and shaping social\ninteractions. Automated emotion recognition,\nthe"
        },
        {
          "Learning · Hybrid Feature Selection.": "computational\nidentiﬁcation of aﬀective states from physiological or behavioral"
        },
        {
          "Learning · Hybrid Feature Selection.": "signals, has therefore attracted considerable attention in ﬁelds ranging from af-"
        },
        {
          "Learning · Hybrid Feature Selection.": "fective\ncomputing and healthcare analysis\nto driver monitoring and adaptive"
        },
        {
          "Learning · Hybrid Feature Selection.": "learning systems\n[1]. By accurately decoding a user’s\nemotional\nstate\nin real"
        },
        {
          "Learning · Hybrid Feature Selection.": "time,\nemotion recognition systems\ncan enable personalized feedback, enhance"
        },
        {
          "Learning · Hybrid Feature Selection.": "user engagement, and predict subsequent behaviors [4]."
        },
        {
          "Learning · Hybrid Feature Selection.": "Most existing emotion recognition systems rely solely on unimodal cues such"
        },
        {
          "Learning · Hybrid Feature Selection.": "as speech [6] or facial expressions [9], which may be insuﬃcient under challenging"
        },
        {
          "Learning · Hybrid Feature Selection.": "conditions. Among physiological modalities, EEG and eye movement data oﬀer"
        },
        {
          "Learning · Hybrid Feature Selection.": "distinct advantages. EEG captures high-temporal-resolution cortical dynamics"
        },
        {
          "Learning · Hybrid Feature Selection.": "associated with emotional valence and arousal, while eye movements reﬂect at-"
        },
        {
          "Learning · Hybrid Feature Selection.": "tentional\nshifts and autonomic nervous\nsystem activity [13]. Unlike speech or"
        },
        {
          "Learning · Hybrid Feature Selection.": "⋆"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nJianlu Wang et al.": "facial expressions,\nthese modalities are less\nsusceptible to social masking and"
        },
        {
          "2\nJianlu Wang et al.": "provide complementary perspectives on emotional states. Nevertheless, integrat-"
        },
        {
          "2\nJianlu Wang et al.": "ing these modalities remains challenging due to noise, high dimensionality, and"
        },
        {
          "2\nJianlu Wang et al.": "temporal misalignment [8]."
        },
        {
          "2\nJianlu Wang et al.": "To address these challenges, we propose an emotion recognition framework"
        },
        {
          "2\nJianlu Wang et al.": "comprising three core components: (1) a hybrid feature selection module (PCA-"
        },
        {
          "2\nJianlu Wang et al.": "RFE) for noise reduction and discriminative feature extraction, (2) a temporally-"
        },
        {
          "2\nJianlu Wang et al.": "aware cross-modal attention mechanism to dynamically align EEG and eye move-"
        },
        {
          "2\nJianlu Wang et al.": "ment\nsequences, and (3) a residual multi-layer perceptron (MLP) classiﬁer for"
        },
        {
          "2\nJianlu Wang et al.": "hierarchical emotion pattern reﬁnement."
        },
        {
          "2\nJianlu Wang et al.": "The remainder of this paper is organized as follows. Section 2 reviews related"
        },
        {
          "2\nJianlu Wang et al.": "work on emotion recognition and multimodal\nlearning. Section 3 introduces the"
        },
        {
          "2\nJianlu Wang et al.": "details of\nthe proposed method. Section 4 presents\nexperimental\nresults and"
        },
        {
          "2\nJianlu Wang et al.": "analysis. Section 5 concludes the paper and outlines future research directions."
        },
        {
          "2\nJianlu Wang et al.": "2\nRelated Work"
        },
        {
          "2\nJianlu Wang et al.": "2.1\nEmotion Recognition"
        },
        {
          "2\nJianlu Wang et al.": "Emotion recognition aims to identify human emotional states by analyzing be-"
        },
        {
          "2\nJianlu Wang et al.": "havioral and physiological\nsignals,\nsuch as\nspeech,\nfacial\nexpressions, or neu-"
        },
        {
          "2\nJianlu Wang et al.": "rophysiological data [8]. Traditional methods primarily relied on handcrafted"
        },
        {
          "2\nJianlu Wang et al.": "feature engineering combined with Machine Learning (ML) methods,\nincluding"
        },
        {
          "2\nJianlu Wang et al.": "Support Vector Machines (SVM) and Random Forests (RF) [8]. However, with"
        },
        {
          "2\nJianlu Wang et al.": "the advancement of Deep Learning (DL), models such as Convolutional Neural"
        },
        {
          "2\nJianlu Wang et al.": "Networks (CNNs), Recurrent Neural Networks (RNNs), and Multilayer Percep-"
        },
        {
          "2\nJianlu Wang et al.": "trons (MLPs) have demonstrated superior performance by automatically learn-"
        },
        {
          "2\nJianlu Wang et al.": "ing hierarchical feature representations from raw data [11]. These Deep Learning-"
        },
        {
          "2\nJianlu Wang et al.": "based approaches have become fundamental\nin advancing emotion recognition"
        },
        {
          "2\nJianlu Wang et al.": "task by eﬀectively capturing complex patterns in behavioral and physiological"
        },
        {
          "2\nJianlu Wang et al.": "data [5]."
        },
        {
          "2\nJianlu Wang et al.": "2.2\nMultimodal Learning"
        },
        {
          "2\nJianlu Wang et al.": "Multimodal\nlearning involves integrating information from multiple modalities"
        },
        {
          "2\nJianlu Wang et al.": "to enhance learning accuracy and robustness\n[11]. Compared to unimodal ap-"
        },
        {
          "2\nJianlu Wang et al.": "proaches, multimodal methods help mitigate noise and ambiguity inherent\nin"
        },
        {
          "2\nJianlu Wang et al.": "single-modality data [7]. Common techniques include early fusion (concatenating"
        },
        {
          "2\nJianlu Wang et al.": "features from diﬀerent modalities) and late fusion (combining modality-speciﬁc"
        },
        {
          "2\nJianlu Wang et al.": "predictions) [2]. Multimodal\nlearning has achieved signiﬁcant success in emotion"
        },
        {
          "2\nJianlu Wang et al.": "recognition, particularly when integrating physiological and behavioral signals"
        },
        {
          "2\nJianlu Wang et al.": "such as EEG and eye-tracking features [12]."
        },
        {
          "2\nJianlu Wang et al.": "In the context of emotion recognition, cross-modal approaches such as transformer-"
        },
        {
          "2\nJianlu Wang et al.": "based architectures have gained traction due to their ability to capture long-"
        },
        {
          "2\nJianlu Wang et al.": "range dependencies [10]. However, MLPs often outperform transformers in sce-"
        },
        {
          "2\nJianlu Wang et al.": "narios with limited training data or high-dimensional multimodal\ninputs\n[7]."
        },
        {
          "2\nJianlu Wang et al.": "Therefore, MLP-based cross-modal method is particularly suitable for the SEED-"
        },
        {
          "2\nJianlu Wang et al.": "IV [12] dataset, which integrates EEG and eye-tracking data."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Final feature": ""
        },
        {
          "Final feature": ""
        },
        {
          "Final feature": ""
        },
        {
          "Final feature": ""
        },
        {
          "Final feature": ""
        },
        {
          "Final feature": ""
        },
        {
          "Final feature": "max"
        },
        {
          "Final feature": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nJianlu Wang et al.": "features. (2) The main selection mechanism utilizes Recursive Feature Elimina-"
        },
        {
          "4\nJianlu Wang et al.": "tion (RFE) with a linear SVM. For EEG, 200 discriminative features are selected"
        },
        {
          "4\nJianlu Wang et al.": "by progressively eliminating redundant channels. For eye-movement data, 20 op-"
        },
        {
          "4\nJianlu Wang et al.": "timal features are retained based on their relevance to emotion classiﬁcation. (3)"
        },
        {
          "4\nJianlu Wang et al.": "At the same time, Principal Component Analysis (PCA) is performed to reduce"
        },
        {
          "4\nJianlu Wang et al.": "feature dimensionality while preserving the global feature structure. Speciﬁcally,"
        },
        {
          "4\nJianlu Wang et al.": "80 principal components are retained for EEG."
        },
        {
          "4\nJianlu Wang et al.": "tation is constructed by concatenating the PCA-reduced components and the"
        },
        {
          "4\nJianlu Wang et al.": "RFE-selected raw features, resulting in fused feature dimensions of 280 for EEG"
        },
        {
          "4\nJianlu Wang et al.": "and 51 for eye movement data."
        },
        {
          "4\nJianlu Wang et al.": "3.3\nCross-modal Attention"
        },
        {
          "4\nJianlu Wang et al.": "The cross-modal attention mechanism dynamically aligns complementary EEG"
        },
        {
          "4\nJianlu Wang et al.": "RTα×dα) and eye movement (Xβ\n(denoted as Xα"
        },
        {
          "4\nJianlu Wang et al.": "∈\n∈"
        },
        {
          "4\nJianlu Wang et al.": "puting attention weights between their temporal positions. Speciﬁcally, queries"
        },
        {
          "4\nJianlu Wang et al.": "(Qα = XαWQα ), keys (Kβ = XβWKβ ), and values"
        },
        {
          "4\nJianlu Wang et al.": "rived from learnable projections, where WQα , WKβ , WVβ are weight matrices."
        },
        {
          "4\nJianlu Wang et al.": "The output Yα, sharing the temporal\nlength Tα of EEG,"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Recognition Performance (Mean ± Standard Deviation)",
      "data": [
        {
          "100": ""
        },
        {
          "100": "80"
        },
        {
          "100": "60"
        },
        {
          "100": "40"
        },
        {
          "100": ""
        },
        {
          "100": "20"
        },
        {
          "100": ""
        },
        {
          "100": "0"
        },
        {
          "100": ""
        },
        {
          "100": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nJianlu Wang et al.": "neutral from 71.11% to 91.71%, highlighting the signiﬁcance of feature selection,"
        },
        {
          "6\nJianlu Wang et al.": "cross-modal attention and MLP classiﬁcation."
        },
        {
          "6\nJianlu Wang et al.": "5\nConclusion"
        },
        {
          "6\nJianlu Wang et al.": "In this paper, we proposed a cross-modal\nemotion recognition framework to"
        },
        {
          "6\nJianlu Wang et al.": "address noise in raw multimodal data and limited temporal\ninteraction model-"
        },
        {
          "6\nJianlu Wang et al.": "ing. Experiments demonstrated its superiority, achieving state-of-the-art perfor-"
        },
        {
          "6\nJianlu Wang et al.": "mance with 90.62% accuracy and 90.56% F1-score. In future work, we will focus"
        },
        {
          "6\nJianlu Wang et al.": "on advanced feature selection strategies and temporal-aware modeling to further"
        },
        {
          "6\nJianlu Wang et al.": "improve accuracy and generalizability in real-world applications."
        },
        {
          "6\nJianlu Wang et al.": "Acknowledgment This work was supported by the China National College Stu-"
        },
        {
          "6\nJianlu Wang et al.": "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066)."
        },
        {
          "6\nJianlu Wang et al.": "References"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "References"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "1. Calvo, R.A., D’Mello, S.: Aﬀect detection: An interdisciplinary review of models,"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "methods, and their applications.\nIEEE Transactions on aﬀective computing 1(1),"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "18–37 (2010)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "2. Gadzicki, K., Khamsehashari, R., Zetzsche, C.: Early vs late fusion in multimodal"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "convolutional neural networks.\nIn: 2020\nIEEE 23rd international\nconference on"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "information fusion (FUSION). pp. 1–6. IEEE (2020)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "3. Hag, A., Handayani, D., Altalhi, M., Pillai, T., Mantoro, T., Kit, M.H., Al-Shargie,"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "F.: Enhancing eeg-based mental stress state recognition using an improved hybrid"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "feature selection algorithm. Sensors 21(24),\n8370 (2021)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "4. Hudlicka, E.: To feel or not to feel: The role of aﬀect in human–computer interac-"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "tion. International\njournal of human-computer studies 59(1-2), 1–32 (2003)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "5.\nJafari, M., Shoeibi, A., Khodatars, M., Bagherzadeh, S., Shalbaf, A., García, D.L.,"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "Gorriz, J.M., Acharya, U.R.: Emotion recognition in eeg signals using deep learning"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "methods: A review. Computers in Biology and Medicine 165, 107450 (2023)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "6. Koolagudi, S.G., Rao, K.S.: Emotion recognition from speech: a review.\nInterna-"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "tional\njournal of speech technology 15, 99–117 (2012)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "7. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y., et al.: Multimodal"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "deep learning. In: ICML. vol. 11, pp. 689–696 (2011)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "8.\nSaxena, A., Khanna, A., Gupta, D.: Emotion recognition and detection methods:"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "A comprehensive survey. Journal of Artiﬁcial Intelligence and Systems 2(1), 53–79"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "(2020)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "9. Tarnowski, P., Kołodziej, M., Majkowski, A., Rak, R.J.: Emotion recognition using"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "facial expressions. Procedia Computer Science 108, 1175–1184 (2017)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "10. Tsai, Y.H.H., Bai, S., Liang, P.P., Kolter, J.Z., Morency, L.P., Salakhutdinov, R.:"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "Multimodal\ntransformer\nfor unaligned multimodal\nlanguage\nsequences.\nIn: Pro-"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "ceedings\nof\nthe\nconference. Association for\ncomputational\nlinguistics. Meeting."
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "vol. 2019, p. 6558 (2019)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "11. Zhang, S., Yang, Y., Chen, C., Zhang, X., Leng, Q., Zhao, X.: Deep learning-"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "based multimodal emotion recognition from audio, visual, and text modalities: A"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "systematic review of\nrecent advancements and future prospects. Expert Systems"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "with Applications 237, 121692 (2024)"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "12. Zheng, W.L., Liu, W., Lu, Y., Lu, B.L., Cichocki, A.: Emotionmeter: A multi-"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "modal\nframework for\nrecognizing human emotions.\nIEEE transactions on cyber-"
        },
        {
          "dent Innovation and Entrepreneurship Training Program (Grant No. 202410424066).": "netics 49(3), 1110–1122 (2018)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Title Suppressed Due to Excessive Length\n7": "13. Zheng, W.L., Lu, B.L.:\nInvestigating critical\nfrequency bands and channels\nfor"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "eeg-based emotion recognition with deep neural networks.\nIEEE Transactions on"
        },
        {
          "Title Suppressed Due to Excessive Length\n7": "autonomous mental development 7(3), 162–175 (2015)"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S D'mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "2",
      "title": "Early vs late fusion in multimodal convolutional neural networks",
      "authors": [
        "K Gadzicki",
        "R Khamsehashari",
        "C Zetzsche"
      ],
      "year": "2020",
      "venue": "2020 IEEE 23rd international conference on information fusion (FUSION)"
    },
    {
      "citation_id": "3",
      "title": "Enhancing eeg-based mental stress state recognition using an improved hybrid feature selection algorithm",
      "authors": [
        "A Hag",
        "D Handayani",
        "M Altalhi",
        "T Pillai",
        "T Mantoro",
        "M Kit",
        "F Al-Shargie"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "4",
      "title": "To feel or not to feel: The role of affect in human-computer interaction",
      "authors": [
        "E Hudlicka"
      ],
      "year": "2003",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in eeg signals using deep learning methods: A review",
      "authors": [
        "M Jafari",
        "A Shoeibi",
        "M Khodatars",
        "S Bagherzadeh",
        "A Shalbaf",
        "D García",
        "J Gorriz",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "S Koolagudi",
        "K Rao"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "7",
      "title": "Multimodal deep learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "ICML"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition and detection methods: A comprehensive survey",
      "authors": [
        "A Saxena",
        "A Khanna",
        "D Gupta"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence and Systems"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition using facial expressions",
      "authors": [
        "P Tarnowski",
        "M Kołodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "10",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "11",
      "title": "Deep learningbased multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "X Zhang",
        "Q Leng",
        "X Zhao"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "12",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "Emotionmeter: A multimodal framework for recognizing human emotions"
    },
    {
      "citation_id": "13",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W Zheng",
        "B Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    }
  ]
}