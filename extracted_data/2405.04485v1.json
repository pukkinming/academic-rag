{
  "paper_id": "2405.04485v1",
  "title": "Adapting Wavlm For Speech Emotion Recognition",
  "published": "2024-05-07T16:53:42Z",
  "authors": [
    "Daria Diatlova",
    "Anton Udalov",
    "Vitalii Shutov",
    "Egor Spirin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal finetuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus. More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances. We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is a highly-demanded task in practical applications. These include emotion recognition for analyzing customer satisfaction from voice messages and calls  [1] , creating a robust Automatic Speech Recognition (ASR) model  [2] , or providing essential content for generating avatars in video calls  [3] .\n\nRecently, the interest among machine learning researchers in using self-supervised learning (SSL) models for audio processing has grown  [4, 5, 6] . This can be explained by the successful application of transfer learning, which involves leveraging the knowledge of SSL upstream models, initially trained on a large amount of unlabeled data, by a downstream model  [7] . The most commonly used method to transfer knowledge to a downstream model involves fine-tuning the upstream model by adding new layers from the downstream model  [7] .\n\nThe application of transfer learning in the audio domain, particularly SER, has received considerable attention of late. In  [8] , the authors demonstrated that using an SSL model as a feature extractor outperforms SER solutions based on low-level descriptors, even when provided with eight times less labeled data. Following that, works focusing on the specific utilization of SSL to enhance SER performance have emerged  [8, 9, 10] . These approaches include feature-wise objective constraints  [9] , the utilization of text encoders  [8, 10] , pooling mechanics  [11] , and more.\n\nIn this paper, we share the technical details of creating a SSL-based model for speech emotion recognition using WavLM  [6]  in the Odyssey 2024 challenge  [12] , and address several research questions.\n\n• Does applying time-dimensional pooling over WavLM output impact the quality of speech emotion recognition?\n\n• Can incorporating information about the speaker's gender enhance emotion classification?\n\n• Does leveraging information about the text of an utterance help emotion classification?\n\nUsing the MSP Podcast dataset for verification, we obtained a solution which we then used for the Odyssey 2024 challenge.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we discuss WavLM  [6] , a self-supervised audio processing model, and its implications for SER. We explore cutting-edge strategies for information gathering and highlight the importance of advanced pooling techniques. Additionally, we consider the incorporation of auxiliary data such as text and gender embeddings to enhance SER systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Wavlm",
      "text": "WavLM  [6]  is a self-supervised audio processing model that utilizes the Transformer encoder architecture  [13] . The model has been enhanced with a gated relative position bias, improving its ability to model sequential information. Specifically, WavLM takes a raw audio signal X ∈ R n , where n is the number of samples in the raw signal, and produces an encoded output Z ∈ R [l×m×h] , where:\n\n• l is the number of Transformer layers,\n\n• m is the sequence length,\n\n• h is the hidden dimension.\n\nTo accommodate various computational needs and applications, WavLM is available in two configurations: Base, with 12 Transformer layers, and Large, with 24 layers. The WavLM model is trained on extensive audio corpora to encode a wide range of audio applications. It incorporates a speech denoising objective and masked speech prediction during pre-training, enabling efficient extraction of speech and acoustic features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Self-Supervised Learning In Ser",
      "text": "Self-supervised learning (SSL) has shown promising results in SER  [8, 9] , offering innovative ways to decode emotional subtleties in speech. It is common for SER models built using SSL to use the Upstream + Downstream model architecture  [10] .\n\nThe upstream block aims to extract features for the downstream model to predict emotion classes.\n\nBefore passing the upstream model's output to the downstream model, it is typically reduced in both the layer and time dimensions. A common approach for layer-wise reduction is:\n\n• to take the Ẑ ∈ R [ln×m×h] hidden representation, where ln is the output from the last Transformer layer  [14] .\n\n• to select the weighted average sum of all layers  [14] .\n\nFor the second case, a vector of trainable weight parameters w ∈ R l is initialized. Then, a layer-wise averaged output Ẑ = Z • w, where Ẑ ∈ R [m×h] , is passed through the time dimension reduction block. The simplest implementation of the time-reduction block is to apply average pooling  [15] :\n\nẐi m , where xout ∈ R [h] . After this step, xout is passed to the downstream block for further classification.\n\nThe choice of the downstream architecture, as well as the modification applied to the process of obtaining xout, can enhance the quality of the SER model.\n\nThe combination of mean average pooling and sophisticated aggregators, such as ECAPA-TDNN  [16] , with linear classifiers improved SER performance in  [10] . In  [11] , attentive correlation pooling by adding weights to the statistical estimates was introduced. This technique aims to address the challenge of utilizing emotional information across extended periods of time. Additionally, the authors of  [11]  demonstrated the benefits of label smoothing for dealing with the emotional ambiguity of speech. State-of-the-art SER performance was achieved by combining attentive correlation pooling with label smoothing and dropout, without the need for fine-tuning the SSL model.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Information Accumulation",
      "text": "Information accumulation techniques, particularly those involving pooling mechanisms and the integration of auxiliary information such as text and gender embeddings, play a pivotal role in enhancing SER systems.\n\nIn  [17] , introduced a two-stage fine-tuning process for wav2vec 2.0, emphasizing the use of automatic speech recognition (ASR) and gender pretraining to enrich the SER model with additional features. By first embedding these attributes into the model and then focusing on emotion recognition, the approach effectively leverages multitask learning (MTL) and adversarial learning to incorporate auxiliary task information, aiming to mitigate the gradient conflict issue commonly observed in MTL setups. This method highlights the utility of informed feature extraction for boosting SER performance.\n\nFurthermore, the concept of attentive statistics pooling  [18]  proposes an advanced pooling strategy that goes beyond the traditional average pooling method by including higher-order statistics, such as standard deviation, weighted by attention mechanisms. This approach calculates both the weighted mean and standard deviation of frame-level features, enhancing the model's ability to capture speaker discriminability through feature variability over time. By applying attention to both means and variances, it introduces a more nuanced understanding of speaker characteristics vital for SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Modifications",
      "text": "In this section, we propose several modifications aimed at addressing our research questions. More specifically, we describe 1) replacing the commonly used average pooling with pooling types, 2) using gender information for predictions, and 3) using text information in conjunction with the pooled output.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Time-Dimensional Pooling",
      "text": "The output of WavLM is usually reduced over time-dimension using simple average pooling and then passed to the classification head, as discussed in Section 2.2. Instead of average pooling, we suggest using STD and attention pooling.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Std Pooling",
      "text": "In standard deviation (STD) pooling the average pooling output (Eq. 1, where m is the sequence length and Zi is the layerwise averaged output of WavLM for a time frame i) is used along with the standard deviation output (Eq. 2). The result-ing statistics are concatenated over the hidden dimension (Eq. 3). Compared to a simple average pooling, which is generally robust to noisy output, STD pooling focuses on regions with high variability. It can emphasize distinct features that differ from neighboring regions, which can be beneficial for emotion classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Attention Pooling",
      "text": "Attention pooling  [18]  makes it possible for certain frames to become more valuable based on context. Attention weights are computed as shown in Eq. 4, where p ∈ R h is the trainable parameter and • denotes matrix multiplication.\n\nẐ is multiplied element-wise by the attention weights (Eq. 5), then the mean and variance are computed from the resulting output and concatenated as described in Eq. 3.\n\n(5)\n\nAttention pooling provides the opportunity to give certain frames more weight than others when computing the mean and standard deviation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gender Conditioning",
      "text": "The MSP Podcast corpus includes gender labels: male and female. Therefore, we added a lookup table of trainable embeddings.\n\nFor the conditioning mechanism, we used a pointwise multiplication of pooled output and gender embeddings as shown in Eq. 6. In section 5.2, we provide an ablation study on other conditioning mechanisms.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text Conditioning",
      "text": "Along with gender information, the MSP Podcast corpus contains textual annotations paired with audio clips. To combine the textual information with the encoded audio representation and gender embeddings, we employed the Sentence Transformer  [19] , which maps transcriptions to a 384-dimensional dense vector, etext. Conditioning was applied as depicted in Eq. 7, where F represents a sequence of transformations: Linear Layer  [20] , Layer Normalization  [21] , ReLU  [22] , and Dropout  [23] .\n\nThe ablation study on conditioning approaches can be found in Section 5.3.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this section, we provide an overview of the dataset, the baseline proposed by the Odyssey Challenge 2024  [12]  Organizers, and the setup for training and evaluating the models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Neutral",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "The dataset used for training and evaluation is the MSP Podcast Corpus  [24] . It consists of 90,522 speaking turns, with the original train, development, and test3 split. The test3 dataset does not have publicly available labels. Therefore, all our experiments, except for the final submission result, are presented using only the development set.\n\nFor the train and development set labels, we used the consensus (plurality vote of 3 out of 5 assessors) of the primary emotion, filtering out the classes \"Other\" and \"No agreement\". This resulted in 53,386 samples in the training set and 15,341 samples in the development set. On the early stages of experiments we collected the \"confident\" training set by taking the consensus (plurality vote of 4 out of 5 assessors) of the primary emotion. This resulted in just 23,509 samples, but did not lead to any improvement of the F1-macro score.\n\nThe MSP Podcast dataset exhibits a high level of imbalance, with the class distribution of the training and development sets shown in Figure  1 . Note that the order of the number of samples of each class relative to the total number of classes is different in the training and validation datasets. Early on in our experiments, we attempted to balance the dataset by augmenting utterances from classes with fewer samples, but did not observe any significant improvements.\n\nThe test3 dataset lacks gender labels, unlike the training and validation datasets. To obtain gender labels, we utilized an internal gender-prediction model. As for text transcriptions, we observed that some examples in the training and validation datasets also lack text transcriptions. We used the Whisper  [25]  V3 Large model to transcribe these audio recordings, along with the test3 dataset.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Baseline",
      "text": "An official baseline was provided by the organizers of the Odyssey 2024 challenge  [12] . The baseline model is a WavLM Large model, as described in Section 2.1, which produces the output Z ∈ R [l×m×h] , where l = 24 (Transformer layers), h = 768 (hidden dimension), and m is the dynamically changed sequence length shape.\n\nFor classification, the weighted average sum of all layers is selected as discussed in Section 2.2. The pooled output is then passed through the projection layer, which consists of the Linear Layer  [20] , Layer Normalization  [21] , ReLU  [22] , and Dropout  [23] . Then, the attention pooling block (Section 3.1.2) is applied to the output and passed through the final Linear Layer to get the logits for each emotion class.\n\nThe model is trained with the weighted cross entropy loss  [26] , the weights are declared as shown in Eq. 8, where wi is the weight for class i, Ntotal is the size of train dataset, m is the number of classes and Nclass is the number of samples in class i. wi = Ntotal • Nclass m (8)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training And Evaluation",
      "text": "All our experiments are based on WavLM Large. Unlike the baseline model, we used a single Linear Layer as a projection layer. We did not notice a significant difference in quality from the baseline model. However, our version is less memoryintensive, which is crucial when training large models. The size of the projector linear layer is set to 256 by default unless specified otherwise. We performed various experiments by adjusting the projection layer size, with the results detailed in Table  1 . From the results, it is evident that the optimal choices for the projection layer size are 16, 32, and 256. The default selection of a hidden size of 256 is motivated by the requirement to combine the output of the projection layer with gender and text embeddings, where a larger embedding size makes it possible to encode more information.\n\nIn some of our experiments, we apply label smoothing within cross-entropy, which is motivated by emotional data ambiguity. The transition from hard labels to soft labels consistently outperforms cross-entropy loss and leads to faster convergence during training  [27] . Label smoothing is defined by parameter γ, where 1 -γ of the weight is assigned to the true label, and γ is distributed across other classes.\n\nTable  2  presents the results from applying label smoothing. It can be seen that, in our case, the usage of label smoothing leads to F1-macro score degradation.\n\nAll models were trained using the AdamW optimizer  [28]  with a learning rate of 1e-5 and weight decay of 0.01 for 20 epochs employing a batch size of 32 on a single A100 GPU.\n\nWe keep the layers of WavLM CNN feature encoder frozen during all experiments. For model training, we use audio clips of no longer than 5 seconds. For longer utterances, a 5-second segment is randomly selected on each epoch using cropping.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we answer the research questions stated in Section 1 by conducting our experiments with the modifications from Section 3. For statistical significance, we run each experiment with several seeds and average the results. We conducted experiments using STD and attention pooling as described in Section 3.1, and compared the results to using standard average pooling.\n\nFrom the results presented in Table  3 , it is evident that the change of pooling type influences overall performance of the SER model based on F1-macro score. STD pooling yielded the best F1-macro score in our experimental setup, while attention pooling performed worse than standard average pooling in our experiment setup.\n\nNote that the baseline model, which uses average pooling, achieves an F1-macro score of 0.31 on the development set. Therefore, it is worthwhile to increase the number of training steps to achieve its best performance. However, we chose to use STD pooling for the rest of our experiments as it converges faster, which is crucial for working within the Challenge. As outlined in Section 3.2, we utilized gender information to condition the pooled output. For this ablation study, we employed STD temporal pooling. In this setup, \"sum\" and \"multiplication\" denote the addition and the multiplication of the pooled output with the gender embedding, \"sum / 2\" indicates that the output is divided by 2 after addition. In the \"stack + linear\" configuration, the gender embedding and pooled output are concatenated along the hidden dimension and passed through a linear layer. The term \"CLN\" refers to conditioning layer normalization  [21] , as shown in Eq. 9, where g and f represent linear layers. As shown by the results in Table  4 , the sum and normalized sum reach the best F1-macro scores. CLN and multiplication reach the second best F1-macro score, but it is equal to no conditioning case. Meanwhile, the stack + linear setup leads to F1-macro degradation.\n\nIn summary, combining the WavLM output with gender information can enhance the accuracy of the SER model, but the specific strategy of combining them is crucial.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Does Leveraging Information About The Text Of An Utterance Help Emotion Classification?",
      "text": "In this experiment, we followed a setup similar to the one in Section 3.3, incorporating STD pooling together with gender and text embeddings. The conditioning is applied using multiplication, normalized sum (\"sum / 3\"), and CLN.\n\nFor CLN, we first concatenated the gender and text embeddings along the hidden dimension and passed the resulting vector through a linear layer to reduce the hidden dimension of a conditioned vector to the initial size of 256.\n\nFrom the results presented in Table  5 , it is evident that the addition of text information led to the degradation in the F1 score compared to using only the gender embedding. The best result was achieved using CLN and sum / 3 conditioning. In conclusion, combining the text information with the encoded WavLM output and gender embeddings did not improve the accuracy of the SER model in our specific case. This can be explained by the chosen text encoder's insensitivity to emotional speech, as it encodes text primarily based on context. Alternatively, it may be due the limited correlation between text and emotion in most classes, or the strategies used to combine this information.\n\nIt is important to mention that we included text embedding in the WavLM output along with the information about gender in our experiment setup. This addition helped us enhance the quality of emotion recognition in the previous experiment (Section 3.2). However, combining it with information about the text may negatively impact recognition quality, as it reduces the",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "In this section, we explain how we combined several models for our Odyssey 2024 challenge submission and discuss the final results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fusion Process",
      "text": "Fusing the predictions from multiple models improve the generalization ability and can lead to improved performance  [29] . We employed the Constrained Optimization By Linear Approximation algorithm  [30]  to train the weights matrix wfusion ∈ R nm×l , where nm is the number of models and l is the number of classes. The predicted label p from the predictions matrix M ∈ R nm×l was determined as shown in Eq. 10. The weights matrix w f usion was trained with the following constraints: nm i=1 w i f usion = l i=1 w i f usion = 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Configuration",
      "text": "We aimed to combine the best of our models based on the F1macro score and the models with the most diverse configurations. As a result, we choose 5 models with configurations described in Table  6 . The F1 score for each emotion class and F1-macro for each model are reported in Table  7 .\n\nPlease note that the results in Table  7  were obtained during our work on the challenge, and there are some differences in the F1-macro compared to the results reported in Section 5. For instance, model 3 in Table  7  has the same configuration as the model in the fourth row of Table  4 , but it shows an F1-macro of 0.32, as opposed to 0.31. This difference is due to running additional experiments after submission to provide statistically significant results for the research questions in Section 5.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Result Analysis",
      "text": "Table  7  demonstrates that each of the models selected for fusion achieves the highest F1 score for two or three emotional classes compared to the others. The idea behind fusion is to leverage the strengths of all models. The last row in Table  7  displays the F1 scores for the fused model predictions. By combining predictions from the 5 models, we achieved an F1-macro score of 0.35, which is 0.02 higher than the best individual model's F1 score. Notably, the F1 scores for the \"Angry\" and \"Contempt\" classes after fusion are also higher than the maximum among the 5 models. For the other classes, the F1 score lies between the lowest and highest values in the column.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we discussed the modifications made to the WavLM model during its fine-tuning for speech emotion recognition as part of our participation in the Odyssey Challenge 2024. We posed several research questions to investigate the importance of time-dimensional pooling, gender, and text accumulation in training the SER model. Our experiments on the MSP Podcast Corpus led us to the conclusion that incorporating STD pooling with accumulated gender information could enhance the performance of SER models based on WavLM.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Label distribution in the training and development sets. Note that the dataset is highly imbalanced, and the distribution of",
      "page": 3
    },
    {
      "caption": "Figure 1: Note that the order of the number",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 6: The F1 score for each emotion class and 2012,pp.5109–5112.",
      "data": [
        {
          "Parameter": "Temporal pooling\nPooled output size\nGender conditioning\nText conditioning\nLabel smoothing",
          "Model 1 (Baseline)": "attention\n256\n-\n-\n0.0",
          "Model 2": "std\n32\n-\n-\n0.0",
          "Model 3": "std\n256\nmultiplication\n-\n0.0",
          "Model 4": "std\n256\nsum / 3\nsum / 3\n0.0",
          "Model 5": "std\n256\nsum / 3\nsum / 3\n0.1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 6: The F1 score for each emotion class and 2012,pp.5109–5112.",
      "data": [
        {
          "Model": "Model 1 (Baseline)\nModel 2\nModel 3\nModel 4\nModel 5",
          "F1 Macro": "0.31\n0.33\n0.32\n0.31\n0.31",
          "Neutral": "0.55\n0.58\n0.51\n0.13\n0.29",
          "Happy": "0.57\n0.52\n0.58\n0.59\n0.59",
          "Angry": "0.40\n0.53\n0.53\n0.40\n0.42",
          "Contempt": "0.20\n0.15\n0.38\n0.15\n0.15",
          "Sad": "0.13\n0.40\n0.21\n0.42\n0.42",
          "Surprise": "0.42\n0.19\n0.20\n0.21\n0.21",
          "Disgust": "0.03\n0.21\n0.14\n0.55\n0.12",
          "Fear": "0.15\n0.04\n0.02\n0.04\n0.05"
        },
        {
          "Model": "Fusion",
          "F1 Macro": "0.35",
          "Neutral": "0.53",
          "Happy": "0.57",
          "Angry": "0.61",
          "Contempt": "0.43",
          "Sad": "0.17",
          "Surprise": "0.24",
          "Disgust": "0.19",
          "Fear": "0.05"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Negative emotions detection as an indicator of dialogs quality in call centers",
      "authors": [
        "Christophe Vaudable",
        "Laurence Devillers"
      ],
      "year": "2012",
      "venue": "2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "Application of emotion recognition and modification for emotional telugu speech recognition",
      "authors": [
        "Vishnu Vidyadhara",
        "Raju Vegesna",
        "Krishna Gurugubelli",
        "Anil Vuppala"
      ],
      "year": "2019",
      "venue": "Mobile Networks and Applications"
    },
    {
      "citation_id": "4",
      "title": "Expressive talking avatars",
      "authors": [
        "Ye Pan",
        "Shuai Tan",
        "Shengran Cheng",
        "Qunfen Lin",
        "Zijiao Zeng",
        "Kenny Mitchell"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato"
    },
    {
      "citation_id": "6",
      "title": "data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning, Kamalika Chaudhuri, Stefanie Jegelka"
    },
    {
      "citation_id": "7",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Jian Wu",
        "Long Zhou",
        "Y Shuo Ren",
        "Qian",
        "Micheal Yao Qian",
        "Furu Zeng",
        "Wei"
      ],
      "year": "2021",
      "venue": "IEEE Journal on Selected Topics in Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "A review of deep transfer learning and recent advancements",
      "authors": [
        "Mohammadreza Iman",
        "Hamid Reza Arabnia",
        "Khaled Rasheed"
      ],
      "year": "2023",
      "venue": "A review of deep transfer learning and recent advancements"
    },
    {
      "citation_id": "9",
      "title": "Recognizing more emotions with less data using selfsupervised transfer learning",
      "authors": [
        "Jonathan Boigne",
        "Biman Liyanage",
        "Ted Östrem"
      ],
      "venue": "Recognizing more emotions with less data using selfsupervised transfer learning"
    },
    {
      "citation_id": "10",
      "title": "Temporal Context in Speech Emotion Recognition",
      "authors": [
        "Yangyang Xia",
        "Li-Wei Chen",
        "Alexander Rudnicky",
        "Richard Stern"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "Speech emotion recognition using self-supervised features",
      "arxiv": "arXiv:2202.03896"
    },
    {
      "citation_id": "12",
      "title": "Speech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing",
      "authors": [
        "Sofoklis Kakouros",
        "Themos Stafylakis",
        "Ladislav Mošner",
        "L Burget"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Odyssey2024 -speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Reddy Naini",
        "L Moro-Velazquez",
        "T Thebaud",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Odyssey 2024: The Speaker and Language Recognition Workshop)"
    },
    {
      "citation_id": "14",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Peihao Chen",
        "Xiangmin Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "authors": [
        "Brecht Desplanques",
        "Jenthe Thienpondt",
        "Kris Demuynck"
      ],
      "year": "2020",
      "venue": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification"
    },
    {
      "citation_id": "18",
      "title": "Twostage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining",
      "authors": [
        "Yuan Gao",
        "Chenhui Chu",
        "Tatsuya Kawahara"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "19",
      "title": "Attentive Statistics Pooling for Deep Speaker Embedding",
      "authors": [
        "Koji Okabe",
        "Takafumi Koshinaka",
        "Koichi Shinoda"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Learning internal representations by error propagation, parallel distributed processing, explorations in the microstructure of cognition",
      "authors": [
        "Geoffrey David E Rumelhart",
        "Ronald Hinton",
        "Williams"
      ],
      "year": "1986",
      "venue": "Biometrika"
    },
    {
      "citation_id": "22",
      "title": "Layer normalization",
      "authors": [
        "Jimmy Lei Ba",
        "Jamie Ryan Kiros",
        "Geoffrey Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization"
    },
    {
      "citation_id": "23",
      "title": "Deep learning using rectified linear units (relu)",
      "authors": [
        "Abien Fred"
      ],
      "year": "2018",
      "venue": "Deep learning using rectified linear units (relu)",
      "arxiv": "arXiv:1803.08375"
    },
    {
      "citation_id": "24",
      "title": "Improving neural networks by preventing co-adaptation of feature detectors",
      "authors": [
        "Geoffrey Hinton",
        "Nitish Srivastava",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "year": "2012",
      "venue": "Improving neural networks by preventing co-adaptation of feature detectors",
      "arxiv": "arXiv:1207.0580"
    },
    {
      "citation_id": "25",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "27",
      "title": "The regression analysis of binary sequences",
      "authors": [
        "David R Cox"
      ],
      "year": "1958",
      "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology"
    },
    {
      "citation_id": "28",
      "title": "Are all losses created equal: A neural collapse perspective",
      "authors": [
        "Jinxin Zhou",
        "Chong You",
        "Xiao Li",
        "Kangning Liu",
        "Sheng Liu",
        "Qing Qu",
        "Zhihui Zhu"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations, ICLR 2019"
    },
    {
      "citation_id": "30",
      "title": "Ensemble methods in machine learning",
      "authors": [
        "G Thomas",
        "Dietterich"
      ],
      "year": "2000",
      "venue": "International workshop on multiple classifier systems"
    },
    {
      "citation_id": "31",
      "title": "Performance of hybrid quantumclassical variational heuristics for combinatorial optimization",
      "authors": [
        "Giacomo Nannicini"
      ],
      "year": "2019",
      "venue": "Physical Review E"
    }
  ]
}