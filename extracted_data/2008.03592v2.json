{
  "paper_id": "2008.03592v2",
  "title": "Speech Driven Talking Face Generation From A Single Image And An Emotion Condition",
  "published": "2020-08-08T20:46:31Z",
  "authors": [
    "Sefik Emre Eskimez",
    "You Zhang",
    "Zhiyao Duan"
  ],
  "keywords": [
    "Talking face generation",
    "emotion",
    "audiovisual",
    "multimodal"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Visual emotion expression plays an important role in audiovisual speech communication. In this work, we propose a novel approach to rendering visual emotion expression in speechdriven talking face generation. Specifically, we design an end-toend talking face generation system that takes a speech utterance, a single face image, and a categorical emotion label as input to render a talking face video synchronized with the speech and expressing the conditioned emotion. Objective evaluation on image quality, audiovisual synchronization, and visual emotion expression shows that the proposed system outperforms a stateof-the-art baseline system. Subjective evaluation of visual emotion expression and video realness also demonstrates the superiority of the proposed system. Furthermore, we conduct a human emotion recognition pilot study using generated videos with mismatched emotions among the audio and visual modalities. Results show that humans respond to the visual modality more significantly than the audio modality on this task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH communication does not solely depend on the acoustic signal. Visual cues, when present, also play a vital role. The presence of visual cues improves speech comprehension  [1] ,  [2] ,  [3] ,  [4]  in noisy environments and for the hardof-hearing population. Consequently, researchers developed systems that can automatically generate talking faces from the speech in order to provide visual cues when they are not available  [5] ,  [6] ,  [7] ,  [8] ,  [9] ,  [10] ,  [11] ,  [12] . These systems can increase the accessibility of abundantly available audio-only resources for the hearing impaired population and can also increase the quality of human-computer interactions  [13] ,  [14] . They also have broad applications in entertainment, education, and healthcare.\n\nDuring speech communication, emotion has a direct impact on the transmitted message and can change the meaning drastically  [15] . Studies have shown that predicting emotions purely from speech audio is quite difficult for untrained people  [16]  and that we heavily rely on visual cues in emotion interpretation  [17] . Therefore, to make the visual rendering more realistic and to improve speech communication, it is important for automatic talking face generation systems to render visual emotion expressions.\n\nOne approach to emotional talking face generation is to first estimate the expressed emotions from the speech utterance and then render them in the generated talking faces. This approach, however, is limited by the speech emotion recognition accuracy and does not allow independent control of emotion expression in the visual rendering. In this work, we take a different approach: we ignore emotions expressed in the speech audio and condition the talking face generation on an independent emotion variable. This approach provides direct and more flexible control of visual emotion expression and can enable more personalized applications in entertainment, education, and interactive assistive devices. It also provides a powerful tool for behavioral psychologists to conduct emotionrelevant experiments that were not possible before. For example, one can investigate how humans respond to and interact with their conversational partners' emotional expressions by manipulating these emotions in audio and visual modalities independently.\n\nIn this work, we propose the first neural network system that generates emotional talking faces from speech conditioned on categorical emotions. The network takes a speech utterance, a reference face image, and a categorical emotion condition as inputs then generates a talking face that is synchronized with the input speech and contains emotional expressions. Our main contributions are as follows:\n\n• We propose a new talking face generation method that can be conditioned on categorical emotions. • We propose an emotion discriminative loss that classifies rendered visual emotions. • We conduct a pilot study on human emotion perception using talking face videos with mismatched emotions among the audio and visual modalities. The rest of the paper is organized as follows: We first present related work on talking faces in Section II. We then describe the proposed method and objective functions in Section III. Then, we present experimentation details, the objective evaluations, and Amazon Mechanical Turk (AMT) subjective evaluations in Section IV. Finally, we conclude the paper in Section V. Our source code is publicly available 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotion Models",
      "text": "In affective computing, researchers leverage emotion models in order to develop automatic systems that can detect emotions. The most utilized emotion models for automatic systems are 1) categorical models 2) dimensional models. Readers are referred to  [18] ,  [19] ,  [20] ,  [21] ,  [22]  for more comprehensive coverage of emotion models.\n\nCategorical emotion models assume there are a small number of emotions that are hard-wired to the human brain  [23] ,  [24] . Ekman's model suggested six basic emotion categories: anger, disgust, fear, happiness, neutral, and sadness.\n\nDimensional models argue that emotions are correlated, and each emotion category can be represented with a combination of values from emotional dimensions  [25] ,  [26] ,  [27] ,  [28] ,  [21] ,  [22] . The most famous example is the arousal-valance (AV) dimensions, where each emotion is represented with an arousal axis that determines if the emotion is active or passive and with a valance dimension that determines if the emotion is positive or negative. Another example is the hourglass of emotions  [21] ,  [22] . These models allow a more precise representation of emotions compared to the categorical models.\n\nIt should be noted that the selection of an emotion model for an affective computing task highly depends on the availability of the labels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Multimodal Emotion Analysis",
      "text": "In this section, we cover some of the key trends in multimodal emotion analysis. Cambria et al.  [29]  stated that affective computing and sentiment analysis is an interdisciplinary effort in combining traditional psychological emotion research with machine learning. There are many works on multimodal sentiment analysis  [30] ,  [31] ,  [32] ,  [33] ,  [34] ,  [35] ,  [36] ,  [37] ,  [38] .\n\nChaturvedi et al.  [34]  proposed a fuzzy sentiment classifier to predict mixed sentiment. Ma et al.  [39]  summarized empathetic dialogue systems, identifying that most of the systems focus on emotion-expressiveness or emotion-awareness. In emotion-expressive systems, emotion labels are employed for the loss calculation. In emotion-aware systems, emotion labels are taken as additional input, which is more similar to our approach. Another important line of research is sentic blending  [30] ,  [36] ,  [38] . The conceptual and emotional information related to natural language can be defined as semantic and sentic, respectively. The key idea is to fuse many single modality systems that have different time scales and output labels.\n\nOur emotion study fits well with this research line but is novel since our generated videos with mismatched emotion provide more insights on which modality humans rely on.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Emotional Talking Face Generation",
      "text": "The automatic generation of talking faces from the speech is drawing increasing attention from researchers in recent years. One approach is to first convert the speech input to face landmarks  [6] ,  [40] ,  [9] ,  [41] ,  [42] ,  [10] ,  [5] ,  [12]  and then estimate video frames using the predicted landmarks. In Suwajanakorn et al.'s two-stage system  [40] , a long shortterm memory (LSTM) network first predicts the principal component analysis (PCA) coefficients of face landmarks from speech features, and then retrieves candidate frames from the dataset according to the PCA coefficients, stitching them together. However, this system works only for a single speaker. Another two-stage system was proposed by Chen et al.  [5] . The system first predicts 68 face landmarks from speech using an LSTM-based network  [7] , and then predicts a few talking face images from the conditioned image and the face landmarks. They also employ a discriminator network to improve image quality. In another work, Egor et al.  [43]  proposed a style-based landmark-to-image conversion method using generative adversarial networks (GANs) with a few shots of the target face. This method, however, lacks landmark adaptation methods to solve personality mismatch issues.\n\nSome researchers designed systems that directly map speech features to video frames. Features extracted from the speech often include the Mel-frequency cepstral coefficients (MFCCs), energy, and the first-and second-order temporal derivatives of these features. Gutierrez et al.  [44]  proposed an integral system that employs the k nearest-neighbor (KNN) algorithm to map the speech dynamics to video frames. The KNN procedure requires a lot of memory and has a long search time, which leads to impracticality in real applications. Some approaches that model the conversion from speech features to the movement of articulators  [45] ,  [46] ,  [47] , but they only focus on specific regions of the face, thus generating less natural or expressive animation. Chung et al.  [6]  proposed a convolutional neural network (CNN) that takes as input a face image and speech features and generates a talking face video. The generated video is then sharpened by another CNN, which is trained on pairs of artificially blurred images and their clear originals. Chen et al.  [48]  proposed another method that predicts video frames of the lip region from speech features and a conditioned lip image. They introduced a GAN loss in addition to the reconstruction loss to sharpen the generated overly smooth video frames. However, this method is limited to only generating the lip region instead of the entire talking face. Zhou et al.  [12]  proposed a GAN-based method that models the whole face and introduced a temporal-GAN loss in addition to the reconstruction loss to improve the temporal dependency across frames. Song et al.  [9]  proposed another method that generates talking faces by using a conditional recurrent adversarial network to improve the realness. Yu et al.  [11]  adopted optical flow and a self-attention mechanism to capture adjacent and long-range temporal dependencies across video frames.\n\nIn addition to the above-mentioned two-stage or speechfeature-driven approaches, there are also end-to-end systems that generate talking faces directly from a conditioned image and the speech signal. Vougioukas et al.  [42]  proposed a temporal-GAN method to generate more realistic image sequences. They further improved their methods with three discriminators  [10]  that focus on improving the realness of video frames, the continuity between generated frames, and the synchronization between audio and visual data. Eskimez et al.  [49]  proposed an end-to-end talking face generation system that is robust to noisy speech input. The system contains a frame discriminator to improve image quality and a pair discriminator to improve lip-speech synchronization. They proposed a mouth region mask (MRM) to further improve the lip-speech synchronization and showed that it leads to better alignment than the baselines.\n\nRegarding emotional talking face generation, existing work is somewhat limited. Cosatto et al.  [50]  sample facial details from a database and then project to a 3D head model to allow realistic expressions. However, the generated emotional expressions focus more on the upper part of the face and lack variation for long animations. Karras et al.  [41]  adopted an end-to-end network to learn a latent representation of emotion states and use the latent code as a control to generate 3D mesh animations. This method effectively discovers emotion variations in the data, but the learned emotion states are difficult to interpret and do not model facial features such as wrinkled eyes and head motion to generate facial expressions. Sadoughi et al.  [51]  extended the conditional-GAN-based model to take the target emotion as an input, but this method is limited to generating the lip area instead of the whole face.\n\nRecently, Fang et al.  [52]  proposed a talking face generation system that takes audio and an image as input. This system enforces the generated videos to convey the emotion contained within the speech input. This is different from our method, which allows independent control of the emotion of the generated visual signal from that of the audio input. Also, their generated videos contain a high amount of visual artifacts (e.g., ambiguity, pixel jittering, face deformation) that render them unrealistic.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Multimodal Human Emotion Perception",
      "text": "Emotion perception from auditory and visual stimuli has been examined in recent years. Existing work  [53] ,  [54] ,  [55] ,  [56] ,  [57]  concludes that different modalities complement each other and that there are also intermodal effects. Cowie  [56]  showed that perception is sensitive to stimuli from multiple modalities in data from both simulated and natural interactions. Jessen et al.  [55]  suggested that emotional visual content yields a more reliable prediction of auditory information. Schirmer et al.  [53]  explored modalities in terms of neural responses and showed that each modality provides a distinct insight, and that multimodal perception converges for holistic emotion recognition.\n\nMost of the existing work was focused on emotionally congruent stimuli from these two modalities; little work examined incongruent stimuli. Tsiourti et al.  [58]  investigated human responses to emotions expressed by the body and voice of humanoid robots, showing that cross-modal incongruency decreased emotion recognition accuracy. Piwek et al.  [59]  found that subjects weighted visual cues higher in emotion judgments when presented emotionally incongruent audiovisual clips with happy or angry emotion. However, the visual content was conveyed by point-light displays instead of natural images.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "Instead of inferring emotion from the input speech  [41] ,  [51] , in this work, we propose to use emotions as an input condition to our system. The motivation is to decouple the speech and emotion conditions. This allows us to manipulate emotions during the generation of face videos. Figure  1  shows an overview of the system, which employs the GAN framework. Our generator network architecture is built based on our previous work  [49] , with a modification to accept the emotion condition input. For the discriminator networks, we use one discriminator to distinguish between emotions expressed in videos, and another discriminator to distinguish between the real and generated video frames.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Generator",
      "text": "The generator network contains the following sub-networks: speech, image, noise, and emotion encoders, and a video decoder.\n\n1) Speech Encoder: The speech encoder processes the input speech waveform and outputs a speech embedding. It follows the original implementation of  [49]  without any modification. It contains five convolutional layers with 1-D kernels operating in the time domain. The number of filters, filter sizes, and strides for these layers are as follows:  (64, 63, 4) , (128, 31, 4), (256, 17, 2), (512, 9, 2), (16, 1, 1), respectively. Each convolutional layer is followed by a LeakyReLU activation with a 0.2 slope. Since our network accepts 8 kHz speech signals, our speech encoder outputs 125 feature vectors per 1 second of speech. We add a context layer after these five convolutional layers to concatenate the past and future speech features. The context layer reduces the 125 time-steps to 25 time-steps by passing only every fifth frame to the next layer. Therefore, our generated videos are in 25 frames-perseconds (FPS). The output of the context layer is fed to a fully connected layer, followed by two LSTM layers, which output the speech embedding sequence.\n\n2) Image Encoder: The image encoder computes an image embedding from the input condition face image. The architecture follows the original implementation without any modification  [49] . It contains six layers of 2-D convolutional layers with the following number of filters, kernel sizes, and down-sampling factors: (64, 3, 2), (128, 3, 2), (256, 3, 2), (512, 3, 2), (512, 3, 2), (512, 4, 1), respectively. A LeakyReLU activation with a 0.2 slope follows each convolutional layer. Note that nearest-neighbor interpolation is used for downsampling rather than using strides. This eliminates the artifacts in the generated images. The final image embeddings and intermediate representations are all passed to the video decoder using U-Net style skip connections  [60] .\n\n3) Emotion Encoder: The emotion label is first encoded as a one-hot vector and fed into the emotion encoder. The emotion encoder uses a two-layer fully connected (FC) neural network to project the one-hot vector to an emotion embedding. This embedding is replicated for each time step. Again, we use a LeakyReLU activation with a 0.2 slope after every FC layer.\n\n4) Noise Encoder: For each frame of the video, we generate a noise vector drawn from the standard Gaussian distribution. A single-layer LSTM processes this sequence of noise vectors and outputs the noise embedding. This module aims to model the head movements that are not correlated with speech, image, and emotion.\n\n5) Video Decoder: We modify the video decoder described in  [49]  to accept the additional emotion embedding. We concatenate the speech, image, noise, and emotion embeddings and feed them into the decoder. For each time step, the decoder",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Frame Discriminator",
      "text": "The frame discriminator aims to improve the image quality of the generated video and to keep the target identity consistent throughout the video. First, we repeat the target image for the number of frames in the input video and concatenate them together. Then, each frame is processed by five layers of 2-D convolutional layers. The number of filters, kernel sizes, and strides of these convolutional layers are as follows: (64, 3, 2), (128, 3, 2), (256, 3, 2), (512, 3, 2), (512, 3, 2), respectively. The output is then flattened and fed into a two-layer FC network, which classifies the frame as fake or real. Each layer is followed by a LeakyReLU activation with a 0.2 slope except for the last layer, where we do not use an activation, since our system employs Wasserstein GAN with a gradient penalty  [61] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Emotion Discriminator",
      "text": "The emotion discriminator is essentially a video-based emotion classifier, with the inclusion of an additional class for fake videos. It aims to improve the emotional expression generated by our network. The first part of the network follows the same architecture as the frame discriminator: five layers of 2-D convolutional layer followed by two FC layers. We process each frame of the video and feed the resulting sequence into an LSTM layer. The last time step of the output of the LSTM layer is fed into an FC layer that outputs probabilities of the seven classes: six emotions (anger, disgust, fear, happiness, neutral, and sadness) plus the fake class as in  [62] . When we take a training step for the discriminator, we calculate the sparse categorical cross-entropy loss using the emotion label for the real video and the fake label for the generated video. When updating the generator, we calculate the sparse categorical cross-entropy loss using the emotion label we used for generating the video.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Objective Functions",
      "text": "Our system employs multiple objective functions that focus on different aspects of the generated videos: an MRM loss proposed in  [49]  to improve mouth-audio synchronization, a perceptual loss to improve image quality, a frame GAN loss for image quality, and an emotion GAN loss for emotion expression.\n\n1) Mouth Region Mask (MRM) Loss: The MRM loss is a weighted L1 reconstruction loss between the generated and ground-truth videos around the mouth region. It uses a 2D Gaussian centered at the mean position of mouth coordinates as the weights. The intuition of MRM is to manually drive the attention of the network to the mouth region to improve the mouth-audio synchronization.\n\n2) Perceptual Loss: We employ a pre-trained VGG-19 network  [63]  and calculate intermediate features of the following layers from both the generated and ground-truth videos: 4, 9, 18, 27, and 36. Then, a mean-squared loss between these intermediate features is calculated as the perceptual loss to improve image quality.\n\n3) Frame Discriminator Loss: To further improve the image quality, especially the sharpness, we use a frame GAN loss calculated by the frame discriminator. Instead of the vanilla GAN loss, we use Wasserstein GAN for more stable training.\n\n4) Emotion Discriminator Loss: To ensure emotion expression in generated videos, we use an emotion GAN loss calculated by the emotion discriminator, which is a categorical cross-entropy loss using six emotion classes plus a \"fake\" class, similar to  [62] . In a vanilla GAN discriminator, samples are only classified as real or fake, rather than choosing between multiple emotion classes; if the generator only generates samples from a single emotion class all the time, the vanilla discriminator would still classify them as real. The proposed discriminator, on the other hand, incorporates multi-class classification losses and mitigates this issue of mode collapse for a multi-class generation.\n\nThe full objective function for the generator step is as follows:\n\nwhere J GEN is the generator loss, L M RM 1 is the MRM loss, L P erceptual 2 is the perceptual loss, J F D is the frame GAN loss, J ED is the emotion GAN loss, and α, β, γ, δ are the respective weights of each component.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we describe the data used in experiments, the hyper-parameters of the neural networks, and the objective and subjective evaluation procedure. We choose the temporal GAN approach described in  [10]  as our baseline since it is the closest to our method. We use the pre-trained model and inference code provided by the authors to generate baseline videos. Although it cannot control the emotions through a conditioned input, it can generate emotional expressions that are inferred from the speech.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Dataset",
      "text": "We used the Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D) dataset  [64] . It contains video clips of 91 actors (48 male and 43 female) expressing six categorical emotions: anger, disgust, fear, happiness, neutral, and sadness. The age range of the actors is between 20 to 74. Each video clip shows one actor speaking a sentence from a set of 12 sentences and simulating one of the emotion categories. The image resolution of the provided videos is 480x360, and the sampling rate is 30 frames per second (FPS). The audio is sampled at 44.1 kHz. We downsampled the video to 25 FPS and the audio to 8 kHz. We followed the same train (70%), validation (15%), and test (15%) splits as  [10] . We used the same files for these splits to ensure a fair comparison. During testing, the speech utterance, the conditioned emotion, and the conditioned image input to the generator network for each generation are all from the same ground-truth video, where the condition image is the first frame of the video.\n\nAs the same actor's face in different videos may be at different spatial locations, for easing the training, we need to align them across videos. For alignment, first, we choose a template image for the actor where the face is symmetrical. We extracted face landmarks from this template image as the template landmarks. Then, for each video of the actor, we estimated the similarity transform parameters between the template landmarks and extracted landmarks of the first frame using three points: the temporal mean points of the left eye, the right eye, and the nose. Note that we only took the first frame of each video to estimate the transformation, and used it to align the remaining frames to the template image. In this way, the faces in the resulting videos start from the same spatial location but can wander off to different parts of the scene. This allows us to model the natural head movements in addition to facial expressions.\n\nDuring training, we randomly augmented the data using the Albumentations library  [65]  to improve the generalization capability of our network. The data augmentation includes randomly changing brightness, contrast, gamma, hue, saturation, and value. In addition, our algorithm includes contrast limited adaptive histogram equalization, adding random Gaussian noise to the image, and shuffling the channels, and shifting RGB values for each channel.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "To initialize our network, we trained it from scratch using only MRM and perceptual losses for 100k iterations. Then, we trained it for another 100k iteration using the full objective function. We used Adam optimizer for all networks with β 1 = 0.5, β 2 = 0.99. The learning rate for the generator was 1e-4 during the initialization and 1e-5 during the GAN training. Both discriminators' learning rates were 1e-4. The constants α, β, γ, andδ mentioned in Section III-D were 100, 1, 0.01, and 0.001, respectively. The weight for the gradient penalty when training the frame discriminator was 10. All images were normalized between the -1 to 1 value range. During initialization, the mini-batch size was set to 8, and during GAN training, it was set to 4. The number of frames per sample was set to 32. The training took approximately one week using a GTX 1080 TI GPU. For the baseline method, we use the pre-trained model (trained with the CREMA-D dataset) provided by the authors.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Objective Evaluation",
      "text": "We evaluated the image quality of the generated videos using Peak SNR (PSNR) and Structural Similarity (SSIM)  [66]  between the generated video frames and the ground-truth video frames. To measure the audiovisual synchronization, we used the normalized landmarks distance (NLMD)  [49]  between landmarks extracted from the generated and ground-truth video frames. The baseline method generates 96x128 images, while our method yields 128x128 images. In other words, the foreground/background ratio differs in the generated videos. To ensure a fair comparison, we aligned the ground-truth, baseline, and proposed videos into a template image and cropped them into the same size using similarity transformation. Figure  2  shows the aligned videos, and Figure  3  shows example videos generated from the same condition image and speech, but different emotion conditions. Table  I  shows the objective evaluation results of the baseline and our proposed methods. It can be seen that our method outperforms the baseline on all of the three metrics. We believe that perceptual loss is responsible for the improvement in image quality (PSNR and SSIM). For audiovisual synchronization (NLMD), even though our method does not use a discriminator paired with a synchronization loss as in  [10] , the improvement is as high as 8.9%, showing the effectiveness of the MRM loss. 1) Video-based Emotion Classification: In order to validate the emotional expression in the generated videos, we trained a video-based emotion recognition network using the CREMA-D train set. This network uses the same architecture as the emotion discriminator in Figure  1 . We then classified the emotions within the ground-truth videos and our generated videos of the test set. The results are shown in Table  II . The 6-class emotion classification accuracy on the ground-truth videos is 62.71%, which is comparable with  [67] , suggesting the validity of the video-based emotion classifier. The accuracy and F1-Score on the generated videos are slightly higher, even though the classifier was not trained on generated videos. This suggests that emotions are well expressed, and slightly exaggerated perhaps, in the generated videos.\n\nWe further show the confusion matrices of these two classification results in Figure  4 . We observe similar patterns. First, they both have a strong diagonal. In particular, happiness is the easiest emotion to classify. This may be because happiness often contains smiling that is distinctive from other facial expressions, allowing the classifier as well as our generation system to capture it clearly. Second, some emotions are commonly confused with each other, such as fear and sadness. On the other hand, there are also differences in these confusion matrices. In particular, in the ground-truth videos, both fear and sadness are often misclassified as disgust, while in the generated videos, no other emotions are misclassified as disgust. Overall, the similarities outweigh the differences, showing that the emotional expressions in the generated videos resemble those of the ground truth. 2  D. Subjective Evaluation 1) Research Questions: We design our subjective evaluation to investigate the following research questions: 1) Is our model effective in expressing emotions for video rendering? 2) How real are the generated videos of our model? 3) Which modality do people primarily rely on to perceive emotions? We conduct our evaluation on AMT.\n\n2) Experimental Setup: Our AMT study consists of two Human Intelligence Tasks (HIT). For the first task, we randomly presented subjects generated and ground-truth videos and asked them to rate the realness and provide suggestions for making the videos more real. We also asked subjects to assign an emotion label to each video. This task aimed to answer the first and second research questions. For the second task, we generated videos that contain mismatched emotions in the audio and visual modalities. We asked subjects to assign Fig.  3 : Frames of different talking face videos generated (in different rows) using the same face image (the first column) and speech utterance but different emotion conditions (from top to bottom: anger, disgust, fear, happiness, neutral, and sadness). One frame is shown for every 0.2 seconds. one or two emotion labels to these videos. By doing so, we aimed to answer the third research question.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Task 1 -Emotion Classification And Realness Evaluation.",
      "text": "In the first task, we pooled videos taken from ground-truth videos of the test set of the CREMA-D dataset and generated videos from the baseline and our models. For the baseline system, each video was generated from the speech recording and the first frame of the ground-truth video, while for the proposed system, each video was generated from the speech recording and the first frame of the ground-truth video, as well as the ground-truth emotion condition. We downsampled the ground-truth and baseline videos to 25 FPS to make them consistent with our generated ones. As described earlier, our method generates talking faces in 128 × 128 image size, while the baseline method generates videos in 96×128 image size. If we were to set the ground-truth videos to any of the two sizes, the subjects might be negatively biased toward the generated videos with the other size. To avoid this potential problem, we aligned the ground-truth videos with template faces of both sizes and obtained two sets of ground truth videos.\n\nWe released six batches of videos in total. For each batch, we randomly selected five videos from the two sets of groundtruth videos, five from the baseline videos, and five from our generated videos. One video from each category was repeated to check the consistency of the subjects' answers. Therefore, there were in total 18 videos in each batch. The videos in each batch were randomly shuffled. Across all of the six batches, the total number of videos for each emotion category was equal.\n\nWe recruited a total of 60 valid subjects (i.e., 10 for each batch) from AMT. The subjects were required to be located in the United States and to have a lifetime HIT approval rate higher than 95%. To encourage the subjects to treat the experiments more seriously, we made a bonus payment based on the subject's performance. Subjects were informed about the bonus payment before they started the experiments.\n\nSubjects were informed that some of the 18 videos were recordings of real people, while other videos were rendered by artificial intelligence (AI) based on a single face image of one person and a speech recording of another person. Before presenting the 18 videos, we also presented two example videos of real recordings for each emotion, each in the two image sizes (128×128 and 96×128), to familiarize the subjects with these emotional expressions. These example emotions were ordered in alphabetical order.\n\nWe then asked subjects the following three questions for each video: 1) Which emotion is primarily expressed by the person? This question is a multiple-choice question, and the subjects were asked to select one from the six emotions. the subjects could choose more than one aspect. The choices are None, Image quality, Lip synchronization, Head movement, and Other.\n\nAfter receiving a survey, we checked its completeness and the consistency of answers to the nine questions of the three repeated videos. We rejected a total of 11 incomplete surveys and those that did not meet the consistency requirement. We then recruited other subjects until we collected 60 valid surveys. For our answer consistency requirement, an answer was considered inconsistent from the previous answer, if 1) the emotion classification was different for the first question, 2) the realness rating differed more than one level for the second question, or 3) the aspect selection differed for more than one options. Among the nine repeated questions, if more than five answers were inconsistent, then the entire survey was rejected.\n\nTask 2 -Emotion Perception of Videos with Mismatched Emotions. As described in Section II-B, little work on human emotion perception used emotionally incongruent stimuli between the audio and visual modalities, and among these works, none used videos of humans as stimuli. Our emotional talking face generation system makes it possible to investigate human emotion perception from emotionally incongruent stimuli present in human speaking videos.\n\nIn the second task, we presented generated videos from our proposed system based on a face image, a speech recording, and an emotion condition. Both the face image and the speech recording were taken from a ground-truth video in the test set of the CREMA-D dataset. As a result, the speech recording conveyed a certain emotion. The emotion condition input, however, was not necessarily the same as the speech emotion, allowing for the possibility of generating videos with mismatched emotions between the audio and visual modalities. As there are six emotions in the dataset, there were 36 emotion pairs and 30 of them were mismatched. We generated 2 videos for each of the 36 pairs, shuffled them, and split them evenly into six batches. We also repeated two videos in each batch to check the answer consistency. Therefore, there were a total of 14 videos in each batch.\n\nWe recruited a total of 60 subjects (i.e., 10 for each batch) from AMT, with the same requirements as Task 1. We rejected a total of 4 incomplete surveys and those who had more than two inconsistent answers among the four repeated questions and recruited other subjects until we collected 60 valid surveys. The participants who completed Task 1 could not see this task from the AMT platform. The same bonus mechanism in Task 1 was applied to Task 2. In the survey, subjects were notified that all of the videos were AI rendered. Before presenting the generated videos, the subjects were also presented two example ground-truth videos for each emotion, only in the image size of 128 × 128, to familiarize them with the emotions. They were asked the following two multiplechoice questions for each video: 1) Which is the primary emotion expressed by the person? The subjects could select one of the six emotions. 2) Which is the secondary emotion expressed by the person? The subjects could select one of the six emotions and a None option if they only perceived the primary emotion.\n\n3) Experimental Results: Task 1 -Emotion Classification. The confusion matrices of subjective emotion classification for ground-truth videos, baseline generated videos, and our generated videos are shown in Figure  5 . Our videos yield a more diagonal confusion matrix compared with the baseline videos and result in patterns similar to those produced from the ground-truth videos. Specifically, subjects are more likely to classify the emotions in the baseline videos as neutral, while this happens much less frequently for our generated videos. This shows the power of the emotion condition input that our method utilizes. The overall classification accuracy is 59.2% (ground-truth), 28.9% (baseline), 55.3% (ours), respectively, demonstrating the efficacy in expressing emotions of our proposed emotional talking face generation system. It must be noted that the baseline system infers emotion from the speech input instead of taking the emotion condition as input. As emotion recognition from the speech is itself a challenging task, errors in this stage naturally influence visual emotion expression in the generated videos. Therefore, poor performance from the baseline system is expected. Interestingly, the 59.2% human emotion classification accuracy on ground-truth videos is slightly lower than that of our emotional classifier in Section IV-C1, showing the challenge of visual speech emotion classification for humans. This observation is similar to a speech emotion classification observation in  [16] .\n\nTask 1 -Realness Evaluation. For the realness question, the five options are mapped to a scale from 1 to 5, where \"definitely real\" corresponds to 5 and \"definitely unreal\" corresponds to 1. The result is shown in Figure  6 . The average rating across all videos and subjects is 3.94, 3.71, and 3.81 for ground-truth, baseline, and our videos, respectively. This suggests that our generated videos are slightly more realistic than the baseline videos, yet they are still not as realistic as the ground-truth videos. Interestingly, even the ground-truth videos only received an average rating close to 4 (somewhat real). We think that this might be due to the relatively lower image resolution than what the subjects typically see in their daily life. This might also because the generated videos (especially OURS) are quite realistic, lowering the subjects' confidence in rating the ground-truth videos. A Wilcoxon signed-rank test  [68]  shows that the median difference between our ratings and the baseline ratings is statistically significantly greater than zero, at the significance level of 0.05 (p = 0.048).\n\nFigure  7  shows the histograms of aspects suggested by the subjects to improve the realness of the videos. Consistent with the realness question, ground-truth videos received the most \"none\" votes, while our generated videos received the second most and the baseline received the least . The total count of votes for the four aspects to improve (\"image quality\", \"lip synchronization\", \"head movement\", \"other\") is 299 (groundtruth), 337 (baseline) and 325 (ours), respectively. Among the detailed aspects, the baseline videos received the most votes on \"image quality\" and \"lip synchronization\"; but it also received the least votes on \"head movement\" . This might be due to the fact that the baseline method is trained with 30 FPS videos and adopted a sequence discriminator to render head movements. On the other hand, our generated videos performed similarly to ground-truth ones on \"lip synchronization\" and \"head movement\", suggesting the effectiveness of our proposed MRM loss. Nevertheless, the \"image quality\" of our generated videos is considered to need more improvement than the ground-truth videos.  Task 2-Emotion Perception of Videos with Mismatched Emotions. In Task 2, subjects were asked the primary and secondary (if any) emotions they perceived from each video generated by our system, to investigate which modality people primarily rely on for emotion recognition. Overall, 426 of the 840 videos received two emotion labels. We first compared the primary emotion label with the visual emotion (i.e., the condition emotion when generating the video) and the audio emotion, respectively. The confusion matrices are shown in Figure  8 . Overall, 35.2% of the primary emotion labels match with the visual emotion, while only 25.1% of them match with audio emotion. If we only consider videos with mismatched emotions, these numbers become 31.4% and 19.6%, respectively. This suggests that the subjects relied on the visual modality much more heavily than the audio modality for emotion perception. Among the six emotions, happiness and disgust seem to be the easiest to perceive from the visual modality, while anger and fear are the most difficult.\n\nWe then considered both primary and secondary emotions when comparing them with the audio and visual emotions. In this case, 44.9% of labeled emotions can be matched to the visual emotion, while 33.8% can be matched to the audio emotion. Similarly, if we only consider videos with mismatched emotions, these numbers become 41.1% and 28.2%. Again, this shows that the visual modality has a much greater effect than the audio modality on audiovisual speech emotion recognition for humans.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusions",
      "text": "In this work, we proposed a novel emotional talking face generation system that is conditioned on speech, reference image, and categorical emotion inputs. We evaluated our network against the ground-truth videos and a baseline system  [10]  and validated that our method can generate emotional expressions effectively. In addition, we conducted a subjective study on AMT, showing that our method yields close performance to the ground-truth videos in terms of realness and emotion classification. Furthermore, we also conducted a pilot study on human emotion perception from audiovisual speech with mismatched emotions expressed in the audio and visual modalities, showing that visual perception is more dominant than auditory perception. For future work, we plan to improve the image quality of generated videos. We also plan to extend this work to 3D animation and rendering.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows an overview of the system, which employs the GAN",
      "page": 3
    },
    {
      "caption": "Figure 1: Overview of the proposed neural network system. It accepts a reference image, a speech waveform, a random vector",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the aligned videos, and Figure 3 shows example videos",
      "page": 6
    },
    {
      "caption": "Figure 2: Four examples comparing spatially aligned and cropped",
      "page": 6
    },
    {
      "caption": "Figure 1: We then classiﬁed the",
      "page": 6
    },
    {
      "caption": "Figure 4: We observe similar patterns. First,",
      "page": 6
    },
    {
      "caption": "Figure 3: Frames of different talking face videos generated (in different rows) using the same face image (the ﬁrst column) and",
      "page": 7
    },
    {
      "caption": "Figure 4: Confusion matrices of video-based emotion classiﬁca-",
      "page": 8
    },
    {
      "caption": "Figure 5: Our videos yield a",
      "page": 8
    },
    {
      "caption": "Figure 5: Confusion matrices of human emotion classiﬁcation in Task 1 on ground-truth videos (left), baseline generated videos",
      "page": 9
    },
    {
      "caption": "Figure 6: The average",
      "page": 9
    },
    {
      "caption": "Figure 7: shows the histograms of aspects suggested by the",
      "page": 9
    },
    {
      "caption": "Figure 6: User ratings on the realness of ground-truth (GT),",
      "page": 9
    },
    {
      "caption": "Figure 7: Total count of chosen aspects for realness improvement",
      "page": 9
    },
    {
      "caption": "Figure 8: Overall, 35.2% of the primary emotion labels match",
      "page": 9
    },
    {
      "caption": "Figure 8: Confusion matrices of the primary emotion label that",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GT": "BL\nOURS"
        },
        {
          "GT": "BL\nOURS\nGT\nBL\nOURS\nGT\nBL\nOURS\nGT"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Bi-sensory articulation functions for normal hearing and sensorineural hearing loss patients",
      "authors": [
        "C Binnie"
      ],
      "year": "1973",
      "venue": "Journal of the Academy of Rehabilitative Audiology"
    },
    {
      "citation_id": "2",
      "title": "Auditory and auditory-visual intelligibility of speech in fluctuating maskers for normal-hearing and hearingimpaired listeners",
      "authors": [
        "J Bernstein",
        "K Grant"
      ],
      "year": "2009",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "3",
      "title": "The role of visual speech cues in reducing energetic and informational masking",
      "authors": [
        "K Helfer",
        "R Freyman"
      ],
      "year": "2005",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "4",
      "title": "Auditory selective attention is enhanced by a task-irrelevant temporally coherent visual stimulus in human listeners",
      "authors": [
        "R Maddox",
        "H Atilgan",
        "J Bizley",
        "A Lee"
      ],
      "year": "2015",
      "venue": "eLife"
    },
    {
      "citation_id": "5",
      "title": "Hierarchical crossmodal talking face generation with dynamic pixel-wise loss",
      "authors": [
        "L Chen",
        "R Maddox",
        "Z Duan",
        "C Xu"
      ],
      "year": "2019",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "6",
      "title": "You said that?",
      "authors": [
        "J Chung",
        "A Jamaludin",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "7",
      "title": "Generating talking face landmarks from speech",
      "authors": [
        "S Eskimez",
        "R Maddox",
        "C Xu",
        "Z Duan"
      ],
      "year": "2018",
      "venue": "International Conference on Latent Variable Analysis and Signal Separation"
    },
    {
      "citation_id": "8",
      "title": "You said that?: Synthesising talking faces from audio",
      "authors": [
        "A Jamaludin",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Talking face generation by conditional recurrent adversarial network",
      "authors": [
        "Y Song",
        "J Zhu",
        "D Li",
        "A Wang",
        "H Qi"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19",
      "doi": "10.24963/ijcai.2019/129"
    },
    {
      "citation_id": "10",
      "title": "Realistic speech-driven facial animation with gans",
      "authors": [
        "K Vougioukas",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Mining audio, text and visual information for talking face generation",
      "authors": [
        "L Yu",
        "J Yu",
        "Q Ling"
      ],
      "year": "2019",
      "venue": "International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "12",
      "title": "Talking face generation by adversarially disentangled audio-visual representation",
      "authors": [
        "H Zhou",
        "Y Liu",
        "Z Liu",
        "P Luo",
        "X Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Humanoid audio-visual avatar with emotive text-to-speech synthesis",
      "authors": [
        "H Tang",
        "Y Fu",
        "J Tu",
        "M Hasegawa-Johnson",
        "T Huang"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Real-time vision and speech driven avatars for multimedia applications",
      "authors": [
        "O Schreer",
        "R Englert",
        "P Eisert",
        "R Tanger"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "Transient voice changes associated with emotional stimuli",
      "authors": [
        "M Alpert",
        "R Kurtzberg",
        "A Friedhoff"
      ],
      "year": "1963",
      "venue": "Archives of General Psychiatry"
    },
    {
      "citation_id": "16",
      "title": "Emotion classification: how does an automated system compare to naive human coders",
      "authors": [
        "S Eskimez",
        "K Imade",
        "N Yang",
        "M Sturge-Apple",
        "Z Duan",
        "W Heinzelman"
      ],
      "year": "2016",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "The perceptual and cognitive role of visual and auditory channels in conveying emotional information",
      "authors": [
        "A Esposito"
      ],
      "year": "2009",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "18",
      "title": "Psychological models of emotion",
      "authors": [
        "K Scherer"
      ],
      "year": "2000",
      "venue": "The neuropsychology of emotion"
    },
    {
      "citation_id": "19",
      "title": "Emotion models: a review",
      "authors": [
        "S Ps",
        "G Mahalakshmi"
      ],
      "year": "2017",
      "venue": "International Journal of Control Theory and Applications"
    },
    {
      "citation_id": "20",
      "title": "Emotion modeling in social simulation: a survey",
      "authors": [
        "M Bourgais",
        "P Taillandier",
        "L Vercouter",
        "C Adam"
      ],
      "year": "2018",
      "venue": "Journal of Artificial Societies and Social Simulation"
    },
    {
      "citation_id": "21",
      "title": "The hourglass of emotions",
      "authors": [
        "E Cambria",
        "A Livingstone",
        "A Hussain"
      ],
      "year": "2012",
      "venue": "Cognitive behavioural systems"
    },
    {
      "citation_id": "22",
      "title": "The hourglass model revisited",
      "authors": [
        "Y Susanto",
        "A Livingstone",
        "B Ng",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "23",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "24",
      "title": "Emotion in the human face: Guidelines for research and an integration of findings",
      "authors": [
        "P Ekman",
        "W Friesen",
        "P Ellsworth"
      ],
      "year": "2013",
      "venue": "Emotion in the human face: Guidelines for research and an integration of findings"
    },
    {
      "citation_id": "25",
      "title": "An approach to environmental psychology",
      "authors": [
        "A Mehrabian",
        "J Russell"
      ],
      "year": "1974",
      "venue": "An approach to environmental psychology"
    },
    {
      "citation_id": "26",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "27",
      "title": "Neural correlates of processing valence and arousal in affective words",
      "authors": [
        "P Lewis",
        "H Critchley",
        "P Rotshtein",
        "R Dolan"
      ],
      "year": "2007",
      "venue": "Cerebral cortex"
    },
    {
      "citation_id": "28",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Affective computing and sentiment analysis",
      "authors": [
        "E Cambria",
        "D Das",
        "S Bandyopadhyay",
        "A Feraco"
      ],
      "year": "2017",
      "venue": "A practical guide to sentiment analysis"
    },
    {
      "citation_id": "30",
      "title": "Sentic blending: Scalable multimodal fusion for the continuous interpretation of semantics and sentics",
      "authors": [
        "E Cambria",
        "N Howard",
        "J Hsu",
        "A Hussain"
      ],
      "year": "2013",
      "venue": "2013 IEEE symposium on computational intelligence for human-like intelligence (CIHLI)"
    },
    {
      "citation_id": "31",
      "title": "Fusing audio, visual and textual clues for sentiment analysis from multimodal content",
      "authors": [
        "S Poria",
        "E Cambria",
        "N Howard",
        "G.-B Huang",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "32",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S.-F Chang"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "33",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "34",
      "title": "Fuzzy commonsense reasoning for multimodal sentiment analysis",
      "authors": [
        "I Chaturvedi",
        "R Satapathy",
        "S Cavallari",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "35",
      "title": "Multimodal sentiment analysis: A survey and comparison",
      "authors": [
        "R Kaur",
        "S Kautish"
      ],
      "year": "2019",
      "venue": "International Journal of Service Science, Management, Engineering, and Technology (IJSSMET)"
    },
    {
      "citation_id": "36",
      "title": "Ten years of sentic computing",
      "authors": [
        "Y Susanto",
        "E Cambria",
        "B Ng",
        "A Hussain"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "37",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "38",
      "title": "Sentiment analysis and topic recognition in video transcriptions",
      "authors": [
        "L Stappen",
        "A Baird",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "39",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Y Ma",
        "K Nguyen",
        "F Xing",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "40",
      "title": "Synthesizing obama: learning lip sync from audio",
      "authors": [
        "S Suwajanakorn",
        "S Seitz",
        "I Kemelmacher-Shlizerman"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "41",
      "title": "Audio-driven facial animation by joint end-to-end learning of pose and emotion",
      "authors": [
        "T Karras",
        "T Aila",
        "S Laine",
        "A Herva",
        "J Lehtinen"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "42",
      "title": "End-to-end speech-driven facial animation with temporal gans",
      "authors": [
        "K Vougioukas",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2018",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "43",
      "title": "Few-shot adversarial learning of realistic neural talking head models",
      "authors": [
        "E Zakharov",
        "A Shysheya",
        "E Burkov",
        "V Lempitsky"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Speech-driven facial animation with realistic dynamics",
      "authors": [
        "R Gutierrez-Osuna",
        "P Kakumanu",
        "A Esposito",
        "O Garcia",
        "A Bojórquez",
        "J Castillo",
        "I Rudomín"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "45",
      "title": "Realistic mouth-synching for speech-driven talking face using articulatory modelling",
      "authors": [
        "L Xie",
        "Z Liu"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Real-time continuous phoneme recognition system using class-dependent tied-mixture hmm with hbt structure for speechdriven lip-sync",
      "authors": [
        "J Park",
        "H Ko"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "47",
      "title": "Animating expressive faces across languages",
      "authors": [
        "A Verma",
        "L Subramaniam",
        "N Rajput",
        "C Neti",
        "T Faruquie"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "48",
      "title": "Lip movements generation at a glance",
      "authors": [
        "L Chen",
        "Z Li",
        "R Maddox",
        "Z Duan",
        "C Xu"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "49",
      "title": "End-to-end generation of talking faces from noisy speech",
      "authors": [
        "S Eskimez",
        "R Maddox",
        "C Xu",
        "Z Duan"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "50",
      "title": "Photo-realistic talking-heads from image samples",
      "authors": [
        "E Cosatto",
        "H Graf"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Speech-driven expressive talking lips with conditional sequential generative adversarial networks",
      "authors": [
        "N Sadoughi",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Facial expression gan for voice-driven face generation",
      "authors": [
        "Z Fang",
        "Z Liu",
        "T Liu",
        "C.-C Hung",
        "J Xiao",
        "G Feng"
      ],
      "year": "2021",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "53",
      "title": "Emotion perception from face, voice, and touch: comparisons and convergence",
      "authors": [
        "A Schirmer",
        "R Adolphs"
      ],
      "year": "2017",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "54",
      "title": "Multimodal databases of everyday emotion: Facing up to complexity",
      "authors": [
        "E Douglas-Cowie",
        "L Devillers",
        "J.-C Martin",
        "R Cowie",
        "S Savvidou",
        "S Abrilian",
        "C Cox"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "55",
      "title": "On the role of crossmodal prediction in audiovisual emotion perception",
      "authors": [
        "S Jessen",
        "S Kotz"
      ],
      "year": "2013",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "56",
      "title": "Perceiving emotion: towards a realistic understanding of the task",
      "authors": [
        "R Cowie"
      ],
      "year": "2009",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "57",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "C Busso",
        "Z Deng",
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "S Lee",
        "U Neumann",
        "S Narayanan"
      ],
      "year": "2004",
      "venue": "Proceedings of the 6th International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "58",
      "title": "Multimodal integration of emotional signals from voice, body, and context: Effects of (in) congruence on emotion recognition and attitudes towards robots",
      "authors": [
        "C Tsiourti",
        "A Weiss",
        "K Wac",
        "M Vincze"
      ],
      "year": "2019",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "59",
      "title": "Audiovisual integration of emotional signals from others' social interactions",
      "authors": [
        "L Piwek",
        "F Pollick",
        "K Petrini"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "60",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2015",
      "venue": "International Conference on Medical Image Computing and Computer-assisted Intervention"
    },
    {
      "citation_id": "61",
      "title": "Improved training of wasserstein gans",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "62",
      "title": "Bagan: Data augmentation with balancing gan",
      "authors": [
        "G Mariani",
        "F Scheidegger",
        "R Istrate",
        "C Bekas",
        "C Malossi"
      ],
      "year": "2018",
      "venue": "Bagan: Data augmentation with balancing gan",
      "arxiv": "arXiv:1803.09655"
    },
    {
      "citation_id": "63",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "64",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "Albumentations: Fast and flexible image augmentations",
      "authors": [
        "A Buslaev",
        "V Iglovikov",
        "E Khvedchenya",
        "A Parinov",
        "M Druzhinin",
        "A Kalinin"
      ],
      "year": "2020",
      "venue": "Information"
    },
    {
      "citation_id": "66",
      "title": "Image quality assessment: from error visibility to structural similarity",
      "authors": [
        "Z Wang",
        "A Bovik",
        "H Sheikh",
        "E Simoncelli"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "67",
      "title": "Emotion recognition system from speech and visual information based on convolutional neural networks",
      "authors": [
        "N.-C Ristea",
        "L Dut",
        "A Radoi"
      ],
      "year": "2019",
      "venue": "International Conference on Speech Technology and Human-Computer Dialogue (SpeD)"
    },
    {
      "citation_id": "68",
      "title": "He is currently a Ph.D. student in the Audio Information Research lab at the University of Rochester. His research interests lie in machine learning and its applications in speech and audio, such as audio-visual analysis, synthetic voice spoofing detection, spatial audio, etc. Zhiyao Duan (S'09, M'13) is an associate professor in Electrical and Computer Engineering, Computer Science and Data Science at the University of Rochester. He received his B.S. in Automation and M",
      "authors": [
        "E Cureton"
      ],
      "year": "1967",
      "venue": "respectively, and received his Ph.D. in Computer Science from Northwestern University in 2013"
    }
  ]
}