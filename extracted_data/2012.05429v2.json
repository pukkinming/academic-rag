{
  "paper_id": "2012.05429v2",
  "title": "Multi-Classifier Interactive Learning For Ambiguous Speech Emotion Recognition",
  "published": "2020-12-10T02:58:34Z",
  "authors": [
    "Ying Zhou",
    "Xuefeng Liang",
    "Yu Gu",
    "Yifei Yin",
    "Longshan Yao"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Interactive learning",
    "Multi-classifier approach"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, speech emotion recognition technology is of great significance in industrial applications such as call centers, social robots and health care. The combination of speech recognition and speech emotion recognition can improve the feedback efficiency and the quality of service. Thus, the speech emotion recognition has been attracted much attention in both industry and academic. Since emotions existing in an entire utterance may have varied probabilities, speech emotion is likely to be ambiguous, which poses great challenges to recognition tasks. However, previous studies commonly assigned a singlelabel or multi-label to each utterance in certain. Therefore, their algorithms result in low accuracies because of the inappropriate representation. Inspired by the optimally interacting theory, we address the ambiguous speech emotions by proposing a novel multi-classifier interactive learning (MCIL) method. In MCIL, multiple different classifiers first mimic several individuals, who have inconsistent cognitions of ambiguous emotions, and construct new ambiguous labels (the emotion probability distribution). Then, they are retrained with the new labels to interact with their cognitions. This procedure enables each classifier to learn better representations of ambiguous data from others, and further improves the recognition ability. The experiments on three benchmark corpora (MAS, IEMOCAP, and FAU-AIBO) demonstrate that MCIL does not only improve each classifier's performance, but also raises their recognition consistency from moderate to substantial.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "W ITH the rapid development of Artificial Intelligence, the technology of speech emotion recognition (SER) is becoming deeply involved into a wide bank of industrial applications, e.g. call center, health care, social robot, to name a few. The previous research reported that, in the daily communication of human-being, language accounts for 45%, of which text accounting for only 7% while emotional expression accounting for 38%. Thus, emotion plays an important role in human communications. A high-quality SER does not only make human machine interaction more naturally, but also improve the efficiency and effectiveness of industrial services. For instance, NTT DoCoMo enables the intelligent customer service system to detect whether customers express a negative emotion. If yes, the phone call will be switched to the human service. Sony AIBO pet robot is able to recognize a few speech emotions, and adjusts its personality and interactive behavior according to the owner's emotion. In addition, SER technology is being applied in tutorial systems to improve social interaction abilities of children with autism spectrum disorders.\n\nHowever, the performance of current SER technologies remains suboptimal due to the ambiguity of emotions. The psychology study  [1]  demonstrated that speech emotions were somewhat ambiguous, which was also confirmed by the research using a statistical model  [2] . From the perspective of machine learning, we found the classification boundaries between emotion categories are not clear as well, as shown in Fig.  1 .a. Nevertheless, the conventional studies  [3, 4]  often assumed that emotions could be distinguishable, hence, assigned a precise/single label to each of them, which may not represent speech emotions very well. Commonly, they resulted in low accuracies.\n\nRecently, a few studies tried to model the emotion ambiguity in their methods. For example, Fayek et al.  [5]  designed a soft-target label. Lotfian et al.  [6]  considered multiple emotions in one utterance as a multi-task problem. Ando et al.  [7]  used multi-label to represent the emotion ambiguity. Multi-label means each sample is assigned to a set of target labels, in which every label is certain, but emotions existing in utterances usually have varied probabilities. In addition, due to all these methods heavily relying on the statistics of experts' voting, they have limited generalization to be applied to the databases without voting information, such as Mandarin Affective Speech (MAS) dataset. More interesting, we have observed that SER methods performed inconsistently on these emotional categories  [7, 8] . The possible reason is that these methods are good at learning features of some specific emotions respectively. To our best knowledge, there is no one approach that can integrate the strengths of these methods into one.\n\nWhen faced with this complicated issue of inconsistent recognition of ambiguous data, human beings have a wise strategy against it. Bahrami et al.  [9]  found that interpersonal communication can improve a person's ability to disambiguate uncertain information, which is called the optimally interacting theory. Motivated by Bahrami's theory, we present a multiclassifier interactive learning (MCIL) method to address ambiguous data. Our goal is to identify an alternative that automatically constructs ambiguous labels (i.e. emotion probability distribution) instead of precise labels to ambiguous emotion data. In MCIL, multiple classifiers are firstly trained on a portion of ambiguous data using precise labels. Subsequently, they are used to vote for the other ambiguous data. Thus, the statistics of voting are used to construct ambiguous labels for retraining. Finally, these classifiers are retrained with ambiguous labels to interact and learn better information of ambiguous data, as shown in Fig.  1 .b, which mimics human interaction strategy.\n\nUnlike previous methods, MCIL integrates varied information learned by different classifiers. It does not only improve the performance of each classifier, but also results in more consistent recognition results among multiple classifiers. Moreover, the classifiers in MCIL are firstly trained by the precise labels given by the database, then construct the ambiguous labels. This strategy enhances the generalization ability of MCIL, and make it also work on the databases without the information of experts' voting.\n\nExtensive experiments demonstrate that our proposed MCIL outperforms the state-of-the-art methods on three benchmarks, i.e. 3.1% on MAS, 2.0% on IEMOCAP, and 1.58% on FAU-AIBO. The main contributions of this work are following:\n\n• We propose multi-classifier interactive learning (MCIL), which uses multiple different classifiers to mimic human interacting behavior to address the ambiguous speech emotion recognition. MCIL can integrate varied information learned by different classifiers. • MCIL firstly trains each classifier using precise labels given by the database, and then constructs the ambiguous labels. Thus, MCIL has a better generalization ability. • Experimental results demonstrate that our proposed method raises both the performance and consistency among multiple classifiers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Deep Learning Approachs In Ser",
      "text": "Since the first publication of the successful use of a convolutional neural network (CNN) for learning feature representations from speech signals  [10] , several researchers have followed this trend to use deep neural networks to automatically learn feature representation  [11] . Cummins et al.  [12]  proposed a CNNs based method, that used a pre-trained AlexNet to extract deep spectrum features and used an SVM for classification. Li et al.  [13]  used two different convolution cores to extract temporal domain features and frequency domain features, then two different kinds of features were concatenated and fed to convolutional layers, and attention pooling was used in the last layer to increase accuracy. Wu et al.  [14]  proposed to extract features with CNN, then combined a capsule network and a gated recurrent unit to address the classification task. Dai et al.  [8]  combined Cross-Entropy loss and center loss to enhance the discriminating power of their proposed approach. All these methods use precise labels as the ground truth.\n\nUnlike the above methods, a few recent studies suggested that the precise labels could not well represent the ambiguity of speech emotions, and then attempted to address this issue. Lotfian et al.  [6]  proposed a multi-task learning framework by specifying secondary emotions in addition to the dominant emotion, but was lack of other minor emotions. To solve this problem, soft-target label was presented by Ando et al.  [15] , which described the reference intensities of the target category. However, it assumed that all emotions existed in an utterance, and then estimated their proportions. This strategy made softtarget learning and computation complicated. To simplify the problem, Ando et al.  [7]  proposed multi-label learning to represent all emotions that existed in utterance, instead of the emotion distribution. We can see all the above methods cannot represent the ambiguity well. Moreover, these methods heavily depend on the statistics of experts' voting, thus, limits their generalization ability. On the contrast, MCIL can be applied on all types of datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Multi-Classifier Approaches",
      "text": "To achieve a robust performance on complicated problems, many studies tried to mimic the collective decision-making behavior of human, such as ensemble learning  [16] , co-training  [17] , and tri-training  [18] .\n\nEnsemble learning was designed to combine the decisions from diverse classifiers to improve the overall performance. The underlying idea is that even if one classifier gets a wrong prediction, other classifiers are still able to correct the error to maintain the performance (i.e. bagging  [19] , boosting  [20]  and stacking  [21] ). However, there is no interaction between classifiers, which indicates that classifiers cannot improve their individual performance through the ensemble learning process.\n\nInstead, co-training and tri-training, two typical semisupervised algorithms, improve the performance of each classifier by interacting data among classifiers. In co-training algorithm, firstly, two classifiers are trained on a labeled dataset with two different views. Afterwards, two classifiers swap the samples according to their high confident predictions during semi-supervised procedure, in which these samples are treated as the new training data. Inspired by co-training, tri-training was proposed to train three classifiers on three training subsets, which are obtained by bootstrap sampling from labeled datasets. In semi-supervised procedure, each new sample is predicted by two classifiers. If the predicted labels are identical, this sample will be marked as the training data for the third classifier with this label. Due to above mechanism, one can see that only the discriminating data are involved in training and interacted, but those ambiguous data usually are excluded in the training process.\n\nIn contrast, MCIL improves the performance of classifiers by interacting their decisions, specifically, constructing the ambiguous label distribution as the new data label. It brings two merits: 1) a better representation of ambiguous data; 2) all data are included in the training procedure.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Mathematically Analyzing The Effectiveness",
      "text": "OF OPTIMAL INTERACTIVE THEORY The optimal interactive theory  [9]  says that multiple individuals can come to an optimal joint decision by sharing information with each other. It has been proved by several psychological experiments that individuals with similar cognition abilities can perform better in an interactive process. Here, we try to mathematically analyze the effectiveness of the theory.\n\nIn above experiments, psychologists constructed a psychometric curve (a cumulative Gaussian function 1  ) according to each participant that plots the cognition performance, P , of each individual against the clarity of the data, ∆c, in the task, as shown in Fig.  3 . The curve is also determined by the participant's cognition ability, σ, and data clarity bias, b. The σ is the variance of the psychometric curve, hence, denotes the participant's ability of making the right decision. The smaller σ is, a better cognition performance achieves. The b denotes the clarity of the data when the individual's cognition accuracy rises the fastest. A smaller b indicates that the individual is more sensitive to the data with a lower clarity, thus, can make a better decision. Analogously, in machine learning, the performance of a classifier on ambiguous data could be also represented as a cumulative Gaussian function. The accuracy of each classifier, P , is determined by the classification capability of the classifier, σ, the clarity of the data, ∆c, and the data clarity bias b,\n\nwhere,\n\nGiven two different classifiers C 1 and C 2 , Figure  3  shows that their classification accuracies rise with the increase of clarity of the data, ∆c. However, due to different classification capabilities, their accuracies increase with different rates. It should be noted that the maximum slope S of the curve indicates the sensitivity of the classifier to the change of clarity of data. A larger S indicates a smaller variance, which means a better classification performance. S can be obtained by taking partial derivative of ∆c over P (∆c), which is inversely proportional to the variance of curve, σ,\n\nAdditional psychological studies  [22, 23]  have shown that multiple participants will lead to a new psychometric curve after interactive learning, as the red curve shown in Fig.  3 . To prove it, we define a joint performance, P joint , after interactive learning among multiple classifiers according to the psychological model in  [24] ,\n\nwhere,\n\nwhere, the weight is defined as\n\nTaken the partial derivative of ∆c over P joint (∆c), S joint can be approximated by\n\nAccording to the inequality relation, we have\n\nThis result is consistent to the red curve in Fig.  3 . It can be concluded that the sensitivity, S joint , of the classifier has been enhanced by interactive learning.\n\nMoreover, according to Eq. 5 and Eq. 6, the variance σ 2 joint after interactive learning can be written as\n\nTo evaluate the σ 2 joint , we subtract arbitrary k-th σ 2 k that does not take interactive learning.\n\nthen, we have\n\nThis result also proves the classification capability of classifiers can be improved through interactive learning on the task of recognizing the ambiguous data.\n\nThe above analysis shows the effectiveness of interactive learning. It indicates that classifiers could simulate the interaction process between human beings. Therefore, the question becomes how to implement interactive learning for multiple classifiers. We then propose the MCIL algorithm.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. The Proposed Method",
      "text": "In this section, we first describe the problem formulation of emotion ambiguity. Subsequently, the idea of constructing ambiguous labels is explained. Finally, the function of interactive learning is detailed. Figure  2  shows our framework.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "In past studies, the precise labels were commonly used in SER task, which were mostly defined as Y ∈ {0, 1, 2, 3, 4} to denote angry, happy, neutral, panic, and sadness. However, far from the optimal situation, there are always some ambiguous data between emotions in the real world. Therefore, it is not always feasible to use precise labels to represent these data.\n\nTo solve this issue, we begin designing ambiguous labels for emotion, which are defined as follows:\n\nwhere, y a x , y h x , y n x , y p x , y s x ∈ [0, 1] represent the probability that the ambiguous degree of emotion x belongs to angry, happy, neutral, panic, and sadness category, respectively. Therefore, Y x indicates that a sample is suggested to be in a category with a higher probability label.\n\nIn practice, ambiguous distribution (y a x , y h x , y n x , y p x , y s x ) is usually unknown and difficult to obtain. Ideally, we can invite N participants to manually annotate a precise label for these emotional data. Thus, an ambiguous label can be obtained by their voting:\n\nwhere v represents the number of votes for each category on the sample x. However, this is time-consuming and requires a large workload. Therefore, we present a novel idea of using multiple classifiers to simulate different individual's inconsistent cognition to vote for these samples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. The Construction Of Ambiguous Labels",
      "text": "In optimally interacting theory, individuals are able to get a better solution to a complicated problem by communicating with others. To mimic this process, we employ N neural networks {N et i } with different architectures for training and testing on emotional speech datasets to construct ambiguous labels for interactive communication.\n\nTo this end, we design three datasets D 1 , D 2 , and D 3 , respectively. D 1 contains the samples with precise labels, which are less ambiguous and used to train N classifiers in the precise label learning stage. D 3 consists of ambiguous samples and is employed to evaluate the performance of interactive learning. D 2 includes a large number of samples that are unlabeled, and is applied to ambiguous label construction and subsequent retraining.\n\nFirst, as shown in the left section of Fig.  2 , all classifiers are trained on D 1 for precise label learning. Here, we use the Cross-Entropy to optimize the {N et i }, which is defined as follows,  (14)  where n denotes the number of emotions, m denotes the number of the samples, y ji denotes the ground truth, and ŷji represents the output of N et i .\n\nSince each N et i has a different architecture, the learned representation is different from that of the others. Furthermore, each category of ambiguous emotion brings uncertain knowledge to each N et i . Thus inconsistent cognition among human beings can be mimicked.\n\nConsequently, the well-trained {N et i } are used to classify the unlabeled data in D 2 . Then, the classification results of N networks obtained from each sample, are counted by voting and normalized to be ambiguous labels.\n\nThe label of a sample x is denoted as Y x = (y a x , y h x , y n x , y p x , y s x ), which represents the probability that x belongs to each category. As the results from the different knowledges of N networks, no manual annotation is required. Meanwhile, we regard the constructed labels as the results of communication among these networks.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Interactive Learning",
      "text": "At this moment, the data in D 2 have their new ambiguous labels Y x , which contain slightly different information from precise labels. Afterwards, we are able to retrain {N et * i } on D 2 to minimize the gap between the predicted label and the label Y x given by {N et i }, as shown in the middle section of Fig.  2 . We call this interactive learning.\n\nSince ambiguous labels are a probability distribution, we select KL divergence to represent them. Therefore, {N et * i } are optimized by KL loss during the retraining stage, which is defined below,\n\nwhere Ŷ denotes the predicted label, Y denotes the ambiguous label given by {N et i }, and m denotes the number of samples. This procedure mimics how human beings learn new knowledge by communicating with others. Similarly, each {N et * i } also learns more comprehensive information of data from other networks. Other studies use precise labels for recognition, in other words, each sample belongs to only one emotional category in their methods. In contrast with them, our output is a distribution that may span more than one category. In order to be consistent with other studies, we choose the highest probability in the distribution as final output category as illustrated in the right section of Fig.  2 .\n\nSince the conclusion of Bahrami's paper  [9]  pointed that \"The general consensus from extensive earlier work on collective decision-making is that groups rarely outperform their best members\", we follow the consensus and choose the best classifier of MCIL to make the final decision.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Experimental Setting A. Database",
      "text": "To evaluate MCIL, we conduct the performance test on three benchmark databases. Mandarin Affective Speech (MAS). Since Chinese culture and emotional expression are conservative, which cause Chinese pronunciation and intonation to be mild, the features of data in Chinese corpora are somewhat ambiguous. The speakers expressed 5 different emotions including angry, happy, neutral, panic and sadness to act the utterance. Speakers read the same sentence three times repeatedly and 20 sentences in total for each emotion. Interactive Emotional Dyadic Motion Capture (IEMO-CAP). IEMOCAP corpus  [25]  was a commonly used English corpus for SER. Following  [26, 27] , we used both improvised and scripted data, and chose angry, happy, neutral, sadness and excited as the basic emotions. With reference to  [8, 25] , we merged happy and excited as happy since they are close in the activation and valence domain. FAU-AIBO Emotion Corpus (FAU-AIBO). FAU-AIBO is a corpus of German children communicating with Sony's AIBO pet robot  [28] . The corpus can be divided into 2 or 5 emotional categories. To verify the robustness of MCIL, we chose the 2emotion, which are NEG(active): states with negative valence (angry, torchy, reprimand, emphatic) and IDL(e): all other states. The details of three corpora are summarized in Table  I .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "The following two metrics are employed to evaluate the effectiveness of MCIL on speech emotion recognition: Classification accuracy. We evaluated the performance of each classifier with respect to recognition accuracy tested on D 3 . Consistency among multiple classifiers. Due to the different classifier architectures and ambiguous data, the recognition results of {N et i } and {N et * i } should not be consistent. One of the purposes of interactive learning is to improve classification consistency among all classifiers. Thus, a Kappa value was used to evaluate the consistency of classifiers:\n\nwhere,\n\nwhere, d denotes the number of test samples, n denotes the number of classifiers, c denotes the number of categories,\n\ndenotes the number of votes for the sample i is classified as category j, P i denotes the consistency for each sample i, and P j denotes the consistency for each category.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Implementation Details",
      "text": "The five classifiers in the proposed framework were the 34layer ResNet  [29] , 121-layer DenseNet  [30] , SqueezeNet  [31] , 11-layer VGG  [32] , and AlexNet  [33] . For DenseNet, ResNet, AlexNet, and VGG, according to the number of categories in FAU-AIBO, IEMOCAP, and MAS, the last fully-connected layer was adjusted to two, four, and five outputs, respectively. For SqueezeNet, the output dimension of the final convolution layer was reduced to two, four, and five, respectively. For each classifier, we adjusted the number of layers to avoid overfitting.\n\nThe training procedure of all classifiers was implemented using PyTorch on an NVIDIA 2080Ti GPU, and all five N et i were pre-trained. The corpora were divided into three groups as shown in Table  II . D 1 , which contained samples with precise labels that are less ambiguous than those in the other groups, was used to train N et i in the precise label learning stage. To avoid over-fitting, we expand D 1 by horizontally flipping the samples as data augmentation. D 2 , which contained unlabeled samples, was used to construct the ambiguous label and retrain the five N et * i . Finally, D 3 , which contained labeled but more ambiguous samples, was used to evaluate the performance of classifiers. Precise label learning: To train these five N et i for precise labels, we applied 5-fold cross-validation on D 1 . Since random cropping or resizing may influence the recognition of spectrograms, all samples were used with the original size in this work. During training, the parameters of the first few layers were fixed, and the subsequent layers were optimized. The objective function was Cross-Entropy loss. These models were trained using Adam with a batch size of 16, and the learning rate decaying exponentially from 10 -4 to 10 -8 . After training, five N et i were tested on D 3 , and their results were used as the baseline. Interactive learning: First, five trained N et i classified the unlabeled samples in D 2 . Their results constructed the ambiguous label Y x for each sample x. Then, these N et * i were retrained on D 2 using the new label Y . The classifiers were optimized by minimizing the new objective function KL-loss. We fixed the first few layers of each classifier and retrained them with a batch size of 16, weight decay of 0.0005, and learning rate decaying exponentially from 10 -4 to 10 -8 across 50 epochs. The retraining was stopped when no change of KLloss value.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Experimental Results Analysis A. Ablation Studies: The Efficiency Of 3, 5, And 7 Classifiers In Mcil",
      "text": "To analyze the influence of the number of the classifiers used in MCIL, we further evaluate the using of 3, 5, and 7 classifiers on IEMOCAP that is widely used. The results are listed in table V.\n\nAs shown in the table, using 5 classifiers to train MCIL achieves the best results. When we increase the number of classifiers from 3 to 5, we can observe clear improvements from 2.75% to 6.75% for all classifiers. Moreover, four of five classifiers (i.e ResNet, VGG, SqueezeNet, and DenseNet) obtain the best performance. The reason behind could be that 5 classifiers bring more diversities of the classifiers with inconsistent cognition than 3 classifiers. This leads to a better representation of ambiguous data, and further improve the consistency and accuracy of all classifiers. However, this does not mean increasing the number of classifier will always get a better results. As we can see, 7-classifer does not outperform 5-classifier. The explanation might be that the two additional have lower performances than that of other classifiers in baseline. They weaken the cognitive ability of the entire group of all classifiers, thus, lead to the decreasement of the overall accuracy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. The Effectiveness Of The Mcil Method",
      "text": "To evaluate the performance of our proposal, we first test the five classifiers for ambiguous emotion recognition with and without interactive learning. Table  III  lists their performance changes on D 3 of MAS and IEMOCAP.\n\nAs we can see, on both corpora, the overall accuracy of five N et * i are entirely improved. Firstly, we analyze the performance on the most ambiguous emotion categories angry and happy in Mandarin, which obtain the lower performance among five emotions. Although, VGG obtains 53.5% and 41.5% accuracies of angry and happy when trained with the precise label, respectively, MCIL introduces the remarkable improvements of 5.5% and 5%. The five N et * i all lead to performance improvement on ambiguous emotion angry. Secondly, we see that the neutral emotion achieves the highest accuracy among all emotion categories. ResNet and SqueezeNet can obtain 10.5% and 8.5% improvement on the neutral emotion, respectively. The overall accuracy of VGG rises remarkably by 5.8%, and DenseNet increases from 60.4% to 62.2% as well.\n\nMeanwhile, the results from the IEMOCAP corpus are similar to the MAS. The overall accuracy of ResNet achieves a remarkable 6.8% increase, and AlexNet rises from 58.5% to 64.0%. DenseNet, VGG, and SqueezeNet obtain 3.5%, 3.0%, and 3.0% improvements, respectively.\n\nTable  IV  lists the performance of FAU-AIBO. Since FAU-AIBO is severely unbalanced, specifically, IDL is much bigger than NEG in terms of both total sample size and clear sample size, all classifiers perform better on IDL. After interactive learning, we can observe all five networks are improved on both categories. Particularly, the accuracy of SqueezeNet rises 8.4% on IDL, and rises 13.97% on NEG, and finally results",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Confusion Matrix",
      "text": "We further evaluate the performance of MCIL on each emotion category in MAS and IEMOCAP. Table VI and table VII list the confusion matrix of DenseNet with and without the interactive learning.\n\nOn MAS, four emotion categories have improvement after interactive learning. As we can see, on most ambiguous emotion categories angry and happy in Mandarin, there is a 1.5% decrease of misclassifying from happy to angry. And the most unique emotion neutral achieves the highest accuracy 78.5% among all emotion categories. We can also observe a 5.5% accuracy increase on neutral when using interactive learning. Finally, sadness is the only emotion category that decreases in accuracy. We have discovered that during the stage of ambiguous labels construction, some ambiguous labels that should belong to sadness are more inclined to neutral. Consequently, this causes a decrease in the accuracy of sadness.\n\nOn IEMOCAP, the performances of three of the four emotion categories are improved. Specifically, 3.0% and 1.0% accuracy increases on happy and neutral can be observed after using interactive learning. Different from the MAS, the sadness achieves a remarkably 17% increase from 61.0% to  78.0%, which is also the highest accuracy 78.0% among all emotion categories. The only exceptional emotion category that decreases 7.0% in accuracy is happy. We can also observe that there is a 5.0% misclassification increase from angry to happy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Comparison Of Visualized Feature Representations",
      "text": "To gain insight into MCIL, we extended the effectiveness examination at the feature representation level. Each emotion category in D 3 of MAS database was visualized by t-SNE in Fig.  1  using the learned feature representations of VGG net with and without employing MCIL.\n\nFigure  1 .a denotes the feature representations trained with only precise label learning, treated as a baseline. Figure  1 .b denotes the feature representations with interactive learning. There exist clearly less overlaps between categories in Fig.  1 .b compared with Fig.  1 .a. This illustrates that VGG net obtains a stronger separability on ambiguous emotions after employing MCIL. To have a quantitative comparison, we compute the normalized inner-class distances of feature representations with and without employing MCIL. The result shows the distance decreases from 0.663 of baseline to 0.638 of MCIL, which means that the feature representations of MCIL are more compact and consistent.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Comparison With State-Of-The-Art",
      "text": "We compared MCIL with the state-of-the-art of SER methods (i.e. Cummins et al.  [12] , Li et al.  [13] , Wu et al.  [14] , Dai et al.  [8]  and Ando et al.  [7] ) to evaluate the  effectiveness. Since datasets in those methods are different from ours, to make a fair comparison, we retrain these methods on D 1 using their own hyperparameters and learning rates. The final averaged test result is obtained by 5-fold crossvalidation. In addition, we implemented tri-training  [18]  because it also employs more than one classifier to improve the performance. To verify the conclusion of Bahrami's paper  [9] , the majority voting of five {N et * i } is used to embody the collective decision-making result. Please note that we choose the DenseNet trained by MCIL, which is the best classifier, to compare with other methods. The results are listed in Table  VIII .\n\nOn the MAS corpus, MCIL achieves 62.2% accuracy, which outperforms the other six methods from 3.1% to 14.7%. On the IEMOCAP corpus, MCIL also achieves a superior accuracy, outperforms other methods from 2.0% to 11.7%. On the FAU-AIBO corpus, MCIL surpasses others from 1.58% to 4.16%. The improvement on FAU-AIBO is lower than the ones on the other two databases. The reason is that FAU-AIBO has two categories, which is simpler than multiple categories in MAS and IEMOCAP. This result demonstrates that MCIL is better at handling more complicated ambiguous data. As Ando's method  [7]  required the statistic of experts' voting to construct the multi-label, it can be only applied to IEMOCAP corpus. MAS does not include the voting information, and experts voted each word instead of the utterance in FAU-AIBO.\n\nInterestingly, tri-training reaches a good result on FAU-AIBO but performs worst on MAS and IEMOCAP. The possible reason might be the number of emotion categories. The votes of three classifiers are not sufficient for the databases with more than two categories. Moreover, tri-training selects more discriminating data and ignore the ambiguous samples during classifiers interacting. Therefore, they perform worse than MCIL.\n\nAs for majority voting result, it achieves 59.10% and 65.00% accuracy on MAS and IEMOCAP, which only performs slightly worse than MCIL. And on FAU-AIBO, it performs the third worst of all the method. This result verifies the conclusion in Bahrami's paper, \"groups rarely outperform their best members\".",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "F. Comparison With State-Of-The-Arts On Each Emotion",
      "text": "To evaluate the performance of MCIL and the state-of-thearts on each emotions, we further investigate the accuracy of those methods on MAS, IEMOCAP, and FAU-AIBO, which are listed in table IX, table X and table XI, respectively. As we can observe, these methods perform quite differently on different emotions.   As shown in table IX, on MAS, MCIL is much better than the other five competing methods on the accuracy of the most ambiguous category happy. It shows from 5.5% to 42.5% improvement than the state-of-the-art, which demonstrates the effectiveness of interactive learning for ambiguous emotion recognition. As for the less-ambiguous emotion category neutral,majority voting obtains the best performance, the reason behind this is that all five classifiers performs good and more consistent on neutral. And our MCIL achieves the second best accuracy, which obtains at least an 8.0% improvement, and is only 0.5% worse than the majority voting result. On the angry and panic categories, MCIL still ranks at the second, outperforms other four methods. Although, Li's method reaches a remarkable high accuracy on angry, but has rather low accuracy on happy and panic. In the category of sadness, three methods outperform us only from 4.0% to 6.5%. Moreover, MCIL method achieves 62.2% overall accuracy which is a considerable rise from 3.1% to 14.8% compared with the other five methods.\n\nOn IEMOCAP, as shown in table X, MCIL achieves better performance on two of the four emotion categories: neutral and sadness. On neutral, our method demonstrates a 7.0% to 35.0% raise compared with other methods. A similar trend can be found on sadness. As for happy and angry, MCIL also obtains the second best performance. Again, Li's method performs very good on angry, but the worst on happy. For overall accuracy, our MCIL also obtains the considerable increases from 2.0% to 24.97% compared with the state-ofthe-arts.\n\nTable  XI  shows that these methods perform differently on different emotions on the Fau-AIBO corpus. For example, Dai's method performs better on IDL, but the worst on NEG. Wu's approach performs better on NEG, but achieves the worst accuracy on IDL. Both of them indicate that their methods work well for only specific emotions. Contrasts with those methods, the accuracies of two categoris of our MCIL are more balanced. Our method achieves 77.48% on IDL, which is the third best, and 48.0% on NEG, which is the second best. Meanwhile, MCIL gains best overall accuracy, and outperforms other methods from 4.16% to 1.58%.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "G. Consistency Evaluation",
      "text": "The optimally interacting theory also indicates that interpersonal communication can improve cognition consistency among different people. Therefore, we evaluated the classification consistency among five N et i and N et * i using the Fleiss Kappa (K) value on three corpora. Figure  4  illustrates the difference between the K values with and without using interactive learning. As we can see, the K value of {N et i } trained by the precise labels achieves only 0.5088, 0.5245, and 0.3554 respectively. This is because D 3 mainly contains ambiguous samples, which confuse the five N et i in coming to an agreement. It also shows that K rises by 0.15, 0.19, and 0.25 on {N et * i } after interactive learning, and the consistency has generally been improved from moderate agreement to substantial agreement.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this study, we have addressed the issue of ambiguous SER system by presenting a novel multi-classifier interaction learning (MCIL) method. The MCIL consisted of two novel components: ambiguous label construction and interactive learning. Multi-classifiers were applied to construct ambiguous labels of emotion, which can better represent ambiguous emotion. The interactive learning, which used the KL divergence, was found to be a more feasible strategy for objective measurement. The effectiveness of MCIL was evaluated on three benchmarks: MAS, IEMOCAP, and FAU-AIBO. The experiments show MCIL outperforms state-of-the-art methods on both recognition accuracy and consistency of classification. Both achievements indicated that interactive learning is an effective method for recognizing ambiguous data. Our future study is going to investigate how to improve the robustness of ambiguous label construction.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Feature visualization of ﬁve emotions learned by VGG",
      "page": 1
    },
    {
      "caption": "Figure 2: An overview of our proposed MCIL framework for SER. In precise label learning, multiple classiﬁers simulate",
      "page": 3
    },
    {
      "caption": "Figure 3: The curve is also determined by",
      "page": 3
    },
    {
      "caption": "Figure 3: To prove it, we deﬁne a joint performance, P joint, after",
      "page": 3
    },
    {
      "caption": "Figure 3: The psychometric curves of participants (classiﬁers).",
      "page": 4
    },
    {
      "caption": "Figure 2: shows our framework.",
      "page": 4
    },
    {
      "caption": "Figure 1: using the learned feature representations of VGG net",
      "page": 7
    },
    {
      "caption": "Figure 1: a denotes the feature representations trained with",
      "page": 7
    },
    {
      "caption": "Figure 1: a. This illustrates that VGG net obtains a",
      "page": 7
    },
    {
      "caption": "Figure 4: The Fleiss Kappa K values of multiple classiﬁers with",
      "page": 9
    },
    {
      "caption": "Figure 4: illustrates",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Angry": "Baseline\nMCIL\n57.0%\n61.5%\n71.0%\n64.0%\n58.0%\n61.5%\n64.0%\n69.0%\n53.5%\n59.0%\n63.0%\n63.0%\n54.0%\n59.0%\n55.0%\n58.0%\n55.0%\n59.5%\n63.0%\n63.0%",
          "Happy": "Baseline\nMCIL\n48.5%\n52.5%\n49.0%\n52.0%\n43.5%\n47.0%\n51.0%\n50.0%\n41.5%\n46.5%\n59.0%\n50.0%\n43.0%\n43.0%\n57.0%\n55.0%\n46.5%\n43.5%\n59.0%\n57.0%",
          "Neutral": "Baseline\nMCIL\n73.0%\n78.5%\n73.0%\n74.0%\n68.5%\n79.0%\n60.0%\n71.0%\n70.5%\n77.5%\n62.0%\n73.0%\n70.5%\n77.0%\n58.0%\n66.0%\n65.5%\n74.0%\n68.0%\n69.0%",
          "Panic": "Baseline\nMCIL\n57.5%\n59.5%\n55.5%\n62.5%\n48.5%\n57.0%\n52.5%\n60.5%\n56.5%\n64.5%",
          "Sadness": "Baseline\nMCIL\n66.0%\n59.0%\n61.0%\n78.0%\n59.0%\n57.0%\n67.0%\n79.0%\n58.5%\n61.5%\n69.0%\n78.0%\n63.0%\n53.5%\n64.0%\n77.0%\n58.0%\n59.0%\n66.0%\n81.0%",
          "Overall": "Baseline\nMCIL\n60.4%\n62.2%\n63.5%\n67.0%\n56.9%\n61.4%\n60.5%\n67.3%\n54.5%\n60.3%\n63.0%\n66.0%\n56.6%\n58.6%\n58.5%\n64.0%\n56.3%\n60.1%\n64.0%\n67.0%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IDL": "Baseline\nMCIL\n77.48%\n76.11%\n77.08%\n74.25%\n75.87%\n74.41%\n76.59%\n76.51%\n68.85%\n60.45%",
          "NEG": "Baseline\nMCIL\n48.00%\n46.24%\n48.36%\n45.54%\n49.53%\n48.94%\n46.24%\n45.49%\n53.76%\n39.79%",
          "Overall": "Baseline\nMCIL\n65.47%\n63.94%\n65.38%\n62.55%\n65.14%\n64.04%\n64.23%\n63.86%\n62.70%\n52.03%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Angry": "Baseline\nMCIL\n61.50%\n57.00%\n17.00%\n15.50%\n4.00%\n5.50%\n8.50%\n7.50%\n2.00%\n1.50%",
          "Happy": "Baseline\nMCIL\n21.00%\n21.00%\n52.50%\n48.50%\n4.50%\n4.00%\n17.00%\n20.00%\n0.50%\n0.50%",
          "Neutral": "Baseline\nMCIL\n6.00%\n4.50%\n8.50%\n10.00%\n78.50%\n73.00%\n5.50%\n6.00%\n22.50%\n29.50%",
          "Panic": "Baseline\nMCIL\n13.50%\n11.50%\n15.50%\n17.50%\n1.50%\n1.00%\n59.50%\n57.50%\n9.00%\n9.50%",
          "Sadness": "Baseline\nMCIL\n2.50%\n1.50%\n10.50%\n4.50%\n17.00%\n11.00%\n11.50%\n7.00%\n66.00%\n59.00%"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Angry\nHappy\nNeutral\nSadness",
          "Angry": "Baseline\nMCIL\n71.00%\n64.00%\n15.00%\n12.00%\n3.00%\n1.00%\n2.00%\n2.00%",
          "Happy": "Baseline\nMCIL\n12.00%\n17.00%\n49.00%\n52.00%\n18.00%\n14.00%\n17.00%\n9.00%",
          "Neutral": "Baseline\nMCIL\n15.00%\n16.00%\n30.00%\n32.00%\n73.00%\n74.00%\n20.00%\n11.00%",
          "Sadness": "Baseline\nMCIL\n2.00%\n3.00%\n6.00%\n4.00%\n6.00%\n11.00%\n61.00%\n78.00%"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The cognitive structure of emotions",
      "authors": [
        "A Ortony",
        "G Clore",
        "A Collins"
      ],
      "year": "1990",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "2",
      "title": "A multiple perception model on emotional speech",
      "authors": [
        "J Tao",
        "Y Li",
        "S Pan"
      ],
      "year": "2009",
      "venue": "International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "3",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2016",
      "venue": "IJCNN"
    },
    {
      "citation_id": "6",
      "title": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition based on multi-label emotion existence model",
      "authors": [
        "A Ando",
        "R Masumura",
        "H Kamiyama",
        "S Kobashikawa",
        "Y Aono"
      ],
      "year": "2019",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "8",
      "title": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "authors": [
        "D Dai",
        "Z Wu",
        "R Li",
        "X Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Optimally interacting minds",
      "authors": [
        "B Bahrami",
        "K Olsen",
        "P Latham",
        "A Roepstorff",
        "G Rees",
        "C Frith"
      ],
      "year": "2010",
      "venue": "Science"
    },
    {
      "citation_id": "10",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE TMM"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using voiced segment selection algorithm",
      "authors": [
        "Y Gu",
        "E Postma",
        "H.-X Lin",
        "J Herik"
      ],
      "year": "2016",
      "venue": "ECAI"
    },
    {
      "citation_id": "12",
      "title": "An image-based deep spectrum feature representation for the recognition of emotional speech",
      "authors": [
        "N Cummins",
        "S Amiriparian",
        "G Hagerer",
        "A Batliner",
        "S Steidl",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "ACM MM"
    },
    {
      "citation_id": "13",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L.-R Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "X Wu",
        "S Liu",
        "Y Cao",
        "X Li",
        "J Yu",
        "D Dai",
        "X Ma",
        "S Hu",
        "Z Wu",
        "X Liu"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Soft-target training with ambiguous emotional utterances for dnn-based speech emotion classification",
      "authors": [
        "A Ando",
        "S Kobashikawa",
        "H Kamiyama",
        "R Masumura",
        "Y Ijima",
        "Y Aono"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Ensemble methods in machine learning",
      "authors": [
        "T Dietterich"
      ],
      "year": "2000",
      "venue": "International workshop on multiple classifier systems"
    },
    {
      "citation_id": "17",
      "title": "Combining labeled and unlabeled data with co-training",
      "authors": [
        "A Blum",
        "T Mitchell"
      ],
      "year": "1998",
      "venue": "Proceedings of the eleventh annual conference on Computational learning theory"
    },
    {
      "citation_id": "18",
      "title": "Tri-training: Exploiting unlabeled data using three classifiers",
      "authors": [
        "Z Zhou",
        "M Li"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on knowledge and Data Engineering"
    },
    {
      "citation_id": "19",
      "title": "Bagging predictors",
      "authors": [
        "L Breiman"
      ],
      "year": "1996",
      "venue": "Machine learning"
    },
    {
      "citation_id": "20",
      "title": "The strength of weak learnability",
      "authors": [
        "R Schapire"
      ],
      "year": "1990",
      "venue": "Machine learning"
    },
    {
      "citation_id": "21",
      "title": "Stacked generalization",
      "authors": [
        "D Wolpert"
      ],
      "year": "1992",
      "venue": "Stacked generalization"
    },
    {
      "citation_id": "22",
      "title": "Humans integrate visual and haptic information in a statistically optimal fashion",
      "authors": [
        "M Ernst",
        "M Banks"
      ],
      "year": "2002",
      "venue": "Nature"
    },
    {
      "citation_id": "23",
      "title": "The ventriloquist effect results from near-optimal bimodal integration",
      "authors": [
        "D Alais",
        "D Burr"
      ],
      "year": "2004",
      "venue": "Current biology"
    },
    {
      "citation_id": "24",
      "title": "Signal integration in human visual speed perception",
      "authors": [
        "M Jogan",
        "A Stocker"
      ],
      "year": "2015",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition"
    },
    {
      "citation_id": "27",
      "title": "Deep learning of segmentlevel feature representation with multiple instance learning for utterance-level speech emotion recognition",
      "authors": [
        "S Mao",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "Deep learning of segmentlevel feature representation with multiple instance learning for utterance-level speech emotion recognition"
    },
    {
      "citation_id": "28",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: the fau aibo emotion corpus",
      "authors": [
        "A Batliner",
        "S Steidl",
        "E Nöth"
      ],
      "year": "2008",
      "venue": "Proc. of a Satellite Workshop of LREC"
    },
    {
      "citation_id": "29",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "30",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "31",
      "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters a<0.5 mb model size",
      "authors": [
        "F Iandola",
        "S Han",
        "M Moskewicz",
        "K Ashraf",
        "W Dally",
        "K Keutzer"
      ],
      "year": "2016",
      "venue": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters a<0.5 mb model size",
      "arxiv": "arXiv:1602.07360"
    },
    {
      "citation_id": "32",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "33",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "NIPS"
    }
  ]
}