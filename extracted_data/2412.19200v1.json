{
  "paper_id": "2412.19200v1",
  "title": "Personalized Dynamic Music Emotion Recognition With Dual-Scale Attention-Based Meta-Learning",
  "published": "2024-12-26T12:47:35Z",
  "authors": [
    "Dengming Zhang",
    "Weitao You",
    "Ziheng Liu",
    "Lingyun Sun",
    "Pei Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of different moments in music, playing a crucial role in music information retrieval. The existing DMER methods struggle to capture long-term dependencies when dealing with sequence data, which limits their performance. Furthermore, these methods often overlook the influence of individual differences on emotion perception, even though everyone has their own personalized emotional perception in the real world. Motivated by these issues, we explore more effective sequence processing methods and introduce the Personalized DMER (PDMER) problem, which requires models to predict emotions that align with personalized perception. Specifically, we propose a Dual-Scale Attention-Based Meta-Learning (DSAML) method. This method fuses features from a dual-scale feature extractor and captures both short and long-term dependencies using a dual-scale attention transformer, improving the performance in traditional DMER. To achieve PDMER, we design a novel task construction strategy that divides tasks by annotators. Samples in a task are annotated by the same annotator, ensuring consistent perception. Leveraging this strategy alongside meta-learning, DSAML can predict personalized perception of emotions with just one personalized annotation sample. Our objective and subjective experiments demonstrate that our method can achieve state-of-the-art performance in both traditional DMER and PDMER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Music Emotion Recognition (MER) technology focuses on identifying emotions conveyed by music, applying to music therapy  (Dingle et al. 2015) , music recommendation  (Liu et al. 2023; Tran et al. 2023) , and music generation  (Huang and Huang 2020; Ji and Yang 2024) . To describe the music emotion, Russell's two-dimensional valence-arousal (V-A) emotional model  (Russell 1980 ) is widely used in MER, where valence describes the extent to which an emotion is positive or negative, and arousal refers to its intensity. Existing MER tasks are generally divided into static MER (SMER) and dynamic MER (DMER)  (Han et al. 2022) . SMER inputs music and outputs only one V-A label to describe the emotion, which fails to describe the variations in emotion within the music. For example, Beethoven's Symphony No. 5 can't be described as simply positive and intense, as it also contains moments of sadness and tranquility. In contrast, DMER predicts the V-A label sequence, using a sequence label to describe the emotional changes in the music, which can more accurately express the emotions.\n\nExisting DMER work focuses on utilizing sequential information to predict, as the emotion of each moment in music is related to the emotions before and after. Specifically, long short-term memory (LSTM)  (Hochreiter and Schmidhuber 1997)  attracted the attention of researchers due to its superiority in sequence data processing. These researchers use LSTM to extract sequence features and have shown some effectiveness in DMER  (He and Ferguson 2020; Zhang et al. 2023) . However, previous studies  (Khandelwal et al. 2018; Li et al. 2019; Grigsby et al. 2021)  have found that LSTM struggles to capture long-term dependencies, which limits the capture of the global emotion of music, resulting in poor performance in DMER.\n\nMore importantly, existing work often assumes that all individuals perceive music emotions in the same way, neglecting the significant impact of individual differences on DMER. For example, MER1101 dataset  (Zhang et al. 2023)  involves multiple annotators for labeling and uses their average as labels to attempt to eliminate individual differences in the data as shown in Figure  1 (a). However, emotions are personalized, and different individuals have different perceptions of emotions towards the same song  (Kang and Herremans 2024) . For example, a song that makes one person feel happy may make another person feel sad. These existing works eliminates individual differences in data while eliminating bias, using group emotional perceptions instead of individual emotional perceptions to avoid personalization issues. To make matters worse, real-world applications often face challenges with diverse emotional perceptions from each individual, and removing personalization differences in datasets cannot address this issue. Therefore, we point out the personalized DMER (PDMER) problem, which requires models to predict emotions that align with individual personalized perceptions rather than group perceptions. To address the above issues, we propose a Dual-Scale Attention-Based Meta-Learning (DSAML) method to handle PDMER tasks. Specifically, to achieve DMER, DSAML uses a pre-trained Imagebind model  (Girdhar et al. 2023)  to extract global audio features and introduces an adapter to extract trainable local audio features. Fusion of the two features is fed into the dual-scale attention transformer, which focus on both local and global features to capture more comprehensive emotional features. Finally, a sequence of V-A values is predicted by the sequence prediction module, completing the DMER. To achieve PDMER, the Model-Agnostic Meta-Learning (MAML)  (Finn, Abbeel, and Levine 2017)  is used to personalize the model. Furthermore, we propose a personalized meta-learning task construction strategy for MAML, which divided tasks by annotators instead of music samples. Samples in a task are annotated by the same annotator, ensuring consistent perception. With this strategy, DSAML preserves personality differences, effectively enhancing PDMER performance.\n\nIn summary, this paper has the following contributions: • We propose the DSAML including a dual-scale feature extractor and a dual-scale attention transformer to capture both local and global features, improving the performance of traditional DMER. • We recognize the importance of personalized emotional perception for DMER, introduces the PDMER task, and design a personalized prediction method based on metalearning with a novel task construction strategy. • Objective experiments demonstrate that our method achieves the best performance in both traditional DEMR and PDMER. Subjective experiments also show that our method better conforms to individual personalized emotional perception in the real world.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work Dynamic Music Emotion Recognition",
      "text": "DMER aims to predict the emotions of music at different moments, as the emotions at any moment are often related to those before and after, so it is necessary to consider the temporal dependencies in the music emotions. As the recurrent neural network (RNN) is suitable for sequence data processing, researchers first used RNN to extract sequence features for DMER  (Malik et al. 2017) . However, due to the issue of gradient vanishing in RNNs when dealing with long sequences, researchers in the DMER field have subsequently adopted LSTM to extract sequential features, as LSTM introduces gating mechanisms for long sequence processing. Specifically, Zhang et al.  (Zhang et al. 2023 ) integrated spatial and channel dimension features and used Bi-directional LSTM (BiLSTM) for sequence learning to predict the V-A sequence of music. He et al.  (He and Ferguson 2020)  used multi-view CNN as feature extractors and then used BiL-STM to capture temporal context for predicting the V-A sequence of music. These methods have shown some effectiveness in DMER, but they still struggle to capture long-term dependencies as previously studies demenstrated  (Khandelwal et al. 2018; Li et al. 2019; Grigsby et al. 2021 ). Therefore, we propose a dual-scale feature extractor and a dualscale attention transformer to capture both short-and longterm dependencies, thereby improving the performance of long-sequence data processing.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Personalized Music Emotion Recognition",
      "text": "In the SMER field, researchers have conducted some PMER studies, Yang et al.  (Yang et al. 2007 ) first quantitatively evaluated the impact of personality on MER and found that personalization significantly influences MER. Based on this quantitative experiment, numerous personalized MER studies have appeared in the field of SMER. To achieve PMER, researchers mainly train personalized models using samples annotated by specific users. Personalized training depends on a large amount of user-annotated data, so researchers focus on reducing the number of specific user-annotated samples. Su and Fung  (Su and Fung 2012)  proposed an active learning method to reduce the number of specific userannotated samples by selecting the most informative ones for manual annotation.  Wang et al. (Wang et al. 2012 ) and Chen et al.  (Chen et al. 2014 ) used two-stage training, first training the background model and then adapting it using fewer specific user-annotated samples. However, in the field of DMER, there has been no specialized research on PMER, researchers often use the mean of multiple data annotators as the emotion label which ignores the impact of individual personalization on MER  (Orjesek et al. 2019; Zhang et al. 2023) . In other words, many pieces of DMER research focus on group emotions rather than individual emotions, which limits the practical application performance of existing DMER. Although the SMER methods have achieved good results in PMER, they still require a large amount of user-annotated data (at least 20 samples)  (Chen et al. 2014) . In DMER, users need to annotate more labels (e.g., 1200 labels for 20 30-second music, assuming labeling every 0.5 seconds), making these personalized SMER methods difficult to apply in DMER. Therefore, we design a PDMER method based on meta-learning with a new meta-learning task construction strategy, which only requires one specific user-annotated sample.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Meta-Learning",
      "text": "Meta-learning is widely used in few-shot learning problems because it can learn to solve new tasks with a small number of samples  (Thrun and Pratt 1998; Wang et al. 2020) . Meta-learning methods can be generally divided into three types: metric-based, model-based, and optimization-based meta-learning methods. Among them, metric-based metalearning methods focus on solving classification problems in the feature space  (Snell, Swersky, and Zemel 2017; Sung et al. 2018)  and are difficult to apply to regression tasks like DMER; model-based meta-learning methods focus on designing specific model structures to achieve fast learning goals  (Wang et al. 2020) , which limits the model structure; while optimization-based meta-learning methods adjust existing optimization algorithms to converge with a few samples for new tasks  (Finn, Abbeel, and Levine 2017; Nichol, Achiam, and Schulman 2018) . For example, MAML has no constraints on model structure and can also be applied to regression tasks, making it most suitable for application in DMER. However, directly applying MAML in PDMER still ignores individual personalized effects during the training process, limiting the performance of MAML. Therefore, we proposes a new method for constructing meta-learning tasks: incorporating the annotated data of each annotator and constructing tasks according to annotators, thereby effectively improving the performance of the model in the PDMER task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods Problem Formulation",
      "text": "Given a training dataset D = {(x 1 , y 1 1 ), . . . , (x i , y j i ), . . .}, and personalized annotated data (annotator not present in the training set) S p = {(x p1 , y p p1 ), . . . , (x pn , y p pn )}, where x i is the i-th music sample in D, y j i is the label from the jth annotator for x i , p represents any user with personalized emotional perception, x pn is the n-th music sample in S p , and y p pn is the label for x pn from the user p. In traditional DMER, the goal is to directly predict non-personalized label y q for a new query sample q by training on D. However, in PDMER, our goal is to predict the personalized label y p q for the specific user p by training on D and adapting it with S p .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Architecture",
      "text": "The DSAML model consists of four parts: Input Preprocessor, Dual-Scale Feature Extractor, Dual-Scale Attention Transformer, and Sequence Predictor, as shown in Figure  2 .\n\nInput Preprocessor To achieve DMER, the model needs to extract sequence features from the audio. Therefore, in the input preprocessor, we slice the original audio input. A music segment of length l seconds is sliced into k segments, corresponding to k sequence prediction values and a resolution of k/l Hz. The sliced music segments are then processed to calculate the log Mel-spectrogram.\n\nDual-Scale Feature Extractor The pre-trained Imagebind model has shown good performance in audio feature extraction  (Zou et al. 2023; CHAKHTOUNA, SEKKATE, and Abdellah 2024) . Consequently, DSAML utilizes Imagebind to extract the global feature z g of the audio x. However, the large number of parameters in Imagebind would significantly increases the personalization adaptation time in PDMER. Therefore, this paper freezes the parameters of Imagebind and introduces the Imagebind Adapter module with significantly fewer parameters. Furthermore, since the global feature extracted by Imagebind cannot represent the finer emotional changes in music, the Imagebind Adapter is designed to extract the local feature z l from the spectrogram sequence x ′ of the short music segments. The local and global features are then fused to obtain the audio feature z = σ(z l + z g ), where σ refers to the Sigmoid function. Specifically, as shown in Figure  3 , the Imagebind Adapter module extracts features using two convolutional layers, reduces the number of channels to 1 through a 1 × 1 convolutional layer, and finally maps the feature dimension to D through a fully connected layer.\n\nDual-Scale Attention Transformer Considering that the emotional state of music at a particular moment is often related to the music in the preceding moments, and that the overall emotion also influences the emotional state at that moment, this paper proposes a Dual Attention Transformer to extract context-aware emotional features z ′ from audio features z. The Dual Attention Transformer achieves different scales of attention through a local mask M l and a global mask M g , where the context length n l of M l is much smaller than the context length n g of M g , thereby enabling the Transformer's dual attention to focus on both local and global emotions. For a mask M n with context length n, it can be expressed as:\n\nwhere i and j represent different time steps in the sequence, and |i -j| ≤ n indicates that at any time step i, the information at time step j can be seen only if their distance does not exceed n. Using M l and M g as masks, local features z ′ l and global emotional features z ′ g are extracted from the audio features z:\n\n(2) where f represents the transformer module, and θ represents the parameters of the transformer module. It is noteworthy that z ′ l and z ′ g are extracted using a transformer module with shared parameters, only utilizing different masks to extract features at different scales. Finally, the local and global emotional features are fused z ′ = σ(z ′ l + z ′ g ) to obtain the context-aware emotional features z ′ .\n\nHowever, our analysis of the local attention maps A l and global attention maps A g under different masks reveals that merely constraining the attention through masks does not ensure that A l focuses more on the local context and A g on the global context. As shown in Figure  4 , it is possible that A l might focus on distant moments and A g on nearby moments, which results in extracted and fused features that are no longer comprehensive. To address this issue, we propose a diagonal attention map loss, which constrains the diagonal attention of A l to be higher and that of A g to be lower,\n\nwhere diag(A) denotes the diagonal values of the attention map A, and α and β are hyperparameters that control the diagonal attention of A l and A g , respectively. Specifically, α is set to a higher value to ensure that A l focuses more on the local context, while β is set to a lower value to ensure that A g focuses more on the global context.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Personalized Strategy",
      "text": "DSAML employs MAML for personalized learning and proposes a novel meta-learning task construction strategy to enhance the model's personalized prediction performance.\n\nThe objective of MAML is to find a model parameter θ for the task distribution p(T ), such that the loss L τi is minimized after k steps of learning on a randomly sampled task τ i ∼ p(T ), which can be expressed as:\n\nwhere S i and Q i are the support set and query set randomly sampled from τ i (S i ∩ Q i = ∅), and U k Si (θ) is the operator that updates θ k times using the S i .\n\nDefined in the problem formulation, we have a training dataset D = {(x 1 , y 1 1 ), . . . , (x i , y j i ), . . .} to find the optimal θ, where each music has labels from multiple annotators. As shown in Figure  1 (a), the existing DMER approaches often use the mean of all annotator labels for each music as the label, and train on the processed dataset\n\nThe traditional task construction strategy for training with MAML using D ′ can be represented as:\n\nAlthough this method performs well on traditional DMER and many datasets often directly provide D ′ (Aljanaki, Yang, and Soleymani 2017;  Zhang et al. 2018) , this type of dataset and task construction approach loses personalized emotional preference information and essentially represents group emotional preferences. Therefore, we propose a novel meta-learning task construction strategy, which builds tasks directly from the dataset D based on the annotators:\n\nwhere D i refers to all the music annotated by the i-th user, i.e., D i = {(x i 1 , y i 1 ), (x i 2 , y i 2 ), . . . , (x i m , y i m )}. This method treats each user's personality as different tasks, enabling the model to find a θ that can adapt quickly and perform optimally across all personalities.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Training & Inference Process",
      "text": "During the training process, we construct task distribution p(T ) from the training dataset D using the above personalized task construction strategy. A batch of tasks is randomly sampled from p(T ), and from each task τ i , we will randomly sample support set S i and query set Q i . Specifically, S i is used to optimize θ and obtain θ ′ i = U k Si (θ), then the training loss will be calculated on Q i . After accumulating the losses of all tasks in this batch, the model parameters will be updated as θ ← θ -η∇ θ Ti∼p(T ) L Qi (θ ′ i ), where η is the learning rate. This process repeats until the model converges, and the fitted parameters θ are used for inference.\n\nIn the inference process, we use all samples annotated by a new user p (anyone absent from D) in the personalized data S p . These samples serve as a support set to fast adapt the model θ to the user's personalized emotional perception and obtain the personalized model θ′ p for the new user p. Finally, the model θ′ p can predict any music's emotion that aligns with the personalized perception of the new user p.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "The resolution of DSAML is 2Hz, indicating there is one label every 0.5 seconds. The model architecture consists of 3 layers of Transformer, with a mask context length of n l = 5 and n g = 30. In attention loss, α = 0.5 and β = 0.05.\n\nDuring training, only one sample is used for fast adaptation (i.e., both S i and S p only contain 1 sample), and 15 samples are used for evaluation (i.e., Q i contains 15 samples). The Adam optimizer  (Kingma and Ba 2014)  is employed with a learning rate of 0.00005. We train the model for 2000 episodes on a single NVIDIA GeForce RTX 4090 GPU.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments Experiment Settings",
      "text": "Dataset The performance of DSAML is evaluated using two publicly available DMER datasets, both of which provide V-A value annotations every 0.5 seconds, with all unstable annotations from the beginning to 15 seconds removed.\n\nThe first dataset is the DEAM dataset  (Aljanaki, Yang, and Soleymani 2017) , which includes 1744 45-second clips and 58 full-length songs with an average length of 4 minutes, all containing dynamic annotations from each annotator as illustrated in Figure  5 . In our experiments, we use the 58 full-length songs as the test set, with the remaining 1744 songs as the training set. Notably, 744 of the 45-second clips do not have annotator IDs, meaning we cannot determine the annotators for these songs. Therefore, in our proposed personalized task construction strategy, we only use 1000 songs as the training set.\n\nThe second dataset is the PMEmo dataset  (Zhang et al. 2018) , includes 794 songs of varying lengths and the mean and standard deviation of the dynamic annotations, without individual annotator data. Thus, our personalized task construction strategy cannot be applied to this dataset, and this dataset is only used for evaluating traditional DMER tasks. We discard 122 samples with song lengths less than 25 Objective Metrics. The performance of DSAML is evaluated using three objective metrics: Root Mean Square Error (RMSE), Pearson Correlation Coefficient (PCC), and Concordance Correlation Coefficient (CCC). Among these, the RMSE metric is used to measure the deviation between the predicted values and the actual values. A smaller RMSE value indicates higher prediction accuracy and lower error of the model. The PCC is used to assess the linear correlation between the predicted values and the actual values, with larger values indicating a stronger positive correlation. The CCC combines both precision and consistency aspects, serving as an improvement over the PCC. It not only considers the linear correlation but also takes into account the agreement between the means and variances of the predicted and actual values. A higher CCC value indicates better predictive performance of the model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Objective Experiment",
      "text": "This paper evaluates the performance of DSAML on PDMER and traditional DMER separately. For PDMER, the test set uses personalized annotations from all annotators as labels, referred to as the personalized task. In contrast, for traditional DMER, we follow conventional validation meth- ods where the test set uses the mean of annotations from all annotators as labels, referred to as the traditional DMER task.\n\nTraditional DMER Task In traditional DMER tasks, this paper follows the approach of other DMER studies by using the mean of all annotators as the label for validation. Table  1  presents the performance of our method compared to other methods on the DEAM and PMEmo datasets. It is observed that our method outperforms other methods across all metrics on the DEAM dataset, and also performs well on the PMEmo dataset. This is mainly because the emotional state at any moment in DMER may be influenced by distant moments, and our model architecture design with a dual-scale attention transformer can better capture long-term dependencies, thereby improving prediction accuracy. This is also validated in the ablation study. Moreover, the PCC value in the valence dimension of PMEmo dataset is lower than the DAMFF method, which may be due to the shorter music segments in this dataset, with fewer long-term dependencies. The trend of the valence dimension can be more easily predicted by DAMFF based on short-term dependencies, whereas our model is better at capturing long-term dependencies. However, our method still outperforms other methods on other metrics of PMEmo dataset, especially the more comprehensive CCC metric, demonstrating that our method is more effective in traditional DMER tasks.  PDMER Task In personalized tasks, the test set requires annotation data from each annotator. Since PMEmo dataset does not include these data, so personalized tasks are validated only on the DEAM dataset. Table  2  presents the performance of all methods in personalized tasks. Compared to other baseline methods, our method demonstrates superior performance across all metrics. This can be attributed to the effectiveness of our personalized strategy. Our personalized strategy retains personalized information in the dataset, and our task construction strategy enables the model to differentiate between different personalized perceptions. When facing new personalized tasks, our method can effectively utilize previous knowledge, thereby improving model performance in the PDMER task. Notably, incorporating MAML reduces the performance of other methods, as they learn the support set's personality during training but predict the query set with different personalized perceptions during the training phase, which makes it difficult for the models to fit.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Subjective Experiment",
      "text": "To further demonstrate the effectiveness of DSAML for the PDMER task in real-world scenarios, we conducted a subjective user experiment to evaluate the accuracy of personalized emotion prediction. The experiment involved 22 participants, including 11 females and 11 males. Participants were first required to listen to a 45-second song and adjust the initial V-A curve generated by the DSAML to best match their perception. Based on this adjustment, participants then listened to 10 songs sequentially, all of which come from the test set of the DEAM dataset. After listening to each song, they were asked to rank four V-A curves by perceptual match. The four V-A curves were generated by the following methods: (1) Ground truth, (2) DSAML, (3) DSAML without personalized strategy, and (4) DAMFF. The ranking was based on the consistency between the V-A curve and the participant's perception, with 1 indicating the most consistency and 4 indicating the least consistency.\n\nAs shown in Figure  6 , we analyzed the consistency ranking of different methods in both arousal and valence values, and conducted a significance analysis using the paired t-test. In the arousal dimension, the average ranking of DSAML is at the forefront, and it has a high level of significance compared to other methods, indicating that our model can predict the most consistent personalized emotion with the participants' perceptions. More notably, our model even outperforms the ground truth of the dataset, which to some extent indicates the importance of personalization in the real world for the DMER task. In the valence dimension, the average  ranking of our method is also significantly higher than that of the other methods except the ground truth. This may be due to valence's greater complexity, making it harder to predict than arousal. As Chua et al. pointed out, perceptions of arousal are primarily influenced by auditory information, while perceptions of valence can be influenced by both visual and auditory information  (Chua et al. 2022) . This indicates that the valence dimension is more challenging to predict using only audio information, which is consistent with our results in both objective and subjective experiments.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "In the ablation study, we evaluated the effectiveness of each component of our model. We conducted ablation studies on both traditional DMER and PDMER tasks. PDMER Task Table  4  presents the performance of our model with different components removed in the PDMER task. We can observe that the overall performance drops when the MAML or personalized task construction strategy is removed, indicating that these components are essential for the model's performance. However, it is worth noting that the performance of the model without MAML but with the personalized task construction strategy is worse than the model without both components. This is because the personalized task construction strategy alone leads to multiple different labels for the same sample in the training set, making it difficult for the model to fit during training. This also explains why other traditional DMER methods often train the model using mean labels, as using the original labels directly will lead to a decrease in model performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Traditional Dmer Task",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes the DSAML method for personalized dynamic music emotion recognition (PDMER). DSAML fuses features from a dual-scale feature extractor and captures both short and long-term dependencies using a dualscale attention transformer, improving the performance in traditional DMER. Moreover, a personalized strategy is proposed, which apply a novel task construction strategy into the MAML training process. The proposed task construction strategy divides tasks by annotators, ensuring consistent perception. Leveraging this strategy alongside meta-learning, DSAML can predict personalized perception of emotions with just one personalized annotation sample. Objective experimental results demonstrate that DSAML outperforms previous music emotion recognition methods in both traditional DMER and PDMER tasks. Furthermore, subjective experiments validate the effectiveness of DSAML in realworld scenarios.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The differences between traditional DMER and",
      "page": 1
    },
    {
      "caption": "Figure 1: (a). However, emotions are",
      "page": 2
    },
    {
      "caption": "Figure 2: Input Preprocessor",
      "page": 3
    },
    {
      "caption": "Figure 3: , the Imagebind Adapter",
      "page": 3
    },
    {
      "caption": "Figure 4: , it is possible that",
      "page": 3
    },
    {
      "caption": "Figure 2: The architecture of the DSAML model.",
      "page": 4
    },
    {
      "caption": "Figure 3: The architecture of the Imagebind Adapter",
      "page": 4
    },
    {
      "caption": "Figure 4: Attention Map",
      "page": 4
    },
    {
      "caption": "Figure 1: (a), the existing DMER ap-",
      "page": 4
    },
    {
      "caption": "Figure 5: In our experiments, we use the 58",
      "page": 5
    },
    {
      "caption": "Figure 5: Example of personalized annotations for the same",
      "page": 5
    },
    {
      "caption": "Figure 6: , we analyzed the consistency rank-",
      "page": 6
    },
    {
      "caption": "Figure 6: Average ranking of users using different methods",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance of different models on two datasets of the traditional DMER task.",
      "page": 6
    },
    {
      "caption": "Table 1: presents the performance of our method compared to other",
      "page": 6
    },
    {
      "caption": "Table 2: Performance of different models in the PDMER",
      "page": 6
    },
    {
      "caption": "Table 2: presents the per-",
      "page": 6
    },
    {
      "caption": "Table 3: Performance of ours on two datasets of the traditional DMER task.",
      "page": 7
    },
    {
      "caption": "Table 3: presents the perfor-",
      "page": 7
    },
    {
      "caption": "Table 4: presents the performance of our",
      "page": 7
    },
    {
      "caption": "Table 4: Performance of ours in the PDMER task.",
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "A Aljanaki",
        "Y.-H Yang",
        "M Soleymani"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "2",
      "title": "Modeling Speech Emotion Recognition via Image-Bind representations",
      "authors": [
        "A Chakhtouna",
        "S Sekkate",
        "A Abdellah"
      ],
      "year": "2024",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "3",
      "title": "Linear regression-based adaptation of music emotion recognition models for personalization",
      "authors": [
        "Y.-A Chen",
        "J.-C Wang",
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses",
      "authors": [
        "P Chua",
        "D Makris",
        "D Herremans",
        "G Roig",
        "K Agres"
      ],
      "year": "2022",
      "venue": "Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses",
      "arxiv": "arXiv:2202.10453"
    },
    {
      "citation_id": "5",
      "title": "The influence of music on emotions and cravings in clients in addiction treatment: a study of two clinical samples",
      "authors": [
        "G Dingle",
        "P Kelly",
        "L Flynn",
        "F Baker"
      ],
      "year": "2015",
      "venue": "The Arts in Psychotherapy"
    },
    {
      "citation_id": "6",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel",
        "S Levine"
      ],
      "year": "2017",
      "venue": "In International conference on machine learning"
    },
    {
      "citation_id": "7",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "R Girdhar",
        "A El-Nouby",
        "Z Liu",
        "M Singh",
        "K Alwala",
        "A Joulin",
        "I Misra"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Longrange transformers for dynamic spatiotemporal forecasting",
      "authors": [
        "J Grigsby",
        "Z Wang",
        "N Nguyen",
        "Y Qi"
      ],
      "year": "2021",
      "venue": "Longrange transformers for dynamic spatiotemporal forecasting",
      "arxiv": "arXiv:2109.12218"
    },
    {
      "citation_id": "9",
      "title": "A survey of music emotion recognition",
      "authors": [
        "D Han",
        "Y Kong",
        "J Han",
        "G Wang"
      ],
      "year": "2022",
      "venue": "Frontiers of Computer Science"
    },
    {
      "citation_id": "10",
      "title": "Multi-view neural networks for raw audio-based music emotion recognition",
      "authors": [
        "N He",
        "S Ferguson"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Symposium on Multimedia (ISM)"
    },
    {
      "citation_id": "11",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "12",
      "title": "Emotion-based AI music generation system with CVAE-GAN",
      "authors": [
        "C.-F Huang",
        "C.-Y Huang"
      ],
      "year": "2020",
      "venue": "2020 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)"
    },
    {
      "citation_id": "13",
      "title": "MusER: Musical Element-Based Regularization for Generating Symbolic Music with Emotion",
      "authors": [
        "S Ji",
        "X Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Are we there yet? A brief survey of Music Emotion Prediction Datasets, Models and Outstanding Challenges",
      "authors": [
        "J Kang",
        "D Herremans",
        "H Qi",
        "P Jurafsky"
      ],
      "year": "2018",
      "venue": "Sharp nearby, fuzzy far away: How neural language models use context",
      "arxiv": "arXiv:2406.08809"
    },
    {
      "citation_id": "15",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "16",
      "title": "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting",
      "authors": [
        "S Li",
        "X Jin",
        "Y Xuan",
        "X Zhou",
        "W Chen",
        "Y.-X Wang",
        "X Yan"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "An emotionbased personalized music recommendation framework for emotion improvement",
      "authors": [
        "Z Liu",
        "W Xu",
        "W Zhang",
        "Q Jiang"
      ],
      "year": "2023",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "18",
      "title": "Stacked convolutional and recurrent neural networks for music emotion recognition",
      "authors": [
        "M Malik",
        "S Adavanne",
        "K Drossos",
        "T Virtanen",
        "D Ticha",
        "R Jarina"
      ],
      "year": "2017",
      "venue": "Stacked convolutional and recurrent neural networks for music emotion recognition",
      "arxiv": "arXiv:1706.02292"
    },
    {
      "citation_id": "19",
      "title": "On first-order meta-learning algorithms",
      "authors": [
        "A Nichol",
        "J Achiam",
        "J Schulman"
      ],
      "year": "2018",
      "venue": "On first-order meta-learning algorithms",
      "arxiv": "arXiv:1803.02999"
    },
    {
      "citation_id": "20",
      "title": "DNN based music emotion recognition from raw audio signal",
      "authors": [
        "R Orjesek",
        "R Jarina",
        "M Chmulik",
        "M Kuba"
      ],
      "year": "2019",
      "venue": "2019 29th International Conference Radioelektronika (RADIOELEKTRONIKA)"
    },
    {
      "citation_id": "21",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "22",
      "title": "Prototypical networks for few-shot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Personalized music emotion classification via active learning",
      "authors": [
        "D Su",
        "P Fung"
      ],
      "year": "2012",
      "venue": "Proceedings of the second international ACM workshop on Music information retrieval with user-centered and multimodal strategies"
    },
    {
      "citation_id": "24",
      "title": "Learning to compare: Relation network for few-shot learning",
      "authors": [
        "F Sung",
        "Y Yang",
        "L Zhang",
        "T Xiang",
        "P Torr",
        "T Hospedales"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "25",
      "title": "Learning to learn: Introduction and overview",
      "authors": [
        "S Thrun",
        "L Pratt"
      ],
      "year": "1998",
      "venue": "Learning to learn"
    },
    {
      "citation_id": "26",
      "title": "Emotion-aware music recommendation",
      "authors": [
        "H Tran",
        "T Le",
        "A Do",
        "T Vu",
        "S Bogaerts",
        "B Howard"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Personalized music emotion recognition via model adaptation",
      "authors": [
        "J.-C Wang",
        "Y.-H Yang",
        "H.-M Wang",
        "S.-K Jeng"
      ],
      "year": "2012",
      "venue": "Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "28",
      "title": "Generalizing from a few examples: A survey on few-shot learning",
      "authors": [
        "Y Wang",
        "Q Yao",
        "J Kwok",
        "L Ni"
      ],
      "year": "2020",
      "venue": "ACM computing surveys (csur)"
    },
    {
      "citation_id": "29",
      "title": "Music emotion recognition: The role of individuality",
      "authors": [
        "Y.-H Yang",
        "Y.-F Su",
        "Y.-C Lin",
        "H Chen"
      ],
      "year": "2007",
      "venue": "Proceedings of the international workshop on Humancentered multimedia"
    },
    {
      "citation_id": "30",
      "title": "The PMEmo dataset for music emotion recognition",
      "authors": [
        "K Zhang",
        "H Zhang",
        "S Li",
        "C Yang",
        "L Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 acm on international conference on multimedia retrieval"
    },
    {
      "citation_id": "31",
      "title": "Dual Attention-Based Multi-Scale Feature Fusion Approach for Dynamic Music Emotion Recognition",
      "authors": [
        "L Zhang",
        "X Yang",
        "Y Zhang",
        "J Luo"
      ],
      "year": "2023",
      "venue": "ISMIR"
    },
    {
      "citation_id": "32",
      "title": "EMID: An Emotional Aligned Dataset in Audio-Visual Modality",
      "authors": [
        "J Zou",
        "J Mei",
        "G Ye",
        "T Huai",
        "Q Shen",
        "D Dong"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice"
    }
  ]
}