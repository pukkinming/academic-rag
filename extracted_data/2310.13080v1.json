{
  "paper_id": "2310.13080v1",
  "title": "From Multilingual Complexity To Emotional Clarity: Leveraging Commonsense To Unveil Emotions In Code-Mixed Dialogues",
  "published": "2023-10-19T18:17:00Z",
  "authors": [
    "Shivani Kumar",
    "Ramaneswaran S",
    "Md Shad Akhtar",
    "Tanmoy Chakraborty"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in codemixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained from a dedicated dialogue understanding module. Our comprehensive experimentation showcases the substantial performance improvement obtained through the systematic incorporation of commonsense in ERC. Both quantitative assessments and qualitative analyses further corroborate the validity of our hypothesis, reaffirming the pivotal role of commonsense integration in enhancing ERC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Dialogue serves as the predominant means of information exchange among individuals  (Turnbull, 2003) . Conversations, in their various forms such as text, audio, visual, or face-to-face interactions  (Hakulinen, 2009; Caires and Vieira, 2010) , can encompass a wide range of languages  (Weigand, 2010; Kasper and Wagner, 2014) . In reality, it is commonplace for individuals to engage in informal conversations with acquaintances that involve a mixture of languages  (Tay, 1989;    Sumirat, 2022). For instance, two native Hindi speakers fluent in English may predominantly converse in Hindi while occasionally incorporating English words. Figure  1  illustrates an example of such a dialogue between two speakers in which each utterance incorporates both English and Hindi words with a proper noun. This linguistic phenomenon, characterized by the blending of multiple languages to convey a single nuanced expression, is commonly referred to as code-mixing.\n\nWhile code-mixing indeed enhances our understanding of a statement  (Kasper and Wagner, 2014) , relying solely on the uttered words may not fully capture its true intent  (Thara and Poornachandran, 2018) . In order to facilitate better information assimilation, we often rely on various affective cues present in conversation, including emotions  (Poria et al., 2019; Dynel, 2009; Joshi et al., 2017) . Consequently, the task of Emotion Recognition in Conversation (ERC)  (Hazarika et al., 2018b)  has emerged and gained significant attention. ERC aims to establish a connection between individual utterances in a conversation and their corresponding emotions, encompassing a wide spectrum of possible emotional states. Despite the extensive exploration of ERC in numerous studies  (Hazarika et al., 2018a; Zhong et al., 2019a; Ghosal et al., 2019; Jiao et al., 2019; Shen et al., 2020; Li et al., 2020; Jiao et al., 2020; Hazarika et al., 2021; Tu et al., 2022; Yang et al., 2022; Ma et al., 2022) , the primary focus has been into monolingual dialogues, overlooking the prevalent practice of code-mixing. In this work, we aim to perform the task of ERC for code-mixed multi-party dialogues, thereby enabling the model-ing of emotion analysis in real-world casual conversations. To the best of our knowledge, there is no previous work that deals with ERC for code-mixed conversations, leading to a scarcity of available resources in this domain. As a result, we curate a comprehensive dataset comprising code-mixed conversations, where each utterance is meticulously annotated with its corresponding emotion label.\n\nThe elicited emotion in a conversation can be influenced by numerous commonly understood factors that may not be explicitly expressed within the dialogue itself  (Ghosal et al., 2020a) . Consider an example in which the phrase \"I walked for 20 kilometers\" evokes the emotion of pain. This association stems from the commonsense understanding that walking such a considerable distance would likely result in fatigue, despite it not being explicitly mentioned. Consequently, capturing commonsense information alongside the dialogue context becomes paramount in order to accurately identify the elicited emotion. To address this, we propose incorporating commonsense for solving the task of ERC. However, the most popular commonsense graphs, such as ConceptNet  (Speer et al., 2017)  and COMET  (Bosselut et al., 2019)  are made for English, are known to work for the English language  (Zhong et al., 2021; Ghosal et al., 2020b) , and are not explored for code-mixed input. To overcome this challenge, we develop a pipeline to utilize existing English-based commonsense knowledge graphs to extract relevant knowledge for codemixed inputs. Additionally, we introduce a clever fusion mechanism to combine the dialogue and commonsense features for solving the task at hand. In summary, our contributions are fourfold 1  :\n\n1. We explore, for the first time, the task of ERC for multi-party code-mixed conversations. 2. We propose a novel code-mixed multi-party conversation dataset, E-MASAC, in which each discourse is annotated with emotions. 3. We develop COFFEE 2  , a method to extract commonsense knowledge from English-based commonsense graphs given code-mixed input and fuse it with dialogue context efficiently. 4. We give a detailed quantitative and qualitative analysis of the results obtained and examine the performance of the popular large language models, including ChatGPT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition. Earlier studies in emotion analysis  (Ekman, 1992; Picard, 1997; Cowen and Keltner, 2017; Mencattini et al., 2014; Zhang et al., 2016; Cui et al., 2020)  dealt with only standalone inputs, which lack any contextual information. To this end, the focus of emotion detection shifted to conversations, specifically ERC. While ERC was solved using heuristics and standard machine learning techniques initially  (Fitrianie et al., 2003; Chuang and Wu, 2004; Li et al., 2007) , the trend has recently shifted to employing a wide range of deep learning methods  (Hazarika et al., 2018a; Zhong et al., 2019a; Li et al., 2020; Ghosal et al., 2019; Jiao et al., 2020; Hazarika et al., 2021; Shen et al., 2020; Poria et al., 2017b; Jiao et al., 2019; Tu et al., 2022; Yang et al., 2022; Ma et al., 2022) .\n\nEmotion and commonsense. Given the implicit significance of commonsense knowledge in the process of emotion identification, researchers have delved into the integration of commonsense for the purpose of emotion recognition. In scenarios involving standalone text, where the contextual information is relatively limited, studies propose the utilization of carefully curated latent commonsense concepts that can seamlessly blend with the text  (Balahur et al., 2011; Zhong et al., 2021; Chen et al., 2022) . However, in situations where the context of the text spans longer sequences, like dialogues, it becomes essential to capture it intelligently. Many studies explore the task of ERC with commonsense fusion using ConceptNet  (Speer et al., 2017; Zhong et al., 2021) , Atomic triplets  (Sap et al., 2019; Nie et al., 2023) , and the COMET graph  (Bosselut et al., 2019; Ghosal et al., 2020b; Li et al., 2021) .\n\nEmotion and code-mixing. Existing research on emotion analysis for code-mixed language primarily focuses on standalone social media texts  (Sasidhar et al., 2020; Ilyas et al., 2023; Wadhawan and Aggarwal, 2021)  and reviews  (Suciati and Budi, 2020; Zhu et al., 2022) . While aspects such as sarcasm  (Kumar et al., 2022a,b) , humour  (Bedi et al., 2023) , and offense  (Madhu et al., 2023)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The E-Masac Dataset",
      "text": "A paucity of datasets exists for code-mixed conversations, making tasks on code-mixed dialogues scarce. Nevertheless, a recent dataset, MASAC  (Bedi et al., 2023) , compiled by extracting dialogues from an Indian TV series, contains sarcastic and humorous Hindi-English code-mixed multiparty instances. We extract dialogues from this dataset and perform annotations for the task of ERC to create E-MASAC. The resultant data contains a total of 8, 607 dialogues constituting of 11, 440 utterances. Data statistics are summarised in Table  1 . Emotion distribution based on the annotations of the three sets 3  is illustrated in Figure  2 .\n\nEmotion annotation. Given, as input, a sequence of utterances forming a dialogue, D = {(s 1 , u 1 ), (s 2 , u 2 ), • • • , (s n , u n )}, the aim here is to assign an appropriate emotion, e i , for each utterance, u i , uttered by speaker s j . The emotion e i should come out of a set of possible emotions, E.\n\nFollowing the standard work in ERC for the English language, we use Eckman's emotions as our set of possible emotions as mentioned in Section 3, E = {anger, fear, disgust, sadness, joy, surprise, contempt, neutral}. Each emotion, along with its definition and example, is illustrated in Appendix A.1. We ask three annotators 4  (a, b, c) to annotate each utterance, u i , with the emotion they find most suitable for it, e a i such that e a i ∈ E. A majority voting is done among the three annotations (e a i , e b i , e c i ) to select the final gold truth annotation,  e i . Any discrepancies are resolved by a discussion among the annotators; however, such discrepancies are rare. We calculate the inter-annotator agreement, using Kriprendorff's Alpha score  (Krippendorff, 2011) , between each pair of annotators, α ab = 0.84, α bc = 0.85, and α ac = 0.85. To find out the overall agreement score, we take the average score, α = 0.85.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Methodology: Coffee",
      "text": "As mentioned in Section 1, the manifestation of emotional concepts within an individual during a conversation is not solely influenced by the dialogue context, but also by the implicit knowledge accumulated through life experiences. This form of knowledge can be loosely referred to as commonsense. In light of this, we present an efficient yet straightforward methodology for extracting pertinent concepts from a given commonsense knowledge graph in the context of code-mixed inputs. Additionally, we introduce a clever strategy to seamlessly incorporate the commonsense features with the dialogue representation obtained from a backbone architecture dedicated to dialogue understanding. Figure  3  outlines our proposed approach, COFFEE while each of the intermediate modules is elucidated in detail below.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dialogue Understanding Backbone (Dub)",
      "text": "For input containing long contextual history, such as a dialogue, it becomes crucial to capture and comprehend the entire progression leading up to the present statement. Consequently, an effective dialogue understanding architecture which gives us a concrete dialogue representation is required.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Commonsense Extraction (Ce)",
      "text": "While the conversational context provides insights into the participants and the topic of the dialogue, the comprehension of implicit meanings within statements can be greatly facilitated by incorporating commonsense information. Therefore, in order to capture this valuable commonsense knowledge, we employ the COMET graph  (Bosselut et al., 2019) , which has been trained on ATOMIC triplets  (Sap et al., 2019) , to extract relevant commonsense information for each dialogue instance. However, it is worth noting that the COMET graph is pretrained using triplets in the English language, making it particularly effective for English inputs  (Ghosal et al., 2020a) . Given that our input consists of a mixture of English and Hindi, we have devised a specialized knowledge extraction pipeline to tackle this challenge. The entire process of obtaining commonsense knowledge for a given code-mixed textual input is shown in Figure  3    Hindi words, the initial task is to determine the language of each word to appropriately handle different languages in the most suitable way. 2. Transliteration: The identified Hindi language words are transliterated to Devanagari script from roman script so that language-specific preprocessing can be applied to them. 3. Text Processing: The next step is to preprocess the text. This step involves converting text to lowercase and removal of non-ASCII characters and stopwords. The resultant text is considered important or 'topic specifying' for the text. 4. Translation: Since COMET is trained for monolingual English, the query can only have English terms. Therefore, we translate the Devanagari Hindi 'topics' back to romanised English. 5. Querying COMET: Finally, all the 'topics' together are sent as a query to the COMET graph, and all possible relations are obtained. COMET provides us with a vast array of effecttypes corresponding to the input text. Specifically, it provides us with information such as oEffect,  oReact, oWant, xAttr, xEffect, xIntent, xNeed, xReact, xWant.  Refer Table  2  for the description of each of these values. We carefully select the relevant attributes (c.f. Section 6.2) from the extracted pairs and encode them using the BERT model  (Devlin et al., 2018) . The representation obtained from BERT acts as our commonsense representation. Formally, D cs = CE(D), such that D cs ∈ R m×d where m is the length of the commonsense information, and d is the vector dimension obtained from the BERT model. After we obtain the commonsense representation D cs , we need to integrate it with the dialogue representation D c . Consequently, we devise a sophisticated fusion mechanism as described in the following section.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Commonsense Fusion (Cf)",
      "text": "Several studies discuss knowledge fusion, particularly in the context of multimodal fusion  (Poria et al., 2017a) , where the most successful approaches often employ traditional dot-productbased cross-modal attention  (Bose et al., 2021; Ma and Ma, 2022) . However, the traditional attention scheme results in the direct interaction of the fused information. As each fused information can be originated from a distinct embedding space, a direct fusion may be prone to noise and may not preserve maximum contextual information in the final representations. To address this, taking inspiration from  Yang et al. (2019) , we propose to fuse commonsense knowledge using a context-aware attention mechanism. Specifically, we first generate commonsense conditioned key and value vectors and then perform a scaled dot-product attention using them. We elaborate on the process below.\n\nGiven the dialogue representation D c obtained by a dialogue understanding backbone architecture, we calculate the query, key, and value vectors Q, K, and V ∈ R n×d , respectively, as outlined in Equation 1 where W Q , W K , and W V ∈ R d×n are learnable parameters, and n and d denote the maximum sequence length of the dialogue and dimensionality of the backbone architecture, respectively.\n\nOn the other hand, with the commonsense vector, D cs , we generate commonsense infused key and value vectors K and V , respectively as outlined in Equation  2 , where U k and U v ∈ R d×d are learnable matrices. A scalar λ ∈ R n×1 is employed to regulate the extent of information to integrate from the commonsense knowledge and the amount of information to retain from the dialogue context. λ is a learnable parameter learnt using Equation  3 , where W k 1 , W k 2 , W v 1 , and W v 2 ∈ R d×1 are trained along with the model.\n\nFinally, the commonsense knowledge infused vectors K and V are used to compute the traditional scaled dot-product attention. 6 Experiments and Results",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dialogue Understanding Backbone",
      "text": "As we mentiond earlier, existing approaches for ERC predominantly concentrate on the English language. Nonetheless, we incorporate two state-ofthe-art techniques for ERC using English datasets and leverage four established Transformer-based methodologies as our foundation systems to address the ERC task.\n\nBERT  (Devlin et al., 2018 ) is a pre-trained language model that utilizes a Transformer architecture and bidirectional context to understand the meaning and relationships of words in a sentence. RoBERTa  (Liu et al., 2019)  is an extension of BERT that improves its performance utilizing additional training techniques such as dynamic masking, longer sequences, and more iterations. mBERT 5  (multilingual BERT) is a variant  Although BERT and RoBERTa are trained using monolingual English corpus, we use them for romanised code-mixed input, anticipating that finetuning will help the models grasp Hindi-specific nuances (c.f. Appendix A.2). To ensure a fair comparison, we also include multilingual models such as mBERT and MURIL in our analysis. Additionally, since we are dealing with the task of ERC, we consider two state-of-the-art baseline architectures in this domain for monolingual English dialogues, namely CoMPM and DialogXL and two state-ofthe-art baseline that incorporates commonsense for ERC -KET, and COSMIC.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment Setup And Evaluation Metric",
      "text": "The COMET graph gives us multiple attributes for one input text (c.f. Table  2 ). However, not all of them contributes towards the emotion elicited in the speaker. Consequently, we examine the correlation between the extracted commonsense attributes with emotion labels in our train instances. We use BERT to obtain representation for each commonsense attribute and find out their correlation with the emotion labels. We show this correlation in Fig-  ure 4 . As can be seen, 'xWant' is most positively correlated with the emotion labels, and 'oReact' is most negatively correlated. Consequently, we select the attributes 'xWant', and 'oReact' as commonsense. Further, for evaluating the performance, we select weighted F1 score as our metric of choice to handle the imbalanced class distribution of emotions present in our dataset (c.f. Figure  2 ).   to incorporate commonsense knowledge. Notably, in the absence of commonsense, RoBERTa and Di-alogXL outperform the other systems. However, it is intriguing to observe that mBERT and MURIL, despite being trained on multilingual data, do not surpass the performance of BERT, RoBERTa, or DialogXL. We provide a detailed analysis regarding this in Appendix A.2. Further, when commonsense is included as part of the input using the COFFEE approach, all systems exhibit improved performance. The F1 scores corresponding to individual emotions show a proportional relationship with the quantity of data samples available for each specific emotion, as anticipated within a deep learning architecture. The neutral emotion achieves the highest performance, followed by joy and surprise, as these classes possess a greater number of data samples (see Table  2 ). Conversely, the minority classes such as contempt and disgust consistently obtain the lowest scores across almost all systems. Furthermore, we can observe from the table that the existing strategies of commonsense fusion perform poorly when compared with the COFFEE method. The loss in performance can be attributed to two aspects of the comparative system -KET uses NRC_VAD  (Mohammad, 2018) , which is an English-based lexicon containing VAD scores, i.e., valence, arousal, and dominance scores, to gather words for which knowledge is to be retrieved. Since our input is code-mixed with the matrix language as Hindi, using only the English terms makes the KET approach ineffective. In contrast, although COSMIC uses the COMET graph, it uses the raw representations obtained from the commonsense graph and concatenates them with the utterance representations obtained from the GRU architecture. Since we use the generated natural language commonsense with the smart fusion method, we hypothesize that our model is able to capture and utilize this knowledge effectively. Additionally, we performe a T-test on our results to check the statistical significance of our performance gain and obtained a p-value of 0.0321 for our RoBERTa model which, being less than 0.05, makes our results statistically significant.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Quantitative Analysis",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "Fusion methods We investigate the effectiveness of COFFEE in capturing and incorporating commonsense information. To evaluate different mechanisms for integrating this knowledge into the dialogue context, we present the results in Table  4 . Initially, we explore a straightforward method of concatenating the obtained commonsense knowledge with the dialogue context and passing it through the RoBERTa model. Interestingly, this simple concatenation leads to a decline in the performance of emotion recognition, suggesting that the introduced commonsense information may act as noise in certain cases. This outcome can be attributed to the inherent nature of some utterances, where external knowledge may not be necessary to accurately determine the expressed emotion. For instance, consider the sentence \"Aaj me sad hun\" (\"I am sad today\"), which can be comprehended without relying on commonsense information to identify the emotion as sadness. In such scenarios, enforcing additional information may disrupt the model's behavior, resulting in suboptimal performance. Conversely, by allowing the model the flexibility to decide when and to what extent to incorporate commonsense knowledge, as demon-strated by the attention and COFFEE approaches, we observe an improvement in system performance, with COFFEE yielding the most favorable outcomes.\n\nEffect of language In code-mixing, the input amalgamates two or more languages, often with one language being the dominant one, called the matrix language, while others act as embedding languages. The foundation of grammatical structure comes from the matrix language (Hindi in our case), and solely relying on the embedding language (English in our case) can lead to a decline in the model's performance. On the flip side, the embedding language plays a vital role in capturing accurate contextual details within the input. Therefore, confining ourselves to only the matrix language should also result in a drop in performance. To verify this hypothesis, the third and fourth row of Table  4  shows the performance of the COFFEE methodology, using the RoBERTa model, when we use only English (the embedding language) and only Hindi (the matrix language) in our input. The results reinforce our hypothesis, where the usage of only embedding language (English only) deteriorates the model performance extensively, while the sole use of matrix language (Hindi only) also hampers the performance when compared to the system that uses both the languages.\n\nCOMET attributes We explore the utilization of various COMET attributes as our commonsense information. The last three rows in Table  4  demonstrate the outcomes when we integrate the two most correlated attributes, xWant and oReact with the RoBERTa backbone model using COFFEE. It is evident that the individual consideration of these attributes does not significantly enhance the performance of ERC compared to when they are combined. Additionally, Table  5  presents the weighted F1 scores achieved by the RoBERTa model when each commonsense attribute is incorporated individually using COFFEE. These results align well with the observed correlation between the attributes and the corresponding emotion labels in Figure  4 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "A thorough quantitative analysis, detailed in the previous section, revealed that the integration of commonsense knowledge enhances the performance of all systems under examination. However, to gain a deeper understanding of the underlying reasons for this improvement, we conduct a comprehensive qualitative analysis, comprising of  confusion matrices and subjective evaluations.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Confusion Matrix",
      "text": "Given the superior performance of the RoBERTa model we conduct an examination of its confusion matrices with and without commonsense fusion, as shown in Table  6 . We observe that the RoBERTa model with COFFEE integration achieves a higher number of true positives for most emotions. However, it also exhibits a relatively higher number of false negatives when compared with its standard variant, particularly for the neutral class. This observation suggests that the commonsense-infused model excels in recall but introduces some challenges in terms of precision, thereby presenting an intriguing avenue for future research. Additionally, we notice a heightened level of confusion between neutral and joy emotions, primarily due to their prevalence in the dataset. Both models, however, demonstrate the least confusion between the disgust and surprise emotions, indicating their distinguishable characteristics.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "For the purpose of illustration, we select a single instance from the test set of E-MASAC and present, with it, the ground-truth and the predicted labels for the task of ERC for the best performing RoBERTa model with and without using the COFFEE approach in Table  7 . It can be observed that the inclusion of commonsense knowledge in the model significantly reduces errors. Comparatively, the variant of RoBERTa that does not incorporate commonsense knowledge makes errors in 5 out of 9 instances, whereas the variant utilizing commonsense knowledge, using COFFEE, misclassifies only 3 utterances. Within the test set, numerous similar instances exist where the commonsense-infused variant outperforms its counterpart due to the implicit information embedded in the utterances.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Chatgpt And Code-Mixing",
      "text": "Considering the emergence and popularity of Chat-GPT, it becomes imperative to conduct an analysis of it for the task of ERC in code-mixed dialogues.\n\nAlthough ChatGPT exhibits remarkable performance in a zero-shot setting across various tasks and scenarios, it is important to note its shortcomings, particularly when dealing with code-mixed input. To evaluate its performance, we extract instances from E-MASAC and engage ChatGPT in identifying the emotions evoked within the dialogues. To accomplish this, we construct a prompt that includes a potential set of emotions along with the code-mixed dialogue as input. Specifically, the prompt used is as follows: \"Out of the following emotion set : {Anger, Contempt, Disgust, Fear, Joy, Neutral, Sadness, Sur-prise}, find out the emotion for the last utterance given the following conversation. <Conv>\" While ChatGPT successfully discerned emotions in short and straightforward conversations, it faltered in identifying the appropriate emotion as the dialogue context expanded, sometimes even spanning more than three utterances. We present more details and example instances in Appendix A.3.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we investigated the task of Emotion Recognition in Conversation (ERC) for code-mixed dialogues, the first effort in its kind. We curate an emotion lad dialogue dataset for Hindi-English conversations and proposed a new methodology that leverages existing commonsense knowledge graphs to extract pertinent commonsense concepts for code-mixed inputs. The extracted common-sense is integrated into a backbone architecture through a novel fusion technique that uses context aware attention mechanism. Our findings indicated that the incorporation of commonsense features significantly enhances the performance of ERC, as evidenced both quantitatively and qualitatively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "This work marks the inception of emotion identification in code-mixed conversations, opening up a plethora of research possibilities. One avenue for future studies is training the tokenizer specifically for code-mixed input, which can enhance the model's performance. Moreover, it is worth noting that our dataset comprises dialogues extracted from a situational comedy (sit-com), which may not encompass all real-world scenarios, potentially limiting the generalizability of our model to unseen situations. Consequently, future investigations can delve into diversifying the dataset to incorporate a broader range of contexts. Furthermore, conducting in-depth analysis to identify the optimal combination of commonsense attributes from COMET for ERC would shed further light on improving the system's performance. Although these avenues were beyond the scope of our current study, they present exciting prospects for future research. zero-shot scenarios across diverse tasks, it is essential to acknowledge its limitations, especially when confronted with code-mixed input. To assess its efficacy, we select instances from the E-MASAC and task ChatGPT with the identification of emotions elicited within the dialogues. By subjecting Chat-GPT to this evaluation, we gain insights into its effectiveness for the ERC task. To accomplish this, we construct a prompt that includes a potential set of emotions along with the code-mixed dialogue as input. Specifically, the prompt is as follows: \"Out of the following emotion set : {Anger, Contempt, Disgust, Fear, Joy, Neutral, Sadness, Sur-prise}, find out the emotion for the last utterance given the following conversation. <Conv>\" Although ChatGPT demonstrated proficiency in discerning emotions within concise and uncomplicated conversations, its performance waned when faced with the challenge of identifying the accurate emotion as the dialogue context extended, occasionally encompassing more than three utterances. Figure  6  shows four such instances and Table  9  shows their translation for easy understanding.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example of a code-mixed dialogue between",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates an example of",
      "page": 1
    },
    {
      "caption": "Figure 2: Emotion annotation.",
      "page": 3
    },
    {
      "caption": "Figure 2: Emotion distribution for E-MASAC.",
      "page": 3
    },
    {
      "caption": "Figure 3: outlines our proposed approach,",
      "page": 3
    },
    {
      "caption": "Figure 3: A schematic diagram of COFFEE. The Commonsense Extraction (CE) module takes a code-mixed input and",
      "page": 4
    },
    {
      "caption": "Figure 3: and is compre-",
      "page": 4
    },
    {
      "caption": "Figure 4: Correlation between different commonsense",
      "page": 6
    },
    {
      "caption": "Figure 5: illustrates the PCA",
      "page": 13
    },
    {
      "caption": "Figure 5: Embedding space for RoBERTa before and",
      "page": 13
    },
    {
      "caption": "Figure 6: shows four such instances and Table 9",
      "page": 13
    },
    {
      "caption": "Figure 6: Screenshots of ChatGPT responses when prompted to provide emotions for the last utterance in the",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Uttlen": "Avg Max"
        },
        {
          "Column_1": "8506\n45\n56",
          "Column_2": "8506\n1354\n1580",
          "Column_3": "3.60\n4.13\n4.32",
          "Uttlen": "10.82 113\n10.12 218\n10.61 84"
        },
        {
          "Column_1": "8607",
          "Column_2": "11440",
          "Column_3": "12.05",
          "Uttlen": "31.55 415"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Commonsense effect-types returned by the",
      "data": [
        {
          "Fusion Gate (FG)\nDc": "^\nDcs"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: ). However, not all of",
      "data": [
        {
          "Anger Contempt Disgust Fear Joy neutral Sadness Surprise": "0.23 0.18 0.11 0.20 0.45 0.54 0.16 0.32\n0.26 0.21 0.16 0.06 0.47 0.57 0.12 0.34\n0.10 0.11 0.00 0.11 0.23 0.50 0.13 0.08\n0.24 0.22 0.07 0.00 0.42 0.51 0.06 0.23\n0.10 0.12 0.00 0.00 0.44 0.57 0.02 0.00\n0.25 0.09 0.07 0.17 0.43 0.59 0.17 0.28"
        },
        {
          "Anger Contempt Disgust Fear Joy neutral Sadness Surprise": "0.24(↑0.01) 0.2(↑0.02) 0.12(↑0.01) 0.19(↓0.01) 0.46(↑0.01) 0.56(↑0.02) 0.18(↑0.02) 0.35(↑0.03)\n0.29(↑0.03) 0.24(↑0.03) 0.18(↑0.02) 0.10(↑0.04) 0.49(↑0.02) 0.61(↑0.04) 0.18(↑0.06) 0.34(↑0.00)\n0.11(↑0.01) 0.13(↑0.02) 0.04(↑0.04) 0.12(↑0.01) 0.24(↑0.01) 0.51(↑0.01) 0.12(↓0.01) 0.10(↑0.02)\n0.26(↑0.02) 0.21(↓0.01) 0.10(↑0.03) 0.01(↑0.01) 0.46(↑0.04) 0.52(↑0.01) 0.08(↑0.02) 0.22(↓0.01)\n0.11(↑0.01) 0.14(↑0.02) 0.02(↑0.02) 0.02(↑0.02) 0.45(↑0.01) 0.56(↓0.01) 0.03(↑0.01) 0.10(↑0.01)\n0.26(↑0.01) 0.11(↑0.02) 0.10(↑0.03) 0.19(↑0.02) 0.44(↑0.01) 0.59(↑0.00) 0.20(↑0.03) 0.31(↑0.03)"
        },
        {
          "Anger Contempt Disgust Fear Joy neutral Sadness Surprise": "0.14(↓0.15) 0.11(↓0.13) 0.09(↓0.09) 0(↓0.10) 0.34(↓0.15) 0.41(↓0.20) 0.08(↓0.10) 0.19(↓0.15)\n0.21(↓0.08) 0.18(↓0.06) 0.15(↓0.03) 0.03(↓0.07) 0.39(↓0.10) 0.49(↓0.12) 0.13(↓0.05) 0.27(↓0.07)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: ). Conversely, the minority catenation leads to a decline in the performance",
      "data": [
        {
          "Anger Contempt Disgust Fear Joy neutral Sadness Surprise": "0.26 0.21 0.16 0.06 0.47 0.57 0.12 0.34"
        },
        {
          "Anger Contempt Disgust Fear Joy neutral Sadness Surprise": "0.22(↓0.04) 0.19(↓0.02) 0.15(↓0.01) 0.04(↓0.02) 0.44(↓0.03) 0.52(↓0.05) 0.09(↓0.03) 0.31(↓0.03)\n0.27(↑0.01) 0.21(↑0.00) 0.16(↑0.00) 0.08(↑0.02) 0.48(↑0.01) 0.59(↑0.02) 0.11(↓0.01) 0.33(↓0.01)\n0.11(↓0.15) 0.09(↓0.12) 0.01(↓0.15) 0(↓0.06) 0.16(↓0.31) 0.24(↓0.33) 0.02(↓0.10) 0.11(↓0.23)\n0.20(↓0.06) 0.15(↓0.06) 0.12(↓0.04) 0.02(↓0.04) 0.36(↓0.11) 0.53(↓0.04) 0.12(↑0.00) 0.29(↓0.05)\n0.26(↑0.00) 0.22(↑0.01) 0.15(↓0.01) 0.04(↑0.02) 0.47(↑0.00) 0.59(↑0.02) 0.16(↑0.04) 0.33(↓0.01)\n0.27(↑0.01) 0.24(↑0.03) 0.17(↑0.01) 0.07(↑0.01) 0.43(↓0.04) 0.59(↑0.02) 0.18(↑0.06) 0.33(↓0.01)\n0.29(↑0.03) 0.24(↑0.03) 0.18(↑0.02) 0.10(↑0.04) 0.49(↑0.02) 0.61(↑0.04) 0.18(↑0.06) 0.34(↑0.00)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Maya\nMonisha\nSahil\nMaya\nMonisha\nMaya\nMonisha\nMaya\nMonisha",
          "Column_2": "KhatamhogayaSahilit’sover!(It’sover,Sahil!)\nMummyji,tissuepaperkeaur2boxeshai,laatihun.(Mummyji,Ihavetwomoreboxesoftissuepaper,\nI’llbringthem.)\nMonisha,momtissuepaperkibaatnhikarrhihai... (Monisha,momisnottalkingabouttissue\npaper...)\nMylife!Merizindagi!Khatamhogayihai.CanyouimagineSahil?UssRitasetohMonishazyada\nachihai.Canyouimagine?(Mylife!Mylife!It’sover.Canyouimagine,Sahil?Monishaisbetter\nthanthatRita.Canyouimagine?)\nMeinkyaitniburihunmummyji?(AmIthatbad,mummyji?)\nHaanbeta. LekinwoRita! Ohmygod! Saansletihaitohbhicheekhsunaidetihai. Jablogoko\npatachalegakeroseshneloudspeakerseshaadikihai?!(Yes,dear.ButthatRita!Ohmygod!Even\nwhenshebreathes,shemakesasound. WhenpeoplefindoutthatRoseshgotmarriedthrougha\nloudspeaker?!)\nLogokopatachalgayamummyji...(Peoplefoundout,mummyji...)\nWhatdoyoumean?!(Whatdoyoumean?!)\nWosaritaauntykaphoneaayathana...(Saritaauntycalled,right...)",
          "Gold": "sadness\nneutral\nneutral\nsadness\nsadness\nsadness\nneutral\nfear\nneutral",
          "w/oCS": "sadness\njoy\nsadness\nsadness\nsadness\ndisgust\nsurprise\nfear\nsurprise"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Description": "Ariseswhenthetargetisblockedfrompursuingagoaland/ortreatedunfairly\nFeelingofdislikeforandsuperiority(usuallymorally)overanotherperson,\ngroupofpeople,and/ortheiractions\nFeelingofaversiontowardssomethingoffensive\nArisingfromconnectionorsensorypleasure\nAriseswiththethreatofharm,eitherphysical,emotional,orpsychological,\nrealorimagined\nResultingfromthelossofsomeoneorsomethingimportant.Whatcauses\none’ssadnessvariesgreatlybasedonpersonalandculturalnotionsofloss\nAriseswhenthetargetencounterssuddenandunexpectedsounds,move-\nments,situationsandactions\nWhenthetargetisexperiencingnoneoftheemotions"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 9: Examples of dialogues we queried ChatGPT along with their English translations and emotion labels",
      "data": [
        {
          "Code-mixedDialogues": "Rosesh:PaniKiboondebanimerigehne,bagichekikhushbuka\nlibaaspehnemeisarhadokepaaslehrau,sanansanan,meivaayu,\nhawa,meizameen,pavan.\nIndravardhan:Roseshyekavitatunenahinlikhihai.\nRosesh:Arey,aapkokaisepatachala?\nIndravardhan:Kyunkiyeacchihai!Sachbatakisnelikhi?!",
          "EnglishTranslation": "Rosesh:Waterdropletshaveturnedintomyjewelry,Iadornthe\nattireofthegarden’sfragrance,Iswayneartheborders,swaying\ngently.Iamtheair,thebreeze,Iamtheearth,thewind.\nIndravardhan:Rosesh,youdidn’twritethispoem.\nRosesh:Oh,howdidyouknow?\nIndravardhan:Becauseit’sgood!Tellmethetruth,whowrote\nit?!"
        },
        {
          "Code-mixedDialogues": "Maya:KhatamhogayaSahil!it’sover!\nMonisha:Mummyjitissuepaperkeaurdoboxhai.Laatihun...\nSahil:Monisha,momtissuepaperkibaatnhikarrhihai.\nMaya: Mylife! Merizindagi! Khatamhogayihai! Canyou\nimagineSahil?UssRitasetohMonishazyadaachihai!Canyou\nimagine?\nMonisha:Meinkyaitniburihunmummyji?\nMaya:Haanbeta,lekinwoRita!Ohmygod!Saansletihaitoh\nbhicheeksunaidetihai. Jablogokopatachalegakeroseshne\nloudspeakerseshaadikihai...\nMonisha:Logokopatachalgayamummyji...\nMaya:Whatdoyoumean?\nMonisha:Wosaritaauntykaphoneaayathana...",
          "EnglishTranslation": "Maya:It’sover,Sahil!\nMonisha:Mummyji,Ihavetwomoreboxesoftissuepaper.I’ll\nbringthem.\nSahil:Monisha,Momisnottalkingabouttissuepaper...\nMaya: Mylife! Mylife! It’sover. Canyouimagine,Sahil?\nMonishaisbetterthanthatRita.Canyouimagine?\nMonisha:AmIthatbad,mummyji?\nMaya: Yes,dear. ButthatRita! Ohmygod! Evenwhenshe\nbreathes,shemakesasound.WhenpeoplefindoutthatRosesh\ngotmarriedthroughaloudspeaker?!\nMonisha:Peoplefoundout,mummyji...\nMaya:Whatdoyoumean?!\nMonisha:Saritaauntycalled,right..."
        },
        {
          "Code-mixedDialogues": "Baldev:HiMaya.\nMaya:Hellobaldev!Whatapleasantsurprise!\nBaldev:SorrymainBinaphonekiaGaya.\nIndravardhan:Noproblem. Hamarekelewalahainavahbhi\nbagairboleaadhamaktaHaiKabhiKabhi.",
          "EnglishTranslation": "Baldev:HiMaya.\nMaya:HelloBaldev!Whatapleasantsurprise!\nBaldev:Sorry,Icamewithoutcalling.\nIndravardhan:Noproblem.Ourbananaselleralsocomeswith-\noutsayinganythingandsometimessurprisesus."
        },
        {
          "Code-mixedDialogues": "Indravardhan:GoodmorningMaya\nMaya:Goodmorningsweetie\nIndravardhan:Nashtanashtanashta\nMaya:VitthalSabkonashtado\nIndravardhan:Maindudhhargisnahinpiyunga.MayaMujhe\ndiariaHaiSubahSe6barJachukaHunMain",
          "EnglishTranslation": "Indravardhan:Goodmorning,Maya.\nMaya:Goodmorning,sweetie.\nIndravardhan:Breakfast,breakfast,breakfast.\nMaya:Vitthal,servebreakfasttoeveryone.\nIndravardhan:Iwillneverdrinkmilk.Maya,Ihavehaddiarrhea\nsixtimessincemorning."
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotinet: A knowledge base for emotion detection in text built on the appraisal theories",
      "authors": [
        "Alexandra Balahur",
        "M Jesús",
        "Andrés Hermida",
        "Rafael Montoyo",
        "Munoz"
      ],
      "year": "2011",
      "venue": "Natural Language Processing and Information Systems: 16th International Conference on Applications of Natural Language to Information Systems, NLDB 2011"
    },
    {
      "citation_id": "2",
      "title": "Multi-modal sarcasm detection and humor classification in code-mixed conversations",
      "authors": [
        "Manjot Bedi",
        "Shivani Kumar",
        "Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2021.3083522"
    },
    {
      "citation_id": "3",
      "title": "Two headed dragons: multimodal fusion and cross modal transactions",
      "authors": [
        "Rupak Bose",
        "Shivam Pande",
        "Biplab Banerjee"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "4",
      "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "Antoine Bosselut",
        "Hannah Rashkin",
        "Maarten Sap",
        "Chaitanya Malaviya",
        "Asli Celikyilmaz",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1470"
    },
    {
      "citation_id": "5",
      "title": "Conversation types",
      "authors": [
        "Luís Caires",
        "Hugo Vieira"
      ],
      "year": "2009",
      "venue": "Theoretical Computer Science",
      "doi": "10.1016/j.tcs.2010.09.010"
    },
    {
      "citation_id": "6",
      "title": "Incorporating structured emotion commonsense knowledge and interpersonal relation into context-aware emotion recognition",
      "authors": [
        "Jing Chen",
        "Tao Yang",
        "Ziqiang Huang",
        "Kejun Wang",
        "Meichen Liu",
        "Chunyan Lyu"
      ],
      "year": "2022",
      "venue": "Applied Intelligence",
      "doi": "10.1007/s10489-022-03729-4"
    },
    {
      "citation_id": "7",
      "title": "Multimodal emotion recognition from speech and text",
      "authors": [
        "Ze-Jing Chuang",
        "Chung-Hsien Wu"
      ],
      "year": "2004",
      "venue": "International Journal of Computational Linguistics & Chinese Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2017",
      "venue": "PNAS"
    },
    {
      "citation_id": "9",
      "title": "Eegbased emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "Heng Cui",
        "Aiping Liu",
        "Xu Zhang",
        "Xiang Chen",
        "Kongqiao Wang",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "10",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "11",
      "title": "Beyond a joke: Types of conversational humour",
      "authors": [
        "Marta Dynel"
      ],
      "year": "2009",
      "venue": "Language and Linguistics Compass",
      "doi": "10.1111/j.1749-818X.2009.00152.x"
    },
    {
      "citation_id": "12",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "13",
      "title": "A multi-modal eliza using natural language processing and emotion recognition",
      "authors": [
        "Siska Fitrianie",
        "Pascal Wiggers"
      ],
      "year": "2003",
      "venue": "Text, Speech and Dialogue: 6th International Conference, TSD 2003, České Budéjovice"
    },
    {
      "citation_id": "14",
      "title": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "15",
      "title": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020b. COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "16",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "17",
      "title": "Conversation types. The pragmatics of interaction",
      "authors": [
        "Auli Hakulinen"
      ],
      "year": "2009",
      "venue": "Conversation types. The pragmatics of interaction"
    },
    {
      "citation_id": "18",
      "title": "2018a. Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "EMNLP"
    },
    {
      "citation_id": "19",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "20",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Roger Zimmermann",
        "Rada Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "21",
      "title": "Emotion detection in codemixed roman urdu -english text",
      "authors": [
        "Abdullah Ilyas",
        "Khurram Shahzad",
        "Muhammad Kamran"
      ],
      "year": "2023",
      "venue": "ACM Trans. Asian Low-Resour. Lang. Inf. Process",
      "doi": "10.1145/3552515"
    },
    {
      "citation_id": "22",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "23",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael R Lyu"
      ],
      "year": "2019",
      "venue": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "arxiv": "arXiv:1904.04446"
    },
    {
      "citation_id": "24",
      "title": "Automatic sarcasm detection: A survey",
      "authors": [
        "Aditya Joshi",
        "Pushpak Bhattacharyya",
        "Mark Carman"
      ],
      "year": "2017",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3124420"
    },
    {
      "citation_id": "25",
      "title": "Conversation analysis in applied linguistics",
      "authors": [
        "Gabriele Kasper",
        "Johannes Wagner"
      ],
      "year": "2014",
      "venue": "Annual Review of Applied Linguistics",
      "doi": "10.1017/S0267190514000014"
    },
    {
      "citation_id": "26",
      "title": "Muril: Multilingual representations for indian languages",
      "authors": [
        "Simran Khanuja",
        "Diksha Bansal",
        "Sarvesh Mehtani",
        "Savya Khosla",
        "Atreyee Dey",
        "Balaji Gopalan",
        "Dilip Kumar Margam",
        "Pooja Aggarwal",
        "Rajiv Teja Nagipogu",
        "Shachi Dave",
        "Shruti Gupta",
        "Subhash Chandra Bose",
        "Vish Gali",
        "Partha Subramanian",
        "Talukdar"
      ],
      "year": "2021",
      "venue": "Muril: Multilingual representations for indian languages"
    },
    {
      "citation_id": "27",
      "title": "Computing krippendorff's alpha-reliability",
      "authors": [
        "Klaus Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing krippendorff's alpha-reliability"
    },
    {
      "citation_id": "28",
      "title": "2022a. When did you become so smart, oh wise one?! sarcasm explanation in multi-modal multi-party dialogues",
      "authors": [
        "Shivani Kumar",
        "Atharva Kulkarni",
        "Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.411"
    },
    {
      "citation_id": "29",
      "title": "Md Shad Akhtar, and Tanmoy Chakraborty. 2022b. Explaining (sarcastic) utterances to enhance affect understanding in multimodal dialogues",
      "authors": [
        "Shivani Kumar",
        "Ishani Mondal"
      ],
      "venue": "Md Shad Akhtar, and Tanmoy Chakraborty. 2022b. Explaining (sarcastic) utterances to enhance affect understanding in multimodal dialogues"
    },
    {
      "citation_id": "30",
      "title": "CoMPM: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.416"
    },
    {
      "citation_id": "31",
      "title": "Enhancing emotion inference in conversations with commonsense knowledge",
      "authors": [
        "Dayu Li",
        "Xiaodan Zhu",
        "Yang Li",
        "Suge Wang",
        "Deyu Li",
        "Jian Liao",
        "Jianxing Zheng"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2021.107449"
    },
    {
      "citation_id": "32",
      "title": "Research on textual emotion recognition incorporating personality factor",
      "authors": [
        "Haifang Li",
        "Na Pang",
        "Shangbo Guo",
        "Heping Wang"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Robotics and Biomimetics (ROBIO)"
    },
    {
      "citation_id": "33",
      "title": "Bieru: bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "Wei Li",
        "Wei Shao",
        "Shaoxiong Ji",
        "Erik Cambria"
      ],
      "year": "2020",
      "venue": "Bieru: bidirectional emotional recurrent unit for conversational sentiment analysis",
      "arxiv": "arXiv:2006.00492"
    },
    {
      "citation_id": "34",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "35",
      "title": "A multi-view network for real-time emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Xuejun Pan",
        "Yijia Zhang",
        "Zhihao Yang"
      ],
      "year": "2022",
      "venue": "A multi-view network for real-time emotion recognition in conversations"
    },
    {
      "citation_id": "36",
      "title": "Multimodal sentiment analysis on unaligned sequences via holographic embedding",
      "authors": [
        "Yukun Ma",
        "Bin Ma"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "37",
      "title": "Detecting offensive speech in conversational code-mixed dialogue on social media: A contextual dataset and benchmark experiments. Expert Systems with Applications",
      "authors": [
        "Hiren Madhu",
        "Shrey Satapara",
        "Sandip Modha",
        "Thomas Mandl",
        "Prasenjit Majumder"
      ],
      "year": "2023",
      "venue": "Detecting offensive speech in conversational code-mixed dialogue on social media: A contextual dataset and benchmark experiments. Expert Systems with Applications"
    },
    {
      "citation_id": "38",
      "title": "Speech emotion recognition using amplitude modulation parameters and a combined feature selection procedure",
      "authors": [
        "Arianna Mencattini",
        "Eugenio Martinelli",
        "Giovanni Costantini",
        "Massimiliano Todisco",
        "Barbara Basile",
        "Marco Bozzali",
        "Corrado Di"
      ],
      "year": "2014",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "39",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 English words",
      "authors": [
        "Saif Mohammad"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1017"
    },
    {
      "citation_id": "40",
      "title": "Long dialogue emotion detection based on commonsense knowledge graph guidance",
      "authors": [
        "Weizhi Nie",
        "Yuru Bao",
        "Yue Zhao",
        "Anan Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2023.3267295"
    },
    {
      "citation_id": "41",
      "title": "",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "1997",
      "venue": ""
    },
    {
      "citation_id": "42",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "43",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "44",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2929050"
    },
    {
      "citation_id": "45",
      "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
      "authors": [
        "Maarten Sap",
        "Le Ronan",
        "Emily Bras",
        "Chandra Allaway",
        "Nicholas Bhagavatula",
        "Hannah Lourie",
        "Brendan Rashkin",
        "Noah Roof",
        "Yejin Smith",
        "Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "46",
      "title": "Emotion detection in hinglish(hindi+english) codemixed social media text",
      "authors": [
        "Tulasi Sasidhar",
        "B Premjith",
        "K P Soman"
      ],
      "year": "2020",
      "venue": "Third International Conference on Computing and Network Communications (CoCoNet'19)",
      "doi": "10.1016/j.procs.2020.04.144"
    },
    {
      "citation_id": "47",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "arxiv": "arXiv:2012.08695"
    },
    {
      "citation_id": "48",
      "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "authors": [
        "Robyn Speer",
        "Joshua Chin",
        "Catherine Havasi"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17"
    },
    {
      "citation_id": "49",
      "title": "Aspect-based sentiment analysis and emotion detection for code-mixed review",
      "authors": [
        "Andi Suciati",
        "Indra Budi"
      ],
      "year": "2020",
      "venue": "International Journal of Advanced Computer Science and Applications",
      "doi": "10.14569/IJACSA.2020.0110921"
    },
    {
      "citation_id": "50",
      "title": "The impact of social media on the use of code mixing by generation z",
      "authors": [
        "Iin Naf'an Tarihoran",
        "Sumirat Ratna"
      ],
      "year": "2022",
      "venue": "International Journal of Interactive Mobile Technologies (iJIM)"
    },
    {
      "citation_id": "51",
      "title": "Code switching and code mixing as a communicative strategy in multilingual discourse",
      "authors": [
        "Mary Wj Tay"
      ],
      "year": "1989",
      "venue": "World Englishes"
    },
    {
      "citation_id": "52",
      "title": "Codemixing: A brief survey",
      "authors": [
        "S Thara",
        "Prabaharan Poornachandran"
      ],
      "year": "2018",
      "venue": "2018 International conference on advances in computing, communications and informatics (ICACCI)"
    },
    {
      "citation_id": "53",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "Geng Tu",
        "Jintao Wen",
        "Cheng Liu",
        "Dazhi Jiang",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence",
      "doi": "10.1109/TAI.2022.3149234"
    },
    {
      "citation_id": "54",
      "title": "Language in action: Psychological models of conversation",
      "authors": [
        "William Turnbull"
      ],
      "year": "2003",
      "venue": "Language in action: Psychological models of conversation"
    },
    {
      "citation_id": "55",
      "title": "Towards emotion recognition in hindi-english codemixed data: A transformer based approach",
      "authors": [
        "Anshul Wadhawan",
        "Akshita Aggarwal"
      ],
      "year": "2021",
      "venue": "Towards emotion recognition in hindi-english codemixed data: A transformer based approach"
    },
    {
      "citation_id": "56",
      "title": "Language as dialogue",
      "authors": [
        "Edda Weigand"
      ],
      "year": "2010",
      "venue": "Language as dialogue",
      "doi": "10.1515/iprg.2010.022"
    },
    {
      "citation_id": "57",
      "title": "Context-aware self-attention networks",
      "authors": [
        "Baosong Yang",
        "Jian Li",
        "Derek Wong",
        "Lidia Chao",
        "Xing Wang",
        "Zhaopeng Tu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.3301387"
    },
    {
      "citation_id": "58",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "Lin Yang",
        "Yi Shen",
        "Yue Mao",
        "Longjun Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "59",
      "title": "Intelligent facial emotion recognition using moth-firefly optimization",
      "authors": [
        "Li Zhang",
        "Kamlesh Mistry",
        "Siew Chin Neoh",
        "Chee Peng"
      ],
      "year": "2016",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "60",
      "title": "Care: Commonsense-aware emotional response generation with latent concepts",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Pengfei Li",
        "Chen Zhang",
        "Hao Wang",
        "Chunyan Miao"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "61",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP",
      "doi": "10.18653/v1/D19-1016"
    },
    {
      "citation_id": "62",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "63",
      "title": "Leveraging bilingual-view parallel translation for code-switched emotion detection with adversarial dual-channel encoder",
      "authors": [
        "Xun Zhu",
        "Yinxia Lou",
        "Hongtao Deng",
        "Donghong Ji"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2021.107436"
    },
    {
      "citation_id": "64",
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "authors": [
        "Yukun Zhu",
        "Ryan Kiros",
        "Richard Zemel",
        "Ruslan Salakhutdinov",
        "Raquel Urtasun",
        "Antonio Torralba",
        "Sanja Fidler"
      ],
      "year": "2015",
      "venue": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books"
    },
    {
      "citation_id": "66",
      "title": "Pani Ki boonde bani meri gehne, bagiche ki khushbu ka libaas pehne mei sarhado ke paas lehrau, sanan sanan, mei vaayu, hawa, mei zameen, pavan. Indravardhan: Rosesh ye kavita tune nahin likhi hai. Rosesh: Arey, aapko kaise pata chala? Indravardhan: Kyunki ye acchi hai! Sach bata kisne likhi?! Rosesh: Water droplets have turned into my jewelry, I adorn the attire of the garden's fragrance, I sway near the borders, swaying gently. I am the air, the breeze, I am the earth, the wind. Indravardhan: Rosesh, you didn't write this poem",
      "authors": [
        "Rosesh"
      ],
      "venue": "Pani Ki boonde bani meri gehne, bagiche ki khushbu ka libaas pehne mei sarhado ke paas lehrau, sanan sanan, mei vaayu, hawa, mei zameen, pavan. Indravardhan: Rosesh ye kavita tune nahin likhi hai. Rosesh: Arey, aapko kaise pata chala? Indravardhan: Kyunki ye acchi hai! Sach bata kisne likhi?! Rosesh: Water droplets have turned into my jewelry, I adorn the attire of the garden's fragrance, I sway near the borders, swaying gently. I am the air, the breeze, I am the earth, the wind. Indravardhan: Rosesh, you didn't write this poem"
    },
    {
      "citation_id": "67",
      "title": "Sahil: Monisha, mom tissue paper ki baat nhi kar rhi hai. Maya: My life! Meri zindagi! Khatam ho gayi hai! Can you imagine Sahil? Uss Rita se toh Monisha zyada achi hai! Can you imagine? Monisha: Mein kya itni buri hun mummyji? Maya: Haan beta, lekin wo Rita! Oh my god! Saans leti hai toh bhi cheek sunai deti hai. Jab logo ko pata chalega ke rosesh ne loudspeaker se shaadi ki hai",
      "venue": "Maya: What do you mean? Monisha: Wo sarita aunty ka phone aaya tha na"
    },
    {
      "citation_id": "68",
      "title": "Our banana seller also comes without saying anything and sometimes surprises us. Joy or Surprise 4 Indravardhan: Good morning Maya Maya: Good morning sweetie Indravardhan: Nashta nashta nashta Maya: Vitthal Sabko nashta do Indravardhan: Main dudh hargis nahin piyunga. Maya Mujhe diaria Hai Subah Se 6 bar Ja chuka Hun Main Indravardhan: Good morning, Maya. Maya: Good morning, sweetie. Indravardhan: Breakfast, breakfast, breakfast. Maya: Vitthal, serve breakfast to everyone. Indravardhan: I will never drink milk. Maya, I have had diarrhea six times since morning",
      "authors": [
        "Hi Maya",
        "Maya"
      ],
      "venue": "Our banana seller also comes without saying anything and sometimes surprises us. Joy or Surprise 4 Indravardhan: Good morning Maya Maya: Good morning sweetie Indravardhan: Nashta nashta nashta Maya: Vitthal Sabko nashta do Indravardhan: Main dudh hargis nahin piyunga. Maya Mujhe diaria Hai Subah Se 6 bar Ja chuka Hun Main Indravardhan: Good morning, Maya. Maya: Good morning, sweetie. Indravardhan: Breakfast, breakfast, breakfast. Maya: Vitthal, serve breakfast to everyone. Indravardhan: I will never drink milk. Maya, I have had diarrhea six times since morning"
    }
  ]
}