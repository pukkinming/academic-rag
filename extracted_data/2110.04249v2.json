{
  "paper_id": "2110.04249v2",
  "title": "Can Ai Detect Pain And Express Pain Empathy? A Review From Emotion Recognition And A Human-Centered Ai Perspective Running Head: Pain Recognition And Empathic Expression In Ai",
  "published": "2021-10-08T16:58:57Z",
  "authors": [
    "Siqi Cao",
    "Di Fu",
    "Xu Yang",
    "Stefan Wermter",
    "Xun Liu",
    "Haiyan Wu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Authors' contributions H.W. conceived the presented topic. S.C. led the manuscript writing. X.Y., and S.W.. provided scientific suggestions for Section 2 and Section 3. D.F. and S.W. revised the writing structure. X.L. and H.W. supervised the writing process. All the authors contributed to the final manuscript.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent research has defined pain as an uncomfortable sensory and negative emotional experience with or without discernible tissue damage  (Raja et al., 2020) . From an evolutionary perspective, pain expression significantly affects interpersonal relationships  (Williams, 2002) . For instance, people show empathy for others in pain and are willing to help them relieve painful feelings  (Lockwood et al., 2020) . The challenges associated with pain recognition, in general, involve the following aspects: 1) A lack of cognitive, linguistic, and social abilities can hinder the accuracy of subjective assessments, particularly for young children and patients unable to express their feelings; 2) Self-reporting of pain has long been the predominant measure in medicine but if large-scale health monitoring is required, it can be unreliable and time-consuming. Most importantly, how reliable is the medical diagnosis of pain evaluation? It has been argued that clinical diagnosis is not rigorous and inefficient when subjectivity is involved. For example, depressed patients can display happiness briefly, but not in a genuinely happy mood, indicating that cues that are easy to overlook may mislead diagnosis.\n\nConsidering that clinical decisions are not necessarily reliable or objective to a certain extent, a new field in artificial pain recognition appears to emerge. A growing body of research examines the issue from the perspectives of cognitive neuroscience, philosophy, and human-centered artificial intelligence (AI). Over the past few decades, extensive research has been conducted in human behavioral sciences and neuroscience to determine the biological mechanisms of pain  (Danziger et al., 2009; Krishnan et al., 2016; Lieberman & Eisenberger, 2009; Phelps et al., 2021; Smith et al., 2021) .\n\nResearch encompassing various disciplines has led to significant insights into pain recognition through a broad range of empirical and theoretical studies, laying the groundwork for future exploration of better interaction between humans and computers, humans and artificial intelligence, or humans and robots. In this review, we intend to shed light on artificial empathy and training for empathy through computational pain recognition and to gain insight into what is needed to build a Human-In-The-Loop (HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major modalities of pain recognition: face, voice, and body (Section 2; see Fig.  1 left ). Then, in Section 3, cross-disciplinary ideas for existing and potential pain recognition practices are presented based on research in psychology and cognitive neuroscience (see Fig.  1 middle ). While pain empathy plays a vital role in social functioning, computational pain recognition and artificial pain empathy have received little attention. Therefore, in Section 4, we present a growing body of knowledge concerning artificial pain empathy. We discuss the following questions in more detail: How can AI recognize pain from unimodal or multimodal inputs? Is it possible to contribute to this emerging field by studying human beings? Can we build an AI agent with empathy? Is empathic AI necessary? Our final discussion will also focus on ethical considerations in both research and prospective applications.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Unimodal Pain Recognition",
      "text": "Pain recognition can be divided into two questions: pain detection and pain intensity assessment.\n\nA pain detection problem is a binary classification that aims to determine whether a person is in pain.\n\nIn light of considerations of precision, the focus of the research gradually shifted from pain detection to the estimation of pain intensity. Research on computational pain recognition has primarily focused on techniques based on unimodal input involving the face, voice, or body movement  (Kaltwang et al., 2012; Oshrat et al., 2016; Semwal & Londhe, 2021) . Furthermore, the temporal relationship has been explored, ranging from single static images to dynamic sequences of videos  (Simon et al., 2008) . As a starting point for pain recognition, we analyze several published pain databases based on a variety of modalities (see Supplementary Table  1 ). Each database collected pain expressions in different modes, mainly categorized into three types: acted, spontaneous, and elicited (acted expressions are from actors.\n\nSpontaneous expressions are from patients with chronic or acute pain. Elicited expressions indicate expressions evoked by audiovisual media or real interpersonal interaction). The collection of data in a laboratory setting is, however, controversial.  Zhang et al. (2020)  pointed out that people tend to change their biophysical signals to improve the extent to which their minds are being \"read\". Consequently, intentional noise leads to data instability and invalidity, making data collection challenging regardless of methods being used.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Face-Based",
      "text": "Human faces are the most reliable and informative indicators of mental states, particularly emotions  (Kaltwang et al., 2012)  (see Supplementary Table  2 ). Researchers established the Facial Action Coding System (FACS) to identify different facial muscles (action units, AUs) that represent all kinds of emotions  (Ekman & Friesen, 1978) . Based on FACS, a well-established pain assessment standard with a 16-point scale was developed, known as Prkachin and Solomon Pain Intensity (PSPI)  (Prkachin & Solomon, 2008) . In addition, an Active Appearance Model (AAM) is also commonly used by researchers to identify facial patterns associated with spontaneous pain  (Ashraf et al., 2007) .\n\nA growing concern has been raised regarding the ambiguity of pain, as some reports suggested that pain can sometimes appear as a combination of other emotions, such as disgust  (Kunz et al., 2013) .\n\nIdentifying specific characteristics of pain is one of the most important aspects of pain recognition. Furthermore, whether methods frequently used for estimating pain intensity, such as regression, are stable has been controversial. For example, pain and eye closure have a positive correlation, while blinking and eye closure have similar facial characteristics when viewed independently. Pain intensity will be overestimated in images with closed eyes, increasing variance in continuous pain evaluations  (Zhou et al., 2016) . Future research needs to differentiate between video frame sequences according to their temporal relationship.\n\nFor many years, it has been considered time-consuming to code facial features in FACS-based video sequences. AU, PSPI, and AAMs provide the most comprehensive understanding of facial pain templates. Apart from facial model-based pain recognition, feature learning-based approaches have also been developed  (Mauricio et al., 2019) .  Zhou et al. (2016)",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Voice-Based",
      "text": "Despite extensive research on facial cues, few studies have examined how pain can be detected through vocal signals. For humans in the initial developmental stage, sound is more informative to express their feelings (i.e., crying babies)  (Tuduce et al., 2018) . Research on vocal information has traditionally focused on fundamental problems such as blind source separation  (Bell & Sejnowski, 1995; Herault & Jutten, 1986 ) and speech translation  (Jia et al., 2019) . A preliminary study found that facial signals can sometimes be independent of voice generation, such as the fundamental frequencies of phonation and intonation  (Campanella & Belin, 2007) . The separation of voice and facial expressions raises the question of whether we can independently extract pain information from voice or consider voice as a complementary source that benefits pain recognition (see Supplementary Table  3 ). Literature on computational pain recognition has not yet addressed the voice-based signal. In fact, linguistic and non-linguistic signals can be helpful to the medical service  (Besse et al., 2016) .\n\nIn general, the vocal signal can be divided into two categories (linguistic and non-linguistic), each with distinct characteristics  (Noroozi et al., 2018) . On the one hand, the linguistic signal is more complex and subjective than the non-linguistic signal, and natural language varies in different contexts (i.e., recreation and work). Currently, social platforms are an integral part of our daily lives, providing an abundance of text data. The abundant learning materials entail labor-saving methods such as semisupervised learning. In contrast to sentiment analysis, text analysis is less prevalent in computational medical diagnostics. In the long run, such NLP-based technological developments should benefit initial and primary online medical consultations.\n\nOn the other hand, information delivery depends on how we use non-linguistic signs-such as pitch, intensity, and tone. Non-linguistic transformations (i.e., Mel-frequency cepstral coefficients, MFCCs; filter bank energies, FBEs; linear spectral pairs, LSP) are critical to extract useful indicators for pain recognition  (Noroozi et al., 2017) . For example, people can subtly convey opposite information, such as appreciation and sarcasm, by adjusting their tone and stress  (Poria et al., 2017) .\n\nA study found a correlation between bio-signal parameters related to speech prosody and self-reported pain levels  (Oshrat et al., 2016) . Therefore, \"how\" we speak (non-linguistic) is sometimes even more important than \"what\" we say (linguistic). Thus, the extraction of speech features from speech and models for recognizing pain based on speech is crucial to study. A popular audio feature extraction toolkit called OpenSMILE has been proposed to meet the needs of extracting voice features  (Eyben, Wöllmer, Graves, et al., 2010; Eyben, Wöllmer, & Schuller, 2010) . Additionally, background noise can interfere with voice-based pain recognition. To reduce background noise, researchers need to cleanse the audio information  (Hao et al., 2020) . Generally, pain recognition through vocal coding is at a very early stage of feature mining compared to sophisticated facial coding systems. A metalearning process such as stacked ensemble learning may provide insights for speech-based pain recognition (Fig.  2 ).\n\nFig.  2  A meta-learning process for voice-based pain assessment.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Body-Based",
      "text": "Advanced algorithmic models in computer vision relating to human visual systems have driven the development of body-based processing  (Parisi et al., 2016) . Symbolic movements (such as head nodding) can provide an implicit explanation  (Sado et al., 2021) .  Castellano et al. (2008)  suggested that gestures could be the most accurate indicator of pain in a unimodal emotion recognition system, followed by speech and facial expressions. As a result, incorporating body movements into multimodal pain recognition could be beneficial in the long run  (Werner et al., 2018) . In three databases, studies have examined the head movements and posture patterns of patients in pain-and non-pain states and found remarkable differences, suggesting that body-based pain recognition is feasible  (Semwal & Londhe, 2021) . In contrast to a large amount of research on face-based pain recognition, few studies have considered body-based pain recognition (see Supplementary Table  4 ).\n\nThe gap in body-based pain recognition could be partly due to the absence of a valid interpretation model for body movement. An analysis of previous research on emotion recognition provides insight into the model of body movement. In particular, two aspects of gestures are examined: propositional (marker-based) and non-propositional (non-selected marker-based) movements. Video sequences capture a variety of body movements, and positional markers are selected from those movements  (Stathopoulou & Tsihrintzis, 2011) .  Noroozi et al. (2018)     2005 ) investigated context cues and body cues to identify people's emotional states from an abstract perspective based on valence (pleasant/unpleasant) and intensity. However, there is much less research on the choice of either a direct (feature selection based on raw data) or an indirect approach (body models of specific body action units mapping to pain.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Researchers At Carnegie Mellon University (Cmu) Proposed An Openpose Body Posture",
      "text": "Recognition Project, a real-time multi-person 2D gesture estimation based on deep learning without special hardware to acquire data  (Cao et al., 2017) . It is an open-source library based on a supervised learning approach and CNN architecture. The project estimates human body movement, facial expression, finger movement, and other body gestures. In contrast to single-target detection, multitarget detection can be achieved with a high level of robustness  (Martinez et al., 2019) . Despite the difficulty of tracking human pain levels by nonverbal cues such as gestures or postures, various motion capture utilities with depth sensors can record high-quality data, allowing for future progress in this subject  (Anbarjafari et al., 2018) .\n\nAs a whole, development in basic emotion recognition has made significant contributions to affective computing in recent decades. Many achievements have been made in recognizing pain, and this progress is through a generalization of the methodology developed from basic emotion recognition.\n\nWith the advent of wearable and portable technologies and dry electrode technology, researchers have been able to access additional sources of psychophysiological pain indicators, including electroencephalography (EEG) and Electrodermal Activity (EDA) (i.e., monitoring ICU patients)  (Kächele et al., 2015; Walter et al., 2013) . One of the most significant aspects of bio-signaling is that physiological parameters may not be correlated with specific emotional states. However, they were related to the underlying emotional representation dimensions (valence, arousal, and dominance)  (Schlosberg, 1954) . Hence, physiological signals can provide additional information that may be useful in estimating pain intensity  (Gruss et al., 2015) . A general understanding of the unimodal development of pain recognition presented in this article can assist researchers in developing an optimal multimodal framework or system in the future (Fig.  3 ). Furthermore, we propose that computational pain recognition can be improved by learning directly from human studies by focusing on how individuals perceive and respond to the pain of others (Fig.  4  top).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Indirect Learning From Human Neural Mechanisms",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Integration And Processing Of Multimodal Signals",
      "text": "It is believed that mental processes are derived from interconnected subsystems in the human brain, and each subsystem contributes to several mental processes  (Shallice, 1988) . The issue of how algorithms replicate computation in the brain by using reverse engineering is of particular importance, such as the visual system  (Hubel & Wiesel, 1959) , attention system, and intertwined subsystems  (Koch & Ullman, 1987) . The brain is able to exchange and match cross-modal information (face and voice) in order to identify others  (Blank et al., 2011; Blank et al., 2015) . Humans are able to integrate multimodal information by selectively focusing on the relevant inputs that are related to their goals or tasks  (Fu et al., 2020) . Beyond unimodality, multimodal feature integration methods are rapidly developing  (Gruss et al., 2019; Khan et al., 2013; Thiam & Schwenker, 2017) , particularly on facial and vocal expressions.  Kächele et al. (2015)  showed that multimodal information is more advantageous than unimodal in identifying pain from facial expressions, head posture, and physiological signals in videos (see Supplementary Table  5 ). Unlike unimodal systems, multimodal data fusion has substantially improved recognition accuracy  (Ranganathan et al., 2016) . Nevertheless, it remains unclear at which stage cross-modal information intersects. The integration of audiovisual speech with cross-modal processing may be in the early phases of nonprimary auditory brain activity  (Besle et al., 2008) . Also, researchers suggested a hierarchy in which various brain regions encode information independently and interact in the transient cortices  (Campanella & Belin, 2007; Davies-Thompson et al., 2019; Molnár et al., 2020) . Similarly, the optimal time point for an AI to integrate multimodal information (data fusion) remains controversial.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Memory And Learning",
      "text": "Prior knowledge improves our understanding of a situation, and prior experience accelerates our ability to adjust to new situations  (Williams & Lombrozo, 2013) . Prior experience, such as personal medical history, is a precious source of clinical information. Patients with different types of pain, i.e., acute pain and chronic pain, show a distinct way of processing and expressing pain. For example, researchers have found a bias in information processing when experiencing acute pain compared to chronic pain  (Moseley et al., 2005) . In this regard, including information about a person in an AI could be beneficial for recognizing pain in a medical context. At the same time, ethical and security standards have to be considered.\n\nHuman top-down cognitive systems can modulate pain perception, but significant inter-individual differences exist  (Peng et al., 2020) . One of the difficulties in pain recognition is the varying expressions towards the same stimuli; that is, humans vary in their expressions in the same situation  (Grodal, 2007) . Humans have individual differences in expressing pain, so the pattern of responses in prior experiences with similar stimuli is a vital source of learning for pain analysis on specific targets.\n\nA personalization of pain intensity estimation systems  (Kächele et al., 2017)  with multimodal analysis can utilize memory and high-dimensional feature space to represent past information about pain.\n\nHumans rely on the hippocampus as a central hub for memory storage. The hippocampus is activated when an episodic experience replays during resting and sleeping, which has been considered a process that integrates short-and long-term memory  (Singer & Frank, 2009) . Specifically, researchers proposed an example-based manner by which memory \"replays\" itself offline to learn the successes or failure cases that occurred in the past  (Hassabis et al., 2017) . The replay process emphasizes the recirculation of learning. For real-time pain monitoring, a great deal of information from different devices converges in the back-end database. Long-short-term memory (LSTM), inspired by the working memory framework, holds information to a fixed operational state until an acceptable output is required  (Hochreiter & Schmidhuber, 1997) . LSTMs can contribute to instant detection by allowing the most valuable signals to be processed first and other signals to be processed later.\n\nHowever, an advanced application would require translating an individual's past experience, pain threshold, or pain display characteristics to a person-specific feature space before training. Over time, data can be trained offline to improve algorithms in specific environments (i.e., hospitals, homes, and public places), similar to human episodic memory, replaying during sleep to realize a conceptual lifelong learning system  (Sodhani et al., 2022) .",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Direct Learning From Human Studies",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Modulation Of Pain Expression Through Social Contexts",
      "text": "From recognition to cognition, individual cognitive patterns (i.e., pain sensitivity) are essential.\n\nHowever, the pattern is context-dependent, meaning pain expressions vary in different contexts. Social contexts can alter an individual's pain expression. For example, our discomfort toward aversive stimuli is lower when we are in the company of others than when we are alone, referring to the social buffering effect  (Langford et al., 2006; Qi et al., 2020) . Meanwhile, high social threats, such as high electrocutaneous stimuli administered by others, increase pain intensity and unpleasant feelings  (Karos et al., 2020) .  Hammal et al. (2008)  suggested that context provides substantial information for pain recognition. These explicit expressions are influenced by our environment. For example, whether people are surrounded by whom they trust (workplace, hospital, or home)  (Bublatzky et al., 2020) .\n\nTherefore, target separation and context are fundamental issues to address for improvement in real-life applications of pain estimation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Pain Recognition And Response To Others' Pain In Human Studies",
      "text": "Other than physical features of pain expressions, what other sources of learning can be integrated into an AI? In most cases, it is beneficial to get inspired and guided by human studies. What is the process by which humans recognize pain in others, and how do they react based on their understanding of the pain state of others? Researchers used well-calibrated experimental pain models (i.e., heat, cold pressure, or video) to generate stimuli that show the pain of others for observers to respond to  (Bastian et al., 2014; FeldmanHall et al., 2015) . The observers rate the pain level of people in each stimulus, and researchers record their neural signals (EEG, fMRI) of processing these stimuli as well as choices observers make, such as prosocial choices  (Jackson et al., 2005; Peng et al., 2021) .\n\nTwo insights can be gained from some studies concerning human reactions to others in pain. For one thing, stimuli used for eliciting human responses to others' pain have inherent features (standardized labels of others' pain state, and physical features of the stimuli), which are the potential input for any algorithms. In addition, the experiments collected human reactions in several dimensions (behavioral ratings or decisions to help people in pain, which is a feasible output/end for an AI to learn). This line of study can shed light on the \"black box\" systems of the human cognitive system between the input of victims in pain and the output of observers' reactions. Ultimately, the goal is to explore pain recognition from a human perspective and to allow current computational pain recognition to benefit from human cognitive systems.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Prospects In Pain Recognition",
      "text": "For the above sections, we first reviewed unimodal-and multimodal-pain recognition systems.\n\nWe introduced some established databases, followed by an overview of the progress in face-, voiceand gesture-based pain recognition. Regarding robustness, consistency, and implementation prerequisites, all single modalities are restricted. First, there is a shortage of sufficient data, including targets with different pain levels and real-life big clinical data (i.e., spontaneous painful expressions of different modalities). Also, realistic scenarios are often ignored. Second, multimodal pain models ought to be prepared on substantial information from various contexts to construct generalizable models. Third, a meta-analysis of diverse methods regarding specific modalities provides insights into pain recognition. For example, what information can be weighted more than the other to improve performance? How can recognition accuracy be enhanced by combining the outputs of different independent models? Specifically, ensemble learning, which combines several weak classifiers into one integrated classifier, is a potential solution for building better models of pain analysis. Moreover, it is also a way to address how to deal with multisensory information processing, and how intelligent recognition can be done in the real world with more available signals in the future (Fig.  3 ).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Beyond Pain Recognition",
      "text": "In retrospect, pain is often associated with empathy and noble virtues, such as altruism, from a psychological perspective  (Cameron et al., 2019; Fehr & Fischbacher, 2003; Staub & Vollhardt, 2008) .\n\nEmpathic people can recognize and comprehend others' emotions by experiencing and sharing the emotional states of others  (Singer, 2006) . However, the substantial relationship between pain and empathy has not been explored in AI research. An AI agent with human-like empathic responses is considered more caring, likeable, and trustworthy  (Rodrigues et al., 2015) . In the current state-of-theart, one question is, \"can an AI express empathy at all?\". There is still a long way to go before we get closer to this question. However, could the recognition of pain, which has relatively clear and definite features for an AI agent to learn, lay the foundation for artificial pain empathy? There are two steps for an AI to implement pain empathy: first, the identification of pain, and second, the expression of empathy  (Khatibi & Mazidi, 2019) . The above sections have reviewed the pain recognition progress.\n\nIn this section, we describe a relatively novel field associated with an affective AI -artificial pain empathy.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Artificial Pain Empathy",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Mimicry",
      "text": "Pain empathy is the capacity associated with feeling and evaluating others' pain states and understanding others, often prompting prosocial actions  (De Waal, 2008; Fitzgibbon et al., 2010; Wang et al., 2019) . Studies from animals and humans suggested that mimicry of body movement is the underlying mechanism for empathy which derives from a mirror system in the human brain  (Rizzolatti & Craighero, 2004) . In social interactions, emotional mimicry is crucial since it reflects a desire to connect with another person  (Hess & Fischer, 2013) . Thus, the initial step to addressing pain empathy is to recognize and imitate human facial expressions or gestures in real-time. To some extent, mimicry generates similarity and, in turn, facilitates people's empathy toward a human-designed machine  (Breazeal, 2003) .  Miura et al. (2008)  found that human-like body movements make it easier for people to empathize and the embodiment of an interactive partner influences human mimicry behaviors. A physical presence and human-like artificial entity tend to generate more mimicry than virtual nonhuman counterparts  (Perugia et al., 2020) . In addition, emotional mimicry by robots may show its empathic \"trait\", improving the human-robot interaction experience  (Leite et al., 2012) .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Modulation By Top-Down Cognitive And Presence Of Social Partners",
      "text": "What functions can AI agents or robots have when they cannot feel suffering like humans? First, some insights can be gained from studies on people born with the congenital absence of pain.  Danziger et al. (2009)  found that pain-related brain regions of congenitally pain-free patients are activated when seeing others experiencing pain, indicating a shared synchrony pain aversion with others.  Krishnan et al. (2016)  found that the experience of vicarious pain (observing others in pain) is neurologically separate from experiencing actual pain on our own, suggesting that empathy is more cognitive than sensational. Thus, empathy not only has the intrinsic characteristics of sensibility but also comprises the process of top-down cognitive regulation. Heyes (2018) proposed a dual-system model of empathy.\n\nThe model includes both early views in which empathy largely depends on a bottom-up process, a spontaneous response activated by stimuli (system Ⅰ). Meanwhile, it also covers the control mechanism of empathy that belongs to a top-down process, indicating that high-level cognitive systems contribute to the regulation of empathy (system Ⅱ). The two-system model of empathy lays the foundation for the investigations on artificial empathy from a cognitive perspective.\n\nSecond, the social buffering effect indicates that the presence of social partners effectively modulates human reactions toward aversive stimuli  (Qi et al., 2020) . A recent study on rodents found a neural circuit of the buffering effect. Their results showed that brief social interaction with a peer mouse experiencing pain or morphine analgesia resulted in the transfer of these experiences to its social partner  (Smith et al., 2021) . Hence, a theoretical hypothesis is suggested that painless robots can serve as analgesic companions, thereby reducing people's pain perception. In fact, mental support is more human-centered and user-friendly than realizing high-level pain recognition. Some effort has been put into this practice. For example, the Institute for Creative Technologies at the University of Southern California created an empathic AI system that functions as a virtual counselor, and mainly serves veterans with post-traumatic stress disorder  (Gaggioli, 2017) . Furthermore, when humans suffer together, they prefer to bond  (Peng et al., 2021) . Thus, the painsharing demonstration of artificial agents increases the possibility of building trust with people. In addition, practices in human studies showed that neuromodulation, an array of non-invasive, minimally invasive, and surgical electrical therapies, is conducive to pain relief  (Knotkova et al., 2021) . People can self-adjust their feelings by training them with positive feedback when making target responses, and the technique is called neurofeedback. Researchers have found evidence that the sensorydiscriminative aspect of pain is associated with EEG signals that are deliberatively trained. However, due to differences in training efficacy, future research is needed to examine individualized neurofeedback for pain regulation  (Peng et al., 2020) .",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Empathy Training For Humans",
      "text": "Another potential application for AI research to contribute to HILP systems is to use a generative model. The generative models that generate human images and speech have come close to equivalents in the real-world  (Oord et al., 2016) . On the one hand, synthetic data can be used to train the recognition models. On the other hand, the data can be applied to the training process for medical caregivers to improve people's estimation of pain intensity. A recent study indicated the feasibility of such training pain. People underwent a 3.5 to 5 hours online training program called the index of facial pain expression (IFPE). After the short training, observers improved their identification of others' psychiatric pain expressions  (Rash et al., 2019) . Hence, an important task for future artificial agents is to help people with emotional deficiency to establish the ability to perceive, interpret, express, and regulate emotions  (Javed & Park, 2019) .\n\nTo sum up, artificial assistants for pain modulation are becoming available to assist humans. The presence of them and simple pain-sharing displays can have a significant effect in reducing pain perception. More sophisticated neuromodulation is also feasible in the future application of artificial medical assistants. To jointly contribute to the field, it is required to integrate psychology, ethics, and cognitive neuroscience.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ethical Considerations",
      "text": "Concerning the overall topic, ethical issues should be considered with attention and responsibility for future work on affective assistants. Ideally, computational pain recognition can be applied to assist people in pain. Artificial empathy refers to the capacity of computer systems to recognize and respond to people's behaviors, expressions, and emotions  (Asada, 2015a (Asada, , 2015b;; Asada et al., 2012) . With each interaction people have with an AI system, adaptive artificial empathy can be expected  (Harris & Sharlin, 2010) . In this paper, we some existing databases, which may be helpful for developing algorithms that can generate empathic expressions. Furthermore, an AI agent needs to recognize and generate an empathic response to others' pain experiences. However, research has not yet fully considered an affective AI with proper responses to others' pain experiences. By learning from human-human interaction, an AI system could also learn human behaviors by interacting with real people in an \"empathic\" way  (Paiva et al., 2017) .\n\nAlthough there is a promising future for artificial systems with empathy-like responses, the concept is more than modeling humans  (James et al., 2018; Putta et al., 2022; Srinivasan & San Miguel González, 2022) . There are important issues to be considered: What artificial systems could be advantageous and beneficial for human life? What boundaries do such service functions need to avoid crossing? Generally, the precision, flexibility, and convenience of advanced services require personal information from users. Therefore, the privacy and security of personal data need to be considered highly. Rules for accountability require legal support and government supervision. More significantly, if artificial systems are launched, both the creator and the user must be fully informed about the information exchanged, possible health concerns and the limitations of interpreting affect.\n\nTrustworthiness and understanding of AI systems' limitations, like algorithmic bias by biased data, are important topics to address, particularly when people interact with artificial empathy services.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Conclusion",
      "text": "This paper highlights the potential of multimodal signals to improve pain recognition in computational models and assistive systems. Regular pain monitoring by assistive AI systems may allow patients to obtain timely treatment, and an AI assistant may be helpful for human-computer interaction. However, challenges include noise estimation in unimodal sources, multiple feature extraction, comparison of fusion methods, and establishing balanced and compatible datasets to address the need for real-time applications. Moreover, an AI assistant should show socially acceptable verbal and non-verbal signals to achieve positive interactions. Although AI can demonstrate empathic responses, there are inherent limitations and issues regarding the semantic understanding of intention and trust when processing affective information. Integrating and exploring affective AI jointly by psychologists and computer scientists will help better interpret how humans understand each other based on sensory and emotional states. If pain recognition and human-like empathic expressions are developed in an ethical manner, an artificial agent could also be considered a helpful assistant trained to express empathy and safely interact with humans.  Face (F); audio (A); Bio-physiology (Bio); RGB Depth and Thermal (RGBDT); Geature (G); Finger pressure (FP); Hand movements (HM); Semi-Naive Bayesian Classifier (SNBC), Circular Classifier Chain (CCC).",
      "page_start": 15,
      "page_end": 28
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This review's overall illustration of topics, contents, and goals.",
      "page": 3
    },
    {
      "caption": "Figure 1: left). Then, in Section 3,",
      "page": 4
    },
    {
      "caption": "Figure 1: middle). While pain empathy plays a",
      "page": 4
    },
    {
      "caption": "Figure 2: A meta-learning process for voice-based pain assessment.",
      "page": 7
    },
    {
      "caption": "Figure 4: bottom).",
      "page": 9
    },
    {
      "caption": "Figure 4: Pain recognition under the Human-In-The-Loop framework.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Siqi Cao1, 2, 3, Di Fu1, 2, 5, Xu Yang4, Stefan Wermter5, Xun Liu1, 2*, Haiyan Wu6*": "1CAS Key Laboratory of Behavioral Science, Institute of Psychology, Chinese Academy of Sciences,"
        },
        {
          "Siqi Cao1, 2, 3, Di Fu1, 2, 5, Xu Yang4, Stefan Wermter5, Xun Liu1, 2*, Haiyan Wu6*": "Beijing, China"
        },
        {
          "Siqi Cao1, 2, 3, Di Fu1, 2, 5, Xu Yang4, Stefan Wermter5, Xun Liu1, 2*, Haiyan Wu6*": "2Department of Psychology, University of Chinese Academy of Sciences, Beijing, China"
        },
        {
          "Siqi Cao1, 2, 3, Di Fu1, 2, 5, Xu Yang4, Stefan Wermter5, Xun Liu1, 2*, Haiyan Wu6*": "3Department of Experimental Psychology, University of Oxford, Oxford, UK"
        },
        {
          "Siqi Cao1, 2, 3, Di Fu1, 2, 5, Xu Yang4, Stefan Wermter5, Xun Liu1, 2*, Haiyan Wu6*": "4State Key Laboratory for Management and Control of Complex Systems, Institute of Automation,"
        },
        {
          "Siqi Cao1, 2, 3, Di Fu1, 2, 5, Xu Yang4, Stefan Wermter5, Xun Liu1, 2*, Haiyan Wu6*": "Chinese Academy of Sciences, Beijing, China"
        },
        {
          "Siqi Cao1, 2, 3, Di Fu1, 2, 5, Xu Yang4, Stefan Wermter5, Xun Liu1, 2*, Haiyan Wu6*": "5Department of Informatics, University of Hamburg, Hamburg, Germany"
        },
        {
          "Siqi Cao1, 2, 3, Di Fu1, 2, 5, Xu Yang4, Stefan Wermter5, Xun Liu1, 2*, Haiyan Wu6*": "6Centre for Cognitive and Brain Sciences and Department of Psychology, University of Macau, Taipa,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Sensory and emotional experiences such as pain and empathy are essential for mental and physical"
        },
        {
          "Abstract": "health.  Cognitive  neuroscience  has  been  working  on  revealing  mechanisms  underlying  pain  and"
        },
        {
          "Abstract": "empathy.  Furthermore,  as  trending  research  areas,  computational  pain  recognition  and  empathic"
        },
        {
          "Abstract": "artificial intelligence (AI) show progress and promise for healthcare or human-computer interaction."
        },
        {
          "Abstract": "Although AI  research  has  recently  made  it  increasingly  possible  to  create  artificial  systems  with"
        },
        {
          "Abstract": "affective processing, most cognitive neuroscience and AI research do not jointly address the issues of"
        },
        {
          "Abstract": "empathy in AI and cognitive neuroscience. The main aim of this paper is to introduce key advances,"
        },
        {
          "Abstract": "cognitive challenges and technical barriers in computational pain recognition and the implementation"
        },
        {
          "Abstract": "of artificial empathy. Our discussion covers the following topics: How can AI recognize pain from"
        },
        {
          "Abstract": "unimodal and multimodal information? Is it crucial for AI to be empathic? What are the benefits and"
        },
        {
          "Abstract": "challenges  of  empathic AI?  Despite  some  consensus  on  the  importance  of AI,  including  empathic"
        },
        {
          "Abstract": "recognition and responses, we also highlight future challenges for artificial empathy and possible paths"
        },
        {
          "Abstract": "from interdisciplinary perspectives. Furthermore, we discuss challenges for responsible evaluation of"
        },
        {
          "Abstract": "cognitive methods and computational techniques and show approaches to future work to contribute to"
        },
        {
          "Abstract": "affective assistants capable of empathy."
        },
        {
          "Abstract": "Keyword pain recognition, artificial empathy, human-centered AI, Human-In-The-Loop"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1 Introduction": "Recent research has defined pain as an uncomfortable sensory and negative emotional experience"
        },
        {
          "1 Introduction": "with or without discernible tissue damage (Raja et al., 2020). From an evolutionary perspective, pain"
        },
        {
          "1 Introduction": "expression  significantly  affects  interpersonal  relationships  (Williams,  2002).  For  instance,  people"
        },
        {
          "1 Introduction": "show empathy for others in pain and are willing to help them relieve painful feelings (Lockwood et al.,"
        },
        {
          "1 Introduction": "2020). The challenges associated with pain recognition, in general, involve the following aspects: 1)"
        },
        {
          "1 Introduction": "A lack of cognitive, linguistic, and social abilities can hinder the accuracy of subjective assessments,"
        },
        {
          "1 Introduction": "particularly for young children and patients unable to express their feelings; 2) Self-reporting of pain"
        },
        {
          "1 Introduction": "has long been the predominant measure in medicine but if large-scale health monitoring is required, it"
        },
        {
          "1 Introduction": "can be unreliable and time-consuming. Most importantly, how reliable is the medical diagnosis of pain"
        },
        {
          "1 Introduction": "evaluation? It has been argued that clinical diagnosis is not rigorous and inefficient when subjectivity"
        },
        {
          "1 Introduction": "is  involved.  For  example,  depressed  patients  can  display  happiness  briefly,  but  not  in  a  genuinely"
        },
        {
          "1 Introduction": "happy mood, indicating that cues that are easy to overlook may mislead diagnosis."
        },
        {
          "1 Introduction": "Considering that clinical decisions are not necessarily reliable or objective to a certain extent, a"
        },
        {
          "1 Introduction": "new field in artificial pain recognition appears to emerge. A growing body of research examines the"
        },
        {
          "1 Introduction": "issue  from  the  perspectives  of  cognitive  neuroscience,  philosophy,  and  human-centered  artificial"
        },
        {
          "1 Introduction": "intelligence  (AI).  Over  the  past  few  decades,  extensive  research  has  been  conducted  in  human"
        },
        {
          "1 Introduction": "behavioral sciences and neuroscience to determine the biological mechanisms of pain (Danziger et al.,"
        },
        {
          "1 Introduction": "2009; Krishnan et al., 2016; Lieberman & Eisenberger, 2009; Phelps et al., 2021; Smith et al., 2021)."
        },
        {
          "1 Introduction": "Research encompassing various disciplines has led to significant insights into pain recognition through"
        },
        {
          "1 Introduction": "a broad range of empirical and theoretical studies, laying the groundwork for future exploration of"
        },
        {
          "1 Introduction": "better interaction between humans and computers, humans and artificial intelligence, or humans and"
        },
        {
          "1 Introduction": "robots."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: ). Each database collected pain expressions in different modes,",
      "data": [
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "modalities of pain recognition: face, voice, and body (Section 2; see Fig. 1 left). Then, in Section 3,"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "cross-disciplinary ideas for existing and potential pain recognition practices are presented based on"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "research in psychology and cognitive neuroscience (see Fig. 1 middle). While pain empathy plays a"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "vital  role  in  social  functioning,  computational  pain  recognition  and  artificial  pain  empathy  have"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "received little attention. Therefore, in Section 4, we present a growing body of knowledge concerning"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "artificial pain empathy. We discuss the following questions in more detail: How can AI recognize pain"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "from unimodal or multimodal inputs? Is it possible to contribute to this emerging field by studying"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "human  beings?  Can  we  build  an  AI  agent  with  empathy?  Is  empathic  AI  necessary?  Our  final"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "discussion will also focus on ethical considerations in both research and prospective applications."
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "2 Unimodal pain recognition"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "Pain recognition can be divided into two questions: pain detection and pain intensity assessment."
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "A pain detection problem is a binary classification that aims to determine whether a person is in pain."
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "In light of considerations of precision, the focus of the research gradually shifted from pain detection"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "to the estimation of pain intensity. Research on computational pain recognition has primarily focused"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "on techniques based on unimodal input involving the face, voice, or body movement (Kaltwang et al.,"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "2012; Oshrat et al., 2016; Semwal & Londhe, 2021). Furthermore, the temporal relationship has been"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "explored, ranging from single static images to dynamic sequences of videos (Simon et al., 2008). As a"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "starting point for pain recognition, we analyze several published pain databases based on a variety of"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "modalities (see Supplementary Table 1). Each database collected pain expressions in different modes,"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "mainly categorized into three types: acted, spontaneous, and elicited (acted expressions are from actors."
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "Spontaneous expressions are from patients with chronic or acute pain. Elicited expressions indicate"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "expressions evoked by audiovisual media or real interpersonal interaction). The collection of data in a"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "laboratory setting is, however, controversial. Zhang et al. (2020) pointed out that people tend to change"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "their biophysical signals to improve the extent to which their minds are being \"read\". Consequently,"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "intentional noise leads to data instability and invalidity, making data collection challenging regardless"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "of methods being used."
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "2.1 Face-based"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "Human  faces  are  the  most  reliable  and  informative  indicators  of  mental  states,  particularly"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "emotions  (Kaltwang  et  al.,  2012)  (see  Supplementary  Table  2).  Researchers  established  the  Facial"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "Action Coding System (FACS) to identify different facial muscles (action units, AUs) that represent"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "all kinds of emotions (Ekman & Friesen, 1978). Based on FACS, a well-established pain assessment"
        },
        {
          "(HITL) system for pain recognition and artificial pain empathy. To begin with, we review three major": "standard with a 16-point scale was developed, known as Prkachin and Solomon Pain Intensity (PSPI)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "used by researchers to identify facial patterns associated with spontaneous pain (Ashraf et al., 2007)."
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": ""
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "that pain can sometimes appear as a combination of other emotions, such as disgust (Kunz et al., 2013)."
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "Identifying specific characteristics of pain is one of the most important aspects of pain recognition."
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "Furthermore, whether methods frequently used for estimating pain intensity, such as regression, are"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "stable has been controversial. For example, pain and eye closure have a positive correlation, while"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "blinking and eye closure have similar facial characteristics when viewed independently. Pain intensity"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "will be overestimated in images with closed eyes, increasing variance in continuous pain evaluations"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "(Zhou et al., 2016). Future research needs to differentiate between video frame sequences according"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "to their temporal relationship."
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": ""
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "video sequences. AU, PSPI, and AAMs provide the most comprehensive understanding of facial pain"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "templates. Apart from facial model-based pain recognition, feature learning-based approaches have"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "also been developed (Mauricio et al., 2019). Zhou et al. (2016) worked on a smooth estimation using"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "a refined Recurrent Convolutional Neural Network (RCNN) framework. The strength of RCNN is that"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "it encodes spatial information (CNN-based advantage) along with temporal information in sequence"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "(RNN-based advantage), which yielded good results regarding both accuracy and running speed on"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "the fully labeled UNBC-McMaster Shoulder Pain Expression Archive database. In addition, Rodriguez"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "et al. (2017) employed a deep learning model with a long-short-term memory (LSTM) to incorporate"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "temporal information into the model. The model outperformed the previous state-of-the-art methods"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "on scores of areas under the curve (AUC), mean standard error (MSE), Pearson correlation coefficient"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "(PCC),  and \nthe"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "background interference with adaptive weight distribution in facial regions. Bargshady et al. (2020)"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "exploited an Ensemble Deep Learning Model (EDLM) to classify pain and generate a 5-level intensity"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "estimation  using  the  Multimodal  database.  They  also  validated  the  generalizability  on  the  UNBC-"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "McMaster Shoulder Pain dataset."
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "2.2 Voice-based"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": ""
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "through vocal signals. For humans in the initial developmental stage, sound is more informative to"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "express their feelings (i.e., crying babies) (Tuduce et al., 2018). Research on vocal information has"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "traditionally  focused  on  fundamental  problems  such  as  blind  source  separation  (Bell  &  Sejnowski,"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "1995; Herault & Jutten, 1986) and speech translation (Jia et al., 2019). A preliminary study found that"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "facial signals can sometimes be independent of voice generation, such as the fundamental frequencies"
        },
        {
          "(Prkachin  &  Solomon,  2008).  In  addition,  an  Active  Appearance  Model  (AAM)  is  also  commonly": "of  phonation  and"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: ). Literature on computational pain recognition has not yet addressed the voice-based signal. In fact,",
      "data": [
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "or consider voice as a complementary source that benefits pain recognition (see Supplementary Table"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "3). Literature on computational pain recognition has not yet addressed the voice-based signal. In fact,"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "linguistic and non-linguistic signals can be helpful to the medical service (Besse et al., 2016)."
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "In general, the vocal signal can be divided into two categories (linguistic and non-linguistic), each"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "with  distinct  characteristics  (Noroozi  et  al.,  2018).  On  the  one  hand,  the  linguistic  signal  is  more"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "complex and subjective than the non-linguistic signal, and natural language varies in different contexts"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "(i.e., recreation and work). Currently, social platforms are an integral part of our daily lives, providing"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "an abundance of text data. The abundant learning materials entail labor-saving methods such as semi-"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "supervised learning. In contrast to sentiment analysis, text analysis is less prevalent in computational"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "medical  diagnostics.  In  the  long  run,  such  NLP-based  technological  developments  should  benefit"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "initial and primary online medical consultations."
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "On the other hand, information delivery depends on how we use non-linguistic signs—such as"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "pitch,  intensity,  and  tone.  Non-linguistic  transformations  (i.e.,  Mel-frequency  cepstral  coefficients,"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "MFCCs; filter bank energies, FBEs; linear spectral pairs, LSP) are critical to extract useful indicators"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "for  pain  recognition  (Noroozi  et  al.,  2017).  For  example,  people  can  subtly  convey  opposite"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "information, such as appreciation and sarcasm, by adjusting their tone and stress (Poria et al., 2017)."
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "A study found a correlation between bio-signal parameters related to speech prosody and self-reported"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "pain levels (Oshrat et al., 2016). Therefore, \"how\" we speak (non-linguistic) is sometimes even more"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "important than \"what\" we say (linguistic). Thus, the extraction of speech features from speech and"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "models for recognizing pain based on speech is crucial to study. A popular audio feature extraction"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "toolkit called OpenSMILE has been proposed to meet the needs of extracting voice features (Eyben,"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "Wöllmer, Graves, et al., 2010; Eyben, Wöllmer, & Schuller, 2010). Additionally, background noise"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "can  interfere  with  voice-based  pain  recognition.  To  reduce  background  noise,  researchers  need  to"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "cleanse the audio information (Hao et al., 2020). Generally, pain recognition through vocal coding is"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "at  a  very  early  stage  of  feature  mining  compared  to  sophisticated  facial  coding  systems.  A  meta-"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "learning  process  such  as  stacked  ensemble  learning  may  provide  insights  for  speech-based  pain"
        },
        {
          "expressions raises the question of whether we can independently extract pain information from voice": "recognition (Fig. 2)."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "2.3 Body-based"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "Advanced algorithmic models in computer vision relating to human visual systems have driven"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "the development of body-based processing (Parisi et al., 2016). Symbolic movements (such as head"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "nodding) can provide an implicit explanation (Sado et al., 2021). Castellano et al. (2008) suggested"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "that gestures could be the most accurate indicator of pain in a unimodal emotion recognition system,"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "followed by speech and facial expressions. As a result, incorporating body movements into multimodal"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "pain recognition could be beneficial in the long run (Werner et al., 2018). In three databases, studies"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "have examined the head movements and posture patterns of patients in pain- and non-pain states and"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "found  remarkable  differences,  suggesting  that  body-based  pain  recognition  is  feasible  (Semwal  &"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "Londhe, 2021). In contrast to a large amount of research on face-based pain recognition, few studies"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "have considered body-based pain recognition (see Supplementary Table 4)."
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "The gap in body-based pain recognition could be partly due to the absence of a valid interpretation"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "model for body movement. An analysis of previous research on emotion recognition provides insight"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "into the model of body movement. In particular, two aspects of gestures are examined: propositional"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "(marker-based)  and  non-propositional  (non-selected  marker-based)  movements.  Video  sequences"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "capture  a  variety  of  body  movements,  and  positional  markers  are  selected  from  those  movements"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "(Stathopoulou & Tsihrintzis, 2011). Noroozi et al. (2018) summarized human body models into two"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "types: part-based and kinematic models. Part-based approaches represent the human body as a flexible"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "configuration of body parts. Kinematic models consist of a series of interconnected joints. However,"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "gestures can be acquired intrinsically (nodding to show approval) or extrinsically (in some cultures"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "waving hands to the side to show rejection instead of greetings). Cultural contexts should be considered"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "in future research to develop an integrated model relevant to the context. For non-positional movement"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "qualities, Castellano et al. (2007) used amplitude, speed, and fluidity of movement to classify eight"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "emotions instead of gestures. Burgoon et al. (2005) investigated context cues and body cues to identify"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "people's  emotional  states  from  an  abstract  perspective  based  on  valence  (pleasant/unpleasant)  and"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "intensity. However, there is much less research on the choice of either a direct (feature selection based"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "on raw data) or an indirect approach (body models of specific body action units mapping to pain."
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "Researchers  at  Carnegie  Mellon  University  (CMU)  proposed  an  OpenPose  Body  Posture"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "Recognition Project, a real-time multi-person 2D gesture estimation based on deep learning without"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "special hardware to acquire data (Cao et al., 2017). It is an open-source library based on a supervised"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "learning  approach  and  CNN  architecture.  The  project  estimates  human  body  movement,  facial"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "expression, finger movement, and other body gestures. In contrast to single-target detection, multi-"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "target detection can be achieved with a high level of robustness (Martinez et al., 2019). Despite the"
        },
        {
          "Fig. 2 A meta-learning process for voice-based pain assessment.": "difficulty of tracking human pain levels by nonverbal cues such as gestures or postures, various motion"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "subject (Anbarjafari et al., 2018)."
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": ""
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "affective computing in recent decades. Many achievements have been made in recognizing pain, and"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "this progress is through a generalization of the methodology developed from basic emotion recognition."
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "With the advent of wearable and portable technologies and dry electrode technology, researchers have"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "been"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "electroencephalography  (EEG)  and  Electrodermal  Activity  (EDA)  (i.e.,  monitoring  ICU  patients)"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "(Kächele et al., 2015; Walter et al., 2013). One of the most significant aspects of bio-signaling is that"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "physiological parameters may not be correlated with specific emotional states. However, they were"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "related  to  the  underlying  emotional  representation  dimensions  (valence,  arousal,  and  dominance)"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "(Schlosberg, 1954). Hence, physiological signals can provide additional information that may be useful"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "in estimating pain intensity (Gruss et al., 2015). A general understanding of the unimodal development"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "of pain recognition presented in this article can assist researchers in developing an optimal multimodal"
        },
        {
          "capture utilities with depth sensors can record high-quality data, allowing for future progress in this": "framework or system in the future (Fig.3)."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "neurological processes."
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "So far, there has been little attention on whether people can accurately recognize another person's"
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "pain and how humans can recognize pain states based on multimodal information, in particular. Studies"
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "of  human  cognition  may  enrich  perspectives  for  designing  new  research  directions  and  technology"
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "with Human-In-The-Loop (HITL). Under the framework of HITL, this section highlights technical"
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "developments  based  on  human  cognitive  systems,  such  as  multimodal  information  integration  and"
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "memory  systems,  which  we  regard  as  a  form  of  indirect  learning  from  humans  (Fig.  4  bottom)."
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "Furthermore, we propose that computational pain recognition can be improved by learning directly"
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "from human studies by focusing on how individuals perceive and respond to the pain of others (Fig. 4"
        },
        {
          "research  should  switch  its  attention  to  an  understanding  of  human  cognitive,  behavioural,  and": "top)."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: ). Unlike unimodal systems, multimodal",
      "data": [
        {
          "advantageous": "physiological signals in videos (see Supplementary Table 5). Unlike unimodal systems, multimodal",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "data fusion has substantially improved recognition accuracy (Ranganathan et al., 2016). Nevertheless,",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "it remains unclear at which stage cross-modal information intersects. The integration of audiovisual",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "speech with cross-modal processing may be in the early phases of nonprimary auditory brain activity",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "(Besle et al., 2008). Also, researchers suggested a hierarchy in which various brain regions encode",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "information independently and interact in the transient cortices (Campanella & Belin, 2007; Davies-",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "Thompson et al., 2019; Molnár et al., 2020). Similarly, the optimal time point for an AI to integrate",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "multimodal information (data fusion) remains controversial.",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "Memory and learning",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "ability to adjust to new situations (Williams & Lombrozo, 2013). Prior experience, such as personal",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "medical history, is a precious source of clinical information. Patients with different types of pain, i.e.,",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "acute  pain  and  chronic  pain,  show  a  distinct  way  of  processing  and  expressing  pain.  For  example,",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "researchers have found a bias in information processing when experiencing acute pain compared to",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "chronic pain (Moseley et al., 2005). In this regard, including information about a person in an AI could",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "be beneficial for recognizing pain in a medical context. At the same time, ethical and security standards",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "have to be considered.",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "differences  exist  (Peng  et  al.,  2020).  One  of  the  difficulties  in  pain  recognition  is  the  varying",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "expressions towards the same stimuli; that is, humans vary in their expressions in the same situation",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "(Grodal, 2007). Humans have individual differences in expressing pain, so the pattern of responses in",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "prior experiences with similar stimuli is a vital source of learning for pain analysis on specific targets.",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "A personalization of pain intensity estimation systems (Kächele et al., 2017) with multimodal analysis",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "can utilize memory and high-dimensional feature space to represent past information about pain.",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "activated when an episodic experience replays during resting and sleeping, which has been considered",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "a  process",
          "than  unimodal \nin": "integrates  short-  and",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": "long-term  memory  (Singer  &  Frank,  2009).  Specifically,"
        },
        {
          "advantageous": "researchers proposed an example-based manner by which memory \"replays\" itself offline to learn the",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "successes  or  failure  cases  that  occurred  in  the  past  (Hassabis  et  al.,  2017).  The  replay  process",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "emphasizes the recirculation of learning. For real-time pain monitoring, a great deal of information",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "from different devices converges in the back-end database. Long-short-term memory (LSTM), inspired",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "by the working memory framework, holds information to a fixed operational state until an acceptable",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "output is required (Hochreiter & Schmidhuber, 1997). LSTMs can contribute to instant detection by",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        },
        {
          "advantageous": "allowing  the  most  valuable  signals  to  be  processed  first  and  other  signals  to  be  processed  later.",
          "than  unimodal \nin": "",
          "identifying  pain \nfrom \nfacial  expressions,  head  posture,  and": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "threshold, or pain display characteristics to a person-specific feature space before training. Over time,"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "data can be trained offline to improve algorithms in specific environments (i.e., hospitals, homes, and"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "public places), similar to human episodic memory, replaying during sleep to realize a conceptual life-"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "long learning system (Sodhani et al., 2022)."
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "3.2 Direct learning from human studies"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "Modulation of pain expression through social contexts"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": ""
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "However, the pattern is context-dependent, meaning pain expressions vary in different contexts. Social"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "contexts can alter an individual's pain expression. For example, our discomfort toward aversive stimuli"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "is lower when we are in the company of others than when we are alone, referring to the social buffering"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "effect  (Langford  et  al.,  2006;  Qi  et  al.,  2020).  Meanwhile,  high  social"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "electrocutaneous stimuli administered by others, increase pain intensity and unpleasant feelings (Karos"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "et al., 2020). Hammal et al. (2008) suggested that context provides substantial information for pain"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "recognition.  These  explicit  expressions  are  influenced  by  our  environment.  For  example,  whether"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "people are surrounded by whom they trust (workplace, hospital, or home) (Bublatzky et al., 2020)."
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "Therefore, target separation and context are fundamental issues to address for improvement in real-life"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "applications of pain estimation."
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "Pain recognition and response to others’ pain in human studies"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": ""
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "into an AI? In most cases, it is beneficial to get inspired and guided by human studies. What is the"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "process by which humans recognize pain in others, and how do they react based on their understanding"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "of the pain state of others? Researchers used well-calibrated experimental pain models (i.e., heat, cold"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "pressure, or video) to generate stimuli that show the pain of others for observers to respond to (Bastian"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "et al., 2014; FeldmanHall et al., 2015). The observers rate the pain level of people in each stimulus,"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "and researchers record their neural signals (EEG, fMRI) of processing these stimuli as well as choices"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "observers make, such as prosocial choices (Jackson et al., 2005; Peng et al., 2021)."
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": ""
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "one \nthing,  stimuli  used \nfor  eliciting  human \nresponses \nto  others'  pain  have"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "(standardized labels of others' pain state, and physical features of the stimuli), which are the potential"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "input for any algorithms. In addition, the experiments collected human reactions in several dimensions"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "(behavioral ratings or decisions to help people in pain, which is a feasible output/end for an AI to learn)."
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "This line of study can shed light on the \"black box\" systems of the human cognitive system between"
        },
        {
          "However,  an  advanced  application  would  require  translating  an  individual's  past  experience,  pain": "the input of victims in pain and the output of observers' reactions. Ultimately, the goal is to explore"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "benefit from human cognitive systems."
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "3.3 Prospects in pain recognition"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": ""
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "We introduced some established databases, followed by an overview of the progress in face-, voice-"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "and  gesture-based  pain"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "prerequisites, all single modalities are restricted. First, there is a shortage of sufficient data, including"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "targets with different pain levels and real-life big clinical data (i.e., spontaneous painful expressions"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "of different modalities). Also, realistic scenarios are often ignored. Second, multimodal pain models"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "ought  to  be  prepared  on  substantial  information  from  various  contexts  to  construct  generalizable"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "models. Third, a meta-analysis of diverse methods regarding specific modalities provides insights into"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "pain  recognition.  For  example,  what  information  can  be  weighted  more  than  the  other  to  improve"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "performance?  How  can  recognition  accuracy  be  enhanced  by  combining  the  outputs  of  different"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "independent models? Specifically, ensemble learning, which combines several weak classifiers into"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "one integrated classifier, is a potential solution for building better models of pain analysis. Moreover,"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "it is also a way to address how to deal with multisensory information processing, and how intelligent"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "recognition can be done in the real world with more available signals in the future (Fig. 3)."
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "4 Beyond pain recognition"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": ""
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "psychological perspective (Cameron et al., 2019; Fehr & Fischbacher, 2003; Staub & Vollhardt, 2008)."
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "Empathic  people  can  recognize  and  comprehend  others'  emotions  by  experiencing  and  sharing  the"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "emotional  states  of  others  (Singer,  2006).  However,  the  substantial  relationship  between  pain  and"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "empathy has not been explored in AI research. An AI agent with human-like empathic responses is"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "considered more caring, likeable, and trustworthy (Rodrigues et al., 2015). In the current state-of-the-"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "art, one question is, \"can an AI express empathy at all?\". There is still a long way to go before we get"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "closer to this question. However, could the recognition of pain, which has relatively clear and definite"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "features for an AI agent to learn, lay the foundation for artificial pain empathy? There are two steps"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "for an AI to implement pain empathy: first, the identification of pain, and second, the expression of"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "empathy (Khatibi & Mazidi, 2019). The above sections have reviewed the pain recognition progress."
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "In this section, we describe a relatively novel field associated with an affective AI — artificial pain"
        },
        {
          "pain  recognition  from  a  human  perspective  and  to  allow  current  computational  pain  recognition  to": "empathy."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.1 Artificial pain empathy": "Mimicry"
        },
        {
          "4.1 Artificial pain empathy": "Pain  empathy  is  the  capacity  associated  with  feeling  and  evaluating  others'  pain  states  and"
        },
        {
          "4.1 Artificial pain empathy": "understanding others, often prompting prosocial actions (De Waal, 2008; Fitzgibbon et al., 2010; Wang"
        },
        {
          "4.1 Artificial pain empathy": "et  al.,  2019).  Studies  from  animals  and  humans  suggested  that  mimicry  of  body  movement  is  the"
        },
        {
          "4.1 Artificial pain empathy": "underlying mechanism for empathy which derives from a mirror system in the human brain (Rizzolatti"
        },
        {
          "4.1 Artificial pain empathy": "& Craighero, 2004). In social interactions, emotional mimicry is crucial since it reflects a desire to"
        },
        {
          "4.1 Artificial pain empathy": "connect with another person (Hess & Fischer, 2013). Thus, the initial step to addressing pain empathy"
        },
        {
          "4.1 Artificial pain empathy": "is to recognize and imitate human facial expressions or gestures in real-time. To some extent, mimicry"
        },
        {
          "4.1 Artificial pain empathy": "generates  similarity  and,  in  turn,  facilitates  people's  empathy  toward  a  human-designed  machine"
        },
        {
          "4.1 Artificial pain empathy": "(Breazeal, 2003). Miura et al. (2008) found that human-like body movements make it easier for people"
        },
        {
          "4.1 Artificial pain empathy": "to empathize and the embodiment of an interactive partner influences human mimicry behaviors. A"
        },
        {
          "4.1 Artificial pain empathy": "physical  presence  and  human-like  artificial  entity  tend  to  generate  more  mimicry  than  virtual  non-"
        },
        {
          "4.1 Artificial pain empathy": "human  counterparts  (Perugia  et  al.,  2020).  In  addition,  emotional  mimicry  by  robots  may  show  its"
        },
        {
          "4.1 Artificial pain empathy": "empathic \"trait\", improving the human-robot interaction experience (Leite et al., 2012)."
        },
        {
          "4.1 Artificial pain empathy": "Modulation by top-down cognitive and presence of social partners"
        },
        {
          "4.1 Artificial pain empathy": "What functions can AI agents or robots have when they cannot feel suffering like humans? First,"
        },
        {
          "4.1 Artificial pain empathy": "some insights can be gained from studies on people born with the congenital absence of pain. Danziger"
        },
        {
          "4.1 Artificial pain empathy": "et al. (2009) found that pain-related brain regions of congenitally pain-free patients are activated when"
        },
        {
          "4.1 Artificial pain empathy": "seeing others experiencing pain, indicating a shared synchrony pain aversion with others. Krishnan et"
        },
        {
          "4.1 Artificial pain empathy": "al.  (2016)  found  that  the  experience  of  vicarious  pain  (observing  others  in  pain)  is  neurologically"
        },
        {
          "4.1 Artificial pain empathy": "separate from experiencing actual pain on our own, suggesting that empathy is more cognitive than"
        },
        {
          "4.1 Artificial pain empathy": "sensational. Thus, empathy not only has the intrinsic characteristics of sensibility but also comprises"
        },
        {
          "4.1 Artificial pain empathy": "the process of top-down cognitive regulation.  Heyes (2018) proposed a dual-system model of empathy."
        },
        {
          "4.1 Artificial pain empathy": "The model includes both early views in which empathy largely depends on a bottom-up process, a"
        },
        {
          "4.1 Artificial pain empathy": "spontaneous response activated by stimuli (system Ⅰ). Meanwhile, it also covers the control mechanism"
        },
        {
          "4.1 Artificial pain empathy": "of empathy that belongs to a top-down process, indicating that high-level cognitive systems contribute"
        },
        {
          "4.1 Artificial pain empathy": "to the regulation of empathy (system Ⅱ). The two-system model of empathy lays the foundation for"
        },
        {
          "4.1 Artificial pain empathy": "the investigations on artificial empathy from a cognitive perspective."
        },
        {
          "4.1 Artificial pain empathy": "Second,  the  social  buffering  effect  indicates  that  the  presence  of  social  partners  effectively"
        },
        {
          "4.1 Artificial pain empathy": "modulates human reactions toward aversive stimuli (Qi et al., 2020). A recent study on rodents found"
        },
        {
          "4.1 Artificial pain empathy": "a neural circuit of the buffering effect. Their results showed that brief social interaction with a peer"
        },
        {
          "4.1 Artificial pain empathy": "mouse  experiencing  pain  or  morphine  analgesia  resulted  in  the  transfer  of  these  experiences  to  its"
        },
        {
          "4.1 Artificial pain empathy": "social partner (Smith et al., 2021). Hence, a theoretical hypothesis is suggested that painless robots can"
        },
        {
          "4.1 Artificial pain empathy": "serve as analgesic companions, thereby reducing people's pain perception. In fact, mental support is"
        },
        {
          "4.1 Artificial pain empathy": "more  human-centered  and  user-friendly  than  realizing  high-level  pain  recognition.  Some  effort  has"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "Southern California created an empathic AI system that functions as a virtual counselor, and mainly"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "serves veterans with post-traumatic stress disorder (Gaggioli, 2017)."
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": ""
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "sharing  demonstration  of  artificial  agents  increases  the  possibility  of  building  trust  with  people.  In"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "addition, practices in human studies showed that neuromodulation, an array of non-invasive, minimally"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "invasive, and surgical electrical therapies, is conducive to pain relief (Knotkova et al., 2021). People"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "can self-adjust their feelings by training them with positive feedback when making target responses,"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "and  the  technique  is  called  neurofeedback.  Researchers  have  found  evidence  that  the  sensory-"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "discriminative aspect of pain is associated with EEG signals that are deliberatively trained. However,"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "due"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "neurofeedback for pain regulation (Peng et al., 2020)."
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "4.2 Empathy training for humans"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": ""
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "model. The generative models that generate human images and speech have come close to equivalents"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "in the real-world (Oord et al., 2016). On the one hand, synthetic data can be used to train the recognition"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "models. On the other hand, the data can be applied to the training process for medical caregivers to"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "improve people's estimation of pain intensity. A recent study indicated the feasibility of such training"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "pain.  People  underwent  a  3.5  to  5  hours  online  training  program  called  the  index  of  facial  pain"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "expression  (IFPE).  After"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "psychiatric pain expressions (Rash et al., 2019). Hence, an important task for future artificial agents is"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "to help people with emotional deficiency to establish the ability to perceive, interpret, express, and"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "regulate emotions (Javed & Park, 2019)."
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": ""
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "presence  of  them  and  simple  pain-sharing  displays  can  have  a  significant  effect  in  reducing  pain"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "perception. More sophisticated neuromodulation is also feasible in the future application of artificial"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "medical assistants. To jointly contribute to the field, it is required to integrate psychology, ethics, and"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "cognitive neuroscience."
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "4.3 Ethical considerations"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": ""
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "for future work on affective assistants. Ideally, computational pain recognition can be applied to assist"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "people in pain. Artificial empathy refers to the capacity of computer systems to recognize and respond"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "to people's behaviors, expressions, and emotions (Asada, 2015a, 2015b; Asada et al., 2012). With each"
        },
        {
          "been put into this practice. For example, the Institute for Creative Technologies at the University of": "interaction  people  have  with  an AI  system,  adaptive  artificial  empathy  can  be  expected  (Harris  &"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "developing  algorithms  that  can  generate  empathic  expressions.  Furthermore,  an AI  agent  needs  to"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "recognize and generate an empathic response to others' pain experiences. However, research has not"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "yet fully considered an affective AI with proper responses to others' pain experiences. By learning from"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "human-human  interaction,  an AI  system  could  also  learn  human  behaviors  by  interacting  with  real"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "people in an \"empathic\" way (Paiva et al., 2017)."
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": ""
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "concept is more than modeling humans (James et al., 2018; Putta et al., 2022; Srinivasan & San Miguel"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "González,  2022).  There  are  important  issues  to  be  considered:  What  artificial  systems  could  be"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "advantageous and beneficial for human life? What boundaries do such service functions need to avoid"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "crossing? Generally, the precision, flexibility, and convenience of advanced services require personal"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "information from users. Therefore, the privacy and security of personal data need to be considered"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "highly. Rules for accountability require legal support and government supervision. More significantly,"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "if  artificial  systems  are  launched,  both  the  creator  and  the  user  must  be  fully  informed  about  the"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "information"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "Trustworthiness and understanding of AI systems' limitations, like algorithmic bias by biased data, are"
        },
        {
          "Sharlin,  2010).  In  this  paper,  we  reviewed  some  existing  databases,  which  may  be  helpful  for": "important topics to address, particularly when people interact with artificial empathy services."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "Towards  Artificial"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "https://doi.org/10.1007/s12369-014-0253-z"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "painful face: Pain expression recognition using active appearance models."
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "Neural computation, 7(6), 1129-1159."
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "Audiovisual Interactions in the Auditory Cortex during Speech Perception: Intracranial Recordings in Humans."
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "The Journal of Neuroscience, 28, 14301-14310."
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "Pain assessment with short message service and interactive voice response in outpatients with cancer and pain: a"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "feasibility study. Pain Practice, 16(3), 320-326."
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "Recognition  Areas.  Journal  of  Neuroscience,  31(36),  12906-12915.  https://doi.org/10.1523/jneurosci.2091-"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "to \nRecognize"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "https://doi.org/10.1002/hbm.22631"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "about ambiguous facial emotions: Behavioral and magnetoencephalographic correlates. Neuroimage, 215, 116814."
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "https://doi.org/https://doi.org/10.1016/j.neuroimage.2020.116814"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "hard work: People choose to avoid empathy because of its cognitive costs. Journal of Experimental Psychology:"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "General, 148(6), 962-976. https://doi.org/10.1037/xge0000595"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "535-543. https://doi.org/10.1016/j.tics.2007.10.001"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "with \nCongenital"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "https://doi.org/10.1016/j.neuron.2008.11.023"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "for  Face  and  Voice"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "https://doi.org/10.1093/cercor/bhy240"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": "Psychology, 59(1), 279-300. https://doi.org/10.1146/annurev.psych.59.103006.093625"
        },
        {
          "https://doi.org/https://doi.org/10.1016/j.neures.2014.12.002": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "in  a  3-D  activation-valence-time  continuum  using  acoustic  and  linguistic  cues.  Journal  on  Multimodal  User"
        },
        {
          "Alto: Consulting Psychologist Press.": "Interfaces, 3(1-2), 7-19."
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "extractor. Proceedings of the 18th ACM international conference on Multimedia,"
        },
        {
          "Alto: Consulting Psychologist Press.": "Fischbacher,"
        },
        {
          "Alto: Consulting Psychologist Press.": "https://www.nature.com/articles/nature02043.pdf"
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "empathy"
        },
        {
          "Alto: Consulting Psychologist Press.": "https://doi.org/10.1016/j.neubiorev.2009.10.007"
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "Computational Models Learn From Human Selective Attention? A Review From an Audiovisual Unimodal and"
        },
        {
          "Alto: Consulting Psychologist Press.": "Crossmodal Perspective. Frontiers in Integrative Neuroscience, 14."
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "Analyzing  Pain  Responses  to  Thermal  and  Electrical  Stimuli.  Journal  of  visualized  experiments  :  JoVE,  146."
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "rates via biopotential feature patterns with support vector machines. PloS one, 10(10)."
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "Visions of Computer Science - BCS International Academic Conference, Imperial College, London, UK, 22-24"
        },
        {
          "Alto: Consulting Psychologist Press.": "September 2008,"
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "speech enhancement approach for low SNR and non-stationary noise."
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "5th ACM/IEEE International Conference on Human-Robot Interaction (HRI),"
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "conditional process modeling. In: University of Kansas, KS."
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "Physiology, 148(3), 574-591. https://doi.org/10.1113/jphysiol.1959.sp006308"
        },
        {
          "Alto: Consulting Psychologist Press.": ""
        },
        {
          "Alto: Consulting Psychologist Press.": "Emotions in Speech. 2018 27th IEEE International Symposium on Robot and Human Interactive Communication"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(RO-MAN),": "Javed, H., & Park, C. H. (2019). Interactions With an Empathetic Agent: Regulating Emotions and Improving Engagement"
        },
        {
          "(RO-MAN),": "in Autism. IEEE Robotics & Automation Magazine, 26(2), 40-48. https://doi.org/10.1109/MRA.2019.2904638"
        },
        {
          "(RO-MAN),": "Jia,  Y.,  Weiss,  R.  J.,  Biadsy,  F.,  Macherey,  W.,  Johnson,  M.,  Chen,  Z.,  &  Wu,  Y.  (2019).  Direct  speech-to-speech"
        },
        {
          "(RO-MAN),": "translation with a sequence-to-sequence model. arXiv preprint arXiv:1904.06037."
        },
        {
          "(RO-MAN),": "Kächele, M., Amirian, M., Thiam, P., Werner, P., Walter, S., Palm, G., & Schwenker, F. (2017). Adaptive confidence"
        },
        {
          "(RO-MAN),": "learning for the personalization of pain intensity estimation systems. Evolving Systems, 8, 71-83."
        },
        {
          "(RO-MAN),": "Kächele, M., Werner, P., Al-Hamadi, A., Palm, G., Walter, S., & Schwenker, F. (2015). Bio-visual fusion for person-"
        },
        {
          "(RO-MAN),": "independent recognition of pain intensity. International Workshop on Multiple Classifier Systems,"
        },
        {
          "(RO-MAN),": "Kaltwang, S., Rudovic, O., & Pantic, M. (2012). Continuous pain intensity estimation from facial expressions. International"
        },
        {
          "(RO-MAN),": "Symposium on Visual Computing,"
        },
        {
          "(RO-MAN),": "Karos, K., Meulders, A., Goubert, L., & Vlaeyen, J. W. S. (2020). Hide Your Pain: Social Threat Increases Pain Reports"
        },
        {
          "(RO-MAN),": "and  Aggression,  but  Reduces  Facial  Pain  Expression  and  Empathy.  The  Journal  of  Pain,  21(3),  334-346."
        },
        {
          "(RO-MAN),": "https://doi.org/https://doi.org/10.1016/j.jpain.2019.06.014"
        },
        {
          "(RO-MAN),": "Khan, R. A., Meyer, A., Konik, H., & Bouakaz, S. (2013). Pain detection through shape and appearance features. 2013"
        },
        {
          "(RO-MAN),": "IEEE International Conference on Multimedia and Expo (ICME),"
        },
        {
          "(RO-MAN),": "Khatibi, A., & Mazidi, M. (2019). Observers' impression of the person in pain influences their pain estimation and tendency"
        },
        {
          "(RO-MAN),": "to help. European Journal of Pain, 23(5), 936-944. https://doi.org/10.1002/ejp.1361"
        },
        {
          "(RO-MAN),": "Knotkova,  H.,  Hamani,  C.,  Sivanesan,  E.,  Le  Beuffe,  M.  F.  E.,  Moon,  J.  Y.,  Cohen,  S.  P.,  &  Huntoon,  M.  A.  (2021)."
        },
        {
          "(RO-MAN),": "Neuromodulation for chronic pain. The Lancet, 397(10289), 2111-2124."
        },
        {
          "(RO-MAN),": "Koch, C., & Ullman, S. (1987). Shifts in selective visual attention: towards the underlying neural circuitry. In Matters of"
        },
        {
          "(RO-MAN),": "intelligence (pp. 115-141). Springer."
        },
        {
          "(RO-MAN),": "Krishnan, A., Woo, C.-W., Chang, L. J., Ruzic, L., Gu, X., López-Solà, M., Jackson, P. L., Pujol, J., Fan, J., & Wager, T."
        },
        {
          "(RO-MAN),": "D. (2016). Somatic and vicarious pain are represented by dissociable multivariate brain patterns. Elife, 5, e15166."
        },
        {
          "(RO-MAN),": "https://doi.org/10.7554/eLife.15166"
        },
        {
          "(RO-MAN),": "Kunz, M., Peter, J., Huster, S., & Lautenbacher, S. (2013). Pain and disgust: The facial signaling of two aversive bodily"
        },
        {
          "(RO-MAN),": "experiences. PloS one, 8(12), e83277."
        },
        {
          "(RO-MAN),": "Langford, D. J., Crager, S. E., Shehzad, Z., Smith, S. B., Sotocinal, S. G., Levenstadt, J. S., Chanda, M. L., Levitin, D. J.,"
        },
        {
          "(RO-MAN),": "& Mogil, J. S. (2006). Social Modulation of Pain as Evidence for Empathy in Mice. Science, 312(5782), 1967-"
        },
        {
          "(RO-MAN),": "1970. https://doi.org/10.1126/science.1128322"
        },
        {
          "(RO-MAN),": "Leite, I., Pereira, A., Mascarenhas, S., Martinho, C., Prada, R., & Paiva, A. (2012). The influence of empathy in human–"
        },
        {
          "(RO-MAN),": "robot relations. International journal of human-computer studies, 71. https://doi.org/10.1016/j.ijhcs.2012.09.005"
        },
        {
          "(RO-MAN),": "Lieberman,  M.  D.,  &  Eisenberger,  N.  I.  (2009).  Pains  and  pleasures  of  social  life.  Science,  323(5916),  890-891."
        },
        {
          "(RO-MAN),": "https://science.sciencemag.org/content/323/5916/890.long"
        },
        {
          "(RO-MAN),": "Lockwood,  P.  L.,  Klein-Flügge,  M.  C.,  Abdurahman,  A.,  &  Crockett,  M.  J.  (2020).  Model-free  decision  making  is"
        },
        {
          "(RO-MAN),": "prioritized when learning to avoid harming others. Proceedings of the National Academy of Sciences, 117(44),"
        },
        {
          "(RO-MAN),": "27719-27730."
        },
        {
          "(RO-MAN),": "Martinez, G. H., Raaj, Y., Idrees, H., Xiang, D., Joo, H., Simon, T., & Sheikh, Y. (2019, 2019-10-01). Single-Network"
        },
        {
          "(RO-MAN),": "Whole-Body Pose Estimation. 2019 IEEE/CVF International Conference on Computer Vision (ICCV),"
        },
        {
          "(RO-MAN),": "Mauricio, A., Cappabianco, F., Veloso, A., & Cámara, G. (2019). A Sequential Approach for Pain Recognition Based on"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "https://doi.org/10.1007/978-3-030-34995-0_27"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Miura, N., Sugiura, M., Takahashi, M., Moridaira, T., Miyamoto, A., Kuroki, Y., & Kawashima, R. (2008). An advantage"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "of  bipedal  humanoid  robot  on  the  empathy  generation:  A  neuroimaging  study.  2008  IEEE/RSJ  International"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Conference on Intelligent Robots and Systems,"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Molnár, Z., Luhmann, H., & Kanold, P. (2020). Transient cortical circuits match spontaneous and sensory-driven activity"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "during development. Science 370, eabb2153. In."
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Moseley,  G.  L.,  Sim,  D.  F.,  Henry,  M.  L.,  &  Souvlis,  T.  (2005).  Experimental  hand  pain  delays  recognition  of  the"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "contralateral  hand—Evidence  that  acute  and  chronic  pain  have  opposite  effects  on  information  processing?"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Cognitive  Brain  Research,  25(1),  188-194.  https://doi.org/https://doi.org/10.1016/j.cogbrainres.2005.05.008"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Noroozi, F., Kaminska, D., Corneanu, C., Sapinski, T., Escalera, S., & Anbarjafari, G. (2018). Survey on emotional body"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "gesture recognition. IEEE transactions on affective computing."
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Noroozi, F., Sapiński, T., Kamińska, D., & Anbarjafari, G. (2017). Vocal-based emotion recognition using random forests"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "and decision tree. International Journal of Speech Technology, 20(2), 239-246. https://doi.org/10.1007/s10772-"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "017-9396-2"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., & Kavukcuoglu,"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "K. (2016). Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499."
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for Physical"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Pain Detection. https://doi.org/10.21437/SpeechProsody.2016-86"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Paiva,  A.,  Leite,  I.,  Boukricha,  H.,  &  Wachsmuth,  I.  (2017).  Empathy  in  virtual  agents  and  robots:  a  survey.  ACM"
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Transactions on Interactive Intelligent Systems (TiiS), 7(3), 1-40."
        },
        {
          "Facial Representations. In Lecture Notes in Computer Science (pp. 295-304). Springer International Publishing.": "Parisi, G. I., Magg, S., & Wermter, S. (2016, 2016). Human motion assessment in real time using recurrent self-organization."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "another  person  reduces  human  autonomic  responses  to  aversive  sounds.  Proceedings  of  the  Royal  Society  B,"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "287(1919), 20192241."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Raja, S. N., Carr, D. B., Cohen, M., Finnerup, N. B., Flor, H., Gibson, S., Keefe, F. J., Mogil, J. S., Ringkamp, M., Sluka,"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "K. A., Song, X.-J., Stevens, B., Sullivan, M. D., Tutelman, P. R., Ushida, T., & Vader, K. (2020). The revised"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "International Association for the Study of Pain definition of pain: concepts, challenges, and compromises. Pain,"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "161(9), 1976-1982. https://doi.org/10.1097/j.pain.0000000000001939"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Ranganathan,  H.,  Chakraborty,  S.,  &  Panchanathan,  S.  (2016).  Multimodal  emotion  recognition  using  deep  learning"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "architectures. 2016 IEEE Winter Conference on Applications of Computer Vision (WACV),"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Rash,  J.  A.,  Prkachin,  K.  M.,  Solomon,  P.  E.,  &  Campbell,  T.  S.  (2019).  Assessing  the  efficacy  of  a  manual-based"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "intervention for improving the detection of facial pain expression. European Journal of Pain, 23(5), 1006-1019."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "https://doi.org/10.1002/ejp.1369"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Rizzolatti, G., & Craighero, L. (2004). THE MIRROR-NEURON SYSTEM. Annual Review of Neuroscience, 27(1), 169-"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "192."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Rodrigues,  S.  H.,  Mascarenhas,  S.,  Dias,  J.,  &  Paiva,  A.  (2015).  A  Process  Model  of  Empathy  For  Virtual  Agents."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Interacting with Computers, 27(4), 371-391. https://doi.org/10.1093/iwc/iwu001"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Sado, F., Chu, Wei, Kerzel, M., & Wermter, S. (2021). Explainable Goal-Driven Agents and Robots -- A Comprehensive"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Review. arXiv pre-print server. https://doi.org/None"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "arxiv:2004.09705"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Semwal, A., & Londhe, N. D. (2021). Head Movement Dynamics based Pain Detection using Spatio-Temporal Network."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "2021 8th International Conference on Signal Processing and Integrated Networks (SPIN),"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Singer,  A.  C.,  &  Frank,  L.  M.  (2009).  Rewarded  Outcomes  Enhance  Reactivation  of  Experience  in  the  Hippocampus."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Neuron, 64(6), 910-921. https://doi.org/https://doi.org/10.1016/j.neuron.2009.11.016"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Singer, T. (2006). The empathic brain: how, when and why? Trends in Cognitive Sciences, 10(10), 435-441."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Smith,  M.  L.,  Asada,  N.,  &  Malenka,  R.  C.  (2021).  Anterior  cingulate  inputs  to  nucleus  accumbens  control  the  social"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "transfer of pain and analgesia. Science, 371(6525), 153-159. https://doi.org/10.1126/science.abe3040"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Sodhani,  S.,  Faramarzi,  M.,  Mehta,  S.  V.,  Malviya,  P.,  Abdelsalam,  M.,  Janarthanan,  J.,  &  Chandar,  S.  (2022).  An"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Introduction to Lifelong Supervised Learning. arXiv preprint arXiv:2207.04354."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Srinivasan, R., & San Miguel González, B. (2022). The role of empathy for artificial intelligence accountability. Journal"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "of Responsible Technology, 9, 100021. https://doi.org/https://doi.org/10.1016/j.jrt.2021.100021"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Stathopoulou, I.-O., & Tsihrintzis, G. A. (2011). Emotion recognition from body movements and gestures. In Intelligent"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Interactive Multimedia Systems and Services (pp. 295-303). Springer."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Staub, E., & Vollhardt, J. (2008). Altruism born of suffering: The roots of caring and helping after victimization and other"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "trauma. American Journal of Orthopsychiatry, 78(3), 267-280."
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Thiam,  P.,  &  Schwenker,  F.  (2017,  28  Nov.-1  Dec.  2017).  Multi-modal  data  fusion  for  pain  intensity  assessment  and"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "classification.  2017  Seventh  International  Conference  on  Image  Processing  Theory,  Tools  and  Applications"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "(IPTA),"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Tuduce,  R.  I.,  Cucu,  H.,  &  Burileanu,  C.  (2018,  4-6  July  2018).  Why  is  My  Baby  Crying?  An  In-Depth  Analysis  of"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Paralinguistic  Features  and  Classical  Machine  Learning  Algorithms  for  Baby  Cry  Classification.  2018  41st"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "International Conference on Telecommunications and Signal Processing (TSP),"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Walter, S., Gruss, S., Ehleiter, H., Tan, J., Traue, H. C., Werner, P., Al-Hamadi, A., Crawcour, S., Andrade, A. O., & da"
        },
        {
          "Qi, Y., Herrmann, M. J., Bell, L., Fackler, A., Han, S., Deckert, J., & Hein, G. (2020). The mere physical presence of": "Silva,  G.  M.  (2013).  The  biovid  heat  pain  database  data  for  the  advancement  and  systematic  validation  of  an"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "Wang, C., Zhang, T., Shan, Z., Liu, J., Yuan, D., & Li, X. (2019). Dynamic interpersonal neural synchronization underlying"
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "pain‐induced cooperation in females. Human Brain Mapping, 40(11), 3222-3232."
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "Werner, P., Al-Hamadi, A., Limbrecht-Ecklundt, K., Walter, S., & Traue, H. C. (2018). Head movements and postures as"
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "pain behavior. PloS one, 13(2), e0192767. https://doi.org/10.1371/journal.pone.0192767"
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "Williams.  (2002).  Facial  expression  of  pain:  an  evolutionary  account.  Behavioral  and  Brain  Sciences,  25(4),  439-455."
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "Williams, J. J., & Lombrozo, T. (2013). Explanation and prior knowledge interact to guide learning. Cognitive psychology,"
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "66(1), 55-84."
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "Zhou, J., Hong, X., Su, F., & Zhao, G. (2016). Recurrent convolutional neural network regression for continuous pain"
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "intensity estimation in video. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
          "automated pain recognition system. 2013 IEEE international conference on cybernetics (CYBCO),": "Workshops,"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Supplementary Information": "Can AI detect pain and express pain empathy? A review from emotion"
        },
        {
          "Supplementary Information": "recognition and a human-centered AI perspective"
        },
        {
          "Supplementary Information": "Running head: Pain Recognition and Empathic Expression in AI"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table 1: Pain databases",
      "data": [
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": "All  videos \nrecord \nthe"
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": "faces \nof \nparticipants"
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": "with \nshoulder \npain."
        },
        {
          "Database": "UNBC-McMaster",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": ""
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "129 adults",
          "Reference": "",
          "Description": "Participants \nare"
        },
        {
          "Database": "Shoulder Pain",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "Lucey \net",
          "Description": ""
        },
        {
          "Database": "",
          "Type": "spontaneous",
          "Modality": "video",
          "Subjects": "shoulder pain",
          "Reference": "",
          "Description": "instructed  to  perform  a"
        },
        {
          "Database": "Expression Archive",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "al. (2011)",
          "Description": ""
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "patients",
          "Reference": "",
          "Description": "sequence  of  tests  that"
        },
        {
          "Database": "(Lucey et al., 2011)",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": ""
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": "require \nmotions \nof"
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": "limbs \nin \ntwo  different"
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": "scenarios."
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": "All videos include basic"
        },
        {
          "Database": "STOIC database",
          "Type": "",
          "Modality": "",
          "Subjects": "10 actors",
          "Reference": "Roy et al.",
          "Description": ""
        },
        {
          "Database": "",
          "Type": "acted",
          "Modality": "video",
          "Subjects": "",
          "Reference": "",
          "Description": "emotions, painful and"
        },
        {
          "Database": "(Roy et al., 2007)",
          "Type": "",
          "Modality": "",
          "Subjects": "(age 20-45)",
          "Reference": "(2007)",
          "Description": ""
        },
        {
          "Database": "",
          "Type": "",
          "Modality": "",
          "Subjects": "",
          "Reference": "",
          "Description": "neutral expressions."
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "(Harrison et al.,",
          "142 infants": "(age 0-12",
          "Harrison": "et al."
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "2014)",
          "142 infants": "months)",
          "Harrison": "(2014)"
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "The SenseEmotion",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "45 healthy",
          "Harrison": "Velana et"
        },
        {
          "YouTube Dataset": "Database (Velana",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "subjects",
          "Harrison": "al. (2017)"
        },
        {
          "YouTube Dataset": "et al., 2017)",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "X-ITE Pain",
          "142 infants": "134 healthy",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": "Gruss et"
        },
        {
          "YouTube Dataset": "Database (Gruss et",
          "142 infants": "adults (age",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": "al. (2019)"
        },
        {
          "YouTube Dataset": "al., 2019)",
          "142 infants": "18-50)",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "Duesseldorf Acute",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "80 subjects",
          "Harrison": "Ren et al."
        },
        {
          "YouTube Dataset": "Pain (DAP) Corpus",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "(age 18-70)",
          "Harrison": "(2018)"
        },
        {
          "YouTube Dataset": "(Ren et al., 2018)",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "The iCOPEvid",
          "142 infants": "",
          "Harrison": "Brahnam"
        },
        {
          "YouTube Dataset": "dataset (Brahnam",
          "142 infants": "49 neonates",
          "Harrison": "et al."
        },
        {
          "YouTube Dataset": "et al., 2020)",
          "142 infants": "",
          "Harrison": "(2020)"
        },
        {
          "YouTube Dataset": "Skin Conductance Level (SCL), Electrocardiogram (ECG), Electromyogram (EMG), surface",
          "142 infants": "",
          "Harrison": ""
        },
        {
          "YouTube Dataset": "Electromyographic study (sEMG), Respiration (RSP), and Electroencephalogram (EEG).",
          "142 infants": "",
          "Harrison": ""
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 2: Pain recognition based on facial expression",
      "data": [
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "References"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Lucey et al. (2009)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Lucey et al. (2011)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Florea et al. (2014)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Kaltwang et al. (2012)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Chen et al. (2012)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Khan et al. (2013)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Pedersen (2015)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Zhou et al. (2016)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Zhao et al. (2016)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Rodriguez et al. (2017)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Xin et al. (2020)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": "Bargshady et al. (2020)"
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        },
        {
          "Table 2 Pain recognition based on facial expression": ""
        }
      ],
      "page": 25
    },
    {
      "caption": "Table 3: Pain recognition based on voice",
      "data": [
        {
          "Table 3 Pain recognition based on voice": "References"
        },
        {
          "Table 3 Pain recognition based on voice": ""
        },
        {
          "Table 3 Pain recognition based on voice": "Tsai et al. (2017)"
        },
        {
          "Table 3 Pain recognition based on voice": ""
        },
        {
          "Table 3 Pain recognition based on voice": ""
        },
        {
          "Table 3 Pain recognition based on voice": "Oshrat et al. (2016)"
        },
        {
          "Table 3 Pain recognition based on voice": ""
        },
        {
          "Table 3 Pain recognition based on voice": ""
        },
        {
          "Table 3 Pain recognition based on voice": "Hossain (2016)"
        },
        {
          "Table 3 Pain recognition based on voice": ""
        },
        {
          "Table 3 Pain recognition based on voice": ""
        },
        {
          "Table 3 Pain recognition based on voice": "Tuduce (2018)"
        },
        {
          "Table 3 Pain recognition based on voice": ""
        },
        {
          "Table 3 Pain recognition based on voice": "Gaussian Mixture Model (GMM); Waikato Environment for Knowledge Analysis (WEKA); Deep Bottleneck"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table 4: Pain recognition based on body",
      "data": [
        {
          "Table 4 Pain recognition based on body": "References"
        },
        {
          "Table 4 Pain recognition based on body": ""
        },
        {
          "Table 4 Pain recognition based on body": "Olugbade et al. (2014)"
        },
        {
          "Table 4 Pain recognition based on body": ""
        },
        {
          "Table 4 Pain recognition based on body": "Saha et al. (2015)"
        },
        {
          "Table 4 Pain recognition based on body": "Olugbade et al. (2015)"
        },
        {
          "Table 4 Pain recognition based on body": ""
        },
        {
          "Table 4 Pain recognition based on body": "Saha et al. (2016)"
        },
        {
          "Table 4 Pain recognition based on body": ""
        }
      ],
      "page": 27
    },
    {
      "caption": "Table 5: State of the art of multimodal pain recognition",
      "data": [
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "References"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Thiam and"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Schwenker (2017)"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Werner et al."
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "(2019)"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Kächele et al."
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "(2015b)"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Haque et al."
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "(2018)"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Kessler et al."
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "(2017)"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Salekin et al."
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "(2021)"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Rivas xet al."
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "(2020)"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Badura et al."
        },
        {
          "Table 5 State of the art of multimodal pain recognition": ""
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "(2021)"
        },
        {
          "Table 5 State of the art of multimodal pain recognition": "Face (F); audio (A); Bio-physiology (Bio); RGB Depth and Thermal (RGBDT); Geature (G); Finger pressure (FP); Hand"
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": ""
        },
        {
          "References": "&  Shafizadeh,  M.  (2015).  The  automatic  detection  of  chronic  pain-related  expression:  requirements,"
        },
        {
          "References": "challenges and the multimodal EmoPain dataset. IEEE transactions on affective computing, 7(4), 435-451."
        },
        {
          "References": ""
        },
        {
          "References": "Physiotherapy Using Wavelet Scattering Transform. Sensors, 21(4), 1311."
        },
        {
          "References": ""
        },
        {
          "References": "intensity \nfrom \nfacial  expressions.  Artificial"
        },
        {
          "References": "https://doi.org/10.1016/j.artmed.2020.101954"
        },
        {
          "References": ""
        },
        {
          "References": "displays \nof \nacute \npain. \nArtificial \nIntelligence"
        },
        {
          "References": "https://doi.org/https://doi.org/10.1016/j.artmed.2004.12.003"
        },
        {
          "References": ""
        },
        {
          "References": "detection in videos using the iCOPEvid dataset and an ensemble of descriptors extracted from Gaussian of"
        },
        {
          "References": "Local Descriptors. Applied Computing and Informatics."
        },
        {
          "References": ""
        },
        {
          "References": "19th IEEE International Conference on Image Processing,"
        },
        {
          "References": ""
        },
        {
          "References": "intensity estimation. European Conference on Computer Vision,"
        },
        {
          "References": ""
        },
        {
          "References": "for Analyzing Pain Responses to Thermal and Electrical Stimuli. Journal of visualized experiments : JoVE,"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "Anbarjafari, G., & Nasrollahi, K. (2018). Deep multimodal pain recognition: a database and comparison of"
        },
        {
          "References": "spatio-temporal visual modalities. 2018 13th IEEE International Conference on Automatic Face & Gesture"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "Pound, C. M. (2014). Too many crying babies: a systematic review of pain management practices during"
        },
        {
          "References": "immunizations on YouTube. BMC Pediatrics, 14(1), 134. https://doi.org/10.1186/1471-2431-14-134"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "for  person-independent,  continuous  estimation  of  pain"
        },
        {
          "References": "Engineering Applications of Neural Networks,"
        },
        {
          "References": ""
        },
        {
          "References": "International Symposium on Visual Computing,"
        },
        {
          "References": ""
        },
        {
          "References": "photoplethysmography  for  pain  recognition.  2017  International  Conference  on  Companion  Technology"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "IEEE International Conference on Multimedia and Expo (ICME),"
        },
        {
          "References": ""
        },
        {
          "References": "detecting pain using facial actions. 2009 3rd International Conference on Affective Computing and Intelligent"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "Automatic pain monitoring using the UNBC-McMaster shoulder pain expression archive database. Image"
        },
        {
          "References": "and Vision Computing, 30(3), 197-205. https://doi.org/10.1016/j.imavis.2011.12.003"
        },
        {
          "References": ""
        },
        {
          "References": "shoulder pain expression archive database. Face and Gesture 2011,"
        },
        {
          "References": ""
        },
        {
          "References": "Detection of Painful Reaching for Chronic Pain Rehabilitation Systems."
        },
        {
          "References": ""
        },
        {
          "References": "kinematics and muscle activity for physical rehabilitation in chronic pain."
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Physical Pain Detection. https://doi.org/10.21437/SpeechProsody.2016-86"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Pedersen, H. (2015). Learning Appearance Features for Pain Detection Using the UNBC-McMaster Shoulder Pain"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Publishing. https://doi.org/10.1007/978-3-319-20904-3_12"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Ren, Z., Cummins, N., Han, J., Schnieder, S., Krajewski, J., & Schuller, B. (2018, 10-12 Oct. 2018). Evaluation of the"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "ITG-Symposium,"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Rivas, J. J., del Carmen Lara, M., Castrejon, L., Hernandez-Franco, J., Orihuela-Espina, F., Palafox, L., Williams, A.,"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "virtual rehabilitation. IEEE transactions on affective computing."
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Rodriguez, P., Cucurull, G., Gonzàlez, J., Gonfaus, J. M., Nasrollahi, K., Moeslund, T. B., & Roca, F. X. (2017). Deep"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Pain:  Exploiting  Long  Short-Term  Memory  Networks"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Transactions on Cybernetics, 1-11. https://doi.org/10.1109/TCYB.2017.2662199"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Roy, S., Roy, C., Éthier-Majcher, C., Fortin, I., Belin, P., & Gosselin, F. (2007). STOIC: A database of dynamic and"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "static faces expressing highly recognizable emotions. J. Vis, 7, 944."
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Saha, S., Datta, S., Konar, A., Banerjee, B., & Nagar, A. K. (2016, 2016). A novel gesture recognition system based"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "on fuzzy logic for healthcare applications."
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Saha, S., Pal, M., Konar, A., & Bhattacharya, D. (2015). Automatic gesture recognition for health care using relieff"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Salekin, M. S., Zamzmi, G., Goldgof, D., Kasturi, R., Ho, T., & Sun, Y. (2021). Multimodal spatio-temporal deep"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "104150."
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Thiam, P., & Schwenker, F. (2017, 28 Nov.-1 Dec. 2017). Multi-modal data fusion for pain intensity assessment and"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "(IPTA),"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Tsai, F.-S., Weng, Y.-M., Ng, C.-J., & Lee, C.-C. (2017, 2017). Embedding stacked bottleneck vocal features in a"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "LSTM architecture for automatic pain level classification during emergency triage."
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Tuduce, R. I., Cucu, H., & Burileanu, C. (2018, 4-6 July 2018). Why is My Baby Crying? An In-Depth Analysis of"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "International Conference on Telecommunications and Signal Processing (TSP),"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Velana, M., Gruss, S., Layher, G., Thiam, P., Zhang, Y., Schork, D., Kessler, V., Meudt, S., Neumann, H., Kim, J.,"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "System. \nIn  Lecture  Notes \nin  Computer  Science"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "https://doi.org/10.1007/978-3-319-59259-6_11"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Walter, S., Gruss, S., Ehleiter, H., Tan, J., Traue, H. C., Werner, P., Al-Hamadi, A., Crawcour, S., Andrade, A. O., &"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Werner, P., Al-Hamadi, A., Gruss, S., & Walter, S. (2019). Twofold-Multimodal Pain Recognition with the X-ITE"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Workshops and Demos (ACIIW), 290-296."
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Xin,  X.,  Lin,  X.,  Yang,  S.,  &  Zheng,  X.  (2020).  Pain  intensity  estimation  based  on  a  spatial  transformation  and"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "attention CNN. PloS one, 15(8), e0232412. https://doi.org/10.1371/journal.pone.0232412"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Zhang,  X.,  Yin,  L.,  Cohn,  J.  F.,  Canavan,  S.,  Reale,  M.,  Horowitz,  A.,  Liu,  P.,  &  Girard,  J.  M.  (2014).  Bp4d-"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Computing, 32(10), 692-706."
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Zhao,  R.,  Gan,  Q.,  Wang,  S.,  &  Ji,  Q.  (2016).  Facial  expression  intensity  estimation  using  ordinal  information."
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Zhou, J., Hong, X., Su, F., & Zhao, G. (2016). Recurrent convolutional neural network regression for continuous pain"
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": ""
        },
        {
          "Oshrat, Y., Bloch, A., Lerner, A., Cohen, A., Avigal, M., & Zeilig, G. (2016). Speech Prosody as a Biosignal for": "Recognition Workshops,"
        }
      ],
      "page": 30
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Machine Learning for Face",
      "authors": [
        "G Anbarjafari",
        "P Rasti",
        "F Noroozi",
        "J Gorbova",
        "R Haamer"
      ],
      "year": "2018",
      "venue": "Machine Learning for Face"
    },
    {
      "citation_id": "2",
      "title": "Development of artificial empathy",
      "authors": [
        "M Asada"
      ],
      "year": "2015",
      "venue": "Neuroscience Research"
    },
    {
      "citation_id": "3",
      "title": "Towards Artificial Empathy",
      "authors": [
        "M Asada"
      ],
      "year": "2015",
      "venue": "International Journal of Social Robotics",
      "doi": "10.1007/s12369-014-0253-z"
    },
    {
      "citation_id": "4",
      "title": "Why Not Artificial Sympathy? Social Robotics",
      "authors": [
        "M Asada",
        "Y Nagai",
        "H Ishihara"
      ],
      "year": "2012",
      "venue": "Why Not Artificial Sympathy? Social Robotics"
    },
    {
      "citation_id": "5",
      "title": "The painful face: Pain expression recognition using active appearance models",
      "authors": [
        "A Ashraf",
        "S Lucey",
        "J Cohn",
        "T Chen",
        "Z Ambadar",
        "K Prkachin",
        "P Solomon",
        "B Theobald"
      ],
      "year": "2007",
      "venue": "The painful face: Pain expression recognition using active appearance models"
    },
    {
      "citation_id": "6",
      "title": "An information-maximization approach to blind separation and blind deconvolution",
      "authors": [
        "A Bell",
        "T Sejnowski"
      ],
      "year": "1995",
      "venue": "Neural computation"
    },
    {
      "citation_id": "7",
      "title": "Visual Activation and Audiovisual Interactions in the Auditory Cortex during Speech Perception: Intracranial Recordings in Humans",
      "authors": [
        "J Besle",
        "C Fischer",
        "A Bidet-Caulet",
        "F Lecaignard",
        "O Bertrand",
        "M.-H Giard"
      ],
      "year": "2008",
      "venue": "The Journal of Neuroscience"
    },
    {
      "citation_id": "8",
      "title": "Pain assessment with short message service and interactive voice response in outpatients with cancer and pain: a feasibility study",
      "authors": [
        "K Besse",
        "N Faber-Te Boveldt",
        "G Janssen",
        "M Vernooij-Dassen",
        "K Vissers",
        "Y Engels"
      ],
      "year": "2016",
      "venue": "Pain Practice"
    },
    {
      "citation_id": "9",
      "title": "Direct Structural Connections between Voice-and Face-Recognition Areas",
      "authors": [
        "H Blank",
        "A Anwander",
        "K Von Kriegstein"
      ],
      "year": "2011",
      "venue": "Journal of Neuroscience",
      "doi": "10.1523/jneurosci.2091-11.2011"
    },
    {
      "citation_id": "10",
      "title": "How the Human Brain Exchanges Information Across Sensory Modalities to Recognize Other People",
      "authors": [
        "H Blank",
        "S Kiebel",
        "K Von Kriegstein"
      ],
      "year": "2015",
      "venue": "Human Brain Mapping",
      "doi": "10.1002/hbm.22631"
    },
    {
      "citation_id": "11",
      "title": "Emotion and sociable humanoid robots",
      "authors": [
        "C Breazeal"
      ],
      "year": "2003",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "12",
      "title": "Contextual information resolves uncertainty about ambiguous facial emotions: Behavioral and magnetoencephalographic correlates",
      "authors": [
        "F Bublatzky",
        "F Kavcıoğlu",
        "P Guerra",
        "S Doll",
        "M Junghöfer"
      ],
      "year": "2020",
      "venue": "Contextual information resolves uncertainty about ambiguous facial emotions: Behavioral and magnetoencephalographic correlates",
      "doi": "10.1016/j.neuroimage.2020.116814"
    },
    {
      "citation_id": "13",
      "title": "Empathy is hard work: People choose to avoid empathy because of its cognitive costs",
      "authors": [
        "C Cameron",
        "C Hutcherson",
        "A Ferguson",
        "J Scheffer",
        "E Hadjiandreou",
        "M Inzlicht"
      ],
      "year": "2019",
      "venue": "Journal of Experimental Psychology: General",
      "doi": "10.1037/xge0000595"
    },
    {
      "citation_id": "14",
      "title": "Integrating face and voice in person perception",
      "authors": [
        "S Campanella",
        "P Belin"
      ],
      "year": "2007",
      "venue": "Trends in Cognitive Sciences",
      "doi": "10.1016/j.tics.2007.10.001"
    },
    {
      "citation_id": "15",
      "title": "Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "T Simon",
        "S.-E Wei",
        "Y Sheikh"
      ],
      "year": "2017",
      "venue": "Realtime multi-person 2d pose estimation using part affinity fields"
    },
    {
      "citation_id": "16",
      "title": "Can We Share a Pain We Never Felt? Neural Correlates of Empathy in Patients with Congenital Insensitivity to Pain",
      "authors": [
        "N Danziger",
        "I Faillenot",
        "R Peyron"
      ],
      "year": "2009",
      "venue": "Neuron",
      "doi": "10.1016/j.neuron.2008.11.023"
    },
    {
      "citation_id": "17",
      "title": "Hierarchical Brain Network for Face and Voice Integration of Emotion Expression",
      "authors": [
        "J Davies-Thompson",
        "G Elli",
        "M Rezk",
        "S Benetti",
        "M Van Ackeren",
        "O Collignon"
      ],
      "year": "2019",
      "venue": "Cerebral Cortex",
      "doi": "10.1093/cercor/bhy240"
    },
    {
      "citation_id": "18",
      "title": "Putting the Altruism Back into Altruism: The Evolution of Empathy",
      "authors": [
        "F De Waal"
      ],
      "year": "2008",
      "venue": "Annual Review of Psychology",
      "doi": "10.1146/annurev.psych.59.103006.093625"
    },
    {
      "citation_id": "19",
      "title": "Facial action coding system: A technique for the measurement of facial movement",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system: A technique for the measurement of facial movement"
    },
    {
      "citation_id": "20",
      "title": "On-line emotion recognition in a 3-D activation-valence-time continuum using acoustic and linguistic cues",
      "authors": [
        "F Eyben",
        "M Graves",
        "A Schuller",
        "B Douglas-Cowie",
        "E Cowie"
      ],
      "year": "2010",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "21",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller",
        "E Fehr",
        "U Fischbacher"
      ],
      "year": "2003",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Shared pain: From empathy to synaesthesia",
      "authors": [
        "B Fitzgibbon",
        "M Giummarra",
        "N Georgiou-Karistianis",
        "P Enticott",
        "J Bradshaw"
      ],
      "year": "2010",
      "venue": "Neuroscience & Biobehavioral Reviews",
      "doi": "10.1016/j.neubiorev.2009.10.007"
    },
    {
      "citation_id": "23",
      "title": "What Can Computational Models Learn From Human Selective Attention? A Review From an Audiovisual Unimodal and Crossmodal Perspective",
      "authors": [
        "D Fu",
        "C Weber",
        "G Yang",
        "M Kerzel",
        "W Nan",
        "P Barros",
        "H Wu",
        "X Liu",
        "S Wermter"
      ],
      "year": "2020",
      "venue": "Frontiers in Integrative Neuroscience"
    },
    {
      "citation_id": "24",
      "title": "Artificial intelligence: the future of cybertherapy?",
      "authors": [
        "A Gaggioli"
      ],
      "year": "2017",
      "venue": "Cyberpsychology, Behavior, and Social Networking"
    },
    {
      "citation_id": "25",
      "title": "Pain, sadness, aggression, and joy: An evolutionary approach to film emotions",
      "authors": [
        "T Grodal"
      ],
      "year": "2007",
      "venue": "Pain, sadness, aggression, and joy: An evolutionary approach to film emotions"
    },
    {
      "citation_id": "26",
      "title": "Multi-Modal Signals for Analyzing Pain Responses to Thermal and Electrical Stimuli",
      "authors": [
        "S Gruss",
        "M Geiger",
        "P Werner",
        "O Wilhelm",
        "H Traue",
        "A Al-Hamadi",
        "S Walter"
      ],
      "year": "2019",
      "venue": "Journal of visualized experiments"
    },
    {
      "citation_id": "27",
      "title": "Pain intensity recognition rates via biopotential feature patterns with support vector machines",
      "authors": [
        "S Gruss",
        "R Treister",
        "P Werner",
        "H Traue",
        "S Crawcour",
        "A Andrade",
        "S Walter"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "28",
      "title": "Spontaneous Pain Expression Recognition in Video Sequences. Visions of Computer Science -BCS International Academic Conference",
      "authors": [
        "Z Hammal",
        "M Kunz",
        "M Arguin",
        "F Gosselin"
      ],
      "year": "2008",
      "venue": "Spontaneous Pain Expression Recognition in Video Sequences. Visions of Computer Science -BCS International Academic Conference"
    },
    {
      "citation_id": "29",
      "title": "Masking and inpainting: A two-stage speech enhancement approach for low SNR and non-stationary noise",
      "authors": [
        "X Hao",
        "X Su",
        "S Wen",
        "Z Wang",
        "Y Pan",
        "F Bao",
        "W Chen"
      ],
      "year": "2020",
      "venue": "Masking and inpainting: A two-stage speech enhancement approach for low SNR and non-stationary noise"
    },
    {
      "citation_id": "30",
      "title": "Exploring Emotive Actuation and its role in human-robot interaction. 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
      "authors": [
        "J Harris",
        "E Sharlin"
      ],
      "year": "2010",
      "venue": "Exploring Emotive Actuation and its role in human-robot interaction. 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "31",
      "title": "Neuroscience-inspired artificial intelligence",
      "authors": [
        "D Hassabis",
        "D Kumaran",
        "C Summerfield",
        "M Botvinick"
      ],
      "year": "2017",
      "venue": "Neuron"
    },
    {
      "citation_id": "32",
      "title": "PROCESS: A versatile computational tool for observed variable mediation, moderation, and conditional process modeling",
      "authors": [
        "A Hayes"
      ],
      "year": "2012",
      "venue": "PROCESS: A versatile computational tool for observed variable mediation, moderation, and conditional process modeling"
    },
    {
      "citation_id": "33",
      "title": "Space or time adaptive signal processing by neural network models",
      "authors": [
        "J Herault",
        "C Jutten"
      ],
      "year": "1986",
      "venue": "Personality and social psychology review"
    },
    {
      "citation_id": "34",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "35",
      "title": "Receptive fields of single neurones in the cat's striate cortex",
      "authors": [
        "D Hubel",
        "T Wiesel"
      ],
      "year": "1959",
      "venue": "The Journal of Physiology",
      "doi": "10.1113/jphysiol.1959.sp006308"
    },
    {
      "citation_id": "36",
      "title": "Interactions With an Empathetic Agent: Regulating Emotions and Improving Engagement in Autism",
      "authors": [
        "J James",
        "C Watson",
        "B Macdonald"
      ],
      "year": "2018",
      "venue": "27th IEEE International Symposium on Robot and Human Interactive Communication Javed",
      "doi": "10.1109/MRA.2019.2904638"
    },
    {
      "citation_id": "37",
      "title": "Direct speech-to-speech translation with a sequence-to-sequence model",
      "authors": [
        "Y Jia",
        "R Weiss",
        "F Biadsy",
        "W Macherey",
        "M Johnson",
        "Z Chen",
        "Y Wu"
      ],
      "year": "2019",
      "venue": "Direct speech-to-speech translation with a sequence-to-sequence model",
      "arxiv": "arXiv:1904.06037"
    },
    {
      "citation_id": "38",
      "title": "Adaptive confidence learning for the personalization of pain intensity estimation systems",
      "authors": [
        "M Kächele",
        "M Amirian",
        "P Thiam",
        "P Werner",
        "S Walter",
        "G Palm",
        "F Schwenker"
      ],
      "year": "2017",
      "venue": "Evolving Systems"
    },
    {
      "citation_id": "39",
      "title": "Bio-visual fusion for personindependent recognition of pain intensity",
      "authors": [
        "M Kächele",
        "P Werner",
        "A Al-Hamadi",
        "G Palm",
        "S Walter",
        "F Schwenker"
      ],
      "year": "2012",
      "venue": "Continuous pain intensity estimation from facial expressions. International Symposium on Visual Computing"
    },
    {
      "citation_id": "40",
      "title": "Hide Your Pain: Social Threat Increases Pain Reports and Aggression, but Reduces Facial Pain Expression and Empathy",
      "authors": [
        "K Karos",
        "A Meulders",
        "L Goubert",
        "J Vlaeyen"
      ],
      "year": "2020",
      "venue": "The Journal of Pain",
      "doi": "10.1016/j.jpain.2019.06.014"
    },
    {
      "citation_id": "41",
      "title": "Pain detection through shape and appearance features",
      "authors": [
        "R Khan",
        "A Meyer",
        "H Konik",
        "S Bouakaz"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "42",
      "title": "Observers' impression of the person in pain influences their pain estimation and tendency to help",
      "authors": [
        "A Khatibi",
        "M Mazidi"
      ],
      "year": "2019",
      "venue": "European Journal of Pain",
      "doi": "10.1002/ejp.1361"
    },
    {
      "citation_id": "43",
      "title": "Neuromodulation for chronic pain",
      "authors": [
        "H Knotkova",
        "C Hamani",
        "E Sivanesan",
        "M Le Beuffe",
        "J Moon",
        "S Cohen",
        "M Huntoon"
      ],
      "year": "2021",
      "venue": "The Lancet"
    },
    {
      "citation_id": "44",
      "title": "Shifts in selective visual attention: towards the underlying neural circuitry",
      "authors": [
        "C Koch",
        "S Ullman"
      ],
      "year": "1987",
      "venue": "Matters of intelligence"
    },
    {
      "citation_id": "45",
      "title": "Somatic and vicarious pain are represented by dissociable multivariate brain patterns",
      "authors": [
        "A Krishnan",
        "C.-W Woo",
        "L Chang",
        "L Ruzic",
        "X Gu",
        "M López-Solà",
        "P Jackson",
        "J Pujol",
        "J Fan",
        "T Wager"
      ],
      "year": "2016",
      "venue": "Somatic and vicarious pain are represented by dissociable multivariate brain patterns",
      "doi": "10.7554/eLife.15166"
    },
    {
      "citation_id": "46",
      "title": "Pain and disgust: The facial signaling of two aversive bodily experiences",
      "authors": [
        "M Kunz",
        "J Peter",
        "S Huster",
        "S Lautenbacher"
      ],
      "year": "2013",
      "venue": "PloS one"
    },
    {
      "citation_id": "47",
      "title": "Social Modulation of Pain as Evidence for Empathy in Mice",
      "authors": [
        "D Langford",
        "S Crager",
        "Z Shehzad",
        "S Smith",
        "S Sotocinal",
        "J Levenstadt",
        "M Chanda",
        "D Levitin",
        "J Mogil"
      ],
      "year": "2006",
      "venue": "Science",
      "doi": "10.1126/science.1128322"
    },
    {
      "citation_id": "48",
      "title": "The influence of empathy in humanrobot relations",
      "authors": [
        "I Leite",
        "A Pereira",
        "S Mascarenhas",
        "C Martinho",
        "R Prada",
        "A Paiva"
      ],
      "year": "2012",
      "venue": "International journal of human-computer studies",
      "doi": "10.1016/j.ijhcs.2012.09.005"
    },
    {
      "citation_id": "49",
      "title": "Pains and pleasures of social life",
      "authors": [
        "M Lieberman",
        "N Eisenberger"
      ],
      "year": "2009",
      "venue": "Science"
    },
    {
      "citation_id": "50",
      "title": "Model-free decision making is prioritized when learning to avoid harming others",
      "authors": [
        "P Lockwood",
        "M Klein-Flügge",
        "A Abdurahman",
        "M Crockett"
      ],
      "year": "2020",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "51",
      "title": "Single-Network Whole-Body Pose Estimation",
      "authors": [
        "G Martinez",
        "Y Raaj",
        "H Idrees",
        "D Xiang",
        "H Joo",
        "T Simon",
        "Y Sheikh"
      ],
      "year": "2019",
      "venue": "Single-Network Whole-Body Pose Estimation"
    },
    {
      "citation_id": "52",
      "title": "A Sequential Approach for Pain Recognition Based on Facial Representations",
      "authors": [
        "A Mauricio",
        "F Cappabianco",
        "A Veloso",
        "G Cámara"
      ],
      "year": "2019",
      "venue": "Lecture Notes Computer Science",
      "doi": "10.1007/978-3-030-34995-0_27"
    },
    {
      "citation_id": "53",
      "title": "An advantage of bipedal humanoid robot on the empathy generation: A neuroimaging study",
      "authors": [
        "N Miura",
        "M Sugiura",
        "M Takahashi",
        "T Moridaira",
        "A Miyamoto",
        "Y Kuroki",
        "R Kawashima"
      ],
      "year": "2008",
      "venue": "RSJ International Conference on Intelligent Robots and Systems"
    },
    {
      "citation_id": "54",
      "title": "Transient cortical circuits match spontaneous and sensory-driven activity during development",
      "authors": [
        "Z Molnár",
        "H Luhmann",
        "P Kanold"
      ],
      "year": "2020",
      "venue": "Science"
    },
    {
      "citation_id": "55",
      "title": "Experimental hand pain delays recognition of the contralateral hand-Evidence that acute and chronic pain have opposite effects on information processing?",
      "authors": [
        "G Moseley",
        "D Sim",
        "M Henry",
        "T Souvlis"
      ],
      "year": "2005",
      "venue": "Cognitive Brain Research",
      "doi": "10.1016/j.cogbrainres.2005.05.008"
    },
    {
      "citation_id": "56",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "Survey on emotional body gesture recognition"
    },
    {
      "citation_id": "57",
      "title": "Vocal-based emotion recognition using random forests and decision tree",
      "authors": [
        "F Noroozi",
        "T Sapiński",
        "D Kamińska",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-017-9396-2"
    },
    {
      "citation_id": "58",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "A Oord",
        "S Dieleman",
        "H Zen",
        "K Simonyan",
        "O Vinyals",
        "A Graves",
        "N Kalchbrenner",
        "A Senior",
        "K Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Wavenet: A generative model for raw audio",
      "arxiv": "arXiv:1609.03499"
    },
    {
      "citation_id": "59",
      "title": "Speech Prosody as a Biosignal for Physical Pain Detection",
      "authors": [
        "Y Oshrat",
        "A Bloch",
        "A Lerner",
        "A Cohen",
        "M Avigal",
        "G Zeilig"
      ],
      "year": "2016",
      "venue": "Speech Prosody as a Biosignal for Physical Pain Detection",
      "doi": "10.21437/SpeechProsody.2016-86"
    },
    {
      "citation_id": "60",
      "title": "Empathy in virtual agents and robots: a survey",
      "authors": [
        "A Paiva",
        "I Leite",
        "H Boukricha",
        "I Wachsmuth"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "61",
      "title": "Human motion assessment in real time using recurrent self-organization",
      "authors": [
        "G Parisi",
        "S Magg",
        "S Wermter"
      ],
      "year": "2016",
      "venue": "Human motion assessment in real time using recurrent self-organization"
    },
    {
      "citation_id": "62",
      "title": "Suffer together, bond together: Brain-to-brain synchronization and mutual affective empathy when sharing painful experiences",
      "authors": [
        "W Peng",
        "W Lou",
        "X Huang",
        "Q Ye",
        "R Tong",
        "Y Cui"
      ],
      "year": "2021",
      "venue": "Neuroimage",
      "doi": "10.1016/j.neuroimage.2021.118249"
    },
    {
      "citation_id": "63",
      "title": "Individual variation in alpha neurofeedback training efficacy predicts pain modulation",
      "authors": [
        "W Peng",
        "Y Zhan",
        "Y Jiang",
        "W Nan",
        "R Kadosh",
        "F Wan"
      ],
      "year": "2020",
      "venue": "NeuroImage: Clinical",
      "doi": "10.1016/j.nicl.2020.102454"
    },
    {
      "citation_id": "64",
      "title": "On the Role of Personality and Empathy in Human-Human, Human-Agent, and Human-Robot Mimicry",
      "authors": [
        "G Perugia",
        "M Paetzel",
        "G Castellano"
      ],
      "year": "2020",
      "venue": "Social Robotics",
      "doi": "10.1007/978-3-030-62056-1_11"
    },
    {
      "citation_id": "65",
      "title": "Cognition in the Chronic Pain Experience: Preclinical Insights",
      "authors": [
        "C Phelps",
        "E Navratilova",
        "F Porreca"
      ],
      "year": "2021",
      "venue": "Trends in Cognitive Sciences",
      "doi": "10.1016/j.tics.2021.01.001"
    },
    {
      "citation_id": "66",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "67",
      "title": "The structure, reliability and validity of pain expression: Evidence from patients with shoulder pain",
      "authors": [
        "K Prkachin",
        "P Solomon"
      ],
      "year": "2008",
      "venue": "Pain",
      "doi": "10.1016/j.pain.2008.04.010"
    },
    {
      "citation_id": "68",
      "title": "Empathy scale adaptation for artificial agents: a review with a new subscale proposal. 2022 8th International Conference on Control, Decision and Information Technologies (CoDIT)",
      "authors": [
        "H Putta",
        "K Daher",
        "M Kamali",
        "O Khaled",
        "D Lalanne",
        "E Mugellini"
      ],
      "year": "2022",
      "venue": "Empathy scale adaptation for artificial agents: a review with a new subscale proposal. 2022 8th International Conference on Control, Decision and Information Technologies (CoDIT)"
    },
    {
      "citation_id": "69",
      "title": "The mere physical presence of another person reduces human autonomic responses to aversive sounds",
      "authors": [
        "Y Herrmann",
        "M Bell",
        "L Fackler",
        "A Han",
        "S Deckert",
        "J Hein"
      ],
      "year": "1919",
      "venue": "Proceedings of the Royal Society B"
    },
    {
      "citation_id": "70",
      "title": "The revised International Association for the Study of Pain definition of pain: concepts, challenges, and compromises",
      "authors": [
        "S Raja",
        "D Carr",
        "M Cohen",
        "N Finnerup",
        "H Flor",
        "S Gibson",
        "F Keefe",
        "J Mogil",
        "M Ringkamp",
        "K Sluka",
        "X.-J Song",
        "B Stevens",
        "M Sullivan",
        "P Tutelman",
        "T Ushida",
        "K Vader"
      ],
      "year": "2020",
      "venue": "Pain",
      "doi": "10.1097/j.pain.0000000000001939"
    },
    {
      "citation_id": "71",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "72",
      "title": "Assessing the efficacy of a manual-based intervention for improving the detection of facial pain expression",
      "authors": [
        "J Rash",
        "K Prkachin",
        "P Solomon",
        "T Campbell"
      ],
      "year": "2019",
      "venue": "European Journal of Pain",
      "doi": "10.1002/ejp.1369"
    },
    {
      "citation_id": "73",
      "title": "THE MIRROR-NEURON SYSTEM",
      "authors": [
        "G Rizzolatti",
        "L Craighero"
      ],
      "year": "2004",
      "venue": "Annual Review of Neuroscience"
    },
    {
      "citation_id": "74",
      "title": "A Process Model of Empathy For Virtual Agents",
      "authors": [
        "S Rodrigues",
        "S Mascarenhas",
        "J Dias",
        "A Paiva"
      ],
      "year": "2015",
      "venue": "A Process Model of Empathy For Virtual Agents",
      "doi": "10.1093/iwc/iwu001"
    },
    {
      "citation_id": "75",
      "title": "Explainable Goal-Driven Agents and Robots --A Comprehensive Review",
      "authors": [
        "F Sado",
        "Chu",
        "Wei",
        "M Kerzel",
        "S Wermter"
      ],
      "year": "2021",
      "venue": "Explainable Goal-Driven Agents and Robots --A Comprehensive Review"
    },
    {
      "citation_id": "76",
      "title": "Head Movement Dynamics based Pain Detection using Spatio-Temporal Network",
      "authors": [
        "A Semwal",
        "N Londhe"
      ],
      "year": "2021",
      "venue": "2021 8th International Conference on Signal Processing and Integrated Networks (SPIN)"
    },
    {
      "citation_id": "77",
      "title": "Rewarded Outcomes Enhance Reactivation of Experience in the Hippocampus",
      "authors": [
        "A Singer",
        "L Frank"
      ],
      "year": "2009",
      "venue": "Neuron",
      "doi": "10.1016/j.neuron.2009.11.016"
    },
    {
      "citation_id": "78",
      "title": "The empathic brain: how, when and why?",
      "authors": [
        "T Singer"
      ],
      "year": "2006",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "79",
      "title": "Anterior cingulate inputs to nucleus accumbens control the social transfer of pain and analgesia",
      "authors": [
        "M Smith",
        "N Asada",
        "R Malenka"
      ],
      "year": "2021",
      "venue": "Science",
      "doi": "10.1126/science.abe3040"
    },
    {
      "citation_id": "80",
      "title": "An Introduction to Lifelong Supervised Learning",
      "authors": [
        "S Sodhani",
        "M Faramarzi",
        "S Mehta",
        "P Malviya",
        "M Abdelsalam",
        "J Janarthanan",
        "S Chandar"
      ],
      "year": "2022",
      "venue": "An Introduction to Lifelong Supervised Learning",
      "arxiv": "arXiv:2207.04354"
    },
    {
      "citation_id": "81",
      "title": "The role of empathy for artificial intelligence accountability",
      "authors": [
        "R Srinivasan",
        "San Miguel",
        "B González"
      ],
      "year": "2022",
      "venue": "Journal of Responsible Technology",
      "doi": "10.1016/j.jrt.2021.100021"
    },
    {
      "citation_id": "82",
      "title": "Emotion recognition from body movements and gestures",
      "authors": [
        "I.-O Stathopoulou",
        "G Tsihrintzis"
      ],
      "year": "2011",
      "venue": "Intelligent Interactive Multimedia Systems and Services"
    },
    {
      "citation_id": "83",
      "title": "Altruism born of suffering: The roots of caring and helping after victimization and other trauma",
      "authors": [
        "E Staub",
        "J Vollhardt"
      ],
      "year": "2008",
      "venue": "American Journal of Orthopsychiatry"
    },
    {
      "citation_id": "84",
      "title": "Multi-modal data fusion for pain intensity assessment and classification",
      "authors": [
        "P Thiam",
        "F Schwenker"
      ],
      "year": "2017",
      "venue": "Why is My Baby Crying? An In-Depth Analysis of Paralinguistic Features and Classical Machine Learning Algorithms for Baby Cry Classification. 2018 41st International Conference on Telecommunications and Signal Processing"
    },
    {
      "citation_id": "85",
      "title": "The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system",
      "authors": [
        "S Walter",
        "S Gruss",
        "H Ehleiter",
        "J Tan",
        "H Traue",
        "P Werner",
        "A Al-Hamadi",
        "S Crawcour",
        "A Andrade",
        "G Da Silva"
      ],
      "year": "2013",
      "venue": "The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system"
    },
    {
      "citation_id": "86",
      "title": "Dynamic interpersonal neural synchronization underlying pain-induced cooperation in females",
      "authors": [
        "C Wang",
        "T Zhang",
        "Z Shan",
        "J Liu",
        "D Yuan",
        "X Li"
      ],
      "year": "2019",
      "venue": "Human Brain Mapping"
    },
    {
      "citation_id": "87",
      "title": "Head movements and postures as pain behavior",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "K Limbrecht-Ecklundt",
        "S Walter",
        "H Traue"
      ],
      "year": "2018",
      "venue": "PloS one",
      "doi": "10.1371/journal.pone.0192767"
    },
    {
      "citation_id": "88",
      "title": "Facial expression of pain: an evolutionary account",
      "authors": [
        "Williams"
      ],
      "year": "2002",
      "venue": "Behavioral and Brain Sciences"
    },
    {
      "citation_id": "89",
      "title": "Explanation and prior knowledge interact to guide learning",
      "authors": [
        "J Williams",
        "T Lombrozo"
      ],
      "year": "2013",
      "venue": "Cognitive psychology"
    },
    {
      "citation_id": "90",
      "title": "Recurrent convolutional neural network regression for continuous pain intensity estimation in video",
      "authors": [
        "J Zhou",
        "X Hong",
        "F Su",
        "G Zhao"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "91",
      "title": "The automatic detection of chronic pain-related expression: requirements, challenges and the multimodal EmoPain dataset",
      "authors": [
        "M Aung",
        "S Romera-Paredes",
        "B Martinez",
        "B Singh",
        "A Cella",
        "M Valstar",
        "M Meng",
        "H Kemp",
        "A Shafizadeh"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "92",
      "title": "Multimodal Signal Analysis for Pain Recognition in Physiotherapy Using Wavelet Scattering Transform",
      "authors": [
        "A Badura",
        "A Masłowska",
        "A Myśliwiec",
        "E Piętka"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "93",
      "title": "Ensemble neural network approach detecting pain intensity from facial expressions",
      "authors": [
        "G Bargshady",
        "X Zhou",
        "R Deo",
        "J Soar",
        "F Whittaker",
        "H Wang"
      ],
      "year": "2020",
      "venue": "Artificial Intelligence in Medicine",
      "doi": "10.1016/j.artmed.2020.101954"
    },
    {
      "citation_id": "94",
      "title": "Machine recognition and representation of neonatal facial displays of acute pain",
      "authors": [
        "S Brahnam",
        "C Chuang",
        "F Shih",
        "M Slack"
      ],
      "year": "2006",
      "venue": "Artificial Intelligence in Medicine",
      "doi": "10.1016/j.artmed.2004.12.003"
    },
    {
      "citation_id": "95",
      "title": "Neonatal pain detection in videos using the iCOPEvid dataset and an ensemble of descriptors extracted from Gaussian of Local Descriptors",
      "authors": [
        "S Brahnam",
        "L Nanni",
        "S Mcmurtrey",
        "A Lumini",
        "R Brattin",
        "M Slack",
        "T Barrier"
      ],
      "year": "2020",
      "venue": "Neonatal pain detection in videos using the iCOPEvid dataset and an ensemble of descriptors extracted from Gaussian of Local Descriptors"
    },
    {
      "citation_id": "96",
      "title": "Learning pain from emotion: transferred hot data representation for pain intensity estimation",
      "authors": [
        "J Chen",
        "X Liu",
        "P Tu",
        "A Aragones",
        "C Florea",
        "L Florea",
        "C Vertan"
      ],
      "year": "2012",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "97",
      "title": "Deep multimodal pain recognition: a database and comparison of spatio-temporal visual modalities",
      "authors": [
        "S Gruss",
        "M Geiger",
        "P Werner",
        "O Wilhelm",
        "H Traue",
        "A Al-Hamadi",
        "S Walter",
        "M Haque",
        "R Bautista",
        "F Noroozi",
        "K Kulkarni",
        "C Laursen",
        "R Irani",
        "M Bellantonio",
        "S Escalera",
        "G Anbarjafari",
        "K Nasrollahi"
      ],
      "year": "2018",
      "venue": "Journal of visualized experiments : JoVE"
    },
    {
      "citation_id": "98",
      "title": "Too many crying babies: a systematic review of pain management practices during immunizations on YouTube",
      "authors": [
        "D Harrison",
        "M Sampson",
        "J Reszel",
        "K Abdulla",
        "N Barrowman",
        "J Cumber",
        "A Fuller",
        "C Li",
        "S Nicholls",
        "C Pound"
      ],
      "year": "2014",
      "venue": "BMC Pediatrics",
      "doi": "10.1186/1471-2431-14-134"
    },
    {
      "citation_id": "99",
      "title": "Patient state recognition system for healthcare using speech and facial expressions",
      "authors": [
        "M Hossain"
      ],
      "year": "2016",
      "venue": "Journal of medical systems"
    },
    {
      "citation_id": "100",
      "title": "Multimodal data fusion for person-independent, continuous estimation of pain intensity",
      "authors": [
        "M Kächele",
        "P Thiam",
        "M Amirian",
        "P Werner",
        "S Walter",
        "F Schwenker",
        "G Palm",
        "S Kaltwang",
        "O Rudovic",
        "M Pantic",
        "V Kessler",
        "P Thiam",
        "M Amirian",
        "F Schwenker",
        "R Icct), Khan",
        "A Meyer",
        "H Konik",
        "S Bouakaz"
      ],
      "year": "2012",
      "venue": "International Conference on Engineering Applications of Neural Networks"
    },
    {
      "citation_id": "101",
      "title": "Painful monitoring: Automatic pain monitoring using the UNBC-McMaster shoulder pain expression archive database",
      "authors": [
        "P Lucey",
        "J Cohn",
        "S Lucey",
        "I Matthews",
        "S Sridharan",
        "K Prkachin",
        "Workshops",
        "P Lucey",
        "J Cohn",
        "K Prkachin",
        "P Solomon",
        "S Chew",
        "I Matthews"
      ],
      "year": "2009",
      "venue": "3rd International Conference on Affective Computing and Intelligent Interaction and",
      "doi": "10.1016/j.imavis.2011.12.003"
    },
    {
      "citation_id": "102",
      "title": "Bi-Modal Detection of Painful Reaching for Chronic Pain Rehabilitation Systems",
      "authors": [
        "P Lucey",
        "J Cohn",
        "K Prkachin",
        "P Solomon",
        "I Matthews"
      ],
      "year": "2011",
      "venue": "Face and Gesture"
    },
    {
      "citation_id": "103",
      "title": "Pain level recognition using kinematics and muscle activity for physical rehabilitation in chronic pain",
      "authors": [
        "T Olugbade",
        "N Bianchi-Berthouze",
        "N Marquardt",
        "A Williams"
      ],
      "year": "2015",
      "venue": "Pain level recognition using kinematics and muscle activity for physical rehabilitation in chronic pain"
    },
    {
      "citation_id": "104",
      "title": "Speech Prosody as a Biosignal for Physical Pain Detection",
      "authors": [
        "Bloch Oshrat",
        "A Lerner",
        "A Cohen",
        "A Avigal",
        "M Zeilig"
      ],
      "year": "2016",
      "venue": "Speech Prosody as a Biosignal for Physical Pain Detection",
      "doi": "10.21437/SpeechProsody.2016-86"
    },
    {
      "citation_id": "105",
      "title": "Learning Appearance Features for Pain Detection Using the UNBC-McMaster Shoulder Pain Expression Archive Database",
      "authors": [
        "H Pedersen"
      ],
      "year": "2015",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-319-20904-3_12"
    },
    {
      "citation_id": "106",
      "title": "Evaluation of the Pain Level from Speech: Introducing a Novel Pain Database and Benchmarks",
      "authors": [
        "Z Ren",
        "N Cummins",
        "J Han",
        "S Schnieder",
        "J Krajewski",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Evaluation of the Pain Level from Speech: Introducing a Novel Pain Database and Benchmarks"
    },
    {
      "citation_id": "107",
      "title": "Multi-label and multimodal classifier for affective states recognition in virtual rehabilitation",
      "authors": [
        "J Rivas",
        "M Del Carmen Lara",
        "L Castrejon",
        "J Hernandez-Franco",
        "F Orihuela-Espina",
        "L Palafox",
        "A Williams",
        "N Berthouze",
        "E Sucar"
      ],
      "year": "2021",
      "venue": "Multi-label and multimodal classifier for affective states recognition in virtual rehabilitation"
    },
    {
      "citation_id": "108",
      "title": "Deep Pain: Exploiting Long Short-Term Memory Networks for Facial Expression Classification",
      "authors": [
        "P Rodriguez",
        "G Cucurull",
        "J Gonzàlez",
        "J Gonfaus",
        "K Nasrollahi",
        "T Moeslund",
        "F Roca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cybernetics",
      "doi": "10.1109/TCYB.2017.2662199"
    },
    {
      "citation_id": "109",
      "title": "STOIC: A database of dynamic and static faces expressing highly recognizable emotions",
      "authors": [
        "S Roy",
        "C Roy",
        "C Éthier-Majcher",
        "I Fortin",
        "P Belin",
        "F Gosselin"
      ],
      "year": "2007",
      "venue": "J. Vis"
    },
    {
      "citation_id": "110",
      "title": "A novel gesture recognition system based on fuzzy logic for healthcare applications",
      "authors": [
        "S Saha",
        "S Datta",
        "A Konar",
        "B Banerjee",
        "A Nagar"
      ],
      "year": "2016",
      "venue": "A novel gesture recognition system based on fuzzy logic for healthcare applications"
    },
    {
      "citation_id": "111",
      "title": "Automatic gesture recognition for health care using relieff and fuzzy kNN",
      "authors": [
        "S Saha",
        "M Pal",
        "A Konar",
        "D Bhattacharya"
      ],
      "year": "2015",
      "venue": "Information Systems Design and Intelligent Applications"
    },
    {
      "citation_id": "112",
      "title": "Multimodal spatio-temporal deep learning approach for neonatal postoperative pain assessment",
      "authors": [
        "M Salekin",
        "G Zamzmi",
        "D Goldgof",
        "R Kasturi",
        "T Ho",
        "Y Sun"
      ],
      "year": "2021",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "113",
      "title": "Embedding stacked bottleneck vocal features in a LSTM architecture for automatic pain level classification during emergency triage",
      "authors": [
        "P Thiam",
        "F Schwenker"
      ],
      "year": "2017",
      "venue": "Seventh International Conference on Image Processing Theory, Tools and Applications"
    },
    {
      "citation_id": "114",
      "title": "Why is My Baby Crying? An In-Depth Analysis of Paralinguistic Features and Classical Machine Learning Algorithms for Baby Cry Classification",
      "authors": [
        "R Tuduce",
        "H Cucu",
        "C Burileanu"
      ],
      "year": "2018",
      "venue": "41st International Conference on Telecommunications and Signal Processing"
    },
    {
      "citation_id": "115",
      "title": "The SenseEmotion Database: A Multimodal Database for the Development and Systematic Validation of an Automatic Pain-and Emotion-Recognition System",
      "authors": [
        "M Velana",
        "S Gruss",
        "G Layher",
        "P Thiam",
        "Y Zhang",
        "D Schork",
        "V Kessler",
        "S Meudt",
        "H Neumann",
        "J Kim",
        "F Schwenker",
        "E André",
        "H Traue",
        "S Walter"
      ],
      "year": "2017",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-319-59259-6_11"
    },
    {
      "citation_id": "116",
      "title": "The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system",
      "authors": [
        "S Walter",
        "S Gruss",
        "H Ehleiter",
        "J Tan",
        "H Traue",
        "P Werner",
        "A Al-Hamadi",
        "S Crawcour",
        "A Andrade",
        "G Da Silva"
      ],
      "year": "2013",
      "venue": "IEEE"
    },
    {
      "citation_id": "117",
      "title": "Twofold-Multimodal Pain Recognition with the X-ITE Pain Database",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "S Gruss",
        "S Walter"
      ],
      "year": "2019",
      "venue": "th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "118",
      "title": "Pain intensity estimation based on a spatial transformation and attention CNN",
      "authors": [
        "X Xin",
        "X Lin",
        "S Yang",
        "X Zheng"
      ],
      "year": "2020",
      "venue": "PloS one",
      "doi": "10.1371/journal.pone.0232412"
    },
    {
      "citation_id": "119",
      "title": "Bp4dspontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "120",
      "title": "Recurrent convolutional neural network regression for continuous pain intensity estimation in video",
      "authors": [
        "R Zhao",
        "Q Gan",
        "S Wang",
        "Q Ji",
        "J Zhou",
        "X Hong",
        "F Su",
        "G Zhao"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    }
  ]
}