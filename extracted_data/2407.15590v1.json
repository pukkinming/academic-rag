{
  "paper_id": "2407.15590v1",
  "title": "All Rivers Run Into The Sea: Unified Modality Brain-Like Emotional Central Mechanism",
  "published": "2024-07-22T12:26:31Z",
  "authors": [
    "Xinji Mai",
    "Junxiong Lin",
    "Haoran Wang",
    "Zeng Tao",
    "Yan Wang",
    "Shaoqi Yan",
    "Xuan Tong",
    "Jiawen Yu",
    "Boyang Wang",
    "Ziheng Zhou",
    "Qing Zhao",
    "Shuyong Gao",
    "Wenqiang Zhang"
  ],
  "keywords": [
    "Affective Computing",
    "Modality Missingness",
    "Cross-Modal Plasticity",
    "Dynamic Facial Expression Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the field of affective computing, fully leveraging information from a variety of sensory modalities is essential for the comprehensive understanding and processing of human emotions. Inspired by the process through which the human brain handles emotions",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The fusion of multimodal sensory information forms the cornerstone of human perception and cognition  [12] . However, the realms of multimodal fusion and Modality Missingness pose significant challenges within the field of affective computing. Modality Missingness refers to the unavailability of multimodal information in real-world affective computing tasks, where only certain specific modalities can be utilized. In affective computing, Modality Missingness poses significant challenges to methods based on text  [18, 32, 46] , audio  [6, 33] , video  [34] , and the like. Even for multimodal approaches  [1, 26] , performance can significantly deteriorate in environments where modalities are absent. The unpredictable nature of Modality Missingness can drastically diminish the accuracy and robustness of computational models designed to interpret human emotions. This issue becomes even more pronounced in dynamic and unstructured environments, where the availability of multimodal data may be inconsistent.\n\nRecent advancements in neuroscience have underscored the phenomenon of cross-modal plasticity  [4, 7, 9] , wherein the lack of input from one sensory modality leads to compensatory enhancements in the processing capabilities of others  [10, 13, 15] ,. For instance, in the case of individuals with vision loss, certain neurons in the primary visual cortex, which would typically process visual stimuli, can be recruited by other sensory modalities to enhance their processing capabilities  [16, 35, 38] . This observation aligns with the everyday experience of heightened auditory and tactile sensitivities in individuals who are blind. Such insights reflect the brain's remarkable ability to reorganize and optimize sensory processing under constrained conditions  [39] . In the field of emotional understanding, Albert Mehrabian proposed the 7%-38%-55% rule  [31] , which suggests that 7% of emotional information is conveyed through verbal expression, 38% through tone of voice, and 55% through facial expressions, highlighting the predominant role of visual information.\n\nDynamic Facial Expression Recognition (DFER) represents a pivotal downstream task within the realm of affective computing, placing a greater emphasis on the understanding of visual emotions. This is consistent with Albert's theory. Additionally, a distinct advantage within the DFER domain is that the datasets inherently provide raw visual information, as DFER necessitates original facial data for the recognition of emotions. In contrast, other visual-based affective computing tasks often process their data into features, significantly constraining us to test our methods in scenarios that more closely resemble real-world conditions. Based on this, we explore the avenues of Modality Missingness and multimodal fusion within the context of multimodal DFER tasks.\n\nInspired by these insights, as well as the use of multiple prompts in continuous learning tasks, we propose a Unified Modality Brainlike Emotional net for affective computing, named UMBEnet, marking a paradigm shift in the challenge of unified modal emotional understanding. UMBEnet's design draws inspiration from the brain's ability to reconfigure and augment its processing capabilities in response to sensory deprivation, integrating a Dual-Stream (DS) structure and a Sparse Feature Fusion (SFF) module. We have designed a Prompt Pool (First Stream) that employs trainable multiple prompts and a mechanism that simulates neural impulse transmission, capable of fully harnessing multimodal information, and blending it with inherent prompts (Second Stream) that store emotional information to form a Dual-Stream mechanism. Prompts are considered analogous to neurons in the brain, capable of storing a certain amount of information. The design of the Prompt Pool aims to emulate the brain's ability to select and interpret information in varying contexts, particularly in environments where modalities are absent. The design of inherent prompts seeks to emulate the activation of the amygdala, the emotional center of the brain, integrating multimodal information for judgment. SFF introduces a mechanism akin to actual neural impulse transmission. Considering that most features carry low-value information for transmission, sparse matrix fusion mimics the sparsity of neural impulse transmission in the brain  [2] , sparsely blending the prompts from the dual-stream mechanism. Inspired by the 7%-38%-55% rule, we designed an imbalanced multimodal encoder within UMBEnet. This encoder allocates a larger proportion of parameters to visual processing, reflecting the leading role of visual information in emotion recognition, while reducing the parameters for textual and auditory processing.\n\nUMBEnet introduces several innovations to affective computing, with three main contributions:\n\nâ€¢ We innovatively combined inherent prompts with a Prompt Pool to design and introduce a Dual-Stream (DS) structure. This dual-stream approach ensures a comprehensive understanding of human emotions by leveraging the strengths of different sensory modalities.\n\nâ€¢ Our framework includes a Sparse Feature Fusion (SFF) module, optimizing the use of available sensory data. By sparsely integrating modality fusion prompts with inherent prompts, this module allows for the efficient fusion of multimodal information, significantly enhancing the robustness and accuracy of emotion recognition across diverse scenarios.\n\nâ€¢ Drawing inspiration from the neural anatomical structure of the human brain, UMBEnet employs a Brain-like Emotional Processing Framework (BEPF). This biomimetic design closely mimics the human brain's emotional center, not only offering a more natural and effective way of understanding emotions but also improving system performance in emotion recognition tasks.\n\nThese contributions collectively enable UMBEnet to consistently outperform existing SOTA methods in DFER domain, particularly in scenarios involving multimodality and modal absence.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Work 2.1 Dynamic Facial Expression Recognition",
      "text": "In Dynamic Facial Expression Recognition (DFER), the trend has shifted from static to dynamic analysis, emphasizing temporal dynamics in expressions  [44] . This shift is propelled by deep learning advancements, with 3D Convolutional Neural Networks (C3D) capturing both spatial and temporal data dimensions  [42] , and Transformers like Visual Transformers (ViT) excelling in feature extraction and sequence processing  [11] . These innovations offer refined emotion recognition, addressing the complexities of video data and long-term dependencies in facial expressions. Furthermore, contrastive learning methods, exemplified by CLIP  [37]  and its enhancement, CLIPER  [24] , have forged synergies between visual and textual modalities, elevating recognition precision across diverse scenarios. Our approach diverges from the aforementioned methodologies by adopting a novel brain-inspired architecture, deviating from the conventional design philosophies of DFER methods. Our approach, particularly the prompt pool design and activation mechanism, innovatively addresses the challenge of missing modalities, merging inherent prompts within our SSF framework. This fusion not only aids in managing multimodal data but also enriches the interpretability of DFER systems.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Missingness",
      "text": "Affective computing strives to empower computers with the ability to recognize and understand human emotional states by integrating information from various sources such as voice, facial expressions, and physiological signals. The absence of one or more of these modalities, a situation known as modality missingness, complicates the task significantly  [3] . To tackle such challenges, recent studies have delved into multimodal DFER methods, focusing on innovative strategies like modality compensation and data fusion to handle the absence or corruption of critical sensory data  [47] . However, at present, the methods of modality missingness in the field of emotional computing are very limited, and most of them can not achieve good performance.\n\nOur approach introduces the challenges of Modality Missingness and multimodal fusion into the realm of DFER, aiming to fully leverage all available modal information in a manner that aligns with the decision-making structure of the human brain's emotional center.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Modal Plasticity And Brain Sciences",
      "text": "The brain's capability to process multimodal information offers critical theoretical insights for the field of affective computing. Anatomical discoveries indicate that the emotional center of the brain is located in the amygdala  [22] . Humans gather multimodal information through photoreceptors in the eyes, hair cells in the ears, etc., and process this information through primary visual and auditory centers, which are then integrated by the amygdala and analyzed by the cerebral cortex  [40] . The theory of cross-modal plasticity  [4]  also proves that the primary center can recruit neurons from other centers for analysis  [8, 28, 29] . These anatomical insights are crucial for designing algorithms capable of mimicking the brain's ability to process complex emotional information  [17] , especially in situations where data are missing or distorted.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "UMBEnet employs a brain-like emotional processing framework, consisting of two key components: DS and SFF. In the sections below, we detail the key design and functionality of each component.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Umbenet'S Brain-Like Structure",
      "text": "UMBEnet initially consists of a series of transformer-based encoders acting as feature extractors, processing different modal inputs separately. Let the original inputs for visual, textual, and audio modalities be ğ‘‹ ğ‘£ , ğ‘‹ ğ‘¡ , ğ‘‹ ğ‘ respectively. The corresponding modal encoders ğ¸ ğ‘£ , ğ¸ ğ‘¡ , ğ¸ ğ‘ transform these inputs into high-dimensional feature vectors, such that ğ¹ ğ‘£ , ğ¹ ğ‘¡ , ğ¹ ğ‘ âˆˆ R ğµÃ—ğ¹ . :\n\nwhere ğ¸ ğ‘šğ‘œğ‘‘ğ‘ğ‘™ represent the encoder functions for visual, textual, and audio inputs, respectively. Next, these feature vectors are fed into an adaptive self-attention module for information fusion. In the adaptive self-attention module, features from each modality are weighted and combined to generate a comprehensive multimodal embedding ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› :\n\nIn the adaptive self-attention mechanism AdaptiveFusion, we utilize the Query (Q), Key (K), and Value (V) mechanisms of the Transformer for information processing and the module requires an additional modal mask matrix:\n\nwhere ğ‘„, ğ¾, and ğ‘‰ represent the Query, Key, and Value matrices, respectively, and ğ‘‘ ğ‘˜ is the dimensionality factor for appropriate scaling. ğ‘€ represents the modal mask matrix that is added to the scaled dot products of the queries and keys. Through self-attention mechanism and the modal mask matrix, our model can generate specialized attention weights for each modal feature, thereby optimizing the fusion process and ensuring that meaningful final outputs ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› can still be produced even when some modal information is missing. After fusion by the adaptive self-attention module, the synthesized embedding ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› is sent into a predefined Prompt Pool with the purpose of finding a set of prompts that best match the current situation. Let the Prompt Pool be ğ‘ƒ = {ğ‘ 1 , ğ‘ 2 , ..., ğ‘ ğ‘› }, the selection of the most matching set of prompts can be represented as:\n\nIn the Prompt Pool, the process involves searching for the embedding most similar to the current multimodal input information, and then using that embedding as the Key to find the corresponding prompt as the Value. This process ensures that the selected prompt can be decoupled from the current multimodal input information and can also learn information missing from the modalities.\n\nFinally, the prompts selected from the Prompt Pool are concatenated with original learnable prompts to form the final prompt representation ğ‘ƒ ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ . Thereafter, ğ‘ƒ ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ together with ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› are used to compute the contrastive loss to identify the closest emotion category:\n\nWhere ğ‘— traverses all possible category prompts. By minimizing the contrastive loss L ğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘ ğ‘¡ , UMBEnet learns to accurately map multimodal inputs to their corresponding emotional categories.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dual-Stream Structure (Ds)",
      "text": "The Prompt Pool ğ‘ƒ contains a series of predefined textual prompts, each aimed at representing a specific emotional state or scenario. We define the Prompt Pool as ğ‘ƒ = {ğ‘ 1 , ğ‘ 2 , ..., ğ‘ ğ‘› }, where ğ‘› is the size of the Prompt Pool, and each ğ‘ ğ‘— âˆˆ ğ‘ƒ represents a specific emotional scenario or state. Each ğ‘ ğ‘— can be further represented as a set of Key-Value pairs, i.e., (ğ‘˜ ğ‘— , ğ‘£ ğ‘— ), where ğ‘˜ ğ‘— and ğ‘£ ğ‘— respectively represent the key and value. In our framework, the value ğ‘£ ğ‘— corresponds to a piece of prompt, while the key ğ‘˜ ğ‘— is used to associate the prompt with a specific state.\n\nIn Figure  3 , within the Prompt Pool, unimodal features correspond to unimodal prompts, while multimodal fused features correspond to multimodal prompts. This mechanism enables our model to fully capitalize on multimodal information, effectively addressing the challenges posed by missing modalities.\n\nThe value part ğ‘£ ğ‘— of each prompt ğ‘ ğ‘— is a sequence of tokens of length ğ¿ ğ‘ , embedded into the same embedding space ğ· as the multimodal features ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› , expressed as:\n\nHere, ğ‘£ ğ‘—ğ‘– represents the embedding vector of the ğ‘–-th token in ğ‘£ ğ‘— .\n\nIn UMBEnet, the function of the Prompt Pool is not merely to provide a fixed set of textual collections for the model to choose from. By adopting a learnable key-value pair structure, we allow the model to dynamically select the most matching prompt while dealing with a specific multimodal input. This matching process can be realized by calculating the similarity between the input features ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› and each key ğ‘˜ ğ‘— , then selecting the prompt with the highest similarity for subsequent processing.\n\nWhere cos denotes the cosine similarity function, ğ‘— * is the index of the prompt most matching with the fused feature ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› , and ğ‘ * is the value of the selected prompt. Figure  4  illustrates how key-value pairs simulate the mechanism of neural impulse transmission: Neurotransmitters are released by one neuron, bind to receptors, and transmit neural impulses, where the content of transmission is not the neurotransmitter itself but transforms into electrical signals. Inspired by this, both our key and value are set to be trainable. The key is trained to align with the query's latent space, while the value is trained to learn modal information. Similarity between query and key is computed through different mapping functions, and the corresponding value is outputted.\n\nThrough this mechanism, the key-value pairs in the Prompt Pool make the connection coupled, not only enhancing the interpretability of the model but also improving its adaptability to different states. This design allows UMBEnet to respond more flexibly and accurately when facing complex multimodal emotion recognition tasks.\n\nThe design of the Prompt Pool (ğ‘ƒ) and the concatenation with inherent prompts aim to mimic the brain's strategy of activating different neurons for varying tasks, particularly under Modality Missingness. For a set of prompts, the process of selecting and The operation of the activation mechanism modeled after neural impulse transmission is depicted on the left. In this process, neurotransmitters, once received by receptors during transmission, convert not into the neurotransmitters themselves but into electrical signals, analogous to a specialized key-value pair system where receptors and electrical signals correlate. The top right corner illustrates the query function, representing selectable mapping functions within this framework. This neural-inspired approach provides a biomimetic method for prompt activation, reflecting the intricacy and efficiency of neural communication in UM-BEnet's architecture.\n\nconcatenating prompts can be formalized as:\n\nwhere ğ‘˜ represents the number of prompt to be selected, and ğ‘‡ğ‘œğ‘ ğ‘˜ denotes select the largest k indexes from the sequence of the cosine similarity between the fused features ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› and ğ‘˜ğ‘’ğ‘¦. This selection process is crucial for adaptively responding to different emotional contexts.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sparse Feature Fusion (Ssf)",
      "text": "During the modal fusion stage, we employ the SSF mechanism to further process and synthesize the prompts selected from the Prompt Pool. Suppose the set of ğ‘¡ğ‘œğ‘˜ ğ‘ most matching prompts selected from the Prompt Pool forms ğ‘ƒ * = {ğ‘ * 1 , ğ‘ * 2 , . . . , ğ‘ * ğ‘¡ğ‘œğ‘ ğ‘˜ }, where each ğ‘ * ğ‘– is a prompt value corresponding to a specific emotional state selected through the aforementioned process.\n\nNext, these selected prompts are sent into a sparse feature fusion process to capture their interactions and their relations with the original multimodal information:\n\nWhere ğ‘ƒ â€² represents the set of prompts processed by the SSF mechanism.\n\nThe SSF is implemented through a series of linear layers and ReLU activations, finalized with L1 regularization for sparsity, followed by another linear layer and a sigmoid function to compute a sparse matrix. Specifically, for a given input ğ‘‹ , the sparse feature fusion can be expressed as:\n\nand ğ‘Š 1 , ğ‘Š 2 are weight matrices, ğ‘ 1 , ğ‘ 2 are bias terms, and ğœ is the sigmoid function ensuring the output is in the (0, 1) range, simulating the sparsity in neural activations.\n\nDuring the sparse feature fusion process, L1 regularization is applied to the weight matrix ğ‘Š 1 to promote sparsity. The new cost function incorporating L1 regularization for the weight matrix ğ‘Š 1 is given by:\n\nwhere ğ¿(ğ‘Š 1 ) is the cost function after regularization, ğ¿(ğ‘œğ‘Ÿğ‘–) is original loss, ğœ† is the regularization parameter, and ğ‘Š 1,ğ‘– represents the elements of the weight matrix ğ‘Š 1 . The regularization term encourages the sparsity in ğ‘Š 1 by penalizing the absolute values of the weights. Then, the obtained set of prompts ğ‘ƒ â€² is concatenated with a set of inherent learnable prompts ğ‘ƒ ğ‘–ğ‘›â„ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘¡ , forming a comprehensive set of prompts ğ‘ƒ ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’ğ‘‘ = ğ‘ƒ â€² âŠ• ğ‘ƒ ğ‘–ğ‘›â„ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘¡ , where âŠ• denotes the concatenation operation. This process aims to combine dynamically selected prompts with fixed, task-related inherent prompts to enhance the model's expressiveness and adaptability.\n\nFinally, we use the contrastive loss to optimize the model, ensuring the selected set of prompts matches correctly with the multimodal input information. The model outputs the final emotional classification results by evaluating the match between ğ¹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› and each category-corresponding comprehensive set of prompts ğ‘ƒ ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’ğ‘‘ :\n\nIn this way, UMBEnet can use multimodal information and rich semantic prompts for accurate emotion recognition.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Audio Video Text",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "1-State 2-State",
      "text": "Figure  5 : The training strategy of UMBEnet unfolds in 2stages: First, prompts are trained with unimodal inputs; Second, prompts activated in the first stage are aggregated and retrained to enhance integration and responsiveness.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training Strategy",
      "text": "As illustrated in Figure  5 , UMBEnet's comprehensive training strategy begins by independently training prompts within the Prompt Pool using unimodal information. Initially, the system inputs audio (A), visual (V), and textual (T) modalities separately to tailor prompts specific to each modality. This first phase of training ensures that the prompts are finely tuned to respond to the unique features of each input type.\n\nIn the subsequent phase, prompts that were activated in the first stage are transferred to a new Prompt Pool. Here, the system undergoes training with randomly missing modal information, simulating scenarios where certain modalities may be miss or incomplete. This two-tiered training approach allows the model to delve deeply into the nuances of modal information, effectively leveraging the partial knowledge obtained from each modality to compensate for any missing data. Through this strategy, UMBEnet enhances its capacity to interpret complex emotional prompts by becoming proficient at drawing inferences from incomplete or asymmetrically available data. The employment of this training regime not only promotes the robustness of the system in handling real-world scenarios where multimodal data might not always be complete but also aligns with the cognitive flexibility inherent in human emotional understanding, where inferences are often drawn from partial information.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments 4.1 Experimental Setup",
      "text": "All experiments in this study were conducted in a hardware environment with the following specifications: two NVIDIA GeForce RTX 3090 graphics cards and a computer equipped with an Intel(R) Xeon(R) CPU 5218R @ 2.10GHz. During the model training process, we chose the Adam optimizer as our optimization algorithm. The initial learning rate was set to 0.002, and we adopted a mini-batch training approach with a batch size of 16. Furthermore, to prevent overfitting and improve the model's generalization ability, we designed a dynamic learning rate adjustment mechanism based on the performance on the validation set. Specifically, if there is no significant decrease in loss on the validation set for three consecutive training epochs, the learning rate will be reduced to 0.6 times its original value. When the learning rate drops below 1 Ã— 10 -7 , we consider the model to have reached early convergence, at which point the training process will be terminated. In addition, to further control the phenomenon of overfitting, we will take corresponding measures to adjust when the accuracy of the model on the training set is more than 80%, we will terminate the training in advance. In the experimental results, bold represents the best, and underline represents the second best. The confusion matrices and feature visualizations in Figures  6  and 7  showcase the exceptional performance of UMBEnet on FERV39k, DFEW, and MAFW. For more confusion matrices, please see supplementary materials.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "When evaluating model performance, we adopted WAR and UAR as evaluation metrics. WAR (Weighted Accuracy Recall) refers to the weighted average of the prediction accuracy of the classification model on each class. This metric considers the sample size of each class, making it more suitable for imbalanced datasets. UAR",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparative Experiments",
      "text": "As shown in tables 1 and 2, to appraise the performance of our UMBEnet model in the task of emotion recognition, we carried out a suite of comparative tests on three of the largest unrestricted publicly available datasets within the DFER sphere: MAFW  [27] , FERV39K  [44] , and DFEW  [21] . Notably, our approach holds its ground even against methods such as MAE that have been pretrained on extensive data-a comparison often deemed unfair due to the considerable advantages conferred by extensive pre-training. Moreover, our method substantially outperforms the CLIP-based methods that utilize the same backbone as ours, underscoring the efficacy of UMBEnet in processing emotional data.\n\nAmong the methods that do not rely on large pre-training models, our method surpasses the SOTA AEN method by 4.56% (WAR) and 7.89% (UAR) in scenarios with missing modalities in unimodal settings, and extends the lead to 5.56% (WAR) in multimodal settings. Against SOTA CLIP-based methods with the same backbone, our method outperforms DFER-CLIP by 4.94% (WAR). Remarkably, even against the self-supervised MAE-DFER method, which has been pre-trained on extensive datasets, our method exceeds performance by 1.14% (UAR on DFEW), 0.89% (UAR on FERV39K), 0.03% (WAR on FERV39K), and 0.73% (WAR on MAFW) across the three datasets of DFEW, FERV39K, and MAFW in unimodal scenarios. It's important to note that comparisons with self-supervised methods are often considered unfair due to their training on vast amounts of data. On multimodal datasets, our method comprehensively surpasses existing methods, with UMBEnet exceeding MAE-DFER by 6.41% (UAR on MAFW) and 13.72% (WAR on MAFW). Tables 2 and 3 display the superior performance of our approach in the domain of multimodal DFER. DFEW and MAFW represent the two largest datasets in the field of DFER, with DFEW comprising audio and video modalities, where our method achieves SOTA results. The MAFW dataset includes three modalities: audio, video, and text. However, the text modality lacks neutral label annotations. Introducing filler noise text information leads to an artificially high accuracy in neutral classification, suggesting that the model may be learning from noise. Given the novelty of the dataset, previous DFER methods have not highlighted this issue, and the absence of confusion matrices or open-source code from those studies precludes a fair comparison. Therefore, we recalculated WAR and UAR for a 10-class scheme, excluding neutral accuracy, as shown in Table  3 .\n\nOur approach significantly outperforms the existing SOTA methods in a multimodal 11-class setting, and even with the neutral class removed in a 10-class configuration, our method still substantially surpasses the SOTA with a 7.08% increase in UAR and a 9.33% increase in WAR. Comparisons in the 10-class setting, with missing modalities, reveal that the most effective modality is visual, followed by textual, with audio being the least effective. It is evident that our method significantly outperforms the existing SOTA methods, both for the 11-class and the reduced 10-class configurations.\n\nIn our experiments with missing modalities in Table  3 , we discovered that inferring with a full-modal model in the absence of certain modalities can achieve, or even surpass, the performance of training unimodal models from scratch. This is attributed to the two-stage training strategy employed within the full-modal model, suggesting that information from different modalities can often be cross-utilized to enhance performance. This finding underscores the efficacy of our approach in leveraging cross-modal information, thereby boosting the robustness and adaptability of the model in handling missing modal scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "In the ablation study, we delve into the impact of different modalities on model performance and conduct ablations on the MAFW dataset. As shown in Table  3 , we ablated the effects of modalities and, thanks to the design of our unbalanced encoder, we achieved SOTA results even with missing modalities (for example, using only visual input). Moreover, as predicted, the audio modality contributes less to accuracy, which further validates the design of our unbalanced encoder. Additionally, in Table  4 , we tested various hyperparameter settings on the MAFW dataset, including TOPK, pool size, and prompt length, to analyze their impact on model performance. An overview of Table  4 reveals",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "Our work introduces UMBEnet, a novel unified modal model that departs from the paradigms of previous DFER methods, mirroring the complex neural architecture of the human brain in emotional understanding and effectively addressing the challenges of Modality Missingness and multimodal fusion. Extensive testing on leading DFER benchmarks-DFEW, FERV39k, and MAFW-has demonstrated the superior performance of UMBEnet, especially under various channel conditions or in their absence. We believe UMBEnet will be instructive to the entire multimodal community, and we will continue to explore the use of UMBEnet in other multimodal areas in future.\n\nâ€¢",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Research Motivation",
      "text": "Cross-modal plasticity refers to the brain's ability to adjust and reorganize its functions to enhance the processing capabilities of other sensory modalities following the loss or absence of input from a particular sensory modality. This phenomenon primarily involves neural plasticity, which is the structural and functional changes in neurons and neural networks under the influence of experience or the environment. These changes include the formation, strengthening, or weakening of synapses, and the establishment of new neural pathways. In cases of sensory deprivation, such as blindness or deafness, the brain regions originally receiving input from the now-absent modality no longer do so. Instead, neural centers of other modalities may take over these areas, reallocating and optimizing their functions. For example, in experiments involving visual deprivation and auditory enhancement, the brain areas originally processing visual information, such as the visual cortex, begin to process auditory or tactile information. This reorganization can lead to increased auditory and tactile sensitivity, as observed in the extraordinary auditory localization abilities of blind individuals. Similarly, in experiments involving auditory deprivation and visual/tactile enhancement, individuals with hearing loss exhibit enhanced visual and tactile functions, such as improved visualspatial processing abilities and heightened tactile perception. The mechanisms underlying cross-modal plasticity involve a variety of molecular and cellular events, such as changes in neurotrophic factors, synaptic connectivity reorganization, and the rebalancing of inhibitory and excitatory neurotransmissions. The brain utilizes these mechanisms to optimize the remaining sensory inputs to compensate for the lost senses. This plasticity helps individuals adapt to sensory loss and enhances the functions of other senses, revealing the brain's remarkable adaptability to changes in sensory inputs.\n\nNeuroanatomy is the scientific field that studies the structure and function of the brain, including how the brain processes information through its complex network structures. Specifically, in the context of multimodal information processing, neuroanatomy demonstrates how the brain integrates information from our various sensory systems, including vision, hearing, touch, smell, and taste. The parietal lobe, located in the upper part of the brain, is a primary area for processing tactile, visual, and spatial information. The temporal lobe primarily handles auditory information and some visual memory. It contains several key multimodal areas, such as the superior temporal sulcus (STS), which is an important area for integrating visual and auditory information related to facial expressions and body movements, as well as associated sounds. Additionally, the temporal lobe is involved in language comprehension and emotional processing. The frontal lobe, located at the front of the brain, is central to decision-making, planning, and social behavior. The dorsolateral prefrontal cortex (DLPFC) within the frontal lobe is key for multimodal information processing, responsible for merging information from different sensory sources to support complex cognitive tasks such as problem-solving and decision-making. The insula, deep within the brain, acts as an integration center for information from multiple sensory systems, playing a critical role in regulating emotional responses, pain perception, taste, smell, and visceral sensations. Its role in multimodal information processing includes integrating internal and external sensory signals and assessing emotionally relevant stimuli. The amygdala, a critical area in the brain for processing emotional responses, especially in managing emotions like fear and happiness, receives and integrates various sensory information, such as visual and auditory signals that relate to emotional responses and social prompts. Multimodal information processing relies not only on the functions of individual brain regions but also on extensive neural networks. These networks are interconnected through complex axonal connections. For example, visual-auditory information is integrated in the superior temporal sulcus and other areas of the temporal lobe, which have extensive connections with the parietal and frontal lobes, jointly participating in tasks such as spatial localization and speech understanding.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experimental",
      "text": "In the experimental section, regarding the performance experiments, the confusion matrix for Table  1  has already been presented in the main text. We have additionally included the confusion matrices for Tables  2, 3 , and 4 as Figures 1, 2, 3 and 4 in the supplementary materials. In these experiments, SENET consistently demonstrated performance as shown in the tables, and even exceeded the performance indicated therein.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Future Work",
      "text": "In the field of affective computing, leveraging information from various sensory channels is crucial for a nuanced understanding and interpretation of human emotions. Inspired by the theory of crosschannel plasticity, we propose a new unified channel paradigm for affective computing, named UMBEnet. UMBEnet seamlessly integrates multimodal information across visual, auditory, and textual domains to create a more proficient system that enhances the accuracy and resilience of emotion recognition efforts. Our approach is grounded in the complex neuroanatomical structures of the human brain, incorporating a brain-like emotional processing framework that utilizes inherent cues and dynamic cue pools, along with sparse feature fusion techniques. Rigorous experimental validation on the most scalable benchmarks in the DFER field-specifically, DFEW, FERV39k, and MAFW-has definitively confirmed that UMBEnet We believe that the BEPF framework, DS structure, and SFF module proposed in this paper offer significant guidance for addressing issues of fusion and modality absence within the multimodal domain. Our work provides the multimodal community with a novel and distinct model framework for multimodal fusion and addressing modality absence, differing from previous approaches. In the future, we aim to extend the BEPF framework to more multimodal domains, believing it to be a transferable multimodal framework that requires minimal modification for adaptation.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The research motivation of our designed UMBEnet. (1) and (2) illustrate the theory of cross-modal plasticity, wherein",
      "page": 1
    },
    {
      "caption": "Figure 2: Overall architecture of UMBEnet. Figure 2a shows a brain-like emotional processing framework (BEPF). The left",
      "page": 4
    },
    {
      "caption": "Figure 2: b illustrates the structure of",
      "page": 4
    },
    {
      "caption": "Figure 2: c presents the",
      "page": 4
    },
    {
      "caption": "Figure 3: , within the Prompt Pool, unimodal features corre-",
      "page": 4
    },
    {
      "caption": "Figure 3: Demonstration of the Prompt Poolâ€™s functionality",
      "page": 5
    },
    {
      "caption": "Figure 4: illustrates how key-value pairs simulate the mechanism",
      "page": 5
    },
    {
      "caption": "Figure 4: The operation of the activation mechanism modeled",
      "page": 5
    },
    {
      "caption": "Figure 5: The training strategy of UMBEnet unfolds in 2-",
      "page": 6
    },
    {
      "caption": "Figure 5: , UMBEnetâ€™s comprehensive training strat-",
      "page": 6
    },
    {
      "caption": "Figure 6: Features before and after processing by the model.",
      "page": 7
    },
    {
      "caption": "Figure 7: Partial confusion matrices of UMBEnet on",
      "page": 7
    },
    {
      "caption": "Figure 8: corroborate our",
      "page": 8
    },
    {
      "caption": "Figure 8: Visualization of the Prompt Pool under differ-",
      "page": 8
    },
    {
      "caption": "Figure 8: a.The Prompt Pool",
      "page": 8
    },
    {
      "caption": "Figure 1: Confusion Matrices of Overall Model Performance",
      "page": 12
    },
    {
      "caption": "Figure 2: Confusion Matrices of Multimode Performance",
      "page": 12
    },
    {
      "caption": "Figure 3: Confusion Matrices of Missing Mode (UMBEnet on",
      "page": 13
    },
    {
      "caption": "Figure 4: Confusion Matrices of UMBEnet Hyperparameter Ablation Study (UMBEnet on MAFW for 11-class and 10-class",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature\nPrompt": "(a) Multimodal prompt method\n(b) Brain physiological structure"
        },
        {
          "Feature\nPrompt": "(c) Biomimetic design of neurotransmitter"
        },
        {
          "Feature\nPrompt": "Figure 1: The research motivation of our designed UMBEnet. (1) and (2) illustrate the theory of cross-modal plasticity, wherein"
        },
        {
          "Feature\nPrompt": "the human brain can recruit neurons from the center of the absent modality to enhance the analytical capabilities of the"
        },
        {
          "Feature\nPrompt": "remaining modalities when a certain modality is missing. Inspired by this, we have designed a Dual-Stream (DS) structure as"
        },
        {
          "Feature\nPrompt": "shown in (3), where the Prompt Pool contains prompts of different modalities, which after extraction are fused with inherent"
        },
        {
          "Feature\nPrompt": "prompts. This mechanism simulates how neurons of different modalities are activated and eventually integrated for analysis in"
        },
        {
          "Feature\nPrompt": "the emotional center. (a) shows how in traditional multimodal methods, prompts correspond one-to-one with features without"
        },
        {
          "Feature\nPrompt": "any directional flow of information. (b) illustrates features first activate neurons corresponding to their modality, and neurons"
        },
        {
          "Feature\nPrompt": "can be activated by multiple modalities simultaneously, after which the activated neurons process the feature information in an"
        },
        {
          "Feature\nPrompt": "integrated manner in the human brain. Inspired by this, we introduce a biomimetic design in (c) that imitates neurotransmitter"
        },
        {
          "Feature\nPrompt": "activation, where multimodal features first activate the corresponding modalityâ€™s prompts through key-value pairs. These"
        },
        {
          "Feature\nPrompt": "prompts are then processed collectively before being used for comparative prediction. (c) in conjunction with the Prompt Pool"
        },
        {
          "Feature\nPrompt": "in (3), culminates in a structure analogous to that shown in (b). Notably, the actual utilization involves the â€™valueâ€™ rather than"
        },
        {
          "Feature\nPrompt": "the â€™keyâ€™, mirroring the process where neurons, once activated by neurotransmitters, propagate electrical signals."
        },
        {
          "Feature\nPrompt": "ABSTRACT"
        },
        {
          "Feature\nPrompt": "*Co-corresponding authors: Wenqiang Zhang (wqzhang@fudan.edu.cn), Yan Wang"
        },
        {
          "Feature\nPrompt": "(yanwang19@fudan.edu.cn)."
        },
        {
          "Feature\nPrompt": "In the field of affective computing, fully leveraging information"
        },
        {
          "Feature\nPrompt": "from a variety of sensory modalities is essential for the comprehen-"
        },
        {
          "Feature\nPrompt": "Permission to make digital or hard copies of all or part of this work for personal or"
        },
        {
          "Feature\nPrompt": "classroom use is granted without fee provided that copies are not made or distributed\nsive understanding and processing of human emotions. Inspired"
        },
        {
          "Feature\nPrompt": "for profit or commercial advantage and that copies bear this notice and the full citation"
        },
        {
          "Feature\nPrompt": "by the process through which the human brain handles emotions"
        },
        {
          "Feature\nPrompt": "on the first page. Copyrights for components of this work owned by others than the"
        },
        {
          "Feature\nPrompt": "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or"
        },
        {
          "Feature\nPrompt": "republish, to post on servers or to redistribute to lists, requires prior specific permission\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM."
        },
        {
          "Feature\nPrompt": "and/or a fee. Request permissions from permissions@acm.org.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM"
        },
        {
          "Feature\nPrompt": "https://doi.org/10.1145/nnnnnnn.nnnnnnn\nACM MM, 2024, Melbourne, Australia"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "sensory processing under constrained conditions [39]. In the field"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "of emotional understanding, Albert Mehrabian proposed the 7%-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "38%-55% rule[31], which suggests that 7% of emotional information"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "is conveyed through verbal expression, 38% through tone of voice,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "and 55% through facial expressions, highlighting the predominant"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "role of visual information."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Dynamic Facial Expression Recognition (DFER) represents a"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "pivotal downstream task within the realm of affective computing,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "placing a greater emphasis on the understanding of visual emotions."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "This is consistent with Albertâ€™s theory. Additionally, a distinct"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "advantage within the DFER domain is that the datasets inherently"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "provide raw visual information, as DFER necessitates original facial"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "data for the recognition of emotions. In contrast, other visual-based"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "affective computing tasks often process their data into features,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "significantly constraining us to test our methods in scenarios that"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "more closely resemble real-world conditions. Based on this, we"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "explore the avenues of Modality Missingness and multimodal fusion"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "within the context of multimodal DFER tasks."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Inspired by these insights, as well as the use of multiple prompts"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "in continuous learning tasks, we propose a Unified Modality Brain-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "like Emotional net for affective computing, named UMBEnet, mark-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "ing a paradigm shift in the challenge of unified modal emotional un-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "derstanding. UMBEnetâ€™s design draws inspiration from the brainâ€™s"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "ability to reconfigure and augment its processing capabilities in"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "response to sensory deprivation, integrating a Dual-Stream (DS)"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "structure and a Sparse Feature Fusion (SFF) module. We have de-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "signed a Prompt Pool (First Stream) that employs trainable multiple"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "prompts and a mechanism that simulates neural\nimpulse trans-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "mission, capable of fully harnessing multimodal information, and"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "blending it with inherent prompts (Second Stream) that store emo-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "tional information to form a Dual-Stream mechanism. Prompts are"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "considered analogous to neurons in the brain, capable of storing"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "a certain amount of information. The design of the Prompt Pool"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "aims to emulate the brainâ€™s ability to select and interpret\ninfor-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "mation in varying contexts, particularly in environments where"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "modalities are absent. The design of\ninherent prompts seeks to"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "emulate the activation of the amygdala, the emotional center of"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "the brain, integrating multimodal information for judgment. SFF"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "introduces a mechanism akin to actual neural impulse transmission."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Considering that most features carry low-value information for"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "transmission, sparse matrix fusion mimics the sparsity of neural"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "impulse transmission in the brain[2], sparsely blending the prompts"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "from the dual-stream mechanism. Inspired by the 7%-38%-55% rule,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "we designed an imbalanced multimodal encoder within UMBEnet."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "This encoder allocates a larger proportion of parameters to visual"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "processing, reflecting the leading role of visual information in emo-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "tion recognition, while reducing the parameters for textual and"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "auditory processing."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "UMBEnet introduces several innovations to affective computing,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "with three main contributions:"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "We innovatively combined inherent prompts with a Prompt"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "â€¢"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Pool to design and introduce a Dual-Stream (DS) structure."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "This dual-stream approach ensures a comprehensive under-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "standing of human emotions by leveraging the strengths of"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "different sensory modalities."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Our framework includes a Sparse Feature Fusion (SFF)"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "â€¢"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "module, optimizing the use of available sensory data. By"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "All rivers run into the sea:": "Unified Modality Brain-like Emotional Central Mechanism"
        },
        {
          "All rivers run into the sea:": "sparsely integrating modality fusion prompts with inher-"
        },
        {
          "All rivers run into the sea:": "ent prompts, this module allows for the efficient fusion of"
        },
        {
          "All rivers run into the sea:": "multimodal information, significantly enhancing the robust-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "ness and accuracy of emotion recognition across diverse"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "scenarios."
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "Drawing inspiration from the neural anatomical structure"
        },
        {
          "All rivers run into the sea:": "â€¢"
        },
        {
          "All rivers run into the sea:": "of the human brain, UMBEnet employs a Brain-like Emo-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "tional Processing Framework (BEPF). This biomimetic"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "design closely mimics the human brainâ€™s emotional center,"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "not only offering a more natural and effective way of under-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "standing emotions but also improving system performance"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "in emotion recognition tasks."
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "These contributions collectively enable UMBEnet to consistently"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "outperform existing SOTA methods in DFER domain, particularly"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "in scenarios involving multimodality and modal absence."
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "2\nRELATED WORK"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "2.1\nDynamic Facial Expression Recognition"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "In Dynamic Facial Expression Recognition (DFER), the trend has"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "shifted from static to dynamic analysis, emphasizing temporal dy-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "namics in expressions [44]. This shift is propelled by deep learn-"
        },
        {
          "All rivers run into the sea:": "ing advancements, with 3D Convolutional Neural Networks (C3D)"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "capturing both spatial and temporal data dimensions [42], and"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "Transformers like Visual Transformers (ViT) excelling in feature"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "extraction and sequence processing [11]. These innovations offer"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "refined emotion recognition, addressing the complexities of video"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "data and long-term dependencies in facial expressions. Further-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "more, contrastive learning methods, exemplified by CLIP[37] and"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "its enhancement, CLIPER [24], have forged synergies between vi-"
        },
        {
          "All rivers run into the sea:": "sual and textual modalities, elevating recognition precision across"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "diverse scenarios."
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "Our approach diverges from the aforementioned methodologies"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "by adopting a novel brain-inspired architecture, deviating from the"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "conventional design philosophies of DFER methods. Our approach,"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "particularly the prompt pool design and activation mechanism,"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "innovatively addresses the challenge of missing modalities, merg-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "ing inherent prompts within our SSF framework. This fusion not"
        },
        {
          "All rivers run into the sea:": "only aids in managing multimodal data but also enriches the inter-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "pretability of DFER systems."
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "2.2\nModality Missingness"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "Affective computing strives to empower computers with the ability"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "to recognize and understand human emotional states by integrating"
        },
        {
          "All rivers run into the sea:": "information from various sources such as voice, facial expressions,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "architecture of the Dual-Stream (DS), with the left side showing the actual structure and the right side providing a flattened"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "perspective to concretely understand the Prompt Pool and its activation mechanism."
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "After fusion by the adaptive self-attention module, the synthe-"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "sized embedding ğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› is sent into a predefined Prompt Pool with"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ""
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "the purpose of finding a set of prompts that best match the current"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "situation. Let the Prompt Pool be ğ‘ƒ =\n, the selection\nğ‘1, ğ‘2, ..., ğ‘ğ‘›"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "{\n}"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "of the most matching set of prompts can be represented as:"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ""
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ""
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ","
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "(4)\nğ‘ƒ âˆ— = ğ‘ƒğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ (\nğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›)"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ""
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "In the Prompt Pool, the process involves searching for the em-"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ""
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "bedding most similar to the current multimodal input information,"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "and then using that embedding as the Key to find the corresponding"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ""
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "prompt as the Value. This process ensures that the selected prompt"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "can be decoupled from the current multimodal input information"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "and can also learn information missing from the modalities."
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "Finally, the prompts selected from the Prompt Pool are concate-"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "nated with original\nlearnable prompts to form the final prompt"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ""
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "representation ğ‘ƒğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ . Thereafter, ğ‘ƒğ‘“ ğ‘–ğ‘›ğ‘ğ‘™\ntogether with ğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› are"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "used to compute the contrastive loss to identify the closest emotion"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "category:"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": ""
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "exp\ncos"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "(\n(\n(\nğ‘ƒğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ )))\n,"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "log\n(5)\nğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘ ğ‘¡ ="
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "cos\nL\nâˆ’\nğ‘ ğ‘—\nğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›, ğ¸ğ‘\nğ‘— exp"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "(\n(\n(\n)))"
        },
        {
          "the Sparse Feature Fusion (SFF), including how multimodal prompts are merged with inherent prompts. Figure 2c presents the": "Ë"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the model to dynamically select the most matching prompt while": "dealing with a specific multimodal input. This matching process can"
        },
        {
          "the model to dynamically select the most matching prompt while": "be realized by calculating the similarity between the input features"
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": "ğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› and each key ğ‘˜ ğ‘— , then selecting the prompt with the highest"
        },
        {
          "the model to dynamically select the most matching prompt while": "similarity for subsequent processing."
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": ",\n(8)\nğ‘— âˆ— = cos\nğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›, ğ‘˜ ğ‘—"
        },
        {
          "the model to dynamically select the most matching prompt while": "(\n)"
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": "(9)\nğ‘âˆ— = ğ‘£ ğ‘— âˆ—,"
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": "Where cos denotes the cosine similarity function,\nğ‘— âˆ— is the index of"
        },
        {
          "the model to dynamically select the most matching prompt while": "the prompt most matching with the fused feature ğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›, and ğ‘âˆ—"
        },
        {
          "the model to dynamically select the most matching prompt while": "is the value of the selected prompt."
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": "Audio"
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": "Video"
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": ""
        },
        {
          "the model to dynamically select the most matching prompt while": "Text"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "3.4\nTraining Strategy"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "As illustrated in Figure 5, UMBEnetâ€™s comprehensive training strat-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "egy begins by independently training prompts within the Prompt"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Pool using unimodal information. Initially, the system inputs au-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "dio (A), visual (V), and textual (T) modalities separately to tailor"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "prompts specific to each modality. This first phase of training en-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "sures that the prompts are finely tuned to respond to the unique"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "features of each input type."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "In the subsequent phase, prompts that were activated in the first"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "stage are transferred to a new Prompt Pool. Here, the system under-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "goes training with randomly missing modal information, simulating"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "scenarios where certain modalities may be miss or incomplete. This"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "two-tiered training approach allows the model to delve deeply into"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "the nuances of modal information, effectively leveraging the partial"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "knowledge obtained from each modality to compensate for any"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "missing data. Through this strategy, UMBEnet enhances its capacity"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "to interpret complex emotional prompts by becoming proficient at"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "drawing inferences from incomplete or asymmetrically available"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "data. The employment of this training regime not only promotes the"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "robustness of the system in handling real-world scenarios where"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "multimodal data might not always be complete but also aligns with"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "the cognitive flexibility inherent in human emotional understand-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "ing, where inferences are often drawn from partial\ninformation."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "4\nEXPERIMENTS"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "4.1\nExperimental Setup"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "All experiments in this study were conducted in a hardware envi-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "ronment with the following specifications: two NVIDIA GeForce"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "RTX 3090 graphics cards and a computer equipped with an Intel(R)"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Xeon(R) CPU 5218R @ 2.10GHz. During the model training process,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "we chose the Adam optimizer as our optimization algorithm. The"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "initial learning rate was set to 0.002, and we adopted a mini-batch"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "training approach with a batch size of 16. Furthermore, to prevent"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "overfitting and improve the modelâ€™s generalization ability, we de-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "signed a dynamic learning rate adjustment mechanism based on"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "the performance on the validation set. Specifically,\nif there is no"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "significant decrease in loss on the validation set for three consecu-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "tive training epochs, the learning rate will be reduced to 0.6 times"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "7, we"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "10âˆ’"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Ã—"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "consider the model to have reached early convergence, at which"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "point the training process will be terminated. In addition, to further"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "control the phenomenon of overfitting, we will take corresponding"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "measures to adjust when the accuracy of the model on the training"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "set is more than 80%, we will terminate the training in advance. In"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "the experimental results, bold represents the best, and underline"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "represents the second best. The confusion matrices and feature visu-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "alizations in Figures 6 and 7 showcase the exceptional performance"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "of UMBEnet on FERV39k, DFEW, and MAFW. For more confusion"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "matrices, please see supplementary materials."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "4.2\nEvaluation Metrics"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "When evaluating model performance, we adopted WAR and UAR"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "as evaluation metrics. WAR (Weighted Accuracy Recall) refers to"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "the weighted average of the prediction accuracy of the classifica-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "tion model on each class. This metric considers the sample size of"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "each class, making it more suitable for imbalanced datasets. UAR"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "All rivers run into the sea:": "Unified Modality Brain-like Emotional Central Mechanism"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "Figure 6: Features before and after processing by the model."
        },
        {
          "All rivers run into the sea:": "The left side displays features just entered into the model,"
        },
        {
          "All rivers run into the sea:": "scattered overall; the right side shows features before out-"
        },
        {
          "All rivers run into the sea:": "put, demonstrating improved clustering. 0-10 in the legend"
        },
        {
          "All rivers run into the sea:": "represents 11-class classification."
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "(Unweighted Average Recall) refers to the average recall of the clas-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "sification model on each class, regardless of the sample size of the"
        },
        {
          "All rivers run into the sea:": "class. This metric treats each class equally, thus being more suitable"
        },
        {
          "All rivers run into the sea:": "for balanced datasets. These two metrics provide a comprehensive"
        },
        {
          "All rivers run into the sea:": "assessment of the modelâ€™s performance across multiple classes and"
        },
        {
          "All rivers run into the sea:": "are widely used evaluation metrics in the DFER domain."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "class. This metric treats each class equally, thus being more suitable": "for balanced datasets. These two metrics provide a comprehensive",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "vs. other SOTA methods on DFEW and FERV39K for 7-class"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "assessment of the modelâ€™s performance across multiple classes and",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "classification. * represents visual and audio modal input)."
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "are widely used evaluation metrics in the DFER domain.",
          "Table 1: Overall Model Performance Comparison (UMBEnet": ""
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "Publication"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": ""
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "CVPRâ€™15"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "ICCVâ€™17"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "ICCVâ€™17"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "CVPRâ€™18"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "CVPRâ€™18"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "/"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "/"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "MMâ€™20"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "MMâ€™21"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "arXivâ€™22"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "C&Câ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "AAAIâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "PRâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "ICASSPâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "AAAIâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "PRâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "CVPRâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "CVPRWâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": ""
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "arXivâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "arXivâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "BMVCâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": ""
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "MMâ€™23"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "/"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": ""
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "",
          "Table 1: Overall Model Performance Comparison (UMBEnet": "/"
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "7:",
          "Table 1: Overall Model Performance Comparison (UMBEnet": ""
        },
        {
          "class. This metric treats each class equally, thus being more suitable": "FERV39K, DFEW, and MAFW.",
          "Table 1: Overall Model Performance Comparison (UMBEnet": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: UMBEnet Hyperparameter Ablation Study (UM-",
      "data": [
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "tributes less to accuracy, which further validates the design of our"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "unbalanced encoder. Additionally,"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "hyperparameter settings on the MAFW dataset, including TOPK,"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "length,"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "performance. An overview of Table 4 reveals a generally positive"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "correlation between the size of the designed Prompt Pool and the"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "number of chosen prompts (topk) with accuracy, and a similar pos-"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "itive relationship with the overall"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "the training strategy ablation section, we observe that the Prompt"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "Pool, which underwent two-stage hybrid training. In our ablation"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "experiments focused on training strategies, we discovered that"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "our proposed two-stage transfer training approach significantly"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "enhances performance. Even within the same task, our training"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "strategy achieved improvements of 0.48% in UAR and 1.26% in WAR"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "in the 11-class configuration, and 0.57% in UAR and 1.49% in WAR"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "in the 10-class setup. It significantly outperforms the one trained"
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": ""
        },
        {
          "only visual input). Moreover, as predicted, the audio modality con-": "in a single phase. The visualizations in Figure 8 corroborate our"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: UMBEnet Hyperparameter Ablation Study (UM-",
      "data": [
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Our approach significantly outperforms the existing SOTA meth-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "ods in a multimodal 11-class setting, and even with the neutral class"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "removed in a 10-class configuration, our method still substantially"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "surpasses the SOTA with a 7.08% increase in UAR and a 9.33%"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "increase in WAR. Comparisons in the 10-class setting, with missing"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "modalities, reveal that the most effective modality is visual, followed"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "by textual, with audio being the least effective. It is evident that"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "our method significantly outperforms the existing SOTA methods,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "both for the 11-class and the reduced 10-class configurations."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "In our experiments with missing modalities in Table 3, we dis-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "covered that inferring with a full-modal model in the absence of"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "certain modalities can achieve, or even surpass, the performance"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "of training unimodal models from scratch. This is attributed to the"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "two-stage training strategy employed within the full-modal model,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "suggesting that information from different modalities can often be"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "cross-utilized to enhance performance. This finding underscores"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "the efficacy of our approach in leveraging cross-modal information,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "thereby boosting the robustness and adaptability of the model in"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "vs. other SOTA methods on MAFW for 11-class classification)."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Publication"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "CVPRâ€™15"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "/"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "/"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "/"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "/"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "MMâ€™21"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "AAAIâ€™23"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "arXivâ€™23"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "BMVCâ€™23"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "MMâ€™23"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "/"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "/"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: UMBEnet Hyperparameter Ablation Study (UM-",
      "data": [
        {
          "Table 3: Performance comparison between multimode and": "",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "classification).",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "Method\nMode",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "UMBEnet\nV",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "A",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "T",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "V+T",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "V+A",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "V+A+T",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "Missing Modal Inference",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "",
          "hypothesis.": "(a) Prompt Pool of 1-stage training"
        },
        {
          "Table 3: Performance comparison between multimode and": "UMBEnet\nV",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "A",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "",
          "hypothesis.": "Figure 8: Visualization of"
        },
        {
          "Table 3: Performance comparison between multimode and": "T",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "V+T",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "V+A",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "V+A+T",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "",
          "hypothesis.": ""
        },
        {
          "Table 3: Performance comparison between multimode and": "",
          "hypothesis.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "All rivers run into the sea:": "Unified Modality Brain-like Emotional Central Mechanism"
        },
        {
          "All rivers run into the sea:": "5\nCONCLUSION"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "Our work introduces UMBEnet, a novel unified modal model that"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "departs from the paradigms of previous DFER methods, mirror-"
        },
        {
          "All rivers run into the sea:": "ing the complex neural architecture of the human brain in emo-"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "tional understanding and effectively addressing the challenges of"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "Modality Missingness and multimodal\nfusion. Extensive testing"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "on leading DFER benchmarksâ€”DFEW, FERV39k, and MAFWâ€”has"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "demonstrated the superior performance of UMBEnet, especially"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "under various channel conditions or in their absence. We believe"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "UMBEnet will be instructive to the entire multimodal community,"
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": "and we will continue to explore the use of UMBEnet in other multi-"
        },
        {
          "All rivers run into the sea:": "modal areas in future."
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        },
        {
          "All rivers run into the sea:": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "In International conference on machine learning. PMLR, 8748â€“8763."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[38] Norihiro Sadato, Tomohisa Okada, Manabu Honda, and Yoshiharu Yonekura."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "2002. Critical period for cross-modal plasticity in blind humans: a functional"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "MRI study. Neuroimage 16, 2 (2002), 389â€“400."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[39] K Sathian and Randall Stilla. 2010. Cross-modal plasticity of tactile perception in"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "blindness. Restorative neurology and neuroscience 28, 2 (2010), 271â€“281."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[40] Barry E Stein and M Alex Meredith. 1993. The merging of the senses. MIT press."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[41]\nLicai Sun, Zheng Lian, Bin Liu, and Jianhua Tao. 2023. Mae-dfer: Efficient masked"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "autoencoder for self-supervised dynamic facial expression recognition. In Pro-"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "ceedings of the 31st ACM International Conference on Multimedia. 6110â€“6121."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[42] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "2015.\nLearning spatiotemporal\nfeatures with 3d convolutional networks.\nIn"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Proceedings of the IEEE international conference on computer vision. 4489â€“4497."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[43] Hanyang Wang, Bo Li, Shuang Wu, Siyuan Shen, Feng Liu, Shouhong Ding,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "and Aimin Zhou. 2023. Rethinking the Learning Paradigm for Dynamic Facial"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Expression Recognition. In Proceedings of the IEEE/CVF Conference on Computer"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Vision and Pattern Recognition. 17958â€“17968."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[44]\nYan Wang, Yixuan Sun, Yiwen Huang, Zhongying Liu, Shuyong Gao, Wei Zhang,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Weifeng Ge, and Wenqiang Zhang. 2022.\nFerv39k: A large-scale multi-scene"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "dataset for facial expression recognition in videos. In Proceedings of the IEEE/CVF"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "conference on computer vision and pattern recognition. 20922â€“20931."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[45] Yan Wang, Yixuan Sun, Wei Song, Shuyong Gao, Yiwen Huang, Zhaoyu Chen,"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Weifeng Ge, and Wenqiang Zhang. 2022. Dpcnet: Dual path multi-excitation"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "collaborative network for facial expression representation learning in videos. In"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Proceedings of the 30th ACM International Conference on Multimedia. 101â€“110."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[46] Ali Yadollahi, Ameneh Gholipour Shahraki, and Osmar R Zaiane. 2017. Current"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "state of text sentiment analysis from opinion to emotion mining. ACM Computing"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Surveys (CSUR) 50, 2 (2017), 1â€“33."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[47]\nShiqing Zhang, Yijiao Yang, Chen Chen, Xingnan Zhang, Qingming Leng, and"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Xiaoming Zhao. 2023. Deep learning-based multimodal emotion recognition from"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "audio, visual, and text modalities: A systematic review of recent advancements"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "and future prospects. Expert Systems with Applications (2023), 121692."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "[48] Zengqun Zhao and Ioannis Patras. 2023. Prompting Visual-Language Models for"
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": "Dynamic Facial Expression Recognition. arXiv preprint arXiv:2308.13382 (2023)."
        },
        {
          "Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, Yan Wang, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "1\nOVERVIEW"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "The additional material mainly consists of two parts: the research"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "motivation and the experiment."
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "2 Research Motivation is a supplement to 1 Introduction part,"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "â€¢"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "3 Experimental part is a supplement to 4 Experiment."
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "â€¢"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "4 Future Work part is a supplement to 5 Conclusion"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "â€¢"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "2\nRESEARCH MOTIVATION"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "Cross-modal plasticity refers to the brainâ€™s ability to adjust and"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "reorganize its functions to enhance the processing capabilities of"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "other sensory modalities following the loss or absence of\ninput"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "from a particular sensory modality. This phenomenon primarily"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "involves neural plasticity, which is the structural and functional"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "changes in neurons and neural networks under the influence of ex-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "perience or the environment. These changes include the formation,"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "strengthening, or weakening of synapses, and the establishment"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "of new neural pathways. In cases of sensory deprivation, such as"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "blindness or deafness, the brain regions originally receiving input"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "from the now-absent modality no longer do so. Instead, neural cen-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "ters of other modalities may take over these areas, reallocating and"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "optimizing their functions. For example, in experiments involving"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "visual deprivation and auditory enhancement, the brain areas origi-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "nally processing visual information, such as the visual cortex, begin"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "to process auditory or tactile information. This reorganization can"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "lead to increased auditory and tactile sensitivity, as observed in"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "the extraordinary auditory localization abilities of blind individu-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "als. Similarly, in experiments involving auditory deprivation and"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "visual/tactile enhancement, individuals with hearing loss exhibit"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "enhanced visual and tactile functions, such as improved visual-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "spatial processing abilities and heightened tactile perception. The"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "mechanisms underlying cross-modal plasticity involve a variety"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "of molecular and cellular events, such as changes in neurotrophic"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "factors, synaptic connectivity reorganization, and the rebalancing"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "of inhibitory and excitatory neurotransmissions. The brain utilizes"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": ""
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "these mechanisms to optimize the remaining sensory inputs to"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "compensate for the lost senses. This plasticity helps individuals"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "adapt to sensory loss and enhances the functions of other senses,"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "revealing the brainâ€™s remarkable adaptability to changes in sensory"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "inputs."
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "Neuroanatomy is the scientific field that studies the structure"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "and function of the brain,\nincluding how the brain processes in-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "formation through its complex network structures. Specifically, in"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "the context of multimodal information processing, neuroanatomy"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "demonstrates how the brain integrates information from our vari-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "ous sensory systems, including vision, hearing, touch, smell, and"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "taste. The parietal\nlobe,\nlocated in the upper part of the brain,\nis"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "a primary area for processing tactile, visual, and spatial informa-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "tion. The temporal\nlobe primarily handles auditory information"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "and some visual memory. It contains several key multimodal areas,"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "such as the superior temporal sulcus (STS), which is an impor-"
        },
        {
          "Unified Modality Brain-like Emotional Central Mechanism": "tant area for integrating visual and auditory information related"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACM MM, 2024, Melbourne, Australia": "surpasses the performance benchmarks set by existing state-of-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "the-art (SOTA) methods. Particularly in scenarios characterized by"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "multi-channel configurations or the absence of such configurations,"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "UMBEnet clearly exceeds contemporary leading solutions."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "We believe that the BEPF framework, DS structure, and SFF mod-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ule proposed in this paper offer significant guidance for addressing"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "issues of fusion and modality absence within the multimodal do-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "main. Our work provides the multimodal community with a novel"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "and distinct model framework for multimodal fusion and address-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ing modality absence, differing from previous approaches. In the"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "future, we aim to extend the BEPF framework to more multimodal"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "domains, believing it to be a transferable multimodal framework"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "that requires minimal modification for adaptation."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Supplementary Materials: All rivers run into the sea:": "Unified Modality Brain-like Emotional Central Mechanism"
        },
        {
          "Supplementary Materials: All rivers run into the sea:": "V"
        },
        {
          "Supplementary Materials: All rivers run into the sea:": "A"
        },
        {
          "Supplementary Materials: All rivers run into the sea:": "V + A\nT"
        },
        {
          "Supplementary Materials: All rivers run into the sea:": "V + T\nV + A + T\nFigure 3: Confusion Matrices of Missing Mode (UMBEnet on"
        },
        {
          "Supplementary Materials: All rivers run into the sea:": "MAFW for 11-class and 10-class classification)."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "classification)."
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal video sentiment analysis using deep learning approaches, a survey",
      "authors": [
        "Sarah Abdu",
        "Ahmed Yousef",
        "Ashraf Salem"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "2",
      "title": "Mapping thalamic innervation to individual L2/3 pyramidal neurons and modeling their 'readout'of visual input",
      "authors": [
        "Aygul Balcioglu",
        "Rebecca Gillani",
        "Michael Doron",
        "Kendyll Burnell",
        "Taeyun Ku",
        "Alev Erisir",
        "Kwanghun Chung",
        "Idan Segev",
        "Elly Nedivi"
      ],
      "year": "2023",
      "venue": "Nature Neuroscience"
    },
    {
      "citation_id": "3",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas BaltruÅ¡aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal machine learning: A survey and taxonomy"
    },
    {
      "citation_id": "4",
      "title": "Cross-modal plasticity: where and how?",
      "authors": [
        "Daphne Bavelier",
        "Helen Neville"
      ],
      "year": "2002",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "5",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Audio-visual sentiment analysis for learning emotional arcs in movies",
      "authors": [
        "Eric Chu",
        "Deb Roy"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "7",
      "title": "Functional relevance of cross-modal plasticity in blind humans",
      "authors": [
        "Pablo Leonardo G Cohen",
        "Alvaro Celnik",
        "Brian Pascual-Leone",
        "Lala Corwell",
        "James Faiz",
        "Manabu Dambrosia",
        "Norihiro Honda",
        "Christian Sadato",
        "M Gerloff",
        "Dolores CatalÃ¡"
      ],
      "year": "1997",
      "venue": "Nature"
    },
    {
      "citation_id": "8",
      "title": "Period of susceptibility for cross-modal plasticity in the blind",
      "authors": [
        "Robert Leonardo G Cohen",
        "Norihiro Weeks",
        "Pablo Sadato",
        "Kenji Celnik",
        "Mark Ishii",
        "Hallett"
      ],
      "year": "1999",
      "venue": "Annals of Neurology: Official Journal of the American Neurological Association and the Child Neurology Society"
    },
    {
      "citation_id": "9",
      "title": "Crossmodal plasticity for the spatial processing of sounds in visually deprived subjects",
      "authors": [
        "Patrice Olivier Collignon",
        "Maryse Voss",
        "Franco Lassonde",
        "Lepore"
      ],
      "year": "2009",
      "venue": "Experimental brain research"
    },
    {
      "citation_id": "10",
      "title": "Human brain anatomy in computerized images",
      "authors": [
        "Hanna Damasio"
      ],
      "year": "2005",
      "venue": "Human brain anatomy in computerized images"
    },
    {
      "citation_id": "11",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "Brain anatomical networks in early human brain development",
      "authors": [
        "Yong Fan",
        "Feng Shi",
        "Jeffrey Smith",
        "Weili Lin",
        "John Gilmore",
        "Dinggang Shen"
      ],
      "year": "2011",
      "venue": "Brain anatomical networks in early human brain development"
    },
    {
      "citation_id": "14",
      "title": "EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition",
      "authors": [
        "Niki Maria",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition",
      "arxiv": "arXiv:2310.16640"
    },
    {
      "citation_id": "15",
      "title": "The functional anatomy of time: what and when in the brain",
      "authors": [
        "Karl Friston",
        "Gyorgy BuzsÃ¡ki"
      ],
      "year": "2016",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "16",
      "title": "Cross-modal plasticity for sensory and motor activation patterns in blind subjects",
      "authors": [
        "Er Gizewski",
        "Armin Gasser",
        "De Greiff",
        "Michael Boehm",
        "Forsting"
      ],
      "year": "2003",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "17",
      "title": "Cross-modal plasticity in developmental and age-related hearing loss: Clinical implications",
      "authors": [
        "Hannah Glick",
        "Anu Sharma"
      ],
      "year": "2017",
      "venue": "Hearing research"
    },
    {
      "citation_id": "18",
      "title": "The role of text pre-processing in sentiment analysis",
      "authors": [
        "Emma Haddi",
        "Xiaohui Liu",
        "Yong Shi"
      ],
      "year": "2013",
      "venue": "Procedia computer science"
    },
    {
      "citation_id": "19",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet",
      "authors": [
        "Kensho Hara",
        "Hirokatsu Kataoka",
        "Yutaka Satoh"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "22",
      "title": "Emotion circuits in the brain",
      "authors": [
        "Joseph Ledoux"
      ],
      "year": "2000",
      "venue": "Annual review of neuroscience"
    },
    {
      "citation_id": "23",
      "title": "Frame Level Emotion Guided Dynamic Facial Expression Recognition With Emotion Grouping",
      "authors": [
        "Bokyeung Lee",
        "Hyunuk Shin",
        "Bonhwa Ku",
        "Hanseok Ko"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "CLIPER: A Unified Vision-Language Framework for In-the-Wild Facial Expression Recognition",
      "authors": [
        "Hanting Li",
        "Hongjing Niu",
        "Zhaoqing Zhu",
        "Feng Zhao"
      ],
      "year": "2023",
      "venue": "CLIPER: A Unified Vision-Language Framework for In-the-Wild Facial Expression Recognition",
      "arxiv": "arXiv:2303.00193"
    },
    {
      "citation_id": "25",
      "title": "NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression Recognition",
      "authors": [
        "Hanting Li",
        "Mingzhe Sui",
        "Zhaoqing Zhu"
      ],
      "year": "2022",
      "venue": "NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression Recognition",
      "arxiv": "arXiv:2206.04975"
    },
    {
      "citation_id": "26",
      "title": "Multimodal fusion for video sentiment analysis",
      "authors": [
        "Ruichen Li",
        "Jinming Zhao",
        "Jingwen Hu",
        "Shuai Guo",
        "Qin Jin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "27",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng",
        "Wenbin Wang",
        "Guanghao Yin",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "28",
      "title": "Early crossmodal plasticity in adults",
      "authors": [
        "Luca Lo Verde",
        "Maria Concetta Morrone",
        "Claudia Lunghi"
      ],
      "year": "2017",
      "venue": "Journal of Cognitive Neuroscience"
    },
    {
      "citation_id": "29",
      "title": "Cross-modal plasticity in specific auditory cortices underlies visual compensations in the deaf",
      "authors": [
        "Alex Stephen G Lomber",
        "Andrej Meredith",
        "Kral"
      ],
      "year": "2010",
      "venue": "Nature neuroscience"
    },
    {
      "citation_id": "30",
      "title": "Logo-Former: Local-Global Spatio-Temporal Transformer for Dynamic Facial Expression Recognition",
      "authors": [
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "",
      "authors": [
        "Albert Mehrabian"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "32",
      "title": "Sentiment analysis: Detecting valence, emotions, and other affectual states from text",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2016",
      "venue": "Emotion measurement"
    },
    {
      "citation_id": "33",
      "title": "Fusing audio, textual, and visual features for sentiment analysis of news videos",
      "authors": [
        "MoisÃ©s Pereira",
        "FlÃ¡vio PÃ¡dua",
        "Adriano Pereira",
        "FabrÃ­cio Benevenuto",
        "Daniel Dalip"
      ],
      "year": "2016",
      "venue": "Proceedings of the International AAAI Conference on Web and Social Media"
    },
    {
      "citation_id": "34",
      "title": "Fusing audio, visual and textual clues for sentiment analysis from multimodal content",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Newton Howard",
        "Guang-Bin Huang",
        "Amir Hussain"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "35",
      "title": "Cross-modal plasticity in early blindness",
      "authors": [
        "Maurice Ptito",
        "Ron Kupers"
      ],
      "year": "2005",
      "venue": "Journal of integrative neuroscience"
    },
    {
      "citation_id": "36",
      "title": "Learning spatio-temporal representation with pseudo-3d residual networks",
      "authors": [
        "Zhaofan Qiu",
        "Ting Yao",
        "Tao Mei"
      ],
      "year": "2017",
      "venue": "Learning spatio-temporal representation with pseudo-3d residual networks"
    },
    {
      "citation_id": "37",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "38",
      "title": "Critical period for cross-modal plasticity in blind humans: a functional MRI study",
      "authors": [
        "Norihiro Sadato",
        "Tomohisa Okada",
        "Manabu Honda",
        "Yoshiharu Yonekura"
      ],
      "year": "2002",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "39",
      "title": "Cross-modal plasticity of tactile perception in blindness",
      "authors": [
        "K Sathian",
        "Randall Stilla"
      ],
      "year": "2010",
      "venue": "Restorative neurology and neuroscience"
    },
    {
      "citation_id": "40",
      "title": "The merging of the senses",
      "authors": [
        "E Barry",
        "M Alex Stein",
        "Meredith"
      ],
      "year": "1993",
      "venue": "The merging of the senses"
    },
    {
      "citation_id": "41",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "42",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "43",
      "title": "Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition",
      "authors": [
        "Hanyang Wang",
        "Bo Li",
        "Shuang Wu",
        "Siyuan Shen",
        "Feng Liu",
        "Shouhong Ding",
        "Aimin Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "44",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Yan Wang",
        "Yixuan Sun",
        "Yiwen Huang",
        "Zhongying Liu",
        "Shuyong Gao",
        "Wei Zhang",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "45",
      "title": "Dpcnet: Dual path multi-excitation collaborative network for facial expression representation learning in videos",
      "authors": [
        "Yan Wang",
        "Yixuan Sun",
        "Wei Song",
        "Shuyong Gao",
        "Yiwen Huang",
        "Zhaoyu Chen",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Current state of text sentiment analysis from opinion to emotion mining",
      "authors": [
        "Ali Yadollahi",
        "Ameneh Gholipour Shahraki",
        "Osmar R Zaiane"
      ],
      "year": "2017",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "47",
      "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "Shiqing Zhang",
        "Yijiao Yang",
        "Chen Chen",
        "Xingnan Zhang",
        "Qingming Leng",
        "Xiaoming Zhao"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "48",
      "title": "Prompting Visual-Language Models for Dynamic Facial Expression Recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "Prompting Visual-Language Models for Dynamic Facial Expression Recognition",
      "arxiv": "arXiv:2308.13382"
    }
  ]
}