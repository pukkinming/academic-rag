{
  "paper_id": "2509.16623v1",
  "title": "Cgtgait: Collaborative Graph And Transformer For Gait Emotion Recognition",
  "published": "2025-09-20T10:48:51Z",
  "authors": [
    "Junjie Zhou",
    "Haijun Xiong",
    "Junhao Lu",
    "Ziyu Lin",
    "Bin Feng"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Skeleton-based gait emotion recognition has received significant attention due to its wide-ranging applications. However, existing methods primarily focus on extracting spatial and local temporal motion information, failing to capture long-range temporal representations. In this paper, we propose CGTGait, a novel framework that collaboratively integrates graph convolution and transformers to extract discriminative spatiotemporal features for gait emotion recognition. Specifically, CGTGait consists of multiple CGT blocks, where each block employs graph convolution to capture frame-level spatial topology and the transformer to model global temporal dependencies. Additionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to effectively aggregate posture and motion spatiotemporal features, facilitating the exchange of complementary information between the two streams. We evaluate our method on two widely used datasets, Emotion-Gait and ELMD, demonstrating that our CGTGait achieves state-ofthe-art or at least competitive performance while reducing computational complexity by approximately 82.2% (only requiring 0.34G FLOPs) during testing. Code is available at https://github.com/githubzjj1/CGTGait.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition has been widely applied in various fields, including human-computer interaction, video surveillance, and smart healthcare  [49, 21, 61] . Some studies  [10, 17, 24, 29]  have shown that gait patterns (e.g., walking speed, stride length, and arm swing angles) can reflect an individual's emotional state. Compared to facial emotion recognition  [26] , gait-based methods do not require close interaction or active cooperation from subjects  [52, 28] . With the use of 3D skeletons, gait emotion recognition (GER)  [5, 32, 57]  has gained significant atten-* Corresponding Author. tion in the computer vision community.\n\nRecently, graph convolution-based methods  [4, 53, 59, 67]  have achieved significant progress, as the skeleton naturally forms a graph structure in non-Euclidean geometric spaces, where each graph node corresponds directly to a skeletal joint. Prior studies typically adopt network architectures similar to those used in action recognition, such as the Graph Convolutional Network (GCN) for spatial typology modeling and the Temporal Convolutional Network (TCN) for temporal dynamic modeling. However, research  [53]  highlights a key distinction between action recognition and emotion recognition: While action recognition primarily focuses on local joint movements, emotion recognition involves classifying emotions based on the overall body state. Consequently, TCN alone is insufficient for capturing long-range temporal dependencies, which are crucial for emotion recognition.\n\nOn the other hand, some methods employ dual-stream network architectures to capture more comprehensive emotion features. For example, BPM-GCN  [57]  is the first method that fuses human posture information and higherorder motion features by dynamic weight addition. However, this fusion manner fails to fully capture the correlation between different streams and does not effectively leverage their complementary information for improved recognition. Therefore, we employ the Transformer to align and complement features across different streams based on semantic similarity, forming a more comprehensive emotion representation.\n\nTransformer  [22, 44]  has become the dominant model in natural language processing (NLP) due to its powerful long-range modeling capabilities. Inspired by the success of NLP, numerous studies  [3, 6, 12, 31, 55, 63]  have extended its application to computer vision, achieving state-of-the-art performance. Building on this progress, researchers have explored its potential in video understanding tasks, including action recognition  [23, 50] , video generation  [14, 19] , and spatiotemporal grounding  [47, 48] . Recently, large multimodal models  [27, 30]  have also adopted transformerbased architectures. Motivated by these advancements, we incorporate transformers into skeleton-based gait emotion recognition to capture global temporal representations and bridge information gaps between different streams (i.e., posture and motion).\n\nSpecifically, in this paper, we propose a novel framework, called CGTGait, which is the first work to integrate GCNs and Transformers for skeleton-based gait emotion recognition. CGTGait adopts a dual-stream architecture comprising posture and motion streams, where each stream utilizes stacked CGT blocks to capture both framelevel spatial topology and global temporal information. Additionally, we incorporate a contrastive learning strategy within each CGT block to enhance feature discriminability and robustness by constraining the distance between confident and ambiguous samples. To further bridge spatiotemporal information gaps between the two streams, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module, which utilizes the temporal cross-transformer and dynamic spatial attention to facilitate the exchange of complementary information between posture and motion features. The former facilitates long-range temporal dependencies and context awareness across frames between different streams, while the latter adaptively emphasizes key spatial typology information, ensuring a more comprehensive understanding of emotional expressions. We conduct extensive experiments on two public datasets (Emotion-Gait  [1]  and ELMD  [2] ) to validate the effectiveness of CGTGait. The results demonstrate that our method achieves superior or at least competitive performance with state-of-the-art methods while significantly reducing computational complexity by approximately 82.2% (i.e., only requiring 0.34G FLOPs) during testing (as shown in Figure  1 ).\n\nThe main contributions of this paper can be summarized as follows:\n\n‚Ä¢ We propose CGTGait, a novel gait emotion recognition method that couples graphs and transformers to capture more discriminative spatiotemporal representations. To the best of our knowledge, CGTGait is the first method to leverage the global temporal modeling capability of transformers for skeleton-based gait emotion recognition.\n\n‚Ä¢ We introduce BCSF, a Bidirectional Cross-Stream Fusion module that dynamically aligns posture and motion representations using the temporal crosstransformer and dynamic spatial attention, effectively bridging spatiotemporal gaps and facilitating the exchange of complementary information between posture and motion features.\n\n‚Ä¢ Our method achieves state-of-the-art or comparable performance on two datasets (Emotion-Gait  [1]  and ELMD  [2] ), while significantly reducing test computational complexity by approximately 82.2%, requiring only 0.34G FLOPs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we review the most relevant studies on gait emotion recognition and the application of Transformers in skeleton-based action recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Gait Emotion Recognition",
      "text": "Early studies  [9, 20, 35, 45, 8]  employ traditional machine learning methods with hand-crafted features to extract emotional information, such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Support Vector Machines (SVM). For example, Li et al.  [24]  identify emotions by leveraging the discrete Fourier transform and statistical principles. However, hand-crafted feature modeling is complex, and variations across different datasets significantly hinder its generalization ability.\n\nRecently, the rapid advancement of deep learning has significantly contributed to the development of gait emotion recognition. Based on their encoding methods, deep learning models can be broadly categorized into sequencebased, image-based, and graph-based methods  [5, 18] . Sequence-based methods  [2, 41, 42]  typically employ Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) to model inter-frame information in skeleton sequences. Image-based methods  [34, 18]  transform skeleton sequences into images and then leverage CNN to extract hierarchical emotional representations. However, both sequence-and image-based methods struggle to capture interactions between distant, non-adjacent joints due to the lack of spatial modeling or the limited receptive field of convolutional kernels. As skeleton structures naturally form graphs in non-Euclidean geometric spaces, the core idea of graph-based methods  [1, 5, 33, 53, 4, 29, 66, 57, 59, 67, 56]  is primarily to leverage GCN to model spatial topologies and TCN to capture temporal motion dynamics. For example, Chen and Sun  [4]  present STA-GCN, which adaptively aggregates implicit and explicit spatial features along with multi-scale joint motion information. Additionally, Zhai et al.  [57]  propose a dual-steam network BPM-GCN to extract emotion features from both posture and motion streams. However, existing studies fail to fully exploit the complementary information between different streams, leading to the loss of discriminative features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transformers In Skeleton-Based Action Recognition",
      "text": "The Transformer model excels at capturing global temporal information, in contrast to TCN, which has a limited receptive field. Several works  [60, 11, 46, 40, 39]  have leveraged transformers for skeleton-based action recognition to extract more comprehensive spatiotemporal features. For example, Zhou et al.  [64]  introduce Hyperformer, which integrates skeleton structure information into the Transformer so as to capture inherent high-order joint relationships. Similarly, Wu et al.  [51]  propose FreqMix-Former, which enhances spatiotemporal correlation extraction by integrating frequency features with spatial representations. Additionally, other works  [7, 13, 15, 65, 36]  have explored hybrid architectures that combine GCNs and Transformers for improved spatiotemporal feature extraction. Notably, Chi et al.  [7]  integrate NeuralODEs with Transformer to enable online action recognition.\n\nIn this paper, we propose CGTGait, the first framework to collaboratively integrate both GCNs and Transformers for extracting more discriminative features in skeletonbased gait emotion recognition. Furthermore, we employ the cross-transformer to enhance cross-frame context understanding between different streams.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "In this section, we first present the overall architecture of our method in Section 3.1. Next, we describe the detailed structure of the collaborative graph and transformer in Section 3.2. Finally, we provide a comprehensive explanation of the bidirectional cross-stream fusion module in Section 3.3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overview",
      "text": "We represent a skeleton sequence with T frames as X ‚àà R Cin√óT√óN , where C in and N denote the dimensionality of each joint and the number of joints per frame, respectively. As shown in Figure  2 , our proposed CGTGait consists of two streams: the posture stream and the motion stream. The posture stream directly takes the original sequence X as input and employs four stacked Collaborative Graph and Transformer (CGT) blocks to extract the human posture spatiotemporal feature f p ‚àà R Cp√óTp√óN (e.g., joint angles and distances). The motion stream first applies a motion extractor M(‚Ä¢) to generate the motion sequence X ‚àà R C ‚Ä≤ in √óT√óN (i.e., C ‚Ä≤ in = 8, including three velocity components, overall velocity, three acceleration components, and overall acceleration for each joint). Subsequently, another set of four stacked CGT blocks is employed to capture the high-order motion feature f m ‚àà R Cm√óTm√óN . Additionally, a Bidirectional Cross-Stream Fusion (BCSF) module is introduced after two CGT blocks to align posture and motion features, thereby facilitating the exchange of complementary information between the two streams. Finally, the test score is obtained by averaging the scores from both streams. Furthermore, the posture stream is enhanced with 31 affective features  [57]  (i.e., 14 angle features, 9 distance features, and 8 area features) to more effectively capture emotional cues from the gait sequence. Additional details are provided in the following sections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Collaborative Graph And Transformer",
      "text": "Motivation. As discussed in Section 1, long-range dependencies between gait frames encode distinctive biological information, which is essential for accurate emotion recognition. However, existing methods primarily rely on TCN to capture local temporal details, neglecting longrange temporal dependencies. To address this limitation, we propose the Collaborative Graph and Transformer (CGT) block for comprehensive global spatiotemporal feature extraction.\n\nOperation. The details of the CGT block are illustrated in Figure  2(b) . By leveraging the interaction between GCN and Transformer, the CGT block effectively captures framelevel spatial topology and global temporal dependencies, thereby obtaining a more comprehensive and discriminative spatiotemporal representation. Specifically, our CGT block can be formulated as follows:\n\nwhere G(‚Ä¢) and T (‚Ä¢) denote graph modeling and transformer modeling, respectively, and Conv(‚Ä¢) represents the residual connection. Finally, we incorporate an FR Head  [62]  and a contrastive loss L CL to enhance the discriminative capability and robustness of the output feature f out by minimizing the distance between confident and ambiguous samples.\n\nGraph Modeling: Given an input feature f in ‚àà R Ci√óTi√óN , we first compute the self-attention adjacency matrix C k ‚àà R N√óN to establish global joint connections for each sequence, formulated as:\n\nwhere Conv Œ∏ (‚Ä¢) and Conv œÜ (‚Ä¢) denote two CNNs with a kernel size of 1 √ó 1. Softmax(‚Ä¢) and the symbol of ‚äó represent the softmax function and matrix multiplication, respectively. Subsequently, the output feature f outg ‚àà R Co√óTi√óN of graph modeling process is formulated as follows:\n\nHere, A k ‚àà R N√óN is a predefined adjacency matrix that represents the human physical structure based on prior knowledge, while B k ‚àà R N√óN is a learnable adjacency matrix, initialized as zero and updated in an end-to-end manner. K v denotes the number of graph subsets. Through graph modeling, we obtain frame-level global spatial topology information.\n\nTransformer Modeling: The transformer modeling is designed to capture global relationships among different frames, consisting of a multi-head self-attention (MHSA) mechanism, a feedforward network (FFN), and a temporal downsample CNN. Specifically, the feature f outg is first linearly transformed into the query Q ‚àà R N√óTi√óCt , key K ‚àà R N√óTi√óCt , and value V ‚àà R N√óTi√óCt as follows:\n\nwhere W Q , W K , and W V ‚àà R Co√óCt are learnable weight matrices. The MHSA mechanism then splits the Q, K, and V into h heads and performs self-attention in parallel across these heads. The output from all heads is subsequently concatenated and linearly projected to form the final output. Consequently, the complete transformer modeling is formulated as:\n\nHere, h is the number of heads in MHSA, and LN(‚Ä¢) denotes the LayerNorm function. Finally, we apply a pointwise CNN to map f trans . To reduce information redundancy and computational complexity, the stride of the pointwise convolution in the last two CGT blocks is set to 2 √ó 1.\n\nThrough transformer modeling, we extract global temporal dependencies.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Global Average Pooling",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "P",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Elementwise Sum",
      "text": "ùùà Sigmoid T Transpose Element-wise Product\n\nThe network structure of our proposed BCSF module. BCSF consists of two main components: the temporal crosstransformer and dynamic spatial attention. The temporal crosstransformer promotes context awareness across frames, while the dynamic spatial attention highlights key spatial topology information.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Bidirectional Cross-Stream Fusion",
      "text": "Motivation. As a video-based recognition task, existing studies fail to fully utilize the temporal information across different streams, leading to a loss of discriminative features. Motivated by the success of Transformers in multimodal learning  [38] , we present the Bidirectional Cross-Stream Fusion (BCSF) module to enhance information exchange between two streams, with a particular focus on context awareness across frames for more comprehensive emotion recognition.\n\nOperation. As shown in Figure  3 , the BCSF module comprises two temporal cross-transformers and two dynamic spatial attentions, effectively bridging spatiotemporal gaps and facilitating cross-stream information exchange. The BCSF process can be formulated as:\n\nwhere T SF p and T SF m denote the spatiotemporal fusion of the posture and motion streams, respectively. This design allows the network to attend to human posture while effectively capturing motion dynamics, thereby enhancing emotion recognition performance.\n\nGiven the symmetry of dual-stream fusion, we introduce the detailed structure using T SF p as an example.\n\nTemporal Fusion: Similar to Equation  4 , we first linearly project the two input features f pi and f mi ‚àà R C ‚Ä≤ i √óT ‚Ä≤ i √óN into three matrices: the posture stream's query Q p , the motion stream's key K m , and the motion stream's value V m . A multi-head cross-attention (MHCA) is then applied to facilitate dual-stream feature learning and fusion, as follows:\n\nwhere C k is the channel number of K m . Next, a feedforward network (FFN) is employed to further integrate the extracted cross-stream features. To enhance learning while preserving the original stream features, residual connections are incorporated. This process can be formulated as follows:\n\nHere,\n\ni √óN is the output of temporal fusion. Similarly, applying Equation  7 and Equation 8 to the motion stream yields its temporal fusion feature\n\ni √óN . This temporal fusion strategy effectively aligns the temporal features of the posture and motion streams, enabling the extraction of complementary cross-stream features.\n\nSpatial Fusion: First, the feature f ct m undergoes global average pooling (GAP) along the channel and temporal dimensions to extract the spatial joint topology feature across the entire sequence. Then, a multilayer perceptron (MLP) with a sigmoid activation function is applied to generate the spatial joint attention SAttn m ‚àà R N , which is formulated as:\n\nwhere œÉ(‚Ä¢) and P(‚Ä¢) denote the sigmoid function and GAP operation, respectively. The values in SAttn m represent the relative importance of joints in the motion stream. Finally, the weighted feature based on SAttn m and f ct m is added to f ct p to produce the final spatial fusion feature\n\ni √óN , which is formulated as:\n\nSimilarly, by applying Equation  9 and Equation  10 , we obtain the another output\n\ni √óN . This spatial fusion manner focuses on the key spatial joint information for emotion recognition, promoting the interaction between dual-stream spatial features.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Training Objective",
      "text": "As shown in Figure  2 , three types of loss functions are employed to train our network CGTGait: classification loss, affective distillation loss, and FR contrastive loss.\n\nClassification Loss: For both the posture and motion streams, we use cross-entropy loss for classification supervision, formulated as:\n\nwhere y is the ground truth. p p and p m denote the probability scores of the posture and motion streams, respectively. Affective Distillation Loss: As mentioned in Section 3.1, to enrich the posture feature with affective information and improve discriminative capability, we apply the affective distillation loss, formulated as:\n\nwhere y a is the ground truth for the affective feature b af f . FR Contrastive Loss: As mentioned in Section 3.2, we apply contrastive loss within each CGT block to refine the feature. The total FR contrastive loss is formulated as:\n\nwhere L pi CL and L mi CL denote the contrastive loss in the i-th CGT block of the posture and motion streams, respectively. Œª i is the balancing coefficient.\n\nThus, the overall learning objective can be formulated as:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first introduce the used datasets in Section 4.1. Then, we describe the implementation details in Section 4.2. Next, we evaluate the effectiveness of our proposed CGTGait through performance experiments conducted on two datasets, presented in Section 4.3 and Section 4.4, respectively. Finally, we perform ablation studies to verify the positive impact of each component in CGT-Gait, detailed in Section 4.5.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "Emotion-Gait. The Emotion-Gait  [1]  is a widely used dataset containing 2,177 gait samples across four emotion classes: happy, sad, angry, and neutral. Of these, 1,835 samples, each consisting of 240 frames, are sourced from the Edinburgh Locomotion MOCAP Database  [16] . The remaining 342 samples, with varying frame lengths (ranging from 27 to 75), are collected by Bhattacharya et al.  [1] . In all our experiments, we divide the Emotion-Gait dataset into training and test sets using a 9:1 ratio.\n\nELMD. The ELMD  [2]  consists of 1,835 gait sequences, each containing 240 frames, with four emotion labels (i.e., happy, sad, angry, and neutral) annotated by multiple participants. As described above, ELMD is divided into training and test sets following a 9:1 ratio. ACCV'20 81.50 Proxemo  [34]  IROS'20 82.40 STEP  [1]  AAAI'20 83.15 TAEW  [2]  ECCV'20 83.20 TNTC  [18]  ICASSP  '22 85 .97 STA-GCN  [4]  ICME'23 86.20 CAGE  [32]  AAAI'23 79.59 TT-GCN  [59]  TCSS'24 80.11 AP-Gait  [54]  MTA'24 85.20 AST-GCN  [5]  TCSVT  '24 88 .04 ST-Gait++  [29]  CVPRW'24 87.50 EPIC  [33]  JBHI'24 89.66 MAHANN  [58]  Physic A'24 90.20 BPM-GCN  [57]  TAFFC'24 90.37 MS-GCN  [68]  CMC'25 90.00 TS-ST  [25]  Sensors  '",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "Hyper-parameters. The input skeleton sequence size C in √ó T √ó N is uniformly set to 3 √ó 48 √ó 16 on two datasets. Following the protocol in  [43] , we apply random rotation and translation as data augmentation strategies during training. The input-output channels for the four CGT blocks in both the posture and motion streams are set to  (3, 64) ,  (64, 64) ,  (64, 128)  and (128, 256), respectively. The number of heads, h in Equation 5 and Equation  7 , is set to 8. The balancing coefficients [Œª 1 , Œª 2 , Œª 3 , Œª 4 ] in Equation 13 are defined as [0.1, 0.2, 0.5, 1.0]. Training details. All experiments are conducted on an NVIDIA GeForce RTX 3090 GPU using the Pytorch  [37]  framework. SGD with a momentum of 0.9 is employed as the optimizer. The initial learning rate is set to 0.01 and decays by a factor of 0.1 every 30 epochs. Our method is trained for 80 epochs with a batch size of 32.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance Comparison",
      "text": "As shown in Table  1  and Table  2 , we compare the performance and testing computational complexity of our proposed CGTGait with existing methods on the Emotion-Gait  [1]  and ELMD  [2]  datasets. Our comparison leads to the following key findings: 1) CGTGait achieves stateof-the-art or highly competitive performance compared to existing methods across two datasets, demonstrating its effectiveness. 2) Compared to STEP  [1]  with the minimum computational complexity, our method shows a performance improvement of 7.22% and 8.84% on Emotion-Gait and ELMD, respectively, with a minimal increase in computational complexity of only 0.03G FLOPs (0.34G vs. 0.31G), which is acceptable. 3) When compared to the previous best method, BMP-GCN  [57] , CGTGait demonstrates a significant reduction in computational complexity. Specifically, it requires only 0.34G FLOPs (approximately an 82.2% decrease), while improving accuracy by 1.66% on the ELMD dataset, highlighting its superior efficiency and performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization",
      "text": "In Figure  4 , we present confusion matrices comparing our method with the baseline model, which removes BCSF from CGTGait and replaces the Transformer with a standard TCN (kernel size 9 √ó 1). The visualizations indicate that CGTGait significantly improves the recognition accuracy across all emotional categories on both datasets. Furthermore, the macro F1 score increases substantially by 0.080 (from 0.766 to 0.846) and 0.095 (from 0.740 to 0.835), respectively. Notably, on Emotion-Gait, CGTGait outperforms the baseline by  24   emotions. These results highlight the effectiveness of our method in addressing the challenge of uneven data distribution (e.g., only 15.3% of happy emotions and 9.1% of sad emotions on Emotion-Gait).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Studies",
      "text": "Effectiveness of Proposed Components in CGTGait. As shown in Table  3 , we evaluate the effectiveness of the proposed components in CGTGait. The baseline model is the same as in Section 4.4. Our findings reveal that: 1) Both CGT and BCSF contribute to performance improvements (+0.88% and +1.11% for CGT, and +2.75% and 2.21% for BCSF), demonstrating their effectiveness. 2) Compared to CGT, BCSF shows a more substantial improvement (+0.88% vs. +2.75%, and +1.11% vs. +2.21%), highlighting the greater significance of feature interaction between the two streams in enhancing emotion recognition performance. 3) The combination of CGT and BCSF provides a more comprehensive spatiotemporal feature representation, leading to the best overall performance and underscoring their complementarity.\n\nImpact of Spatiotemporal Modeling Strategy in CGT. In Table  4 , we examine the effect of different spatiotemporal modeling strategies in CGT on emotion recognition. The results demonstrate that the serial connection of GCN and Transformer, particularly in the GCN-Transformer configuration, outperforms the parallel strategy. Specifically, performance improves from 86.70% to 90.37% on Emotion-Gait and from 90.61% to 92.27% on ELMD. These findings suggest that the serial connection is more effective in capturing rich and discriminative features, resulting in better   emotion recognition performance.\n\nEffectiveness of Spatiotemporal Fusion in BCSF. As shown in Table  5 , we investigate the effectiveness of spatiotemporal fusion in BCSF. In this case, the baseline model is CGTGait without BCSF. We find that both spatial and temporal fusions generally improve recognition performance, and their combination yields the best results, highlighting their complementary nature. Additionally, temporal fusion proves more effective than spatial fusion (+3.67% vs. +0.92%, and +1.65% vs. +0.55%), which also verifies the importance of temporal modeling in gait emotion recognition tasks.\n\nImpact of BCSF Position. As shown in Table  6 , we evaluate the impact of BCSF at different positions. The results demonstrate that the best performance is achieved when BCSF is applied after the second CGT block. We analyze the reasons as follows: 1) In early stages, the ability to extract semantic information from the different streams is insufficient, meaning that premature interaction cannot effectively integrate more discriminative semantic information. 2) At a later stage, each stream may lose its distinct emotional information, leading to ineffective integration of meaningful information during subsequent interactions.  Impact of the Number of CGT Blocks. In Table  7 , we investigate the influences of the number of CGT Blocks. Our findings indicate that an excessive number of CGT blocks can lead to network overfitting and performance degradation. Consequently, we select a stack of four CGT blocks to achieve optimal performance.\n\nImpact of the Number of Heads h. The influence of the number of heads (h in Equation 5 and Equation  7 ) is listed in Table  8 . We find that gradually increasing h can effectively improve performance, indicating that more heads enrich the temporal modeling's capacity. However, an excessive number of heads can lead to performance degradation. We set h to 4 to achieve optimal performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a novel approach, CGTGait, which leverages collaborative graphs and transformers for gait emotion recognition. Specifically, we propose CGT to effectively capture both spatial topology and global temporal information. In addition, we present BCSF, a module designed to integrate posture and motion stream features via a temporal cross-transformer and dynamic spatial attention. Extensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art or at least competitive performance while significantly reducing computational complexity.\n\nLimitation. In this paper, we integrate graphs with transformers through a straightforward serial connection strategy, without delving into the potential interactions between the two components. Therefore, a more effective integration strategy enables richer spatiotemporal feature learning. We leave the exploration of such strategies for future work.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Performance vs. Computational Complexity.",
      "page": 1
    },
    {
      "caption": "Figure 2: (a) Overview of our CGTGait. CGTGait is a dual-stream architecture, comprising posture and motion streams. Each stream",
      "page": 3
    },
    {
      "caption": "Figure 2: , our proposed CGTGait",
      "page": 3
    },
    {
      "caption": "Figure 2: (b). By leveraging the interaction between GCN",
      "page": 4
    },
    {
      "caption": "Figure 3: The network structure of our proposed BCSF mod-",
      "page": 5
    },
    {
      "caption": "Figure 3: , the BCSF mod-",
      "page": 5
    },
    {
      "caption": "Figure 2: , three types of loss functions are",
      "page": 5
    },
    {
      "caption": "Figure 4: Visualization of confusion matrices comparing the baseline model and CGTGait on the Emotion-Gait and ELMD datasets.",
      "page": 7
    },
    {
      "caption": "Figure 4: , we present confusion matrices comparing",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: and Table 2, we compare the per-",
      "data": [
        {
          "\u0000\u001c\u0000\u0019\u0000\u0011\u0000\u001b\u0000\b\n\u0000\u001c\u0000\u0011\u0000\u001b\u0000\b": "",
          "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u001c\u0000\u0011\u0000\u001b\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u0019\u0000\b": "\u0000\u0015\u0000\u0011\u0000\u0017\u0000\b"
        },
        {
          "\u0000\u001c\u0000\u0019\u0000\u0011\u0000\u001b\u0000\b\n\u0000\u001c\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0016\u0000\u0011\u0000\u0013\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0014\u0000\u0015\u0000\u0011\u0000\u0014\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u0019\u0000\b": "\u0000\u0019\u0000\u0016\u0000\u0011\u0000\u0019\u0000\b"
        },
        {
          "\u0000\u001c\u0000\u0019\u0000\u0011\u0000\u001b\u0000\b\n\u0000\u001c\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0014\u0000\u0019\u0000\u0011\u0000\u001a\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0019\u0000\u0014\u0000\u0011\u0000\u0014\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u0019\u0000\b": "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: and Table 2, we compare the per-",
      "data": [
        {
          "\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b\n\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b\n\u0000\u001b\u0000\u0011\u0000\u0014\u0000\b\n\u0000\u001b\u0000\u0019\u0000\u0011\u0000\u0018\u0000\b\n\u0000\u0017\u0000\u0011\u0000\u0016\u0000\b\n\u0000\u0015\u0000\u0019\u0000\u0011\u0000\u0014\u0000\b": "",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u0018\u0000\u0011\u0000\u0017\u0000\b"
        },
        {
          "\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b\n\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b\n\u0000\u001b\u0000\u0011\u0000\u0014\u0000\b\n\u0000\u001b\u0000\u0019\u0000\u0011\u0000\u0018\u0000\b\n\u0000\u0017\u0000\u0011\u0000\u0016\u0000\b\n\u0000\u0015\u0000\u0019\u0000\u0011\u0000\u0014\u0000\b": "",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b"
        },
        {
          "\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b\n\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b\n\u0000\u001b\u0000\u0011\u0000\u0014\u0000\b\n\u0000\u001b\u0000\u0019\u0000\u0011\u0000\u0018\u0000\b\n\u0000\u0017\u0000\u0011\u0000\u0016\u0000\b\n\u0000\u0015\u0000\u0019\u0000\u0011\u0000\u0014\u0000\b": "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b\n\u0000\u0014\u0000\u0017\u0000\u0011\u0000\u0016\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0017\u0000\b"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: and Table 2, we compare the per-",
      "data": [
        {
          "\u0000\u001c\u0000\u001a\u0000\u0011\u0000\u0017\u0000\b\n\u0000\u0014\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "",
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\b": "\u0000\u001b\u0000\u0014\u0000\u0011\u0000\u0014\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0018\u0000\u0011\u0000\u0017\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u0015\u0000\u0011\u0000\u001a\u0000\b"
        },
        {
          "\u0000\u001c\u0000\u001a\u0000\u0011\u0000\u0017\u0000\b\n\u0000\u0014\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0014\u0000\u0016\u0000\u0011\u0000\u0013\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\b": "\u0000\u0014\u0000\u001a\u0000\u0011\u0000\u0017\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0019\u0000\u0018\u0000\u0011\u0000\u0015\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u0017\u0000\u0011\u0000\u0016\u0000\b"
        },
        {
          "\u0000\u001c\u0000\u001a\u0000\u0011\u0000\u0017\u0000\b\n\u0000\u0014\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\b": "\u0000\u0018\u0000\u001a\u0000\u0011\u0000\u0014\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u0017\u0000\u0015\u0000\u0011\u0000\u001c\u0000\b"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: and Table 2, we compare the per-",
      "data": [
        {
          "\u0000\u001c\u0000\u001a\u0000\u0011\u0000\u0019\u0000\b\n\u0000\u0017\u0000\u0011\u0000\u001c\u0000\b": "",
          "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u001b\u0000\u0013\u0000\u0011\u0000\u0018\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u0015\u0000\u0011\u0000\u0017\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u0019\u0000\b": "\u0000\u0014\u0000\u0015\u0000\u0011\u0000\u0015\u0000\b"
        },
        {
          "\u0000\u001c\u0000\u001a\u0000\u0011\u0000\u0019\u0000\b\n\u0000\u0017\u0000\u0011\u0000\u001c\u0000\b": "\u0000\u0016\u0000\u0011\u0000\u0013\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u001c\u0000\u0011\u0000\u0014\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u001b\u0000\u001a\u0000\u0011\u0000\u001c\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u0019\u0000\b": "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b"
        },
        {
          "\u0000\u001c\u0000\u001a\u0000\u0011\u0000\u0019\u0000\b\n\u0000\u0017\u0000\u0011\u0000\u001c\u0000\b": "\u0000\u0014\u0000\u0014\u0000\u0011\u0000\u0014\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\b": "\u0000\u0015\u0000\u0015\u0000\u0011\u0000\u0015\u0000\b",
          "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b": "\u0000\u0013\u0000\u0011\u0000\u0013\u0000\b",
          "\u0000\u0014\u0000\u0011\u0000\u0019\u0000\b": "\u0000\u0019\u0000\u0019\u0000\u0011\u0000\u001a\u0000\b"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "2",
      "title": "Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping",
      "authors": [
        "U Bhattacharya",
        "C Roncal",
        "T Mittal",
        "R Chandra",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "3",
      "title": "End-to-end object detection with transformers",
      "authors": [
        "N Carion",
        "F Massa",
        "G Synnaeve",
        "N Usunier",
        "A Kirillov",
        "S Zagoruyko"
      ],
      "year": "2020",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "4",
      "title": "Sta-gcn: Spatial temporal adaptive graph convolutional network for gait emotion recognition",
      "authors": [
        "C Chen",
        "X Sun"
      ],
      "year": "2006",
      "venue": "2023 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "5",
      "title": "Ast-gcn: Augmented spatial temporal graph convolutional neural network for gait emotion recognition",
      "authors": [
        "C Chen",
        "X Sun",
        "Z Tu",
        "M Wang"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "6",
      "title": "Crossvit: Crossattention multi-scale vision transformer for image classification",
      "authors": [
        "C.-F Chen",
        "Q Fan",
        "R Panda"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "7",
      "title": "Infogcn++: Learning representation by predicting the future for online skeleton-based action recognition",
      "authors": [
        "S Chi",
        "H -G. Chi",
        "Q Huang",
        "K Ramani"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition through gait on mobile devices",
      "authors": [
        "M Chiu",
        "J Shu",
        "P Hui"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)"
    },
    {
      "citation_id": "9",
      "title": "Body expression recognition from animated 3d skeleton",
      "authors": [
        "A Crenn",
        "R Khan",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2016",
      "venue": "2016 International Conference on 3D Imaging (IC3D)"
    },
    {
      "citation_id": "10",
      "title": "From emotions to mood disorders: A survey on gait analysis methodology",
      "authors": [
        "F Deligianni",
        "Y Guo",
        "G.-Z Yang"
      ],
      "year": "2019",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "11",
      "title": "Skateformer: skeletal-temporal transformer for human action recognition",
      "authors": [
        "J Do",
        "M Kim"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "13",
      "title": "Hierarchical aggregated graph neural network for skeleton-based action recognition",
      "authors": [
        "P Geng",
        "X Lu",
        "W Li",
        "L Lyu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "I2v-adapter: A general image-to-video adapter for diffusion models",
      "authors": [
        "X Guo",
        "M Zheng",
        "L Hou",
        "Y Gao",
        "Y Deng",
        "P Wan",
        "D Zhang",
        "Y Liu",
        "W Hu",
        "Z Zha"
      ],
      "year": "2024",
      "venue": "ACM SIGGRAPH 2024 Conference Papers"
    },
    {
      "citation_id": "15",
      "title": "Mg-gct: A motionguided graph convolutional transformer for traffic gesture recognition",
      "authors": [
        "X Guo",
        "Q Zhu",
        "Y Wang",
        "Y Mo"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "16",
      "title": "A recurrent variational autoencoder for human motion synthesis",
      "authors": [
        "I Habibie",
        "D Holden",
        "J Schwarz",
        "J Yearsley",
        "T Komura"
      ],
      "year": "2017",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "17",
      "title": "Not all is noticed: Kinematic cues of emotion-specific gait",
      "authors": [
        "S Halovic",
        "C Kroos"
      ],
      "year": "2018",
      "venue": "Human movement science"
    },
    {
      "citation_id": "18",
      "title": "Tntc: Two-stream network with transformer-based complementarity for gait-based emotion recognition",
      "authors": [
        "C Hu",
        "W Sheng",
        "B Dong",
        "X Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "19",
      "title": "Videobooth: Diffusion-based video generation with image prompts",
      "authors": [
        "Y Jiang",
        "T Wu",
        "S Yang",
        "C Si",
        "D Lin",
        "Y Qiao",
        "C Loy",
        "Z Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Recognition of affect based on gait patterns",
      "authors": [
        "M Karg",
        "K K√ºhnlenz",
        "M Buss"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "21",
      "title": "Crowd emotion prediction for human-vehicle interaction through modified transfer learning and fuzzy logic ranking",
      "authors": [
        "M Khosravi",
        "K Rezaee",
        "M Moghimi",
        "S Wan",
        "V Menon"
      ],
      "year": "2023",
      "venue": "IEEE transactions on intelligent transportation systems"
    },
    {
      "citation_id": "22",
      "title": "I-bert: Integer-only bert quantization",
      "authors": [
        "S Kim",
        "A Gholami",
        "Z Yao",
        "M Mahoney",
        "K Keutzer"
      ],
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "23",
      "title": "Cast: cross-attention in space and time for video action recognition",
      "authors": [
        "D Lee",
        "J Lee",
        "J Choi"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Identifying emotions from non-contact gaits information based on microsoft kinects",
      "authors": [
        "B Li",
        "C Zhu",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Gait-to-gait emotional human-robot interaction utilizing trajectories-aware and skeleton-graph-aware spatial-temporal transformer",
      "authors": [
        "C Li",
        "K Seng",
        "L.-M Ang"
      ],
      "year": "2025",
      "venue": "Sensors"
    },
    {
      "citation_id": "26",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "27",
      "title": "Monkey: Image resolution and text label are important things for large multi-modal models",
      "authors": [
        "Z Li",
        "B Yang",
        "Q Liu",
        "Z Ma",
        "S Zhang",
        "J Yang",
        "Y Sun",
        "Y Liu",
        "X Bai"
      ],
      "year": "2024",
      "venue": "Monkey: Image resolution and text label are important things for large multi-modal models"
    },
    {
      "citation_id": "28",
      "title": "Static and dynamic features analysis from human skeletons for gait recognition",
      "authors": [
        "Z Li",
        "S Yu",
        "E Reyes",
        "C Shan",
        "Y.-R Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "29",
      "title": "Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos",
      "authors": [
        "M Lima",
        "W De Lima Costa",
        "E Mart√≠nez",
        "V Teichrieb",
        "St-Gait++"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Improved baselines with visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Y Li",
        "Y Lee"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "32",
      "title": "See your emotion from gait using unlabeled skeleton data",
      "authors": [
        "H Lu",
        "X Hu",
        "B Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Epic: Emotion perception by spatio-temporal interaction context of gait",
      "authors": [
        "H Lu",
        "S Xu",
        "S Zhao",
        "X Hu",
        "R Ma",
        "B Hu"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "34",
      "title": "Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation",
      "authors": [
        "V Narayanan",
        "B Manoghar",
        "V Dorbala",
        "D Manocha",
        "A Bera"
      ],
      "year": "2020",
      "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "35",
      "title": "Extraction of spatio-temporal primitives of emotional body expressions",
      "authors": [
        "L Omlor",
        "M Giese"
      ],
      "year": "2007",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "36",
      "title": "Self-adaptive graph with nonlocal attention network for skeleton-based action recognition",
      "authors": [
        "C Pang",
        "X Gao",
        "Z Chen",
        "L Lyu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "37",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, high-performance deep learning library",
      "arxiv": "arXiv:1912.01703"
    },
    {
      "citation_id": "38",
      "title": "Scalable diffusion models with transformers",
      "authors": [
        "W Peebles",
        "S Xie"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "39",
      "title": "Spatial temporal transformer network for skeleton-based action recognition",
      "authors": [
        "C Plizzari",
        "M Cannici",
        "M Matteucci"
      ],
      "year": "2021",
      "venue": "Pattern recognition. ICPR international workshops and challenges: virtual event"
    },
    {
      "citation_id": "40",
      "title": "Llms are good action recognizers",
      "authors": [
        "H Qu",
        "Y Cai",
        "J Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "42",
      "title": "Learning perceived emotion using affective and deep features for mental health applications",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)"
    },
    {
      "citation_id": "43",
      "title": "Two-stream adaptive graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "L Shi",
        "Y Zhang",
        "J Cheng",
        "H Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "44",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "≈Å Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "45",
      "title": "Recognizing emotions conveyed by human gait",
      "authors": [
        "G Venture",
        "H Kadone",
        "T Zhang",
        "J Gr√®zes",
        "A Berthoz",
        "H Hicheur"
      ],
      "year": "2014",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "46",
      "title": "3mformer: Multi-order multi-mode transformer for skeletal action recognition",
      "authors": [
        "L Wang",
        "P Koniusz"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Efficient spatio-temporal video grounding with semantic-guided feature decomposition",
      "authors": [
        "W Wang",
        "J Liu",
        "Y Su",
        "W Nie"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "48",
      "title": "Videogrounding-dino: Towards open-vocabulary spatio-temporal video grounding",
      "authors": [
        "S Wasim",
        "M Naseer",
        "S Khan",
        "M.-H Yang",
        "F Khan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "Hicem: A high-coverage emotion model for artificial emotional intelligence",
      "authors": [
        "B Wortman",
        "J Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Macdiff: Unified skeleton modeling with masked conditional diffusion",
      "authors": [
        "L Wu",
        "L Lin",
        "J Zhang",
        "Y Ma",
        "J Liu"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "51",
      "title": "Frequency guidance matters: Skeletal action recognition by frequency-aware mixed transformer",
      "authors": [
        "W Wu",
        "C Zheng",
        "Z Yang",
        "C Chen",
        "S Das",
        "A Lu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "52",
      "title": "Emotion recognition from gait analyses: Current research and future directions",
      "authors": [
        "S Xu",
        "J Fang",
        "X Hu",
        "E Ngai",
        "W Wang",
        "Y Guo",
        "V Leung"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "53",
      "title": "Msa-gcn: Multiscale adaptive graph convolution network for gait emotion recognition",
      "authors": [
        "Y Yin",
        "L Jing",
        "F Huang",
        "G Yang",
        "Z Wang"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "54",
      "title": "Affective-pose gait: perceiving emotions from gaits with body pose and human affective prior knowledge. Multimedia Tools and Applications",
      "authors": [
        "Z Yumeng",
        "L Zhen",
        "L Tingting",
        "W Yuanyi",
        "C Yan-Jie"
      ],
      "year": "2024",
      "venue": "Affective-pose gait: perceiving emotions from gaits with body pose and human affective prior knowledge. Multimedia Tools and Applications"
    },
    {
      "citation_id": "55",
      "title": "Restormer: Efficient transformer for high-resolution image restoration",
      "authors": [
        "S Zamir",
        "A Arora",
        "S Khan",
        "M Hayat",
        "F Khan",
        "M.-H Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "56",
      "title": "Gaitcycformer: Leveraging gait cycles and transformers for gait emotion recognition",
      "authors": [
        "Q Zeng",
        "L Shang"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "57",
      "title": "Looking into gait for perceiving emotions via bilateral posture and movement graph convolutional networks",
      "authors": [
        "Y Zhai",
        "G Jia",
        "Y.-K Lai",
        "J Zhang",
        "J Yang",
        "D Tao"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Hierarchical-attention-based neural network for gait emotion recognition",
      "authors": [
        "S Zhang",
        "J Zhang",
        "W Song",
        "L Yang",
        "X Zhao"
      ],
      "year": "2024",
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "59",
      "title": "Tt-gcn: Temporal-tightly graph convolutional network for emotion recognition from gaits",
      "authors": [
        "T Zhang",
        "Y Chen",
        "S Li",
        "X Hu",
        "C Chen"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "60",
      "title": "Stst: Spatialtemporal specialized transformer for skeleton-based action recognition",
      "authors": [
        "Y Zhang",
        "B Wu",
        "W Li",
        "L Duan",
        "C Gan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "61",
      "title": "Fine-grained micro-expression generation based on thinplate spline and relative au constraint",
      "authors": [
        "S Zhao",
        "S Yin",
        "H Tang",
        "R Jin",
        "Y Xu",
        "T Xu",
        "E Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "62",
      "title": "Learning discriminative representations for skeleton based action recognition",
      "authors": [
        "H Zhou",
        "Q Liu",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "Fourmer: An efficient global modeling paradigm for image restoration",
      "authors": [
        "M Zhou",
        "J Huang",
        "C.-L Guo",
        "C Li"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "64",
      "title": "Hypergraph transformer for skeleton-based action recognition",
      "authors": [
        "Y Zhou",
        "Z.-Q Cheng",
        "C Li",
        "Y Fang",
        "Y Geng",
        "X Xie",
        "M Keuper"
      ],
      "year": "2022",
      "venue": "Hypergraph transformer for skeleton-based action recognition",
      "arxiv": "arXiv:2211.09590"
    },
    {
      "citation_id": "65",
      "title": "Mlstformer: Multi-level spatial-temporal transformer for group activity recognition",
      "authors": [
        "X Zhu",
        "Y Zhou",
        "D Wang",
        "W Ouyang",
        "R Su"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "66",
      "title": "Temporal group attention network with affective complementary learning for gait emotion recognition",
      "authors": [
        "Z Zhu",
        "C Chen",
        "H Liu",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "67",
      "title": "G-gcsn: Global graph convolution shrinkage network for emotion perception from gait",
      "authors": [
        "Y Zhuang",
        "L Lin",
        "R Tong",
        "J Liu",
        "Y Iwamot",
        "Y.-W Chen"
      ],
      "year": "2006",
      "venue": "Proceedings of the Asian Conference on Computer Vision"
    },
    {
      "citation_id": "68",
      "title": "Occluded gait emotion recognition based on multi-scale suppression graph convolutional network",
      "authors": [
        "Y Zou",
        "N He",
        "J Sun",
        "X Huang",
        "W Wang"
      ],
      "venue": "Computers, Materials & Continua"
    }
  ]
}