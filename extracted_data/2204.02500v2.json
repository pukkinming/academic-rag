{
  "paper_id": "2204.02500v2",
  "title": "User-Level Differential Privacy Against Attribute Inference Attack Of Speech Emotion Recognition In Federated Learning",
  "published": "2022-04-05T21:35:30Z",
  "authors": [
    "Tiantian Feng",
    "Raghuveer Peri",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Differential Privacy",
    "Federated Learning",
    "Privacy Leakage"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Many existing privacy-enhanced speech emotion recognition (SER) frameworks focus on perturbing the original speech data through adversarial training within a centralized machine learning setup. However, this privacy protection scheme can fail since the adversary can still access the perturbed data. In recent years, distributed learning algorithms, especially federated learning (FL), have gained popularity to protect privacy in machine learning applications. While FL provides good intuition to safeguard privacy by keeping the data on local devices, prior work has shown that privacy attacks, such as attribute inference attacks, are achievable for SER systems trained using FL. In this work, we propose to evaluate the user-level differential privacy (UDP) in mitigating the privacy leaks of the SER system in FL. UDP provides theoretical privacy guarantees with privacy parameters and δ. Our results show that the UDP can effectively decrease attribute information leakage while keeping the utility of the SER system with the adversary accessing one model update. However, the efficacy of the UDP suffers when the FL system leaks more model updates to the adversary. We make the code publicly available to reproduce the results in https://github.com/usc-sail/fed-ser-leakage.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) has found increasing applications in virtual assistants  [1] , health  [2, 3] , education  [4]  and other emerging human-centered AI applications. SER is prone to privacy leakage issues like other speech technologies because the collected speech data can reveal sensitive information about an individual, including intent, demographic/personality traits, and health states. Federated Learning (FL) methods attempt to address the issues of data privacy by training a model on a central server using the shared model parameters from an edge device without the need for local data  [5] . However, as reported in our prior work, SER applications trained in an FL setup are still vulnerable to attribute inference attacks  [6] . In particular, we found that an adversary with access to local parameter updates can successfully infer the gender of the user (deemed as sensitive in that particular SER use case) operating the edge device. In this work, we propose to apply a recently developed user-level differential privacy (UDP) framework  [7]  to mitigate attribute information leakage in FL-based SER systems.\n\nIn FL algorithms, each edge device trains a local model using its own data, and the central server then aggregates the shared local model parameters. Such a training scheme ensures that local data is not shared with the central server, potentially mitigating privacy leakage. However, recent works have shown 1 This paper was submitted to Insterspeech 2022 for review. that adversaries may still perform privacy attacks, such as membership inference attacks  [8]  and reconstruction attacks  [9, 10] , by using the model parameters shared with the central server. For instance, many works have demonstrated that data reconstruction is achievable through analyzing the model updates in FL setup  [8, 9, 10] . We had previously demonstrated this phenomenon in FL-based SER setup  [6] . Specifically, we showed that an attribute inference attacker could successfully infer a user's gender attribute by using the model updates shared in the FL setup  [6] . A typical approach to protect privacy in FL is differential privacy (DP)  [11, 12] , of which local DP (LDP) is a prominent example  [7] . For instance, user-level DP, a particular LDP approach, provides privacy protections to FL by perturbing each client's shared model before uploading it to the central server. In UDP, the training process of each client satisfies the requirement of ( , δ)-LDP for different privacy levels by adapting Gaussian noise with appropriate variances.\n\nIn this work, we perform an extensive exploration of this framework within the context of FL-based SER. In particular, we investigate the effect of the level of perturbation on privacy leakage and the utility of the trained SER model. In addition, we enhance the capability of the privacy attacker by providing access to multiple model updates for each client in the FL training setup. Our experiments show that when the adversary has only access to a single model update from a client, the UDP can effectively decrease attribute information leakage (thereby mitigating privacy leakage) while retaining the utility of the SER model. However, the efficacy of this mitigation strategy drops substantially when the attacker can observe multiple model updates from the FL process.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "In this section, we first review the attacking framework we proposed in  [6] . We then summarise the proposed UDP algorithm used in this work. To facilitate readability, we summarize the notations adopted in this paper in Table  1 . Fig.  1  shows the attack problem setup we apply in this work. Specifically, the primary task is SER, models for which are trained using the FL framework. In contrast, in the adversarial task the attacker attempts to predict the client's gender label (deemed sensitive in this exemplary scenario). We follow a setup in which we have a private-labeled data set D p from a number of clients, where each client has a feature set X and an emotion label set y. Each client is also associated with a gender label z. In this attack, the adversary tries to infer the sensitive attribute z k of the k th client using global model θ t and its local model θ t+1 k .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Attack Framework",
      "text": "We use an attack framework similar to membership inference attack  [13] . Below is a summary of the attack framework, and a more detailed description can be found in  [6] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Arxiv:2204.02500V2 [Cs.Cr] 17 May 2022",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Attack Model Training:",
      "text": "In this paper, the attacker model takes g t k as the input to infer z k of the k th client. Suppose ∇W i and ∇b i are the weight updates and the bias updates in g corresponding to the i th layer in SER training, respectively. Each layer's weight update is first fed into a three-layer CNN feature extractor to compute the hidden representation. We then flatten the output from the CNN module and concatenate it with the layer's bias updates. We then pass this combined representation to a multi-layer perceptron (MLP) classifier to predict gender. In this work, we focus on using the ∇W1 and ∇b1 based on our observation that most information leakage in this application comes from the first layer's training updates  [6] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "User-Level Differential Privacy",
      "text": "The idea of LDP is to perturb the local data using mechanism M such that the data perturbation is guaranteed to protect from inference attacks given parameters and δ. Here, > 0 sets the bound of all outputs on neighboring data sets D and D , which differ by one sample, in a database. δ ∈ [0, 1) indicates the probability that the ratio of the probabilities for two adjacent data sets D and D cannot be bounded by . Given a fixed δ, a lower represents stronger privacy protection  [15] . More formally, LDP can be defined as follows:\n\nIn this paper, we follow the work in  [7]  and select Gaussian mechanism using L2 norm sensitivity as M. In this setup, we perturb an output s(x) by adding Gaussian noise with zeromean and variance σ 2 I for a given s(•) as shown below:\n\nIn the FL setup, the model update function (D p , θ) becomes a natural choice for the sample function in the LDP. Formally, the sensitivity is defined as the upper bound for the noise perturbation given by σ that satisfies ( , δ)-LDP.\n\nMore specifically, the norm clipping technique in deep learning is frequently used to bound the sensitivity function above  [16] . Given the norm clipping threshold C, we can bound the sensitivity as ∇ ≤ 2ηC |D p k | . Furthermore, given total training epoch T , the number of clients participating in a global epoch K, the client sample ratio q = K U , k , and fixed δ k , the following inequality can be derived as shown in  [16]  and  [7] :\n\nThus, we can determine σ k of the Gaussian noise that satisfies ( k , δ k )-LDP for the k th client using the equation below:\n\nSo unlike the normal FL process, where the local client directly uploads the updated model parameters for aggregation, the UDP framework locally adds Gaussian noise with zero mean and variance σ k to θ t+1 k before sending it to the central server. Algorithm 1 shows the federated learning with UDP. Additionally, for a given k , a larger T in the entire training process leads to lower privacy guarantees because the adversary may access more observations of model updates  [7] . This decrease in privacy protection can be related to the composition property associated with DP derived in  [17, 15] :\n\nAlgorithm 1 User-level DP (UDP) 1: Initialize: θ 0 , c 0 , q, T, C, LDP parameters ( i, δi) for every client 2: for Each round t = 0, ..., T -1 do for Each client k ∈ S in parallel do 5: Therefore, we hypothesize that the attack performance increases with more model updates leaked. Finally, we test the attack performance by varying the number of leaked observations, n, of a client to empirically validate this behavior.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ser Data Sets",
      "text": "In this work, we use three corpora widely used in SER, including in our previous attacker work, to evaluate the DP performance. Readers can reference the label distribution of the data set in  [6] . 1. The IEMOCAP database  [18]  contains audio and visual data of acted human interactions with categorical emotions. The corpus has five recorded sessions from ten subjects (five male and five female) in scripted and improvised conditions. Speakers follow a fixed script in the scripted condition and perform spontaneous interactions in the improvised condition. Similar to  [19]  and our previous work  [6] , we only use the data from the improvised condition. We decided to use the four most frequently occurring emotion labels (neutral, sad, happiness, and anger) for training the SER model as suggested in  [19] . 2. The CREMA-D  [20]  corpus has 7,442 speech recordings that simulate different emotional expressions. The whole database is collected from 91 actors (48 male and 43 female).  [21]  corpus consists of human interactions with naturalistic emotions captured from improvised scenarios. The whole data set is from 12 participants (six male and six female). Like the IEMOCAP data set, we only select data recorded in the improvised condition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Msp-Improv",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Preprocessing",
      "text": "We follow the data preprocessing from our previous work  [6] , where we extract the EmoBase feature set and the autoregressive predictive coding (APC)  [22]  feature set of each utterance using the OpenSMILE toolkit  [23]  and SUPERB (Speech Processing Universal PERformance Benchmark)  [24] , respectively. We present results on one knowledge-based feature set (EmoBase) and one deep-learning-based feature set (APC). Due to space constraints in the paper, we present the results using other deep-learning-based speech features in our GitHub repository mentioned earlier. We apply z-normalization to the speech features of each speaker. For the IEMOCAP and the MSP-Improv data set, we divide each speaker's data into 10 shards of equal size to create more clients for the FL training. We leave 20% of speakers as the test data and repeat the experiments five times with test folds of different speakers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Setup",
      "text": "Similar to  [6] , we simulate the experiments using different private training data sets. For instance, when the IEMOCAP data set is the private training data set D p , the MSP-Improv data set and CREMA-D data set are combined to train shadow models Ms 1 , ..., Ms m . Next, we train the attack model Ma using the model updates generated while training Ms 1 , ..., Ms m . Finally, we evaluate the performance of Ma using the model updates generated in the FL that uses IEMOCAP data set as D p . Similarly, we repeat the same experiments with the MSP-Improv data set and the CREMA-D data set as D p .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model And Evaluation Details",
      "text": "We use an MLP for the SER model architecture. The model consists of 2 dense layers with hidden layer dimensions of {256, 128}. We choose ReLU as the activation function and the dropout rate as 0.2. We implement the FedAvg algorithm in training the SER model. Only q = 10% of the clients participate in each global round. 80% of the data at a client is reserved for local training, and the remaining 20% is used for validation. We set the local training batch size as 20, the η as 0.0005, the local training epoch as 1, and the T as 200. We set the norm clipping threshold C = 0.25 and δ k = 0.5 for every client. We evaluate the attacker performance under several privacy budget values k ∈  [5, 10, 25, 50] . We use the pre-trained attacker model from our previous work, and details of the attacker model training are in  [6] . We randomly pick a client's n model updates (generated in FL) and predict its gender label using the aggregated model updates. As we mentioned in section 2.1, we only use the model updates from the first layer as the input for the inference task. We repeat this ten times for each client and aggregate predictions from all clients to report the final results. We empirically test n ∈  [1, 5, 10, all] , where all refers to the scenario where all the updates available from a client are available to the attacker.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ser Performance",
      "text": "The SER results of UDP-based FL at different privacy levels are shown in Table  2 . = ∞ indicates the case of FL without adding UDP. In this work, we report the unweighted average recall (UAR) scores of the SER predictions over the emotion classes. Overall, the SER model performs best in the CREMA-D data set. Across the different datasets and feature sets, we observe that the SER performance decreases by about 1-2% when applying UDP with = 50 and = 25. Moreover, the UAR decreases by around 3-4% when reduces to 10. Finally, the SER performance drops significantly when = 5 in the UDP. These observations comply with the expected output of UDP, where a relatively larger is associated with smaller noises added to the model parameters and thus does not substantially impact the performance of the primary application. To quantify the amount of noise added to the weight parameters, we calculate the weight parameters' signal-to-noise ratio (SNR) at different privacy levels. We find that the SNR is in the range of 14.11 dB to 20.65 dB when = 25, which suggests that the SER model performance  decreases substantially when the energy of the shared weight parameters is less than 25 times the energy of the noise.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Attacker Performance (N = 1)",
      "text": "The attacker results of FL with UDP at different are shown in Table  2 . Similar to the SER performance, we evaluate the attacker using the UAR scores of gender predictions. The table shows that the pre-trained attack model can predict gender with a UAR above 75% in all conditions when no perturbation is added ( = ∞). However, we find that the gender predictions from the attacker model drop intensely even when applying the UDP using = 50 (small perturbation σ k ). As we reduce from 50 to 25 (hence gradually increasing the perturbation), the gender prediction results drop close to the random guess.\n\nThese results indicate that UDP can effectively mitigate the attribute inference attack without sacrificing much utility of the SER model when the attacker has only access to a single model update from a client.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Attacker Performance (N > 1)",
      "text": "Fig.  2  shows the results of the attack performance on FL with access to a varying number of model updates, n and at different privacy levels, for the UDP algorithm. The results show that the attack performance in gender prediction improves by an enormous margin with more model updates of a client leaked to the adversary when = 50 and = 25. For example, the UAR of the gender prediction is 82.3% when the adversary has access to all model updates of a client and = 50 in UDP, which shows that the attacker can infer the gender to a reasonable extent with access to all model updates from a client. However, at = 10 and = 5, the attack performance does not increase much, even with more access to model updates. These results suggest that when the attacker can observe multiple model updates from the UDP-based FL process, the attribute inference attack is achievable with some degradation in the SER performance by applying a small in UDP.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "We evaluated the attribute inference attack of the SER task within FL settings with a user-level DP algorithm. The UDP algorithm used in this paper satisfies the requirement of LDP with privacy parameters and δ. We discover that the UDP algorithm can effectively mitigate the attribute inference attack when the adversary can only access one model update from a client. This defense provides promising results even with a relatively larger at 50 (weaker privacy guarantee). However, as the number of leaked model updates increases, the adversary can infer the gender label with an adequate UAR when are 50 and 25. Since the current adversary trains the attack model using the model updates generated from only two public SER data sets, the attacker can potentially improve the performance of the attack model by including more public SER data sets. Consequently, this may make UDP less effective against the current attribute inference attack framework. Therefore, in future works, we aim to explore adversarial training, which targets to protect specific attributes in the defense.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the",
      "page": 1
    },
    {
      "caption": "Figure 1: The ﬁgure shows the problem setup of the attribute",
      "page": 2
    },
    {
      "caption": "Figure 2: The ﬁgure shows the prediction results of the attribute inference task at different privacy levels (ϵ) and different number of",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the results of the attack performance on FL with",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Fig. 1 shows the",
      "page": 1
    },
    {
      "caption": "Table 1: Notation used in this paper.",
      "page": 2
    },
    {
      "caption": "Table 2: ϵ = ∞indicates the case of FL without",
      "page": 3
    },
    {
      "caption": "Table 2: Prediction results of the SER model and the pre-trained attacker model on private data set Dp. The % unweighted average",
      "page": 4
    },
    {
      "caption": "Table 2: Similar to the SER performance, we evaluate the",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Study on emotion recognition and companion chatbot using deep neural network",
      "authors": [
        "M.-C Lee",
        "S.-Y Chiang",
        "S.-C Yeh",
        "T.-F Wen"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "I Emary"
      ],
      "year": "2013",
      "venue": "Telecommunication Systems"
    },
    {
      "citation_id": "4",
      "title": "Signal processing and machine learning for mental health research and clinical applications",
      "authors": [
        "D Bone",
        "C.-C Lee",
        "T Chaspari",
        "J Gibson",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition in elearning system based on affective computing",
      "authors": [
        "W Li",
        "Y Zhang",
        "Y Fu"
      ],
      "year": "2007",
      "venue": "Third International Conference on Natural Computation (ICNC 2007)"
    },
    {
      "citation_id": "6",
      "title": "Communication-efficient learning of deep networks from decentralized data",
      "authors": [
        "B Mcmahan",
        "E Moore",
        "D Ramage",
        "S Hampson",
        "B Arcas"
      ],
      "year": "2017",
      "venue": "Artificial intelligence and statistics"
    },
    {
      "citation_id": "7",
      "title": "Attribute inference attack of speech emotion recognition in federated learning settings",
      "authors": [
        "T Feng",
        "H Hashemi",
        "R Hebbar",
        "M Annavaram",
        "S Narayanan"
      ],
      "year": "2021",
      "venue": "Attribute inference attack of speech emotion recognition in federated learning settings",
      "arxiv": "arXiv:2112.13416"
    },
    {
      "citation_id": "8",
      "title": "User-level privacy-preserving federated learning: Analysis and performance optimization",
      "authors": [
        "K Wei",
        "J Li",
        "M Ding",
        "C Ma",
        "H Su",
        "B Zhang",
        "H Poor"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Mobile Computing"
    },
    {
      "citation_id": "9",
      "title": "Exploiting unintended feature leakage in collaborative learning",
      "authors": [
        "L Melis",
        "C Song",
        "E Cristofaro",
        "V Shmatikov"
      ],
      "year": "2019",
      "venue": "2019 IEEE Symposium on Security and Privacy (SP)"
    },
    {
      "citation_id": "10",
      "title": "Deep leakage from gradients",
      "authors": [
        "L Zhu",
        "S Han"
      ],
      "year": "2020",
      "venue": "Deep leakage from gradients"
    },
    {
      "citation_id": "11",
      "title": "Towards general deep leakage in federated learning",
      "authors": [
        "J Geng",
        "Y Mou",
        "F Li",
        "Q Li",
        "O Beyan",
        "S Decker",
        "C Rong"
      ],
      "year": "2021",
      "venue": "Towards general deep leakage in federated learning",
      "arxiv": "arXiv:2110.09074"
    },
    {
      "citation_id": "12",
      "title": "Beyond inferring class representatives: User-level privacy leakage from federated learning",
      "authors": [
        "Z Wang",
        "M Song",
        "Z Zhang",
        "Y Song",
        "Q Wang",
        "H Qi"
      ],
      "year": "2019",
      "venue": "IEEE INFOCOM 2019"
    },
    {
      "citation_id": "13",
      "title": "Federated learning with differential privacy: Algorithms and performance analysis",
      "authors": [
        "K Wei",
        "J Li",
        "M Ding",
        "C Ma",
        "H Yang",
        "F Farokhi",
        "S Jin",
        "T Quek",
        "H Poor"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "14",
      "title": "Membership inference attacks against machine learning models",
      "authors": [
        "R Shokri",
        "M Stronati",
        "C Song",
        "V Shmatikov"
      ],
      "year": "2017",
      "venue": "2017 IEEE Symposium on Security and Privacy (SP)"
    },
    {
      "citation_id": "15",
      "title": "Openmoji",
      "venue": "Openmoji"
    },
    {
      "citation_id": "16",
      "title": "Calibrating noise to sensitivity in private data analysis",
      "authors": [
        "C Dwork",
        "F Mcsherry",
        "K Nissim",
        "A Smith"
      ],
      "year": "2006",
      "venue": "Theory of cryptography conference"
    },
    {
      "citation_id": "17",
      "title": "Deep learning with differential privacy",
      "authors": [
        "M Abadi",
        "A Chu",
        "I Goodfellow",
        "H Mcmahan",
        "I Mironov",
        "K Talwar",
        "L Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM SIGSAC conference on computer and communications security"
    },
    {
      "citation_id": "18",
      "title": "Boosting and differential privacy",
      "authors": [
        "C Dwork",
        "G Rothblum",
        "S Vadhan"
      ],
      "year": "2010",
      "venue": "2010 IEEE 51st Annual Symposium on Foundations of Computer Science"
    },
    {
      "citation_id": "19",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "21",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "22",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Y.-A Chung",
        "W.-N Hsu",
        "H Tang",
        "J Glass"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning"
    },
    {
      "citation_id": "24",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong"
      ],
      "venue": ""
    },
    {
      "citation_id": "26",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Li",
        "A Watanabe",
        "H Mohamed",
        "Yi Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    }
  ]
}