{
  "paper_id": "2202.09263v1",
  "title": "Is Cross-Attention Preferable To Self-Attention For Multi-Modal Emotion Recognition?",
  "published": "2022-02-18T15:44:14Z",
  "authors": [
    "Vandana Rajan",
    "Alessio Brutti",
    "Andrea Cavallaro"
  ],
  "keywords": [
    "Multi-modal",
    "emotion recognition",
    "attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Humans express their emotions via facial expressions, voice intonation and word choices. To infer the nature of the underlying emotion, recognition models may use a single modality, such as vision, audio, and text, or a combination of modalities. Generally, models that fuse complementary information from multiple modalities outperform their uni-modal counterparts. However, a successful model that fuses modalities requires components that can effectively aggregate task-relevant information from each modality. As crossmodal attention is seen as an effective mechanism for multi-modal fusion, in this paper we quantify the gain that such a mechanism brings compared to the corresponding self-attention mechanism. To this end, we implement and compare a cross-attention and a selfattention model. In addition to attention, each model uses convolutional layers for local feature extraction and recurrent layers for global sequential modelling. We compare the models using different modality combinations for a 7-class emotion classification task using the IEMOCAP dataset. Experimental results indicate that albeit both models improve upon the state-of-the-art in terms of weighted and unweighted accuracy for tri-and bi-modal configurations, their performance is generally statistically comparable. The code to replicate the experiments is available at https:// github.com/smartcameras/SelfCrossAttn",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition (ER) models use one or more modalities, such as audio (language and para-language), images (facial expressions and body gestures) and text (language) to infer the class of underlying emotion  [1] . Multi-modal models are designed to effectively fuse relevant information from different modalities and generally outperform uni-modal models  [2, 3] . ER models may use as input raw signals (speech or face images)  [4, 5, 6]  or handcrafted features  [3, 7] . Commonly used speech features are low-level descriptors, such as formants, pitch, log energy, zero-crossing rate and Mel Frequency Cepstral Coefficients (MFCCs)  [3, 7] . Facial expressions can be represented by fixed features based on entities that are always present on the face, such as eyes, mouth and eyebrows and/or transitory features based on temporary entities like wrinkles and bulges  [8] . Tokenized words can be mapped into linguistic features using word embedding algorithms, such as word2vec  [9]  or GloVe  [10] .\n\nER models based on Deep Neural Networks (DNNs) may contain convolutional layers to extract local task-relevant components from the input and recurrent layers to facilitate the global sequential modelling  [4, 5] . Attention mechanisms  [11]  integrated in DNN architectures encourage the ER model to focus on task-relevant time instants  [3, 6] . The general purpose of attention mechanism is to provide varying levels of weights to different time-steps in a sequence. There are two types of attention mechanisms, namely self (or intra-modal) attention and cross (or inter-modal) attention. A self-attention mechanism computes the representation of a uni-modal sequence by relating different positions of the same sequence  [6, 12] . Cross-modal attention mechanisms use one modality to estimate the relevance of each position in another modality  [13] . For example, a self-attention mechanism between 2 recurrent layers can be used to emphasise task-relevant time-steps in an input speech signal  [6] , whereas an iterative multi-hop cross-attention mechanism may select and aggregate information from multi-modal features obtained with Gated Recurrent Unit (GRU) layers  [3, 7] . Transformers  [14] , which contain a Multi-Head Attention (MHA) module, are also becoming popular in modelling uni-modal as well as multi-modal emotional data  [15, 13, 16, 17] . Cross-modal transformers use cross-attention to calculate the relevance of each time-step in a target modality representation using a different sourcemodality  [13, 17] . A serial  [13, 17]  or parallel  [16]  combination of cross and self-attention transformers aims to capture the cross-modal and intra-modal relationships for multi-modal fusion. Considering the interest in models combining self and cross attention-based transformer encoders  [13, 16, 17] , we conduct the first study comparing the two types of attention mechanisms (without the other transformer components). To understand the differences between the two types of attention mechanisms, we extensively compare a model based only on cross-attention and one based only on self-attention for bi-and tri-modal combinations. We compare the two models on the IEMOCAP  [18]  dataset for 7-class emotion classification and conclude that the cross-attention model does not outperform the self-attention model. Nevertheless, both models improve the state-of-the-art results on tri-modal as well as bi-modal emotion recognition tasks in terms of weighted and unweighted accuracy metrics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cross And Self Attention Models",
      "text": "Self and cross-attention models first process individual modalities using modality-specific encoders. The encoded features are then fed into self or cross Multi-Head Attention (MHA)  [14]  modules, respectively. A global representation of the utterance clip is generated as temporal average at the outputs of each attention module. The resulting features are then concatenated and their mean and standard deviation are obtained using a statistical pooling layer. The concatenation of mean and standard deviation vectors is then fed to fully connected layers. The emotional class predictions are obtained through a softmax operation. A detailed explanation is given as follows:\n\nLet Xa ∈ R ta×da be the audio features corresponding to an ut- terance clip, where ta is the sequence length and da is the feature dimension. The audio encoder consists of a 1D convolution layer followed by a bi-directional GRU. The convolution layer, which refines the input feature sequence by finding task-relevant patterns, operates as follows:\n\nwhere X a ∈ R t a ×d a is the output with length t a and dimension d a , t ∈ [0, t a -1], * is the convolution operator, W are the weights and b are the biases associated with the layer. Thus, the convolution layer modifies the sequence length as well as the feature dimension. The bi-directional GRU layer models contextual inter-dependence of the features across time. For each element in the sequence, the bi-GRU layer computes the following functions:\n\nwhere ht and ht-1 are the hidden states at times t and t-1, X a (t) is the input at time t. rt, zt and nt are the reset, update and new gates, W and b are the corresponding weights and biases, σ and φ h are the sigmoid and hyperbolic tangent functions and is the Hadamard product. At the output of bi-GRU, the forward and backward hidden states for each time-step are concatenated and the refined audio features can be represented as ea ∈ R t a ×d , where d is twice the number of hidden neurons in the GRU. Similar to audio, the vision encoder consists of one 1D convolution layer followed by a bi-GRU layer. If Xv ∈ R tv ×dv represents the vision features corresponding to an utterance, then at the output of vision encoder, the features are refined to ev ∈ R t v ×d . For the text modality, the encoder consists of only one bi-GRU layer. The input and output of text encoder can be represented by X l ∈ R t l ×d l and e l ∈ R t l ×d respectively.\n\nWe use the MHA module  [14]  for self and cross-attention modelling. An MHA module consists of multiple such attention operations to capture richer interpretations of the sequence. Each MHA module requires 3 inputs, namely, Query (Q), Key (K) and Value (V ), each of which is first projected H times to different sub-spaces using linear layers, where H refers to number of heads. Projections for each sub-space h ∈ {0, ...., H -1} can be calculated as\n\nwhere m ∈ {a, v, l} denotes the modality. In each of these subspaces, scaled dot-product attention is performed on the projections. For a sub-space h, the attention operation is given as\n\nwhere Att h (•) and d k refer to the attention operation in sub-space h and feature dimensionality, respectively. The outputs of all H attentions are concatenated and passed through a linear layer to obtain the final output of an MHA module.\n\nIn the cross-attention model, a source modality is given as K and V , whereas a target modality is fed as Q (see Fig.  1 ). The intuition behind such an approach is to discover cross-modal interactions by adapting the source modality to the target modality  [13] . As an example, let us take the case of audio as target modality and vision as the source modality. The refined audio features ea ∈ R t a ×d are transformed to Q using Eq. 3 and vision features ev ∈ R t v ×d to K and V using Eq. (  4 )-  (5) . The cross-modal MHA module then maps the vision to the audio modality and outputs vision features adapted to audio e w av ∈ R t a ×d . Note that the sequence length of the crossattention weighted output is the same as the target modality audio.\n\nTable  1 : Results of a 7-class emotion classification task presented as mean ± standard deviation. AMH refers to AMH  [3]  for tri-modal models and to MHA  [7]  for bi-modal models. KEY -A: audio; V: vision; T: text; Self: self-attention model; Cross: cross-attention model. The best results in each row are in bold font. The symbol * refers to the only three results with statistically significant difference between the self and cross models. With 3 modalities, we have 6 combinations of source-target modalities and hence we use 6 MHA modules. In case of self-attention model, the input sequence corresponding to the same modality is used as Q, K and V (see Fig.  2 ). This helps to capture intra-modal interactions in each modality. For cross-attention model, statistical pooling is done across the concatenation of the temporal averages of 6 cross-modal sequences, whereas for the self-attention model, it is done across the concatenation of the temporal averages of the self-attended sequences of all the 3 modalities. The classifier for both models is:\n\nwhere µ and σ are the mean and standard deviation obtained from the output of statistical pooling layer, represents concatenation operation, f θ 1 and f θ 2 denote the 2 fully connected layers with parameters θ1 and θ2, respectively, and ŷ denotes the one-hot vector of emotion prediction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Validation",
      "text": "In this section, we discuss the dataset and results of using crossand self-attention models for 7-class bi-modal and tri-modal emotion recognition. We also discuss comparison with state-of-the-art methods and experiments with additional model configurations for both models. We use the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [18]  dataset which contains approximately 12 hours of audio-visual dyadic emotional interactions in acted and spontaneous settings. The dataset, recorded with 5 male and 5 female speakers, includes the ground-truth text transcripts. The labelling of each utterance was determined by majority voting from 3 annotators. There is lack of consensus amongst researchers on the use of IEMO-CAP dataset. Some use it for 4 class classification  [13]  by merging different classes (happy and excited, angry and frustrated), while others  [3, 7, 19, 16]  perform 7-class classification. We follow the latter. Since the creators of the dataset did not define a training-testing split, we use the same dataset partition and features as  [3, 7, 19] . The final dataset contains 7,487 utterances in total (1,103 angry, 1,041 excited, 595 happy, 1,084 sad, 1,849 frustrated, 107 surprise and 1,708 neutral). Class sizes smaller than 100 utterances (fear, disgust, other) are eliminated  [3] . We perform 5-fold cross validation to assess the model performance. Data in each fold are split into training, development, and testing sets (8:0.5:1.5). We train and evaluate the model 10 times (with 10 different random seeds) per fold, and the performance is assessed in terms of weighted accuracy (WA) and unweighted accuracy (UWA) metrics.\n\nFor the audio modality, 40D MFCC features (frame size is set to 25 ms at a rate of 10 ms with the Hamming window) are extracted and concatenated with their first and second order derivatives to obtain the final acoustic feature dimension of 120. Audio features are standardised by removing the mean and scaling to unit variance. For vision data, cropped face images of speakers are fed into a ResNet-101  [20]  to obtain 2048D features at a frame rate of 3 Hz. For text modality, each word in an utterance is represented by a 300D GloVe  [10]  embedding. Note that the modalities are sampled at different rates and the maximum sequence length of audio, vision and text modalities is set to 1,000, 32 and 128 respectively.\n\nThe models are implemented using PyTorch  [21] . The bimodal and uni-modal versions of the tri-modal models are created by removing components corresponding to the unused modality/modalities. We use Adam  [22]  optimiser with a learning rate of 0.001. The learning rate is reduced by a factor 0.1 when the validation loss has stopped decreasing for 10 consecutive epochs. Training is stopped when UWA does not improve in the validation set for 10 consecutive epochs and the model with best validation UWA is used for testing. The batch size is 32 and all models are trained using the categorical cross-entropy loss.\n\nThe audio and vision encoders contain one 1D convolution layer each. The kernel size and stride length are both set to 1. The number of input and output channels for audio convolution layer are 1,000 and 500 respectively while for vision they are 32 and 25 respectively. The number of bi-GRU layers for all the 3 modalities is 1. The number of hidden neurons in each bi-GRU layer is 60. The number of attention heads in all MHA modules is 6 and a dropout rate of 0.1 is applied to reduce overfitting. The number of neurons in the first and second fully connected output layers are 60 (same as number of bi-GRU neurons) and 7 (number of output classes) respectively. All parameters were chosen based on the performance on validation set.\n\nTable  1  shows the performance of the self and cross-attention models on 7-class uni-modal, bi-modal and tri-modal emotion recognition tasks. We report the mean and standard deviation obtained across 50 runs (5 folds × 10 repetitions) for each model. We also applied two-tailed t-test with the null hypothesis that the accuracy values of both self and cross-attention models have identical average (expected) values. Comparison of the uni-modal performances shows that the text outperforms the vision and audio modalities. This result is consistent with previous work  [13, 19] . Since uni-modal performance evaluation is not possible with the cross-modal model, we report results with the uni-modal version of the self-attention model. Among bi-modal models, the combination of vision and text modalities gives the best performance for both models. These results are consistent with previous work  [7, 19] . Overall, both models provide comparable performances for bi-and tri-modal cases. Selfattention significantly outperforms cross-attention (P value < .05) only for T+A (text and audio) and the WA of T+V+A (text, video, and audio).\n\nWe compare with methods that use the same set of features and dataset partition. The tri-modal models are compared with AMH  [3] , the current state-of-the-art model, which uses a combination of unimodal GRU layers and an iterative attention mechanism 1 . Note that the self-attention model exceeds the performance of AMH by 4.0 and 2.5 percentage points (pp) over mean in terms of WA and UWA, respectively. Similar figures for the cross-attention model are 3.1 pp and 1.9 pp. We also compare with MDRE  [19] , which uses recurrent layers to model uni-modal signals followed by aggregation and classification using fully connected layers. The better performance of the self and cross-attention models, as well as AMH, compared to MDRE can be attributed to the effectiveness of the attention mechanism. For bi-modal models, we compare with the bi-modal version of AMH called MHA  [7]  and MDRE. Again, both models outperform MHA and MDRE in all the 3 bi-modal cases. Note that we obtain bi-modal results by ablating the tri-modal models and not by fine-tuning for individual bi-modal cases. Also, AMH, MHA and MDRE use prosody features in addition to MFCC features for audio, whereas we use only MFCC features. The state-of-the-art result for text+audio case is obtained by  [16]  (0.560 WA and 0.612 UWA) which is significantly higher than the bi-modal T+A (text and audio) results. We hypothesize two reasons for this: (1) unlike  [16] , the bi-modal models are not fine-tuned for the bi-modal cases; (2)  [16]  uses transformer encoders that contain additional parameters that might help in learning more complex inter-modal relationships, whereas we use only the multi-head attention mechanism. Nevertheless, both models improve the state-of-the-art tri-modal results of AMH.\n\nFig.  3  shows the confusion matrices for the self and crossattention models. For both models we can observe that the classes angry and frustrated are more often confused with each other, and 1 We use the revised results of AMH, MHA and MDRE from https://github.com/david-yoon/ attentive-modality-hopping-for-SER.\n\nWe note that the WA and UWA values were swapped by the authors and we rectify this error in Table  1 .\n\nTable  2 : Weighted accuracy (WA) and Unweighted accuracy (UWA) for 7-class emotion classification using additional tri-modal model configurations. Self and Cross model results are also shown for comparison. KEY -SP: statistical pooling; Cross-noSP and Self-noSP: cross and self-attention models without SP; Cross+Self: combination model that concatenates mean and standard deviation vectors from self and cross-attention models. the class happy gets confused with excited (these 2 classes are inherently similar). The poor performance of both models on the class surprise can be attributed to the fact that this has the smallest sample size in the dataset. These observations are consistent with the previous literature  [3] .\n\nIn addition to the two described model configurations, we also experimented with different variations of the tri-modal models. We removed the statistical pooling layer from both models to assess its significance. The outputs from all temporal averaging modules (see Fig.  1  & 2) were concatenated and passed to the classifier module. These models are shown as Cross-noSP and Self-noSP in Table  2 . We can make two observations. Firstly, the self-attention model outperforms the cross-attention model (P value < .05 for WA) even after ablating statistical pooling. Secondly, the performance of both models decreases without the statistical pooling layer. We also assessed the performance of a combined model created by merging the self and cross-attention models (Cross+Self). The statistical pooling output from both models were concatenated and fed to a common classifier module. We can see that the performance is similar to that of the self-attention model. This might indicate that the cross-attention model does not contribute any additional, relevant information compared to that of the self-attention model.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "Intrigued by the popularity of cross-attention mechanism in multimodal fusion, we compared models based on self-attention and on cross-attention using the IEMOCAP dataset for tri-modal and bi-modal 7-class classification. Results show that there is no meaningful difference between the results of the two models. Thus, within the context of the dataset and architecture we used, we conclude that cross-attention does not outperform self-attention for multi-modal emotion recognition. Furthermore, both the self and the crossattention models improve the state-of-the-art in the recognition task. Future work includes investigating the effectiveness of cross and self-attention models for other multi-modal tasks and modalities.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the tri-modal cross-attention model. KEY",
      "page": 2
    },
    {
      "caption": "Figure 2: Attention and fusion module in the tri-modal self-",
      "page": 2
    },
    {
      "caption": "Figure 1: ). The intu-",
      "page": 2
    },
    {
      "caption": "Figure 2: ). This helps to capture intra-modal",
      "page": 3
    },
    {
      "caption": "Figure 3: Confusion matrices of self (left) and cross-attention mod-",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the confusion matrices for the self and cross-",
      "page": 4
    },
    {
      "caption": "Figure 1: & 2) were concatenated and passed to the classiﬁer module.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "is to provide varying levels of weights to different\ntime-steps in a"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "sequence.\nThere are two types of attention mechanisms, namely"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "self\n(or\nintra-modal)\nattention\nand\ncross\n(or\ninter-modal)\natten-"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "tion. A self-attention mechanism computes the representation of a"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "uni-modal sequence by relating different positions of\nthe same se-"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "quence [6, 12]. Cross-modal attention mechanisms use one modality"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "to estimate the relevance of each position in another modality [13]."
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "For example, a self-attention mechanism between 2 recurrent\nlay-"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "ers can be used to emphasise task-relevant\ntime-steps\nin an input"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "speech signal\n[6], whereas\nan iterative multi-hop cross-attention"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "mechanism may select and aggregate information from multi-modal"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "features obtained with Gated Recurrent Unit\n(GRU)\nlayers [3, 7]."
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "Transformers [14], which contain a Multi-Head Attention (MHA)"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "module,\nare\nalso\nbecoming\npopular\nin modelling\nuni-modal\nas"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "well as multi-modal emotional data [15, 13, 16, 17]. Cross-modal"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "transformers use cross-attention to calculate the relevance of each"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "time-step in a target modality representation using a different source-"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "modality [13, 17]. A serial [13, 17] or parallel [16] combination of"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "cross and self-attention transformers aims to capture the cross-modal"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "and intra-modal relationships for multi-modal\nfusion. Considering"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "the interest in models combining self and cross attention-based trans-"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "former encoders [13, 16, 17], we conduct\nthe ﬁrst study comparing"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "the\ntwo types of\nattention mechanisms\n(without\nthe other\ntrans-"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "former components). To understand the differences between the two"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "types of attention mechanisms, we extensively compare a model"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "based only on cross-attention and one based only on self-attention"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "for bi- and tri-modal combinations. We compare the two models"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "on the\nIEMOCAP [18] dataset\nfor 7-class\nemotion classiﬁcation"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "and conclude that\nthe cross-attention model does not outperform"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "the self-attention model.\nNevertheless, both models\nimprove the"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "state-of-the-art\nresults on tri-modal\nas well\nas bi-modal\nemotion"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "recognition tasks\nin terms of weighted and unweighted accuracy"
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": "metrics."
        },
        {
          "2Fondazione Bruno Kessler, Trento, Italy": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "attention model.\nThe rest of\nthe model\nis\nsame as\nthe tri-modal"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "cross-attention model. KEY - MHA: Multi-Head Attention; Temp.:"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "temporal; Avg.: averaging; µ: mean; σ: standard deviation."
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "Similar to audio, the vision encoder consists of one 1D convolu-"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "tion layer followed by a bi-GRU layer.\nrepresents\nIf Xv ∈ Rtv ×dv"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "the vision features corresponding to an utterance,\nthen at the output"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "of vision encoder,\n. For the\nthe features are reﬁned to ev ∈ Rt(cid:48)"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "text modality,\nthe encoder consists of only one bi-GRU layer. The"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "input and output of text encoder can be represented by Xl ∈ Rtl×dl"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "respectively.\nand el ∈ Rtl×d(cid:48)(cid:48)"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "We use the MHA module [14] for self and cross-attention mod-"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "elling. An MHA module consists of multiple such attention opera-"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "tions to capture richer interpretations of the sequence. Each MHA"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "module requires 3 inputs, namely, Query (Q), Key (K) and Value"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "(V ), each of which is ﬁrst projected H times to different sub-spaces"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "using linear layers, where H refers to number of heads. Projections"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "for each sub-space h ∈ {0, ...., H − 1} can be calculated as"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "(3)\nQh = W Q\nh em,"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "(4)\nKh = W K\nh em,"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "(5)\nVh = W V\nh em,"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "where m ∈ {a, v, l} denotes the modality.\nIn each of\nthese sub-"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "spaces, scaled dot-product attention is performed on the projections."
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "For a sub-space h, the attention operation is given as"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "T\n(cid:19)\n(cid:18) QhKh"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "√\n(6)\nAtth(Qh, Kh, Vh) = Softmax\nVh,"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "dk"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "where Atth(·) and dk refer to the attention operation in sub-space"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "h and feature dimensionality, respectively. The outputs of all H at-"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "tentions are concatenated and passed through a linear layer to obtain"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "the ﬁnal output of an MHA module."
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "In the cross-attention model, a source modality is given as K"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "and V , whereas a target modality is fed as Q (see Fig. 1). The intu-"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "ition behind such an approach is to discover cross-modal interactions"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "by adapting the source modality to the target modality [13]. As an"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "example,\nlet us take the case of audio as target modality and vision"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "are\nas the source modality. The reﬁned audio features ea ∈ Rt(cid:48)"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "to K\ntransformed to Q using Eq. 3 and vision features ev ∈ Rt(cid:48)"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "and V using Eq. (4)-(5). The cross-modal MHA module then maps"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "the vision to the audio modality and outputs vision features adapted"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": ""
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "a×d(cid:48)(cid:48)\nto audio ew\n. Note that the sequence length of the cross-\nav ∈ Rt(cid:48)"
        },
        {
          "Figure\n2:\nAttention\nand\nfusion module\nin\nthe\ntri-modal\nself-": "attention weighted output\nis the same as the target modality audio."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: shows the performance of the self and cross-attention",
      "data": [
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "self and cross models."
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": ""
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "Modality"
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "T"
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "V"
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "A"
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "T+V"
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "T+A"
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "V+A"
        },
        {
          "The best results in each row are in bold font. The symbol * refers to the only three results with statistically signiﬁcant difference between the": "T+V+A"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: shows the performance of the self and cross-attention",
      "data": [
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "T+V+A\n.490 ± .056\n.547 ± .025\n.578 ± .024\n.587 ± .022*",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": ".642 ± .019\n.564 ± .043\n.617 ± .016\n.636 ± .017"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "With 3 modalities, we have 6 combinations of source-target modal-",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "fold, and the performance is assessed in terms of weighted accuracy"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "ities and hence we use 6 MHA modules.\nIn case of self-attention",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "(WA) and unweighted accuracy (UWA) metrics."
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "model,\nthe input sequence corresponding to the same modality is",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "For the audio modality, 40D MFCC features (frame size is set to"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "used as Q, K and V (see Fig. 2). This helps to capture intra-modal",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "25 ms at a rate of 10 ms with the Hamming window) are extracted"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "interactions in each modality. For cross-attention model, statistical",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "and concatenated with their ﬁrst and second order derivatives to ob-"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "pooling is done across the concatenation of\nthe temporal averages",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "tain the ﬁnal acoustic feature dimension of 120. Audio features are"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "of 6 cross-modal sequences, whereas for\nthe self-attention model,",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "standardised by removing the mean and scaling to unit variance. For"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "it\nis done across the concatenation of the temporal averages of the",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "vision data, cropped face images of speakers are fed into a ResNet-"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "self-attended sequences of all the 3 modalities.",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "101 [20]\nto obtain 2048D features at a frame rate of 3 Hz.\nFor"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "The classiﬁer for both models is:",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "text modality, each word in an utterance is represented by a 300D"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "GloVe [10] embedding. Note that the modalities are sampled at dif-"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "(7)\ny = Softmax(fθ2 (fθ1 ([µ (cid:107) σ]))),",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "ferent rates and the maximum sequence length of audio, vision and"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "text modalities is set to 1,000, 32 and 128 respectively."
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "where µ and σ are the mean and standard deviation obtained from the",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": ""
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "The models\nare\nimplemented using PyTorch [21].\nThe bi-"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "output of statistical pooling layer, (cid:107) represents concatenation opera-",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": ""
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "modal and uni-modal versions of\nthe tri-modal models are created"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "tion, fθ1 and fθ2 denote the 2 fully connected layers with parameters",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": ""
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "by\nremoving\ncomponents\ncorresponding\nto\nthe\nunused modal-"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "θ1 and θ2, respectively, and ˆy denotes the one-hot vector of emotion",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": ""
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "ity/modalities. We use Adam [22] optimiser with a learning rate"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "prediction.",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": ""
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "of 0.001.\nThe learning rate is\nreduced by a factor 0.1 when the"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "validation loss has stopped decreasing for 10 consecutive epochs."
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "3. VALIDATION",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "Training is stopped when UWA does not\nimprove in the validation"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "set\nfor 10 consecutive epochs and the model with best validation"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "In this\nsection, we discuss\nthe dataset and results of using cross-",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "UWA is used for\ntesting.\nThe batch size is 32 and all models are"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "and self-attention models for 7-class bi-modal and tri-modal emo-",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "trained using the categorical cross-entropy loss."
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "tion recognition. We also discuss comparison with state-of-the-art",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "The audio and vision encoders contain one 1D convolution layer"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "methods and experiments with additional model conﬁgurations for",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "each. The kernel size and stride length are both set to 1. The number"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "both models.",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "of input and output channels for audio convolution layer are 1,000"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "We\nuse\nthe\nInteractive Emotional Dyadic Motion Capture",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "and 500 respectively while for vision they are 32 and 25 respectively."
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "(IEMOCAP) [18] dataset which contains approximately 12 hours of",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "The number of bi-GRU layers\nfor all\nthe 3 modalities\nis 1.\nThe"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "audio-visual dyadic emotional interactions in acted and spontaneous",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "number of hidden neurons in each bi-GRU layer is 60. The number"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "settings. The dataset, recorded with 5 male and 5 female speakers,",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "of attention heads in all MHA modules is 6 and a dropout rate of 0.1"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "includes\nthe ground-truth text\ntranscripts.\nThe labelling of each",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "is applied to reduce overﬁtting. The number of neurons in the ﬁrst"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "utterance was determined by majority voting from 3 annotators.",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "and second fully connected output layers are 60 (same as number of"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "There is lack of consensus amongst researchers on the use of IEMO-",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "bi-GRU neurons) and 7 (number of output classes) respectively. All"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "CAP dataset. Some use it for 4 class classiﬁcation [13] by merging",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "parameters were chosen based on the performance on validation set."
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "different classes (happy and excited, angry and frustrated), while",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "Table 1 shows the performance of\nthe self and cross-attention"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "others [3, 7, 19, 16] perform 7-class classiﬁcation. We follow the lat-",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "models on 7-class uni-modal, bi-modal and tri-modal emotion recog-"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "ter. Since the creators of the dataset did not deﬁne a training-testing",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "nition tasks. We report\nthe mean and standard deviation obtained"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "split, we use the same dataset partition and features as [3, 7, 19].",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "across 50 runs (5 folds × 10 repetitions) for each model. We also"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "The ﬁnal dataset contains 7,487 utterances\nin total\n(1,103 angry,",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "applied two-tailed t-test with the null hypothesis that\nthe accuracy"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "1,041 excited, 595 happy, 1,084 sad, 1,849 frustrated, 107 surprise",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "values of both self and cross-attention models have identical aver-"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "and 1,708 neutral). Class sizes smaller\nthan 100 utterances (fear,",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "age (expected) values. Comparison of the uni-modal performances"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "disgust, other) are eliminated [3]. We perform 5-fold cross valida-",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "shows that the text outperforms the vision and audio modalities. This"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "tion to assess the model performance. Data in each fold are split",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "result\nis consistent with previous work [13, 19].\nSince uni-modal"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "into training, development, and testing sets (8:0.5:1.5). We train and",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "performance evaluation is not possible with the cross-modal model,"
        },
        {
          ".483 ± .026\nV+A\n.376 ± .024\n.371 ± .042\n.481 ± .024": "evaluate the model 10 times (with 10 different\nrandom seeds) per",
          ".567 ± .026\n.477 ± .025\n.471 ± .047\n.566 ± .022": "we report\nresults with the uni-modal version of\nthe self-attention"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Weighted accuracy (WA) and Unweighted accuracy (UWA)": "for 7-class emotion classiﬁcation using additional\ntri-modal model"
        },
        {
          "Table 2: Weighted accuracy (WA) and Unweighted accuracy (UWA)": "conﬁgurations. Self and Cross model results are also shown for com-"
        },
        {
          "Table 2: Weighted accuracy (WA) and Unweighted accuracy (UWA)": "parison. KEY - SP: statistical pooling; Cross-noSP and Self-noSP:"
        },
        {
          "Table 2: Weighted accuracy (WA) and Unweighted accuracy (UWA)": "cross and self-attention models without SP; Cross+Self:\ncombina-"
        },
        {
          "Table 2: Weighted accuracy (WA) and Unweighted accuracy (UWA)": "tion model\nthat concatenates mean and standard deviation vectors"
        },
        {
          "Table 2: Weighted accuracy (WA) and Unweighted accuracy (UWA)": "from self and cross-attention models."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cross and self-attention models without SP; Cross+Self:": ""
        },
        {
          "cross and self-attention models without SP; Cross+Self:": "from self and cross-attention models."
        },
        {
          "cross and self-attention models without SP; Cross+Self:": "Model"
        },
        {
          "cross and self-attention models without SP; Cross+Self:": "Cross-noSP"
        },
        {
          "cross and self-attention models without SP; Cross+Self:": "Cross"
        },
        {
          "cross and self-attention models without SP; Cross+Self:": "Self-noSP"
        },
        {
          "cross and self-attention models without SP; Cross+Self:": "Self"
        },
        {
          "cross and self-attention models without SP; Cross+Self:": "Cross+Self"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "Cross\n.578 ± .024\n.636 ± .012"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "Self-noSP\n.584 ± .021\n.638 ± .019"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ".587 ± .022\n.642 ± .019\nSelf"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "Cross+Self\n.585 ± .028\n.642 ± .020"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "the class happy gets confused with excited (these 2 classes are in-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "herently similar).\nThe poor performance of both models on the"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "class surprise can be attributed to the fact\nthat\nthis has the smallest"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "sample size in the dataset. These observations are consistent with"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "the previous literature [3]."
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "In addition to the two described model conﬁgurations, we also"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "experimented with different variations of the tri-modal models. We"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "removed the statistical pooling layer from both models to assess its"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "signiﬁcance. The outputs from all temporal averaging modules (see"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "Fig. 1 & 2) were concatenated and passed to the classiﬁer module."
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "These models are shown as Cross-noSP and Self-noSP in Table 2."
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "We can make two observations. Firstly, the self-attention model out-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "performs the cross-attention model (P value < .05 for WA) even after"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "ablating statistical pooling. Secondly, the performance of both mod-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "els decreases without\nthe statistical pooling layer. We also assessed"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "the performance of a combined model created by merging the self"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "and cross-attention models (Cross+Self). The statistical pooling out-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "put from both models were concatenated and fed to a common clas-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "siﬁer module. We can see that\nthe performance is similar to that of"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "the self-attention model. This might indicate that the cross-attention"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "model does not contribute any additional, relevant information com-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "pared to that of the self-attention model."
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "4. CONCLUSION"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "Intrigued by the popularity of cross-attention mechanism in multi-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "modal\nfusion, we\ncompared models based on self-attention and"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "on cross-attention using the IEMOCAP dataset\nfor\ntri-modal and"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "bi-modal 7-class classiﬁcation. Results show that\nthere is no mean-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "ingful difference between the results of the two models. Thus, within"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "the context of the dataset and architecture we used, we conclude that"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "cross-attention does not outperform self-attention for multi-modal"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "emotion recognition.\nFurthermore,\nboth the\nself\nand the\ncross-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "attention models improve the state-of-the-art in the recognition task."
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "Future work includes\ninvestigating the effectiveness of cross and"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "self-attention models for other multi-modal tasks and modalities."
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "Acknowledgement. We thank the authors of [3, 7, 19] for providing"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "the processed and partitioned IEMOCAP dataset. We acknowledge"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "the use of the ESPRC funded Tier 2 facility, JADE."
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "5. REFERENCES"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "[1] George Caridakis, Ginevra Castellano, Loic Kessous, Amaryl-"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": ""
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "lis Raouzaiou, Lori Malatesta, Stelios Asteriadis, and Kostas"
        },
        {
          "Cross-noSP\n.570 ± .021\n.634 ± .015": "Karpouzis, “Multimodal emotion recognition from expressive"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Artiﬁcial Intelligence Applications and Innovations. Springer,",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "“Cross-modal\nknowledge\ntransfer\nvia\ninter-modal\ntransla-"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "2007, pp. 375–388.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "arXiv preprint\ntion and alignment\nfor\naffect\nrecognition,”"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "arXiv:2108.00809, 2021."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[2] Tadas\nBaltruˇsaitis,\nChaitanya Ahuja,\nand\nLouis-Philippe",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Morency,\n“Multimodal machine learning: A survey and tax-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "[16] Licai Sun, Bin Liu, Jianhua Tao, and Zheng Lian, “Multimodal"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "onomy,”\nIEEE Trans. on Pattern Anal. and Mach. Intell., vol.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "cross-and self-attention network for speech emotion recogni-"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "41, no. 2, pp. 423–443, 2018.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "tion,” in Proc. of the IEEE Int. Conf. on Acoustics, Speech and"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "Signal Process., 2021, pp. 4275–4279."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[3]\nSeunghyun Yoon, Subhadeep Dey, Hwanhee Lee, and Kyomin",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Jung, “Attentive modality hopping mechanism for speech emo-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "Niu,\n“Multimodal\ntransformer fusion for continuous emotion"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "tion recognition,” in Proc. of the IEEE Int. Conf. on Acoustics,",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "the IEEE Int. Conf. on Acoustics,\nrecognition,”\nin Proc. of"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Speech and Signal Process., 2020, pp. 3362–3366.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "Speech and Signal Process., 2020, pp. 3507–3511."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[4] George Trigeorgis,\nFabien Ringeval, Raymond Brueckner,",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "[18] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Erik Marchi, Mihalis A Nicolaou, Bj¨orn Schuller, and Stefanos",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Zafeiriou, “Adieu features? end-to-end speech emotion recog-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“IEMO-"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "nition using a deep convolutional recurrent network,” in Proc.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "CAP:\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "of\nthe IEEE Int. Conf. on Acoustics, Speech and Signal Pro-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "Language Resources\nand Evaluation,\nvol.\n42,\nno.\n4,\npp."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "cess., 2016, pp. 5200–5204.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "335–359, 2008."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[5]\nPanagiotis Tzirakis, George Trigeorgis, Mihalis A Nicolaou,",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "[19]\nSeunghyun Yoon, Seokhyun Byun, and Kyomin Jung,\n“Mul-"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Bj¨orn Schuller, and Stefanos Zafeiriou,\n“End-to-end multi-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "timodal speech emotion recognition using audio and text,”\nin"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "modal emotion recognition using deep neural networks,” IEEE",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "IEEE Spoken Language Technology Workshop, 2018, pp. 112–"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "J. of Selected Topics\nin Signal Process., vol. 11, no. 8, pp.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "118."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "1301–1309, 2017.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[6] Vandana Rajan, Alessio Brutti, and Andrea Cavallaro,\n“Con-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "“Deep residual\nlearning for\nimage recognition,”\nin Proc. of"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "ﬂictnet: End-to-end learning for speech-based conﬂict\ninten-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "the IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770–"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "sity estimation,”\nIEEE Signal Processing Letters, vol. 26, no.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "778."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "11, pp. 1668–1672, 2019.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "[21] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[7]\nSeunghyun Yoon, Seokhyun Byun, Subhadeep Dey, and Ky-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmai-"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "omin Jung,\n“Speech emotion recognition using multi-hop at-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "son, Luca Antiga, and Adam Lerer, “Automatic differentiation"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "tention mechanism,” in Proc. of the IEEE Int. Conf. on Acous-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "in PyTorch,” in NIPS-Workshop on Autodiff, 2017."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "tics, Speech and Signal Process., 2019, pp. 2822–2826.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "[22] Diederik P Kingma and Jimmy Ba,\n“Adam: A method for"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[8] Beat Fasel and Juergen Luettin,\n“Automatic facial expression",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "stochastic optimization,”\nin Int. Conf. on Learning Represen-"
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "analysis:\na survey,”\nPattern Recognition, vol. 36, no. 1, pp.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": "tations, 2015."
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "259–275, 2003.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[9] Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey Dean,",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "“Efﬁcient estimation of word representations in vector space,”",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "in Int. Conf. on Learning Representations, 2013.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[10]\nJeffrey Pennington, Richard Socher, and Christopher D Man-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "ning,\n“Glove: Global vectors\nfor word representation,”\nin",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Proc. of the Conf. on Empirical Methods in Natural Language",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Processing, 2014, pp. 1532–1543.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[11] Dzmitry Bahdanau, Kyung Hyun Cho,\nand Yoshua Bengio,",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "“Neural machine translation by jointly learning to align and",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "translate,” in Int. Conf. on Learning Representations, 2015.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[12] Mingyi Chen, Xuanji He, Jing Yang, and Han Zhang, “3-d con-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "volutional recurrent neural networks with attention model for",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "speech emotion recognition,” IEEE Signal Processing Letters,",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "vol. 25, no. 10, pp. 1440–1444, 2018.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[13] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Kolter, Louis-Philippe Morency,\nand Ruslan Salakhutdinov,",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "“Multimodal\ntransformer\nfor unaligned multimodal\nlanguage",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "the Conf. Association for Computa-\nsequences,”\nin Proc. of",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "tional Linguistics. NIH Public Access, 2019, p. 6558.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "[14] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszko-",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "in Neural\nPolosukhin,\n“Attention is all you need,”\nin Adv.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        },
        {
          "faces, body gestures and speech,” in Proc. of the Int. Conf. on": "Information Processing Systems, 2017, pp. 5998–6008.",
          "[15] Vandana\nRajan,\nAlessio\nBrutti,\nand\nAndrea\nCavallaro,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multimodal emotion recognition from expressive faces, body gestures and speech",
      "authors": [
        "George Caridakis",
        "Ginevra Castellano",
        "Loic Kessous",
        "Amaryllis Raouzaiou",
        "Lori Malatesta",
        "Stelios Asteriadis",
        "Kostas Karpouzis"
      ],
      "year": "2007",
      "venue": "Proc. of the Int. Conf. on Artificial Intelligence Applications and Innovations"
    },
    {
      "citation_id": "3",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE Trans. on Pattern Anal. and Mach. Intell"
    },
    {
      "citation_id": "4",
      "title": "Attentive modality hopping mechanism for speech emotion recognition",
      "authors": [
        "Seunghyun Yoon",
        "Subhadeep Dey",
        "Hwanhee Lee",
        "Kyomin Jung"
      ],
      "year": "2020",
      "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Process"
    },
    {
      "citation_id": "5",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Process"
    },
    {
      "citation_id": "6",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE J. of Selected Topics in Signal Process"
    },
    {
      "citation_id": "7",
      "title": "Conflictnet: End-to-end learning for speech-based conflict intensity estimation",
      "authors": [
        "Alessio Vandana Rajan",
        "Andrea Brutti",
        "Cavallaro"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Subhadeep Dey",
        "Kyomin Jung"
      ],
      "year": "2019",
      "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Process"
    },
    {
      "citation_id": "9",
      "title": "Automatic facial expression analysis: a survey",
      "authors": [
        "Beat Fasel",
        "Juergen Luettin"
      ],
      "year": "2003",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "Tomas Mikolov",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Int. Conf. on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proc. of the Conf. on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Hyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Int. Conf. on Learning Representations"
    },
    {
      "citation_id": "13",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "14",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proc. of the Conf. Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Cross-modal knowledge transfer via inter-modal translation and alignment for affect recognition",
      "authors": [
        "Alessio Vandana Rajan",
        "Andrea Brutti",
        "Cavallaro"
      ],
      "year": "2021",
      "venue": "Cross-modal knowledge transfer via inter-modal translation and alignment for affect recognition",
      "arxiv": "arXiv:2108.00809"
    },
    {
      "citation_id": "17",
      "title": "Multimodal cross-and self-attention network for speech emotion recognition",
      "authors": [
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao",
        "Zheng Lian"
      ],
      "year": "2021",
      "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Process"
    },
    {
      "citation_id": "18",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Process"
    },
    {
      "citation_id": "19",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "20",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "21",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "22",
      "title": "Automatic differentiation in PyTorch",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Soumith Chintala",
        "Gregory Chanan",
        "Edward Yang",
        "Zachary Devito",
        "Zeming Lin",
        "Alban Desmaison",
        "Luca Antiga",
        "Adam Lerer"
      ],
      "year": "2017",
      "venue": "Automatic differentiation in PyTorch"
    },
    {
      "citation_id": "23",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "Int. Conf. on Learning Representations"
    }
  ]
}