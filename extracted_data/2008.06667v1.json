{
  "paper_id": "2008.06667v1",
  "title": "Advancing Multiple Instance Learning With Attention Modeling For Categorical Speech Emotion Recognition",
  "published": "2020-08-15T07:23:43Z",
  "authors": [
    "Shuiyang Mao",
    "P. C. Ching",
    "C. -C. Jay Kuo",
    "Tan Lee"
  ],
  "keywords": [
    "categorical speech emotion recognition",
    "weak labeling",
    "multiple instance learning",
    "attention modeling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Categorical speech emotion recognition is typically performed as a sequence-to-label problem, i. e., to determine the discrete emotion label of the input utterance as a whole. One of the main challenges in practice is that most of the existing emotion corpora do not give ground truth labels for each segment; instead, we only have labels for whole utterances. To extract segmentlevel emotional information from such weakly labeled emotion corpora, we propose using multiple instance learning (MIL) to learn segment embeddings in a weakly supervised manner. Also, for a sufficiently long utterance, not all of the segments contain relevant emotional information. In this regard, three attention-based neural network models are then applied to the learned segment embeddings to attend the most salient part of a speech utterance. Experiments on the CASIA corpus and the IEMOCAP database show better or highly competitive results than other state-of-the-art approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic speech emotion recognition (ASER) aims to decode emotional content from audio signals. It has constituted an active research topic in the field of human-machine interaction (HCI). Detection of lies, monitoring of call centers, and medical diagnoses are also considered as promising application scenarios of speech emotion recognition.\n\nCategorical speech emotion recognition at utterance level can be formulated as a sequence-to-label problem. The input utterance is divided into a sequence of acoustic segments, and the output is a single label of emotion type. A few previous studies explored the use of segment units for categorical speech emotion recognition and demonstrated that combining segmentlevel prediction results led to superior performance  [1, 2, 3] . These prior works were mostly based on conventional models, such as support vector machine (SVM) and K-nearest neighbors (K-NN), for segment classification. In this paper, a convolutional neural network (CNN) is applied to extract emotionally relevant features, i. e., to learn emotion relevant segment embeddings. Also, most of the existing emotion corpora do not provide ground truth labels at segment level; instead, they only have labels for whole utterances. A viable solution is to learn local concepts from global annotations, which is the main idea of multiple instance learning (MIL)  [4, 5] . MIL has been successfully applied to sound event detection (SED)  [6, 7] , speech recognition  [8] , and image analysis  [5] . In the MIL problem statement, the training set contains labeled bags that comprise many unlabeled instances, and the task is to predict the labels of unseen bags and instances. For categorical speech emotion recognition, each utterance is treated as a bag, and segments within the utterance as instances. One main feature of this work is the application of deep learning of feature representation in the MIL framework to learn segment embeddings from weak labels at utterance level. Compared to the raw features such as MFCC, energy, or pitch, the learned segment embeddings are more tied to the task of interest, thus naturally highlighting salient portions of the data, which we conjecture would offer an advantage in the final classification.\n\nA key question is how to enable a deep learning model to identify and focus on the most salient parts of a speech utterance when making an utterance-level decision with the learned segment embeddings. In this regard, attention neural network models are investigated. The key idea behind the attention mechanism is to align the input-output sequences such that, in the decoding phase, the major contribution is from the corresponding encoded information. In contrast, the effect of irrelevant ones is minimized. In this work, attention modeling is expected to facilitate a structurally meaningful composition of the utterance representation from the learned emotionally relevant segment embeddings. This is the first attempt to combine MIL-based deep learning of segment embedding with attention modeling for categorical speech emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mil-Based Segment Embedding Learning",
      "text": "We formulate our segment-based approach as a MIL problem following the instance space paradigm  [4] . Each utterance (bag) is first divided into a sequence of segments (instances). These individual segments are then used to train a CNN model. The learned CNN aims to generate emotionally salient embeddings for each segment.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Log Mel Spectrograms",
      "text": "For the segment-level features, we use the 64-bin log Mel filterbanks, which have been extensively evaluated in the existing literature  [9, 10, 11] . They are computed by short-time Fourier transform (STFT) with a window length of 25 ms, hop length of 10 ms, and FFT length of 512. Subsequently, 64-bin log Mel filterbanks are derived from each short-time frame, and the frame-level features are combined to form a time-frequency matrix representation of the segment.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Segment Embeddings",
      "text": "Our segment-based method must address how to train a segment-level model without access to a training set of labeled segments. To address this problem, we follow the most straightforward approach, called Single Instance Learning (SIL)  [12] , i.e., each segment inherits the label of the utterance where it lies. A CNN is then trained on the resulting dataset. The outputs of the penultimate layer of the trained CNN, which we refer to as segment embeddings in this work, are stored and will be employed as inputs to the subsequent recognition part. Besides, a softmax layer sits on top of the CNN model and aims to predict a probability distribution P as follows:\n\nwhere K denotes the number of possible emotions. Figure  2  shows an example of a probability distribution predicted by the trained CNN for the audio file \"Happy liuchanhg 382.wav\" from the CASIA corpus. It can be observed that: (1) the probability distribution of each segment changes across the whole utterance; (2) most of the segments convey information that conforms to the utterance where they lie; and (3) there are segments within one utterance that do not convey any information about the target emotion class or that are more related to other classes, which constitutes confusing information. If we can place additional focus on these more relevant segments, system performance might be improved. In this regard, we have developed three attention-based neural networks, which are described in detail in the following section.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Attention-Based Neural Networks",
      "text": "Attention neural networks assume that the bag-level prediction can be constructed as a weighted sum of the instance-level predictions. Herein, three attention-based neural network models are investigated and compared, i.e., decision-level single attention (D-Single-Att.)  [13] , decision-level multiple attention (D-Multi-Att.)  [14]  and feature-level attention (Feature-Att.)  [15] , as shown in Figure  3 (a)-(c), respectively. We denote the input segment embeddings within a certain speech utterance as X ∈ R T ×M , where T is the number of segments and M represents the dimension of segment embeddings. The output of the second fully-connected (FC) layer is denoted as h, which has a dimension of 120, i. e., H is set to 120 in Figure  3 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Decision-Level Single Attention",
      "text": "In the decision-level single attention model (as shown in Figure  3(a) ), an attention function is applied on the predictions of the instances to obtain the bag-level prediction:\n\nwhere k denotes the k-th emotion class of the instance-level prediction f (h) ∈ [0, 1] K and the bag-level prediction F (B) ∈ [0, 1] K , and w(h) k ∈ [0, 1] is a weight of f (h) k that we refer to as a decision-level attention function:\n\nwhere s(.) can be any non-negative function (i.e., Softmax nonlinearity) to ensure that attention w(.) is normalized. Both the attention function w(.) and the instance-level classifier f (.) depend on a set of learnable parameters.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Decision-Level Multiple Attention",
      "text": "The decision-level multiple attention model is an extension of the above decision-level single attention model. It consists of several single attention modules (we herein use two attention modules, as shown in Figure  3 (b)) applied to intermediate neural network layers. The outputs of these attention modules are concatenated.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature-Level Attention",
      "text": "The limitation of the above decision-level attention neural networks is that the attention function w(.) is only applied to the prediction of the instances f (h). To address this constraint, we also investigate the effect of applying the attention function to the outputs of the hidden layers, which we refer to as the feature-level attention (as shown in Figure  3(c) ), in which the bag-level representation U can be modeled as:\n\nwhere d denotes the d-th dimension of the hidden layer output q(h) ∈ R D and the bag-level representation U ∈ R D ; and v(h) d ∈ [0, 1] is a weight of q(h) d that we refer to as a featurelevel attention function: where u(.) can be any non-negative function (i.e., Sigmoid nonlinearity) to ensure that attention v(.) is normalized. Both the attention function v(.) and the instance-level feature mapping function q(.) depend on a set of learnable parameters. The prediction of a bag B can then be obtained by classifying the baglevel representation U as follows:\n\nwhere g(.) is the final classifier that corresponds to the last neural network layer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Corpora",
      "text": "Two different emotion corpora are used to evaluate the validity of the proposed method, namely, a Chinese emotional corpus (CASIA)  [16]  and an English emotional database (IEMOCAP)  [17] , which have been extensively evaluated in the literature. Specifically, the CASIA corpus  [16]  contains 9,600 utterances that are simulated by four subjects (two males and two females) in six different emotional states, i. e., angry, fear, happy, neutral, sad, and surprise. In our experiments, we only use 7,200 utterances that correspond to 300 linguistically neutral sentences the same statements. All of the emotion categories are selected.\n\nThe IEMOCAP database  [17]  was collected using motion capture and audio/video recording over five dyadic sessions with 10 subjects. At least three evaluators annotated each utterance in the database with the categorical emotion labels chosen from the set: angry, disgusting, excited, fear, frustrate, happy, neutral, sad, surprise, and others. We consider only the utterances with majority agreement (i. e., at least two out of three evaluators assigned the same emotion label) over the emotion classes of angry, happy (combined with the \"excited\" category), neutral and sad, which results in 5,531 utterances in total.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Setup",
      "text": "In our experiment, the size of each speech segment is set to 32 frames, i. e., the total length of a segment is 10 ms × 32 + (25 -10) ms = 335 ms, shifting 60 ms each time. In this way, we collected approximately 200,000 segments for the CA-SIA corpus and 300,000 segments for the IEMOCAP database, respectively. Moreover, since the input length for our attention neural networks has to be equal for all samples, we heuristically set the maximal length for each speech utterance to the average duration of each dataset, i. e., 2.07 s for CASIA and 4.55 s for IEMOCAP, respectively. Longer speech utterances are cut at the maximal length, and shorter ones are padded with zeros.\n\nThe architecture of the CNN model is similar to the SegCNN model as used in our previous work  [18] . The only change we made was to the last three FC layers, i. e., {256, 64, K} units, respectively, where 64 and K correspond to the dimension of segment embeddings and the number of possible emotions, respectively. In the training stage, for both CNN and attention neural networks, ADAM  [19]  optimizer with the default setting in Tensorflow  [20]  was used, with an initial learning rate of 0.001 and an exponential decay scheme with a rate of 0.8 every two epochs. The batch size was set to 128. Early stopping with patience of 3 epochs was utilized to mitigate an overfitting problem.\n\nFor the CASIA corpus, we perform leave-one-fold-out tenfold cross-validation experiments. For the IEMOCAP database, the leave-one-session-out five-fold cross-validation method is carried out. For both datasets, a second cross-validation is performed since we need to utilize the segment-level results to train the attention neural networks. The results are presented in terms of unweighted accuracy (UA).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Baseline Systems",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Maxout Segment Embedding (Cnn-Max-Rf)",
      "text": "In the CNN-MAX-RF baseline, the maxout pooling is directly applied on the segment embedding X ∈ R T ×M :\n\nA Random Forest (RF) is then used to make the utterance-level prediction based on the resultant utterance representation U.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Averaging Segment Embedding (Cnn-Avg-Rf)",
      "text": "The CNN-AVG-RF baseline is similar to the above CNN-MAX-RF baseline. The only difference is that the maxout pooling in the CNN-MAX-RF baseline is replaced by an average pooling in the CNN-AVG-RF baseline.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Maxout Pooling (Cnn-Mp)",
      "text": "In the CNN-MP baseline, the maxout pooling is applied on the instance-level prediction f (h) ∈ [0, 1] K across a certain speech utterance to obtain the bag-level prediction:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods For Comparison Ua [%]",
      "text": "ELM-Decision Tree  [21]  89.60 DNN-HMM  [22]  91.32 LSTM-TF-Att.  [23]  92. Similarly, for the CNN-AP baseline, the average pooling is applied on the instance-level prediction f (h) ∈ [0, 1] K across a particular speech utterance to obtain the bag-level prediction:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "Tables 1-2 show the experimental results on the two mentioned emotion corpora, respectively. The following can be seen: (1) our baseline systems achieved respectable results on both datasets, which proved the effectiveness of the MIL-based framework;\n\n(2) the last two baselines (i. e., the CNN-MP baseline and the CNN-AP baseline) consistently outperformed the first two baselines (i. e., the CNN-MAX-RF baseline and the CNN-AVG-RF baseline). This performance gain might derive from the joint optimization of the aggregation strategy of segment-level representations and the utterance-level decision making of the last two baselines; (3) the attention-based methods substantially augmented the performance of the baseline systems overall. This is mainly attributed to the effectiveness of the attention modeling; (4) due to the positive combination of different attention modules, the decision-level multiple attention modeling achieved noticeably better performance than the decision-level single attention modeling on both datasets; (5) the feature-level attention modeling outperformed the decisionlevel attention neural networks by a significant margin. This is due to the fact that the dimension of v(h) (i.e., D) can be any value, while the dimension of w(h) is fixed to be the number of emotion classes K. With an increase in the dimension of v(h), the capacity of feature-level attention neural networks is increased; and (6) for the CASIA corpus, our feature-level attention-based system achieved the highest recognition accuracy of 95.32%, establishing a new benchmark (to the best of our knowledge). For the IEMOCAP database, which might constitute a more challenging dataset, our methods also achieved competitive results. Figure  4  shows the confusion matrices obtained using feature-level attention modeling on both datasets, respectively.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed to combine multiple instance learning with attention neural networks for better modeling of categorical speech emotion recognition. Three attention-based neural network models were investigated and compared. Experimental results on two well-known emotion corpora showed competitive outcomes. Since we herein blindly used all segments to train the segment-level classifier, it is anticipated with proper segment selection strategy, better results are expected. More advanced neural network architectures and better algorithm optimization will also be investigated in the near future.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the proposed framework for categorical",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates a schematic approach of the proposed",
      "page": 1
    },
    {
      "caption": "Figure 2: An example of CNN outputs for the audio ﬁle",
      "page": 2
    },
    {
      "caption": "Figure 2: shows an example of a probability distri-",
      "page": 2
    },
    {
      "caption": "Figure 3: (a)-(c), respectively. We denote the in-",
      "page": 2
    },
    {
      "caption": "Figure 3: 2.2.1. Decision-level single attention",
      "page": 2
    },
    {
      "caption": "Figure 3: (b)) applied to intermediate neu-",
      "page": 2
    },
    {
      "caption": "Figure 3: (c)), in which the",
      "page": 2
    },
    {
      "caption": "Figure 3: (a) Decision-level single attention neural network; (b) decision-level multiple attention neural network; (c) feature-level",
      "page": 3
    },
    {
      "caption": "Figure 4: Confusion matrices obtained using feature-level attention modeling for: (a) CASIA corpus and (b) IEMOCAP database.",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the confusion matrices ob-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "Abstract",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "Categorical speech emotion recognition is typically performed",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "as a sequence-to-label problem,\ni. e.,\nto determine the discrete",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "emotion label of the input utterance as a whole. One of the main",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "challenges in practice is that most of the existing emotion cor-",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "pora do not give ground truth labels for each segment;\ninstead,",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "we only have labels for whole utterances. To extract segment-",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "level emotional information from such weakly labeled emotion",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "corpora, we propose using multiple instance learning (MIL)",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "to learn segment embeddings in a weakly supervised manner.",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "Also,\nfor a sufﬁciently long utterance, not all of the segments",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "contain relevant emotional\ninformation.\nIn this\nregard,\nthree",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "Figure 1: Illustration of the proposed framework for categorical"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "attention-based neural network models are then applied to the",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "speech emotion recognition."
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "learned segment embeddings to attend the most salient part of",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "a speech utterance. Experiments on the CASIA corpus and the",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "IEMOCAP database show better or highly competitive results",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "than other state-of-the-art approaches.",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "recognition, each utterance is treated as a bag, and segments"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "Index Terms:\ncategorical\nspeech emotion recognition, weak",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "within the utterance as instances. One main feature of this work"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "labeling, multiple instance learning, attention modeling",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "is the application of deep learning of feature representation in"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "the MIL framework to learn segment embeddings from weak"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "1.\nIntroduction",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "labels at utterance level.\nCompared to the raw features such"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "as MFCC, energy, or pitch,\nthe learned segment embeddings"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "Automatic speech emotion recognition (ASER) aims to decode",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "are more tied to the task of interest,\nthus naturally highlighting"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "emotional content from audio signals.\nIt has constituted an ac-",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "salient portions of the data, which we conjecture would offer an"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "tive research topic in the ﬁeld of human-machine interaction",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "advantage in the ﬁnal classiﬁcation."
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "(HCI). Detection of lies, monitoring of call centers, and medical",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "A key question is how to enable a deep learning model\nto"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "diagnoses are also considered as promising application scenar-",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "identify and focus on the most salient parts of a speech utterance"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "ios of speech emotion recognition.",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "when making an utterance-level decision with the learned seg-"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "Categorical speech emotion recognition at utterance level",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "ment embeddings. In this regard, attention neural network mod-"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "can be formulated as a sequence-to-label problem. The input",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "els are investigated. The key idea behind the attention mecha-"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "utterance is divided into a sequence of acoustic segments, and",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "nism is to align the input-output sequences such that, in the de-"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "the output\nis a single label of emotion type. A few previous",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "coding phase, the major contribution is from the corresponding"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "studies explored the use of segment units for categorical speech",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "encoded information.\nIn contrast,\nthe effect of irrelevant ones"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "emotion recognition and demonstrated that combining segment-",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "is minimized.\nIn this work, attention modeling is expected to"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "level prediction results led to superior performance [1, 2, 3].",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "facilitate a structurally meaningful composition of the utterance"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "These prior works were mostly based on conventional models,",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "representation from the learned emotionally relevant segment"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "such as support vector machine (SVM) and K-nearest neighbors",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "embeddings.\nThis is the ﬁrst attempt\nto combine MIL-based"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "(K-NN),\nfor segment classiﬁcation.\nIn this paper, a convolu-",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "deep learning of segment embedding with attention modeling"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "tional neural network (CNN) is applied to extract emotionally",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "for categorical speech emotion recognition."
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "relevant\nfeatures,\ni. e.,\nto learn emotion relevant segment em-",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "beddings. Also, most of\nthe existing emotion corpora do not",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "provide ground truth labels at segment level; instead, they only",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "2. Methodology"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "have labels for whole utterances. A viable solution is to learn",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": ""
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "local concepts from global annotations, which is the main idea",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "Figure\n1\nillustrates\na\nschematic\napproach\nof\nthe\nproposed"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "of multiple instance learning (MIL) [4, 5]. MIL has been suc-",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "method. It comprises a CNN model trained to learn emotionally"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "cessfully applied to sound event detection (SED) [6, 7], speech",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "salient segment embeddings from log-Mel ﬁlterbank features of"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "recognition [8], and image analysis [5].\nIn the MIL problem",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "individual segments. The learned segment embeddings are then"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "statement,\nthe training set contains labeled bags that comprise",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "used as inputs for utterance-level emotion recognition, which"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "many unlabeled instances, and the task is to predict\nthe labels",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "is achieved by a dense-layer neural network implemented with"
        },
        {
          "maoshuiyang@link.cuhk.edu.hk, {pcching,": "of unseen bags and instances. For categorical speech emotion",
          "cckuo@sipi.usc.edu\ntanlee}@ee.cuhk.edu.hk,": "various attention mechanisms."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2. Attention-based neural networks": "Attention neural networks assume that\nthe bag-level prediction"
        },
        {
          "2.2. Attention-based neural networks": "can be constructed as a weighted sum of the instance-level pre-"
        },
        {
          "2.2. Attention-based neural networks": "dictions. Herein,\nthree attention-based neural network models"
        },
        {
          "2.2. Attention-based neural networks": "are investigated and compared,\ni.e., decision-level single atten-"
        },
        {
          "2.2. Attention-based neural networks": "tion (D-Single-Att.)\n[13], decision-level multiple attention (D-"
        },
        {
          "2.2. Attention-based neural networks": "Multi-Att.)\n[14] and feature-level attention (Feature-Att.)\n[15],"
        },
        {
          "2.2. Attention-based neural networks": "as\nshown in Figure 3(a)-(c),\nrespectively. We denote the in-"
        },
        {
          "2.2. Attention-based neural networks": "put segment embeddings within a certain speech utterance as"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "X ∈ RT ×M , where T is the number of segments and M repre-"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "sents the dimension of segment embeddings. The output of the"
        },
        {
          "2.2. Attention-based neural networks": "second fully-connected (FC) layer is denoted as h, which has a"
        },
        {
          "2.2. Attention-based neural networks": "dimension of 120, i. e., H is set to 120 in Figure 3."
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "2.2.1. Decision-level single attention"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "In the decision-level single attention model\n(as shown in Fig-"
        },
        {
          "2.2. Attention-based neural networks": "ure 3(a)), an attention function is applied on the predictions of"
        },
        {
          "2.2. Attention-based neural networks": "the instances to obtain the bag-level prediction:"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "(cid:88) h\n(2)\nw(h)kf (h)k\nF (B)k ="
        },
        {
          "2.2. Attention-based neural networks": "∈B"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "where k denotes\nthe k-th emotion class of\nthe instance-level"
        },
        {
          "2.2. Attention-based neural networks": "prediction f (h) ∈ [0, 1]K and the bag-level prediction F (B) ∈"
        },
        {
          "2.2. Attention-based neural networks": "[0, 1]K , and w(h)k ∈ [0, 1]\nis a weight of f (h)k that we refer"
        },
        {
          "2.2. Attention-based neural networks": "to as a decision-level attention function:"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "(cid:88) h\n(3)\ns(h)k\nw(h)k = s(h)k/"
        },
        {
          "2.2. Attention-based neural networks": "∈B"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "where s(.) can be any non-negative function (i.e., Softmax non-"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "linearity) to ensure that attention w(.) is normalized. Both the"
        },
        {
          "2.2. Attention-based neural networks": "attention function w(.) and the instance-level classiﬁer f (.) de-"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "pend on a set of learnable parameters."
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "2.2.2. Decision-level multiple attention"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "The decision-level multiple attention model\nis an extension of"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "the above decision-level single attention model.\nIt consists of"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "several single attention modules (we herein use two attention"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "modules, as shown in Figure 3(b)) applied to intermediate neu-"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "ral network layers. The outputs of these attention modules are"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "concatenated."
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "2.2.3. Feature-level attention"
        },
        {
          "2.2. Attention-based neural networks": ""
        },
        {
          "2.2. Attention-based neural networks": "The limitation of the above decision-level attention neural net-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "attention neural network.\n(◦: Hadamard product; (cid:80): element-wise summation; T: length of",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "input sequence; M: dimension of\ninput"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "bottleneck features; H: dimension of FC layer; K: number of emotion classes; D: dimension of feature-level attention function.)",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "where u(.) can be any non-negative function (i.e., Sigmoid non-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "duration of each dataset,\ni. e., 2.07 s for CASIA and 4.55 s for"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "linearity) to ensure that attention v(.) is normalized. Both the",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "IEMOCAP,\nrespectively.\nLonger speech utterances are cut at"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "attention function v(.) and the instance-level\nfeature mapping",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "the maximal length, and shorter ones are padded with zeros."
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "function q(.) depend on a set of learnable parameters. The pre-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "The\narchitecture\nof\nthe CNN model\nis\nsimilar\nto\nthe"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "diction of a bag B can then be obtained by classifying the bag-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "SegCNN model\nas\nused\nin\nour\nprevious work\n[18].\nThe"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "level representation U as follows:",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "only change we made was\nto the last\nthree FC layers,\ni. e.,"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "{256, 64, K} units, respectively, where 64 and K correspond to"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "F (B) = g(U)\n(6)",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "the dimension of segment embeddings and the number of possi-"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "ble emotions, respectively.\nIn the training stage, for both CNN"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "where g(.) is the ﬁnal classiﬁer that corresponds to the last neu-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "and attention neural networks, ADAM [19] optimizer with the"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "ral network layer.",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "default setting in Tensorﬂow [20] was used, with an initial learn-"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "ing rate of 0.001 and an exponential decay scheme with a rate"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "3. Emotion Corpora",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "of 0.8 every two epochs. The batch size was set\nto 128. Early"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "stopping with patience of 3 epochs was utilized to mitigate an"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "Two different emotion corpora are used to evaluate the validity",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "overﬁtting problem."
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "of\nthe proposed method, namely, a Chinese emotional corpus",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "For the CASIA corpus, we perform leave-one-fold-out ten-"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "(CASIA) [16] and an English emotional database (IEMOCAP)",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "fold cross-validation experiments. For the IEMOCAP database,"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "[17], which have been extensively evaluated in the literature.",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "the leave-one-session-out ﬁve-fold cross-validation method is"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "Speciﬁcally,\nthe CASIA corpus [16] contains 9,600 utter-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "carried out. For both datasets, a second cross-validation is per-"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "ances that are simulated by four subjects (two males and two fe-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "formed since we need to utilize the segment-level results to train"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "males) in six different emotional states, i. e., angry, fear, happy,",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "the attention neural networks. The results are presented in terms"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "neutral,\nsad,\nand surprise.\nIn our experiments, we only use",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "of unweighted accuracy (UA)."
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "7,200 utterances\nthat correspond to 300 linguistically neutral",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "sentences with the same statements. All of\nthe emotion cate-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "4.2. Baseline systems"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "gories are selected.",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "4.2.1. Maxout segment embedding (CNN-MAX-RF)"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "The IEMOCAP database [17] was collected using motion",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "capture\nand audio/video recording over ﬁve dyadic\nsessions",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "In the CNN-MAX-RF baseline,\nthe maxout pooling is directly"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "with 10 subjects. At least three evaluators annotated each utter-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "applied on the segment embedding X ∈ RT ×M :"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "ance in the database with the categorical emotion labels chosen",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "from the set:\nangry, disgusting, excited,\nfear,\nfrustrate, happy,",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "(7)\nUm = max\n{Xm}"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "0≤t≤T"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "neutral, sad, surprise, and others. We consider only the utter-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "ances with majority agreement\n(i. e., at\nleast\ntwo out of\nthree",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "A Random Forest (RF) is then used to make the utterance-level"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "evaluators assigned the same emotion label) over\nthe emotion",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "prediction based on the resultant utterance representation U."
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "classes of angry, happy (combined with the “excited” category),",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "neutral and sad, which results in 5,531 utterances in total.",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "4.2.2. Averaging segment embedding (CNN-AVG-RF)"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "The CNN-AVG-RF baseline is similar to the above CNN-MAX-"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "4. Experiments",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "RF baseline. The only difference is that\nthe maxout pooling in"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "4.1.\nSetup",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "the CNN-MAX-RF baseline is replaced by an average pooling"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "in the CNN-AVG-RF baseline."
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "In our experiment,\nthe size of each speech segment\nis\nset\nto",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "32 frames,\ni. e.,\nthe total\nlength of a segment\nis 10 ms × 32",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "4.2.3. Maxout pooling (CNN-MP)"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "+ (25 - 10) ms = 335 ms,\nshifting 60 ms each time.\nIn this",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "way, we collected approximately 200,000 segments for the CA-",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "In the CNN-MP baseline, the maxout pooling is applied on the"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "SIA corpus and 300,000 segments for the IEMOCAP database,",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "instance-level prediction f (h) ∈ [0, 1]K across a certain speech"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "respectively. Moreover, since the input\nlength for our attention",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "utterance to obtain the bag-level prediction:"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "neural networks has to be equal for all samples, we heuristically",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "(8)\nF (B)k = max\n{f (h)k}"
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "set the maximal length for each speech utterance to the average",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": ""
        },
        {
          "(a) Decision-level single attention neural network;\nFigure 3:": "",
          "(b) decision-level multiple attention neural network;\n(c)\nfeature-level": "h∈B"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "Methods for comparison",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "UA [%]"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "ELM-Decision Tree [21]",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "59.40"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "DNN-HMM [22]",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "62.23"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "LSTM-TF-Att. [23]",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "63.90"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "CNN-MAX-RF (baseline)",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "61.08"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "CNN-AVG-RF (baseline)",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "61.40"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "CNN-MP (baseline)",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "62.11"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "CNN-AP (baseline)",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "62.74"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "CNN-D-Single-Att. (ours)",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "63.34"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "CNN-D-Multi-Att. (ours)",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "65.82"
        },
        {
          "Table 1: Comparison of UAs on the CASIA corpus.": "CNN-Feature-Att. (ours)",
          "Table 2: Comparison of UAs on the IEMOCAP database.": "66.74"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "95.32\nCNN-Feature-Att. (ours)",
          "65.82\nCNN-D-Multi-Att. (ours)": "66.74\nCNN-Feature-Att. (ours)"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "4.2.4. Average pooling (CNN-AP)",
          "65.82\nCNN-D-Multi-Att. (ours)": "the feature-level attention modeling outperformed the decision-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "level attention neural networks by a signiﬁcant margin.\nThis"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "Similarly, for the CNN-AP baseline, the average pooling is ap-",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "is due to the fact\nthat\nthe dimension of v(h) (i.e., D) can be"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "plied on the instance-level prediction f (h) ∈ [0, 1]K across a",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "any value, while the dimension of w(h) is ﬁxed to be the num-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "particular speech utterance to obtain the bag-level prediction:",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "ber of emotion classes K. With an increase in the dimension"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "of v(h),\nthe capacity of feature-level attention neural networks"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "(9)\nF (B)k = mean\n{f (h)k}",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "h∈B",
          "65.82\nCNN-D-Multi-Att. (ours)": "is increased; and (6)\nfor\nthe CASIA corpus, our\nfeature-level"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "attention-based system achieved the highest\nrecognition accu-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "racy of 95.32%, establishing a new benchmark (to the best of"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "4.3. Results and analysis",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "our knowledge). For the IEMOCAP database, which might con-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "Tables 1-2 show the\nexperimental\nresults on the\ntwo men-",
          "65.82\nCNN-D-Multi-Att. (ours)": "stitute a more challenging dataset, our methods also achieved"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "tioned emotion corpora,\nrespectively.\nThe following can be",
          "65.82\nCNN-D-Multi-Att. (ours)": "competitive results. Figure 4 shows the confusion matrices ob-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "seen:\n(1) our baseline systems achieved respectable results on",
          "65.82\nCNN-D-Multi-Att. (ours)": "tained using feature-level attention modeling on both datasets,"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "both datasets, which proved the effectiveness of the MIL-based",
          "65.82\nCNN-D-Multi-Att. (ours)": "respectively."
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "framework; (2) the last\ntwo baselines (i. e.,\nthe CNN-MP base-",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "line and the CNN-AP baseline) consistently outperformed the",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "5. Conclusion"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "ﬁrst\ntwo baselines (i. e.,\nthe CNN-MAX-RF baseline and the",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "In this paper, we proposed to combine multiple instance learn-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "CNN-AVG-RF baseline).\nThis performance gain might de-",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "ing with attention neural networks for better modeling of cate-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "rive from the joint optimization of the aggregation strategy of",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "gorical speech emotion recognition. Three attention-based neu-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "segment-level\nrepresentations and the utterance-level decision",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "ral network models were investigated and compared.\nExper-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "making of the last\ntwo baselines; (3) the attention-based meth-",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "imental\nresults on two well-known emotion corpora showed"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "ods\nsubstantially augmented the performance of\nthe baseline",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "competitive outcomes.\nSince we herein blindly used all seg-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "systems overall. This is mainly attributed to the effectiveness",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "ments to train the segment-level classiﬁer, it is anticipated with"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "of the attention modeling;\n(4) due to the positive combination",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "proper segment selection strategy, better\nresults are expected."
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "of different attention modules, the decision-level multiple atten-",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "More advanced neural network architectures and better algo-"
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "tion modeling achieved noticeably better performance than the",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "",
          "65.82\nCNN-D-Multi-Att. (ours)": "rithm optimization will also be investigated in the near future."
        },
        {
          "94.57\nCNN-D-Multi-Att. (ours)": "decision-level single attention modeling on both datasets;\n(5)",
          "65.82\nCNN-D-Multi-Att. (ours)": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pp. 1339–1343.": "[9]\nZ. Huang, M. Dong, Q. Mao,\nand Y. Zhan,\n“Speech emotion"
        },
        {
          "pp. 1339–1343.": "recognition using cnn,” in Proc. ACM International Conference"
        },
        {
          "pp. 1339–1343.": "on Multimedia, 2014, pp. 801–804."
        },
        {
          "pp. 1339–1343.": "[10] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient emotion recogni-"
        },
        {
          "pp. 1339–1343.": "tion from speech using deep learning on spectrograms.” in Proc."
        },
        {
          "pp. 1339–1343.": "INTERSPEECH, 2017, pp. 1089–1093."
        },
        {
          "pp. 1339–1343.": "[11]\nL. Zhang, L. Wang,\nJ. Dang, L. Guo, and H. Guan, “Convolu-"
        },
        {
          "pp. 1339–1343.": "tional neural network with spectrogram and perceptual\nfeatures"
        },
        {
          "pp. 1339–1343.": "for speech emotion recognition,” in Proc. ICONIP, 2018, pp. 62–"
        },
        {
          "pp. 1339–1343.": "71."
        },
        {
          "pp. 1339–1343.": "[12] R. C. Bunescu and R. J. Mooney, “Multiple instance learning for"
        },
        {
          "pp. 1339–1343.": "sparse positive bags,” in Proc. ICML, 2007, pp. 105–112."
        },
        {
          "pp. 1339–1343.": "[13] Q. Kong, Y. Xu, W. Wang, and M. D. Plumbley, “Audio set clas-"
        },
        {
          "pp. 1339–1343.": "siﬁcation with attention model: A probabilistic perspective,” in"
        },
        {
          "pp. 1339–1343.": "Proc. ICASSP, 2018, pp. 316–320."
        },
        {
          "pp. 1339–1343.": "[14] C. Yu, K. S. Barsim, Q. Kong,\nand B. Yang,\n“Multi-level at-"
        },
        {
          "pp. 1339–1343.": "tention model\nfor weakly\nsupervised\naudio\nclassiﬁcation,”\nin"
        },
        {
          "pp. 1339–1343.": "arXiv:1803.02353, 2018."
        },
        {
          "pp. 1339–1343.": "[15] Q. Kong, C. Yu, Y. Xu, T.\nIqbal, W. Wang, and M. D. Plumb-"
        },
        {
          "pp. 1339–1343.": "ley, “Weakly labelled audioset\ntagging with attention neural net-"
        },
        {
          "pp. 1339–1343.": "IEEE/ACM Transactions on Audio,\nSpeech,\nand Lan-\nworks,”"
        },
        {
          "pp. 1339–1343.": "guage Processing, vol. 27, no. 11, pp. 1791–1802, 2019."
        },
        {
          "pp. 1339–1343.": "[16]\nJ. Tao, F. Liu, M. Zhang, and H. Jia, “Design of speech corpus for"
        },
        {
          "pp. 1339–1343.": "the 4th Workshop on Blizzard\nmandarin text\nto speech,” in Proc."
        },
        {
          "pp. 1339–1343.": "Challenge, 2005."
        },
        {
          "pp. 1339–1343.": "[17] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "pp. 1339–1343.": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:"
        },
        {
          "pp. 1339–1343.": "Interactive emotional dyadic motion capture database,” Language"
        },
        {
          "pp. 1339–1343.": "resources and evaluation, vol. 42, no. 4, p. 335, 2008."
        },
        {
          "pp. 1339–1343.": "[18]\nS. Mao, P. C. Ching,\nand T. Lee,\n“Deep learning of\nsegment-"
        },
        {
          "pp. 1339–1343.": "level\nfeature representation with multiple instance learning for"
        },
        {
          "pp. 1339–1343.": "INTER-\nutterance-level\nspeech emotion recognition,”\nin Proc."
        },
        {
          "pp. 1339–1343.": "SPEECH, 2019, pp. 1686–1690."
        },
        {
          "pp. 1339–1343.": "[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-"
        },
        {
          "pp. 1339–1343.": "mization,” arXiv preprint arXiv:1412.6980, 2014."
        },
        {
          "pp. 1339–1343.": "[20] M. Abadi et al., “Tensorﬂow: A system for large-scale machine"
        },
        {
          "pp. 1339–1343.": "learning,” in Proc. OSDI, 2016, pp. 265–283."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "Tan, “Speech emotion recognition based on feature selection and"
        },
        {
          "6. References": "[1]\nJ. H. Jeon, R. Xia, and Y. Liu, “Sentence level emotion recognition",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "extreme learning machine decision tree,” Neurocomputing, vol."
        },
        {
          "6. References": "based on decisions from subsentence segments,” in Proc. ICASSP,",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "273, pp. 271–280, 2018."
        },
        {
          "6. References": "2011, pp. 4940–4943.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "[22]\nS. Mao, D. Tao, G. Zhang, P. C. Ching, and T. Lee, “Revisiting"
        },
        {
          "6. References": "[2] M. T. Shami and M. S. Kamel, “Segment-based approach to the",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "hidden markov models for speech emotion recognition,” in Proc."
        },
        {
          "6. References": "recognition of emotions in speech,” in Proc. ICME, 2005, p. 4.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "ICASSP, 2019, pp. 6715–6719."
        },
        {
          "6. References": "[3] B. Schuller\nand G. Rigoll,\n“Timing\nlevels\nin\nsegment-based",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "[23] Y. Xie, R. Liang, Z. Liang, C. Huang, C. Zou, and B. Schuller,"
        },
        {
          "6. References": "speech emotion recognition,” in Proc. INTERSPEECH, 2006, pp.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "“Speech\nemotion\nclassiﬁcation\nusing\nattention-based\nlstm,”"
        },
        {
          "6. References": "1818–1821.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "IEEE/ACM Transactions on Audio, Speech, and Language Pro-"
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "cessing, vol. 27, no. 11, pp. 1675–1685, 2019."
        },
        {
          "6. References": "[4]\nJ. Amores, “Multiple instance classiﬁcation: Review,\ntaxonomy",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "and comparative study,” Artiﬁcial Intelligence, vol. 201, pp. 81–",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "[24] Y. Zhang, J. Du, Z. Wang, J. Zhang, and Y. Tu, “Attention based"
        },
        {
          "6. References": "105, 2013.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "fully convolutional network for speech emotion recognition,” in"
        },
        {
          "6. References": "",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": "Proc. APSIPA ASC, 2018, pp. 1771–1775."
        },
        {
          "6. References": "[5] Y. Xu, T. Mo, Q. Feng, P. Zhong, M. Lai, I. Eric, and C. Chang,",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "“Deep learning of\nfeature representation with multiple instance",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "learning for medical image analysis,” in Proc. ICASSP, 2014, pp.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "1626–1630.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[6]\nT.-W. Su, J.-Y. Liu, and Y.-H. Yang, “Weakly-supervised audio",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "event detection using event-speciﬁc gaussian ﬁlters and fully con-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "volutional networks,” in Proc. ICASSP, 2017, pp. 791–795.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[7] A. Kumar and B. Raj, “Audio event detection using weakly la-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "beled data,” in Proc. ACM International Conference on Multime-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "dia, 2016, pp. 1038–1047.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[8] Y. Wang, J. Li, and F. Metze, “Comparing the max and noisy-or",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "pooling functions in multiple instance learning for weakly super-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "vised sequence learning tasks,” in Proc.\nINTERSPEECH, 2018,",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "pp. 1339–1343.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[9]\nZ. Huang, M. Dong, Q. Mao,\nand Y. Zhan,\n“Speech emotion",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "recognition using cnn,” in Proc. ACM International Conference",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "on Multimedia, 2014, pp. 801–804.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[10] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient emotion recogni-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "tion from speech using deep learning on spectrograms.” in Proc.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "INTERSPEECH, 2017, pp. 1089–1093.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[11]\nL. Zhang, L. Wang,\nJ. Dang, L. Guo, and H. Guan, “Convolu-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "tional neural network with spectrogram and perceptual\nfeatures",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "for speech emotion recognition,” in Proc. ICONIP, 2018, pp. 62–",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "71.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[12] R. C. Bunescu and R. J. Mooney, “Multiple instance learning for",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "sparse positive bags,” in Proc. ICML, 2007, pp. 105–112.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[13] Q. Kong, Y. Xu, W. Wang, and M. D. Plumbley, “Audio set clas-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "siﬁcation with attention model: A probabilistic perspective,” in",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "Proc. ICASSP, 2018, pp. 316–320.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[14] C. Yu, K. S. Barsim, Q. Kong,\nand B. Yang,\n“Multi-level at-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "tention model\nfor weakly\nsupervised\naudio\nclassiﬁcation,”\nin",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "arXiv:1803.02353, 2018.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[15] Q. Kong, C. Yu, Y. Xu, T.\nIqbal, W. Wang, and M. D. Plumb-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "ley, “Weakly labelled audioset\ntagging with attention neural net-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "IEEE/ACM Transactions on Audio,\nSpeech,\nand Lan-\nworks,”",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "guage Processing, vol. 27, no. 11, pp. 1791–1802, 2019.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[16]\nJ. Tao, F. Liu, M. Zhang, and H. Jia, “Design of speech corpus for",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "the 4th Workshop on Blizzard\nmandarin text\nto speech,” in Proc.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "Challenge, 2005.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[17] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "Interactive emotional dyadic motion capture database,” Language",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "resources and evaluation, vol. 42, no. 4, p. 335, 2008.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[18]\nS. Mao, P. C. Ching,\nand T. Lee,\n“Deep learning of\nsegment-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "level\nfeature representation with multiple instance learning for",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "INTER-\nutterance-level\nspeech emotion recognition,”\nin Proc.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "SPEECH, 2019, pp. 1686–1690.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "mization,” arXiv preprint arXiv:1412.6980, 2014.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "[20] M. Abadi et al., “Tensorﬂow: A system for large-scale machine",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        },
        {
          "6. References": "learning,” in Proc. OSDI, 2016, pp. 265–283.",
          "[21]\nZ.-T. Liu, M. Wu, W.-H. Cao,\nJ.-W. Mao,\nJ.-P. Xu, and G.-Z.": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Sentence level emotion recognition based on decisions from subsentence segments",
      "authors": [
        "J Jeon",
        "R Xia",
        "Y Liu"
      ],
      "year": "2011",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "3",
      "title": "Segment-based approach to the recognition of emotions in speech",
      "authors": [
        "M Shami",
        "M Kamel"
      ],
      "year": "2005",
      "venue": "Proc. ICME"
    },
    {
      "citation_id": "4",
      "title": "Timing levels in segment-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2006",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "5",
      "title": "Multiple instance classification: Review, taxonomy and comparative study",
      "authors": [
        "J Amores"
      ],
      "year": "2013",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Deep learning of feature representation with multiple instance learning for medical image analysis",
      "authors": [
        "Y Xu",
        "T Mo",
        "Q Feng",
        "P Zhong",
        "M Lai",
        "I Eric",
        "C Chang"
      ],
      "year": "2014",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "7",
      "title": "Weakly-supervised audio event detection using event-specific gaussian filters and fully convolutional networks",
      "authors": [
        "T.-W Su",
        "J.-Y Liu",
        "Y.-H Yang"
      ],
      "year": "2017",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Audio event detection using weakly labeled data",
      "authors": [
        "A Kumar",
        "B Raj"
      ],
      "year": "2016",
      "venue": "Proc. ACM International Conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Comparing the max and noisy-or pooling functions in multiple instance learning for weakly supervised sequence learning tasks",
      "authors": [
        "Y Wang",
        "J Li",
        "F Metze"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proc. ACM International Conference on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "12",
      "title": "Convolutional neural network with spectrogram and perceptual features for speech emotion recognition",
      "authors": [
        "L Zhang",
        "L Wang",
        "J Dang",
        "L Guo",
        "H Guan"
      ],
      "year": "2018",
      "venue": "Proc. ICONIP"
    },
    {
      "citation_id": "13",
      "title": "Multiple instance learning for sparse positive bags",
      "authors": [
        "R Bunescu",
        "R Mooney"
      ],
      "year": "2007",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "14",
      "title": "Audio set classification with attention model: A probabilistic perspective",
      "authors": [
        "Q Kong",
        "Y Xu",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Multi-level attention model for weakly supervised audio classification",
      "authors": [
        "C Yu",
        "K Barsim",
        "Q Kong",
        "B Yang"
      ],
      "year": "2018",
      "venue": "Multi-level attention model for weakly supervised audio classification",
      "arxiv": "arXiv:1803.02353"
    },
    {
      "citation_id": "16",
      "title": "Weakly labelled audioset tagging with attention neural networks",
      "authors": [
        "Q Kong",
        "C Yu",
        "Y Xu",
        "T Iqbal",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Tao",
        "F Liu",
        "M Zhang",
        "H Jia"
      ],
      "year": "2005",
      "venue": "Proc. the 4th Workshop on Blizzard Challenge"
    },
    {
      "citation_id": "18",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "19",
      "title": "Deep learning of segmentlevel feature representation with multiple instance learning for utterance-level speech emotion recognition",
      "authors": [
        "S Mao",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "Proc. INTER-SPEECH"
    },
    {
      "citation_id": "20",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "21",
      "title": "Tensorflow: A system for large-scale machine learning",
      "authors": [
        "M Abadi"
      ],
      "year": "2016",
      "venue": "Proc. OSDI"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition based on feature selection and extreme learning machine decision tree",
      "authors": [
        "Z.-T Liu",
        "M Wu",
        "W.-H Cao",
        "J.-W Mao",
        "J.-P Xu",
        "G.-Z Tan"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "23",
      "title": "Revisiting hidden markov models for speech emotion recognition",
      "authors": [
        "S Mao",
        "D Tao",
        "G Zhang",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion classification using attention-based lstm",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "C Huang",
        "C Zou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "Proc. APSIPA ASC"
    }
  ]
}