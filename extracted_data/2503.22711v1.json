{
  "paper_id": "2503.22711v1",
  "title": "Modeling Speech Emotion With Label Variance And Analyzing Performance Across Speakers And Unseen Acoustic Conditions",
  "published": "2025-03-24T06:13:27Z",
  "authors": [
    "Vikramjit Mitra",
    "Amrit Romana",
    "Dung T. Tran",
    "Erdrin Azemi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Spontaneous speech emotion data usually contain perceptual grades where graders assign emotion score after listening to the speech files. Such perceptual grades introduce uncertainty in labels due to grader opinion variation. Grader variation is addressed by using consensus grades as groundtruth, where the emotion with the highest vote is selected. Consensus grades fail to consider ambiguous instances where a speech sample may contain multiple emotions, as captured through grader opinion uncertainty. We demonstrate that using the probability density function of the emotion grades as targets instead of the commonly used consensus grades, provide better performance on benchmark evaluation sets compared to results reported in the literature. We show that a saliency driven foundation model (FM) representation selection helps to train a state-of-the-art speech emotion model for both dimensional and categorical emotion recognition. Comparing representations obtained from different FMs, we observed that focusing on overall test-set performance can be deceiving, as it fails to reveal the models generalization capacity across speakers and gender. We demonstrate that performance evaluation across multiple test-sets and performance analysis across gender and speakers are useful in assessing usefulness of emotion models. Finally, we demonstrate that label uncertainty and data-skew pose a challenge to model evaluation, where instead of using the best hypothesis, it is useful to consider the 2-or 3-best hypotheses.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Speech-based emotion models aim to estimate the emotional state of a speaker from their speech utterances. Real-time speech-emotion models can help to improve human-computer interaction  Mitra et al. (2019) ;  Kowtha et al. (2020)  and facilitate health applications  Stasak et al. (2016) ;  Niu et al. (2023) ;  Provost et al. (2024) . Speech emotion research has pursued two distinct definitions of emotion: (1) categorical emotions: for example, fear, anger, joy, sadness, disgust, and surprise  Ekman (1992) , and (2) dimensional emotions: that represent emotion using a 3-dimensional model of Valence, Activation and Dominance  Posner et al. (2005) . Early studies on speech emotion detection focused on acted or elicited emotions  Busso et al. (2008) , however, models trained with acted emotions often fail to generalize for spontaneous emotions  Douglas-Cowie et al. (2005) . Recently, attention has been given to datasets with spontaneous emotions  Mariooryad et al. (2014)  where graders listen to each audio file and assign emotion labels. Such perceptual grading is difficult due to utterances containing mixed, shifting, subtle, or ambiguous emotions. To account for this, Mariooryad et al. have multiple graders review and grade each audio file. Traditionally, researchers addressed label variance by taking the grader consensus  Chou et al. (2024) . However, modeling such variance  Prabhu et al. (2022) ;  Chou et al. (2024) ;  Tavernor et al. (2024)  can be useful to account for audio samples that were perceptually difficult to annotate. In this work, we investigate training models with distributions of grader decisions for categorical emotions, instead of consensus grades, as the target. We hypothesize that modeling label uncertainty can help to improve the model's robustness because consensus grades fail to account for mixed, shifting, subtle, or ambiguous emotions.\n\nRecent studies have shown that pre-trained foundation model (FM) representations are useful for emotion recognition from speech  Srinivasan et al. (2022) ;  Mitra et al. (2022; 2023) . Given that the FMs may not have been trained with emotion labels, the final layer representations may not be optimal for emotion recognition. Earlier studies have investigated intermediate FM representations for various speech tasks  Alain & Bengio (2016) ;  Mitra & Franco (2020) ;  Mitra et al. (2024a) ;  Yang et al. (2024) . In this work, we investigate saliency based FM layer selection for the downstream emotion modeling task. To summarize, in this work, we:\n\n1. Account label uncertainty through the use of categorical emotion pdf as targets.\n\n2. Explore saliency-driven intermediate FM layer representations for emotion recognition.\n\n3. Evaluate performance across speakers, gender and unseen acoustic conditions. We observed that models that provide state-of-the-art (SOTA) results, may not generalize well across speakers and varying acoustic conditions. We found that having a diverse evaluation set along with a diverse evaluation metric is useful for model selection. We found that the traditional 1best hypothesis used in emotion literature may get biased by the training data-skew, in which case 2-or 3-best hypotheses may be useful to account for speech samples containing multiple emotions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "We have used the MSP-Podcast dataset (ver. 1.11)  Mariooryad et al. (2014) ;  Lotfian & Busso (2017)  that contains ≈ 238 hours of speech data spoken by English speakers (N > 1800), consisting of ≈ 152K speaking turns. The speech segments contain single-speaker utterances with a duration of 3 to 11 seconds. The data contain manually assigned valence, activation and dominance scores and categorical emotions (9 categories) from multiple graders. Grader decisions for categorical emotions were converted to a pdf (reflecting the probability of each of the 9 emotions), which was used as the target for our model training. The data split is shown in Table  4  in Appendix A.2. To make our results comparable to  Ghriss et al. (2022) ;  Srinivasan et al. (2022) , we report results on Eval1.6 and Eval1.11 (see Table  4 ). For evaluating model robustness, we have added noise to the MSP test-set at SNR levels 15 dB and 5dB (see Eval 15dB and Eval 5dB in Table  4 , Appendix A.2). We report categorical emotion recognition performance on six emotions: neutral, happy, angry, sad, contempt and surprise. We have used CMU-Mosei,  Zadeh et al. (2018)  and a 5 hour in-house conversational speech data from 85 speakers for cross-corpus speech emotion recognition analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Representations",
      "text": "We explore speech embeddings as features to a TC-GRU model (see Figure  1 ). We use the following pre-trained models to generate those embeddings: (i) HuBERT large  Hsu et al. (2021)  Motivated by  Mitra et al. (2024b; a)  we explore obtaining layer-saliency to obtain the optimal FM layer representation for emotion modeling. Let the N dimensional representation from the k th layer of a FM for an utterance y be represented by a vector\n\nwhere M denotes the sequence length. For a regression task, let the sequence targets be L y , where L y ∈ R D , where the D dimensional vector L denotes the output targets, for each utterance. H y k in eq. 1 is obtained from H y k by taking the mean across all the frames for utterance y. The crosscorrelation based saliency (CCS) of i th dimension of the k th layer is given by:\n\nγ i is the sum of the weighted cross-correlation between the i th dimension and all other dimensions, as shown in eq. 2.\n\nIn our experiments we have used µ CCS,k given in eq. 3 to select salient layers of a pre-trained FM, which is obtained from a randomly sampled 30K utterances in the Train1.11.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "We trained multi-task emotion recognition models with embeddings from HuBERT, WavLM, and Whisper FMs. In addition, we trained a baseline model with mel-filterbank and pitch (MFBF0) feature. In Table  1 , we report dimensional emotion estimation performance obtained from the trained systems and compared them with the state-of-the-art results reported in the literature (see Table  1 ). Note that in Srinivasan et al. (  2022 ) ASR generated transcripts were used, which was not used for the other systems in Table  1 . Finally, we compared categorical emotion recognition performance obtained from the TC-GRU models with respect to results reported in the literature (see Table  2 ).   Next we investigated how these models perform across speakers, where we accumulated model decisions by speaker, and computed the UAR for the categorical emotion predictions. We have used Eval1.11 and Inhouse sets to compare the performance of the models. For performance evaluation across speakers, we introduced a metric: paUAR-X, which measures the percentage of speakers who are above a UAR of X%, where we have used two thresholds: X: 75% and 50%, respectively. Table  3  shows paUAR-75 and paUAR-50 for categorical emotion, obtained across speakers. Note that Tables  1  and 2  show that overall WavLM TC-GRU model performed better than the HuBERT TC-GRU, however table  3  shows that a better system may not necessarily generalize across speakers. In terms of the 1-best hypothesis paUAR-75 and paUAR-50, Whisper TC-GRU model performed better than the others, likely due the fact it was pre-trained with a noisy, more diverse, and larger set of speech. However, even with this best performing model, only 5% and 12% of speakers had UAR above 0.75 for Eval1.11 and Inhouse sets, respectively. In Appendices A.5 and A.6, we explore potential explanations for the speaker-level performance differences including whether gender or emotion label distributions play a role. We find that gender has a significant impact on results, where 7% of female speakers had UAR above 0.75 compared to ≈ 14% of male speakers for the Inhouse evaluation set. This gap illustrates the importance of evaluating model performance at speaker and group levels. Interestingly, even if Tables  1  and 2  show that WavLM TC-GRU model overall performed much better than MFBF0 TC-GRU, their paUAR-75 were comparable for Eval1.11, indicating that the usage of overall metrics while assessing the usefulness of a model can be deceiving. Also note that the speaker level performance obtained from Eval1.11 and the inhouse set was quite different for each of the models investigated, where the performance for Eval1.11 was found to be lower, as it is a harder and larger containing more speakers than the inhouse set (see table 4 in A.2). Note that for Eval1.11, the best model demonstrated an UAR above 0.75 for only ≈ 5% of the speakers. The poor performance across speakers can be attributed to the uncertainty in the labels and the overall skew toward \"neutral\" emotion. For example, in many instances different graders assigned different emotions to the same speech file, which reveals that a speech file can contain a mix-of-emotions due to mixed, shifting, subtle, or ambiguous emotions. Additionally, data skew due to one emotion category being present overwhelmingly in the training set (e.g., \"neutral\") can lead the model to over-estimate that emotion, in which case a 1-best hypothesis may lead to pessimistic results. Appendix A.7 illustrates the relationship between 1-best and 2-best hypothesis, and how by studying both we can obtain better clarity regarding the models generalization capacity. Table  3 , we explored paUAR-X if the target emotion exists within the 2-best or 3-best hypotheses. We find a paUAR-75 of more than 28% can be obtained by considering the 2-best hypothesis and as high as 46% can be obtained with a 3-best hypothesis. These findings indicate that (1) in case of data with uncertain labels and distribution skew, it is helpful to consider multiple model hypothesis and (2) label distribution skew impacts model's generalization capacity across speakers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we demonstrated SOTA results for both dimensional and categorical emotion recognition. The models were found to perform well for unseen datasets (Mosei and Inhouse) and demonstrated reasonable noise robustness. Interestingly, the models failed to generalize across speakers, where we observed that the model performed with an overall UAR of above 0.75 for less than 10% of the speakers. The model offered UAR above 0.5 for ≈ 60% of the speakers. This indicated that using metrics that reflect the overall performance on an eval set may not be prudent, speaker-level and gender-level performance are crucial to assess how well the model will perform across users. We also observed that instead of using the 1-best hypothesis from the model, it is useful to consider 2-best or 3-best hypothesis, as certain utterances may contain multiple emotions, in which case the model may provide more than one likely emotion categories. With 2-best and 3-best hypothesis, we observed that UAR above 0.75 was obtained for > 60% and > 85% of the speakers, respectively. The findings from this study opens the question regarding performance metrics, which can account for co-occurrences of semantically closer emotions, such as \"angry\", \"contempt\", \"disgust\", which may have a higher chance of confusion with each other.\n\nwhere L ccc is the mean of CCC's obtained from each of the N output targets. CCC is defined by:\n\n(5)\n\nwhere µ x and µ y are the means, σ 2 x and σ 2 y are the corresponding variances for the estimated and groundtruth variables, and ρ is the correlation coefficient between them. Neural saliency was used in  Mitra et al. (2024b)  to reduce the number of representations for the downstream task with a goal of model size reduction. \"Saliency\" in this work focuses on layersaliency as outlined in section 3, where the saliency measure was modified to provide a layer-wise collective measure, that informs which transformer layer in the foundation model is more relevant. This measure is particularly important, as given the large number of transformer layers in an FM, it may not be possible to perform layer-wise experimentation of which layer offers the best representation. Layer-wise saliency measure offers a data-driven solution to figure out which layers in the transformer network are better suited for the downstream task, without the need to train downstream models for representations from each individual layer.",
      "page_start": 4,
      "page_end": 7
    },
    {
      "section_name": "A.2 Data Split",
      "text": "We observed that valence is more sensitive to transformer layer representation, compared to activation and dominance (see Figure  2 ). Earlier studies  Chen et al. (2022)    Table  5  shows that representations from emotion-salient layer as compared to the final FM layer, resulted in improvement in emotion recognition performance. It is also interesting to note that relative improvement in valence was higher (> 8% relative) compared to the other dimensional Figure  4  shows performance plotted against emotion distributions for each speaker in Eval1.6. Because UAR is the unweighted average across recall on all emotions, we do not find a strong relationship between UAR and emotion distribution. This suggests UAR is robust to these speaker-level changes and can capture other important factors in speaker-level performance.\n\nFigure  4 : Speaker-level performance (UAR from Whisper TC-GRU) plotted against emotion distributions, for speakers in Eval1.6.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A.7 Relationship Between 1St And 2Nd Best Model Hypotheses",
      "text": "We find that the model's first and second hypotheses show a clear relationship, and that the first hypothesis alone may not fully reflect the model's understanding. Figure  5  illustrates these details, with the samples accurately labeled by the first hypothesis outlined by the horizontal gray bars, and the samples accurately labeled by the second hypothesis outlined by the vertical gray bars. The first hypotheses are highly accurate for happiness and anger, indicated by the white squares within the horizontal gray outlines. However, for most sadness samples, the model identifies neutral as the most likely emotion and sadness as the second most likely emotion, indicated by the white square within the vertical gray outline. Similarly, for surprise samples, the model identifies happiness as the most likely emotion and surprise as the second most likely emotion, where this hierarchy likely results from the closer relationship between happiness and surprise with the former class having more representation in the training data. We also see considerable confusion between contempt, anger, and neutral. When we explore the models second best hypotheses, we find the model correctly detects the overall sentiment but does not distinguish correctly between them. This finding supports our analysis into considering the model's second best hypotheses when determining model predictions.",
      "page_start": 8,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). We use the fol-",
      "page": 2
    },
    {
      "caption": "Figure 1: and the model parameters are described in Appendix A.8. The",
      "page": 3
    },
    {
      "caption": "Figure 1: Multi-task emotion recognition model",
      "page": 3
    },
    {
      "caption": "Figure 2: ). Earlier studies Chen et al. (2022) have found that for WavLM",
      "page": 7
    },
    {
      "caption": "Figure 3: we show how saliency based on individual valence, happy and angry scores vary by",
      "page": 7
    },
    {
      "caption": "Figure 2: Dimensional emotion estimation for different transformer layers in WavLM",
      "page": 8
    },
    {
      "caption": "Figure 3: WavLM layer saliency by valence, happy and angry emotion",
      "page": 8
    },
    {
      "caption": "Figure 4: shows performance plotted against emotion distributions for each speaker in Eval1.6. Be-",
      "page": 10
    },
    {
      "caption": "Figure 4: Speaker-level performance (UAR from Whisper TC-GRU) plotted against emotion distri-",
      "page": 10
    },
    {
      "caption": "Figure 5: illustrates these details,",
      "page": 10
    },
    {
      "caption": "Figure 5: Confusion matrices showing the relationship between 1st and 2nd best model hypotheses",
      "page": 11
    },
    {
      "caption": "Figure 1: Earlier work Mitra et al.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Spontaneous speech emotion data usually contain perceptual grades where graders"
        },
        {
          "ABSTRACT": "assign emotion score after listening to the speech files. Such perceptual grades in-"
        },
        {
          "ABSTRACT": "troduce uncertainty in labels due to grader opinion variation. Grader variation is"
        },
        {
          "ABSTRACT": "addressed by using consensus grades as groundtruth, where the emotion with the"
        },
        {
          "ABSTRACT": "highest vote is selected. Consensus grades fail\nto consider ambiguous instances"
        },
        {
          "ABSTRACT": "where a speech sample may contain multiple emotions, as captured through grader"
        },
        {
          "ABSTRACT": "opinion uncertainty. We demonstrate that using the probability density function"
        },
        {
          "ABSTRACT": "of the emotion grades as targets instead of the commonly used consensus grades,"
        },
        {
          "ABSTRACT": "provide better performance on benchmark evaluation sets compared to results re-"
        },
        {
          "ABSTRACT": "ported in the literature. We show that a saliency driven foundation model (FM)"
        },
        {
          "ABSTRACT": "representation selection helps to train a state-of-the-art speech emotion model for"
        },
        {
          "ABSTRACT": "both dimensional and categorical emotion recognition.\nComparing representa-"
        },
        {
          "ABSTRACT": "tions obtained from different FMs, we observed that focusing on overall\ntest-set"
        },
        {
          "ABSTRACT": "performance can be deceiving, as it fails to reveal\nthe models generalization ca-"
        },
        {
          "ABSTRACT": "pacity across speakers and gender. We demonstrate that performance evaluation"
        },
        {
          "ABSTRACT": "across multiple test-sets and performance analysis across gender and speakers are"
        },
        {
          "ABSTRACT": "useful in assessing usefulness of emotion models. Finally, we demonstrate that la-"
        },
        {
          "ABSTRACT": "bel uncertainty and data-skew pose a challenge to model evaluation, where instead"
        },
        {
          "ABSTRACT": "of using the best hypothesis, it is useful to consider the 2- or 3-best hypotheses."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 4: , Appendix A.2). We report",
      "data": [
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "the FMs may not have been trained with emotion labels,\nthe final\nlayer representations may not be"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "optimal for emotion recognition. Earlier studies have investigated intermediate FM representations"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "for various speech tasks Alain & Bengio (2016); Mitra & Franco (2020); Mitra et al. (2024a); Yang"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "et al.\n(2024).\nIn this work, we investigate saliency based FM layer selection for\nthe downstream"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "emotion modeling task. To summarize, in this work, we:"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "1. Account label uncertainty through the use of categorical emotion pdf as targets."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "2. Explore saliency-driven intermediate FM layer representations for emotion recognition."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "3. Evaluate performance across speakers, gender and unseen acoustic conditions."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "We observed that models that provide state-of-the-art (SOTA) results, may not generalize well across"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "speakers and varying acoustic conditions. We found that having a diverse evaluation set along"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "with a diverse evaluation metric is useful\nfor model selection. We found that\nthe traditional 1-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "best hypothesis used in emotion literature may get biased by the training data-skew,\nin which case"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "2- or 3-best hypotheses may be useful to account for speech samples containing multiple emotions."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "2\nDATA"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "We have used the MSP-Podcast dataset (ver. 1.11) Mariooryad et al. (2014); Lotfian & Busso (2017)"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "that contains ≈ 238 hours of speech data spoken by English speakers (N > 1800), consisting of"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "≈ 152K speaking turns. The speech segments contain single-speaker utterances with a duration of"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "3 to 11 seconds. The data contain manually assigned valence, activation and dominance scores and"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "categorical emotions (9 categories) from multiple graders. Grader decisions for categorical emotions"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "were converted to a pdf (reflecting the probability of each of the 9 emotions), which was used as"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "the target for our model training. The data split is shown in Table 4 in Appendix A.2. To make our"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "results comparable to Ghriss et al. (2022); Srinivasan et al. (2022), we report results on Eval1.6 and"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Eval1.11 (see Table 4). For evaluating model robustness, we have added noise to the MSP test-set"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "at SNR levels 15 dB and 5dB (see Eval15dB and Eval5dB in Table 4, Appendix A.2). We report"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "categorical emotion recognition performance on six emotions: neutral, happy, angry, sad, contempt"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "and surprise. We have used CMU-Mosei, Zadeh et al. (2018) and a 5 hour in-house conversational"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "speech data from 85 speakers for cross-corpus speech emotion recognition analysis."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "3\nREPRESENTATIONS"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "We explore speech embeddings as features to a TC-GRU model\n(see Figure 1). We use the fol-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "lowing pre-trained models to generate those embeddings:\n(i) HuBERT large Hsu et al. (2021), a"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "transformer based acoustic model, pre-trained on 60K hours of Libri-light speech data, generating"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "1024-dimensional embedding.\n(ii) WavLM large Chen et al. (2022), a transformer based acoustic"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "model, generating 1024 dimensional embedding. WavLM has been pre-trained on 60K hours of"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Libri-light, 19K hours of GigaSpeech and 25K hours of VoxPopuli.\n(iii) Whisper medium Rad-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "ford et al. (2023) acoustic model that generates 1024 dimensional embeddings from 24 transformer"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "encoder layers. Whisper is trained with 680K hours of noisy and diverse speech data from the web."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Motivated by Mitra et al. (2024b;a) we explore obtaining layer-saliency to obtain the optimal FM"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "layer\nrepresentation for emotion modeling.\nLet\nthe N dimensional\nrepresentation from the kth"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "layer of a FM for an utterance y be represented by a vector H y"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "k (t) = [X1,k, . . . , Xt,k, . . . , XM,k],"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "where M denotes the sequence length. For a regression task, let the sequence targets be Ly, where"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "y k\nLy ∈ RD, where the D dimensional vector L denotes the output\ntargets,\nfor each utterance. H"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "in eq. 1 is obtained from H y\nthe frames for utterance y. The cross-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "k by taking the mean across all"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "correlation based saliency (CCS) of ith dimension of the kth layer is given by:"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "y k\nCov(H\n,i, Ly)"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\ny k\n1 M\nH y\nwhere, H\n=\n(1)"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "M(cid:88) t\nSCCS,i,k =\n+ γi,\nk (t)"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "σLy\nσH y"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "k,i\n=1"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "N(cid:88)\ny i\ny j\ny j\nCov(H\nCov(H\n, Ly)\n1\n,k, H\n,k)"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)\n,\n(2)\nγi =\nwhere, wj =\nwj"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "N − 1"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "σLy\nσH y\nσH y\nσH y"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "i\nj\nj\nj=1,j̸=i"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "1 D\nD(cid:88) l\n(3)\nµCCS,k =\nSCCSk,l ."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "=1"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "2"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Finally, we compared categorical emotion recognition performance",
      "data": [
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": ""
        },
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": ""
        },
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": "Act."
        },
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": "0.73"
        },
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": "0.77"
        },
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": "0.77"
        },
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": "0.75"
        },
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": "0.75"
        },
        {
          "Table 1: Dimensional emotion estimation performance (CCC ↑) and comparison with SOTA": "0.77"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Finally, we compared categorical emotion recognition performance",
      "data": [
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": ""
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": ""
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": "F 1m"
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": "0.34"
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": "0.49"
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": "0.50"
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": "0.52"
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": "-"
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": "-"
        },
        {
          "Table 2: Categorical emotion recognition performance and comparison with SOTA models": "0.35"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: shows paUAR-75 and paUAR-50 for categorical emotion, obtained across speakers. Note",
      "data": [
        {
          "who are above a UAR of X%, where we have used two thresholds: X: 75% and 50%, respectively.": "Table 3 shows paUAR-75 and paUAR-50 for categorical emotion, obtained across speakers. Note"
        },
        {
          "who are above a UAR of X%, where we have used two thresholds: X: 75% and 50%, respectively.": "that Tables 1 and 2 show that overall WavLM TC-GRU model performed better than the HuBERT"
        },
        {
          "who are above a UAR of X%, where we have used two thresholds: X: 75% and 50%, respectively.": "TC-GRU, however table 3 shows that a better system may not necessarily generalize across speakers."
        },
        {
          "who are above a UAR of X%, where we have used two thresholds: X: 75% and 50%, respectively.": ""
        },
        {
          "who are above a UAR of X%, where we have used two thresholds: X: 75% and 50%, respectively.": ""
        },
        {
          "who are above a UAR of X%, where we have used two thresholds: X: 75% and 50%, respectively.": "Eval Sets"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: shows paUAR-75 and paUAR-50 for categorical emotion, obtained across speakers. Note",
      "data": [
        {
          "Table 3: Emotion recognition performance across speakers": ""
        },
        {
          "Table 3: Emotion recognition performance across speakers": "MFBF0 TC − GRU"
        },
        {
          "Table 3: Emotion recognition performance across speakers": "paUAR-50"
        },
        {
          "Table 3: Emotion recognition performance across speakers": "21.1"
        },
        {
          "Table 3: Emotion recognition performance across speakers": "14.0"
        },
        {
          "Table 3: Emotion recognition performance across speakers": "46.9"
        },
        {
          "Table 3: Emotion recognition performance across speakers": "100.0"
        },
        {
          "Table 3: Emotion recognition performance across speakers": "68.0"
        },
        {
          "Table 3: Emotion recognition performance across speakers": "100.0"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "arXiv\nG. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes."
        },
        {
          "REFERENCES": "preprint arXiv:1610.01644, 2016."
        },
        {
          "REFERENCES": "C. Busso, M. Bulut, C.C. Lee, A. Kazemzadeh, E. Mower, J.N. Kim, S.and Chang, S. Lee, and S.S."
        },
        {
          "REFERENCES": "Narayanan. Iemocap: Interactive emotional dyadic motion capture database. Language resources"
        },
        {
          "REFERENCES": "and evaluation, 42(4):335–359, 2008."
        },
        {
          "REFERENCES": "S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, et al."
        },
        {
          "REFERENCES": "IEEE Journal\nWavlm: Large-scale self-supervised pre-training for full stack speech processing."
        },
        {
          "REFERENCES": "of Selected Topics in Signal Processing, 16(6):1505–1518, 2022."
        },
        {
          "REFERENCES": "Huang-Cheng Chou, Lucas Goncalves, Seong-Gyun Leem, Ali N Salman, Chi-Chun Lee, and Car-"
        },
        {
          "REFERENCES": "los Busso. Minority views matter: Evaluating speech emotion classifiers with human subjective"
        },
        {
          "REFERENCES": "annotations by an all-inclusive aggregation rule.\nIEEE Transactions on Affective Computing,"
        },
        {
          "REFERENCES": "2024."
        },
        {
          "REFERENCES": "N. Das, S. Dingliwal, S. Ronanki, R. Paturi, D. Huang, P. Mathur, J. Yuan, D. Bekal, X. Niu, S.M."
        },
        {
          "REFERENCES": "arXiv preprint\nJayanthi, et al. Speechverse: A large-scale generalizable audio language model."
        },
        {
          "REFERENCES": "arXiv:2405.08295, 2024."
        },
        {
          "REFERENCES": "E. Douglas-Cowie, L. Devillers, J.C. Martin, R. Cowie, S. Savvidou, S. Abrilian, and C. Cox. Mul-"
        },
        {
          "REFERENCES": "timodal databases of everyday emotion: Facing up to complexity.\nIn Proc. of Interspeech, 2005."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "V\n. Mitra, H.Y.S. Chien, V. Kowtha, J.Y. Cheng, and E. Azemi. Speech emotion: Investigating model"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "representations, multi-task learning and knowledge distillation. Proc. of Interspeech, 2022."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "V\n. Mitra, V. Kowtha, H.Y.S. Chien, E. Azemi, and C. Avendano. Pre-trained model representations"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "and their\nrobustness against noise for speech emotion analysis.\nIn Proc. of\nICASSP, pp. 1–5."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "IEEE, 2023."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "V\n. Mitra, A. Chatterjee, K. Zhai, H. Weng, A. Hill, N. Hay, et al.\nPre-trained foundation model"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "representations to uncover breathing patterns in speech. arXiv preprint arXiv:2407.13035, 2024a."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "V\n. Mitra, J. Nie, and E. Azemi.\nInvestigating salient representations and label variance in dimen-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "sional speech emotion analysis.\nIn ICASSP 2024-2024 IEEE International Conference on Acous-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "tics, Speech and Signal Processing (ICASSP), pp. 11111–11115. IEEE, 2024b."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, and Emily Mower Provost. Cap-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "turing mismatch between textual and acoustic emotion expressions\nfor mood identification in"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "bipolar disorder.\nIn Interspeech. Interspeech, 2023."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "J. Posner, J.A. Russell, and B.S. Peterson. The circumplex model of affect: An integrative approach"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "to affective neuroscience, cognitive development, and psychopathology. Development and psy-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "chopathology, 17(3):715–734, 2005."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "N.R. Prabhu, N. Lehmann-Willenbrock, and T. Gerkmann.\nLabel uncertainty modeling and pre-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "diction for speech emotion recognition using t-distributions.\nIn Proc. of ACII, pp. 1–8.\nIEEE,"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "2022."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Emily Mower Provost, Sarah H Sperry,\nJames Tavernor, Steve Anderau, Anastasia Yocum, and"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Melvin G McInnis. Emotion recognition in the real-world: Passively collecting and estimating"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "IEEE Transactions on\nemotions from natural speech data of\nindividuals with bipolar disorder."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Affective Computing, 2024."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "A. Radford, J.W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recog-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "nition via large-scale weak supervision.\nIn International conference on machine learning, pp."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "28492–28518. PMLR, 2023."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "S. Srinivasan, Z. Huang, and K. Kirchhoff. Representation learning through cross-modal conditional"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "teacher-student training for speech emotion recognition. Proc. of ICASSP, pp. 6442–6446, 2022."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "B. Stasak, J. Epps, N. Cummins, and R. Goecke. An investigation of emotional speech in depression"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "classification.\nIn Proc. of Interspeech, pp. 485–489, 2016."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "James Tavernor, Yara El-Tawil, and Emily Mower Provost.\nThe whole is bigger\nthan the sum"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "arXiv preprint\nof\nits parts: Modeling individual annotators\nto capture emotional variability."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "arXiv:2408.11956, 2024."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "H. Wu, H-C. Chou, K-W. Chang, L. Goncalves, J. Du, J-S.R. Jang, C-C. Lee, and H-Y. Lee. Emo-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "superb: An in-depth look at speech emotion recognition. arXiv preprint arXiv:2402.13018, 2024."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "S-W. Yang, H-J. Chang, Z. Huang, A.T. Liu, et al. A large-scale evaluation of speech foundation"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "models.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "A.B. Zadeh, P.P. Liang, S. Poria, E. Cambria, and L.-P. Morency. Multimodal language analysis in"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.\nIn Proceedings of the 56th"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Annual Meeting of the Association for Computational Linguistics, 2018."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse",
      "data": [
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "Split"
        },
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "MSP Train1.11"
        },
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "MSP Valid1.11"
        },
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "MSP Eval1.6"
        },
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "MSP Eval1.11"
        },
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "MSP Eval15dB"
        },
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "MSP Eval5dB"
        },
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "CMU-Mosei"
        },
        {
          "Table 4: MSP-podcast data split, noise-degraded test sets and out-of-domain MOSEI and Inhouse": "Inhouse data"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: shows that representations from emotion-salient layer as compared to the final FM layer,",
      "data": [
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Figure 2: Dimensional emotion estimation for different transformer layers in WavLM"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Figure 3: WavLM layer saliency by valence, happy and angry emotion"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "A.4\nEMOTION MODEL DETAILS"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Table 5 shows that\nrepresentations from emotion-salient\nlayer as compared to the final FM layer,"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "resulted in improvement\nin emotion recognition performance.\nIt\nis also interesting to note that"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "relative improvement\nin valence was higher\n(> 8% relative) compared to the other dimensional"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "8"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": "sentations from final layer and (3) FM representations from the salient layer"
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": "Test set"
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": "Eval1.3"
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": "Eval1.6"
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": "Eval1.11"
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": "Eval15dB"
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": "Eval5dB"
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": "Mosei"
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        },
        {
          "Table 5: Dimensional and categorical emotion estimation using (1) MFBF0 feature, (2) FM repre-": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Table 6: Emotion recognition performance (paUAR-75 and paUAR-50) by gender for Whisper TC-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "GRU model"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Eval Sets\npaUAR-75\npaUAR-50"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Female\nMale\nFemale\nMale"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Eval1.11\n2.7\n7.1\n45.8\n50.4"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Inhouse\n7.1\n13.8\n28.6\n44.8"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "A.6\nPERFORMANCE BY SPEAKER’S EMOTION DISTRIBUTIONS"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Figure 4 shows performance plotted against emotion distributions for each speaker in Eval1.6. Be-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "cause UAR is the unweighted average across recall on all emotions, we do not find a strong rela-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "tionship between UAR and emotion distribution. This suggests UAR is robust to these speaker-level"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "changes and can capture other important factors in speaker-level performance."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "Figure 4: Speaker-level performance (UAR from Whisper TC-GRU) plotted against emotion distri-"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "butions, for speakers in Eval1.6."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "A.7\nRELATIONSHIP BETWEEN 1ST AND 2ND BEST MODEL HYPOTHESES"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "We find that\nthe model’s first and second hypotheses show a clear\nrelationship, and that\nthe first"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "hypothesis alone may not fully reflect the model’s understanding. Figure 5 illustrates these details,"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "with the samples accurately labeled by the first hypothesis outlined by the horizontal gray bars, and"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "the samples accurately labeled by the second hypothesis outlined by the vertical gray bars. The first"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "hypotheses are highly accurate for happiness and anger,\nindicated by the white squares within the"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "horizontal gray outlines. However, for most sadness samples, the model identifies neutral as the most"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "likely emotion and sadness as the second most likely emotion, indicated by the white square within"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "the vertical gray outline. Similarly, for surprise samples, the model identifies happiness as the most"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "likely emotion and surprise as the second most\nlikely emotion, where this hierarchy likely results"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "from the closer\nrelationship between happiness and surprise with the former class having more"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "representation in the training data. We also see considerable confusion between contempt, anger, and"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "neutral. When we explore the models second best hypotheses, we find the model correctly detects"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "the overall sentiment but does not distinguish correctly between them. This finding supports our"
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "analysis into considering the model’s second best hypotheses when determining model predictions."
        },
        {
          "I Can’t Believe It’s Not Better Workshop @ ICLR 2025": "10"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "proper selection of representation layers.": ""
        },
        {
          "proper selection of representation layers.": ""
        },
        {
          "proper selection of representation layers.": ""
        },
        {
          "proper selection of representation layers.": "Model"
        },
        {
          "proper selection of representation layers.": ""
        },
        {
          "proper selection of representation layers.": ""
        },
        {
          "proper selection of representation layers.": "MFBF0 TC-GRU"
        },
        {
          "proper selection of representation layers.": "WavLM TC-GRU"
        },
        {
          "proper selection of representation layers.": "HuBERT TC-GRU"
        },
        {
          "proper selection of representation layers.": "Whisper TC-GRU"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Understanding intermediate layers using linear classifier probes",
      "authors": [
        "G Alain",
        "Y Bengio"
      ],
      "year": "2016",
      "venue": "Understanding intermediate layers using linear classifier probes",
      "arxiv": "arXiv:1610.01644"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "J Kim",
        "S Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an all-inclusive aggregation rule",
      "authors": [
        "Huang-Cheng Chou",
        "Lucas Goncalves",
        "Seong-Gyun Leem",
        "Chi-Chun Ali N Salman",
        "Carlos Lee",
        "Busso"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "A large-scale generalizable audio language model",
      "authors": [
        "N Das",
        "S Dingliwal",
        "S Ronanki",
        "R Paturi",
        "D Huang",
        "P Mathur",
        "J Yuan",
        "D Bekal",
        "X Niu",
        "S Jayanthi"
      ],
      "year": "2024",
      "venue": "A large-scale generalizable audio language model",
      "arxiv": "arXiv:2405.08295"
    },
    {
      "citation_id": "6",
      "title": "Multimodal databases of everyday emotion: Facing up to complexity",
      "authors": [
        "E Douglas-Cowie",
        "L Devillers",
        "J Martin",
        "R Cowie",
        "S Savvidou",
        "S Abrilian",
        "C Cox"
      ],
      "year": "2005",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "7",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "8",
      "title": "Peft-ser: On the use of parameter efficient transfer learning approaches for speech emotion recognition using pre-trained speech models",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "9",
      "title": "Sentiment-aware automatic speech recognition pre-training for enhanced speech emotion recognition",
      "authors": [
        "A Ghriss",
        "B Yang",
        "V Rozgic",
        "E Shriberg",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Detecting emotion primitives from speech and their use in discerning categorical emotions",
      "authors": [
        "V Kowtha",
        "V Mitra",
        "C Bartels",
        "E Marchi",
        "S Booker",
        "W Caruso",
        "S Kajarekar",
        "D Naik"
      ],
      "year": "2020",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "12",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "13",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Building a naturalistic emotional speech corpus by retrieving expressive behaviors from existing speech corpora",
      "authors": [
        "S Mariooryad",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2014",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Investigation and analysis of hyper and hypo neuron pruning to selectively update neurons during unsupervised adaptation",
      "authors": [
        "V Mitra",
        "H Franco"
      ],
      "year": "2020",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Articulatory information and multiview features for large vocabulary continuous speech recognition",
      "authors": [
        "V Mitra",
        "W Wang",
        "C Bartels",
        "H Franco",
        "D Vergyri"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Leveraging acoustic cues and paralinguistic embeddings to detect expression from voice",
      "authors": [
        "V Mitra",
        "S Booker",
        "E Marchi",
        "D Farrar",
        "U Peitz",
        "B Cheng",
        "E Teves",
        "A Mehta",
        "D Naik"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion: Investigating model representations, multi-task learning and knowledge distillation. Proc. of Interspeech",
      "authors": [
        "V Mitra",
        "H Chien",
        "V Kowtha",
        "J Cheng",
        "E Azemi"
      ],
      "year": "2022",
      "venue": "Speech emotion: Investigating model representations, multi-task learning and knowledge distillation. Proc. of Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Pre-trained model representations and their robustness against noise for speech emotion analysis",
      "authors": [
        "V Mitra",
        "V Kowtha",
        "H Chien",
        "E Azemi",
        "C Avendano"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Pre-trained foundation model representations to uncover breathing patterns in speech",
      "authors": [
        "V Mitra",
        "A Chatterjee",
        "K Zhai",
        "H Weng",
        "A Hill",
        "N Hay"
      ],
      "year": "2024",
      "venue": "Pre-trained foundation model representations to uncover breathing patterns in speech",
      "arxiv": "arXiv:2407.13035"
    },
    {
      "citation_id": "21",
      "title": "Investigating salient representations and label variance in dimensional speech emotion analysis",
      "authors": [
        "V Mitra",
        "J Nie",
        "E Azemi"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Capturing mismatch between textual and acoustic emotion expressions for mood identification in bipolar disorder",
      "authors": [
        "Minxue Niu",
        "Amrit Romana",
        "Mimansa Jaiswal",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2023",
      "venue": "In Interspeech. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "24",
      "title": "Label uncertainty modeling and prediction for speech emotion recognition using t-distributions",
      "authors": [
        "N Prabhu",
        "N Lehmann-Willenbrock",
        "T Gerkmann"
      ],
      "year": "2022",
      "venue": "Proc. of ACII"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in the real-world: Passively collecting and estimating emotions from natural speech data of individuals with bipolar disorder",
      "authors": [
        "Emily Mower Provost",
        "Sarah Sperry",
        "James Tavernor",
        "Steve Anderau",
        "Anastasia Yocum",
        "Melvin Mcinnis"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "27",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "28",
      "title": "An investigation of emotional speech in depression classification",
      "authors": [
        "B Stasak",
        "J Epps",
        "N Cummins",
        "R Goecke"
      ],
      "year": "2016",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "29",
      "title": "The whole is bigger than the sum of its parts: Modeling individual annotators to capture emotional variability",
      "authors": [
        "James Tavernor",
        "Yara El-Tawil",
        "Emily Provost"
      ],
      "year": "2024",
      "venue": "The whole is bigger than the sum of its parts: Modeling individual annotators to capture emotional variability",
      "arxiv": "arXiv:2408.11956"
    },
    {
      "citation_id": "30",
      "title": "Emosuperb: An in-depth look at speech emotion recognition",
      "authors": [
        "H Wu",
        "H-C Chou",
        "K-W Chang",
        "L Goncalves",
        "J Du",
        "J-S Jang",
        "C-C Lee",
        "H-Y Lee"
      ],
      "year": "2024",
      "venue": "Emosuperb: An in-depth look at speech emotion recognition",
      "arxiv": "arXiv:2402.13018"
    },
    {
      "citation_id": "31",
      "title": "A large-scale evaluation of speech foundation models",
      "authors": [
        "S-W Yang",
        "H-J. Chang",
        "Z Huang",
        "A Liu"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "32",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}