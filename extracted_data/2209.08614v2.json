{
  "paper_id": "2209.08614v2",
  "title": "Deep Adaptation Of Adult-Child Facial Expressions By Fusing Landmark Features",
  "published": "2022-09-18T17:29:36Z",
  "authors": [
    "Megan A. Witherow",
    "Manar D. Samad",
    "Norou Diawara",
    "Haim Y. Bar",
    "Khan M. Iftekharuddin"
  ],
  "keywords": [
    "Facial expression recognition",
    "Feature fusion",
    "Feature selection",
    "Beta distributions",
    "Domain adaptation",
    "Transfer learning",
    "Child expressions"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Imaging of facial affects may be used to measure psychophysiological attributes of children through their adulthood for applications in education, healthcare, and entertainment, among others. Deep convolutional neural networks show promising results in classifying facial expressions of adults. However, classifier models trained with adult benchmark data are unsuitable for learning child expressions due to discrepancies in psychophysical development. Similarly, models trained with child data perf orm poorly in adult expression classification. We propose domain adaptation to concurrently align distributions of adult and child expressions in a shared latent space for robust classification of either domain. Furthermore, age variations in facial images are studied in age-invariant face recognition yet remain unleveraged in adult-child expression classification. We take inspiration from multiple fields and propose deep adaptive FACial Expressions fusing BEtaMix SElected Landmark Features (FACE-BE-SELF) for adult-child expression classification. For the first time in the literature, a mixture of Beta distributions is used to decompose and select facial features based on correlations with expression, domain, and identity factors. We evaluate FACE-BE-SELF using 5fold cross validation for two pairs of adult-child data sets. Our proposed FACE-BE-SELF approach outperforms transfer learning and other baseline domain adaptation methods in aligning latent representations of adult and child expressions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "ROM infancy to adulthood, facial expressions are a ubiquitous, information-rich component of human social interactions. Facial expressions may provide valuable information about a child's development into an adult  [1] ,  [2] ,  [3] . Many applications of automatic facial expression analysis (FEA), including education (e.g., engagement in the classroom  [4] ,  [5] ,  [6] ), healthcare (e.g., monitoring of pain  [7] ,  [8] , mental health  [9] ,  [10] , autism  [11] ,  [12] ,  [13] ), and entertainment (e.g., video games  [14] ,  [15] ) remain relevant from childhood into adulthood. To better support such applications, FEA models need to generalize across distinctive expression patterns from early childhood to adulthood. Developing models robust to age variations is a challenging problem in FEA  [16] ,  [17] . Most existing approaches optimize the FEA performance on data sets representing specific age ranges. There has been limited work on classifying facial expressions across age groups.\n\nFurthermore, age variations in facial images have been well-studied in facial age estimation and age-invariant face recognition (AIFR), but there has been little cross-pollination among these relevant research areas to improve FEA considering adult-child age variations. In the following sections, we discuss related work on the classification of adult and child expressions and methods from relevant research fields. Then, we propose a novel deep feature adaptation approach to the classification of adult and child expressions inspired by the state-of-the-art domain adaptation learning, facial age estimation, and AIFR literature.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Classification Of Adult & Child Facial Expressions",
      "text": "Existing off-the-shelf FEA tools and research  [18] ,  [19] ,  [20]  have been mostly developed using adult benchmark data sets  [17] ,  [21] ,  [22] . However, facial morphology and kinematics gradually develop throughout childhood  [23] ,  [24] , resulting in a distribution shift between child and adult expression patterns. For models trained on adult data sets, the distribution shift toward adults poorly generalizes distinctive patterns in child expressions  [25] ,  [26] ,  [27] ,  [28] . While benchmark data sets of child facial expressions remain limited, they are growing in number  [29] ,  [30] ,  [31] . Therefore, there has been an emerging trend directed at the classification of child facial expressions  [25] ,  [26] ,  [27] ,  [28] . Recently, deep transfer learning using convolutional neural networks (CNNs) has shown promise for child facial expression classification  [26] ,  [27] ,  [32] . However, recent studies focus only on maximizing performance on child facial expression benchmarks, bounded by limited age range and sample size  [17] . Such models tuned for child expressions fail to generalize to adult expressions  [28] . To overcome the poor generalization problem across age groups, limited existing work on facial expression classification involving mixed age groups (child, adult, elderly) suggests two primary approaches: (1) curating a mixed age training set to match the age distribution of the test set  [33] , and  (2)  classifying images into age groups to determine the ageappropriate model for subsequent classification  [34] . The first approach requires the age distribution of the test set to be known a priori with availability of benchmark data matching. The second approach requires a robust age group classifier to select an appropriate expression classifier model and benchmark data to train expression classifiers for individual age groups. Age group classification is a challenging problem  [35] ,  [36]  and variations in expression make accurate age estimation even more challenging  [34] ,  [37] . Furthermore, developments in both facial structure and muscle movements contribute to visual differences in child and adult expressions. A child's growth is a gradual and uniquely individual process, making the transition unclear when a child manifests the full spectrum of adult expressions.\n\nRecently, domain adaptation has shown an interesting pathway to adapt an adult expression classification model using few child expression samples  [27] . This approach utilizes a dual stream deep CNN architecture and semantically aligns the class conditional distributions of child and adult domains  [27] . The underlying framework of this approach  [38]  is based on learning a domain-invariant latent representation. Such domain-invariant representations have shown to generalize even to unseen domains  [38] . We hypothesize that learning a domain-invariant representation of expressions may prove effective for facial expression classification across child and adult domains.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Recognition Of Age-Varying Facial Images",
      "text": "While limited attention has been given to facial expression classification across age groups, facial age estimation  [39]  and AIFR  [40]  are active research areas. State-of-art approaches for facial age estimation and AIFR benefit from deep learning and fusion of geometric and texture features  [36] ,  [39] ,  [41] . Geometric features derived from facial landmarks capture structural changes associated with childhood development while texture features capture skin artifacts, such as wrinkles, associated with adult aging  [36] ,  [41] . Contemporary studies continue to use traditional feature extraction methods, e.g., local binary patterns, histograms of oriented gradients, etc., but recently emphasize deep learning, e.g., CNNs, for texture feature extraction  [39] ,  [40] ,  [42] . Common geometric landmark features include distances between landmarks, ratios of distances, and areas and angles of triangles formed by landmark triplets  [35] ,  [43] ,  [44] ,  [45] . Similar landmark features, including pairwise distances between landmarks and areas/angles of facial polygons formed by connecting neighboring landmark points, have also shown to be discriminative for FEA  [46] ,  [47] ,  [48] ,  [49] . Therefore, we hypothesize that domain-invariant representation learning of adult and child facial expressions can benefit from a fusion of CNNextracted and landmark-derived features.\n\nThe use of the same feature types in both facial age estimation and AIFR suggests a subset of features correlated with and invariant to age. Statistical latent variable models optimized using the Expectation-Maximization (EM) algorithm have been applied to AIFR to decompose feature sets into age and identity factors  [40] . This approach identifies a set of discriminative features for identity recognition using the identity factor, representing facial identity features invariant to age  [40] . Gong et al.  [50]  have first proposed this approach using hidden factor analysis (HFA). HFA assumes the independence of age and identity, which is untrue in practice as different individuals may have different aging patterns  [40] . To overcome the independence assumption, the modified HFA (MHFA) approach introduces an additional factor representing age and identity-correlated facial appearance variations  [51] . Given that the appearance of facial expressions varies among individuals and age groups, we hypothesize that FEA can benefit from decomposition of feature sets into those correlated with expression, domain (adult or child), and identity. However, MHFA assumes that data are independent and identically distributed (i.i.d.) following a normal distribution with homogenous variances, which may not be true for real world facial expression data. Furthermore, HFA and MHFA require the optimization of one model per feature, making high dimensional feature vectors computationally prohibitive  [50] ,  [51] . Thus, principal component analysis (PCA) has been used for dimensionality reduction prior to HFA or MHFA  [50] ,  [51] . While PCA guarantees that the first principal components explain more of the variance than subsequent principal components, such linear data projection method does not guarantee that the PCA feature space will be discriminative for classification. Each principal component is a linear combination of all input features, making it less intuitive to understand the contribution of individual features. Moreover, all features, even those with limited contribution to discriminability, are needed to reproduce the same principal components.\n\nVery recently, the Beta-Mixture (BetaMix) method  [52]  has been proposed to determine significant correlations among large numbers of variables using a mixture of Beta distributions. The method, based on ideas and results from convex geometry, works well even for moderate sample sizes, e.g., ğ‘ = 10 depending on the number of predictors, and does not require assumptions of i.i.d., normality, or homogeneity of variances. The BetaMix method detects correlations among all the features at once, so the EM algorithm needs to be applied only once for all features rather than for individual features. Since the BetaMix method is appropriate for large feature vectors, dimensionality reduction is not required and the feature correlations may be interpreted directly, allowing for greater understanding of the interaction between the features and domain, identity, and expression factors. The BetaMix method has shown promising results across multiple applications, including feature selection and classification  [52] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contributions",
      "text": "We propose novel deep domain adaptative FACial Expressions fusing BEtaMix SElected Landmark Features (FACE-BE-SELF) for domain-invariant expression classification. To the best of our knowledge, our proposed deep domain adaptive FACE-BE-SELF approach is the first to perform concurrent adult-child domain adaptation and learn a generalized expression representation that may be used for both child and adult facial expression classification. Our contributions are as follows:\n\nâ€¢ We fuse facial landmark measurements with deep feature representations for robust expression learning across age groups. â€¢ The proposed domain adaptation method is compared to baseline CNN, transfer learning, and existing domain adaptation methods for facial expression recognition using multiple benchmark data sets. The remainder of this paper is organized as follows. Section 2 describes the methodology of our approach. Section 3 and 4 present the results and discussion, respectively. Section 5 discusses limitations and Section 6 presents the conclusion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Sets",
      "text": "We evaluate our proposed method using four data sets of facial expression images: 1) the Extended Cohn-Kanade (CK+) data set  [21] ,  [22] , 2) the Aff-Wild2 data set  [53] ,  [54] ,  [55] ,  [56] ,  [57] ,  [58] ,  [59] , 3) the Child Affective Facial Expression (CAFE) data set  [29] ,  [30] , and 4) the Child Emotion Facial Expression Set (ChildEFES)  [31] . We consider both posed and spontaneous data sets. While spontaneous data sets represent most expressions seen in daily life, posed expressions serve a valuable purpose in healthcare applications such as social reciprocity training  [60] ,  [61] ,  [62]  for individuals with autism and facial rehabilitation exercises for individuals with Parkinson's disease and facial palsy  [63] ,  [64] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ck+ Data Set",
      "text": "The CK+ data set  [21] ,  [22]  consists of 593 image sequences of posed facial expressions, including labeled 'anger', 'disgust', 'fear', 'happy', 'sad', 'surprise', and 'contempt' examples, captured from 123 adult subjects (ages 18 to 50 years). A mixture of color and grayscale sequences are present in the data set. Sequences vary in length, but each sequence begins with the neutral expression and ends with the peak expression frame, which has been coded for action units (AUs) from the facial action coding system (FACS). We assign the last three frames of a sequence with its corresponding expression label and label the first frame of each sequence as 'neutral'. This yields 1254 samples: 135 'anger', 177 'disgust', 75 'fear', 207 'happy', 327 'neutral', 84 'sad', and 1369 'surprise'.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Aff-Wild2 Data Set",
      "text": "The Aff-Wild2 data set  [53] ,  [54] ,  [55] ,  [56] ,  [57] ,  [58] ,  [59] , an extension of the Affect-in-the-Wild (Aff-Wild)  [65] ,  [66] ,  [67]  data set, consists of 558 YouTube videos with annotations for three behavioral tasks: valance and arousal, FACS AUs, and facial expressions ('anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise', and 'other'). The facial expression subset of Aff-Wild2 contains 84 videos with 84 ethnically diverse subjects (42 female). Age labels are not provided. Visually, the subjects appear to be mostly adults with few child subjects, including infants. Labeled frames show a variety of different head poses, occlusions, and illumination conditions. Excluding the frames labeled 'other', there are a total of 451794 samples: 18940 'anger', 14545 'disgust', 11336 'fear', 97862 'happy', 197314 'neutral', 80517 'sad', and 31280 'surprise'.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cafe Data Set",
      "text": "The CAFE data set  [29] ,  [30]  consists of 1192 color photographs of 154 child subjects (ages 2 to 8 years) posing 'anger', 'disgust', 'fear', 'happy', 'sad', and 'surprise' expressions, including 'neutral'. The data set includes open and closed mouth variations for each expression except 'surprise', which is posed with open mouth only. We include the mouth closed variant of all expressions except for 'surprise', yielding 707 samples: 119 'anger', 96 'disgust', 79 'fear', 120 'happy', 129 'neutral', 62 'sad', and 102 'surprise'. The data usage agreement for the CAFE dataset does not allow republication of the images.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Childefes Data Set",
      "text": "The ChildEFES data set  [31]  consists of color photos and videos capturing 34 child subjects (ages 4 to 6 years) producing a mixture of spontaneous and posed 'anger', 'disgust', 'fear', 'happy', 'sad', 'surprise', and 'contempt' expressions. The expression labels are assigned based upon the agreement of four FACS judges. We crop the expression-labeled videos to the peak expression. Then, we sample the cropped videos at 20 frames per second to generate image sequences. Since the photographs are a subset of the image sequences, we include only the frames sampled from the videos. This yields 9420 (5107 spontaneous) samples: 1435 (170) 'anger', 1196 (468) 'disgust', 655 (19) 'fear', 2196 (1535) 'happy', 2445 (2372) 'neutral', 1053 (450) 'sad', and 440 (93) 'surprise'. The data usage agreement for the ChildEFES data set does not allow for the use of the images in publications.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Notation",
      "text": "Let input space ğ’³ represent the set of all possible facial images and features. Output space ğ’´ = {0, â€¦ , ğ¾ -1} is the set of ğ¾ expression class labels ('anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'). ğ’³ and ğ’´ are related by a function ğ‘“: ğ’³ â†’ ğ’´. We consider adult facial expressions (CK+, Aff-Wild2) as the source domain and child facial expressions (CAFE, ChildEFES) as the target domain. We represent each source data set as ğ· ğ‘† = {(ğ‘¥ ğ‘– ğ‘† , ğ‘¦ ğ‘– ğ‘† ) | ğ‘¥ ğ‘– ğ‘† âˆˆ ğ’³, ğ‘¦ ğ‘– ğ‘† âˆˆ ğ’´} ğ‘–=1 ğ‘ ğ‘† , ğ‘¥ ğ‘– ğ‘† ~ğ‘ğ‘‹ ğ‘† where ğ‘ ğ‘† is the total number of samples and ğ‘ ğ‘† is the source probability distribution. We represent each target dataset as ğ· ğ‘‡ = {(ğ‘¥ ğ‘– ğ‘‡ , ğ‘¦ ğ‘– ğ‘‡ ) | ğ‘¥ ğ‘– ğ‘‡ âˆˆ ğ’³, ğ‘¦ ğ‘– ğ‘‡ âˆˆ ğ’´} ğ‘–=1 ğ‘ ğ‘‡ , ğ‘¥ ğ‘– ğ‘‡ ~ğ‘ğ‘‹ ğ‘‡ where ğ‘ ğ‘‡ is the total number of samples and ğ‘ ğ‘‡ is the target probability distribution.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing",
      "text": "Data sets are preprocessed following  [26] . The dlib (http://dlib.net/) library is used to detect the face in each image and extract landmark coordinates on the face. The landmarks are used to center and rotate the face so that the eyes are level. The images are cropped such that the left eye is located 30% of the image width in pixels from the left edge. Images are resized to 256 by 256 pixels, converted to grayscale, and normalized to range [0, 1].",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction",
      "text": "Using the dlib library, we extract landmark points located at and around facial features such as the nose, eyes, mouth, and eyebrows as well as the perimeter of the face. These landmark locations are used to derive geometric features from FEA and AIFR literature based on pairs and triplets of landmarks. Inter-landmark distance features  [36] ,  [44] ,  [45] ,  [49]  are measured as the Euclidean distance between pairs of landmarks. Facial triangles  [43] ,  [44] ,  [48]  are extracted based on a Delaunay triangulation over the landmark locations. Each triangle is represented by a landmark triplet and has four associated features: the area of the triangle and its three angles expressed in radians. Fig.  1  shows examples of the extracted features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Landmark Feature Decomposition And Selection",
      "text": "We fit the BetaMix method  [52]  to find significant correlations between the extracted features from adult-child data and three experimental factors taken from the labeled data: expression, domain, and identity. Based on given data, the BetaMix method automatically learns a threshold that represents significant correlations among pairwise features and factors. The extracted features for the source and target data sets are concatenated to form a matrix of ğ‘ƒ predictors and ğ‘ samples, where ğ‘ƒ > ğ‘. Expression, domain, and identity are also entered into the model, yielding a (ğ‘ƒ + 3) Ã— ğ‘ matrix. We assume the data as ğ‘ƒ + 3 points in â„ ğ‘ . Subspaces of â„ ğ‘ lie on the Grassmann manifold, a special type of Riemannian manifold with a nonlinear structure  [52] . The Grassmann manifold ğ”¾ ğ‘,ğ‘‘ is used to study ğ‘‘-dimensional subspaces of â„ ğ‘  [52] . Principal angles ( ğœƒ 1 , â€¦ , ğœƒ ğ‘‘ ) between ğ‘‘ and ğ‘™-dimensional subspaces in ğ”¾ ğ‘,ğ‘‘ have an invariant measure for ğ‘‘ â‰¤ ğ‘™ that can be used to compute the volume and probability of their sets  [52] . These principal angles can be used to determine canonical correlations (ğœŒ 1 , â€¦ , ğœŒ ğ‘‘ ) as ğœŒ ğ‘˜ = cos ğœƒ ğ‘˜ with pairs of canonical variables {ğœ‘ ğ‘˜ , ğœ™ ğ‘˜ } ğ‘˜=1 ğ‘‘ where ğœ‘ ğ‘˜ âˆˆ ğ”¾ ğ‘,ğ‘‘ , ğœ™ ğ‘˜ âˆˆ ğ”¾ ğ‘,ğ‘™  [52] . When ğ‘‘ = ğ‘™ = 1, ğ”¾ ğ‘,1 corresponds to lines through the origin of Euclidean space  [52] . The line is a natural choice of projection due to its computational ease and interpretability. Furthermore, when ğ‘‘ = ğ‘™ = 1, the random variable sin 2 ğœƒ has the following Beta distribution  [52] :\n\nThus, we consider that the predictors and factors lie on ğ”¾ ğ‘,1 and define ğœƒ ğ‘˜ as the angle between the ğ‘˜th pair of predictors/factors, ğ‘˜ = 1, â€¦ . , ((ğ‘ƒ + 3)(ğ‘ƒ + 2)) 2\n\nâ„  [52] . We let ğœ† ğ‘˜ = sin 2 ğœƒ ğ‘˜ . A predictor or factor is considered 'null' if it corresponds to a randomly sampled point in â„ ğ‘ . As shown in  [52] , pairs of null predictors/factors are expected to be mutually perpendicular with high probability, even for moderate values of ğ‘. In relation to Equation (1), a mixture of Beta distributions may be used to determine if the pair represented by each ğœ† ğ‘˜ are 'null' (uncorrelated) or 'nonnull' (correlated). Then, the BetaMix model is defined as:\n\nwhere ğ‘‘ 0 (ğœ† ğ‘˜ ) is the null distribution, ğ‘‘(ğœ† ğ‘˜ ) is the alternative distribution, ğœ„ 0 ğ‘˜ ~ğµğ‘’ğ‘Ÿ(ğ‘ 0 ) is a random indicator that equals one if the ğ‘˜th pair of predictors corresponding to ğœ† ğ‘˜ are 'null', and ğ‘ 0 is the probability of the 'null' component. The 'null' component of the mixture model is defined by the Beta distribution:\n\n)\n\nwhere ğ‘  â‰¤ ğ‘ is the estimated effective sample size. The 'nonnull' component of the mixture model is defined as:\n\nwhere ğ›¼, ğ›½ > 0. The latent mixture variables (ğ›¼, ğ›½, ğ‘ ) are estimated using the EM algorithm. The E-step updates ğœ„ 0 ğ‘˜ with the posterior mean:\n\nand ğ‘ 0 is updated with its maximum likelihood estimate, ğ‘0 = ğ”¼(ğœ„). The M-step obtains the maximum likelihood estimates of ğ›¼, ğ›½, and ğ‘  by solving the following equations:\n\nwhere ğœ“(â€¢) is the digamma function. The E-and M-steps are repeated iteratively to update the parameters until convergence. Pairs of predictors/factors are considered 'nonnull' if the posterior null probability under ğ‘‘ 0 is smaller than threshold ğœ, ğœ„0 ğ‘˜ < ğœ. We denote the maximum ğœ† ğ‘˜ that satisfies ğœ„0 ğ‘˜ < ğœ as ğ‘„. Then, the screening rule for nonnull pairs may be written as ğœ† ğ‘˜ < ğ‘„. Since ğœ† ğ‘˜ = sin 2 ğœƒ ğ‘˜ and ğœŒ ğ‘˜ = cos ğœƒ ğ‘˜ , pairs with a correlation of at least ğœŒ = cos (sin -1 (ğ‘„ 1/2 )) are considered significant. Fig.  2  summarizes the BetaMix method.\n\nBased on the fitted Beta distribution, a graphical model is built where each node is a predictor or factor. An edge connects each nonnull predictor-predictor pair or factorpredictor pair. These edges represent a significant correlation between the connected nodes (predictors or factors). A subgraph formed by a factor and its adjacent predictor nodes captures the subset of predictors that are significantly correlated with the factor. Using these subgraphs, we decompose the feature vector into sets correlated with expression, domain, and identity. For our proposed FACE-BE-SELF approach, we select the features in the expression subgraph and prune features that also appear in the domain or identity subgraphs. The resulting selection of features is used in subsequent feature fusion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Deep Learning Models",
      "text": "We model supervised classification as the following inverse problem: ğ‘Œ = ğ‘“(ğ‘‹; ğ‘¤) For the MLP, we consider ğ‘‹ = ğ‘‰, where feature set ğ‘‰ âˆˆ ğ’± = â„ ğ‘ƒ ğµğ‘’ğ‘¡ğ‘ and ğ‘ƒ ğµğ‘’ğ‘¡ğ‘ is the number of BetaMix-selected features based on significant correlations with expression. The MLP has one hidden layer with 512 hidden units, ReLU activation, and dropout with a probability of 0.5. We consider a latent feature vector ğ‘ âˆˆ ğ’µ = â„ 512 produced by the hidden layer of the MLP. The hidden layer is followed by a softmax output layer of ğ¾ = (ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘’ğ‘ ) nodes. Uniform initialization is applied to all of the MLP weights.\n\nFor the CNN, we consider ğ‘‹ = ğ‘ˆ âˆˆ ğ’° = â„ 256Ã—256 and define ğ‘€(â€¢) as a sequence of three convolutional blocks, each consisting of a convolutional layer with 3x3 filter kernels followed by a 2x2 maximum response pooling, and a fully connected neural network with 512-dimensional hidden layer. This hidden layer also yields a latent feature vector ğ‘ âˆˆ ğ’µ = â„ 512 . We use the uniform distribution to initialize all weights. Dropout with a probability of 0.5 is applied to the 512-dimensional hidden layer. We define ğ¶(â€¢) as a ğ¾-dimensional fully connected layer with softmax mapping from ğ’µ onto ğ’´. This CNN architecture is shown in Fig.  3 .\n\nFor our proposed FACE-BE-SELF feature fusion model, we define ğ‘‹ as a tuple (ğ‘ˆ, ğ‘‰), where ğ‘ˆ âˆˆ ğ’° = â„ 256Ã—256 and ğ‘‰ âˆˆ ğ’± = â„ ğ‘ƒ ğµğ‘’ğ‘¡ğ‘ . Feature extractor ğ‘€(â€¢) is made up of CNN model ğº: ğ’° â†’ ğ’µ ğº , ğ’µ ğº = â„ 512 and the MLP model ğ»: ğ’± â†’ ğ’µ ğ» , ğ’µ ğ» = â„ 512 . We define the concatenation of ğ‘ ğº and ğ‘ ğ» spaces as ğ‘ âˆˆ ğ’µ = â„ 1024 . Then, we define ğ¶(â€¢) as a ğ¾-dimensional fully connected layer with softmax mapping from ğ’µ onto ğ’´. The architecture of the feature fusion model Fig.  2 . Overview of the BetaMix method. Data is entered as a matrix of ğ‘ƒ predictors + 3 factors and ğ‘ samples. Correlations ğœŒ among predictors/factors are computed and ğœƒ = cos -1 ğœŒ is used to compute the pairwise angles between predictors from the correlations. Next, a mixture of Beta distributions is fit over random variables sin 2 ğœƒ. Pairs of predictors are considered 'nonnull' if the posterior null probability under ğ‘‘ 0 is smaller than a threshold ğœ. These 'nonnull' pairs represent significant correlations between variables. The 'nonnull' pairs for expression, domain, and identity are used to decompose the feature vector into sets correlated with each of these three factors.  is shown in Fig.  4 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Deep Domain Adaptation",
      "text": "Rather than maximizing the performance on a target domain, our goal for deep domain adaptation is to optimize the model for maximum performance on both the source and target domains. We assume that the distribution shift between source and target domains can be attributed to covariate shift ğ‘ ğ‘‹ ğ‘† (ğ‘¥) â‰  ğ‘ ğ‘‹ ğ‘‡ (ğ‘¥), rather than a shift in the label distributions, i.e. we assume âˆ€ğ‘¥ âˆˆ ğ’³, ğ‘ ğ‘† (ğ‘Œ |ğ‘‹ = ğ‘¥) = ğ‘ ğ‘‡ (ğ‘Œ | ğ‘‹ = ğ‘¥). We adopt a dual stream architecture (Fig.  5 ) consisting of parallel feature extractors ğ‘€ ğ‘† (â€¢) and ğ‘€ ğ‘‡ (â€¢) for source and target distributions, respectively. Weights are shared between the two branches such that ğ‘€(â€¢) = ğ‘€ ğ‘† (â€¢) = ğ‘€ ğ‘‡ (â€¢). Paired source and target examples ğ‘‹ ğ‘† and ğ‘‹ ğ‘‡ are passed into their respective feature extractors to yield source and target latent representations, i.e., ğ‘ ğ‘† = ğ‘€(ğ‘‹ ğ‘† ) and ğ‘ ğ‘‡ = ğ‘€(ğ‘‹ ğ‘‡ ). Parallel classifiers ğ¶(â€¢), which also share weights, are trained with ğ‘ ğ‘† and ğ‘ ğ‘‡ to optimize performance on both source and target domains.\n\nThe model is optimized using three supervised loss functions: source classification loss â„’ ğ¶ğ‘  (ğ‘“), target classification loss â„’ ğ¶ğ‘¡ (ğ‘“), and domain alignment loss â„’ ğ·ğ´ (ğ‘€). We define â„’ ğ¶ğ‘  and â„’ ğ¶ğ‘¡ as the categorical cross-entropy loss given our multiclass expression classification problem. To address class imbalance in the training sets, we scale each sample's contribution to the overall loss by the frequency of its associated class in the training set. We define â„’ ğ·ğ´ as the contrastive alignment loss  [38] :\n\nWe follow  [38]  in selecting ğ‘‘(â€¢) and ğ‘˜(â€¢) as:\n\nand\n\nwhere â€–â€¢â€– ğ¹ is the Frobenius norm and margin ğ‘š = 1  [38] . The effect of â„’ ğ·ğ´ is to minimize (1) distance between samples of the same class from different domains, and (2) similarity between samples of different classes and domains. The overall loss is:\n\nwhere 0 < ğœ‰ < 1 is a scaling parameter for balancing the contribution of domain alignment loss.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "We perform preprocessing of the CAFE, ChildEFES, CK+, and Aff-Wild2 data sets following Section 2.2. To evaluate our proposed FACE-BE-SELF method, we consider data sets in two source/target pairs: CK+/CAFE (posed expressions only) and Aff-Wild2/ChildEFES (majority spontaneous expressions). We split each data set into multiple train, validation, and test sets using a 5x2 nested cross validation design. In the outer 5-fold cross validation loop, the data is split into train (4 folds) and test (1 fold) sets. In the inner loop, the train set is divided into 2 to yield inner train and validation sets for hyperparameter selection. The validation performance metrics are averaged across the two folds to yield the best hyperparameters. These hyperparameter selections are then used to train the model with the recombined outer loop train set and evaluate on the held-out test fold. This procedure is repeated a total of 5 times, such that each sample appears in one of the 5 test sets. To avoid inflation of performance estimates based on subject-specific features, we generate the cross validation folds such that each subject appears in one fold only and no subject appears in both train and test sets  [26] ,  [27] .\n\nWe fit the BetaMix method on the train sets for each source/target pair. The fitted mixture model identifies nonnull (significantly correlated) pairs of predictors/factors which are used to build a graph with predictors/factors represented as nodes and significant correlations represented as edges. By examining the subgraphs of each factor node and its adjacent predictor nodes, we report the mean number of significantly correlated features for each factor and the overlap of features appearing in multiple factor subgraphs. To select features for subsequent fusion, we consider the expression subgraph, pruning features that also appear in the domain and/or subject subgraphs. We assess the discriminability of our data-driven feature selection compared to that of features selected based upon a range of correlation thresholds (0.1, 0.2, â€¦, 1.0).\n\nThe average overall F1 performance on the inner 2-fold cross validation loop is used to select a value for the loss balancing parameter ğœ‰ (Equation (  13 )) for each of the outer 5-fold cross validation train sets. Other studies  [68] ,  [69]  have found that ğœ‰ is problem-specific and consider values in the range (0.00, 1.00). Due to high computational costs, we choose among representative low (0.01), moderate (0.3), and high (0.8) values in (0.00, 1.00). To better understand the contributions of CNN, BetaMix-selected landmark features, and domain adaptation to our proposed FACE-BE-SELF approach, we perform an ablation study.\n\nThen, we evaluate the performance of our proposed domain adaptation with FACE-BE-SELF approach on two source/target data set pairs and compare against four baseline models: 1) CNN trained on source data (source CNN)  [26] , 2) CNN trained on target data (target CNN)  [26] , 3) three transfer learning approaches (pretraining on source data then, a, training on target data  [26] , b, fine-tuning on the target data  [26] , or c, fine-tuning on a mixture of source and target data), and 4) two existing domain adaptation approaches  [27] ,  [38] .\n\nFor all experiments, we train deep models using the ADAM optimizer with a triangular learning rate policy  [70]  cycling between a minimum learning rate of ğœ ğ‘šğ‘–ğ‘› = 10 -5 and a maximum learning rate of ğœ ğ‘šğ‘ğ‘¥ = 10 -3 . We use a batch size ğ‘› = 32.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "We extract 68 landmark points on the face as shown in Fig.  1 (a) and use these to measure inter-landmark distances. Because the 68 Ã— 68 Euclidean distance matrix is symmetric with zeros (self-distance) in diagonal entries, the total number of inter-landmark distance features is",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Selection Of Landmark Features For Expression, Domain, And Identity Factors",
      "text": "We fit the BetaMix method on each of the 5-fold cross validation training sets for both source/target data set pairs. For a representative CK+/CAFE training set, BetaMix learns the screening rule ğœ† ğ‘˜ = sin 2 (ğœƒ) < ğ‘„ = 0.83. This is equivalent to an angle of 65.7Â° or less between the pairs of factors/features, or a correlation coefficient of at least ğœŒ = cos(65.7Â°) = 0.412. Averaging over the 5 training sets, the mean correlation threshold for CK+/CAFE is 0.414Â±0.025. Similarly, for a representative Aff-Wild2/ChildEFES training set, BetaMix learns the screening rule ğœ† ğ‘˜ = sin 2 (ğœƒ) < ğ‘„ = 0.99. This is equivalent to an angle of 84.3Â° or less between the pairs of factors/features, or a correlation coefficient of at least ğœŒ = cos(84.3Â°) = 0.100. For Aff-Wild2, the mean correlation threshold is 0.045Â±0.031. Fig.  6  shows the mean number of features correlated with 'expression', 'domain', and 'identity', as well as the number correlated with two out of three and all three factors. Considering the CK+/CAFE pair of data sets, Fig.  7  compares the performance of an MLP trained on features selected by the datadriven correlation threshold learned by BetaMix and those selected based upon a range of correlation thresholds (0.1, 0.2, â€¦, 1.0). There is not any feature with a correlation coefficient of 0.6 or greater for the expression factor.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Domain Adaptation",
      "text": "For each train/test split in the outer 5-fold cross validation loop, we select ğœ‰ based on the overall F1 score averaged over the 2 validation sets of the inner 2-fold cross validation loop. For CK+/CAFE, ğœ‰ = 0.01 is selected for all 5 train/test splits of the outer cross validation loop. For Aff-Wild2/ChildEFES, ğœ‰ = 0.01 is selected once, ğœ‰ = 0.3 is selected twice, and ğœ‰ = 0.8 is selected twice.\n\nAblation study results for FACE-BE-SELF are presented in Table  1 . The proposed model is compared with variants that selectively remove one or two of the following model components: CNN, BetaMix-selected landmark features, and domain adaptation. Fig.  8  compares the 5-fold cross validation performance of our proposed FACE-BE-SELF with multiple baselines for the CK+/CAFE and Aff-Wild2/ChildEFES source/target pairs, including CNNs trained on a single domain  [26] , transfer learning  [26] , and domain adaptation approaches  [27] . Since transfer learning performance on the source domain is expected to deteriorate after fine-tuning on target data only, we also compare with transfer learning fine-tuned on a mixture of source and target data. Fig.  9  plots one-versus-rest multiclass receiver operating characteristic (ROC) curves and reports the associated area under curve (AUC) metrics for the proposed FACE-BE-SELF approach.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "This paper presents FACE-BE-SELF for classification of adult and child facial expressions through deep domain adaptation and the fusion of facial landmark features correlated with expressions. Our experiments on four data sets and comparison of eight facial expression classification methods have revealed four important findings as follows. First, the decomposition of landmark features for  While the CNN feature space is known to exhibit adultchild domain shift  [25] ,  [26] ,  [27] ,  [28] , our results suggest the domain shift to be dependent on the domain data set pair. The underlying data dependency (differences in overlap regions of Fig.  6  (a) and Fig.  6 (b) ) may be attributed to differences in sample size and demographics, age ranges, and/or mixture of posed/spontaneous expressions  [16] ,  [17] . Second, a parsimonious feature selection is obtained from the expression subgraph after eliminating features significantly correlated with the other factors (Fig.  7 ). Third, our ablation study shows that fusing these selected landmark features and CNN-extracted image features improves the expression classification performance for both child and adult data (Table  1 ). Fourth, our proposed FACE-BE-SELF method outperforms all baseline models for the posed data sets (CK+/CAFE) and performs competitively for the data sets with spontaneous expressions (Aff-Wild2/ChildEFES). The sections to follow provide detailed discussions in addition to and expanding upon these four key findings.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Selection And Fusion Of Facial Landmark Features",
      "text": "Our comparison of the proposed data-driven feature selection and a range of correlation thresholds reveals that our data driven BetaMix approach yields the largest correlation threshold prior to substantial performance degradation (Fig.  7 ). This threshold corresponds to a parsimonious selection of highly correlated features that preserve useful complementary information for expressions that is discarded at higher thresholds. Furthermore, fusing CNN-extracted features with the selected landmark features improves the classification performance of child and adult facial expressions (Table  1 ). Like age estimation and AIFR, facial expression classification also benefits from the fusion of geometric landmark and texture features  [39] ,  [40] . Given that the feature fusion model outperforms CNN features only, our selected landmark features provide complementary information representative of expressions beyond that learned by the CNN (Table  1 ). The effectiveness of selected features in classification suggests that the proposed BetaMix correlation coefficient threshold is an effective metric in optimizing feature selection for facial expression classification.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Domain Adaptation For Expression Learning",
      "text": "Our findings suggest that domain adaptation methods provide robust representation learning of adult and child facial expressions (Fig.  8 ). During adaptation, source and target performance are jointly optimized via â„’ ğ¶ğ‘  and â„’ ğ¶ğ‘¡ while the class conditional distributions are aligned using â„’ ğ·ğ´ . This optimization procedure ensures balanced performance on both domains. Our findings also confirm that supervision on both domains (as in transfer learning) or a method of domain alignment is required for effective classification (Fig.  8 ). For both CK+/CAFE and Aff-Wild2/ChildEFES source-target pairs, we observe poor cross domain performance for CNNs trained on a single domain (Fig.  8 ). This poor cross domain performance is indicative of distribution shift and replicates the findings of multiple prior studies  [26] ,  [27] ,  [28] .\n\nOur proposed FACE-BE-SELF method yields higher source and target average overall F1 scores for CK+/CAFE than all baseline models, with similar average overall F1 scores for source and target of 0.8443 and 0.8303, respectively, with a difference of 0.0140 (Fig.  8 ). Spontaneous expression classification (Aff-Wild2/ChildEFES) is more challenging than classification of posed facial expressions such as CK+ and CAFE. For example, Aff-Wild2 is the most challenging of the four data sets that we use to evaluate our approach. Current state-of-the-art performance on the official test set for Aff-Wild2 is an overall F1 score of 0.3587, achieved by the best performing team at the recent 3rd Affective Behavior Analysis in-the-wild Competition  [71] . Please note that our results are not directly comparable as  we perform cross validation rather than use the official test set. For Aff-Wild2/ChildEFES, the best performing models are FACE-BE-SELF and fine-tuning on a mixture of source and target data (Fig.  8 ). Compared to fine-tuning on a mixture of source and target data, FACE-BE-SELF performs better on ChildEFES (average overall F1 score 0.4557 > 0.4214) and worse on Aff-Wild2 (average overall F1 score 0.5201 < 0.6032) but has a smaller difference in source and target performance (0.0644 vs 0.1897). Thus, despite poorer performance on Aff-Wild2, FACE-BE-SELF offers better target (ChildEFES) performance and more balanced performance between source and target.\n\nThe ROC curves for CK+/CAFE (Fig.  9  (a)(b)) reveal that despite class imbalance during training, FACE-BE-SELF learns to recognize all classes with AUCs near unity, indicating high sensitivity and specificity. For ChildEFES, the ROC curves show that all classes perform better than chance (Fig.  9 (d) ). 'Surprise', with its distinctive open mouth appearance achieves an AUC of unity while negative expressions 'anger' and 'sad' prove more difficult. The ROC curves for Aff-Wild2 (Fig.  9 (c) ) reflect the challenging nature of the data set with an overall average AUC of 0.52, close to chance level (0.50). The best performing classes are 'anger' (AUC 0.65) and 'fear' (AUC 0.65), while 'disgust' performs worst (0.15).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Explaining Feature Contributions",
      "text": "We perform additional analysis using SHapley Additive exPlanations (SHAP)  [72]  to explain the contributions of different features to the classification of child and adult expressions. To quantify the contributions of both BetaMixselected landmark features and CNN features in the feature fusion model, we use the expected gradients method  [73]  as implemented in the SHAP library (https:// github.com/slundberg/shap) to obtain and visualize the (approximate) SHAP values for both landmark and CNN features. Fig.  10  visualizes the SHAP values for source (CK+) and target (CAFE) domains. We use the same image from the CK+ data set for all visualizations. Fig.  10 (a)  shows the SHAP values associated with source and target image inputs to the CNN feature extractor. For both source and target, areas of the input with the greatest (positive or negative) contribution to expression classification are those involved in producing facial expressions: the eyebrows, eyes, nose, and mouth. The symmetric features (the second area at the left corner of the lips and area between the right eye and eyebrow) are also among the top ten most important features, but in different orders of importance. In addition to these six triangle area features, three inter-landmark distance features are ranked among the top ten for both domains. These features represent distances between the mouth and eyes (2 features) and the mouth and nose (1 feature). As with the image input, the top ten landmark features represent important areas of the face for producing expressions: the eyebrows, eyes, and mouth.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "Although the BetaMix method is robust to dependence among samples, the high degree of similarity among faces (compared to other types of data) and universality of expressions may yield a small effective sample size. Even with a small effective sample size, BetaMix is shown to capture significantly correlated landmark features. However, there may be features that are useful for classification of expressions but are not significantly correlated with expression based on the BetaMix-learned minimum correlation coefficient. Furthermore, the data dependency of BetaMix feature selections may affect performance on unseen data sets. An additional adaptation or fine-tuning step may be required for these models to address possible data dependency. The age ranges studied cover 2 to 8 years for CAFE, 4 to 6 years for ChildEFES, and 18+ years for CK+. Aff-Wild2 does not report specific age ranges. Further research is required to determine if the adapted models are capable of generalizing to participants in other age groups, e.g., teens and pre-teens.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we propose for the first time in literature novel deep domain adaptative FACE-BE-SELF for concurrent learning of adult and child facial expressions. FACE-BE-SELF yields a meaningful and effective selection of features that are correlated with expressions. The explainability and visualization of SHAP values corroborate the facial expression classification performance of our method. The superiority of our method over existing transfer learning and domain adaption methods satisfies the need for a systematic feature selection, feature fusion, and domain adaptation to perform domain-invariant classification. In future work, we plan to investigate the generalizability of this approach to other age groups and data acquisition pipelines. We hope that this approach may be used to yield automated, objective assessments of age or domain varying patterns in other applications.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows examples of the extracted features.",
      "page": 4
    },
    {
      "caption": "Figure 1: Sample image overlaid with: (a) facial landmark points, (b) in-",
      "page": 4
    },
    {
      "caption": "Figure 3: For our proposed FACE-BE-SELF feature fusion model,",
      "page": 5
    },
    {
      "caption": "Figure 2: Overview of the BetaMix method. Data is entered as a matrix of ğ‘ƒ predictors + 3 factors and ğ‘ samples. Correlations ğœŒ among predic-",
      "page": 5
    },
    {
      "caption": "Figure 3: CNN architecture. Model ğ‘“(âˆ™) is partitioned into feature extrac-",
      "page": 5
    },
    {
      "caption": "Figure 4: Feature fusion architecture. Model ğ‘“(âˆ™) is partitioned into fea-",
      "page": 5
    },
    {
      "caption": "Figure 4: 2.6 Deep Domain Adaptation",
      "page": 6
    },
    {
      "caption": "Figure 5: ) consisting of parallel feature extractors ğ‘€ğ‘†(âˆ™) and",
      "page": 6
    },
    {
      "caption": "Figure 5: Domain adaptation framework. Source-target pairs (ğ‘‹ğ‘†, ğ‘‹ğ‘‡) are",
      "page": 6
    },
    {
      "caption": "Figure 1: (a) and use these to measure inter-landmark distances.",
      "page": 7
    },
    {
      "caption": "Figure 1: (c) visualizes the De-",
      "page": 7
    },
    {
      "caption": "Figure 6: shows the",
      "page": 7
    },
    {
      "caption": "Figure 7: compares the perfor-",
      "page": 7
    },
    {
      "caption": "Figure 8: compares the 5-fold cross",
      "page": 7
    },
    {
      "caption": "Figure 9: plots one-versus-rest mul-",
      "page": 7
    },
    {
      "caption": "Figure 6: Mean number of features correlated with expression, do-",
      "page": 7
    },
    {
      "caption": "Figure 7: CK+/CAFE 5-fold cross validation average overall F1 scores for MLP",
      "page": 7
    },
    {
      "caption": "Figure 6: (a)) while the factor subgraphs of Aff-",
      "page": 8
    },
    {
      "caption": "Figure 6: (b)). Features concurrently correlated",
      "page": 8
    },
    {
      "caption": "Figure 6: (a) and Fig. 6 (b)) may be attributed to",
      "page": 8
    },
    {
      "caption": "Figure 7: ). This threshold corresponds to a parsimonious",
      "page": 8
    },
    {
      "caption": "Figure 8: ). During adaptation, source and",
      "page": 8
    },
    {
      "caption": "Figure 8: ). For both CK+/CAFE and Aff-",
      "page": 8
    },
    {
      "caption": "Figure 8: ). This poor cross domain performance is in-",
      "page": 8
    },
    {
      "caption": "Figure 8: ). Spontaneous ex-",
      "page": 8
    },
    {
      "caption": "Figure 8: 5-fold cross validation overall F1 score for comparison models.",
      "page": 8
    },
    {
      "caption": "Figure 9: FACE-BE-SELF ROC Curves for various data sets.",
      "page": 8
    },
    {
      "caption": "Figure 8: ). Compared to fine-tuning on a mix-",
      "page": 9
    },
    {
      "caption": "Figure 9: (a)(b)) reveal",
      "page": 9
    },
    {
      "caption": "Figure 9: (d)). â€˜Surpriseâ€™, with its distinctive open",
      "page": 9
    },
    {
      "caption": "Figure 9: (c)) reflect the challenging",
      "page": 9
    },
    {
      "caption": "Figure 10: visualizes the SHAP values for source",
      "page": 9
    },
    {
      "caption": "Figure 10: (b) and Fig 10 (c) visualize",
      "page": 9
    },
    {
      "caption": "Figure 10: (d) plots these",
      "page": 9
    },
    {
      "caption": "Figure 10: Visualization of SHAP values for FACE-BE-SELF: (a) image input to CNN, (b) top 10 source landmark features, (c) top 10 target",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Â© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material": "for advertising or promotional purposes, collecting new collected works for resale or redistribution to servers or lists, or reuse of any copyrighted component of this"
        },
        {
          "Â© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material": "work in other works."
        }
      ],
      "page": 1
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Father-child play: A systematic review of its frequency, characteristics and potential impact on children's development",
      "authors": [
        "A Amodia-Bidakowska",
        "C Laverty",
        "P Ramchandani"
      ],
      "year": "2020",
      "venue": "Developmental Review"
    },
    {
      "citation_id": "2",
      "title": "Teaching and learning with children: Impact of reciprocal peer learning with a social robot on children's learning and emotive engagement",
      "authors": [
        "H Chen",
        "H Park",
        "C Breazeal"
      ],
      "year": "2020",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "3",
      "title": "Children's perspective on the emotional process",
      "authors": [
        "M Terwogt",
        "H Stegge"
      ],
      "year": "2021",
      "venue": "The social child"
    },
    {
      "citation_id": "4",
      "title": "Attentive or not? Toward a machine learning approach to assessing students' visible engagement in classroom instruction",
      "authors": [
        "P Goldberg"
      ],
      "year": "2021",
      "venue": "Educational Psychology Review"
    },
    {
      "citation_id": "5",
      "title": "Students' affective content analysis in smart classroom environment using deep learning techniques",
      "authors": [
        "S Gupta",
        "T Ashwin",
        "R Guddeti"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "6",
      "title": "Multimodal engagement analysis from facial videos in the classroom",
      "authors": [
        "Ã– SÃ¼mer",
        "P Goldberg",
        "S D'mello",
        "P Gerjets",
        "U Trautwein",
        "E Kasneci"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Automatic detection of pain from facial expressions: a survey",
      "authors": [
        "T Hassan"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Pain assessment from facial expression: Neonatal convolutional neural network (N-CNN)",
      "authors": [
        "G Zamzmi",
        "R Paul",
        "D Goldgof",
        "R Kasturi",
        "Y Sun"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "9",
      "title": "Deep convolution network based emotion analysis towards mental health care",
      "authors": [
        "Z Fei"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "10",
      "title": "Deep learning in mental health outcome research: a scoping review",
      "authors": [
        "C Su",
        "Z Xu",
        "J Pathak",
        "F Wang"
      ],
      "year": "2020",
      "venue": "Translational Psychiatry"
    },
    {
      "citation_id": "11",
      "title": "Computer-analyzed facial expression as a surrogate marker for autism spectrum social core symptoms",
      "authors": [
        "K Owada"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "12",
      "title": "A Feasibility Study of Autism Behavioral Markers in Spontaneous Facial, Visual, and Hand Movement Response Data",
      "authors": [
        "M Samad",
        "N Diawara",
        "J Bobzien",
        "J Harrington",
        "M Witherow",
        "K Iftekharuddin"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "13",
      "title": "A pilot study to identify autism related traits in spontaneous facial actions using computer vision",
      "authors": [
        "M Samad",
        "N Diawara",
        "J Bobzien",
        "C Taylor",
        "J Harrington",
        "K Iftekharuddin"
      ],
      "year": "2019",
      "venue": "Research in Autism Spectrum Disorders"
    },
    {
      "citation_id": "14",
      "title": "Enhancing game experience with facial expression recognition as dynamic balancing",
      "authors": [
        "M Akbar",
        "M Ilmi",
        "I Rumayar",
        "J Moniaga",
        "T.-K Chen",
        "A Chowanda"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "15",
      "title": "Modeling and adjusting in-game difficulty based on facial expression analysis",
      "authors": [
        "P Blom",
        "S Bakkes",
        "P Spronck"
      ],
      "year": "2019",
      "venue": "Entertainment Computing"
    },
    {
      "citation_id": "16",
      "title": "A survey on: Facial emotion recognition invariant to pose, illumination and age",
      "authors": [
        "S Bhattacharya",
        "M Gupta"
      ],
      "year": "2019",
      "venue": "2019 Second International Conference on Advanced Computational and Communication Paradigms"
    },
    {
      "citation_id": "17",
      "title": "A Survey of AI-Based Facial Emotion Recognition: Features, ML & DL Techniques, Age-Wise Datasets and Future Directions",
      "authors": [
        "C Dalvi",
        "M Rathod",
        "S Patil",
        "S Gite",
        "K Kotecha"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "the 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "19",
      "title": "Noldus Information Technology bv",
      "year": "2022",
      "venue": "Noldus Information Technology bv"
    },
    {
      "citation_id": "20",
      "title": "Facial Expession Analysis",
      "year": "2022",
      "venue": "iMotions A/S"
    },
    {
      "citation_id": "21",
      "title": "Comprehensive database for facial expression analysis",
      "authors": [
        "T Kanade",
        "J Cohn",
        "Y Tian"
      ],
      "year": "2000",
      "venue": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "22",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops"
    },
    {
      "citation_id": "23",
      "title": "The growth and development of the soft tissues of the human face",
      "authors": [
        "P Burke",
        "C Hughes-Lawson"
      ],
      "year": "1988",
      "venue": "Journal of anatomy"
    },
    {
      "citation_id": "24",
      "title": "Children facial expression production: influence of age, gender, emotion subtype, elicitation condition and culture",
      "authors": [
        "C Grossard"
      ],
      "year": "2018",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "25",
      "title": "On Automatically Assessing Children's Facial Expressions Quality: A Study, Database, and Protocol",
      "authors": [
        "A Dapogny"
      ],
      "year": "2019",
      "venue": "Frontiers in Computer Science, Original Research"
    },
    {
      "citation_id": "26",
      "title": "Transfer learning approach to multiclass classification of child facial expressions",
      "authors": [
        "M Witherow",
        "M Samad",
        "K Iftekharuddin"
      ],
      "year": "2019",
      "venue": "SPIE Optical Engineering + Applications"
    },
    {
      "citation_id": "27",
      "title": "Learning latent expression labels of child facial expression images through data-limited domain adaptation and transfer learning",
      "authors": [
        "M Witherow",
        "W Shields",
        "M Samad",
        "K Iftekharuddin"
      ],
      "year": "2020",
      "venue": "SPIE Optical Engineering + Applications"
    },
    {
      "citation_id": "28",
      "title": "Facial Expression Recognition for Children: Can Existing Methods Tuned for Adults Be Adopted for Children?",
      "authors": [
        "Z Zheng",
        "X Li",
        "J Barnes",
        "C.-H Park",
        "M Jeon"
      ],
      "year": "2019",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "29",
      "title": "The Child Affective Facial Expression (CAFE) set. Databrary",
      "authors": [
        "V Lobue",
        "C Thrasher"
      ],
      "year": "2014",
      "venue": "The Child Affective Facial Expression (CAFE) set. Databrary"
    },
    {
      "citation_id": "30",
      "title": "The Child Affective Facial Expression (CAFE) set: validity and reliability from untrained adults",
      "authors": [
        "V Lobue",
        "C Thrasher"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "31",
      "title": "The Child Emotion Facial Expression Set: A Database for Emotion Recognition in Children",
      "authors": [
        "J NegrÃ£o"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "32",
      "title": "A novel database of children's spontaneous facial expressions (LIRIS-CSE)",
      "authors": [
        "R Khan",
        "A Crenn",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2019",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "33",
      "title": "Age-Invariant Facial Expression Classification Method Using Deep Learning",
      "authors": [
        "T Rebanowako",
        "A Yadav",
        "R Joshi"
      ],
      "year": "2021",
      "venue": "Proceedings of 6th International Conference on Recent Trends in Computing"
    },
    {
      "citation_id": "34",
      "title": "Facial Expression Recognition Influenced by Human Aging",
      "authors": [
        "G Guo",
        "X Guo",
        "Li"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Age estimation via face images: a survey",
      "authors": [
        "R Angulu",
        "J Tapamo",
        "A Adewumi"
      ],
      "year": "2018",
      "venue": "EURASIP Journal on Image and Video Processing"
    },
    {
      "citation_id": "36",
      "title": "Age-Group Estimation Using Feature and Decision Level Fusion",
      "authors": [
        "R Angulu",
        "J Tapamo",
        "A Adewumi"
      ],
      "year": "2018",
      "venue": "The Computer Journal"
    },
    {
      "citation_id": "37",
      "title": "Expression-Invariant Age Estimation Using Structured Learning",
      "authors": [
        "Z Lou",
        "F Alnajar",
        "J Alvarez",
        "N Hu",
        "T Gevers"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Unified deep supervised domain adaptation and generalization",
      "authors": [
        "S Motiian",
        "M Piccirilli",
        "D Adjeroh",
        "G Doretto"
      ],
      "year": "2017",
      "venue": "Unified deep supervised domain adaptation and generalization"
    },
    {
      "citation_id": "39",
      "title": "Neural networks for facial age estimation: a survey on recent advances",
      "authors": [
        "P Punyani",
        "R Gupta",
        "A Kumar"
      ],
      "year": "2020",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "40",
      "title": "Age invariant face recognition: a survey on facial aging databases, techniques and effect of aging",
      "authors": [
        "M Sawant",
        "K Bhurchandi"
      ],
      "year": "2019",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "41",
      "title": "Age-invariant face recognition system using combined shape and texture features",
      "authors": [
        "A Osman Ali",
        "V Sagayan",
        "A Saeed",
        "H Ameen",
        "A Aziz"
      ],
      "year": "2015",
      "venue": "IET Biometrics"
    },
    {
      "citation_id": "42",
      "title": "Age Invariant Face Recognition Methods: A Review",
      "authors": [
        "K Baruni",
        "N Mokoena",
        "M Veeraragoo",
        "R Holder"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Computational Science and Computational Intelligence (CSCI)"
    },
    {
      "citation_id": "43",
      "title": "Face recognition based on facial landmark detection",
      "authors": [
        "A Juhong",
        "C Pintavirooj"
      ],
      "year": "2017",
      "venue": "2017 10th Biomedical Engineering International Conference (BMEiCON)"
    },
    {
      "citation_id": "44",
      "title": "Age Group Estimation using Facial Features",
      "authors": [
        "A Chinnnaswamy",
        "P Kumar",
        "S Aravind"
      ],
      "year": "2014",
      "venue": "International Journal of Emerging Technologies in Computational and Applied Sciences"
    },
    {
      "citation_id": "45",
      "title": "Estimation of Age Groups based on Facial Features",
      "authors": [
        "A Srivastava"
      ],
      "year": "2018",
      "venue": "International Journal of Engineering and Technical Research"
    },
    {
      "citation_id": "46",
      "title": "An Accurate Facial Expression Detector using Multi-Landmarks Selection and Local Transform Features",
      "authors": [
        "S Rizwan",
        "A Jalal",
        "K Kim"
      ],
      "year": "2020",
      "venue": "An Accurate Facial Expression Detector using Multi-Landmarks Selection and Local Transform Features"
    },
    {
      "citation_id": "47",
      "title": "Facial expression detection using Six Facial Expressions Hexagon (SFEH) model",
      "authors": [
        "M Murtaza",
        "M Sharif",
        "M Abdullahyasmin",
        "T Ahmad"
      ],
      "year": "2019",
      "venue": "presented at the 2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)"
    },
    {
      "citation_id": "48",
      "title": "Influence of shape and texture features on facial expression recognition",
      "authors": [
        "A Barman",
        "P Dutta"
      ],
      "year": "2019",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "49",
      "title": "Micro-Expression Spotting Using Facial Landmarks",
      "authors": [
        "K Beh",
        "K Goh"
      ],
      "year": "2019",
      "venue": "IEEE 15th International Colloquium on Signal Processing & Its Applications (CSPA)"
    },
    {
      "citation_id": "50",
      "title": "Hidden Factor Analysis for Age Invariant Face Recognition",
      "authors": [
        "D Gong",
        "Z Li",
        "D Lin",
        "J Liu",
        "X Tang"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "51",
      "title": "Modified Hidden Factor Analysis for Cross-Age Face Recognition",
      "authors": [
        "H Li",
        "H Zou",
        "H Hu"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "52",
      "title": "On Graphical Models and Convex Geometry",
      "authors": [
        "H Bar",
        "M Wells"
      ],
      "year": "2021",
      "venue": "On Graphical Models and Convex Geometry",
      "arxiv": "arXiv:2106.14255"
    },
    {
      "citation_id": "53",
      "title": "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "54",
      "title": "ABAW: Learning from Synthetic Data & Multi-task Learning Challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "ABAW: Learning from Synthetic Data & Multi-task Learning Challenges"
    },
    {
      "citation_id": "55",
      "title": "Analysing Affective Behavior in the First ABAW 2020 Competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "56",
      "title": "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "57",
      "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "58",
      "title": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "59",
      "title": "Analysing Affective Behavior in the second ABAW2 Competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)"
    },
    {
      "citation_id": "60",
      "title": "Teaching Facial Expression Production in Autism: The Serious Game JEMImE",
      "authors": [
        "C Grossard"
      ],
      "year": "2019",
      "venue": "Creative Education"
    },
    {
      "citation_id": "61",
      "title": "Job interview training targeting nonverbal communication using an android robot for individuals with autism spectrum disorder",
      "authors": [
        "H Kumazaki"
      ],
      "year": "2019",
      "venue": "Autism"
    },
    {
      "citation_id": "62",
      "title": "Inclusion of third-person perspective in CAVE-like immersive 3D virtual reality role-playing games for social reciprocity training of children with an autism spectrum disorder",
      "authors": [
        "W.-T Tsai",
        "I.-J Lee",
        "C.-H Chen"
      ],
      "year": "2021",
      "venue": "Universal Access in the Information Society"
    },
    {
      "citation_id": "63",
      "title": "Give me a kiss! An integrative rehabilitative training program with motor imagery and mirror therapy for recovery of facial palsy",
      "authors": [
        "E Medica"
      ],
      "year": "2019",
      "venue": "European journal of physical and rehabilitation medicine"
    },
    {
      "citation_id": "64",
      "title": "Effects of facial rehabilitation exercise on the mood, facial expressions, and facial muscle activities in patients with Parkinson's disease]",
      "authors": [
        "R Okamoto",
        "K Adachi",
        "K Mizukami"
      ],
      "year": "2019",
      "venue": "Nihon Ronen Igakkai Zasshi"
    },
    {
      "citation_id": "65",
      "title": "Face Behavior Ã  la carte: Expressions, Affect and Action Units in a Single Network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "1910",
      "venue": "ArXiv"
    },
    {
      "citation_id": "66",
      "title": "Deep Affect Prediction in-the-Wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond",
      "authors": [
        "D Kollias"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "67",
      "title": "Aff-wild: valence and arousal'In-the-Wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "68",
      "title": "Balancing discriminability and transferability for source-free domain adaptation",
      "authors": [
        "J Kundu"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "69",
      "title": "Domain Decorrelation with Potential Energy Ranking",
      "authors": [
        "S Pei",
        "J Sun",
        "S Xiang",
        "G Meng"
      ],
      "year": "2022",
      "venue": "Domain Decorrelation with Potential Energy Ranking",
      "arxiv": "arXiv:2207.12194"
    },
    {
      "citation_id": "70",
      "title": "Cyclical learning rates for training neural networks",
      "authors": [
        "L Smith"
      ],
      "year": "2017",
      "venue": "Cyclical learning rates for training neural networks"
    },
    {
      "citation_id": "71",
      "title": "ABAW: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Computer Vision-ECCV 2022 Workshops: Tel"
    },
    {
      "citation_id": "72",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "73",
      "title": "Improving performance of deep learning models with axiomatic attribution priors and expected gradients",
      "authors": [
        "G Erion",
        "J Janizek",
        "P Sturmfels",
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "74",
      "title": "She is currently a PhD candidate at the Vision Laboratory, Dept. of Electrical and Computer Engineering, ODU, and a 2020 NSF Graduate Research Fellow. Her research interests include computer vision, deep learning, human-computer interaction",
      "venue": "She is currently a PhD candidate at the Vision Laboratory, Dept. of Electrical and Computer Engineering, ODU, and a 2020 NSF Graduate Research Fellow. Her research interests include computer vision, deep learning, human-computer interaction"
    },
    {
      "citation_id": "75",
      "title": "His research interests include machine learning, health informatics, computer vision, and natural language processing",
      "authors": [
        "D Manar"
      ],
      "year": "2006",
      "venue": "Diawara received his B.S. at the University Cheick Anta Diop in Dakar, Senegal; MaÃ®trise in Mathematics at University of Le Havre, France; Master's in Statistics at University South Alabama; and Ph"
    },
    {
      "citation_id": "76",
      "title": "His professional interests include statistical modeling, shrinkage estimation, high throughput applications, Bayesian statistics, variable selection, and machine learning. From 1995 to 1997, he was with Motorola, Israel, as a computer programmer. From 1997 until 2003 he worked for MicroPatent, LLC, where he held the position of Director of Software Development",
      "authors": [
        "Y Haim"
      ],
      "venue": "His professional interests include statistical modeling, shrinkage estimation, high throughput applications, Bayesian statistics, variable selection, and machine learning. From 1995 to 1997, he was with Motorola, Israel, as a computer programmer. From 1997 until 2003 he worked for MicroPatent, LLC, where he held the position of Director of Software Development"
    },
    {
      "citation_id": "77",
      "title": "Iftekharuddin (SM'02) received the B.Sc. degree in electrical and electronic engineering from the Bangladesh Institute of Technology",
      "authors": [
        "M Khan"
      ],
      "venue": "1989, and the M.S. and Ph.D. degrees in electrical and computer engineering from the University of"
    },
    {
      "citation_id": "78",
      "title": "VA, USA, and the Director of the ODU Vision Laboratory. His current research interests include computational modeling of intelligent systems and reinforcement learning, stochastic medical image analysis, intersection of bioinformatics and medical image analysis, distortion-invariant automatic target recognition, biologically inspired human and machine centric recognition, and machine learning for robotics",
      "authors": [
        "Dayton Dayton",
        "O Usa"
      ],
      "year": "1991",
      "venue": "VA, USA, and the Director of the ODU Vision Laboratory. His current research interests include computational modeling of intelligent systems and reinforcement learning, stochastic medical image analysis, intersection of bioinformatics and medical image analysis, distortion-invariant automatic target recognition, biologically inspired human and machine centric recognition, and machine learning for robotics"
    }
  ]
}