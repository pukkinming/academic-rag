{
  "paper_id": "2009.11005v3",
  "title": "Exploiting Vietnamese Social Media Characteristics For Textual Emotion Recognition In Vietnamese",
  "published": "2020-09-23T08:49:39Z",
  "authors": [
    "Khang Phuoc-Quy Nguyen",
    "Kiet Van Nguyen"
  ],
  "keywords": [
    "Textual Emotion Recognition",
    "Emotion Prediction",
    "Vietnamese Characteristics",
    "Machine Learning",
    "Multinomial Logistic Regression"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Textual emotion recognition has been a promising research topic in recent years. Many researchers aim to build more accurate and robust emotion detection systems. In this paper, we conduct several experiments to indicate how data pre-processing affects a machine learning method on textual emotion recognition. These experiments are performed on the Vietnamese Social Media Emotion Corpus (UIT-VSMEC) as the benchmark dataset. We explore Vietnamese social media characteristics to propose different pre-processing techniques, and key-clause extraction with emotional context to improve the machine performance on UIT-VSMEC. Our experimental evaluation shows that with appropriate pre-processing techniques based on Vietnamese social media characteristics, Multinomial Logistic Regression (MLR) achieves the best F1-score of 64.40%, a significant improvement of 4.66% over the CNN model built by the authors of UIT-VSMEC (59.74%).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Analyzing people's opinions has never been an easy task due to their versatility and ambiguity. Answering the question \"What do people think about X?\" is quite simple; we achieve this by extracting information from what they said or wrote, but to understand their feelings or answer the question \"How do people feel about X?\" is a difficult task. There are many ways to express the same emotion, but multiple emotions sometimes have the same expression. For example, a smile does not always mean happy, but it can be an act of disgust; people may cry when they feel sad, or when they are afraid of something or even when they are happy. When it comes to emotion recognition using a computer, this problem is more complicated. Emotion detection can be performed based on different sources of information such as speech, text, or image, but text data is still the most popular form, especially in social media. Textual emotion recognition has seen a growing interest from many researchers  [1, 2, 3, 4, 5] .\n\nThere are several approaches to build an emotion detection system, but they all have to go through one step: data preprocessing. Data pre-processing is the process that aims to remove as much as possible noise from the data. Therefore, a variety of pre-processing techniques have been tested by many researchers, and they are proven to be efficient for some particular machine learning tasks.\n\nIn this work, we will focus on evaluating the impact of several pre-processing techniques on the emotion recognition system's performance built on Vietnamese social media data (Vietnamese microtext  [6] ). With a given Vietnamese microtext, our goal is to clean this microtext and then predict the emotion it contains.\n\nOur key contribution is to compile an appropriate set of pre-processing techniques for Vietnamese social media data. We build several classifiers for a different combination of preprocessing techniques by using Multinomial Logistic Regression (MLR)  [7]  on the benchmark dataset UIT-VSMEC  [5] . Finally, we compare the F1-score of each classifier to find out the best combination of pre-processing techniques.\n\nThe structure of this paper is as follows. Related studies are presented in Section II. Section III describes the dataset used in our experiments. Section IV discusses our proposed methods in which we explain the reasons behind each method. In Section V, we present the experiments and results for our proposed method. Finally, we sum up our findings and discuss future work in Section VI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In the past decade, there were many studies on the design of methods and the development of frameworks for textual emotion detection. A detailed and complete overview of approaches can be found in  [8, 9] .\n\nBrilis et al.  [1]  used machine learning approaches to create an automatic mood categories classifier for song lyrics by performing pre-processing steps such as stemming, stop words, and punctuation marks removal combined with a bag-ofwords approach using TF-IDF score (term frequency-inverse document frequency). The authors indicated that the Random Forest algorithm reported the best results, with approximately 71.50% accuracy on the stemmed dataset and 93.70% on the unstemmed.\n\nHo and Cao  [2]  proposed a hidden Markov logic approach to predict the emotion expressed by a given text. The authors' method achieved an F-score of 35.00% on the ISEAR (International Survey on Emotion Antecedents and Reaction) dataset. The authors ignored semantic and syntactic features of the sentence analysis, which made it non-context sensitive and can be the cause for the low F-score.\n\nGhazi, Inkpen, and Szpakowicz  [3]  combined keywordbased, lexicon-based, and machine learning-based approaches to compare the prior emotion of a word with its contextual emotion and their effect on the sentiment expressed by the arXiv:2009.11005v3 [cs.CL] 27 Oct 2020 sentence. Authors showed that using Logistic Regression and their features significantly outperform their baselines (the null hypothesis that was using the least and the most frequent emotion label to predict) and their Support Vector Machines (SVM) model. Da Silva, Hruschka, and Hruschka Jr  [4]  conducted tweet sentiment analysis using classifier ensembles. A classifier ensemble was formed using several machine learning classifiers: Random Forest, SVM, Multinomial Naive Bayes, and Logistic Regression. In their study, authors experimented with various tweet datasets and reported that the classifier ensemble could improve classification accuracy.\n\nIn work presented in  [5] , the authors built the first standard Vietnamese Social Media Emotion Corpus (UIT-VSMEC). This corpus contains exactly 6,927 emotion-annotated sentences (an average agreement of 82.00% across the seven emotions) and then measure two machine learning models and two deep learning models on this dataset. The authors reported the highest weighted F1-score of 59.74% by using the CNN model with pre-trained word2Vec.\n\nAlthough many efforts had been made in building machine learning models, pre-processing has not been adequately addressed for the Vietnamese language yet, and little research was focused on this. Therefore, we decide to experiment on how different pre-processing techniques contribute to the emotion classifier's performance. In this paper, we will evaluate MLR models on UIT-VSMEC.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Dataset",
      "text": "In this paper, we conduct our experiments on UIT-VSMEC. This dataset contains 6,927 Facebook comments with each comment only assigned an emotion label. There are seven emotion labels in this dataset consisting of Ekman's six basic emotions labels  [10]  and \"Other\" label if a comment does not include any emotion or if a comment contains emotion that is different from Ekman's six basic emotions labels. We follow the training, development and test sets of the dataset for conducting our experiments.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Methodology",
      "text": "Our task belongs to textual emotion recognition, which aims to identify human emotion from the text. We separate our task into a pre-processing problem, which prepares the data for our training purpose, and a key-clauses extraction problem, which detects the main clause that is most likely to carry emotion in the given comment.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Pre-Processing",
      "text": "Since the author of UIT-VSMEC had not performed much cleaning on the data, we come up with several pre-processing techniques for Vietnamese social media data (Vietnamese microtext)  [11] . Table I describes seven pre-processing techniques based on Vietnamese social media characteristics.\n\nUIT-VSMEC corpus contains several words that are not in proper formats. For example, \"luônnn\" should be \"luôn\" Removing emojis, emoticons.\n\n3 Keeping emojis and emoticons then transforming them into their word form.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "4",
      "text": "Removing repeated emojis and emoticons in the same comment then transforming them into their word form.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "5",
      "text": "Translating word form of emojis, emoticons into Vietnamese.\n\n6 Spelling correction and acronyms, abbreviations lookup. 7\n\nRemoving stop words and particular words.\n\n(always) and \"thích quáaaaa\" should be \"thích quá\" (really enjoyable).\n\nWe partly solve this problem by using technique 1, in which we use regular expressions to replace a sequence of repeated characters by the character itself. For example, \"hahaaaa\" is converted to \"haha\" and \":]]]]\" is converted to \":]\". We cannot solve this problem entirely because of accents. For example, \"thích quáaaaa\" should be \"thích quá\" (really enjoyable), but our method only changes it to \"thích quáa\". Table  II  shows some examples of the word standardized process.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Table Ii Example Of Pre-Processing Technique 1.",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Words Standardized Words :)))) :) :]]]",
      "text": ":] \"hahaa\" \"haha\" \"hicc\" \"hic\" \"luônnn\" \"luôn\" (always) Some many emojis and emoticons appear in our corpus. An emoticon is a pictorial representation of a facial expression using characters -usually punctuation marks, numbers, and letters -to express a person's feelings or mood. Emojis are much like emoticons, but emojis are pictures rather than typographic approximations. Because emojis are considered as one-letter words and emoticons are considered as a combination of punctuation marks, they will usually be filtered out in the training process despite being the characteristics of social media data.\n\nTo handle emojis and emoticons, we come up with three approaches: remove, transform, and hybrid. The first approach is to use technique 2 to remove all the emojis and emoticons in our corpus. Although this technique helps reduce the vocabulary size, it has one big drawback. It considers emojis and emoticons do not express any feeling of the user. Still, most social media users tend to use emojis or emoticons to share their joy, sorrow, or anger.\n\nThe second approach is to transform emojis and emoticons into their word form. To convert emoticons into word form, we prepare an emoticon dictionary by changing all emoticons extracted from our corpus. For example, \":)\" is replaced by \":slightly smiling face:\" whereas \":(\" is replaced by \":frowning face:\". To transform emojis into word form, we use the demojize function in Python's emoji package. These are the core of technique 3. This approach considers all emojis and emoticons are important and should not be removed. Table  III  shows some examples of the word form of some emojis, emoticons. The third approach is to remove repeated emojis and emoticons in the same corpus, transforming them into their word form. Technique 4 considers duplicated emojis and emoticons in the same corpus are noise data, and only the unique ones should be kept.\n\nThe word forms of emojis and emoticons are in English, but the language of our corpus is Vietnamese. This behaviour makes emojis and emoticons have a different meaning with emotional words in the corpus. We use technique 5 to translate word forms of emojis, emoticons into Vietnamese. We create a translation rule dictionary for this purpose. The dictionary contains 109 translation rules for each word form of emojis and emoticons. For example, \"disappointed_face\" is translated to \"thất vọng\" (disappointed). We present some examples in Table  IV . There are a large number of misspelling words, slang words, and abbreviates in UIT-VSMEC. This behaviour is the nature of microtext, as explained in  [6] . For example, \"bjo\" should be \"bây giờ\" (now) and \"ju jin\" should be \"giữ gìn\" (conserve).\n\nWe correct these spelling errors by compiling an acronym and misspelling dictionary. The compiled dictionary has translations for 736 acronyms and misspelling words. Using the dictionary, we change acronyms and misspelt words to its proper form. Table  V  shows some examples of this technique.\n\nAfter correcting spelling mistakes, we then perform statistical analysis on UIT-VSMEC.  As for technique 7, we select a list of words to test. These words satisfy three conditions: appear more than 14 times in total; appear at least five times in each emotion label; are not noun/noun phrases, verbs, adjectives or adverbs. The test list consists of more than 300 words. We believe training a classifier with full vocabulary will keep a text's context. Then, filtering some stop words and particular words in the validation and test set will ignore the effect of these words. If we also remove these words from the train set, then our classifier will not understand a comment's context. We evaluate the impact of the removal of these words on the validation set using an emotion classifier built on the cleaned train set.\n\nThe first test case is to filter out each word from the test list separately and check for the difference in F1-score. If removing a word leads to a significant decrease in F1-score, then it will be removed from the test list. On the other hand, if the removal of a word gives a higher F1-score, then these words will be moved to a final filter list. The test list now should consist of words that do not impact the F1-score on removal. The second and latter test cases will first filter out words that in the final filter list, then filter each word in the test list and repeat the process. If a test case ends with no new word in the final filter list, then the next test case will filter one additional word from the test list, but if this happens three times on a row (calculation limitation), technique 7 will end.\n\nTechnique 7 aims to find out a list of words that should be removed; these words are a combination of some stop words and some particular words that should be removed in emotion recognition task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Emotion Classifier",
      "text": "In this paper, we use Count Vectorizer and TF-IDF Vectorizer to vectorize input data and then use Multinomial Logistic Regression (MLR)  [6]  to train 22 emotion classifiers for Vietnamese microtext. Besides, Logistic Regression also achieved the best score on the VLSP shared task -Hate Speech Detection on Vietnamese social media texts  [12] . These classifiers are trained by the data that applied different preprocessing techniques. We compare the performance of each classifier to indicate suitable pre-processing methods for our task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Key-Clause Extraction",
      "text": "One common point of a text or a speech is that they will have at least one concluding sentence, in which it helps to identify key information. Furthermore, if the concluding sentence contains a coordinating conjunction, such as \"but\", then this sentence can be separated into two or more clauses /phrases. For example, in the sentence \"I cannot cook very well, but I make quite good fried egg\", both \"I cannot cook very well\" and \"I make good fried egg\" are main/independent clauses (they are of equal importance and could each exist as a separate sentence).\n\nWe propose an idea that exploits this language's characteristic to filter out unimportant clauses in a comment. The idea is to rank (or give weight to) all sentences within a comment then rank the clauses within a sentence and remove clause of a specific rank. A sentence describing a feeling should have a higher rank than a sentence describing a place. The ideal situation is every sentence in a comment can be split into clauses, we then apply an emotion classifier on clause-level and then ensemble result based on clause's rank to get a final prediction. If a comment itself is a sentence and cannot be split into clauses, then this method cannot be applied.\n\nWe have not achieved the final goal of this idea; instead, we come up with a method for extracting key-clause in a sentence. First, we separate each comment into clauses/phrases by using punctuation marks (comma, dot and semi-colon) and coordinating conjunction. If a separated clause contains less than four words, then this clause will be concatenated with its preceding or succeeding clause.\n\nThen, we apply the previously built emotion classifier on each clause of the same comment. Using the emotion prediction on each clause, we create a list of important words; these words appear many times in a correctly classified clause. Example of these words is \"nhưng\" (but), \"tuy nhiên\" (however), \"đúng đắn nhất\" (most righteous). We use these important words to indicate if a clause is the main clause or not. Table  VII  shows three examples of phrases extraction.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Experiment Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Experiments With Pre-Processing Techniques",
      "text": "We conduct several experiments, presented in Table  VIII  and Table  IX , to compare the performance of the preprocessing techniques described in Section IV-A. We use the default setting for Count Vectorizer, TF-IDF Vectorizer, and Multinomial Logistic Regression classifier in these experiments. We also use data in the train set, and the validation set to compare each pre-processing technique's performance and to tune hyper-parameters. Finally, we use the test set to report final, unbiased classifier performance results in the paper. Our purpose is to indicate which vectorizer, pre-processing  As shown in Table  VIII , technique 2 -removing emojis performs worse than technique 3 -keeping emojis and emoticons and then transforming them into the word form (more than 1% different). The reason for this is because Facebook users tend to use emojis, emoticons in their comment, message, and even react to a post with emojis and reactions. The accuracy and F1-score of the model using pre-processing techniques 1, 3, 5, and 1, 4, 5 surpass the original model for each vectorizer type, proving technique 5 to be effective.\n\nAdding technique 6 leads to a slight decrease in accuracy and F1-score in the classifier model that uses Count Vectorizer and pre-processing techniques 1, 3, 5, and classifier models that use TF-IDF Vectorizer and pre-processing technique 1, 4, 5. In contrast, adding technique 6 slightly increases the accuracy and F1-score of the Count Vectorizer model using technique 1, 4, 5, and in the TF-IDF Vectorizer model using technique 1, 3, 5. Although the model that uses Count Vectorizer and pre-processing techniques 1, 4, 5, 6 achieves the best F1-score, the model that uses TF-IDF Vectorizer and pre-processing techniques 1, 3, 5, 6 achieve the best accuracy.\n\nTherefore, we perform hyper-parameter tuning to find out which combination of vectorizer, pre-processing techniques and parameter setting that lead to the best performing model.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Experiments With Hyper-Parameter Fine-Tuning",
      "text": "As a result of the previous experiment, we set the parameter C = 4.5 and class_weight = \"balanced\" for the MLR classifier. In this experiment, we perform hyper-parameter tuning with the n_features (ranging from 3,000 to 50,000) and ngram_range (ranging from 1-1, 1-2 and 1-3 gram) parameter for the Count Vectorizer and TF-IDF Vectorizer. There are several reasons for these ranging. Firstly, UIT-VSMEC has a small vocabulary (4,097 words after pre-processing). Secondly, we do not perform word segmentation on the dataset. Lastly, Vietnamese words comprise of single word and compound word. Sometimes a single word does not have any meaning unless they go with another single word to form a compound word (e.g. \"lác\" and \"đác\" form the word \"lác đác\" -\"little\"). In contrast, some compound words are formed by words that have the meaning of their own, for example, \"tiền công\" (wage) is formed by \"tiền\" (money) and \"công\" (effort). We even use ngram_range from 1-3 gram because the use of compound words, for example, \"phim\" (movie) and \"Ấn Độ\" (India) form the noun phrase \"phim Ấn Độ\" (Indian movie). The best results are presented in table IX along with its parameter setting. Table  IX  shows that keeping repeated emojis performs slightly better than only keep the unique ones. The best accuracy and F1-score is the emotion classifier that uses TF-IDF Vectorizer. Therefore, we will use the TF-IDF Vectorizer with pre-processing technique 1, 3, 5, 6, and the parameters for the TF-IDF vectorizer are n features = 25,000, ngram range = (1,3), and the parameters for the classifier are C = 4.5 and class weight = \"balanced\" to build our classifier model. Thus, we will use this same set-up to train our baseline model on raw data.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Experiments With Pre-Processing Technique 7 And Keyclause Extraction Method",
      "text": "We use the classifier model to evaluate pre-processing technique 7, which removes stop words and particular words, and the performance of our key-clause extraction method. The results are shown in Table  X . Our first observation is that the model that uses preprocessing techniques 1, 3, 5, 6 outperforms the baseline by a large margin. This observation shows the power and effectiveness of data cleaning when working with Vietnamese microtext. There are two reasons for this. First, the preprocessing techniques reduce the vocabulary size by normalizing text data; it means a portion of noise data filter out. Second, we also take advantage of the emojis and emoticons by transforming them into their word form. We consider these are social media data's characteristics.\n\nThe next observation is the impact of the removal of stop words and particular words. Removing stop words yields a higher F1-score (1.12%). This result is reasonable because these words are considered noise data if we want to detect a comment's emotion.\n\nFinally, the implementation of the key-clause extraction method affects the performance of the emotion classifier. The explanation for this is that the clauses extracted from a comment by this method are most likely to carry the emotion. Another explanation is because our proposed method is not completed; it only affect 23 out of 693 comments in the test set.\n\nOur best model using pre-processing techniques 1, 3, 5, 6, 7, and key-clause extraction methods achieve 64.40% in the F1 score, which improves 8.83% compared with the baseline model. Our emotion classifier model outperform the CNN model built by  [5]  with an improvement of 4.66% in F1score. The reason behind this is because of the implementation of pre-processing methods on UIT-VSMEC. The results also confirm the effectiveness of using the key-clause extraction method.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Error Analysis",
      "text": "Table  XI  shows some examples that our model fails to detect the correct emotion label and our explanations. Typical cases include: 1) our model does not understand the context; 2) our model does not understand words that have never appeared before; 3) our model does not filter emoticons and emojis unrelated to the comment; 4) our model recognizes a wrong clause as key-clause. The \":((\"emoticon have a stronger influence than the word \"kinh khủng\" (terrible).\n\ntao đéo ngờ được đây là trường tao :)))) (I did not expect this was my school :))))))",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Surprise Enjoyment",
      "text": "The term \"đéo ngờ được\"(did not expect) has not appeared in the train data so it cannot be recognized to express surpriseness.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "In this study, based on Vietnamese social media characteristics, we explore the effect of our pre-processing techniques and our key-clause extraction technique combined with the machine learning model toward the textual emotion recognition task in Vietnamese social media text. We also reach the best overall weighted F1-score of 64.40% on the pre-processed UIT-VSMEC corpus with Multinomial Logistic Regression using the TF-IDF Vectorizer and key-clause extraction method.\n\nThere are some limitations to our works that we will discuss in the following. First, UIT-VSMEC is not generalized enough. The dataset needs to have more cases about anger, fear, and surprise labels, and the \"Other\" label needs to be more specific if we want to build a better system for real-life tasks. Second, due to language knowledge limitations, we cannot pre-process the data well enough. Finally, our work focuses only on recognizing six basic emotion  [10]  labels with the \"other\" label that is not included.\n\nAs future work, we plan to improve the key-clause extraction method's performance with semantic and syntax techniques. Besides, we also aim to build a larger, manually annotated dataset with fine-grained emotions following the study  [13] . We will apply transfer models such as BERT  [14]  and its variations to improve performance of Vietnamese emotion recognition.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "system’s performance built on Vietnamese social media data"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "(Vietnamese microtext\n[6]). With a given Vietnamese micro-"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "text, our goal\nis\nto clean this microtext and then predict\nthe"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "emotion it contains."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "Our key contribution is\nto compile\nan appropriate\nset of"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "pre-processing techniques\nfor Vietnamese social media data."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "We build several classiﬁers for a different combination of pre-"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "processing techniques by using Multinomial Logistic Regres-"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "sion (MLR)\n[7] on the benchmark dataset UIT-VSMEC [5]."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "Finally, we compare the F1-score of each classiﬁer to ﬁnd out"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "the best combination of pre-processing techniques."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "The\nstructure of\nthis paper\nis\nas\nfollows. Related studies"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "are presented in Section II. Section III describes\nthe dataset"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "used in our experiments. Section IV discusses our proposed"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "methods in which we explain the reasons behind each method."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "In Section V, we present\nthe experiments and results for our"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "proposed method. Finally, we sum up our ﬁndings and discuss"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "future work in Section VI."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "II. RELATED WORK"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": ""
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "In the past decade,\nthere were many studies on the design"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "of methods\nand the development of\nframeworks\nfor\ntextual"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "emotion detection. A detailed and complete overview of ap-"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "proaches can be found in [8, 9]."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "Brilis et al. [1] used machine learning approaches to create"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "an automatic mood categories classiﬁer for song lyrics by per-"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "forming pre-processing steps\nsuch as\nstemming,\nstop words,"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "and\npunctuation marks\nremoval\ncombined with\na\nbag-of-"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "words approach using TF-IDF score (term frequency-inverse"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "document\nfrequency). The authors indicated that\nthe Random"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "Forest algorithm reported the best results, with approximately"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "71.50% accuracy on the stemmed dataset and 93.70% on the"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "unstemmed."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "Ho and Cao [2] proposed a hidden Markov logic approach"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "to predict\nthe emotion expressed by a given text. The authors’"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "method achieved an F-score of 35.00% on the ISEAR (Interna-"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "tional Survey on Emotion Antecedents and Reaction) dataset."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "The\nauthors\nignored semantic\nand syntactic\nfeatures of\nthe"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "sentence\nanalysis, which made\nit non-context\nsensitive\nand"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "can be the cause for\nthe low F-score."
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "Ghazi,\nInkpen,\nand\nSzpakowicz\n[3]\ncombined\nkeyword-"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "based,\nlexicon-based, and machine learning-based approaches"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "to compare the prior emotion of a word with its contextual"
        },
        {
          "1,2Vietnam National University, Ho Chi Minh City, Vietnam": "emotion and their\neffect on the\nsentiment\nexpressed by the"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "6"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "lookup."
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "improve classiﬁcation accuracy.",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "7\nRemoving stop words and particular words."
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "In work presented in [5],\nthe authors built\nthe ﬁrst standard",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "Vietnamese\nSocial Media Emotion Corpus\n(UIT-VSMEC).",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "This\ncorpus\ncontains\nexactly\n6,927\nemotion-annotated\nsen-",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "(always)\nand “th´ıch qu´aaaaa”\nshould be\n“th´ıch qu´a”\n(really"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "tences\n(an\naverage\nagreement\nof\n82.00% across\nthe\nseven",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "enjoyable)."
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "emotions) and then measure two machine learning models and",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "We partly solve this problem by using technique 1, in which"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "two deep learning models on this dataset. The authors reported",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "we use regular expressions to replace a sequence of\nrepeated"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "the highest weighted F1-score of 59.74% by using the CNN",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "characters by the character\nitself. For example, “hahaaaa” is"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "model with pre-trained word2Vec.",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "converted to “haha” and “:]]]]” is converted to “:]”. We cannot"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "Although many efforts had been made in building machine",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "solve this problem entirely because of accents. For example,"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "learning models, pre-processing has not been adequately ad-",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "“th´ıch qu´aaaaa” should be “th´ıch qu´a” (really enjoyable), but"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "dressed for\nthe Vietnamese language yet, and little research",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "our method only changes\nit\nto “th´ıch qu´aa”. Table II\nshows"
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "was\nfocused on this. Therefore, we decide to experiment on",
          "Spelling correction and acronyms, abbreviations": ""
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "",
          "Spelling correction and acronyms, abbreviations": "some examples of\nthe word standardized process."
        },
        {
          "tweet datasets and reported that\nthe classiﬁer ensemble could": "how different pre-processing techniques contribute to the emo-",
          "Spelling correction and acronyms, abbreviations": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "PRE-PROCESSING TECHNIQUES."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "their features signiﬁcantly outperform their baselines (the null",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "hypothesis\nthat was\nusing\nthe\nleast\nand\nthe most\nfrequent",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "Technique\nDescription"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "emotion label\nto predict) and their Support Vector Machines",
          "Table I": "1\nStandardizing words."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "2\nRemoving emojis, emoticons."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "(SVM) model.",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "Keeping emojis and emoticons then transforming"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "Da Silva, Hruschka, and Hruschka Jr\n[4] conducted tweet",
          "Table I": "3"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "them into their word form."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "sentiment analysis using classiﬁer ensembles. A classiﬁer en-",
          "Table I": "Removing repeated emojis and emoticons in the"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "semble was formed using several machine learning classiﬁers:",
          "Table I": "same comment\nthen transforming them into their\n4"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "word form."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "Random Forest, SVM, Multinomial Naive Bayes, and Logistic",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "Translating word form of emojis, emoticons into"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "5"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "Regression.\nIn their study, authors experimented with various",
          "Table I": "Vietnamese."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "tweet datasets and reported that\nthe classiﬁer ensemble could",
          "Table I": "Spelling correction and acronyms, abbreviations"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "6"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "lookup."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "improve classiﬁcation accuracy.",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "7\nRemoving stop words and particular words."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "In work presented in [5],\nthe authors built\nthe ﬁrst standard",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "Vietnamese\nSocial Media Emotion Corpus\n(UIT-VSMEC).",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "This\ncorpus\ncontains\nexactly\n6,927\nemotion-annotated\nsen-",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "(always)\nand “th´ıch qu´aaaaa”\nshould be\n“th´ıch qu´a”\n(really"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "tences\n(an\naverage\nagreement\nof\n82.00% across\nthe\nseven",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "enjoyable)."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "emotions) and then measure two machine learning models and",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "We partly solve this problem by using technique 1, in which"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "two deep learning models on this dataset. The authors reported",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "we use regular expressions to replace a sequence of\nrepeated"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "the highest weighted F1-score of 59.74% by using the CNN",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "characters by the character\nitself. For example, “hahaaaa” is"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "model with pre-trained word2Vec.",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "converted to “haha” and “:]]]]” is converted to “:]”. We cannot"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "Although many efforts had been made in building machine",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "solve this problem entirely because of accents. For example,"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "learning models, pre-processing has not been adequately ad-",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "“th´ıch qu´aaaaa” should be “th´ıch qu´a” (really enjoyable), but"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "dressed for\nthe Vietnamese language yet, and little research",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "our method only changes\nit\nto “th´ıch qu´aa”. Table II\nshows"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "was\nfocused on this. Therefore, we decide to experiment on",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "some examples of\nthe word standardized process."
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "how different pre-processing techniques contribute to the emo-",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "tion classiﬁer’s performance.\nIn this paper, we will evaluate",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "Table II"
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "MLR models on UIT-VSMEC.",
          "Table I": ""
        },
        {
          "sentence. Authors showed that using Logistic Regression and": "",
          "Table I": "EXAMPLE OF PRE-PROCESSING TECHNIQUE 1."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "some examples of\nthe word standardized process.": ""
        },
        {
          "some examples of\nthe word standardized process.": ""
        },
        {
          "some examples of\nthe word standardized process.": "Table II"
        },
        {
          "some examples of\nthe word standardized process.": ""
        },
        {
          "some examples of\nthe word standardized process.": "EXAMPLE OF PRE-PROCESSING TECHNIQUE 1."
        },
        {
          "some examples of\nthe word standardized process.": "Words\nStandardized words"
        },
        {
          "some examples of\nthe word standardized process.": ":))))\n:)"
        },
        {
          "some examples of\nthe word standardized process.": ":]]]\n:]"
        },
        {
          "some examples of\nthe word standardized process.": "“hahaa”\n“haha”"
        },
        {
          "some examples of\nthe word standardized process.": "“hicc”\n”hic”"
        },
        {
          "some examples of\nthe word standardized process.": ""
        },
        {
          "some examples of\nthe word standardized process.": "”luˆonnn”\n”luˆon” (always)"
        },
        {
          "some examples of\nthe word standardized process.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": "EXAMPLE ACRONYMS, MISSPELLING WORDS AND THEIR CORRECT FORM"
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "ing face:”. To transform emojis",
          "Table V": ""
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": "IN THE ACRONYM AND SPELLING CORRECTION DICTIONARY."
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "demojize function in Python’s emoji package. These are the",
          "Table V": ""
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "core of",
          "Table V": "Vietnamese correct"
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": ""
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": "form"
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "emoticons",
          "Table V": ""
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": "có"
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "III\nshows",
          "Table V": ""
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": "người ta"
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "emoticons.",
          "Table V": "cà phê"
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": "cũng"
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": "biết"
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": ""
        },
        {
          "”:slightly smiling face:” whereas ”:(” is replaced by ”:frown-": "",
          "Table V": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table VI": ""
        },
        {
          "Table VI": "Anger"
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": "95"
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": "132"
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": "16"
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": "2"
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": ""
        },
        {
          "Table VI": "7"
        },
        {
          "Table VI": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "(age)"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "As for\ntechnique 7, we select a list of words to test. These"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "words satisfy three conditions: appear more than 14 times in"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "total; appear at\nleast five times in each emotion label; are not"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "noun/noun phrases, verbs, adjectives or adverbs. The test\nlist"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "consists of more than 300 words. We believe training a classifier"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "with full vocabulary will keep a text’s context. Then, filtering"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "some stop words and particular words in the validation and test"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "set will ignore the effect of these words. If we also remove these"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "words from the train set, then our classifier will not understand"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "a comment’s context. We evaluate the impact of the removal of"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "these words on the validation set using an emotion classifier"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "built on the cleaned train set."
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "The first\ntest case is to filter out each word from the test\nlist"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "separately and check for the difference in F1-score. If removing"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "a word leads to a significant decrease in F1-score, then it will be"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "removed from the test list. On the other hand, if the removal of"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "a word gives a higher F1-score, then these words will be moved"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "to a final filter list. The test list now should consist of words that"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "do not\nimpact\nthe F1-score on removal. The second and latter"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "test cases will\nfirst\nfilter out words that\nin the final\nfilter\nlist,"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "then filter each word in the test\nlist and repeat\nthe process. If a"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "test case ends with no new word in the final filter list,\nthen the"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "next\ntest case will filter one additional word from the test\nlist,"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "but if this happens three times on a row (calculation limitation),"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "technique 7 will end."
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "Technique 7 aims to find out a list of words that should be"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "removed;\nthese words are a combination of some stop words"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "and some particular words that should be removed in emotion"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "recognition task."
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "B. Emotion Classifier"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": ""
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "In\nthis\npaper,\nwe\nuse\nCount\nVectorizer\nand\nTF-IDF"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "Vectorizer\nto vectorize input data and then use Multinomial"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "Logistic Regression (MLR) [6] to train 22 emotion classifiers"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "for Vietnamese microtext. Besides, Logistic Regression also"
        },
        {
          "18\n14\n30\n7\n1\n19\n7\n96": "achieved\nthe\nbest\nscore\non\nthe VLSP\nshared\ntask\n- Hate"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "PHRASES EXTRACTION FOR EACH COMMENT."
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "classifiers are trained by the data that applied different pre-",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "processing techniques. We compare the performance of each",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "classifier\nto indicate suitable pre-processing methods\nfor our",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "task.",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "C. Key-clause Extraction",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "One\ncommon\npoint\nof\na\ntext\nor\na\nspeech\nis\nthat\nthey",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "will have at\nleast one concluding sentence,\nin which it helps",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "to identify key information. Furthermore,\nif\nthe\nconcluding",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "sentence contains a coordinating conjunction,\nsuch as “but”,",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "then this sentence can be separated into two or more clauses",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "/phrases. For\nexample,\nin the\nsentence\n“I\ncannot\ncook very",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "well, but\nI make quite good fried egg”, both “I cannot cook",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "very well” and “I make good fried egg” are main/independent",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "clauses (they are of equal importance and could each exist as a",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "separate sentence).",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "We\npropose\nan\nidea\nthat\nexploits\nthis\nlanguage’s",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "characteristic to filter out unimportant clauses in a comment.",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "The idea is to rank (or give weight\nto) all sentences within a",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "techniques, and parameter setting best ﬁt our training purpose."
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "comment\nthen rank the clauses within a sentence and remove",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "Table VIII\nshows\nthe\nimpact\nof\npre-processing\non model"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "clause\nof\na\nspecific\nrank. A sentence\ndescribing\na\nfeeling",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "performance."
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "should have a higher\nrank than a sentence describing a place.",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "The ideal situation is every sentence in a comment can be split",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "Table VIII"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "into clauses, we then apply an emotion classifier on clause-level",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "IMPACT OF PRE-PROCESSING TECHNIQUES ON THE VALIDATION SET OF"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "and then ensemble result based on clause’s rank to get a final",
          "Table VII": "UIT-VSMEC."
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "prediction. If a comment itself is a sentence and cannot be split",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "Vectorizer\nPre-processing\nAccuracy\nF1-score"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "into clauses, then this method cannot be applied.",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "Original\n56.41%\n56.07%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "We have not achieved the final goal of\nthis\nidea;\ninstead,",
          "Table VII": "1\n54.81%\n54.85%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 2\n54.66%\n54.16%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "we\ncome\nup with\na method\nfor\nextracting\nkey-clause\nin\na",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 3\n55.98%\n55.55%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "sentence. First, we separate each comment into clauses/phrases",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "Count Vectorizer\n1+ 3 + 5\n58.75%\n58.17%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "by using punctuation marks (comma, dot and semi-colon) and",
          "Table VII": "1 + 3 + 5 +6\n58.60%\n58.07%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 4\n57.00%\n56.58%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "coordinating conjunction.\nIf a separated clause contains\nless",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 4 +5\n58.45%\n57.87%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "than four words,\nthen this clause will be concatenated with its",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 4 + 5 + 6\n58.75%\n58.25%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "preceding or succeeding clause.",
          "Table VII": "Original\n55.69%\n53.39%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1\n55.83%\n53.60%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "Then, we apply the previously built emotion classifier on",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 2\n55.39%\n53.15%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "each clause of the same comment. Using the emotion prediction",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 3\n57.29%\n55.16%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "on each clause, we create a list of important words; these words",
          "Table VII": "TF-IDF Vectorizer\n1 + 3 + 5\n59.04%\n57.29%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 3 + 5 + 6\n59.33%\n57.99%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "appear many times in a correctly classified clause. Example of",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 4\n57.00%\n54.84%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "these words\nis “nhưng” (but), “tuy nhiên” (however), “đúng",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1 + 4 + 5\n58.89%\n57.34%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "đắn nhất” (most\nrighteous). We use these important words to",
          "Table VII": "1 + 4 + 5 + 6\n58.60%\n56.94%"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "indicate if a clause is the main clause or not. Table VII shows",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "three examples of phrases extraction.",
          "Table VII": "As shown in Table VIII, technique 2 - removing emojis per-"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "forms worse than technique 3 - keeping emojis and emoticons"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "V. EXPERIMENT RESULTS",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "and then transforming them into the word form (more than"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "A. Experiments with pre-processing techniques",
          "Table VII": ""
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "",
          "Table VII": "1% different). The reason for\nthis is because Facebook users"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "We\nconduct\nseveral\nexperiments, presented in Table VIII",
          "Table VII": "tend to use emojis, emoticons in their comment, message, and"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "and\nTable\nIX,\nto\ncompare\nthe\nperformance\nof\nthe\npre-",
          "Table VII": "even react\nto a post with emojis and reactions. The accuracy"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "processing techniques described in Section IV-A. We use the",
          "Table VII": "and F1-score of the model using pre-processing techniques 1,"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "default\nsetting for Count Vectorizer, TF-IDF Vectorizer, and",
          "Table VII": "3, 5, and 1, 4, 5 surpass the original model for each vectorizer"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "Multinomial Logistic Regression\nclassiﬁer\nin\nthese\nexperi-",
          "Table VII": "type, proving technique 5 to be effective."
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "ments. We also use data in the train set, and the validation set",
          "Table VII": "Adding technique 6 leads\nto a slight decrease in accuracy"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "to compare each pre-processing technique’s performance and",
          "Table VII": "and F1-score in the classiﬁer model that uses Count Vectorizer"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "to tune hyper-parameters. Finally, we use the test set\nto report",
          "Table VII": "and pre-processing techniques 1, 3, 5, and classiﬁer models"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "ﬁnal,\nunbiased\nclassiﬁer\nperformance\nresults\nin\nthe\npaper.",
          "Table VII": "that use TF-IDF Vectorizer\nand pre-processing technique 1,"
        },
        {
          "Speech Detection on Vietnamese social media texts [12]. These": "Our purpose\nis\nto indicate which vectorizer, pre-processing",
          "Table VII": "4, 5.\nIn contrast,\nadding technique 6 slightly increases\nthe"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "C = 4.5 and class_weight = “balanced” for the MLR classifier."
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "In this experiment, we perform hyper-parameter tuning with the"
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "n_features"
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "(ranging"
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "Count Vectorizer"
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "reasons"
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "vocabulary (4,097 words after pre-processing). Secondly, we"
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "do\nnot"
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": ""
        },
        {
          "B. Experiments with hyper-parameter ﬁne-tuning": "Vietnamese words"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "technique 1, 4, 5,\nand in the TF-IDF Vectorizer model us-",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "we will use this\nsame set-up to train our baseline model on"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "ing technique 1, 3, 5. Although the model\nthat uses Count",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "raw data."
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "Vectorizer and pre-processing techniques 1, 4, 5, 6 achieves",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "C. Experiments with\npre-processing\ntechnique\n7\nand\nkey-"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "the best F1-score,\nthe model\nthat uses TF-IDF Vectorizer and",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "clause extraction method"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "pre-processing techniques 1, 3, 5, 6 achieve the best accuracy.",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "Therefore, we perform hyper-parameter\ntuning to ﬁnd out",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "We\nuse\nthe\nclassiﬁer model\nto\nevaluate\npre-processing"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "which\ncombination\nof\nvectorizer,\npre-processing\ntechniques",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "technique 7, which removes stop words and particular words,"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "and parameter setting that\nlead to the best performing model.",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "and the performance of our key-clause extraction method. The"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "results are shown in Table X."
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "B. Experiments with hyper-parameter ﬁne-tuning",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "As a result of the previous experiment, we set the parameter",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "Table X"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "EXPERIMENTAL RESULTS OF ALL MODEL ON THE TEST SET OF"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "C = 4.5 and class_weight = “balanced” for the MLR classifier.",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "UIT-VSMEC."
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "In this experiment, we perform hyper-parameter tuning with the",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "n_features\n(ranging from 3,000 to 50,000) and ngram_range",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "Model\nAccuracy\nF1-score"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "MLR (baseline)\n55.41%\n55.57%"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "(ranging\nfrom 1-1,\n1-2\nand\n1-3\ngram)\nparameter\nfor\nthe",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "CNN + word2Vec [5]\n59.74%\n59.74%"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "Count Vectorizer\nand TF-IDF Vectorizer. There\nare\nseveral",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "MLR + pre-processing techniques (1, 3, 5, 6)\n62.91%\n62.95%"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "reasons\nfor\nthese\nranging. Firstly, UIT-VSMEC has\na\nsmall",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "MLR + pre-processing techniques (1, 3, 5, 6)"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "64.07%\n64.07%"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "+ remove stop words (7)"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "vocabulary (4,097 words after pre-processing). Secondly, we",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "MLR+ pre-processing techniques (1, 3, 5, 6, 7)"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "do\nnot\nperform word\nsegmentation\non\nthe\ndataset. Lastly,",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "64.36%\n64.40%"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "+ key-clause extraction"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "Vietnamese words\ncomprise of\nsingle word\nand compound",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "word. Sometimes a single word does not have any meaning",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "Our\nﬁrst\nobservation\nis\nthat\nthe model\nthat\nuses\npre-"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "unless they go with another single word to form a compound",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "processing\ntechniques\n1,\n3,\n5,\n6\noutperforms\nthe\nbaseline"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "word (e.g. “lác” and “đác” form the word “lác đác” - “little”).",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "by\na\nlarge margin. This\nobservation\nshows\nthe\npower\nand"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "In contrast, some compound words are formed by words that",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "effectiveness of data cleaning when working with Vietnamese"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "have the meaning of their own, for example, “tiền công” (wage)",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "microtext. There\nare\ntwo\nreasons\nfor\nthis.\nFirst,\nthe\npre-"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "is formed by “tiền” (money) and “công” (effort). We even use",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "processing techniques reduce the vocabulary size by normal-"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "ngram_range\nfrom 1-3 gram because\nthe use of\ncompound",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "izing text data;\nit means\na portion of noise data ﬁlter out."
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "words, for example, “phim” (movie) and “Ấn Độ” (India) form",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "Second, we also take advantage of\nthe emojis and emoticons"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "the noun phrase “phim Ấn Độ” (Indian movie). The best results",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "by transforming them into their word form. We consider these"
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "are presented in table IX along with its parameter setting.",
          "class weight = “balanced” to build our classiﬁer model. Thus,": ""
        },
        {
          "accuracy and F1-score of\nthe Count Vectorizer model using": "",
          "class weight = “balanced” to build our classiﬁer model. Thus,": "are social media data’s characteristics."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "by transforming them into their word form. We consider these"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "are social media data’s characteristics."
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "The next observation is\nthe impact of\nthe removal of\nstop"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "words\nand particular words. Removing stop words yields\na"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "higher F1-score\n(1.12%). This\nresult\nis\nreasonable\nbecause"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "these words\nare\nconsidered noise data\nif we want\nto detect"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "a comment’s emotion."
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "Finally,\nthe\nimplementation\nof\nthe\nkey-clause\nextraction"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "method\naffects\nthe\nperformance\nof\nthe\nemotion\nclassiﬁer."
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "The explanation for\nthis\nis\nthat\nthe clauses extracted from a"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "comment by this method are most\nlikely to carry the emotion."
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "Another explanation is because our proposed method is not"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "completed;\nit only affect 23 out of 693 comments in the test"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "set."
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "Our best model using pre-processing techniques 1, 3, 5, 6,"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "7, and key-clause extraction methods achieve 64.40% in the"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "F1 score, which improves 8.83% compared with the baseline"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "model. Our\nemotion\nclassiﬁer model\noutperform the CNN"
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": ""
        },
        {
          "Second, we also take advantage of\nthe emojis and emoticons": "model\nbuilt\nby\n[5] with\nan\nimprovement\nof\n4.66% in F1-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "include: 1) our model does not understand the context; 2) our": "model does not understand words\nthat have never\nappeared",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Spyros Brilis et al. “Mood classiﬁcation using lyrics and"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "before; 3) our model does not ﬁlter\nemoticons\nand emojis",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "IFIP Interna-\naudio: A case-study in greek music”.\nIn:"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "unrelated to the comment; 4) our model\nrecognizes a wrong",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "tional Conference on Artiﬁcial Intelligence Applications"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "clause as key-clause.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "and Innovations. Springer. 2012, pp. 421–430."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Dung T Ho\nand Tru H Cao.\n“A high-order\nhidden"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "Table XI",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Markov model\nfor\nemotion\ndetection\nfrom textual"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "SOME ERROR CASES.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "data”. In: Paciﬁc Rim Knowledge Acquisition Workshop."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "Comments\nEmotion\nPredictions\nExplanations",
          "REFERENCES": "Springer. 2012, pp. 94–105."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "người ta có bạn",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "Our model cannot",
          "REFERENCES": "Diman Ghazi, Diana\nInkpen,\nand\nStan\nSzpakowicz."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "bè nhìn vui thật",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "understand the",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "“Prior\nand\ncontextual\nemotion\nof words\nin\nsenten-"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "(they look so\nSadness\nEnjoyment",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "context of this",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "happily having",
          "REFERENCES": "tial\ncontext”.\nIn: Computer Speech & Language 28.1"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "comment.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "friends)",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "(2014), pp. 76–92."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "The \":((\"emoticon",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "nghe ngọt thế :((",
          "REFERENCES": "Nadia FF Da Silva, Eduardo R Hruschka, and Estevam"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "have the strongest",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "(sounds so sweet\nEnjoyment\nSadness",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "influence in this",
          "REFERENCES": "R Hruschka Jr. “Tweet sentiment analysis with classiﬁer"
        },
        {
          "include: 1) our model does not understand the context; 2) our": ":(( )",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "comment.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "ensembles”.\nIn: Decision Support Systems 66 (2014),"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "The \":((\"emoticon",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "pp. 170–179."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "have a stronger",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "kinh khủng thật :((",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "influence than the\nFear\nSadness",
          "REFERENCES": "Vong Anh Ho\net\nal.\n“Emotion\nrecognition\nfor\nviet-"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "(it’s terrilble:(( )",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "word \"kinh khủng\"",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Lin-\nnamese\nsocial media\ntext”.\nIn: Computational"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "(terrible).",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "guistics\n- 16th International Conference of\nthe Paciﬁc"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "The term \"đéo ngờ",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "tao đéo ngờ được",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "được\"(did not expect)",
          "REFERENCES": "Association for Computational Linguistics (2019)."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "đây là trường tao",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "has not appeared in",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Jeffrey Ellen.\n“All\nabout microtext”.\nIn: A Working"
        },
        {
          "include: 1) our model does not understand the context; 2) our": ":))))",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "the train data so it\nSurprise\nEnjoyment",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "(I did not expect",
          "REFERENCES": "Deﬁnition and a Survey of Current Microtext Research"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "cannot be recognized",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "this was my school",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "to express",
          "REFERENCES": "within\nArtiﬁcial\nIntelligence\nand Natural\nLanguage"
        },
        {
          "include: 1) our model does not understand the context; 2) our": ":))))))",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "surpriseness.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Processing, USA: Sciterpress (Science and Technology"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Publications, Lda)\n(2011), pp. 329–336."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Dankmar B¨ohning.\n“Multinomial\nlogistic\nregression"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "VI. CONCLUSION AND FUTURE WORK",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "of\nthe\ninstitute\nof\nStatistical\nalgorithm”.\nIn: Annals"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "In this study, based on Vietnamese social media character-",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Mathematics 44.1 (1992), pp. 197–200."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "istics, we explore the effect of our pre-processing techniques",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Kashﬁa Sailunaz\net\nal.\n“Emotion detection from text"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "and our key-clause\nextraction technique\ncombined with the",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "and speech: a survey”. In: Social Network Analysis and"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "machine learning model\ntoward the textual emotion recogni-",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Mining 8.1 (2018), p. 28."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "tion task in Vietnamese social media text. We also reach the",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Rafael A Calvo and Sidney D’Mello. “Affect detection:"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "best overall weighted F1-score of 64.40% on the pre-processed",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "An\ninterdisciplinary\nreview of models, methods,\nand"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "UIT-VSMEC corpus with Multinomial Logistic Regression",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "IEEE Transactions on affective\ntheir applications”.\nIn:"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "using the TF-IDF Vectorizer and key-clause extraction method.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "computing 1.1 (2010), pp. 18–37."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "There are some limitations to our works that we will discuss",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Paul Ekman. “Basic emotions”.\nIn: Handbook of cog-"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "in the following. First, UIT-VSMEC is not generalized enough.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "nition and emotion 98.45-60 (1999), p. 16."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "The dataset needs\nto have more cases about anger,\nfear, and",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Ranjan Satapathy et al. “Phonetic-based microtext nor-"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "surprise labels, and the “Other” label needs to be more speciﬁc",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "2017\nmalization\nfor\ntwitter\nsentiment\nanalysis”.\nIn:"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "if we want\nto build a better system for real-life tasks. Second,",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "IEEE International Conference on Data Mining Work-"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "due to language knowledge limitations, we cannot pre-process",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "shops (ICDMW).\nIEEE. 2017, pp. 407–413."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "the\ndata well\nenough.\nFinally,\nour work\nfocuses\nonly\non",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Xuan-Son Vu et al. “HSD shared task in VLSP cam-"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "recognizing six basic emotion [10] labels with the “other” label",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "paign 2019: Hate speech detection for social good”. In:"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "that\nis not\nincluded.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "arXiv preprint arXiv:2007.06493 (2020)."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "As future work, we plan to improve the key-clause extrac-",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Dorottya Demszky et\nal.\n“GoEmotions: A Dataset of"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "tion method’s\nperformance with\nsemantic\nand\nsyntax\ntech-",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "the 58th\nFine-Grained Emotions”.\nIn: Proceedings of"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "niques. Besides, we\nalso\naim to\nbuild\na\nlarger, manually",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Annual Meeting of\nthe Association for Computational"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "annotated\ndataset with ﬁne-grained\nemotions\nfollowing\nthe",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Linguistics. Online: Association for Computational Lin-"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "study\n[13]. We will\napply\ntransfer models\nsuch\nas BERT",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "guistics, July 2020, pp. 4040–4054. DOI: 10.18653/v1/"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "[14] and its variations to improve performance of Vietnamese",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "2020 . acl - main . 372. URL: https : / / www. aclweb. org /"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "emotion recognition.",
          "REFERENCES": ""
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "anthology/2020.acl-main.372."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "Jacob Devlin et al. “BERT: Pre-training of Deep Bidi-"
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "rectional Transformers\nfor Language Understanding”."
        },
        {
          "include: 1) our model does not understand the context; 2) our": "",
          "REFERENCES": "In: NAACL-HLT (1). 2019."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mood classification using lyrics and audio: A case-study in greek music",
      "authors": [
        "Spyros Brilis"
      ],
      "year": "2012",
      "venue": "IFIP International Conference on Artificial Intelligence Applications and Innovations"
    },
    {
      "citation_id": "2",
      "title": "A high-order hidden Markov model for emotion detection from textual data",
      "authors": [
        "T Dung",
        "Tru Ho",
        "Cao"
      ],
      "year": "2012",
      "venue": "A high-order hidden Markov model for emotion detection from textual data"
    },
    {
      "citation_id": "3",
      "title": "Prior and contextual emotion of words in sentential context",
      "authors": [
        "Diman Ghazi",
        "Diana Inkpen",
        "Stan Szpakowicz"
      ],
      "year": "2014",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "4",
      "title": "Tweet sentiment analysis with classifier ensembles",
      "authors": [
        "Nadia Ff Da Silva",
        "Eduardo Hruschka",
        "Estevam Hruschka"
      ],
      "year": "2014",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition for vietnamese social media text",
      "authors": [
        "Anh Vong",
        "Ho"
      ],
      "year": "2019",
      "venue": "Computational Linguistics -16th International Conference of the"
    },
    {
      "citation_id": "6",
      "title": "All about microtext",
      "authors": [
        "Jeffrey Ellen"
      ],
      "year": "2011",
      "venue": "A Working Definition and a Survey of Current Microtext Research within Artificial Intelligence and Natural Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Multinomial logistic regression algorithm",
      "authors": [
        "Dankmar Böhning"
      ],
      "year": "1992",
      "venue": "Annals of the institute of Statistical Mathematics"
    },
    {
      "citation_id": "8",
      "title": "Emotion detection from text and speech: a survey",
      "authors": [
        "Kashfia Sailunaz"
      ],
      "year": "2018",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "9",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "A Rafael",
        "Sidney D' Calvo",
        "Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "10",
      "title": "Basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "11",
      "title": "Phonetic-based microtext normalization for twitter sentiment analysis",
      "authors": [
        "Ranjan Satapathy"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Data Mining Workshops (ICDMW)"
    },
    {
      "citation_id": "12",
      "title": "HSD shared task in VLSP campaign 2019: Hate speech detection for social good",
      "authors": [
        "Xuan-Son Vu"
      ],
      "year": "2020",
      "venue": "HSD shared task in VLSP campaign 2019: Hate speech detection for social good",
      "arxiv": "arXiv:2007.06493"
    },
    {
      "citation_id": "13",
      "title": "Online: Association for Computational Linguistics",
      "authors": [
        "Dorottya Demszky"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.372"
    },
    {
      "citation_id": "14",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin"
      ],
      "year": "2019",
      "venue": "NAACL-HLT (1)"
    }
  ]
}