{
  "paper_id": "2410.12567v1",
  "title": "Sequifi: Mitigating Catastrophic Forgetting In Speech Emotion Recognition With Sequential Class-Finetuning",
  "published": "2024-10-16T13:41:18Z",
  "authors": [
    "Sarthak Jain",
    "Orchid Chetia Phukan",
    "Swarup Ranjan Behera",
    "Arun Balaji Buduru",
    "Rajesh Sharma"
  ],
  "keywords": [
    "spotting in resource-constrained devices",
    "enabling ongoing adaptation without CF and maintaining accuracy comparable to retraining",
    "all while incurring significantly lower computational costs. Additionally",
    "Saget et al. (2024)  investigated lifelong learning methods to enhance the rob"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we introduce SeQuiFi, a novel approach for mitigating catastrophic forgetting (CF) in speech emotion recognition (SER). Se-QuiFi adopts a sequential class-finetuning strategy, where the model is fine-tuned incrementally on one emotion class at a time, preserving and enhancing retention for each class. While various state-of-the-art (SOTA) methods, such as regularization-based, memory-based, and weight-averaging techniques, have been proposed to address CF, it still remains a challenge, particularly with diverse and multilingual datasets. Through extensive experiments, we demonstrate that SeQuiFi significantly outperforms both vanilla fine-tuning and SOTA continual learning techniques in terms of accuracy and F1 scores on multiple benchmark SER datasets, including CREMA-D, RAVDESS, Emo-DB, MESD, and SHEMO, covering different languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is a task that has far-reaching implications, from enhancing human-computer interactions to improving diagnostic tools in healthcare settings. Despite notable advancements in SER methodologies, a significant challenge persists, the generalization of models across diverse data distributions and languages. Variations in acoustic properties, linguistic structures, and emotional expressions across different speakers and cultures lead to substantial performance degradation when models trained on a specific dataset or language are deployed in another context.\n\nA commonly preferred strategy to enhance model generalization is fine-tuning on new data, enabling models to adapt to the new distribution. However, this approach carries the risk of Catastrophic Forgetting (CF), wherein the model's ability * Authors contributed equally as first authors to retain previously acquired knowledge diminishes as it adjusts to new tasks. To combat CF, continual learning (CL) as a field has garnered sufficient attention, facilitating the incremental acquisition of new tasks while preserving performance on previously learned ones. These methodologies can be of various types such as regularization-based strategies, memory replay mechanisms, and weightaveraging techniques.\n\nResearch into CL techniques has gained attention in addressing CF across various speechprocessing tasks. Vander Eeckt and Van Hamme (2023) introduced a weight-averaging method for Automatic Speech Recognition (ASR) that averages the weights of models trained on new data distribution and the original data distribution. Similarly,  Plantinga et al. (2023)  also proposed a domain-expert averaging approach for end-to-end ASR.  Pham et al. (2023)  combined weight factorization and elastic-weight-consolidation (EWC) successfully expanding from 10 to 26 languages with minimal performance degradation.  Michieli et al. (2023)  explored online CL for keyword spotting in resource-constrained devices, utilizing high-order temporal statistics to efficiently manage CF while sustaining model performance.  Yang et al. (2022)  presented an online continual learning framework for end-to-end speech recognition models, enabling ongoing adaptation without CF and maintaining accuracy comparable to retraining, all while incurring significantly lower computational costs. Additionally,  Saget et al. (2024)  investigated lifelong learning methods to enhance the robustness of Mean Opinion Score (MOS) predictors for synthetic speech quality, showing advantages in cross-corpus evaluations compared to conventional batch training techniques.\n\nDespite the extensive exploration of CL in ASR and related speech-processing fields, its application in SER is virtually non-existent except  Tavernor et al. (2023) . They carried out the sole effort to bridge this critical gap, introducing an episodic memory mechanism aimed at fostering domainadaptable and robust SER. Their work not only highlights the promise of CL in mitigating CF within SER but also establishes a crucial foundation for future research in this domain. However, CF continues to challenge the effectiveness of SER models when confronted with new environments or unseen data. To tackle this enduring issue, we present SeQuential Finetuning (SeQuiFi) -a novel fine-tuning technique that incrementally trains the model on a new corpus by seeing one emotion class at a time. This retains the class-specific information when exposed to new data distributions, thus preserving the emotion recognition performance.\n\nThe main contributions of this paper are summarized as follows:\n\n• We proposed a novel fine-tuning technique, SeQuiFi, that employs a sequential classfinetuning strategy that improves class retention and significantly reduces catastrophic forgetting during adaptation to new emotional contexts.\n\n• We demonstrate through extensive experiments that SeQuiFi achieves significant gains in accuracy and F1 scores, outperforming existing SOTA continual learning methods.\n\nThe models and code developed for this study will be released after the double blind review process.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Sequifi",
      "text": "processed sequentially. For each class C j , a corresponding data subset D j is fine-tuned, updating the model parameters from θ j-1 to θ j . The final output is the fine-tuned parameters θ k , reflecting adaptation to all k emotion classes with minimized CF.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimentation",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Benchmark Datasets",
      "text": "We evaluate the SER models using a selection of diverse benchmark datasets that encompass a wide range of emotional expressions and contexts. CREMA-D  (Cao et al., 2014)  is a english SER database containing 7,442 emotional utterances from 91 actors across six categories. Emo-DB  (Burkhardt et al., 2005)  features 535 German utterances from 10 actors covering seven emotions. RAVDESS (Livingstone and Russo, 2018) is an English corpus that features audios from 24 actors spanning eight emotions. MESD  (Duville et al., 2021)  offers Spanish recordings reflecting six emotions, while  SHEMO (Mohamad Nezami et al., 2019)  is a persian corpus with includes 3,000 utterances. In our study, we focus solely on distribution change and not on class-incremental scenarios. Therefore, we keep the emotion classes the same, considering happiness, anger, sadness, and neutrality. Data distribution change can involve different data distributions within the same language as well as across languages.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Representation",
      "text": "In our experiments, we utilize representations from a range of pre-trained models (PTMs) alongside MFCC features. The employed PTMs include xvector  (Snyder et al., 2018) , ECAPA (Desplanques et al., 2020) trained for speaker recognition; WavLM  (Chen et al., 2022a)  a general-purpose speech representation learning model; UniSpeech-SAT  (Chen et al., 2022b) , pre-trained in a speakeraware format; and wav2vec2  (Baevski et al., 2020) . Additional details regarding data pre-processing and feature extraction are given in Appendix A.1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modelling",
      "text": "We use LSTM as a downstream network as supported by previous research  (Gupta et al., 2022; Zaiem et al., 2023) . We use two LSTM layers with 64 units each, using tanh activation and L2 regularization to prevent overfitting. This is followed by four dense layers (25, 20, 15, and 10 neurons) with ReLU activations, integrated with batch normalization and dropout for reducing overfitting. The final output is kept to 4 neurons representing the four emotion classes. We use softmax as the activation function in the last layer to get the class-wise probabilities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baseline Continual Learning Algorithms",
      "text": "Vanilla Fine-tuning serves as the simplest form of fine-tuning, wherein the model is directly trained on a new dataset without specific strategies to retain knowledge from previous tasks. Elastic Weight Consolidation (EWC)  (Kirkpatrick et al., 2017)  mitigates CF by selectively slowing down the learning of weights critical for previously learned tasks, allowing the model to retain expertise over time.\n\nWeight Averaging  (Vander Eeckt and Van Hamme, 2023)  addresses CF by averaging weights between old and new models, maintaining performance across tasks while enabling adaptation to new data.\n\nReplay techniques  (Aljundi et al., 2019; Merlin et al., 2022)  utilize a memory buffer to retain previous data, facilitating rehearsal and ensuring that older knowledge is preserved while learning new tasks. For replay, we keep 10% data from the previous dataset, selected randomly as a buffer. However, we make sure to keep the emotion class distribution same.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Details",
      "text": "We train the models with different feature representations with baseline CL algorithms for 60 epochs with a learning rate 1e-3 and Adam as the optimizer.\n\nWe keeo the batch size as 32. We use cross-entropy as the loss function. For the models with SeQuiFi, we train the models with 15 epochs per class, so 60 epochs across the whole dataset. We do this for a fair comparison of SeQuiFi trained models with baseline CL algorithms trained models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "For the experiments with SeQuiFi, we randomly start with a particular class. To assess the generalizability of SeQuiFi, we conduct a 5-fold evaluation, which involves five different sequences of emotion classes. For the same folds, we maintain the distribution consistent with the baseline CL algorithms to ensure fairness. Table  1  presents the evaluation scores of models built using various feature representations and CL algorithms. All scores are averages from a 5-fold evaluation. Our results demonstrate that models trained with SeQuiFi achieve superior performance by effectively mitigating CF and preserving the integrity of the previous data distribution, outperforming both vanilla fine-tuning and all baseline SOTA CL algorithms considered. This enhanced performance can be attributed to Se-QuiFi's ability to retain and amplify class-specific information. Remarkably, SeQuiFi maintains this high level of performance consistently across various feature representations, highlighting its input representation-agnostic capabilities. We further extended our experiments to include five datasets, and SeQuiFi consistently demonstrated superior performance. Detailed results can be found in Appendix Table  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduce, SeQuiFi, a novel finetuning approach for effectively mitigating CF in SER. By employing a sequential fine-tuning strategy, SeQuiFi incrementally refined the model on individual emotion classes, thereby enhancing retention and knowledge preservation. Despite advancements in SOTA CL approaches challenges in addressing CF persisted, particularly with diverse multilingual datasets. Our comprehensive experiments demonstrated that SeQuiFi significantly surpasses both vanilla fine-tuning and SOTA CL techniques, achieving notable improvements in accuracy and F1 scores across benchmark SER datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Limitations",
      "text": "We have focused our experiments exclusively on LSTM as the downstream network. Previous research in speech processing indicates that downstream performance can vary considerably based on model selection  (Zaiem et al., 2023) . Consequently, we intend to broaden our exploration by incorporating a diverse range of downstream networks in future experiments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Considerations",
      "text": "We have experimented with openly available datasets. No user information was accessed.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table 1: Evaluation Scores: A and F1 represent accuracy and macro-average F1 scores, respectively. C, R, E,",
      "data": [
        {
          "SD": "",
          "Model Type": "",
          "CREMA-D": "A\nF1",
          "RAVDESS": "A\nF1",
          "Emo-DB": "A\nF1",
          "MESD": "A\nF1",
          "SHEMO": "A\nF1"
        },
        {
          "SD": "C",
          "Model Type": "IM (x-vector)\nIM (UnispeechSAT)\nIM (WaveLM)\nIM (ECAPA)\nIM (MFCC)\nIM (Wave2Vec)",
          "CREMA-D": "69.69\n68.42\n76.84\n76.62\n44.59\n41.79\n53.27\n47.69\n52.35\n44.43\n55.65\n53.99",
          "RAVDESS": "56.29\n56.51\n49.62\n48.58\n38.51\n34.41\n51.85\n44.85\n42.22\n35.06\n34.81\n34.38",
          "Emo-DB": "70.58\n64.62\n52.94\n42.71\n25.00\n19.63\n26.47\n19.21\n32.35\n24.63\n47.05\n36.74",
          "MESD": "41.73\n38.63\n31.30\n22.72\n25.21\n16.66\n27.82\n10.88\n27.82\n21.25\n26.95\n13.56",
          "SHEMO": "56.02\n44.56\n37.04\n34.33\n20.98\n17.70\n32.29\n29.90\n32.11\n27.62\n16.24\n10.80"
        },
        {
          "SD": "C+R",
          "Model Type": "FT (x-vector)\nFT (Unispeech-SAT)\nFT (WavLM)\nFT (Ecapa)\nFT (MFCC)\nFT (Wav2Vec2)",
          "CREMA-D": "52.04\n46.73\n47.35\n41.53\n29.29\n22.29\n48.37\n41.16\n48.06\n37.87\n30.43\n25.34",
          "RAVDESS": "62.22\n56.65\n61.48\n56.66\n45.18\n36.09\n67.40\n57.54\n55.55\n45.38\n54.81\n45.84",
          "Emo-DB": "67.64\n58.25\n58.82\n48.52\n36.76\n25.98\n48.52\n39.42\n52.94\n39.97\n52.94\n43.85\n29.62\n19.14\n25.92\n10.29\n20.74\n8.58\n28.14\n18.58\n32.59\n27.62\n25.92\n10.29\n67.64\n58.25\n58.82\n48.52\n36.76\n25.98\n48.52\n39.42\n52.94\n39.97\n52.94\n43.85\n67.64\n60.66\n63.23\n57.54\n22.05\n17.07\n45.58\n43.20\n52.94\n39.97\n19.11\n15.27\n85.29\n83.76\n70.58\n72.88\n33.82\n33.02\n55.88\n55.23\n47.05\n43.33\n55.88\n56.71",
          "MESD": "34.78\n29.30\n40.00\n35.03\n27.82\n20.25\n23.47\n19.00\n33.04\n27.47\n30.43\n24.75\n38.23\n19.70\n11.76\n9.48\n51.47\n33.13\n39.70\n14.21\n27.94\n10.91\n27.94\n10.91\n34.78\n29.30\n40.00\n35.03\n27.82\n20.25\n23.47\n19.00\n33.04\n27.47\n30.43\n24.75\n33.04\n32.80\n30.43\n26.57\n26.08\n23.04\n23.47\n19.00\n33.04\n27.47\n30.43\n24.75\n34.78\n34.63\n39.13\n37.97\n24.34\n23.13\n38.26\n37.94\n24.34\n21.54\n22.60\n18.80",
          "SHEMO": "34.12\n30.09\n32.29\n30.46\n32.48\n19.83\n23.54\n21.40\n39.23\n30.39\n22.44\n19.86\n30.43\n20.38\n27.82\n10.88\n20.86\n8.63\n20.86\n8.63\n27.82\n10.95\n20.86\n8.63\n34.12\n30.09\n32.29\n30.46\n32.48\n19.83\n23.54\n21.40\n39.23\n30.39\n22.44\n19.86\n48.90\n43.24\n33.39\n31.20\n15.87\n13.91\n23.54\n21.40\n39.23\n30.39\n22.44\n19.86\n51.45\n46.12\n55.83\n50.64\n32.66\n28.23\n33.21\n33.66\n49.27\n41.65\n40.32\n33.20"
        },
        {
          "SD": "",
          "Model Type": "WA (x-vector)\nWA (Unispeech-SAT)\nWA (WavLM)\nWA (ECAPA)\nWA (MFCC)\nWA (Wav2Vec2)",
          "CREMA-D": "25.61\n10.38\n25.61\n10.20\n25.00\n10.00\n27.45\n14.11\n23.16\n9.40\n27.83\n10.88",
          "RAVDESS": "23.16\n9.40\n25.61\n10.19\n25.61\n10.19\n26.53\n16.66\n25.00\n10.00\n23.47\n9.50",
          "Emo-DB": "",
          "MESD": "",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "EWC (x-vector)\nEWC (Unispeech-SAT)\nEWC (WavLM)\nEWC (ECAPA)\nEWC (MFCC)\nEWC (Wav2Vec2)",
          "CREMA-D": "51.94\n46.54\n36.53\n29.30\n28.98\n20.63\n48.67\n41.36\n30.00\n17.58\n31.30\n25.71",
          "RAVDESS": "58.51\n51.33\n60.74\n56.96\n48.88\n39.62\n68.88\n58.80\n39.25\n28.89\n56.29\n46.93",
          "Emo-DB": "",
          "MESD": "",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "Replay (x-vector)\nReplay (Unispeech-SAT)\nReplay (WavLM)\nReplay (ECAPA)\nReplay (MFCC)\nReplay (Wav2Vec2)",
          "CREMA-D": "63.27\n60.99\n60.20\n56.93\n39.80\n40.50\n54.08\n52.96\n48.98\n39.77\n25.00\n16.23\n71.12\n70.65\n71.02\n70.19\n45.92\n44.75\n71.43\n71.26\n53.98\n51.59\n61.74\n61.54",
          "RAVDESS": "47.40\n44.79\n40.74\n36.75\n28.88\n25.15\n49.62\n46.48\n39.25\n28.89\n22.96\n15.69",
          "Emo-DB": "",
          "MESD": "",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "SeQuiFi (x-vector)\nSeQuiFi (Unispeech-SAT)\nSeQuiFi (WavLM)\nSeQuiFi (ECAPA)\nSeQuiFi (MFCC)\nSeQuiFi (Wav2Vec2)",
          "CREMA-D": "",
          "RAVDESS": "62.22\n61.52\n42.22\n42.42\n34.81\n29.52\n67.40\n66.71\n44.44\n40.51\n44.44\n44.45",
          "Emo-DB": "",
          "MESD": "",
          "SHEMO": ""
        },
        {
          "SD": "C+R+E",
          "Model Type": "FT (x-vector)\nFT (Unispeech-SAT)\nFT (WavLM)\nFT (ECAPA)\nFT (MFCC)\nFT (Wav2vec2)",
          "CREMA-D": "23.16\n11.93\n23.87\n15.05\n25.00\n10.00\n25.00\n10.00\n25.20\n10.41\n23.47\n9.50",
          "RAVDESS": "56.30\n55.13\n46.67\n40.29\n25.93\n23.13\n43.70\n39.64\n38.51\n39.26\n47.41\n46.40",
          "Emo-DB": "49.62\n43.62\n38.51\n37.14\n19.25\n13.87\n28.88\n16.56\n33.05\n38.51\n37.03\n29.76",
          "MESD": "42.60\n36.75\n33.91\n34.12\n26.08\n18.79\n30.43\n17.96\n31.46\n36.52\n25.21\n16.07\n26.50\n12.00\n26.30\n11.25\n25.50\n10.70\n20.90\n10.25\n28.50\n18.10\n27.60\n10.75\n43.78\n35.91\n42.85\n37.60\n25.74\n11.48\n45.59\n42.98\n27.52\n14.31\n31.68\n21.21\n52.43\n40.78\n53.25\n43.10\n29.56\n13.25\n29.78\n12.89\n22.98\n13.99\n42.58\n33.20\n68.00\n66.50\n48.80\n47.20\n39.20\n35.80\n69.00\n68.00\n43.60\n39.70\n44.10\n40.50",
          "SHEMO": "39.60\n31.99\n23.91\n21.11\n27.11\n11.79\n35.43\n20.66\n33.48\n31.88\n29.11\n18.61\n26.00\n11.25\n26.10\n11.10\n25.80\n10.90\n21.10\n10.40\n28.70\n18.20\n27.70\n10.80\n44.10\n36.25\n43.05\n38.30\n26.09\n11.89\n46.11\n43.13\n27.76\n14.65\n31.92\n21.43\n54.89\n43.50\n52.95\n42.80\n30.10\n12.45\n30.30\n13.10\n23.25\n14.30\n43.00\n34.10\n77.77\n66.89\n51.90\n48.85\n34.80\n30.76\n68.92\n61.87\n58.85\n51.81\n60.89\n55.84"
        },
        {
          "SD": "",
          "Model Type": "WA (x-vector)\nWA (Unispeech-SAT)\nWA (WavLM)\nWA (ECAPA)\nWA (MFCC)\nWA (Wav2vec2)",
          "CREMA-D": "25.61\n10.37\n25.61\n10.19\n25.00\n10.00\n27.44\n14.10\n23.16\n9.40\n27.82\n10.88",
          "RAVDESS": "26.66\n13.75\n25.92\n10.73\n25.92\n10.29\n20.74\n10.12\n28.88\n18.41\n27.40\n10.88",
          "Emo-DB": "27.00\n11.50\n26.00\n11.00\n26.50\n11.00\n21.00\n10.30\n29.00\n18.00\n27.50\n11.00",
          "MESD": "",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "EWC (x-vector)\nEWC (Unispeech-SAT)\nEWC (WavLM)\nEWC (ECAPA)\nEWC (MFCC)\nEWC (Wav2vec2)",
          "CREMA-D": "51.93\n46.53\n36.53\n29.29\n28.97\n20.62\n48.67\n41.35\n30.00\n17.58\n31.30\n25.70",
          "RAVDESS": "40.74\n32.90\n42.96\n38.71\n25.92\n10.29\n45.92\n43.89\n27.40\n13.10\n31.85\n20.54",
          "Emo-DB": "42.15\n34.32\n42.45\n37.86\n26.31\n12.54\n46.22\n42.67\n27.89\n14.78\n32.14\n21.67",
          "MESD": "",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "Replay (x-vector)\nReplay (Unispeech-SAT)\nReplay (WavLM)\nReplay (ECAPA)\nReplay (MFCC)\nReplay (Wav2vec2)",
          "CREMA-D": "63.26\n60.98\n60.20\n56.93\n39.79\n40.49\n54.08\n52.95\n48.97\n39.77\n25.00\n16.23\n71.02\n70.79\n71.42\n70.49\n45.81\n43.38\n71.93\n71.61\n55.30\n52.65\n63.47\n63.50",
          "RAVDESS": "42.85\n31.42\n52.14\n42.50\n28.57\n11.11\n28.57\n11.11\n21.42\n13.80\n42.85\n33.71\n63.70\n62.51\n43.70\n42.77\n36.29\n32.15\n67.40\n66.70\n41.48\n36.33\n42.22\n42.59",
          "Emo-DB": "55.12\n45.89\n51.68\n41.75\n30.89\n12.87\n30.05\n13.45\n23.54\n14.55\n43.12\n34.50",
          "MESD": "",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "SeQuiFi (x-vector)\nSeQuiFi (Unispeech-SAT)\nSeQuiFi (WavLM)\nSeQuiFi (ECAPA)\nSeQuiFi (MFCC)\nSeQuiFi (Wav2vec2)",
          "CREMA-D": "",
          "RAVDESS": "",
          "Emo-DB": "75.10\n74.15\n72.30\n70.50\n47.10\n44.55\n74.20\n73.10\n58.90\n55.80\n65.30\n62.00",
          "MESD": "",
          "SHEMO": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SD": "",
          "Model Type": "",
          "CREMA-D": "A\nF1",
          "RAVDESS": "A\nF1",
          "Emo-DB": "A\nF1",
          "MESD": "A\nF1",
          "SHEMO": "A\nF1"
        },
        {
          "SD": "C+R+E+M",
          "Model Type": "FT (x-vector)\nFT (Unispeech-SAT)\nFT (WavLM)\nFT (ECAPA)\nFT (MFCC)\nFT (Wav2vec2)",
          "CREMA-D": "22.34\n15.04\n25.20\n13.81\n24.79\n9.93\n25.00\n10.00\n23.16\n9.40\n23.47\n9.50",
          "RAVDESS": "22.22\n15.11\n28.88\n17.37\n25.92\n10.29\n25.92\n10.29\n20.74\n8.58\n25.92\n10.29",
          "Emo-DB": "52.21\n54.91\n36.76\n37.59\n26.47\n24.10\n51.47\n47.76\n26.47\n26.05\n13.24\n12.24",
          "MESD": "53.12\n55.30\n37.10\n37.98\n25.90\n24.50\n50.89\n48.10\n26.00\n25.80\n12.90\n12.50",
          "SHEMO": "54.00\n55.75\n36.95\n37.80\n26.30\n24.80\n51.20\n47.90\n26.20\n25.95\n13.10\n12.80\n23.15\n14.50\n40.05\n14.45\n39.95\n14.40\n40.15\n14.50\n40.05\n14.45\n39.95\n14.35\n50.80\n46.10\n36.90\n29.70\n29.00\n20.90\n49.10\n41.90\n30.70\n18.20\n31.50\n26.20\n63.10\n60.90\n60.50\n57.30\n40.00\n40.80\n54.50\n53.30\n49.20\n40.10\n25.30\n16.50\n76.00\n77.00\n56.00\n57.00\n38.00\n37.00\n63.00\n62.00\n48.00\n47.00\n61.00\n62.00"
        },
        {
          "SD": "",
          "Model Type": "WA (x-vector)\nWA (Unispeech-SAT)\nWA (WavLM)\nWA (ECAPA)\nWA (MFCC)\nMS (Wav2vec2)",
          "CREMA-D": "22.50\n14.10\n40.20\n14.50\n39.90\n14.30\n40.30\n14.50\n40.10\n14.40\n39.80\n14.20",
          "RAVDESS": "23.00\n14.30\n40.60\n14.70\n40.10\n14.50\n40.50\n14.70\n40.30\n14.60\n40.00\n14.40",
          "Emo-DB": "22.06\n13.93\n39.71\n14.21\n39.71\n14.21\n39.71\n14.21\n39.71\n14.21\n39.71\n14.21",
          "MESD": "22.85\n14.25\n39.95\n14.35\n39.85\n14.25\n40.05\n14.35\n39.95\n14.30\n39.85\n14.25",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "EWC (x-vector)\nEWC (Unispeech-SAT)\nEWC (WavLM)\nEWC (ECAPA)\nEWC (MFCC)\nEWC (Wav2vec2)",
          "CREMA-D": "51.50\n46.00\n37.00\n29.50\n29.10\n20.80\n49.00\n41.70\n30.50\n18.00\n31.70\n26.00",
          "RAVDESS": "40.50\n32.50\n43.20\n38.90\n26.10\n10.40\n46.10\n44.20\n27.80\n13.50\n32.10\n20.80",
          "Emo-DB": "50.30\n45.75\n36.70\n29.30\n28.90\n20.70\n48.50\n41.50\n30.20\n17.80\n31.40\n25.80",
          "MESD": "39.90\n32.20\n42.90\n38.60\n25.80\n10.30\n45.90\n43.80\n27.40\n13.20\n31.90\n20.60",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "Replay (x-vector)\nReplay (Unispeech-SAT)\nReplay (WavLM)\nReplay (ECAPA)\nReplay (MFCC)\nReplay (Wav2vec2)",
          "CREMA-D": "63.00\n60.80\n60.30\n57.10\n39.90\n40.60\n54.30\n53.10\n49.10\n40.00\n25.20\n16.40\n70.40\n69.75\n70.40\n69.29\n45.61\n43.80\n71.73\n71.28\n57.24\n53.53\n66.95\n67.03",
          "RAVDESS": "43.00\n31.50\n52.30\n42.70\n28.70\n11.20\n28.70\n11.20\n21.60\n13.90\n43.00\n33.90\n61.48\n60.10\n43.70\n42.77\n35.55\n31.41\n67.40\n66.63\n42.96\n37.68\n43.70\n42.99",
          "Emo-DB": "62.90\n60.70\n60.10\n56.90\n39.70\n40.40\n54.00\n52.90\n48.90\n39.80\n25.00\n16.20\n83.82\n82.61\n69.11\n71.24\n35.29\n34.80\n57.35\n58.04\n47.05\n42.28\n55.88\n57.12",
          "MESD": "42.90\n31.40\n52.10\n42.50\n28.50\n11.10\n28.50\n11.10\n21.40\n13.80\n42.80\n33.70",
          "SHEMO": ""
        },
        {
          "SD": "",
          "Model Type": "SeQuiFi (x-vector)\nSeQuiFi (Unispeech-SAT)\nSeQuiFi (WavLM)\nSeQuiFi (ECAPA)\nSeQuiFi (MFCC)\nSeQuiFi (Wav2vec2)",
          "CREMA-D": "",
          "RAVDESS": "",
          "Emo-DB": "",
          "MESD": "75.00\n74.50\n55.00\n54.50\n40.00\n39.50\n65.00\n64.50\n50.00\n49.50\n60.00\n59.50",
          "SHEMO": ""
        },
        {
          "SD": "C+R+E+M+S",
          "Model Type": "FT (x-vector)\nFT (Unispeech-SAT)\nFT (WavLM)\nFT (ECAPA)\nFT (MFCC)\nFT (Wav2vec2)",
          "CREMA-D": "29.40\n23.70\n25.80\n15.00\n25.60\n16.40\n24.40\n23.40\n28.40\n22.10\n20.10\n10.10",
          "RAVDESS": "33.50\n27.00\n26.00\n14.00\n26.80\n16.00\n21.50\n20.20\n29.70\n26.80\n22.30\n11.70",
          "Emo-DB": "35.40\n25.80\n29.50\n16.70\n41.30\n27.40\n22.10\n17.70\n13.30\n15.00\n19.20\n11.50",
          "MESD": "34.00\n29.30\n21.80\n10.60\n31.40\n23.50\n41.80\n41.10\n27.00\n22.60\n20.90\n8.80",
          "SHEMO": "36.00\n26.90\n27.00\n13.70\n40.20\n26.80\n27.50\n25.10\n25.40\n19.20\n22.00\n9.50"
        },
        {
          "SD": "",
          "Model Type": "WA (x-vector)\nWA (Unispeech-SAT)\nWA (WavLM)\nWA (ECAPA)\nWA (MFCC)\nWA (Wav2vec2)",
          "CREMA-D": "22.15\n18.50\n23.10\n17.85\n21.90\n17.50\n24.00\n19.20\n20.50\n16.00\n23.25\n18.10",
          "RAVDESS": "27.40\n19.30\n26.25\n18.90\n25.80\n18.60\n27.80\n20.00\n24.70\n17.80\n26.90\n19.40",
          "Emo-DB": "30.10\n25.50\n31.00\n26.00\n32.50\n27.10\n33.00\n28.00\n29.50\n24.00\n30.50\n25.70",
          "MESD": "28.00\n21.90\n29.10\n22.50\n30.00\n23.00\n31.00\n24.50\n26.70\n20.00\n27.90\n21.40",
          "SHEMO": "21.90\n16.80\n15.85\n10.75\n24.80\n20.70\n22.95\n20.85\n18.75\n12.65\n12.88\n11.78"
        },
        {
          "SD": "",
          "Model Type": "EWC (x-vector)\nEWC (Unispeech-SAT)\nEWC (WavLM)\nEWC (ECAPA)\nEWC (MFCC)\nEWC (Wav2vec2)",
          "CREMA-D": "35.12\n29.45\n32.87\n27.34\n30.25\n24.56\n36.42\n28.95\n31.40\n23.90\n34.12\n30.75",
          "RAVDESS": "25.78\n19.56\n22.18\n17.80\n20.90\n15.45\n26.11\n21.25\n22.65\n16.70\n24.33\n18.90",
          "Emo-DB": "32.10\n28.45\n30.20\n26.00\n28.75\n25.20\n34.00\n27.30\n29.30\n24.50\n33.00\n29.00",
          "MESD": "29.00\n24.30\n27.50\n22.40\n26.10\n20.50\n30.00\n25.80\n26.80\n21.00\n28.00\n23.10",
          "SHEMO": "23.92\n18.82\n16.88\n11.78\n21.85\n19.75\n30.90\n28.80\n27.87\n24.77\n35.91\n31.81"
        },
        {
          "SD": "",
          "Model Type": "Replay (x-vector)\nReplay (Unispeech-SAT)\nReplay (WavLM)\nReplay (ECAPA)\nReplay (MFCC)\nReplay (Wav2vec2)",
          "CREMA-D": "28.12\n24.45\n29.87\n25.34\n27.25\n23.56\n30.42\n26.95\n28.40\n22.90\n29.12\n28.75\n69.59\n68.88\n67.04\n66.37\n45.61\n43.84\n70.81\n70.67\n50.71\n48.61\n60.86\n61.34",
          "RAVDESS": "22.78\n19.56\n23.18\n18.80\n21.90\n16.45\n24.11\n21.25\n20.65\n16.70\n25.33\n19.90\n63.70\n62.64\n40.00\n38.05\n33.33\n29.25\n64.44\n63.34\n45.18\n41.68\n42.22\n42.40",
          "Emo-DB": "30.10\n26.45\n31.20\n27.00\n29.75\n25.20\n34.00\n28.30\n29.30\n24.50\n32.00\n29.00\n86.76\n87.13\n75.00\n75.70\n33.82\n30.47\n58.82\n59.64\n51.47\n51.41\n54.41\n55.97",
          "MESD": "27.00\n21.30\n28.50\n22.40\n26.10\n20.50\n30.00\n24.80\n26.80\n21.00\n28.00\n23.10\n39.13\n38.48\n36.52\n35.35\n23.47\n21.65\n41.73\n41.00\n20.00\n15.26\n26.95\n23.22",
          "SHEMO": "26.92\n21.82\n21.88\n19.78\n26.85\n22.75\n28.90\n21.80\n25.87\n21.77\n19.91\n14.81"
        },
        {
          "SD": "",
          "Model Type": "SeQuiFi (x-vector)\nSeQuiFi (Unispeech-SAT)\nSeQuiFi (WavLM)\nSeQuiFi (ECAPA)\nSeQuiFi (MFCC)\nSeQuiFi (Wav2vec2)",
          "CREMA-D": "",
          "RAVDESS": "",
          "Emo-DB": "",
          "MESD": "",
          "SHEMO": "60.25\n58.90\n48.45\n40.30\n18.75\n17.00\n65.40\n59.20\n40.50\n39.00\n49.80\n44.00"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gradient based sample selection for online continual learning",
      "authors": [
        "Rahaf Aljundi",
        "Min Lin",
        "Baptiste Goujaud",
        "Yoshua Bengio"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "2022a. Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "2022b. Unispeech-sat: Universal speech representation learning with speaker aware pre-training",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Zhuo Chen",
        "Shujie Liu",
        "Jian Wu",
        "Yao Qian",
        "Furu Wei",
        "Jinyu Li"
      ],
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "Brecht Desplanques",
        "Jenthe Thienpondt",
        "Kris Demuynck"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "8",
      "title": "The mexican emotional speech database (mesd): elaboration and assessment based on machine learning",
      "authors": [
        "Luz Ecapa-Tdnn ; Mathilde M Duville",
        "David Alonso-Valerdi",
        "Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)",
      "arxiv": "arXiv:2005.07143"
    },
    {
      "citation_id": "9",
      "title": "Adima: Abuse detection in multilingual audio",
      "authors": [
        "Vikram Gupta",
        "Rini Sharon",
        "Ramit Sawhney",
        "Debdoot Mukherjee"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP43922.2022.9746718"
    },
    {
      "citation_id": "10",
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": [
        "James Kirkpatrick",
        "Razvan Pascanu",
        "Neil Rabinowitz",
        "Joel Veness",
        "Guillaume Desjardins",
        "Andrei Rusu",
        "Kieran Milan",
        "John Quan",
        "Tiago Ramalho",
        "Agnieszka Grabska-Barwinska"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "11",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "12",
      "title": "Practical recommendations for replay-based continual learning methods",
      "authors": [
        "Gabriele Merlin",
        "Vincenzo Lomonaco",
        "Andrea Cossu",
        "Antonio Carta",
        "Davide Bacciu"
      ],
      "year": "2022",
      "venue": "International Conference on Image Analysis and Processing"
    },
    {
      "citation_id": "13",
      "title": "Online continual learning in keyword spotting for low-resource devices via pooling high-order temporal statistics",
      "authors": [
        "Umberto Michieli",
        "Pablo Parada",
        "Mete Ozay"
      ],
      "year": "2023",
      "venue": "INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-90"
    },
    {
      "citation_id": "14",
      "title": "Shemo: a large-scale validated database for persian speech emotion detection",
      "authors": [
        "Mohamad Omid",
        "Paria Jamshid Nezami",
        "Mansoureh Lou",
        "Karami"
      ],
      "year": "2019",
      "venue": "Shemo: a large-scale validated database for persian speech emotion detection",
      "doi": "10.1007/s10579-018-9427-x"
    },
    {
      "citation_id": "15",
      "title": "Towards continually learning new languages",
      "authors": [
        "Quan Pham",
        "Jan Niehues",
        "Alex Waibel"
      ],
      "year": "2023",
      "venue": "INTER-SPEECH 2023",
      "doi": "10.21437/Interspeech.2023-1867"
    },
    {
      "citation_id": "16",
      "title": "Continual learning for end-to-end asr by averaging domain experts",
      "authors": [
        "Peter Plantinga",
        "Jaekwon Yoo",
        "Chandra Dhir"
      ],
      "year": "2023",
      "venue": "Continual learning for end-to-end asr by averaging domain experts",
      "arxiv": "arXiv:2305.09681"
    },
    {
      "citation_id": "17",
      "title": "Lifelong learning mos prediction for synthetic speech quality evaluation",
      "authors": [
        "Félix Saget",
        "Meysam Shamsi",
        "Marie Tahon"
      ],
      "year": "2024",
      "venue": "Interspeech 2024",
      "doi": "10.21437/Interspeech.2024-959"
    },
    {
      "citation_id": "18",
      "title": "Xvectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "David Snyder",
        "Daniel Garcia-Romero",
        "Gregory Sell",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "19",
      "title": "Episodic memory for domainadaptable, robust speech emotion recognition",
      "authors": [
        "James Tavernor",
        "Matthew Perez",
        "Emily Provost"
      ],
      "year": "2023",
      "venue": "INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-2111"
    },
    {
      "citation_id": "20",
      "title": "Weight averaging: A simple yet effective method to overcome catastrophic forgetting in automatic speech recognition",
      "authors": [
        "Steven Vander",
        "Hugo Van Hamme"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP49357.2023.10095147"
    },
    {
      "citation_id": "21",
      "title": "Online continual learning of end-to-end speech recognition models",
      "authors": [
        "Muqiao Yang",
        "Ian Lane",
        "Shinji Watanabe"
      ],
      "year": "2022",
      "venue": "Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-11093"
    },
    {
      "citation_id": "22",
      "title": "Speech selfsupervised representation benchmarking: Are we doing it right?",
      "authors": [
        "Salah Zaiem",
        "Youcef Kemiche",
        "Titouan Parcollet",
        "Slim Essid",
        "Mirco Ravanelli"
      ],
      "year": "2023",
      "venue": "INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-1087"
    }
  ]
}