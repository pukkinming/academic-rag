{
  "paper_id": "2106.09539v1",
  "title": "Automatic Analysis Of The Emotional Content Of Speech In Daylong Child-Centered Recordings From A Neonatal Intensive Care Unit",
  "published": "2021-06-14T11:17:52Z",
  "authors": [
    "Einari Vaaras",
    "Sari Ahlqvist-Björkroth",
    "Konstantinos Drossos",
    "Okko Räsänen"
  ],
  "keywords": [
    "speech emotion recognition",
    "speech analysis",
    "real-world audio",
    "daylong audio",
    "LENA recorder"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Researchers have recently started to study how the emotional speech heard by young infants can affect their developmental outcomes. As a part of this research, hundreds of hours of daylong recordings from preterm infants' audio environments were collected from two hospitals in Finland and Estonia in the context of so-called APPLE study. In order to analyze the emotional content of speech in such a massive dataset, an automatic speech emotion recognition (SER) system is required. However, there are no emotion labels or existing indomain SER systems to be used for this purpose. In this paper, we introduce this initially unannotated large-scale real-world audio dataset and describe the development of a functional SER system for the Finnish subset of the data. We explore the effectiveness of alternative state-of-the-art techniques to deploy a SER system to a new domain, comparing cross-corpus generalization, WGAN-based domain adaptation, and active learning in the task. As a result, we show that the best-performing models are able to achieve a classification performance of 73.4% unweighted average recall (UAR) and 73.2% UAR for a binary classification for valence and arousal, respectively. The results also show that active learning achieves the most consistent performance compared to the two alternatives.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In speech emotion recognition (SER), the task is to recognize emotional states of speakers from speech signals  [1, 2] . One potential application of SER is the study of babies' auditory environments, where the early emotional experiences of babies, including affective speech, can impact their later cognitive development. In order to study this relationship, Auditory environment by Parents of Preterm infant; Language development and Eye-movements (APPLE) study has collected a large audio corpus of child-centered daylong audio recordings from neonatal intensive care units (NICUs), recorded in Turku University Hospital, Finland, and Tallinn Children's Hospital, Estonia  [3] . In order to analyze the emotional contents of speech in the recordings, a functional SER system for this new domain is required.\n\nThe purpose of the present study is to develop such a system to analyze these (initially unannotated) hospital-environment audio recordings for their emotional speech content. The absence of in-domain annotations and massive scale of the data raises the question of how to most effectively deploy a SER system for this real-world large-scale dataset.\n\nIn principle, cross-corpus generalization (CCG) is the most straightforward strategy to deploy SER for an unlabeled dataset, but can suffer from domain mismatch. In fact,  [4]  have shown through extensive multi-corpus and multilingual experiments that reliable CCG-based SER was only feasible with certain corpora and emotional classes, highlighting many issues with cross-domain SER model generalization to out-of-domain data (but see also, e.g.,  [5]  for a potential remedy). In order to tackle the issue of domain mismatch, different domain adaptation (DA) methods have been utilized in SER. For instance, Deng et al.  [6]  extended an unsupervised deep denoising autoencoder (AE) by combining it with a supervised learning objective to create a semi-supervised DA method for SER. Another approach in  [7]  used an unsupervised deep neural network (DNN)based adversarial DA approach for SER. The method learns a domain-invariant feature representation between labeled source data and unlabeled target-domain data while maintaining a good performance on the primary SER task. A number of other DA methods for SER have been proposed as well (e.g.,  [8] [9] [10] ).\n\nActive learning (AL) is another strategy and has been successfully applied to SER as well. Zhao and Ma  [11]  presented an iterative AL algorithm, which utilizes conditional random fields, to determine the level of uncertainty for each unlabeled sample. The most uncertain samples were then selected for human annotation. Another study  [12]  examined different AL methods based on uncertainty and diversity maximization in a simulation setup with DNN classifiers. The work showed that the tested AL methods outperformed random sampling-based methods with a constrained labeling budget.\n\nOnly a few SER studies have been conducted on large-scale datasets. Jia et al.  [13]  studied DNN-based SER with a massive 7-million-utterance internet voice corpus. They pretrained their novel DNN-based models with 90,000 unlabeled utterances, and fine-tuned and evaluated them on 3,000 randomly selected manually annotated utterances from the same dataset. Fan et al.  [14]  presented a SER dataset with a total duration of over 200 hours. They proposed a novel SER model containing pyramid convolutions which outperformed other models that were tested on the dataset. Additionally, they showed that existing models are prone to overfit to small-scale datasets, which limits the ability of these models to generalize for real-life data.\n\nHowever, CCG, AL, and DA have rarely been compared to each other directly. Moreover, most of the existing work has been conducted using studio, telephone, or internet speech data. Therefore, our present daylong audio dataset from a hospital context, together with its practical significance, provides an excellent test bench to compare strategies for SER system development in a novel domain with challenging real-world speech data. More specifically, by using the Finnish subset of the data, we compare CCG and state-of-the-art DA and AL in the task to study their feasibility and SER performance in practice. arXiv:2106.09539v1 [eess.AS] 14 Jun 2021 2. Methods",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Medoid-Based Active Learning",
      "text": "Zhao et al.  [15]  presented an AL method called medoid-based active learning (MAL) to effectively utilize a small number of annotations, which serves as the foundation of the AL method used in our experiments. The algorithm can be divided into three subsequent parts: 1) obtaining a distance matrix that contains the pairwise distances between all samples in the dataset, 2) performing k-medoids clustering using the distance matrix, and 3) starting from the largest cluster, querying human annotations for the medoids in a descending cluster size order.\n\nThe distance metric used in the present experiments was selected based on pilot experiments with MAL using existing SER datasets. A 600-dimensional utterance-level log-mel feature representation (see Section 4.1) was first used as the initial feature representation of each sample in a dataset. These features were then compressed into a 32-dimensional latent representation using a DNN-based AE with six layers. Pearson distances dP  [16]  between the bottleneck features were then used to define the affinity matrix A across all the samples. Next, k-medoids clustering was applied to the data. First, one sample was randomly selected as the member of a set S, followed by an addition of k -1 more samples as centroids using the farthestfirst traversal algorithm. Here, the distance from a sample, a, to the set S was defined as\n\nThe samples in S were then used as the initial medoids for a k-medoids clustering algorithm (see e.g.  [17]  for an overview) to assign each sample in the dataset into one of the clusters.\n\nIn the final stage, the clusters were sorted in a descending order based on the number of samples in each cluster, and their medoids were presented to human annotators for labeling. In the experiments, we studied the use of these labels in two different ways: i) assigning each sample in a cluster with the annotated medoid label (as in  [15] ; here referred to as \"cluster labels\"), or ii) only using the medoid samples as labeled data for classifier training, which was not studied in the original MAL paper  [15] . Based on pilot experiments on other datasets, k was set to N  3 , where N is the number of samples in a corpus.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Wasserstein Distance-Based Domain Adaptation",
      "text": "The present DA approach was based on the Wasserstein distance-based domain adaptation (WDA) method proposed in  [18] . In WDA, a neural network (NN) classifier, aka the source model M , is adapted to a target corpus, DT , by using labeled data from source domain corpus/corpora, DS. The source model M consists of two parts, a feature extractor, FS, and a label classifier, CL. The adaptation process of WDA involves two stages, which are demonstrated in Fig.  1 .\n\nThe first stage (Fig.  1 , top) consists of training M using samples XS and their labels YS from DS to obtain a trained FS. This is done using binary cross-entropy  [18]  as the loss:\n\nIn the second stage (Fig.  1 , bottom), FS is adapted to DT to obtain an adapted feature extractor, FT , by minimizing the Wasserstein-1 distance W d between the distributions of DS and DT using an adversarial training process. Following a WGAN framework  [19] , FS is adapted into FT by finding a common\n\nStep 1",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cl Fs Ys Xs",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Predicted Label",
      "text": "Step 2",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cd Ft Xt",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Predicted Domain",
      "text": "Figure  1 : The two-step the adaptation process of WDA. First, FS and CL are trained to classify source corpus samples into emotion categories. In the second step, FS is adapted into FT using a domain discriminator CD with an adversarial loss.\n\nfeature representation for DS and DT by iteratively minimizing the two losses:\n\nwhere CD is the domain discriminator and XT are the target corpus samples. The parameters for CD and FT are updated in turns, where Eqs. 3 and 4 are the loss functions for updating the parameters of CD and FT , respectively. The output features of FT are the input features for CD. Additionally, the parameters of FS serve as the initial parameters of FT . As pointed out in  [18] , the minimization of Eqs. 3 and 4 is shown to minimize W d between the distributions of DS and DT . For a detailed formulation of the WDA algorithm, see Algorithm 1 in  [18] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Corpus Generalization",
      "text": "As our baseline approach, we use CCG with different source corpora and their combinations. Labels of each corpus are first mapped to a common emotion category space, followed by a standard supervised classifier training (see Section 4.2).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Nicu-A",
      "text": "The FinEst NICU Audioset (NICU-A) was collected in the AP-PLE study, and is the primary audio material for which our SER system was aimed to be deployed on. We use the Finnish subset of the dataset, which was recorded at the NICU of Turku University Hospital using LENA-recorders (https://www.lena.org/) placed at the bedside of preterm babies (average age approx. 33 gestational weeks) in intensive care. The data consists of 43 x 16-hour recordings from different participating families (a total of 688 h of audio). The recordings were carried out in relatively calm single family rooms of the NICU, where only the baby, visiting parents (primary talkers), and, occasionally, nurses and doctors carrying out healthcare routines were present.\n\nBroad-class diarization of LENA software  [20]  was used to split each 16-h recording into utterance-sized segments, and to assign a speaker tag (male, female, key child, other child) to the utterances. Based on the validity study reported for the same data in  [21] , adult speech from \"male/female adult\" \"near\" and \"far\" -categories were included in the analyses to capture caregiver speech (but see  [22]  for general guidelines with LENA \"far\" data). Utterances shorter than 600 ms were discarded from further analysis. This resulted in a total of 129,007 utterances with an average length of 1.57 s (approx. 56 h of speech).\n\nEight families were carefully selected as the test data and 35 as the training data based on the representativeness of both data sets in terms of covariates such as child health, parental presence etc. After pre-processing the data of NICU-A, both the training and test sets were partially annotated.\n\nFor the training data, samples were selected for annotation using MAL, as described in Section 4.2.1. Two annotators performed labeling for distinct subsets of the data, except for the first 200 samples that were annotated by both to measure interrater agreement rates. Each sample was annotated in two dimensions: in terms of binary arousal (high/low) and in terms of ternary valence (negative, neutral, positive). The two dimensions were annotated in a random order for each sample. A sample could also be labeled as erroneous, if the samples were corrupted by noise, had overlapping speakers, had very short speech fragments, or did not contain speech at all.\n\nFor the test data, gold standard (GS) annotations were obtained from three speech/clinical experts for a randomly selected subset of samples from the test set. All GS samples were independently annotated for their arousal and valence by all three annotators, followed by majority voting of labels. Samples without majority labels were removed from the test set. GS annotators had access to 10 s of the preceding audio context of each sample to better understand the communicative context.\n\nAfter removing the erroneous files, the training and test sets had 5198 and 345 labeled samples, respectively. Training data inter-annotator agreement rates in terms of kappa scores were 0.78 for valence and 0.64 for arousal. For the GS data, the kappa scores were 0.48 for valence and 0.28 for arousal. The difference between the training and testing agreement rates is explained due to the use of MAL in the selection of the training samples, where the first 200 samples annotated by both annotators were also the most acoustically distinct samples in the training data. The finding also demonstrates the inherent difficulty in annotating a random sample of real-world speech for emotional content.\n\nThe 'neutral' and 'negative' classes for valence were merged for NICU-A, bearing in mind that the APPLE study was primarily interested in the proportion of positive valence over other speech. As a result, training sample counts were 1509 for positive and 3689 for neutral valence, and 3165 and 2033 for high and low arousal, respectively. The corresponding test set counts were 120 (positive) and 225 (neutral) for valence, and 89 (high) and 256 (low) for arousal.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Other Corpora For Ccg And Da Experiments",
      "text": "In addition to NICU-A, four existing SER corpora (referred to as source corpora) were used in the CCG and DA experiments:\n\nThe Berlin Emotional Speech Database (EMO-DB)  [23]  is a widely used corpus and consists of 535 spoken utterances in German from 10 professional actors with seven emotional labels: anger, boredom, disgust, fear, joy, neutral, and sadness.\n\neNTERFACE  [24]  is an audiovisual database consisting of 1287 video samples in English from 42 test subjects from 14 nationalities in six categories: anger, disgust, fear, joy, sadness, and surprise. Only the audio tracks were used in this study.\n\nThe Finnish Emotional Speech Corpus (FESC)  [25]  consists of nine professional actors portraying emotions of five different categories: neutral, sadness, joy, anger, and tenderness. These portrayals were split into 4254 utterances based on long silences as defined by an energy threshold  [26] .\n\nThe Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [27]  is a multimodal database including a total of 7356 recordings from 24 professional actors, out of which 1440 speech-only recordings were used in the present study. Eight different emotional labels were included: neutral, calm, happy, sad, angry, fearful, surprise, and disgust.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4. Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4.1. Features",
      "text": "Log-mel, GeMAPS, and eGeMAPS  [28]  features were used in the CCG and AL experiments. For the DA experiments, only log-mel features were used due to their superior performance in pilot experiments. For the log-mel features, 40 mel filters were used with a Hann window using a 30-ms window size and 10ms shifts. To get constant-dimensional utterance feature representations, seven functionals (the first four moments, min, max, and range) were taken from the time series of the log-mel features. In addition, four functionals (the first four moments) were applied to first and second order delta features. This resulted in a 600-dimensional feature vector for the log-mel features. The 62-and 88-dimensional GeMAPS and eGeMAPS features were extracted using the openSMILE toolkit  [29] . The features for each corpus were z-score normalized at the corpus level.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conducted Experiments",
      "text": "For the source corpora, the emotional labels were mapped into the quarters of the valence-arousal plane following  [4] , with the exception of merging 'neutral' and 'negative' valence to 'neutral' in order to better correspond to the labels of NICU-A. The emotional mapping of  [4]  has been used in multiple SER studies (e.g.  [8, 10, 30, 31] ). All classification tests were conducted on the NICU-A GS data. We use the unweighted average recall (UAR %) as the primary evaluation measure.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Active Learning Experiments",
      "text": "In the AL experiments, MAL was performed for the full unlabeled training set of NICU-A (101,813 samples). To compress the log-mel features of the training set into a latent representation, an AE network was used. The training and validation data for the AE were based on a random split of the training set using a ratio of 80:20 utterances. The encoder of the AE consisted of three fully-connected (FC) ELU  [32]  layers of 512, 512, and 32 units, and the decoder of two 512-unit ELU layers and a linear reconstruction layer. The first two AE layers had a dropout of 0.1. The model was trained using MSE loss, Adam  [33]  optimizer (lr = 10 -4 ), batch size of 1024, and early stopping with a patience of 300. The best model according to the validation loss was then used to compress the data to 32 dimensions. Then, MAL was performed for each of the 35 training set families separately and the data were sent for annotation (Section 3.1).\n\nThe annotated samples were then used for training a support vector machine (SVM) with an RBF kernel. Each sample was weighted inversely proportional to its class frequency to counter class distribution imbalances. Optimal SVM hyperparameters were selected for each feature type and both classification tasks individually based on a grid search using 5-fold crossvalidation over the training data. Then, the SVM was trained on the full training data using these hyperparameters and tested on the GS data. The process was performed separately for the labeled training set of 5,198 samples and for the extended training set of 33,979 samples using the cluster labels from MAL.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Corpus Generalization Experiments",
      "text": "For the CCG experiments, two settings were explored: 1-to-1 and 4-to-1 CCG. In the 1-to-1 setting, each of the source corpora was used individually as the training set. In the 4-to-1 setting, all four source corpora were used for SVM training with similar specifications as with the AL experiments.\n\nTable  1 : UAR (%) performance scores for alternative approaches on the target data. For AL and CCG, log-mel (logm), GeMAPS (Ge), and eGeMAPS (eGe) features are compared. For DA, the unsupervised (US) and semi-supervised (S-S) variant of WDA is compared. The highest accuracies are bolded.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ccg Training Corpus Log-M Ge Ege Log-M Ge Ege",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Domain Adaptation Experiments",
      "text": "For the DA-based experiments, 1-to-1 and 4-to-1 adaptation conditions were examined with the same source corpora as in CCG. All DA experiments were conducted separately for valence and arousal. In the 1-to-1 settings, each source corpus was randomly split into a training and test set in a ratio of 85:15. For the 4-to-1 setting, the training and test sets were the combination of the respective corpus-specific splits. For the first stage of the adaptation process, the training set of each source corpus was used to train M by using the Adam optimizer (lr = 10 -4 ), early stopping with a patience of 100 based on test set accuracy, and batch size of 256. The log-mel features were used as the input features for F , consisting of three FC layers of 512, 512, and 256 units, each followed by batch normalization. The first two layers had LReLU  [34]  nonlinearities and a dropout of 0.4. CL was an NN consisting of three FC layers of 256, 256, and 2 units. The first two layers had LReLU nonlinearities and a dropout of 0.3. The last layer was followed by a softmax function. For each variant of the source data, a separate M was trained for both valence and arousal.\n\nFor the second stage of the adaptation process, the full unlabeled data from the source corpus/corpora and the unlabeled training samples of NICU-A were used for training. Following  [18] , the unsupervised variant of WDA was trained until the first term in Eq. 4 was saturated. For the semi-supervised variant, the labeled training set of NICU-A was used to determine the model accuracy after each epoch, and the model with the highest accuracy was selected for testing. This set was also used to find optimal hyperparameters. CD consisted of four FC layers of 512, 512, 256, and 1 units. The first three layers were followed by ReLU nonlinearities. The parameters of CD and FT were updated with the RMSProp  [35]  and Adam optimizers, respectively. In the 1-to-1 settings, lr = 5 • 10 -5 was used, except with FESC for valence and with RAVDESS for arousal, where lr = 7 • 10 -5 . For the 4-to-1 settings, lr = 7 • 10 -5 was used for valence and lr = 6 • 10 -5 for arousal. The performance of the adapted model was then tested on the GS data.\n\nAll the DA and AL parameters were based on extensive piloting with leave-one-corpus-out simulations using the source corpora, and before any NICU-A data had been labeled.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "The main results are presented in   1 , bottom) are on average higher than the results of CCG, even though the WDA method does not provide a major improvement over CCG on valence. The semi-supervised variant of WDA is also consistently better than the unsupervised variant. The comparison of using either cluster or medoid labels for AL provides somewhat mixed results, depending on the exact condition.\n\nIn terms of features, the GeMAPS and eGeMAPS feature sets outperformed the log-mel features on valence with CCG. For CCG and arousal, the best-performing features varied largely between different training corpora, and the matching Finnish language FESC is a substantially better source for NICU-A than the others, reaching 70.8% UAR with eGeMAPS features. In the AL experiments (Table  1 , top), the eGeMAPS and GeMAPS features achieved the best mean classification accuracy for valence and arousal, respectively.\n\nThe confusion matrices for the best-performing models (Fig.  2 ) indicate that these models do not systematically favor one label over the other when performing predictions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In the present paper, we developed a SER system for largescale analysis of emotional content of speech in initially unannotated real-life child-centered audio recordings from a NICU. CCG, AL, and DA were compared as alternatives for deploying a SER system for this novel dataset from scratch. Our results show that WGAN-based DA outperformed the baseline CCG approach, verifying its usefulness in the absence of any data labels. However, with a very moderate human labeling resource available, k-medoids based AL was superior compared to CCG and DA in valence classification and relatively competitive for arousal as well. However, when classifying arousal, DA resulted in slightly better results than AL. Overall, the results demonstrate that the earlier proposed MAL  [15]  and WDA  [18]  methods are also applicable to practical SER scenarios. The results also show that emotion analysis for LENA-based daylong audio recordings is possible with an accuracy comparable to those reported in earlier literature (e.g., 58.1% for valence and 66.8% for arousal across the multi-corpus tests in  [31] ).",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The ﬁrst stage (Fig. 1, top) consists of training M using",
      "page": 2
    },
    {
      "caption": "Figure 1: , bottom), FS is adapted to DT",
      "page": 2
    },
    {
      "caption": "Figure 1: The two-step the adaptation process of WDA. First,",
      "page": 2
    },
    {
      "caption": "Figure 2: Normalized confusion matrices for valence (left)",
      "page": 4
    },
    {
      "caption": "Figure 2: ) indicate that these models do not systematically favor",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Predicted\nStep 1\nCL\nlabel\nXS\nFS\nYS": "Step 2\nXS\nPredicted\nFT\nCD\ndomain\nXT"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: UAR (%) performance scores for alternative ap- el VVVaaallleeennnccceee el Arousal",
      "data": [
        {
          "CCG": "",
          "Training corpus \nlog-m  Ge \neGe \nlog-m  Ge \neGe": "EMO-DB \n48.5 \n53.8 \n53.4 \n64.1 \n63.7 \n62.7"
        },
        {
          "CCG": "",
          "Training corpus \nlog-m  Ge \neGe \nlog-m  Ge \neGe": "eNTERFACE \n56.8 \n52.7 \n50.2 \n63.1 \n64.3 \n64.1"
        },
        {
          "CCG": "",
          "Training corpus \nlog-m  Ge \neGe \nlog-m  Ge \neGe": "FESC \n45.3 \n57.3 \n54.9 \n56.3 \n68.3 \n70.8"
        },
        {
          "CCG": "",
          "Training corpus \nlog-m  Ge \neGe \nlog-m  Ge \neGe": "RAVDESS \n50.4 \n53.8 \n53.3 \n64.3 \n62.0 \n58.7"
        },
        {
          "CCG": "",
          "Training corpus \nlog-m  Ge \neGe \nlog-m  Ge \neGe": "All source corpora  42.9 \n54.9 \n56.8 \n61.3 \n64.4 \n65.5"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: UAR (%) performance scores for alternative ap- el VVVaaallleeennnccceee el Arousal",
      "data": [
        {
          "DA": "",
          "Source corpus \nUS \nS-S \nUS \nS-S": "EMO-DB \n49.7 \n51.3 \n71.0 \n73.2"
        },
        {
          "DA": "",
          "Source corpus \nUS \nS-S \nUS \nS-S": "eNTERFACE \n57.0 \n58.0 \n67.2 \n68.6"
        },
        {
          "DA": "",
          "Source corpus \nUS \nS-S \nUS \nS-S": "FESC \n46.9 \n47.4 \n61.5 \n63.1"
        },
        {
          "DA": "",
          "Source corpus \nUS \nS-S \nUS \nS-S": "RAVDESS \n57.1 \n57.7 \n66.5 \n68.4"
        },
        {
          "DA": "",
          "Source corpus \nUS \nS-S \nUS \nS-S": "All source corpora \n53.2 \n53.5 \n71.0 \n71.3"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
      "authors": [
        "A Batliner",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "The Automatic Recognition of Emotions in Speech",
      "authors": [
        "A Batliner",
        "B Schuller",
        "D Seppi",
        "S Steidl",
        "L Devillers",
        "L Vidrascu",
        "T Vogt",
        "V Aharonson",
        "N Amir"
      ],
      "year": "2010",
      "venue": "Emotion-Oriented Systems"
    },
    {
      "citation_id": "4",
      "title": "The validity of the Language Environment Analysis system in two neonatal intensive care units",
      "authors": [
        "E Ståhlberg-Forsen",
        "A Aija",
        "B Kaasik",
        "R Latva",
        "S Ahlqvist-L. Toome",
        "L Lehtonen",
        "S Stolt"
      ],
      "year": "2021",
      "venue": "Acta Paediatrica"
    },
    {
      "citation_id": "5",
      "title": "Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wöllmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "f-Similarity Preservation Loss for Soft Labels: A Demonstration on Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "B Zhang",
        "Y Kong",
        "G Essl",
        "E Provost"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Semisupervised Autoencoders for Speech Emotion Recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Domain Adversarial for Acoustic Emotion Recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Trans. Audio, Speech and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Cross lingual speech emotion recognition using canonical correlation analysis on principal component subspace",
      "authors": [
        "H Sagha",
        "J Deng",
        "M Gavryukova",
        "J Han",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Universum Autoencoder-Based Domain Adaptation for Speech Emotion Recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised Adversarial Domain Adaptation for Cross-Lingual Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "12",
      "title": "Active Learning for Speech Emotion Recognition Using Conditional Random Fields",
      "authors": [
        "Z Zhao",
        "X Ma"
      ],
      "year": "2013",
      "venue": "2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing"
    },
    {
      "citation_id": "13",
      "title": "Active Learning for Speech Emotion Recognition Using Deep Neural Network",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "14",
      "title": "Inferring Emotions From Large-Scale Internet Voice Data",
      "authors": [
        "J Jia",
        "S Zhou",
        "Y Yin",
        "B Wu",
        "W Chen",
        "F Meng",
        "Y Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "LSSED: a large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "LSSED: a large-scale dataset and benchmark for speech emotion recognition",
      "arxiv": "arXiv:2102.01754"
    },
    {
      "citation_id": "16",
      "title": "Active learning for sound event classification by clustering unlabeled data",
      "authors": [
        "Z Shuyang",
        "T Heittola",
        "T Virtanen"
      ],
      "year": "2017",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Minimum Pearson Distance Detection for Multilevel Channels With Gain and/or Offset Mismatch",
      "authors": [
        "K Immink",
        "J Weber"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "18",
      "title": "A Simple and Fast Algorithm for K-Medoids Clustering",
      "authors": [
        "H.-S Park",
        "C.-H Jun"
      ],
      "year": "2009",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised Adversarial Domain Adaptation Based on The Wasserstein Distance For Acoustic Scene Classification",
      "authors": [
        "K Drossos",
        "P Magron",
        "T Virtanen"
      ],
      "year": "2019",
      "venue": "2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)"
    },
    {
      "citation_id": "20",
      "title": "Wasserstein Generative Adversarial Networks",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "21",
      "title": "Signal processing for young child speech language development",
      "authors": [
        "D Xu",
        "U Yapanel",
        "S Gray",
        "J Gilkerson",
        "J Richards",
        "J Hansen"
      ],
      "year": "2008",
      "venue": "Proc. 1st Workshop on Child, Computer, and Interaction"
    },
    {
      "citation_id": "22",
      "title": "Language Environment Analysis (LENA)menetelmän validiteetti keskosvauvojen ääniympäristön arvioinnissa",
      "authors": [
        "K Siirilä"
      ],
      "year": "2019",
      "venue": "Language Environment Analysis (LENA)menetelmän validiteetti keskosvauvojen ääniympäristön arvioinnissa"
    },
    {
      "citation_id": "23",
      "title": "A thorough evaluation of the Language Environment Analysis (LENA) system",
      "authors": [
        "A Cristia",
        "M Lavechin",
        "C Scaff",
        "M Soderstrom",
        "C Rowland",
        "O Räsänen",
        "J Bunce",
        "E Bergelson"
      ],
      "year": "2020",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "24",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "9th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "25",
      "title": "The eNTERFACE' 05 Audio-Visual Emotion Database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "26",
      "title": "Emotions in Vowel Segments of Continuous Speech: Analysis of the Glottal Flow Using the Normalised Amplitude Quotient",
      "authors": [
        "M Airas",
        "P Alku"
      ],
      "year": "2006",
      "venue": "Phonetica"
    },
    {
      "citation_id": "27",
      "title": "Automatic Emotional Speech Analysis from Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit",
      "authors": [
        "E Vaaras"
      ],
      "year": "2021",
      "venue": "Automatic Emotional Speech Analysis from Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit"
    },
    {
      "citation_id": "28",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "29",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Recent developments in openSMILE, the Munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Gross",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "MM 2013 -Proceedings of the 2013 ACM Multimedia Conference"
    },
    {
      "citation_id": "31",
      "title": "Using Multiple Databases for Training in Emotion Recognition: To Unite or to Vote?",
      "authors": [
        "B Schuller",
        "Z Zhang",
        "F Weninger",
        "G Rigoll"
      ],
      "year": "2011",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "32",
      "title": "Unsupervised learning in cross-corpus acoustic emotion recognition",
      "authors": [
        "Z Zhang",
        "F Weninger",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE Workshop on Automatic Speech Recognition Understanding"
    },
    {
      "citation_id": "33",
      "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
      "authors": [
        "D.-A Clevert",
        "T Unterthiner",
        "S Hochreiter"
      ],
      "year": "2016",
      "venue": "4th International Conference on Learning Representations"
    },
    {
      "citation_id": "34",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "35",
      "title": "Rectifier Nonlinearities Improve Neural Network Acoustic Models",
      "authors": [
        "A Maas",
        "A Hannun",
        "A Ng"
      ],
      "year": "2013",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "36",
      "title": "Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude",
      "authors": [
        "T Tieleman",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "COURS-ERA: Neural Networks for Machine Learning"
    }
  ]
}