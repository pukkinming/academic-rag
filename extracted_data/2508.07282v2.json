{
  "paper_id": "2508.07282v2",
  "title": "Lessons Learnt: Revisit Key Training Strategies For Effective Speech Emotion Recognition In The Wild",
  "published": "2025-08-10T10:42:49Z",
  "authors": [
    "Jing-Tong Tzeng",
    "Bo-Hao Su",
    "Ya-Tse Wu",
    "Hsing-Hang Chou",
    "Chi-Chun Lee"
  ],
  "keywords": [
    "speech emotion recognition",
    "human-computer interaction",
    "computational paralinguistics",
    "valence recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this study, we revisit key training strategies in machine learning often overlooked in favor of deeper architectures. Specifically, we explore balancing strategies, activation functions, and fine-tuning techniques to enhance speech emotion recognition (SER) in naturalistic conditions. Our findings show that simple modifications improve generalization with minimal architectural changes. Our multi-modal fusion model, integrating these optimizations, achieves a valence CCC of 0.6953, the best valence score in Task 2: Emotional Attribute Regression. Notably, fine-tuning RoBERTa and WavLM separately in a single-modality setting, followed by feature fusion without training the backbone extractor, yields the highest valence performance. Additionally, focal loss and activation functions significantly enhance performance without increasing complexity. These results suggest that refining core components, rather than deepening models, leads to more robust SER in-the-wild.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the advancement of affective computing, Speech Emotion Recognition (SER) has gained significant attention due to its potential in human-computer interaction (HCI)  [1, 2, 3] . As SER continues to be integrated into real-world applications, including call center analytics  [4, 5] , healthcare monitoring  [6, 7] , and digital assistants  [8] , ensuring its robustness in unconstrained environments is crucial. To improve SER's applicability in these settings, researchers have increasingly focused on \"in-thewild\" dataset collection, where speech emotion data is captured under real-world conditions rather than controlled laboratory environments  [9, 10, 11] . However, these datasets introduce significant modeling challenges, as spontaneous emotional expressions vary across speakers, acoustic conditions, and contextual factors. A promising approach to addressing these challenges is self-supervised learning (SSL), which enables models to extract robust speech representations from large-scale unlabeled data, thereby enhancing generalization across diverse conditions  [12, 13, 14] .\n\nDespite these advancements, audio-only SSL features exhibit inherent limitations, particularly in capturing the valence dimension, which is often conveyed more effectively through textual and contextual information rather than acoustic cues alone  [15, 16, 17] . This limitation has driven recent research toward multi-modal approaches, where SSL-derived textual features are integrated with speech signals to develop dualmodality emotion recognition systems  [18, 19, 20] . However, key architectural aspects remain underexplored, including the impact of activation function choices in the fusion stage and whether fine-tuning single-modality SSL models contributes to improving overall system performance. Addressing these gaps is crucial for optimizing multi-modal SER frameworks and ensuring their robustness in real-world applications.\n\nIn this work, we revisit key training strategies for SER systems, with a particular focus on the fusion stage of dualmodality emotion recognition. Our contributions are as follows:\n\n1. The effect of fine-tuning SSL features on individual modalities in multi-modality settings. 2. A comparison of activation functions.\n\n3. An investigation into the impact of textual features between SSL and large language models (LLMs).\n\nTo this end, we fine-tune WavLM and RoBERTa on their respective modalities and optimize fusion strategies, both guided by insights gained from our analysis. As a result, our model achieves the best concordance correlation coefficient (CCC) of 0.6953 on valence in the Speech Emotion Recognition in Naturalistic Conditions Challenge.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "It is widely recognized that arousal and dominance are more speech-dependent, whereas valence relies more heavily on textual information  [16, 17] . As a result, many studies have explored dual-modality fusion between acoustic and textual features to improve SER. For instance, Macary et al. demonstrate that combining self-supervised models such as wav2vec  [21]  and camemBERT  [22]  enhances continuous speech emotion recognition  [19] . Similarly, Chu et al. propose a deeply fused audio-text transformer with stage-wise cross-modal pretraining, improving both SER and sentiment analysis  [20] .\n\nA key observation across these studies is the critical role of SSL features; however, approaches differ on whether foundation models require fine-tuning for emotion recognition and whether fine-tuning is necessary during fusion. Given the \"inthe-wild\" nature of our dataset, this work investigates modalityspecific fine-tuning and examines the impact of activation functions in the fusion layer, contributing to a deeper understanding of optimal fusion strategies for SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "This study proposes a two-stage dual-modality framework for emotion recognition, where speech and text-based features are independently fine-tuned in the first stage and subsequently fused in the second stage. The complete framework is illustrated in Figure  1 .  In order to capture both the acoustic and semantic elements of emotion, we utilized both speech and text foundation features. These components are extracted through specific feature extractors designed for each modality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Foundation Extractor",
      "text": "Through the speech modality, acoustic features are extracted using WavLM  [23] , a self-supervised learning model designed for raw audio processing. WavLM provides robust representations of prosodic and non-verbal characteristics, and has demonstrated strong performance in SER tasks within the Speech Processing Universal Performance Benchmark (SUPERB)  [24] . Additionally, as WavLM is pre-trained on noisy speech, it is well-suited for capturing robust acoustic representations that generalize effectively to real-world scenarios.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text Foundation Extractor",
      "text": "To extract semantic features from the transcribed speech, we use RoBERTa  [25] , a transformer-based model built on BERT architecture. RoBERTa excels at capturing deep contextual relationships in text, enabling it to understand word dependencies, sentence structure, and emotional nuances present in the speech content. Furthermore, it has also shown promising results in both single-modality and multi-modality SER tasks  [26, 27] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Strategy",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Two-Stage Scheme",
      "text": "The training strategy is divided into two distinct stages to optimize the performance of both the individual modality extractors and their integration. In the first stage, audio-based and text-based foundation extractors are fine-tuned independently for emotion recognition, enabling each modality to learn domain-specific features from its respective input. To maintain a consistent feature size, Attentive Statistical Pooling  [28]  is applied in the speech modality, while mean pooling is used in the text modality for training the classification head. This stage establishes a robust foundation for the subsequent fusion of both modalities.\n\nIn the second stage, features for both the audio and text modalities are fused for training on the emotion recognition task. The fusion layer combines the pre-trained features from stage 1, allowing the model to learn the optimal integration of these modalities for downstream emotion recognition. The focus in this stage is to effectively combine the rich, complementary information provided by both modalities to improve the overall accuracy and robustness of emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fusion Strategies",
      "text": "To integrate the dual-modality features, the model includes a fusion layer followed by two fully connected layers: one that preserves input dimensionality and another for the downstream emotion recognition task. The fusion layer uses a simple concatenation method to combine both feature sets into a single vector. We adopt Mish activation  [25] , which has recently gained attention in speech synthesis applications  [29, 30] , in place of the commonly used ReLU in emotion recognition tasks. Unlike ReLU, Mish is a smooth, continuously differentiable activation function that improves gradient flow, prevents neuron saturation, and enhances feature representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Sample Balancing Control",
      "text": "In large in-the-wild datasets, class imbalance often poses a significant challenge for effective model training. To address this issue, we implement weighted cross-entropy (WCE) loss and compare it to balanced sampling and focal loss  [31]  strategies. WCE loss assigns higher weights to minority classes, ensuring that their contributions to the overall loss function are amplified during training. The balanced sampling strategy mitigates imbalance by over-sampling minority classes, ensuring that each batch contains an equal number of samples per class. In contrast, focal loss prioritizes underrepresented classes by downweighting the loss of well-classified samples, placing greater emphasis on harder-to-classify instances. This approach helps the model focus on minority classes, improving recognition accuracy for less frequent emotions and enhancing overall model robustness.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Preparation",
      "text": "We conduct all experiments using the Speech Emotion Recognition in Naturalistic Conditions Challenge dataset  [11] , which To ensure a fair comparison, we strictly follow the same dataset partitioning as the baseline model. Throughout this manuscript, we use 'A', 'C', 'D', 'F', 'H', 'N', 'S', and 'U' to represent anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Settings",
      "text": "We carry out the two-stage training strategy on both multimodal SER tasks (categorical emotions and emotional attributes). In stage 1, we fine-tuned RoBERTa and WavLM separately on the MSP-Podcast dataset using a batch size of 32 and a learning rate of 10 -5 for 20 epochs. In stage 2, we froze both the text and acoustic feature extractors trained in the stage 1, fused their embeddings, and trained the classification head for 5 epochs with a learning rate of 5 × 10 -6 and a batch size of 32 1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "In this section, we revisit several key training strategies in the machine learning pipeline, illustrated in Figure  2 , within the context of large-scale, naturalistic SER in the wild. Through the following subsections, we address several critical questions and challenges encountered during our participation in the competition, offering insights from multiple perspectives.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Are Fusion Mechanisms Consistently Effective Across Different Emotion Representations?",
      "text": "By using concatenation fusion with fine-tuned speech and text foundation extractors and Mish activation, our model achieves the best valence performance. To further evaluate its effective- We hypothesize that this degradation stems from overfitting during fine-tuning, where RoBERTa learns features highly correlated with valence, limiting its generalization to other emotional attributes and inflating valence scores in dimensional predictions. Nevertheless, these findings reinforce that simple concatenation with Mish activation is not only effective but also more robust in handling the complexities of naturalistic SERin-the-wild scenarios.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "How Balancing Strategies Affect Speech Emotion Recognition?",
      "text": "Given the highly imbalanced distribution in speech emotion classification, we evaluate the effectiveness of three balancing strategies under naturalistic SER conditions: WCE, balanced sampling, and focal loss. For balanced sampling, we implement a custom dataloader that ensures each emotion category is sampled in equal proportions within each training batch.\n\nTable  2  shows that focal loss achieves the best overall performance, improving F1-Micro by 10.2% compared to weighted cross-entropy loss. This improvement aligns with expectations, as the gamma parameter in focal loss acts as a modulating factor, reducing the influence of well-classified samples and emphasizing harder-to-classify and minority categories. Unlike weighted sampling, which implicitly up-samples minority classes but lacks variability, focal loss dynamically adjusts the learning focus, ensuring better feature discrimination. Furthermore, our results indicate that focal loss preserves accuracy for the majority classes while significantly enhancing recognition of minority emotions. This balance makes it particularly effective for highly imbalanced SER tasks, especially in in-the-wild settings where class distributions are inherently skewed. These findings suggest that focal loss provides a more adaptive and robust solution for emotion recognition under realworld conditions. In this experiment, we compare the effectiveness of the LLMs and the SSL for SER in the text modality. Specifically, we compare Llama-3.2  [32]  with RoBERTa-large to assess their performance in both categorical emotion classification and emotional attribute regression, as shown in Table  3 .\n\nFor RoBERTa-large, we extract embeddings from the pretrained model and pass them through two fully connected layers for classification and regression. In contrast, we use Llama-3.2 in a zero-shot setting with a carefully designed prompt, as elaborated below, for both tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "• Categorical Emotion Prompts:",
      "text": "Predict the emotion label of the following sentence from a podcast recording. Allowed predicted emotions: ['Anger', 'Contempt', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise'] Transcription: < transcript > Just predict the answer without explanation. Answer:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "• Emotional Attributes Prompts:",
      "text": "Predict the emotional attribute label (valence, arousal, dominance) of the following sentence from a podcast recording. Allowed predicted ranges are from 1 to 7. Transcription:< transcript > Just predict the answer in the format of [arousal, valence, dominance], e.g., [1.0, 2.3, 4.7], without explanation. Answer:\n\nOur results indicate that RoBERTa-large consistently outperforms Llama-3.2, particularly in regression tasks, highlighting RoBERTa's robustness and its ability to learn precise distinctions when fine-tuned for downstream tasks. While Llama-3.2 achieves reasonable performance in classification, its regression results are significantly worse. We hypothesize that while LLMs excel in categorical emotion classification, they struggle with predicting precise emotional attributes, as this requires a deeper understanding of nuanced emotional variations. Without fine-tuning or in-context demonstrations, they have difficulty inferring exact values within a restricted range. These findings suggest that for SER in the text modality, RoBERTalarge remains a more suitable choice than direct inference using a generative LLM, particularly for fine-grained regression tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Quantitative Analysis",
      "text": "To further understand the improvements in valence prediction comprehensively, we first carry out an analysis of the statistical Furthermore, we compare samples where our best model outperforms the baseline, categorized by discrete emotions. Figure  3  illustrates the emotion distribution among these samples. Since valence prediction is a regression task, we compute the MSE loss for both models against the ground truth and filter samples based on lower MSE values (indicating better predictions). Our analysis reveals that our best approach achieves notable improvements in recognizing surprise (4.5% vs. 3.8%) and contempt (4.2% vs. 3.8%), two emotions that are typically more challenging to classify, apart from fear and disgust. This suggests that our model effectively leverages text-based features to enhance valence prediction.\n\nLastly, to quantitatively validate our model's superiority, we segment the samples into three equal-range bins based on the ground truth: [1, 3),  [3, 5) , and  [5, 7) . The CCC scores of each partition are reported in Table  4 . Notably, our model outperforms the baseline most significantly in the [3, 5) range (0.449 vs. 0.395), suggesting that it provides more accurate predictions in this middle interval, thereby ensuring a smoother distribution across the entire valence spectrum.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "Through our experiments and dual-modality fusion architecture, we achieved the best performance on Test3, obtaining a valence CCC of 0.6953 (Table  5 ). Our findings suggest that focal loss and activation functions play a crucial role, particularly in valence regression, even without employing excessively deep or complex models. Additionally, we observe that the concatenation fusion strategy with Mish activation consistently enhances performance across both categorical emotion classification and emotional attribute regression. Furthermore, while LLMs excel in various domains, our results indicate that without fine-tuning or in-context learning, their performance in SER-in-the-wild remains inferior to traditional, robust text models. However, our model exhibits limited improvements in categorical emotion, arousal, and dominance prediction, suggesting that a more generalized SER framework remains an open challenge. Future work should focus on a deeper analysis and more comprehensive modeling approaches to improve SER performance in naturalistic, real-world conditions.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: arXiv:2508.07282v2  [eess.AS]  25 Sep 2025",
      "page": 1
    },
    {
      "caption": "Figure 1: Our framework of the dual-modality (acoustic and textual) SER system",
      "page": 2
    },
    {
      "caption": "Figure 2: Common Machine Learning Pipeline",
      "page": 2
    },
    {
      "caption": "Figure 2: , within the",
      "page": 3
    },
    {
      "caption": "Figure 3: Pie chart of emotion categories",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates the emotion distribution among these sam-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: shows that focal loss achieves the best over-",
      "data": [
        {
          "Speech modality fine-tune": "Stage 1",
          "Text modality fine-tune": "Stage 1",
          "Categorical": "F1-Macro\nF1-Micro\nAcc.",
          "CCC": "Val.\nAro.\nDom."
        },
        {
          "Speech modality fine-tune": "✓\n✗\n✓\n✓\n✓",
          "Text modality fine-tune": "N/A\n✗\n✗\n✗\n✓",
          "Categorical": "0.330\n0.475\n0.475\n0.340\n0.551\n0.550\n0.345\n0.546\n0.546\n0.355\n0.605\n0.605\n0.286\n0.493\n0.493",
          "CCC": "0.604\n0.653\n0.670\n0.619\n0.620\n0.505\n0.626\n0.634\n0.556\n0.683\n0.668\n0.597\n0.682\n0.674\n0.588"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "3",
      "title": "Humancomputer interaction for recognizing speech emotions using multilayer perceptron classifier",
      "authors": [
        "A Alnuaim",
        "M Zakariah",
        "P Shukla",
        "A Alhadlaq",
        "W Hatamleh",
        "H Tarazi",
        "R Sureshbabu",
        "R Ratna"
      ],
      "year": "2022",
      "venue": "Journal of Healthcare Engineering"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "I Emary"
      ],
      "year": "2013",
      "venue": "Telecommunication Systems"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition on call center voice data",
      "authors": [
        "Y Yurtay",
        "H Demirci",
        "H Tiryaki",
        "T Altun"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "6",
      "title": "Call redistribution for a call center based on speech emotion recognition",
      "authors": [
        "M Bojanić",
        "V Delić",
        "A Karpov"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "7",
      "title": "A comprehensive analysis of speech recognition systems in healthcare: Current research challenges and future prospects",
      "authors": [
        "Y Kumar"
      ],
      "year": "2024",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "8",
      "title": "Empathetic speech synthesis and testing for healthcare robots",
      "authors": [
        "J James",
        "B Balamurali",
        "C Watson",
        "B Macdonald"
      ],
      "year": "2021",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "9",
      "title": "Real-time speech emotion analysis for smart home assistants",
      "authors": [
        "R Chatterjee",
        "S Mazumdar",
        "R Sherratt",
        "R Halder",
        "T Maitra",
        "D Giri"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "10",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Lssed: A largescale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "The interspeech 2025 challenge on speech emotion recognition in naturalistic conditions",
      "authors": [
        "A Reddy Naini",
        "L Goncalves",
        "A Salman",
        "P Mote",
        "I Ülgen",
        "T Thebaud",
        "L Velazquez",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2025",
      "venue": "Interspeech 2025"
    },
    {
      "citation_id": "13",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "M Ravanelli",
        "J Zhong",
        "S Pascual",
        "P Swietojanski",
        "J Monteiro",
        "J Trmal",
        "Y Bengio"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Speechbased emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing",
      "authors": [
        "S Kakouros",
        "T Stafylakis",
        "L Mošner",
        "L Burget"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised personalization of an emotion recognition system: The unique properties of the externalization of valence in speech",
      "authors": [
        "K Sridhar",
        "C Busso"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition with asr transcripts: a comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "19",
      "title": "Leveraging semantic information for efficient self-supervised emotion recognition with audio-textual distilled models",
      "authors": [
        "D Oliveira",
        "N Prabhu",
        "T Gerkmann"
      ],
      "venue": "Leveraging semantic information for efficient self-supervised emotion recognition with audio-textual distilled models"
    },
    {
      "citation_id": "20",
      "title": "On the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "21",
      "title": "Selfsupervised cross-modal pretraining for speech emotion recognition and sentiment analysis",
      "authors": [
        "I.-H Chu",
        "Z Chen",
        "X Yu",
        "M Han",
        "J Xiao",
        "P Chang"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022"
    },
    {
      "citation_id": "22",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "23",
      "title": "Camembert: a tasty french language model",
      "authors": [
        "L Martin",
        "B Muller",
        "P Suárez",
        "Y Dupont",
        "L Romary",
        "É De La Clergerie",
        "D Seddah",
        "B Sagot"
      ],
      "year": "2020",
      "venue": "ACL 2020-58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong",
        "S.-W Li",
        "S Watanabe",
        "A Mohamed",
        "H Yi Lee"
      ],
      "venue": "Superb: Speech processing universal performance benchmark"
    },
    {
      "citation_id": "26",
      "title": "Mish: A self regularized non-monotonic activation function",
      "authors": [
        "D Misra"
      ],
      "year": "2019",
      "venue": "Mish: A self regularized non-monotonic activation function",
      "arxiv": "arXiv:1908.08681"
    },
    {
      "citation_id": "27",
      "title": "Comparative analyses of bert, roberta, distilbert, and xlnet for text-based emotion recognition",
      "authors": [
        "A Adoma",
        "N.-M Henry",
        "W Chen"
      ],
      "year": "2020",
      "venue": "2020 17th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)"
    },
    {
      "citation_id": "28",
      "title": "Robinnet: A multimodal speech emotion recognition system with speaker recognition for social interactions",
      "authors": [
        "Y Khurana",
        "S Gupta",
        "R Sathyaraj",
        "S Raja"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "29",
      "title": "Attentive statistics pooling for deep speaker embedding",
      "authors": [
        "K Okabe",
        "T Koshinaka",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Attentive statistics pooling for deep speaker embedding"
    },
    {
      "citation_id": "30",
      "title": "Sc-glowtts: An efficient zero-shot multi-speaker text-to-speech model",
      "authors": [
        "E Casanova",
        "C Shulby",
        "E Gölge",
        "N Müller",
        "F De Oliveira",
        "A Candido",
        "A Da Silva Soares",
        "S Aluisio",
        "M Ponti"
      ],
      "venue": "Sc-glowtts: An efficient zero-shot multi-speaker text-to-speech model"
    },
    {
      "citation_id": "31",
      "title": "Meta-stylespeech: Multi-speaker adaptive text-to-speech generation",
      "authors": [
        "D Min",
        "D Lee",
        "E Yang",
        "S Hwang"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "32",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollar"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "33",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Dubey",
        "A Jauhri",
        "A Pandey",
        "A Kadian",
        "A Al-Dahle",
        "A Letman",
        "A Mathur",
        "A Schelten",
        "A Yang",
        "A Fan"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    }
  ]
}