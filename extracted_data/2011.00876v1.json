{
  "paper_id": "2011.00876v1",
  "title": "Multimodal Continuous Emotion Recognition Using Deep Multi-Task Learning With Correlation Loss",
  "published": "2020-11-02T10:30:22Z",
  "authors": [
    "Berkay Köprü",
    "Engin Erzin"
  ],
  "keywords": [
    "speech recognition",
    "human-computer interaction",
    "computational paralinguistics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this study, we focus on continuous emotion recognition using body motion and speech signals to estimate Activation, Valence, and Dominance (AVD) attributes. Semi-End-To-End network architecture is proposed where both extracted features and raw signals are fed, and this network is trained using multi-task learning (MTL) rather than the state-of-theart single task learning (STL). Furthermore, correlation losses, Concordance Correlation Coefficient (CCC) and Pearson Correlation Coefficient (PCC), are used as an optimization objective during the training. Experiments are conducted on CreativeIT and RECOLA database, and evaluations are performed using the CCC metric. To highlight the effect of MTL, correlation losses and multi-modality, we respectively compare the performance of MTL against STL, CCC loss against root mean square error (MSE) loss and, PCC loss, multi-modality against single modality. We observe significant performance improvements with MTL training over STL, especially for estimation of the valence. Furthermore, the CCC loss achieves more than 7% CCC improvements on CreativeIT, and 13% improvements on RECOLA against MSE loss.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human beings excite emotions in response to external and internal events of major significance to itself. The emission of emotions, also known as excitations, are represented by a 3-dimensional continuous space, and emotions like happiness and sadness correponds to regions in this space. The coordinates are Activation, Valence and Dominance (AVD) indicating activeness-passiveness, positiveness-negativeness, and dominance-submissiveness respectively  [1] ,  [2] . Emotions are episodes of coordinate changes in several components including neuro-physiologic activation, motor expression and subjective feeling but possibly also action tendencies and cognitive processes  [3] .\n\nIn the context of emotion recognition, continuity of the labels separates into Continuous Emotion Recognition (CER) and Emotion Classification (EC). CER research focuses on prediction of AVD values  [4] ,  [5] , while EC focuses on classification of discrete emotions  [6] -  [9] . Another significant difference between CER and EC studies is the loss function that is used during training. While CER studies are mandated by mean squared error (MSE)  [4] , EC is mandated by crossentropy  [9] .\n\nCER research initially concentrated on the speech, and produced uni-modal regressors as speech not only highly reflects the neuro-physiological changes in the research but also its maturation in terms of analysis  [10] -  [12] . However, studies were recently directed into multi-modality where descriptors from body, face, speech and text are combined  [4] ,  [13] . Especially, combining speech with body motion and facial expressions has become a common practice, and multi-modal approaches have quickly proven their superiority against the uni-modal approaches  [4] ,  [13] .\n\nTraditionally, CER set-up consists of a feature extraction, feature summarization and regression blocks  [4] ,  [6] ,  [14] . With the rise of Deep Neural Networks (DNN) and their ability to model non-linear behaviors, initially the Feed-Forward Networks occupied the regressor blocks  [14] . Later, Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN) were introduced into CER. RNNs replaced the Feed-Forward Networks to capture long term relationships  [15] , while CNNs were adapted successfully as a meta-feature extractor by  [7] ,  [16] ,  [17] . As extracting low level descriptors such as acoustic features requires domain expertise and suffers from task independence, end-to-end learning, where raw signals are fed into deep architectures, were proposed as the replacement  [18] . Such end-to-end systems then successfully adapted to the emotion recognition problem by  [5] ,  [19] ,  [20] .\n\nThe state-of-art approaches attack the CER problem by training a predictor for each output. This approach not only increases the complexity of the general task but also suffers from over-fitting due to limited amount of training data. By training the predictor with multi-task learning (MTL), one can force it to learn shared representations and increase the generalization ability  [21] . Although MTL is exploited by  [13] ,  [20] ,  [22]  in the context of CER, unlike our approach these studies trained the proposed architectures with the MSE loss.\n\nStudies on CER evaluate themselves regarding correlation metrics Concordance Correlation Coefficient (CCC)  [12] , or Pearson Correlation Coefficient (PCC)  [4]  while training the predictors using different type of error metrics such as MSE or mean absolute error (MAE) loss . This mismatch causes a degradation in the performance of proposed architectures as also discussed in  [5] ,  [19] .\n\nIn response to these issues in the research, our study tries to address these weaknesses. The main contributions of this study are below:\n\n• A multi-modal deep neural network architecture based on speech and body motion is proposed.  In this section, we first define the feature representations of the speech and body motion signals. Then, the dataset that the experiments are conducted on is introduced, and finally we provide the details of the proposed framework.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Feature Representations",
      "text": "Mel-frequency cepstral coefficients (MFCC) are extracted from the speech signals, and utilized to generate the acoustic features. Each speech frame is summarized into the 39 dimensional acoustic feature vector which includes the energy, the first 12 MFCCs and their first and second derivatives. The MFCC feature vector is represented as f s l ∈ R 39×1 for the lth time frame.\n\nTo represent the body motion, at each instance raw Euler rotation angles in (x,y,z) dimensions along with their deltas are utilized. The resulting body motion feature vector for the lth frame is defined as",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Dataset",
      "text": "In this study, USC CreativeIT  [23]  and RECOLA databases are used to train the proposed architecture. The CreativeIT database consist of pairs of 3.5 minutes length interplays performed by 16 actors to mimic dyadic interactions. After removal of recordings that have missing labels for any of AVD, the training dataset is generated from 40 recordings. Then the recordings are divided into 5 sessions regarding mutual exclusiveness of the speakers. Dividing the dataset into this mutually exclusive sessions enables speaker independent CER.\n\nThe RECOLA database is a popular database for audiovisual emotion recognition, as it is used for the AVEC challenges  [24] ,  [25]  and  [26] . The database contains dyadic interactions of 27 French-speaking subjects. The recordings are separated into three parts 9 train subjects, 9 development and 9 test subjects, as the AVEC challenges urge. By following the procedure in  [27] , only train set is used for training and results are provided from development set. The database is annotated with frame rate of 40 ms, by six annotators. In this study, the mean of these 6 annotations are used as the reference. In addition, while experimenting on RECOLA, body Euler angles in Figure  1 , are replaced with the facial activation units, yaw, pitch, mean and standard deviation of optical flow.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Framing",
      "text": "In order to capture the temporal continuity and variations in the features and to consider the slow varying nature of emotions, we choose a sequence of feature vectors to form a temporal feature-image representation. Since the frame rate of the speech and body features is 60 fps, we choose to form the sequence of feature vectors with a stride of t = 10 frames as\n\nwhere F l ∈ R P ×N is the feature image at frame l, P is the feature vector dimension and N is the number of frames defining the temporal extent.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Architecture",
      "text": "The proposed semi-end-to-end architecture is depicted in the Figure  1 . The proposed architecture mimics a single-task end-to-end framework which is composed of a multiple CNN layers, Max-Pooling Layers, RNN layers and a final fully connected layer  [19] . In this architecture, CNN acts as a metafeature extractor having the ability to extract task-dependent features or meta-features either from the raw signal or input features.\n\nRNN's are famous for their ability to model sequential information. Combination of CNN and RNN layers are effectively used to find the long-term relationships for the image captioning task  [28] ,  [29] . On top of the single end-to-end framework (CER-STL), the proposed architecture involves MTL and these tasks share the same meta-feature extractor. We will refer to the proposed architecture as CER-MTL.\n\n1) Modality Fusion: In our framework, we apply an early fusion and concatenate the modalities at the input. Hence, the input of the multimodal architecture is F b,s l ∈ R 63×N . Concatenating the modalities at the input is both beneficial in terms of complexity of the total architecture, and also from the learning point of view. The most complexity hungry part of the proposed architecture is the CNN layer due to the large input dimensionality. As a result, using the same layer for each task benefits from complexity. In addition, early fusion enables the learning of cross modal features.\n\n2) Loss: In this study we utilize the PCC and CCC as loss functions, in mutually exclusive setups, between the batch of ground truth and predictions.\n\n3) Multiple Task Learning: The objective function of the MTL could be written as the weighted sum of objective functions of individual task which are predicting activation, valence or dominance. Let J(θ) be the objective function of\n\nwhere α m is the weight of the loss of the m th task L m where m ∈ {activation, valence, dominance}.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experiments And Results",
      "text": "The proposed architecture is implemented in Keras framework with TensorfFlow backend. All the conducted experiments are held on NVIDIA GeForce GTx 1080 Ti. During the training Adam optimizer with a learning rate of 0.00005 is used. The training is held with a batch size of 256 for 100 epochs with an early stopping option of 20 epoch. All these hyper-parameters are optimized during the experiments.\n\nThe CNN layer of the CER networks has 25 filters with kernel size of 3, and the max-pooling layer has a kernel size of 2. The output features are then fed into 5 GRU nodes per task. To add further non-linearity the GRU's outputs are fed into a fully connected layer having 2 node, and finally a linear node is added to regress the AVD values. The weights of the MTL are selected uniform as α m = 1/3 for m ∈ {1, 2, 3}.\n\nTo depict the effectiveness of the proposed architecture we devised experiments comparing MTL with STL, correlation losses (PCC and CCC) with MSE loss, multi-modality with single modality and different window sizes. For all the experiments held, CCC is used as the evaluation metric.\n\nResults in Table  I  demonstrate the effect of loss function and using different modalities for CER. For the multimodal case, CER-MTL with CCC loss outperforms the same architecture with PCC and MSE losses, while PCC loss outperforms MSE loss for all the of prediction modalities. For instance the proposed framework achieves 0.38 CCC for activation when it is trained with CCC loss, the same architecture achieves only 0.33 and 0.26 CCC's when it is trained with respectively PCC and MSE losses. These results state that using correlation based metrics are more suitable as the loss function when the evaluation metrics is also correlation based. In fact, with the results presented in Table  II  where CCC loss achieves at least 10% improvements, PCC loss fails to outperform MSE loss on prediction of activation. With regard to this observation, our argument on correlation metrics and losses can be strengthen as, using evaluation metric as the loss function of the DNN architecture provides a robust performance enhancement. However, the downfall of the correlation based loss functions are they require longer batch sizes than regular error metrics like MSE. As the longer batch sizes prone to over-fitting the learning rates should be chosen carefully. In our study, we used relatively small learning rates like between 0.0001 and 0.00005. However, our findings regarding overfitting and learning rate are aligned with  [20] .\n\nThe Figure  2  depicts the predicted AVD values and the CCC correlation of the predictions from CER-MTL with CCC loss, PCC loss and MSE loss over 256 frames, where the CCC values at time index l is calculated by considering the previous 100 predictions before time l. The proposed CCC training outperforms the MSE training by at least 10% CCC difference. In addition, MSE training is unresponsive to the significant changes in the reference, while CCC and PCC loss is highly responsive.\n\nThe performance comparison of unimodal and multimodal architectures is presented in Table  I . Our experiments show that speech carry more information about affective states as it outperforms the motion based architecture with at least 10% CCC difference on creative IT and 2% CCC on RECOLA. Especially, we found that body motion is almost uncorrelated with valence attribute. Another observation in regardless of the loss function activation and valence attributes are mandated by the speech modality.\n\nThe CCC results of the proposed MTL system and the baseline STL system are given in Table  III . CER-MTL outperforms CER-STL on predicting activation and valence  Table  IV  demonstrates the effect of using different window sizes for CER. Note that increasing the window size significantly effect the prediction of activation while there is no such significant trend for valence and dominance. Intuitively increasing receptive field size would add more information to be learnt, the slow varying nature of affective states might add repetition rather than information. Hence, the increasing the input size directly effects the complexity/number of parameters of the CER-MTL which makes it prone to over-fitting.\n\nIn  [12]  the efficiency of the RNN's are discussed, and as a conclusion a complex enough CNN is found enough to model emotions via speech. We have conducted the similar experiments in this study and results are depicted in Table  V . We compared our architecture (CER-MTL) with the pure CNN architecture from  [12]  with a modification at the filter lengths due to the size of data. The original CER-MTL surpasses the one proposed in  [12]  by at least 11% CCC. Although originally Pure CNN  [12] , trained on SEWA corpus  [30] , the similar performance results are expected on the CreativeIT database as well since it includes 120 minutes of training set. This duration is comparable to size which is 130 minutes of training and development duration in  [12] . The performance degradation of Pure CNN  [12] , could be due to its high complexity, which cause overfitting. Table  VI  presents the performance comparison of the proposed CER-MTL architecture with the state-of-the-art solutions on the RECOLA dataset. CER-MTL (N = 60) outperforms the state-of-the-art solutions by at least 1% CCC on activation prediction. This comparison represents the power of the proposed architecture.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Act Val",
      "text": "Pure CNN  [12]  0.51 0.26 End-to-end  [31]  0.74 0.33 DDAT  [32]  0.78 0.50 End-to-end Multimodal  [33]  0.75 0.41 FER-P&G-Net  [27]  0.60 0.69 CER-MTL (N = 60) 0.79 0.37 and performance by forcing to learn shared representations. Moreover, early fusion and MTL bring significant saving in the complexity of the architecture that prevents the overfitting with the limited affective labeled data.\n\nWe conducted experiments on CreativeIT and RECOLA databases to capture effect of window size on the CER, MTL and CCC loss. We found that increasing window size brings a significant improvement on the prediction of activation. The results depict that CER-MTL outperforms CER-STL, especially brings a 6% CCC improvement on prediction of valence. In addition, experiments showed that CER-MTL achieves at least 7% and 13% higher CCC values on respectively CreativeIT and RECOLA datasets when the models are trained with the CCC loss rather than the MSE loss. These performance improvements suggest the effectiveness of the proposed system.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , are replaced with the facial activation units, yaw,",
      "page": 2
    },
    {
      "caption": "Figure 1: Semi-End-to-End framework exploiting MTL for CER",
      "page": 2
    },
    {
      "caption": "Figure 1: The proposed architecture mimics a single-task",
      "page": 2
    },
    {
      "caption": "Figure 2: Prediction of AVD modalities over time for correlation loss and MSE loss",
      "page": 3
    },
    {
      "caption": "Figure 2: depicts the predicted AVD values and the",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "Deriv",
          "Column_3": "",
          "Column_4": "atives"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.38 0.16 0.18": "0.15 0.06 0.10",
          "0.33 0.16 0.14": "0.14 0.02 0.09"
        },
        {
          "0.38 0.16 0.18": "0.37 0.13 0.18",
          "0.33 0.16 0.14": "0.32 0.13 0.18"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.66 0.34": "0.15 0.06",
          "0.49 0.24": "0.09 0.18"
        },
        {
          "0.66 0.34": "0.61 0.32",
          "0.49 0.24": "0.34 0.13"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.38 0.16 0.18": "0.42 0.16 0.18"
        },
        {
          "0.38 0.16 0.18": "0.45 0.16 0.18"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.38 0.16 0.18": "0.31 0.10 0.16"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological review"
    },
    {
      "citation_id": "2",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "3",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C.-N Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "4",
      "title": "Cross-subject continuous emotion recognition using speech and body motion in dyadic interactions",
      "authors": [
        "S Fatima",
        "E Erzin"
      ],
      "year": "2017",
      "venue": "Cross-subject continuous emotion recognition using speech and body motion in dyadic interactions"
    },
    {
      "citation_id": "5",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition through multiple modalities: face, body gesture, speech",
      "authors": [
        "G Castellano"
      ],
      "year": "2008",
      "venue": "Affect and emotion in human-computer interaction"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition through multiple modalities: face, body gesture, speech",
      "authors": [
        "G Castellano",
        "L Kessous",
        "G Caridakis"
      ],
      "year": "2008",
      "venue": "Affect and emotion in human-computer interaction"
    },
    {
      "citation_id": "8",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H.-W Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "9",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition in speech using neural networks",
      "authors": [
        "J Nicholson",
        "K Takahashi",
        "R Nakatsu"
      ],
      "year": "1999",
      "venue": "ICONIP 1999, 6th International Conference on Neural Information Processing -Proceedings"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O Kwon",
        "K Chan",
        "J Hao",
        "T Lee"
      ],
      "year": "2003",
      "venue": "EUROSPEECH 2003 -8th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "12",
      "title": "Continuous Emotion Recognition in Speech -Do We Need Recurrence?",
      "authors": [
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Continuous Emotion Recognition in Speech -Do We Need Recurrence?"
    },
    {
      "citation_id": "13",
      "title": "Multimodal multi-task learning for dimensional and continuous emotion recognition",
      "authors": [
        "S Chen",
        "J Zhao",
        "Q Jin",
        "S Wang"
      ],
      "year": "2017",
      "venue": "AVEC 2017 -Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge, co-located with MM 2017"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "15",
      "title": "Prediction-based learning for continuous emotion recognition in speech",
      "authors": [
        "J Han",
        "Z Zhang",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "16",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "arxiv": "arXiv:1708.07050"
    },
    {
      "citation_id": "17",
      "title": "The priori emotion dataset: Linking mood to emotion detected in-the-wild",
      "authors": [
        "S Khorram",
        "M Jaiswal",
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2018",
      "venue": "The priori emotion dataset: Linking mood to emotion detected in-the-wild",
      "arxiv": "arXiv:1806.10658"
    },
    {
      "citation_id": "18",
      "title": "Towards end-to-end speech recognition with recurrent neural networks",
      "authors": [
        "A Graves",
        "N Jaitly"
      ],
      "year": "2014",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "19",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "20",
      "title": "Attention-augmented end-to-end multi-task learning for emotion prediction from speech",
      "authors": [
        "Z Zhang",
        "B Wu",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "A bayesian/information theoretic model of learning to learn via multiple task sampling",
      "authors": [
        "J Baxter"
      ],
      "year": "1997",
      "venue": "Machine learning"
    },
    {
      "citation_id": "22",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Jointly predicting arousal, valence and dominance with multi-task learning"
    },
    {
      "citation_id": "23",
      "title": "The usc creativeit database: A multimodal database of theatrical improvisation",
      "authors": [
        "A Metallinou",
        "C.-C Lee",
        "C Busso",
        "S Carnicke",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Multimodal Corpora: Advances in Capturing, Coding and Analyzing Multimodality"
    },
    {
      "citation_id": "24",
      "title": "Av+ ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "S Jaiswal",
        "E Marchi",
        "D Lalanne",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "25",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge"
    },
    {
      "citation_id": "26",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne",
        "A Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "27",
      "title": "Continuous emotion recognition in videos by fusing facial expression, head pose and eye gaze",
      "authors": [
        "S Wu",
        "Z Du",
        "W Li",
        "D Huang",
        "Y Wang"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction, ser. ICMI '19"
    },
    {
      "citation_id": "28",
      "title": "Show and tell: A neural image caption generator",
      "authors": [
        "O Vinyals",
        "A Toshev",
        "S Bengio",
        "D Erhan"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "K Xu",
        "J Ba",
        "R Kiros",
        "K Cho",
        "A Courville",
        "R Salakhudinov",
        "R Zemel",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "30",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "B Schuller",
        "K Star",
        "E Hajiyev",
        "M Pantic",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "32",
      "title": "Dynamic difficulty awareness training for continuous emotion prediction",
      "authors": [
        "Z Zhang",
        "J Han",
        "E Coutinho",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    }
  ]
}