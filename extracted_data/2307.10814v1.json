{
  "paper_id": "2307.10814v1",
  "title": "Cross-Corpus Multilingual Speech Emotion Recognition: Amharic Vs. Other Languages",
  "published": "2023-07-20T12:24:23Z",
  "authors": [
    "Ephrem Afele Retta",
    "Richard Sutcliffe",
    "Jabar Mahmood",
    "Michael Abebe Berwo",
    "Eiad Almekhlafi",
    "Sajjad Ahmed Khan",
    "Shehzad Ashraf Chaudhry",
    "Mustafa Mhamed",
    "Jun Feng"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Convolutional neural network",
    "cross-corpus",
    "multilingual",
    "multiple training languages",
    "Amharic"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language. However, where training data for a language does not exist, data from other languages can be used instead. We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU. For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED). For English, German and Urdu",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions assist individuals to communicate and to comprehend others' points of view  [50] . Speech emotion recognition (SER) is the task of comprehending emotion in a voice signal, regardless of its semantic content  [17] . SER datasets are not available in all languages. Moreover, the quantity and quality of the training data which is available varies considerably from one language to another. For example, when evaluated across several datasets, differences in corpus language, speaker age, labeling techniques, and recording settings significantly influence model performance  [48, 49] . This encourages the development of more robust SER systems capable of identifying emotion from data in different languages. This can then permit the implementation of voice-based emotion recognition systems in real-time for an extensive variety of industrial and medical applications.\n\nThe majority of research on SER has concentrated on a single corpus, without considering cross-lingual and cross-corpus effects. One reason is that, in comparison to the list of spoken languages, we only have a small number of corpora for the study of speech analysis  [44] . Furthermore, even when only considering the English language, accessible resources vary in quality and size, resulting in the dataset sparsity problem observed in SER research. In such instances, learning from a single data source makes it challenging for SER to function effectively. As a result, more adaptable models that can learn from a wide range of resources in several languages are necessary for practical applications.\n\nSeveral researchers have investigated cross-corpus SER in order to enhance classification accuracy across several languages. These works employed a variety of publicly accessible databases to highlight the most interesting trends  [37] . Even though some research has addressed the difficulty of cross-corpus SER, as described in Schuller et al.  [37] , the challenges posed by minority languages such as Amharic have not been investigated. Amharic is the second-largest Semitic language in the world after Arabic and it also the national language of Ethiopia  [30] . In terms of the number of speakers and the significance of its politics, history, and culture, it is one of the 55 most important languages in the world  [28] . Dealing with such languages is critical to the practicality of next-generation systems  [1] , which must be available for many languages.\n\nIn previous work  [32] , we created tbe first spontaneous emotional dataset for Amharic. This contains 2,474 recordings made by 65 speakers (25 male, 40 female) and uses five emotions: fear, neutral, happy, sad, and angry. The Amharic Speech Emotion Dataset (ASED) is publicly available for download  3  . The ASED dataset allows us to carry out the work reported here.\n\nThe contributions of this paper are as follows:\n\n• We investigate different scenarios for monolingual, cross-lingual and multilingual SER using datasets for Amharic and three other languages (English, German and Urdu). • We experiment with a novel approach in which a model is trained on data in several non-Amharic languages before being tested on Amharic. We show that training on two non-Amharic languages gives a better result than training on just one. • This is the first work that shows the performance tendencies of Amharic SER utilizing several languages, to the best of our knowledge.\n\nThe structure of this paper is as follows: Section 2 presents previous work. Section 3 explains our approach, datasets, and feature extraction methods for SER. Section 4 presents the proposed deep learning architecture and experimental settings. Section 5 describes the experiments and outcomes. Finally, Section 6 gives conclusions and next steps.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Over the last two decades, much important research has been conducted on speaker-independent SER. This work has shown that several factors influence accuracy, including the dataset utilized, the features extracted, and the classifier network employed to predict emotions. Sailunaz el al.  [34]  present a thorough survey of the datasets available, the features extracted, and the networks most commonly employed for SER. However, while there has been preliminary research on enhancing the robustness of SER by combining multiple emotional speech corpora to form the training set and thereby minimising data scarcity, there is a shortage of studies on multilingual cross-corpus SER  [37, 36] . In the following, we first summarize related cross-lingual work. After that we outline multilingual studies. Information about all the research is shown in Table  1 .\n\nConcerning cross-lingual studies, Lefter et al.  [25]  carried out an early study in which they trained a SER classifier on one or more datasets and then tested on another. In a cross-lingual setting, training on ENT and testing on DES gave the lowest Equal Error Rate for Anger (29.9%).\n\nAlbornoz et al.  [1]  proposed a SER classifier for emotion detection, focusing on emotion identification in unknown languages. The results showed what could be expected from a system trained with a different language, reaching 45% on average. The standard multi-class SVM performed better than the classifier implemented using Emotion Profiles (EP). On average, the Standard Classifier (SC) reached 56.8%, whereas the Emotional Profile Classifier (EPC) obtained 52.1%.\n\nXiao et al.  [46]  examined SER for Mandarin Chinese vs. Western languages like German and Danish. The authors concentrated on gender-specific SER and attained classification rates that were higher than chance but lower than baseline accuracy. The best classification rate in the cross-language family test on male speech samples (71.62%), was when the Chinese Dual mode Emotional Speech Database (CDESD) was used for training and Emo-DB was used for testing.\n\nSagha et al.  [33]  utilized language detection to improve cross-lingual SER. They found that using a language identifier followed by network selection rather than a network trained on all existing languages was superior for recognizing the emotions of a speaker whose language is unknown. On average, the Language IDentification (LID) approach for selecting training corpora was superior to using all the available corpora when the spoken language was not known.\n\nMeftah et al.  [27]  proposed Deep Belief Networks (DBN) for cross-corpus SER and evaluated them in comparison with MLP via emotional speech corpora for Arabic (KSUEmotions) and English (EPST). Training on one dataset and testing on the other yielded similar results for both directions and both models. The best result was Arabic→English using DBN (Valence 53.22%, Arousal 57.2%).\n\nLatif et al.  [23]  extracted eGeMAPS features from their raw audio data. They used SVM with a Gaussian kernel for classifying data into their respective categories. The best result was when training on EMO-DB and then testing on URDU (57.87%).\n\nLatif et al.  [24]  also used eGeMAPS features and they employed five different corpora for three different languages to investigate cross-corpus and cross-language emotion recognition using Deep Belief Networks (DBNs). IEMOCAP performs well on EMO-DB compared to FAU-AIBO even though both the latter datasets are German.\n\nLatif et al.  [22]  studied SER using languages from various language families, such as Urdu vs. Italian or German. The best cross-lingual results were obtained by training on URDU and testing on EMODB (65.3%) and the worse were by training on URDU and testing on SAVEE (53.2%).\n\nGoel et al.  [13]  used transfer learning to carry out multi-task learning experiments and discovered that traditional machine learning architectures  [44, 3]  can perform as well as deep learning neural networks for SER provided the researchers pick appropriate input features. Training the model on IEMOCAP and testing on EMO-DB obtained the best performance (65%).\n\nZehra et al.  [47]  presented an ensemble learning approach for cross-corpus machine learning SER, utilizing the SAVEE, URDU, EMO-DB, and EMOVO databases. The method employed three of the most prominent machine learning algorithms, Sequential Minimal Optimization (SMO), Random Forest (RF), and Decision Tree (J48), plus a majority voting mechanism. The ensemble approach was worse than the other classifiers except when training on EMOVO and testing on URDU (62.5%).\n\nJarod et al.  [8]  used prosody prediction and employed eight different corpora for five European languages to investigate cross-lingual and multilingual emotion recognition using Wav2Vec2XLSR. The multilingual setup outperformed the monolingual one for all selected European languages, except English, by a very small margin.\n\nPandey et al.  [31]  proposed a SER classifier for emotion detection, focusing on learning emotions, irrespective of culture. They also used 3D Mel-Spectrogram features (henceforth MelSpec) and employed five different corpora for five languages to investigate cross-lingual emotion recognition using an Attention-Gated Tensor Factorized Neural Network (AG-TFNN). The best result was Fold2→German using 3D TFNN. In addition, Fold5→Telugu had better performance when compared to Fold4→Hindi, even though both languages are of Indian origin.\n\nWe now consider multilingual approaches in which several datasets in different languages are used for training. In addition to the cross-lingual experiments referred to earlier, Lefter et al.  [25]  also carried out some multilingual work in which they trained on various pairs or triples of datasets chosen from EMO-DB, DES and ENT, and tested on each of these individually. The best result was obtained by training on all three and testing on EMO-DB (Equal Error Rate 20.5%).\n\nFigure  1 : Block diagram of our approach for SER.\n\nLatif et. al  [23]  used four different corpora (SAVEE, EMOVO, EMO-DB and URDU) for four different languages to investigate multilingual emotion recognition using Support Vector Machines (SVM). When training on EMO-DB, EMOVO and SAVEE and testing on URDU, a result of 70.98% was achieved, which was higher then any pair of these datasets.\n\nLatif et al.  [22]  also used SAVEE, EMOVO, EMO-DB and URDU. The best performance was training on SAVEE, EMOVO, URDU and testing on EMO-DB (68%). The worst performance was training on the same three datasets and testing on EMOVO (61.8%)\n\nRegarding the model used, Latif et al.  [23] , Albornoz et al.  [1] , Lefter et al.  [25] , and Sagha et al.  [33]  are all based on SVMs. Meftah et al.  [27]  and Latif et al.  [24]  utilized DBN, Goel et al.  [13] , Jarod et al.  [8]  and Pandey et al.  [31]  applied machine learning and deep learning methods, Zehra et al.  [47]  used ensemble methods, and lastly Xiao et al.  [46]  and Latif et al.  [22]  applied GAN, and SMO respectively. Concerning the earlier studies we observe that the SVM algorithm performs poorly on large data sets. It also performs poorly in situations with more characteristics per data point, especially in multi-class situations. When attempting to extract features from DBN plus low-level acoustic information, vs. DBN with eGeMAPS, the latter significantly outperformed. Additionally, deep learning models outperform conventional classifiers. However, the model of Goel et al.  [13]  extracts features quite well but requires a lot of training time. As previously indicated, an ensemble strategy only provided the best performance in one scenario. Furthermore, the existing techniques in SER lack preprocessing operations. We conclude that, across many datasets, binary performance outperforms multiple classes. Plus none of the previous work has focused on the Amharic language.\n\nHere, we first present the preprocessing strategy before the extraction of features from the signal. Second, we propose an architecture, based on the VGG model, which offers good results. Third, we provide a classification benchmark for Amharic and three non-Amharic languages using deep Neural Networks. Finally, we contrast the effectiveness of our novel training scenarios to demonstrate the efficiency of cross-lingual and multilingual approaches.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Approach",
      "text": "Many factors influence SER accuracy in a cross-corpus and multilingual context. The dataset utilized, the features extracted from the speech signals, and the neural network classifiers implemented to identify emotion are all essential aspects that might significantly impact the results. Our SER method is summarized in Figure  2 . We use four corpora (ASED, RAVDESS, EMO-DB, URDU) to test SER in Amharic, English, German and Urdu respectively.\n\nOne difficulty faced with this research is that datasets use different sets of emotion labels, as can be seen in Table  2 . Following previous work  [6, 9]  we address this by mapping labels onto just two classes, positive valence and negative valence, as indicated in the table.\n\nFurther details on the chosen datasets, feature extraction, and classifier are provided below.   [4]  is for German, uses five emotions and contains ten everyday sentences, five made of one phrase, five made of two phrases. There are ten speakers (5 male, 5 female), nine qualified in acting and about 535 raw utterances in total. Recording is done at 16 kHz and 16 bits, and was carried out in the Anechoic chamber of the Technical Acoustics Department at the Technical University Berlin.\n\nRAVDESS  [26]  is for English, uses eight emotions and contains just two sentences. The 24 speakers (12 male, 12 female) are professional actors. Interestingly, emotion in this dataset is 'self-induced'  [42] , rather than Acted. Moreover, there are two levels of each emotion. There are 4,320 utterances. Project investigators selected the best two clips for each speaker and each emotion. Recording was at 48 kHz and 16 bits, and it was carried out in a professional recording studio at Ryerson University.\n\nURDU  [23]  is for Urdu, uses four emotions and comprises 400 audio recordings from Urdu TV talk shows. There are 38 speakers (27 male, 11 female). Emotions are not acted, but occur naturally during the conversations between guests on the talk shows.\n\nASED  [32]  is for Amharic and was created by the authors in previous work. It uses five emotions and consists of 2,474 recordings made by 65 speakers (25 male, 40 female). Recording is done at 16 kHz and 16 bits. The ASED dataset is accessible to the public for research purposes (see URL in earlier footnote).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Data Preprocessing",
      "text": "Before proceeding to feature extraction, a number of pre-processing steps were performed on the datasets as shown in Figure  3 . Recordings were first downsampled to 16kHz and converted to mono. Most of the sound clips in the datasets are 5 seconds in length or less. A few are longer than this. Therefore, we extended any shorter clips to 5 seconds by adding silence to the end. Conversely any longer clips were cut off in order to make them exactly 5 seconds long. The statistics of clip lengths are shown in Table 3",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction For Ser",
      "text": "A vast amount of information reflecting emotional characteristics is present in the speech signal. One of the key issues within SER research is the choice of features which should be used.\n\nPreviously, traditional feature extraction methods such as prosodic features were used for SER  [10, 11] , including the variance, intensity, spoken word rate, and pitch. However, some traditional features are shared across different emotions, as discussed by Gangamohan et al.  [11] . For example, as observed in Table  11 .2 of Gangamohan et al., angry and happy utterances have similar trends in F0 and speaking rate, compared to neutral speech.\n\nManually extracted traditional features may work well with traditional classification methods in machine learning, where a set of features or attributes describes each instance in a dataset  [16] . In contrast, however, deep learning can itself determine which features to focus on to recognize verbal emotions. Finding some set of feature vectors or properties that can give a compact representation of the input audio signal has therefore become the main aim of feature extraction methods. The spectrum extraction methods convert the input sound waveform to some discrete shape or feature vector. Normally, the speech signal is not static but when looking at a short period of time, it acts as a static signal. This short, detached snap is called a frame. The acoustic model extracts features from the frames  [7, 2] . Feature extraction deals with obtaining useful information for reference by removing irrelevant information. These extracted feature vectors are fed into deep learning models. In short, spectrum extraction methods can convert audio signals into vectors that deep learning models can handle. The model can then be trained to learn the features of each emotion and hence classify it. Overall, this is one reason why deep learning models can perform better than machine learning models.\n\nAfter reviewing many works on SER, it is clear that Mel-Frequency Cepstral Coefficients (MFCC) are widely used in audio classification and speech emotion recognition  [15] . MFCC is a coefficient that expresses the short-term power spectrum of a sound. It uses a series of steps to imitate the human cochlea, thereby converting audio signals. The Mel scale is significant because it approximates the human perception of sound instead of being a linear scale  [39] . In previous work  [32]  we compared MFCC to alternatives and found it to be the best. This is the reason we choose MFCC features for the present study.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Architectures And Settings",
      "text": "Most prior research uses CNN-based models for SER  [21] . Among such models, the notable ones include AlexNet  [20] , VGG  [41, 29] , and ResNet50  [14, 12] . This section provides a short overview of the models. Our proposed model, VGGE, is a variant of VGG. • AlexNet is one of the famous CNN models used in applications such as image classification and recognition, and is widely employed for SER classification  [35] . It achieved an outstanding result at the ImageNet competition in 2012  [18] .\n\n• VGG  [41]  appeared in 2014, created by the Oxford Robotics Institute. It is well known that the early CNN layers capture the general features of sounds such as wavelength, amplitude, etc., and later layers capture more specific features such as the spectrum and the cepstral coefficients of waves. This makes a VGG-style model suitable for the SER task. After some experimentation, we found that a model based on VGG but using four layers gave the best performance. We call this proposed model VGGE and used it for our experiments. Figure  2  shows the settings for VGGE.\n\n• ResNet  [14]  was launched in late 2015. This was the first time that networks having more than a hundred layers were trained. Subsequently it has been applied to SER classification  [12] .\n\nConcerning the experimental setup, the standard code for AlexNet and ResNet50 was downloaded and used for the experiments. For VGGE, the network configuration was altered, as shown in Figure  2 . For the other models, the standard network configuration and parameters were used.\n\nIn all experiments, the librosa v0.7.2 library  [38]  was used to extract MFCC features.\n\nWe used the Keras deep learning library, version 2.0, with a Tensorflow 1.6.0 backend to build the classification models. The models were trained using a machine with an NVIDIA GeForce GTX 1050. Our model employed the Adam optimization algorithm, with categorical cross-entropy as the loss function; training was terminated after 100 epochs and batch size was set to 32.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "Mostly, two methods are utilized for speaker-independent SER  [40] : The first method is Leave One Speaker Out (LOSO)  [3, 5, 45] ; Here, when the corpus contains n speakers, we use n -1 speakers for training and the remaining speaker for testing. For cross-validation, the experiment is repeated n times with a different test speaker each time. In the second method, the training and testing sets have been determined previously  [23, 43, 19] .\n\nIn our work, we followed the second approach. For the first monolingual experiment (train on a corpus, test on the same corpus) the data was split into training, testing, and validation sets randomly five times, each time ensuring that the split sets are speaker-independent. As shown in Table  4 , all the datasets were split 70% train, 20% test, and 10% validation.\n\nIn the first experiment, we also carried out a sentence-independent study where sentences used for training were not used for testing.\n\nThe second and third experiments are the cross-lingual experiment (train on a corpus in one language, test on a corpus in another language) and the multilingual experiment (train on two or three corpora joined together, each in a different non-Amharic language, and test on the Amharic ASED corpus. In these experiments, the speakers in the validation sets are not seen in the training sets. Moreover, the speakers in the testing set are by definition not the same as those in the training and validation sets, as they are from different datasets.\n\nFigure  5  shows a label distribution that is balanced across partitions. The performance of the proposed classification of Amharic language data used in monolingual, cross-lingual, and multi-lingual SER experiments is evaluated using F1-score and accuracy. We have shared the file names for the audio files that belonged to train, validation, and test partitions in the experiments  4  . For each experiment, the models were trained five times and the average result was reported.  The aim was to carry out an initial comparison of the proposed VGGE model with the two existing models discussed above, AlexNet and ResNet50. Four datasets were used, ASED, RAVDESS, EMO-DB, and URDU. To allow comparison with the other experiments, the emotion labels for each dataset were mapped onto just two labels, Positive valence and Negative valence, as shown by the scheme in Table  2 . This follows the standard approach found in other work  [6, 9] . When comparing to other papers, we should bear in mind the label mapping which we needed to adopt in order to undertake the later cross-lingual and multilingual experiments. Looking at the table, we can see that the ASED, RAVDESS, EMO-DB and URDU datasets originally had five, eight, seven and four emotion classes respectively, and that these are now being mapped onto just two classes, positive and negative emotion. This simplifies the task, which can account for higher performance figures than in other published works.\n\nExperiment 1 has two parts. In Experiment 1.1, the groups of speakers used for training and testing were varied. In Experiment 1.2, the dataset sentences used for training and testing were varied.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Experiment 1.1: Independence Of Speakers",
      "text": "Results of the experiment are shown in Table  5 . We can see that VGGE was the best on ASED (Amharic) and EMO-DB (German), ResNet50 was the best on RAVDESS (English) and AlexNet was the best on URDU (Urdu).\n\nIt is interesting to look at the average figures on the bottom row of the table. ASED (82.53%) and RAVDESS (82.71%) are very close, EMO-DB (77.78%) is 4.75% lower than ASED, and URDU (84.58%) is 2.05% higher than ASED. Generally, the differences are not that large when we consider that the languages have very different characteristics, and that the datasets were created independently by different researchers. Moreover, recall that the original data is being mapped onto two sentiment classes from the original four-to-eight classes (see Section 6.1 and Table  2 ). Subject to these points, we might conclude that Amharic and English monolingual mono-corpus SER are of similar difficulty, German is more difficult and Urdu is easier. As languages, English and German are perhaps the most similar, since they are both within the Germanic branch of the Indo-European language group. Urdu is also Indo-European, but from the Indo-Iranian branch. Finally, Amharic is from the Semitic branch of the Afro-Asiatic group.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Experiment 1.2: Independence Of Sentences",
      "text": "Recall that the datasets all consist of different sentences spoken in every emotion, with the exception of the URDU dataset, based on TV talk show conversation, where individual sentences are not identified. Hence, URDU was not used here.\n\nIn this experiment, sentences were either used for training or testing. For each of the datasets shown in the table, the proposed VGGE model, along with AlexNet and ResNet50, were trained using MFCC features. Each model was trained five times using a 80%/20% train/test split, and the average results were computed.\n\nResults are in Table  6 . The trends are similar to Experiment 1.1. This time, VGGE is the best on ASED and RAVDESS, while ResNet50 is the best on EMO-DB. Concerning the averages, ASED and RAVDESS are fairly close (84.46%, 81.11%), while EMO-DB is lower (66.67%). So this again suggests that Amharic and English monolingual SER are of similar difficulty and easier, within the context of these particular datasets and this task, while German SER is more difficult.\n\n7 Experiment 2: Comparison of SER methods for Amharic cross-lingual SER\n\nThe aim was to compare the three models AlexNet, VGGE, and ResNet50 (Section 4) when applied to cross-lingual SER. This time, the systems are trained on data in one language and then tested on data in another language. Firstly, the three models are trained on ASED and then tested on EMO-DB, then trained on EMO-DB and tested on ASED, and so on, for different combinations. To allow this cross-training, dataset-specific emotion labels are mapped onto two classes, Positive and Negative, using the same method as for Experiment 1.\n\nOnce again, MFCC features were used for all models. The network configuration for VGGE was the same as in the preceding Experiment (Figure  2 ). For the other models, the standard configuration and settings were used.\n\nResults are presented in Table  7 . As line 1 of the table shows, we first trained on ASED and evaluated on EMO-DB (henceforth written ASED→EMO-DB). VGGE gave the best accuracy (66.67%), followed closely by AlexNet (65.80%), and then ResNet50 (64.06%). For EMO-DB→ASED, VGGE was best (64.22%), also followed by AlexNet (62.39%), and then ResNet50 (58.72%).\n\nNext, for ASED→RAVDESS, AlexNet was best (66.00%), followed by ResNet50 (61.75%) and VGGE (59.25%). For RAVDESS→ASED, AlexNet was best (65.87%), closely followed by ResNet50 (64.16%) and then VGGE (61.43%).\n\nThirdly, we used ASED→URDU. Here, ResNet50 was best (61.56%), followed by AlexNet (60.00%) and VGGE (59.69%). For URDU→ASED, ResNet50 was best (61.33%), followed by VGGE (60.00%) and AlexNet (50.67%).\n\nIt is interesting that for ASED↔EMO-DB), VGGE was best, for ASED↔RAVDESS, AlexNet was best, and for ASED↔URDU, ResNet50 was best. What is more, the figures for AlexNet on ASED↔RAVDESS in the two directions (66.00%, 65.87%, difference 0.13%) were very close, as were those for ResNet50 on ASED↔URDU (61.56%, 61.33%, difference 0.23%), while those for VGGE on ASED↔EMO-DB) (66.67%, 64.22%, difference 2.45%) were slightly further apart.\n\nWe can therefore conclude that the performance of the three models was very similar overall. This is supported by the average accuracy figures for AlexNet, VGGE and ResNet50 (61.79%, 61.88%, 61.93%) which are also very close, only 0.14% from the smallest to the biggest. The results in the table also show the average F1-score performance for VGGE (55.99%) is higher than that for AlexNet (54.81%, 1.18% lower) and ResNet50 (53.30%, 2.69% lower). Hence, it is concluded from these results that the prediction performance of VGGE was best, closely followed by AlexNet and then ResNet50. However, the range of F1-scores is small, only 2.69% from the smallest to the biggest, indicating only a slight difference in performance between different scenarios.\n\nRegarding the results as a whole, two points can be made. First, the accuracy obtained by training on one language and testing on another is surprisingly good. Second, the best language to train on when testing on Amharic seems to vary by model; for AlexNet it is RAVDESS (65.87%), for VGGE it is EMO-DB (64.22%) and for ResNet50 it is RAVDESS again (64.16%).\n\nFinally, we can compare our results for this experiment (Table  7 ) with those given for previous cross-lingual studies in Section 2. Generally, they seem comparable. Our average results are around 62%. In the previous studies we see 56.8%  [1] , 57.87%  [23] , 65.3%  [22] , and 62.5%  [47] . The highest is 71.62%  [46] . In looking at these figures, we must remember that the exact methods and evaluation criteria used in previous experiments vary, so exact comparisons are not possible. Many different languages and datasets are used, emotion labels may need to be combined or transformed in different ways, and so on. Please refer to Section 2 for the details regarding these figures.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Experiment 3: Multilingual Ser",
      "text": "In the previous experiment, we trained in one language and tested in another. In this final experiment, we trained on several non-Amharic languages and then tested on Amharic.\n\nThe same three models were used, AlexNet, VGGE and ResNet50, with the same settings and training regime as in the previous experiments. When RAVDESS is added to EMO-DB+URDU to make EMO-DB+RAVDESS+URDU, the performance of VGGE falls 1.53% to 68.41%. In the results presented in Table  9 , the upper right-hand column shows the average accuracy, These results suggest that, by using several non-Amharic datasets for training, we can obtain a better result, by several percent, than when using one non-Amharic dataset for training, when testing on Amharic.\n\nComparing with previous studies in Section 2, there are only three which present multilingual experiments. Lefter et al.  [25]  report that training on three datasets, EMO-DB, DES and ENT, and testing on EMO-DB gave the best result, better than their cross-lingual trials. This concurs with our own findings, where average results for Experiment 3 (Table  8 , bottom line) were higher than those of Experiment 2 (Table  7 , bottom line). Latif et al.  [23]  found that training on EMO-DB, EMOVO and SAVEE and testing on URDU gained a better result than using just two training datasets. Latif et al.  [22]  also obtained the best result when training on three datasets.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we first proposed a variant of the well-known VGG model, which we call VGGE, and then applied AlexNet, VGGE and ResNet50 to the task of Speech Emotion Recognition, focusing on the Amharic language. This was made possible by the existence of the publicly-available Amharic Speech Emotion Dataset (ASED) which we created in previous work  [32] . In Experiment 1, we trained the three models on four datasets, ASED (Amharic), RAVDESS (English), EMO-DB (German), and URDU (Urdu). In each case, a model was trained on one dataset and then tested on that same dataset. Speaker-independent and sentence-independent training variants were tried. The results suggested that Amharic and English monolingual SER are almost equally difficult on the datasets we used for these languages, German is harder, and Urdu is easier.\n\nIn Experiment 2, we trained on SER data in one language and tested on data in another language, for various language pairs. When ASED was the target, the best dataset to train on was RAVDESS for AlexNet and ResNet50, and EMO-DB for VGGE. This could indicate that, in terms of SER, Amharic is more similar to English and German than it is to Urdu.\n\nIn Experiment 3, we combined datasets for two or three different non-Amharic languages for training, and used the Amharic dataset for testing. The best result in Experiment 3 (EMO-DB+URDU→ASED, VGGE, 69.94%) was 4.07% higher than the best result in Experiment 2 (RAVDESS→ASED, AlexNet, 65.87%). In addition, the best overall average figure in Experiment 3 (VGGE, 66.44%) was 4.51% higher than the best overall average figure in Experiment 2 (ResNet50, 61.93%). These findings suggest that if several non-Amharic datasets are used for SER training, the results can be better than if one non-Amharic dataset is used, when testing is on Amharic. Overall, the experiments demonstrate how cross-lingual and multilingual approaches can be used to create effective SER systems for languages with little or no training data, confirming the findings of previous studies. Future work could involve improving SER performance when training on non-target languages, and trying to predict which combination of source languages will give the best result.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of our approach for SER.",
      "page": 5
    },
    {
      "caption": "Figure 2: We use four corpora",
      "page": 5
    },
    {
      "caption": "Figure 3: Recordings were first downsampled to 16kHz and converted to mono. Most of the sound clips in the datasets",
      "page": 6
    },
    {
      "caption": "Figure 2: Network architecture of proposed VGGE, based on well-known VGG model.",
      "page": 7
    },
    {
      "caption": "Figure 3: Data Preprocessing.",
      "page": 7
    },
    {
      "caption": "Figure 4: Distribution of utterance lengths for all datasets, based on duration ranges. 2.0 −3 in the figure means",
      "page": 8
    },
    {
      "caption": "Figure 2: shows the settings for VGGE.",
      "page": 9
    },
    {
      "caption": "Figure 2: For the other models, the",
      "page": 9
    },
    {
      "caption": "Figure 5: shows a label distribution that is balanced across partitions. The performance of the proposed classification",
      "page": 9
    },
    {
      "caption": "Figure 5: Class distribution within Datasets.",
      "page": 10
    },
    {
      "caption": "Figure 2: ). For the other models, the standard configuration and settings were used.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ref": "[25]\n[1]\n[46]\n[33]\n[27]\n[23]\n[24]\n[22]\n[13]\n[8]\n[31]\n[47]",
          "Methods\nEmployed": "SVM\nSVM, SC, EPC\nSMO\nSVM\nDBN, MLP\nSVM\nDBN\nGAN\nLSTM, LR, SVM\nCNN and\nWav2Vec2-XLSR\nAG-TFNN\nSMO, RF, J48\nEnsemble",
          "Feature\nExtraction": "Prosodic\nProsodic\nVarious\nMFCC\nVarious\nMFCC\nVarious\nMelSpec\nLow-level\nAcoustic\neGeMAPS\neGeMAPS\neGeMAPS\nVarious\nISO9\nprosody\nMelSpec\nSpectral\nProsodic\neGeMAPS",
          "Databases &\nLanguages": "EMO-DB (German), DES (Danish)\nENT (English), SA (Afrikaans)\nRML (Mandarin, English, Italian\nPersian, Punjabi, Urdu)\nCDESD (Mandarin), EMO-DB, DES\nEU-EmoSS (English, French, German,\nSpanish), VESD (Chinese), CASIA (Chinese)\nKSUEmotions (Arabic), EPST (English)\nSAVEE (English), EMOVO (Italian),\nEMO-DB, URDU (Urdu)\nFAU-AIBO (German), IEMOCAP (English)\nEMO-DB, SAVEE, EMOVO\nEMO-DB, SAVEE, EMOVO, URDU\nEMOVO, EMO-DB, SAVEE, IEMOCAP,\nMASC (Chinese)\nIEMOCAP, CREMA-D (English), ESD (English),\nSynpaflex (French), Oreau (French),\nEMO-DB, EMOVO, emoUERJ (Portuguese)\nEMO-DB, eNTERFACE (English),\nIITKGP-SEHSC (Hindi), IITKGP-SESC (Telugu),\nShEMO-DB (Persian)\nSAVEE, URDU, EMO-DB, EMOVO",
          "Expts": "XM\nX\nX\nX\nX\nXM\nX\nXM\nX\nX\nM\nX",
          "Classes": "3\n6\nArousal\nAppraisal\nSpace\nArousal\nValence\nPlane\n2\n2\n2\n2\n5\n4\n2\n2"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Aspect": "Language",
          "ASED": "Amharic",
          "RAVDESS": "English",
          "EMO-DB": "German",
          "URDU": "Urdu"
        },
        {
          "Aspect": "Recordings",
          "ASED": "2474",
          "RAVDESS": "1440",
          "EMO-DB": "535",
          "URDU": "400"
        },
        {
          "Aspect": "Sentences",
          "ASED": "27",
          "RAVDESS": "2",
          "EMO-DB": "10",
          "URDU": "-"
        },
        {
          "Aspect": "Participants",
          "ASED": "65",
          "RAVDESS": "24",
          "EMO-DB": "10",
          "URDU": "38"
        },
        {
          "Aspect": "Emotions",
          "ASED": "5",
          "RAVDESS": "8",
          "EMO-DB": "7",
          "URDU": "4"
        },
        {
          "Aspect": "Positive valence",
          "ASED": "Neutral, Happy",
          "RAVDESS": "Neutral, Happy,\nCalm, Surprise",
          "EMO-DB": "Neutral, Happiness",
          "URDU": "Neutral, Happy"
        },
        {
          "Aspect": "Negative valence",
          "ASED": "Fear, Sadness,\nAngry",
          "RAVDESS": "Fear, Sadness,\nAngry, Disgust",
          "EMO-DB": "Anger, Sadness,\nFear, Disgust,\nBoredom",
          "URDU": "Angry, Sad"
        },
        {
          "Aspect": "References",
          "ASED": "[32]",
          "RAVDESS": "[26]",
          "EMO-DB": "[4]",
          "URDU": "[23]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Duration": "1-2.0",
          "ASED": "",
          "EMO-DB": "126",
          "RAVDESS": "",
          "URDU": ""
        },
        {
          "Duration": "2-3.0",
          "ASED": "850",
          "EMO-DB": "224",
          "RAVDESS": "",
          "URDU": "200"
        },
        {
          "Duration": "3.0-4",
          "ASED": "1624",
          "EMO-DB": "136",
          "RAVDESS": "1440",
          "URDU": "200"
        },
        {
          "Duration": "4.0-5",
          "ASED": "",
          "EMO-DB": "24",
          "RAVDESS": "",
          "URDU": ""
        },
        {
          "Duration": "5.0-6",
          "ASED": "",
          "EMO-DB": "20",
          "RAVDESS": "",
          "URDU": ""
        },
        {
          "Duration": "6.0-7",
          "ASED": "",
          "EMO-DB": "3",
          "RAVDESS": "",
          "URDU": ""
        },
        {
          "Duration": "7.0-8",
          "ASED": "",
          "EMO-DB": "1",
          "RAVDESS": "",
          "URDU": ""
        },
        {
          "Duration": "8.0-9",
          "ASED": "",
          "EMO-DB": "1",
          "RAVDESS": "",
          "URDU": ""
        },
        {
          "Duration": "STD",
          "ASED": "0.444",
          "EMO-DB": "1.067",
          "RAVDESS": "0",
          "URDU": "0.5"
        },
        {
          "Duration": "Mean",
          "ASED": "2.967",
          "EMO-DB": "2.267",
          "RAVDESS": "3",
          "URDU": "2.5"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Datasets": "Labels",
          "Train": "Positive",
          "Test": "Positive",
          "Validation": "Positive"
        },
        {
          "Datasets": "ASED",
          "Train": "693",
          "Test": "199",
          "Validation": "99"
        },
        {
          "Datasets": "EMODB",
          "Train": "95",
          "Test": "27",
          "Validation": "14"
        },
        {
          "Datasets": "RAVDESS",
          "Train": "456",
          "Test": "140",
          "Validation": "56"
        },
        {
          "Datasets": "URDU",
          "Train": "140",
          "Test": "40",
          "Validation": "20"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: Experiment 1.1: Monolingual SER results for different datasets (train in one language, test in the same",
      "data": [
        {
          "Model": "AlexNet",
          "ASED": "78.71",
          "EMO-DB": "68.52",
          "RAVDESS": "80.63",
          "URDU": "93.75"
        },
        {
          "Model": "VGGE",
          "ASED": "84.76",
          "EMO-DB": "85.19",
          "RAVDESS": "83.13",
          "URDU": "70.00"
        },
        {
          "Model": "ResNet50",
          "ASED": "84.13",
          "EMO-DB": "79.63",
          "RAVDESS": "84.38",
          "URDU": "90.00"
        },
        {
          "Model": "Average",
          "ASED": "82.53",
          "EMO-DB": "77.78",
          "RAVDESS": "82.71",
          "URDU": "84.58"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "AlexNet",
          "ASED": "80.93",
          "EMO-DB": "55.74",
          "RAVDESS": "82.22"
        },
        {
          "Model": "VGGE",
          "ASED": "86.63",
          "EMO-DB": "70.49",
          "RAVDESS": "83.33"
        },
        {
          "Model": "ResNet50",
          "ASED": "85.82",
          "EMO-DB": "73.77",
          "RAVDESS": "77.78"
        },
        {
          "Model": "Average",
          "ASED": "84.46",
          "EMO-DB": "66.67",
          "RAVDESS": "81.11"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "AlexNet",
          "Training": "ASED\nEMO-DB\nASED\nRAVDESS\nASED\nURDU",
          "Testing": "EMO-DB\nASED\nRAVDESS\nASED\nURDU\nASED",
          "Accuracy": "65.80\n62.39\n66.00\n65.87\n60.00\n50.67",
          "F1-score": "56.85\n58.53\n53.17\n55.57\n56.28\n48.45"
        },
        {
          "Model": "Average",
          "Training": "",
          "Testing": "",
          "Accuracy": "61.79%",
          "F1-score": "54.81%"
        },
        {
          "Model": "VGGE",
          "Training": "ASED\nEMO-DB\nASED\nRAVDESS\nASED\nURDU",
          "Testing": "EMO-DB\nASED\nRAVDESS\nASED\nURDU\nASED",
          "Accuracy": "66.67\n64.22\n59.25\n61.43\n59.69\n60.00",
          "F1-score": "52.55\n58.53\n51.85\n62.75\n56.34\n53.94"
        },
        {
          "Model": "Average",
          "Training": "",
          "Testing": "",
          "Accuracy": "61.88%",
          "F1-score": "55.99%"
        },
        {
          "Model": "ResNet50",
          "Training": "ASED\nEMO-DB\nASED\nRAVDESS\nASED\nURDU",
          "Testing": "EMO-DB\nASED\nRAVDESS\nASED\nURDU\nASED",
          "Accuracy": "64.06\n58.72\n61.75\n64.16\n61.56\n61.33",
          "F1-score": "50.42\n45.94\n48.68\n52.66\n62.06\n60.03"
        },
        {
          "Model": "Average",
          "Training": "",
          "Testing": "",
          "Accuracy": "61.93%",
          "F1-score": "53.30%"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 8: ,bottomline)werehigherthanthoseofExperiment2(Table7,bottomline). Latifetal. [23]foundthattrainingon",
      "data": [
        {
          "Model": "AlexNet",
          "Training": "EMO-DB+RAVDESS\nEMO-DB+URDU\nRAVDESS+URDU\nEMO-DB+RAVDESS+URDU",
          "Testing": "ASED\nASED\nASED\nASED",
          "Accuracy": "69.06\n57.23\n62.46\n69.77",
          "F1-score": "61.28\n48.38\n51.27\n62.30"
        },
        {
          "Model": "Average",
          "Training": "",
          "Testing": "",
          "Accuracy": "64.63%",
          "F1-score": "55.81%"
        },
        {
          "Model": "VGGE",
          "Training": "EMO-DB+RAVDESS\nEMO-DB+URDU\nRAVDESS+URDU\nEMO-DB+RAVDESS+URDU",
          "Testing": "ASED\nASED\nASED\nASED",
          "Accuracy": "60.50\n69.94\n66.89\n68.41",
          "F1-score": "61.12\n65.26\n64.56\n60.17"
        },
        {
          "Model": "Average",
          "Training": "",
          "Testing": "",
          "Accuracy": "66.44%",
          "F1-score": "62.78%"
        },
        {
          "Model": "ResNet50",
          "Training": "EMO-DB+RAVDESS\nEMO-DB+URDU\nRAVDESS+URDU\nEMO-DB+RAVDESS+URDU",
          "Testing": "ASED\nASED\nASED\nASED",
          "Accuracy": "61.33\n46.24\n64.51\n63.18",
          "F1-score": "43.52\n44.57\n56.17\n62.32"
        },
        {
          "Model": "Average",
          "Training": "",
          "Testing": "",
          "Accuracy": "58.82%",
          "F1-score": "51.65%"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 8: ,bottomline)werehigherthanthoseofExperiment2(Table7,bottomline). Latifetal. [23]foundthattrainingon",
      "data": [
        {
          "Training": "EMO-DB + RAVDESS\nEMO-DB + URDU\nRAVDESS + URDU\nEMO-DB + RAVDESS + URDU",
          "Testing": "ASED\nASED\nASED\nASED",
          "AlexNet": "69.06\n57.23\n62.46\n69.77",
          "VGGE": "60.5\n69.94\n66.89\n68.41",
          "ResNet50": "61.33\n46.24\n64.51\n63.18",
          "Average accuracy": "63.63\n57.80\n64.62\n67.12"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 8: ,bottomline)werehigherthanthoseofExperiment2(Table7,bottomline). Latifetal. [23]foundthattrainingon",
      "data": [
        {
          "Training": "EMO-DB + RAVDESS\nEMO-DB + URDU\nRAVDESS + URDU\nEMO-DB + RAVDESS + URDU",
          "Testing": "ASED\nASED\nASED\nASED",
          "AlexNet": "61.00\n48.57\n51.64\n62.45",
          "VGGE": "61.21\n65.98\n64.63\n60.28",
          "ResNet50": "43.44\n38.96\n56.23\n62.99",
          "Average F1-score": "55.31\n52.74\n57.33\n59.79"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in never-seen languages using a novel ensemble method with emotion profiles",
      "authors": [
        "E Albornoz",
        "D Milone"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "A classification benchmark for arabic alphabet phonemes with diacritics in deep neural networks",
      "authors": [
        "E Almekhlafi",
        "A.-M Moeen",
        "E Zhang",
        "J Wang",
        "J Peng"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "3",
      "title": "Speaker dependent, speaker independent and cross language emotion recognition from speech using gmm and hmm",
      "authors": [
        "M Bhaykar",
        "J Yadav",
        "K Rao"
      ],
      "year": "2013",
      "venue": "2013 National conference on communications (NCC)"
    },
    {
      "citation_id": "4",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "5",
      "title": "Multiscale amplitude feature and significance of enhanced vocal tract information for emotion classification",
      "authors": [
        "S Deb",
        "S Dandapat"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "6",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "2013 humaine association conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "7",
      "title": "Acoustic sensors in biomedical applications",
      "authors": [
        "N Dey",
        "A Ashour",
        "W Mohamed",
        "N Nguyen"
      ],
      "year": "2019",
      "venue": "Acoustic sensors for biomedical applications"
    },
    {
      "citation_id": "8",
      "title": "Learning multilingual expressive speech representation for prosody prediction without parallel data",
      "authors": [
        "J Duret",
        "T Parcollet",
        "Y Estève"
      ],
      "venue": "Speech Synthesis Workshop (SSW)"
    },
    {
      "citation_id": "9",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "10",
      "title": "An experimental study of the durational characteristics of the voice during the expression of emotion",
      "authors": [
        "G Fairbanks",
        "L Hoaglin"
      ],
      "year": "1941",
      "venue": "Communications Monographs"
    },
    {
      "citation_id": "11",
      "title": "Analysis of emotional speech-a review",
      "authors": [
        "P Gangamohan",
        "S Kadiri",
        "B Yegnanarayana"
      ],
      "year": "2016",
      "venue": "Toward Robotic Socially Believable Behaving Systems"
    },
    {
      "citation_id": "12",
      "title": "Deep transfer learning: A new deep learning glitch classification method for advanced ligo",
      "authors": [
        "D George",
        "H Shen",
        "E Huerta"
      ],
      "year": "2017",
      "venue": "Deep transfer learning: A new deep learning glitch classification method for advanced ligo",
      "arxiv": "arXiv:1706.07446"
    },
    {
      "citation_id": "13",
      "title": "Cross lingual cross corpus speech emotion recognition",
      "authors": [
        "S Goel",
        "H Beigi"
      ],
      "year": "2020",
      "venue": "Cross lingual cross corpus speech emotion recognition",
      "arxiv": "arXiv:2003.07996"
    },
    {
      "citation_id": "14",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "A novel category detection of social media reviews in the restaurant industry",
      "authors": [
        "M Khan",
        "A Javed",
        "M Ihsan",
        "U Tariq"
      ],
      "year": "2020",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "18",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "An optimal two stage feature selection for speech emotion recognition using acoustic features",
      "authors": [
        "S Kuchibhotla",
        "H Vankayalapati",
        "K Anne"
      ],
      "year": "2016",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using mfcc features and lstm network",
      "authors": [
        "H Kumbhar",
        "S Bhandari"
      ],
      "year": "2019",
      "venue": "2019 5th International Conference On Computing, Communication, Control And Automation (ICCUBEA)"
    },
    {
      "citation_id": "21",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "22",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "23",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "24",
      "title": "Cross corpus speech emotion classification-an effective transfer learning technique",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Cross corpus speech emotion classification-an effective transfer learning technique",
      "arxiv": "arXiv:1801.06353"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition from speech by combining databases and fusion of classifiers",
      "authors": [
        "I Lefter",
        "L Rothkrantz",
        "P Wiggers",
        "D Van Leeuwen"
      ],
      "year": "2010",
      "venue": "International Conference on Text, Speech and Dialogue"
    },
    {
      "citation_id": "26",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "27",
      "title": "Cross-corpus arabic and english emotion recognition",
      "authors": [
        "A Meftah",
        "Y Seddiq",
        "Y Alotaibi",
        "S.-A Selouani"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)"
    },
    {
      "citation_id": "28",
      "title": "Text independent amharic language dialect recognition using neuro-fuzzy gaussian membership function",
      "authors": [
        "A Mengistu",
        "M Bedane"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Studies in Computers, Science and Engineering"
    },
    {
      "citation_id": "29",
      "title": "Pruning convolutional neural networks for resource efficient inference",
      "authors": [
        "P Molchanov",
        "S Tyree",
        "T Karras",
        "T Aila",
        "J Kautz"
      ],
      "year": "2016",
      "venue": "Pruning convolutional neural networks for resource efficient inference",
      "arxiv": "arXiv:1611.06440"
    },
    {
      "citation_id": "30",
      "title": "Social network hate speech detection for amharic language",
      "authors": [
        "Z Mossie",
        "J.-H Wang"
      ],
      "year": "2018",
      "venue": "Computer Science & Information Technology"
    },
    {
      "citation_id": "31",
      "title": "Multi-cultural speech emotion recognition using language and speaker cues",
      "authors": [
        "S Pandey",
        "H Shekhawat",
        "S Prasanna"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "32",
      "title": "A new amharic speech emotion dataset and classification benchmark",
      "authors": [
        "E Retta",
        "E Almekhlafi",
        "R Sutcliffe",
        "M Mhamed",
        "H Ali",
        "J Feng"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "33",
      "title": "Enhancing multilingual recognition of emotion in speech by language identification",
      "authors": [
        "H Sagha",
        "P Matejka",
        "M Gavryukova",
        "F Povolnỳ",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Emotion detection from text and speech: a survey",
      "authors": [
        "K Sailunaz",
        "M Dhaliwal",
        "J Rokne",
        "R Alhajj"
      ],
      "year": "2018",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "35",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "36",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "37",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wöllmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Bengali spoken digit classification: A deep learning approach using convolutional neural network",
      "authors": [
        "R Sharmin",
        "S Rahut",
        "M Huq"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition and classification in speech using artificial neural networks",
      "authors": [
        "A Shaw",
        "R Vardhan",
        "S Saxena"
      ],
      "year": "2016",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "40",
      "title": "Speech emotion recognition system: A review",
      "authors": [
        "A Shinde",
        "V Patil"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition system: A review"
    },
    {
      "citation_id": "41",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "42",
      "title": "An actor prepares (new york",
      "authors": [
        "C Stanislavski"
      ],
      "year": "1936",
      "venue": "Theatre Arts"
    },
    {
      "citation_id": "43",
      "title": "Study of feature combination using hmm and svm for multilingual odiya speech emotion recognition",
      "authors": [
        "M Swain",
        "S Sahoo",
        "A Routray",
        "P Kabisatpathy",
        "J Kundu"
      ],
      "year": "2015",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "44",
      "title": "Transfer learning for speech and language processing",
      "authors": [
        "D Wang",
        "T Zheng"
      ],
      "year": "2015",
      "venue": "2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)"
    },
    {
      "citation_id": "45",
      "title": "Wavelet packet analysis for speaker-independent emotion recognition",
      "authors": [
        "K Wang",
        "G Su",
        "L Liu",
        "S Wang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "46",
      "title": "Speech emotion recognition cross language families: Mandarin vs. western languages",
      "authors": [
        "Z Xiao",
        "D Wu",
        "X Zhang",
        "Z Tao"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Progress in Informatics and Computing (PIC)"
    },
    {
      "citation_id": "47",
      "title": "Cross corpus multi-lingual speech emotion recognition using ensemble learning",
      "authors": [
        "W Zehra",
        "A Javed",
        "Z Jalil",
        "H Khan",
        "T Gadekallu"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "48",
      "title": "Cross-corpus acoustic emotion recognition from singing and speaking: A multi-task learning approach",
      "authors": [
        "B Zhang",
        "E Provost",
        "G Essl"
      ],
      "year": "2016",
      "venue": "2016 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "49",
      "title": "Unsupervised learning in cross-corpus acoustic emotion recognition",
      "authors": [
        "Z Zhang",
        "F Weninger",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "50",
      "title": "Ensemble learning of hybrid acoustic features for speech emotion recognition",
      "authors": [
        "K Zvarevashe",
        "O Olugbara"
      ],
      "year": "2020",
      "venue": "Algorithms"
    }
  ]
}