{
  "paper_id": "2504.09221v1",
  "title": "Cmcrd: Cross-Modal Contrastive Representation Distillation For Emotion Recognition",
  "published": "2025-04-12T13:56:20Z",
  "authors": [
    "Siyuan Kan",
    "Huanyu Wu",
    "Zhenyao Cui",
    "Fan Huang",
    "Xiaolong Xu",
    "Dongrui Wu"
  ],
  "keywords": [
    "Affective computing",
    "brain-computer interface",
    "knowledge distilling",
    "cross-modal learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is an important component of affective computing, and also human-machine interaction. Unimodal emotion recognition is convenient, but the accuracy may not be high enough; on the contrary, multi-modal emotion recognition may be more accurate, but it also increases the complexity and cost of the data collection system. This paper considers cross-modal emotion recognition, i.e., using both electroencephalography (EEG) and eye movement in training, but only EEG or eye movement in test. We propose cross-modal contrastive representation distillation (CMCRD), which uses a pre-trained eye movement classification model to assist the training of an EEG classification model, improving feature extraction from EEG signals, or vice versa. During test, only EEG signals (or eye movement signals) are acquired, eliminating the need for multi-modal data. CMCRD not only improves the emotion recognition accuracy, but also makes the system more simplified and practical. Experiments using three different neural network architectures on three multi-modal emotion recognition datasets demonstrated the effectiveness of CMCRD. Compared with the EEG-only model, it improved the average classification accuracy by about 6.2%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion, as a crucial medium for human communication with the external world  [1] , has a huge impact on an individual's perception, communication, decision making, etc. Emotion recognition uses data from one or more modalities to help machine identify and better understand human emotions, which has become an important technology in human-computer interaction and attracted increasing attention  [2] -  [5] .\n\nIn emotion recognition, typical modalities used include facial expressions  [6] ,  [7] , speech  [8] ,  [9] , eye movements  [10] , electroencephalography (EEG)  [11] , and so on. Among them, EEG provides a direct measure of brain activities and the origin of emotions  [12] ,  [13] , and is more difficult to fake. With the development of deep learning, EEG-based emotion recognition has advanced from traditional machine learning approaches like Support Vector Machine and k-Nearest Neighbors  [14]  to deep learning approaches, including Convolutional Neural Networks (CNNs)  [15] ,  [16] , Recurrent Neural Networks  [17] -  [19] , Graph Neural Networks (GNNs)  [20] , Dynamical Graph Convolutional Neural Networks (DGCNN)  [21] , multi-head attention mechanisms  [22] ,  [23] , and so on.\n\nAlthough numerous EEG-based emotion recognition models have been proposed, the performance of EEG-only models remains unreliable, because emotions are influenced by both internal and external activities of the individual. Therefore, researchers started to consider multi-modal fusion, i.e., combine EEG with other modalities  [24] ,  [25] . One example is to combine EEG signals reflecting internal cognitive state with eye movement signals reflecting external subconscious behaviors, which not only improves the accuracy of emotion recognition, but also has high interpretability  [10] ,  [26] ,  [27] .\n\nWhile multi-modal fusion can improve emotion recognition accuracy, it also introduces drawbacks. Collecting multi-modal data is complex and requires multiple devices, which may be inconvenient for daily use  [28] . A multi-modal fusion model typically has more parameters, which increase the computational cost. To address these challenges and leverage the strengths of the EEG signals, we propose cross-modal supervised contrastive representation distillation (CMCRD). Fig.  1  shows a comparison of unimodal, multi-modal and cross-modal emotion recognition. In the cross-modal scenario, a pre-trained eye movement classification model assists the training of an EEG classification model, improving feature extraction from EEG signals, or vice versa. During test, only EEG signals (or eye movement signals) are acquired, eliminating the need for multi-modal data. Our approach not only improves the accuracy of emotion recognition, but also makes the system more simplified and practical. Experiments using three different neural network architectures on three multimodal emotion recognition datasets demonstrated the effectiveness of CMCRD.\n\nThe remainder of this paper is organized as follows: Section 2 presents related works. Section 3 introduces our proposed CM-CRD. Section 4 presents experiment results. Finally, Section 5 draws conclusions and points out future research directions. an overview of several knowledge distillation algorithms.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "Human emotions are a highly complex physiological process. Therefore, multiple modalities are integrated to help improve the accuracy of emotion recognition. Many studies have demonstrated that multi-modal fusion approaches for emotion recognition outperform approaches based on a single modality  [24] ,  [25] ,  [29] . Recent multi-modal emotion recognition studies highlight EEG and eye movement signals as key indicators of external behavior and internal physiology, with a significant increase in research combining these signals for classification. Zheng et al.  [29]  were the first to employ a multi-modal emotion recognition framework at the feature fusion level for the recognition of three-class emotion, demonstrating that eye movement signals and EEG signals are complementary modalities, their features enhancing each other. Inspired by this pioneering work, Lu et al.  [26]  demonstrated the inherent relationship between the two modalities and attempted to further improve emotion recognition accuracy by utilizing various modality fusion strategies. Liu et al.  [30]  employed a multi-modal autoencoder to extract high-dimensional interactions between modalities for feature fusion, aiming to model the interactions of multi-modal physiological signals. Liu et al.  [31]  used weighted summation and attention-based aggregation strategies to capture the interactions between modalities, thereby enabling more accurate emotion recognition. These studies suggest that the fusion of EEG and eye movement signals appears to be a reliable approach that can significantly improve performance.\n\nHowever, multi-modal models also have shortcomings. The challenges in multi-modal data collection and the high hardware requirements have consistently limited the application of multimodal models.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Cross-Modal Emotion Recognition",
      "text": "In recent years, to alleviate the challenges of multi-modal models, more researchers are turning to cross-modal learning, using multimodal data only during training and single-modal data during testing. This approach facilitates model application and deployment. Spampinato et al.  [32]  adopted an RNN-based approach to learn EEG data evoked by visual stimuli and used a CNN-based approach to map images to these EEG representations, allowing automatic visual classification in the brain-based visual object manifold. Jiang et al.  [33]  tried to use cross-modal approaches in the field of emotion recognition. Yan et al.  [34]  were among the first to explore the use of conditional generative adversarial networks, attempting to generate the corresponding EEG signals from eye movement data during testing, thus eliminating the dependence on EEG signals during the testing phase. Fei et al.  [28]  aligned different modalities in a common latent space, using correlation analysis constraints to perform cross-modal tasks, which also demonstrate superior performance. Liu et al.  [35]  developed an adaptive feedback mechanism for cross-modal knowledge distillation to enhance the classification performance of the unimodal student model by allowing the multi-modal teacher model to adjust knowledge transfer adaptively. Du et al.  [36]  applied the Mixtureof-Products-of-Experts (MoPoE) formulation to align the latent representations of images, text, and EEG signals. This alignment enables image and text classifiers to classify latent EEG representations of images, allowing the EEG model to identify categories not used in the training from EEG signals. Fu et al.  [37]  achieved cross-modal learning by means of feature guidance, utilizing the feature re-weighting module.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "Since the introduction of the concept of knowledge distillation  [38] , cross-modal approaches leveraging it have been successfully validated in various fields, such as computer vision and natural language processing. Fully Information Transfer Network (FitNet) achieves knowledge transfer by passing intermediate layer features  [39] . Neuron Selectivity Transfer (NST) guides the student model to imitate the selective activations of neurons in the teacher model to facilitate knowledge transfer  [40] . Probabilistic Knowledge Transfer (PKT) transfers knowledge by guiding the student model to learn the probability distribution of the teacher model  [41] . Tung et al.  [42]  introduced Similarity-preserving (SP) knowledge distillation, which emphasizes the similarity of intermediate layer features. Relational Knowledge Distillation (RKD) focuses on relational information between the teacher and student models  [43] . Tian et al.  [44]  attempted to apply Contrastive Representation Distillation (CRD) to achieve cross-modal transfer from RGB images to depth images, while Arandjelovic et al.  [45]  implemented cross-modal transfer between sound and images by AVE-Net.\n\nHowever, research on utilizing knowledge distillation for cross-modal learning tasks in emotion recognition based on EEG signals remains limited. In the following sections, we will introduce the knowledge distillation approach that we employed to achieve cross-modal learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cmcrd",
      "text": "This section introduces the proposed CMCRD algorithm, which uses eye movement to guide the training of the EEG classifier. The code is available at https://github.com/kssyyy/CMCRD.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview Of Cmcrd",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Teacher Model Training",
      "text": "Only eye movement signals are utilized in training the teacher model, which consists of two parts: a feature extractor h t , and a classifier g t . Both parts are multilayer perceptrons. Minimum class confusion  [46]  is used to reduce the distribution differences among subjects.\n\nLet ŷt i• = [ŷ t i1 , ..., ŷt ic ] ⊤ be the logit outputs of g t for an input x t i , where c is the number of classes. The information entropy is computed as a measure of uncertainty:\n\nH(ŷ t i ) is then transformed into the importance of x t i in computing the class confusion:\n\nHere W ii is the i-th diagonal element of the diagonal weight matrix W, and n is the batch size. The class confusion is defined as:\n\nwhere ŷt •j = [ŷ t 1j , ..., ŷt nj ] ⊤ contains the logits of class j for all n samples in the batch.\n\nCategory normalization is further applied to alleviate classimbalance:\n\nFinally, the minimum class confusion loss is computed as:\n\nThe overall loss function for training the teacher model is:\n\nwhere L cls is the cross-entropy loss, and λ 1 is a trade-off parameter.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Student Model Training",
      "text": "CRD  [44]  is widely employed for cross-modal representation transfer, by enhancing the lower bound of mutual information between the teacher and student model representations. However, CRD requires a large amount of data to construct contrastive learning samples, which may not be available in EEG datasets. Furthermore, it requires the student model to match the teacher's output even if it is wrong, which may cause incorrect knowledge transfer. This subsection introduces CMCRD, an improvement of CRD, to training the student model, which consists of a feature extractor h s and a classifier g s . Let x t i and x s i be the i-th sample from the teacher and student modalities, respectively. Define\n\n, where M is the total number of samples in the dataset. Then, k(T, S), the differences between the input features, can be expressed as:\n\nwhere l t and l s are two linear layers to map the extracted features of the teacher and the student models to the same dimensionality, τ is the temperature used to adjust the concentration level, and N is the number of negative samples (those from different categories) corresponding to each sample. Let p(T, S) be the joint probability distribution, p(T )p(S) be the product of marginal probability distributions, and D be the indicator whether the teacher and student model inputs belong to the same class, i.e., D = 1 if they do, and D = 0 if not. Define q(T, S|D = 1) = p(T, S), (8) q(T, S|D = 0) = p(T )p(S),\n\nand\n\ni.e., E q(T,S|D=1) is the conditional expectation under the joint probability distribution, and E q(T,S|D=0) the conditional expectation under the product of marginal distributions. Then, the lower bound of the mutual information between T and S can be expressed as  [44] :\n\nTo increase the lower bound and hence the performance of the student model, CMCRD minimizes -L(k(T, S)) in its loss function. Additionally, CMCRD also introduces weighted unimodal supervised contrastive learning to maximize the utilization of limited samples.\n\nUnlike CRD, in CMCRD's contrastive learning process, the predictions of the teacher model are constructed into a positive guidance set P and a negative guidance set N:\n\nwhere y i is the true class label of x t i . As illustrated in Fig.  3 , if the teacher model's representation is classified correctly, then the student model should mimic it to improve the classification accuracy. Otherwise, the student model's representation should be different. The CMCRD loss is then:\n\nwhere W ii has been defined in  (2) , and τ is the distillation temperature. Sampling weighting using W ii enables the student model to extract more knowledge from the more confident samples. As will be demonstrated in Section 4.8, sampling weighting improves the student model performance.\n\nThe overall loss function for the student model is:\n\nwhere L cls is the cross-entropy loss, and λ 2 a trade-off parameter.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "This section presents experiment results to validate the performance of CMCRD.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "Three public multi-modal datasets for emotion recognition were used:\n\n1) SEED  [47] ,  [48] : This dataset contains 15 participants (7 males and 8 females), each with three sessions. In each session, each participants watched 15 film clips, with 5 clips from each of the three emotional categories: happy, sad, and neutral. 2) SEED-IV  [4] : This dataset also contains 15 participants (7 males and 8 females), each with three sessions. In each session, each participant watched 24 film clips, with 6 clips from each of the four emotional categories: happy, sad, fear, and neutral. Based on non-overlapping 4-second time windows, differential entropy (DE) features were computed across five frequency bands (δ : 1 -3 Hz, θ : 4 -7 Hz, α : 8 -13 Hz, β : 14 -30 Hz, γ : 31 -50 Hz). Thus, the EEG features had 62 × 5 = 310 dimensions.\n\nThe eye movement signals had 33/31/33 features for the three datasets, respectively, including pupil diameter, saccade dispersion, fixation dispersion, fixation duration, blink duration, saccade duration, saccade amplitude, blink frequency, and maximum fixation duration, among others.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Setup",
      "text": "Following previous studies  [18] ,  [21] , we conducted both withinsubject and cross-subject classification experiments on all three datasets.\n\nIn within-subject classification, for each dataset, data from all three sessions were used. The first 9/16/10 trials were used for training, and the last 6/8/5 trials for testing, for the three datasets, respectively. The average classification accuracies on all subjects are reported, along with their standard deviations.\n\nIn cross-subject classification, leave-one-subject-out crossvalidation was performed. In each iteration, one subject was chosen as the test set while all others were used for training and validation. The average classification accuracies across all subjects, and the standard deviations, are reported.\n\nCMCRD was compared with several representative knowledge distillation algorithms, including CRD  [44] , KD  [38] , FitNet  [39] , NST  [40] , SP  [42] , RKD  [43]  and PKT  [41] . Each algorithm was applied to three representative networks, DNN (6-layer MLP), DGCNN  [20] , and BiHDH  [18] , which have demonstrated effectiveness in EEG emotion classification. L2 regularization was used to alleviate overfitting. All models were trained using mini-batch gradient descent, with learning rate 10 -3 and batch size 32.\n\nFor CMCRD, the trade-off parameters λ 1 in (6) and λ 2 in (15) were set to 0.1 and 0.02, respectively. The temperature coefficient τ in  (7)  was set to 0.07. As the three datasets have different sizes, the number of negative samples N in (11) in within-subject experiments was set to 900, 1,200, and 900 for SEED, SEED-IV, and SEED-V, respectively, and 21,000, 25,000, and 17,000 in cross-subject experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Within-Subject Experiment Results",
      "text": "Table  1  presents the within-subject classification results on the three datasets, including those from single-modal classification (first two rows), baseline algorithms (middle section), and CM-CRD (last row). Compared with EEG-only models, the classification accuracy of CMCRD increased by about 3.5% on SEED, 3% on SEED-IV, and 10% on SEED-V. Compared with other knowledge distillation algorithms, CMCRD achieved the best performance across all datasets on all models. More specifically, on average it outperformed CRD by about 4%, and the best performing baseline by about 2%. Interestingly, Table  1  also shows that, while the eye movement only model did not perform well, knowledge distillation from eye movement still enhanced the EEG classification accuracy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cross-Subject Experiment Results",
      "text": "Table  2  presents the cross-subject classification accuracies on the three datasets. Compared with EEG-only models, CMCRD increased the average classification accuracy by about 3.8% on SEED, 4.5% on SEED-IV, and 6.8% on SEED-V. Compared with other knowledge distillation algorithms, CMCRD achieved the best performance across almost all datasets on all models. More specifically, on average it outperformed CRD by about 2%, and the best performing baseline by about 1.7%.\n\nWe performed paired t-tests to evaluate whether the performance improvements of CMCRD over other knowledge distillation approaches were statistically significantly. The p-values, adjusted by the Benjamini-Hochberg False Discovery Rate correction, are shown in Table  3 . Those smaller than 0.05 are highlighted in bold. Clearly, most performance improvements of CMCRD over others were statistically significant, particularly on SEED-V. This is because SEED-V has the largest number of classes, making the classification most challenging; however, CMCRD can effectively cope with this challenge, resulting in the largest performance improvements.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Feature Visualization",
      "text": "Fig.  4  shows t-SNE  [49]  visualization of features extracted by the DNN model from the second subject in SEED-V. The first row compares the outputs of the DNN model's feature extractor with and without the CMCRD algorithm under the subject-dependent setting, while the second row presents the same comparison under the subject-independent setting.\n\nComparing Fig.  4 (a) and (b), as well as Fig.  4 (c) and (d), it can be observed that the features extracted from CMCRD are more compact within the same cluster, and more separated among different clusters, facilitating classification.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Eeg-Guided Eye Movement Classification",
      "text": "Previous experiments used eye movement signals to guide the training of EEG signals. Table  4  shows the results when EEG signals were used to guide the training of eye movement signals. CMCRD also demonstrated superior performance in this setting. Particularly, in within-subject experiments, CMCRD improved the eye movement only model by about 3.5% on SEED, 5% on SEED-IV, and 4% on SEED-V. In cross-subject experiments, CMCRD improved the eye movement only models by about 2.5% on SEED, 3% on SEED-IV, and 5.5% on SEED-V. On average, CMCRD outperformed CRD by about 2%, and the bestperforming knowledge distillation approach by 1.6%.\n\nThough the EEG only model generally outperformed the eye movement only model (since EEG has many more channels than eye movement), CMCRD achieved comparable performance with the EEG only model. This makes it very promising for real-world applications, as eye movement signals are easier to acquire than EEG signals. These results also demonstrate the versatility of CMCRD.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Multi-Modal Models",
      "text": "We compared EEG-guided eye movement model and eye movement guided EEG model with a multi-modal model that uses decision fusion to combine the predictions of two models. Table  5  shows the results, where EEG+EM denotes the multi-modal model. Overall, the multi-modal model outperformed the eye movement guided EEG model, which in turn outperformed the EEG-guided eye movement model. However, in certain cases, the cross-modal model may outperform, or achieve comparable performance with, the multi-modal model. Table  6  shows the training and testing time of the DNN. The platform was a server with 251 GB RAM, an Intel Xeon Gold 8375C 2.9G Hz CPU and a NVIDIA RTX 3090 GPU. Compared with the multi-modal model, both cross-modal models required less training time, and less than half of the testing time.\n\nIn summary, the cross-modal models may sometimes achieve comparable performance with the multi-modal model, and their computational cost, particularly in testing, is much lower. Additionally, they require fewer devices to collect the physiological signals in testing. So, the cross-modal models may be preferred in real-world applications.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "This subsection performs ablation studies on the DGCNN model to evaluate the contributions of the three modules in CMCRD, i.e., minimal class confusion in teacher model training, and unimodal supervision and information entropy weighting in student model training. Table  7  presents the results, where the three modules are referred to as MCC, US, and IEW, respectively. Regardless of whether other modules are used, both minimal class confusion and unimodal supervision always improved the performance. Their combination offered even larger performance  improvements. The effect of information entropy weighting itself was unstable; however, when used together with unimodal supervision, it consistently improved the performance. In summary, using all three modules together led to the best performance.   knowledge from the more confident samples. During the testing phase, only the EEG (or eye movement) modality is required.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "CMCRD not only improves the emotion recognition accuracy, but also makes the system more simplified and practical. Experiments using three different neural network architectures on three multi-modal emotion recognition datasets demonstrated the effectiveness of CMCRD. Compared with the EEG-only model, it improved the average classification accuracy by about 6.2%. While CMCRD has demonstrated promising performance, it still has some limitations that will be considered in our future work:\n\n1) CMCRD in this paper only considers emotion classification. However, emotion recognition can also be formulated as regression problems in the 3-dimensional space of valance, arousal and dominance. It is meaningful to extend CMCRD from classification to regression. 2) CMCRD in this paper considers only the supervised training scenario. It is also meaningful to extend it to unsupervised and semi-supervised learning scenarios. 3) CMCRD does not explicitly address individual differences, which is pervasive in emotion recognition. It may be integrated with domain adaptation to cope with this challenge.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows a comparison",
      "page": 1
    },
    {
      "caption": "Figure 1: Comparison of unimodal, multi-modal and cross-modal emotion",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the architecture of CMCRD, which includes an",
      "page": 2
    },
    {
      "caption": "Figure 2: The architecture of CMCRD. Solid arrows represent the data ﬂow,",
      "page": 3
    },
    {
      "caption": "Figure 3: , if the teacher model’s representation",
      "page": 4
    },
    {
      "caption": "Figure 3: Contrastive learning in CRD and CMCRD.",
      "page": 4
    },
    {
      "caption": "Figure 4: shows t-SNE [49] visualization of features extracted by the",
      "page": 5
    },
    {
      "caption": "Figure 4: (a) and (b), as well as Fig. 4(c) and (d),",
      "page": 5
    },
    {
      "caption": "Figure 4: t-SNE visualization of features extracted by the DNN model from",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "Abstract—Emotion recognition is an important component of affective computing, and also human-machine interaction. Unimodal"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "emotion recognition is convenient, but the accuracy may not be high enough; on the contrary, multi-modal emotion recognition may be"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "more accurate, but it also increases the complexity and cost of the data collection system. This paper considers cross-modal emotion"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "recognition, i.e., using both electroencephalography (EEG) and eye movement in training, but only EEG or eye movement in test. We"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "propose cross-modal contrastive representation distillation (CMCRD), which uses a pre-trained eye movement classiﬁcation model to"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "assist\nthe training of an EEG classiﬁcation model, improving feature extraction from EEG signals, or vice versa. During test, only EEG"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "signals (or eye movement signals) are acquired, eliminating the need for multi-modal data. CMCRD not only improves the emotion"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "recognition accuracy, but also makes the system more simpliﬁed and practical. Experiments using three different neural network"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "architectures on three multi-modal emotion recognition datasets demonstrated the effectiveness of CMCRD. Compared with the"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "EEG-only model, it improved the average classiﬁcation accuracy by about 6.2%."
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "learning\nIndex Terms—Affective computing, brain-computer interface, knowledge distilling, cross-modal"
        },
        {
          "IEEE\nSiyuan Kan, Huanyu Wu, Zhenyao Cui, Fan Huang, Xiaolong Xu and Dongrui Wu, Fellow,": "✦"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": "modal emotion recognition datasets demonstrated the effectiveness"
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": "of CMCRD."
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": "The remainder of this paper is organized as follows: Section 2"
        },
        {
          "using three different neural network architectures on three multi-": "presents\nrelated works. Section 3 introduces our proposed CM-"
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": "CRD. Section 4 presents\nexperiment\nresults. Finally, Section 5"
        },
        {
          "using three different neural network architectures on three multi-": "draws conclusions and points out future research directions."
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": "2\nRELATED WORK"
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": "This section ﬁrst\nintroduces the commonly used multi-modal and"
        },
        {
          "using three different neural network architectures on three multi-": ""
        },
        {
          "using three different neural network architectures on three multi-": "cross-modal algorithms in emotion recognition, and then provides"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "automatic visual\nclassiﬁcation in the\nbrain-based visual object"
        },
        {
          "2": "manifold.\nJiang et al.\n[33]\ntried to use cross-modal approaches"
        },
        {
          "2": "in the ﬁeld of emotion recognition. Yan et al.\n[34] were among"
        },
        {
          "2": "the ﬁrst\nto explore the use of conditional generative adversarial"
        },
        {
          "2": "networks, attempting to generate the corresponding EEG signals"
        },
        {
          "2": "from eye movement\ndata\nduring\ntesting,\nthus\neliminating\nthe"
        },
        {
          "2": "dependence on EEG signals during the testing phase. Fei et al. [28]"
        },
        {
          "2": "aligned different modalities in a common latent space, using cor-"
        },
        {
          "2": "relation analysis constraints to perform cross-modal\ntasks, which"
        },
        {
          "2": "also demonstrate superior performance. Liu et al. [35] developed"
        },
        {
          "2": "an adaptive feedback mechanism for cross-modal knowledge dis-"
        },
        {
          "2": "tillation to enhance the classiﬁcation performance of the unimodal"
        },
        {
          "2": "student model by allowing the multi-modal teacher model to adjust"
        },
        {
          "2": "knowledge transfer adaptively. Du et al. [36] applied the Mixture-"
        },
        {
          "2": "of-Products-of-Experts\n(MoPoE)\nformulation to align the latent"
        },
        {
          "2": "representations of images,\ntext, and EEG signals. This alignment"
        },
        {
          "2": ""
        },
        {
          "2": "enables image and text classiﬁers to classify latent EEG represen-"
        },
        {
          "2": "tations of images, allowing the EEG model\nto identify categories"
        },
        {
          "2": "not used in the training from EEG signals. Fu et al. [37] achieved"
        },
        {
          "2": ""
        },
        {
          "2": "cross-modal\nlearning by means of\nfeature guidance, utilizing the"
        },
        {
          "2": "feature re-weighting module."
        },
        {
          "2": ""
        },
        {
          "2": "2.3\nKnowledge Distillation"
        },
        {
          "2": ""
        },
        {
          "2": "Since\nthe\nintroduction\nof\nthe\nconcept\nof\nknowledge\ndistilla-"
        },
        {
          "2": ""
        },
        {
          "2": "tion [38],\ncross-modal approaches\nleveraging it have been suc-"
        },
        {
          "2": ""
        },
        {
          "2": "cessfully validated in various ﬁelds, such as computer vision and"
        },
        {
          "2": ""
        },
        {
          "2": "natural\nlanguage processing. Fully Information Transfer Network"
        },
        {
          "2": ""
        },
        {
          "2": "(FitNet)\nachieves\nknowledge\ntransfer\nby\npassing\nintermediate"
        },
        {
          "2": ""
        },
        {
          "2": "layer features [39]. Neuron Selectivity Transfer (NST) guides the"
        },
        {
          "2": ""
        },
        {
          "2": "student model to imitate the selective activations of neurons in the"
        },
        {
          "2": ""
        },
        {
          "2": "teacher model\nto facilitate knowledge transfer\n[40]. Probabilistic"
        },
        {
          "2": ""
        },
        {
          "2": "Knowledge Transfer\n(PKT)\ntransfers knowledge by guiding the"
        },
        {
          "2": ""
        },
        {
          "2": "student model\nto learn the probability distribution of\nthe teacher"
        },
        {
          "2": ""
        },
        {
          "2": "et\nal.\nmodel\n[41]. Tung\n[42]\nintroduced Similarity-preserving"
        },
        {
          "2": ""
        },
        {
          "2": "(SP) knowledge distillation, which emphasizes\nthe similarity of"
        },
        {
          "2": ""
        },
        {
          "2": "intermediate\nlayer\nfeatures. Relational Knowledge Distillation"
        },
        {
          "2": ""
        },
        {
          "2": "(RKD) focuses on relational\ninformation between the teacher and"
        },
        {
          "2": ""
        },
        {
          "2": "student models [43]. Tian et al. [44] attempted to apply Contrastive"
        },
        {
          "2": ""
        },
        {
          "2": "Representation Distillation (CRD) to achieve cross-modal transfer"
        },
        {
          "2": ""
        },
        {
          "2": "from RGB images to depth images, while Arandjelovic et al. [45]"
        },
        {
          "2": ""
        },
        {
          "2": "implemented cross-modal\ntransfer between sound and images by"
        },
        {
          "2": ""
        },
        {
          "2": "AVE-Net."
        },
        {
          "2": ""
        },
        {
          "2": "However,\nresearch\non\nutilizing\nknowledge\ndistillation\nfor"
        },
        {
          "2": ""
        },
        {
          "2": "cross-modal\nlearning tasks in emotion recognition based on EEG"
        },
        {
          "2": ""
        },
        {
          "2": "signals remains limited.\nIn the following sections, we will\nintro-"
        },
        {
          "2": ""
        },
        {
          "2": "duce\nthe knowledge distillation approach that we\nemployed to"
        },
        {
          "2": ""
        },
        {
          "2": "achieve cross-modal learning."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "3\nCMCRD"
        },
        {
          "2": ""
        },
        {
          "2": "This section introduces the proposed CMCRD algorithm, which"
        },
        {
          "2": ""
        },
        {
          "2": "uses eye movement\nto guide the training of\nthe EEG classiﬁer."
        },
        {
          "2": ""
        },
        {
          "2": "The code is available at https://github.com/kssyyy/CMCRD."
        },
        {
          "2": "3.1\nOverview of CMCRD"
        },
        {
          "2": "Fig.\n2\nshows\nthe\narchitecture\nof CMCRD, which\nincludes\nan"
        },
        {
          "2": "eye movement\nclassiﬁcation model\nand\nan EEG classiﬁcation"
        },
        {
          "2": "model. During training, both eye movement and EEG signals are"
        },
        {
          "2": "available;\nthe eye movement\nsignal\nis used to train the teacher"
        },
        {
          "2": "model, which then guides the training of\nthe EEG classiﬁcation"
        },
        {
          "2": "model. During testing, only the EEG input is available, and hence"
        },
        {
          "2": "only the EEG classiﬁcation model is used."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "The overall loss function for training the teacher model is:"
        },
        {
          "3": "(6)\nLt = Lcls + λ1Lmcc,"
        },
        {
          "3": "where Lcls\nis the cross-entropy loss, and λ1 is a trade-off param-"
        },
        {
          "3": "eter."
        },
        {
          "3": "3.3\nStudent Model Training"
        },
        {
          "3": "CRD [44]\nis widely\nemployed\nfor\ncross-modal\nrepresentation"
        },
        {
          "3": "transfer, by enhancing the\nlower bound of mutual\ninformation"
        },
        {
          "3": "between the teacher and student model representations. However,"
        },
        {
          "3": "CRD requires\na\nlarge\namount of data\nto\nconstruct\ncontrastive"
        },
        {
          "3": "learning samples, which may not be available in EEG datasets."
        },
        {
          "3": "Furthermore,\nit requires the student model\nto match the teacher’s"
        },
        {
          "3": "output even if it\nis wrong, which may cause incorrect knowledge"
        },
        {
          "3": "transfer."
        },
        {
          "3": "This subsection introduces CMCRD, an improvement of CRD,"
        },
        {
          "3": "to training the student model, which consists of a feature extractor"
        },
        {
          "3": "hs and a classiﬁer gs."
        },
        {
          "3": "Let xt\nand xs\nbe\nthe\nsample\nfrom the\nteacher\nand\ni-th"
        },
        {
          "3": "i\ni"
        },
        {
          "3": "respectively. Deﬁne T = {ht(xt"
        },
        {
          "3": "student modalities,\nand\ni)}M\ni=1"
        },
        {
          "3": "S = {hs(xs"
        },
        {
          "3": "i)}M\ni=1, where M is the total number of samples in the"
        },
        {
          "3": ""
        },
        {
          "3": "dataset. Then, k(T, S), the differences between the input features,"
        },
        {
          "3": ""
        },
        {
          "3": "can be expressed as:"
        },
        {
          "3": ""
        },
        {
          "3": "t\ns"
        },
        {
          "3": "(T )l\n(S)/τ"
        },
        {
          "3": "el"
        },
        {
          "3": "(7)\n,\nk(T, S) ="
        },
        {
          "3": "elt(T )ls(S)/τ + N"
        },
        {
          "3": "M"
        },
        {
          "3": "where lt and ls are two linear layers to map the extracted features"
        },
        {
          "3": "of the teacher and the student models to the same dimensionality,"
        },
        {
          "3": "τ is the temperature used to adjust the concentration level, and N"
        },
        {
          "3": "is the number of negative samples (those from different categories)"
        },
        {
          "3": "corresponding to each sample."
        },
        {
          "3": "Let p(T, S) be the joint probability distribution, p(T )p(S) be"
        },
        {
          "3": ""
        },
        {
          "3": "the product of marginal probability distributions, and D be the"
        },
        {
          "3": "indicator whether the teacher and student model\ninputs belong to"
        },
        {
          "3": "the same class,\ni.e., D = 1 if they do, and D = 0 if not. Deﬁne"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: presents the cross-subject classification accuracies on",
      "data": [
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": "SEED-IV"
        },
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": "DGCNN"
        },
        {
          "TABLE 1": "68.45±12.31"
        },
        {
          "TABLE 1": "65.84±11.03"
        },
        {
          "TABLE 1": "68.19±15.04"
        },
        {
          "TABLE 1": "69.48±15.57"
        },
        {
          "TABLE 1": "68.67±11.45"
        },
        {
          "TABLE 1": "70.05±13.98"
        },
        {
          "TABLE 1": "70.80±10.43"
        },
        {
          "TABLE 1": "70.26±11.36"
        },
        {
          "TABLE 1": "70.42±10.05"
        },
        {
          "TABLE 1": "72.01±14.66"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: presents the cross-subject classification accuracies on",
      "data": [
        {
          "TABLE 2": ""
        },
        {
          "TABLE 2": ""
        },
        {
          "TABLE 2": ""
        },
        {
          "TABLE 2": "DNN"
        },
        {
          "TABLE 2": "54.09±13.42"
        },
        {
          "TABLE 2": "67.58±7.62"
        },
        {
          "TABLE 2": "58.74±14.39"
        },
        {
          "TABLE 2": "57.79±13.94"
        },
        {
          "TABLE 2": "56.90±14.21"
        },
        {
          "TABLE 2": "59.40±12.69"
        },
        {
          "TABLE 2": "57.24±14.77"
        },
        {
          "TABLE 2": "58.13±15.94"
        },
        {
          "TABLE 2": "56.76±11.82"
        },
        {
          "TABLE 2": "60.08±14.40"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 6: shows the training and testing time of the DNN. The supervision and information entropy weighting in student model",
      "data": [
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "DNN"
        },
        {
          "TABLE 3": "0.2571"
        },
        {
          "TABLE 3": "0.0599"
        },
        {
          "TABLE 3": "0.1261"
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "0.4150"
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "0.0944"
        },
        {
          "TABLE 3": "0.0751"
        },
        {
          "TABLE 3": "0.0644"
        },
        {
          "TABLE 3": "0.3443"
        },
        {
          "TABLE 3": "0.0599"
        },
        {
          "TABLE 3": "0.0082"
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "0.6338"
        },
        {
          "TABLE 3": ""
        },
        {
          "TABLE 3": "0.0606"
        },
        {
          "TABLE 3": "0.0905"
        },
        {
          "TABLE 3": "0.0092"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: shows the training and testing time of the DNN. The supervision and information entropy weighting in student model",
      "data": [
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "DNN"
        },
        {
          "TABLE 4": "63.87±10.06"
        },
        {
          "TABLE 4": "64.34±16.08"
        },
        {
          "TABLE 4": "71.58±11.08"
        },
        {
          "TABLE 4": "69.63±8.69"
        },
        {
          "TABLE 4": "71.22±8.02"
        },
        {
          "TABLE 4": "70.94±8.32"
        },
        {
          "TABLE 4": "67.95±9.17"
        },
        {
          "TABLE 4": "69.76±8.31"
        },
        {
          "TABLE 4": "70.75±7.01"
        },
        {
          "TABLE 4": "72.30±6.59"
        },
        {
          "TABLE 4": "54.09±13.42"
        },
        {
          "TABLE 4": "67.58±7.62"
        },
        {
          "TABLE 4": "71.45±9.24"
        },
        {
          "TABLE 4": "69.51±8.84"
        },
        {
          "TABLE 4": "71.87±9.76"
        },
        {
          "TABLE 4": "71.96±8.56"
        },
        {
          "TABLE 4": "72.14±6.98"
        },
        {
          "TABLE 4": "70.00±7.05"
        },
        {
          "TABLE 4": "71.87±8.92"
        },
        {
          "TABLE 4": "72.34±7.05"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: shows the training and testing time of the DNN. The supervision and information entropy weighting in student model",
      "data": [
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "4.7\nComparison with Multi-Modal Models",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "comparable performance with the multi-modal model, and their"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "computational cost, particularly in testing,\nis much lower. Addi-"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "We compared EEG-guided eye movement model and eye move-",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": ""
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "tionally,\nthey require fewer devices\nto collect\nthe physiological"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "ment guided EEG model with a multi-modal model\nthat uses",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": ""
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "signals in testing. So, the cross-modal models may be preferred in"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "decision fusion to combine the predictions of\ntwo models. Ta-",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": ""
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "real-world applications."
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "ble 5 shows the results, where EEG+EM denotes the multi-modal",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": ""
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "model. Overall,\nthe multi-modal model\noutperformed\nthe\neye",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": ""
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "movement guided EEG model, which in turn outperformed the",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "4.8\nAblation Study"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "EEG-guided\neye movement model. However,\nin\ncertain\ncases,",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "This subsection performs ablation studies on the DGCNN model"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "the cross-modal model may outperform, or achieve comparable",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "to evaluate the contributions of the three modules in CMCRD, i.e.,"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "performance with,\nthe multi-modal model.",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "minimal class confusion in teacher model\ntraining, and unimodal"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "Table 6 shows the training and testing time of the DNN. The",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "supervision and information entropy weighting in student model"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "platform was a server with 251 GB RAM, an Intel Xeon Gold",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "training. Table 7 presents the results, where the three modules are"
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "8375C 2.9G Hz CPU and a NVIDIA RTX 3090 GPU. Compared",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "referred to as MCC, US, and IEW, respectively."
        },
        {
          "69.82±13.96\n72.83±15.15\n77.87±12.73": "with the multi-modal model, both cross-modal models\nrequired",
          "72.34±7.05\n64.17±11.13\n67.39±4.85\n66.38±9.30\n58.79±9.42\n69.47±11.88": "Regardless of whether other modules are used, both minimal"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 6": ""
        },
        {
          "TABLE 6": "Within-Subject"
        },
        {
          "TABLE 6": ""
        },
        {
          "TABLE 6": "Training"
        },
        {
          "TABLE 6": "30.75"
        },
        {
          "TABLE 6": "19.18"
        },
        {
          "TABLE 6": "23.46"
        },
        {
          "TABLE 6": "28.51"
        },
        {
          "TABLE 6": "22.61"
        },
        {
          "TABLE 6": "21.53"
        },
        {
          "TABLE 6": "27.11"
        },
        {
          "TABLE 6": "24.52"
        },
        {
          "TABLE 6": "23.49"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "model training; US: unimodal supervision in student model training;"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "IEW: information entropy weighting in student model training."
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "MCC"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "×"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "X"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "×"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "×"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "X"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "X"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "×"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "X"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "×"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "X"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "×"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "×"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "X"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "X"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "×"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": "X"
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        },
        {
          "Ablation study results (%). MCC: minimal class confusion in teacher": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "[17]\nT. Zhang, W. Zheng, Z. Cui, Y. Zong,\nand Y. Li,\n“Spatial–temporal"
        },
        {
          "8": "IEEE Trans.\non\nrecurrent\nneural\nnetwork\nfor\nemotion\nrecognition,”"
        },
        {
          "8": ""
        },
        {
          "8": "Cybernetics, vol. 49, no. 3, pp. 839–847, 2018."
        },
        {
          "8": ""
        },
        {
          "8": "[18] Y. Li, L. Wang, W. Zheng, Y. Zong, L. Qi, Z. Cui, T. Zhang,\nand"
        },
        {
          "8": ""
        },
        {
          "8": "T. Song, “A novel bi-hemispheric discrepancy model\nfor EEG emotion"
        },
        {
          "8": "IEEE Trans.\non Cognitive\nrecognition,”\nand Developmental Systems,"
        },
        {
          "8": "vol. 13, no. 2, pp. 354–367, 2020."
        },
        {
          "8": ""
        },
        {
          "8": "[19] M. K. Chowdary,\nJ. Anitha, and D. J. Hemanth, “Emotion recognition"
        },
        {
          "8": "from EEG signals using recurrent neural networks,” Electronics, vol. 11,"
        },
        {
          "8": "no. 15, p. 2387, 2022."
        },
        {
          "8": ""
        },
        {
          "8": "[20]\nT. Song, W. Zheng, P. Song,\nand Z. Cui,\n“EEG emotion recognition"
        },
        {
          "8": ""
        },
        {
          "8": "using dynamical graph convolutional neural networks,” IEEE Trans. on"
        },
        {
          "8": ""
        },
        {
          "8": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018."
        },
        {
          "8": "[21]\nP. Zhong, D. Wang, and C. Miao, “EEG-based emotion recognition using"
        },
        {
          "8": "regularized graph neural networks,” IEEE Trans. on Affective Computing,"
        },
        {
          "8": "vol. 13, no. 3, pp. 1290–1301, 2020."
        },
        {
          "8": ""
        },
        {
          "8": "[22] Y. Song, Q. Zheng, B. Liu, and X. Gao, “EEG conformer: Convolutional"
        },
        {
          "8": ""
        },
        {
          "8": "transformer for EEG decoding and visualization,” IEEE Trans. on Neural"
        },
        {
          "8": "Systems and Rehabilitation Engineering, vol. 31, pp. 710–719, 2022."
        },
        {
          "8": "[23] Y. Ding, Y. Li, H. Sun, R. Liu, C. Tong, and C. Guan, “EEG-Deformer:"
        },
        {
          "8": "A dense convolutional\ntransformer for brain-computer interfaces,” IEEE"
        },
        {
          "8": ""
        },
        {
          "8": "Journal of Biomedical and Health Informatics, 2024."
        },
        {
          "8": ""
        },
        {
          "8": "[24]\nJ. Zhu, C. Yang, X. Xie, S. Wei, Y. Li, X. Li,\nand B. Hu,\n“Mutual"
        },
        {
          "8": "information based fusion model (MIBFM): Mild depression recognition"
        },
        {
          "8": "using EEG and pupil area signals,” IEEE Trans. on Affective Computing,"
        },
        {
          "8": ""
        },
        {
          "8": "vol. 14, no. 3, pp. 2102–2115, 2022."
        },
        {
          "8": ""
        },
        {
          "8": "[25] B. Fu, C. Gu, M. Fu, Y. Xia, and Y. Liu, “A novel feature fusion network"
        },
        {
          "8": ""
        },
        {
          "8": "for multimodal\nemotion\nrecognition\nfrom EEG and\neye movement"
        },
        {
          "8": ""
        },
        {
          "8": "signals,” Frontiers in Neuroscience, vol. 17, p. 1234162, 2023."
        },
        {
          "8": ""
        },
        {
          "8": "[26] Y. Lu, W.-L. Zheng, B. Li, and B.-L. Lu, “Combining eye movements"
        },
        {
          "8": ""
        },
        {
          "8": "and EEG to enhance emotion recognition,” in Proc. 24th Int’l Joint Conf."
        },
        {
          "8": ""
        },
        {
          "8": "on Artiﬁcial Intelligence, vol. 15, Buenos Aires, Argentina, Jul. 2015, pp."
        },
        {
          "8": ""
        },
        {
          "8": "1170–1176."
        },
        {
          "8": ""
        },
        {
          "8": "[27]\nJ. Zhu, S. Wei, X. Xie, C. Yang, Y. Li, X. Li, and B. Hu, “Content-based"
        },
        {
          "8": ""
        },
        {
          "8": "multiple evidence fusion on EEG and eye movements for mild depression"
        },
        {
          "8": ""
        },
        {
          "8": "recognition,” Computer Methods and Programs in Biomedicine, vol. 226,"
        },
        {
          "8": ""
        },
        {
          "8": "p. 107100, 2022."
        },
        {
          "8": ""
        },
        {
          "8": "[28] C. Fei, R. Li, L.-M. Zhao, Z. Li, and B.-L. Lu, “A cross-modality deep"
        },
        {
          "8": ""
        },
        {
          "8": "learning method for measuring decision conﬁdence from eye movement"
        },
        {
          "8": ""
        },
        {
          "8": "Int’l Conf. on IEEE Engineering in\nsignals,” in Proc. 2022 44th Annual"
        },
        {
          "8": ""
        },
        {
          "8": "Medicine and Biology Society, Glasgow, UK, Jul. 2022, pp. 3342–3345."
        },
        {
          "8": ""
        },
        {
          "8": "[29] W.-L. Zheng, B.-N. Dong, and B.-L. Lu, “Multimodal emotion recogni-"
        },
        {
          "8": "tion using EEG and eye tracking data,” in 2014 6th Annual Int’l Conf. on"
        },
        {
          "8": "IEEE Engineering in Medicine and Biology Society, Chicago,\nIL, Aug."
        },
        {
          "8": "2014, pp. 5040–5043."
        },
        {
          "8": "[30] W. Liu, W.-L. Zheng, and B.-L. Lu, “Emotion recognition using mul-"
        },
        {
          "8": "23rd\nInt’l Conf.\non Neural\nInformation\ntimodal\ndeep\nlearning,”\nin"
        },
        {
          "8": "Processing.\nKyoto, Japan: Springer, Oct. 2016, pp. 521–529."
        },
        {
          "8": "[31] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition"
        },
        {
          "8": "performance\nand robustness of multimodal deep learning models\nfor"
        },
        {
          "8": "multimodal emotion recognition,” IEEE Trans. on Cognitive and De-"
        },
        {
          "8": "velopmental Systems, vol. 14, no. 2, pp. 715–729, 2021."
        },
        {
          "8": ""
        },
        {
          "8": "[32] C. Spampinato, S. Palazzo,\nI. Kavasidis, D. Giordano, N. Souly,\nand"
        },
        {
          "8": ""
        },
        {
          "8": "M. Shah, “Deep learning human mind for automated visual classiﬁca-"
        },
        {
          "8": ""
        },
        {
          "8": "tion,” in Proc. IEEE Conf. on Computer Vision and Pattern Recognition,"
        },
        {
          "8": ""
        },
        {
          "8": "Honolulu, HI, Jul. 2017, pp. 6809–6817."
        },
        {
          "8": ""
        },
        {
          "8": "[33] H. Jiang, X. Guan, W.-Y. Zhao, L.-M. Zhao, and B.-L. Lu, “Generating"
        },
        {
          "8": ""
        },
        {
          "8": "multimodal features for emotion classiﬁcation from eye movement sig-"
        },
        {
          "8": ""
        },
        {
          "8": "nals.” Australian Journal of\nIntelligent Information Processing Systems,"
        },
        {
          "8": ""
        },
        {
          "8": "vol. 15, no. 3, pp. 59–66, 2019."
        },
        {
          "8": ""
        },
        {
          "8": "[34] X. Yan, L.-M. Zhao, and B.-L. Lu, “Simplifying multimodal emotion"
        },
        {
          "8": ""
        },
        {
          "8": "recognition with single eye movement modality,” in Proc. 29th ACM"
        },
        {
          "8": ""
        },
        {
          "8": "Int’l Conf. on Multimedia, Virtual Event, Oct. 2021, pp. 1057–1063."
        },
        {
          "8": ""
        },
        {
          "8": "[35] Y. Liu, Z.\nJia, and H. Wang, “EmotionKD: A cross-modal knowledge"
        },
        {
          "8": ""
        },
        {
          "8": "distillation framework for emotion recognition based on physiological"
        },
        {
          "8": ""
        },
        {
          "8": "signals,” in Proc. 31st ACM Int’l Conf. on Multimedia, Ottawa, Canada,"
        },
        {
          "8": ""
        },
        {
          "8": "Oct. 2023, pp. 6122–6131."
        },
        {
          "8": ""
        },
        {
          "8": "[36] C. Du, K. Fu, J. Li, and H. He, “Decoding visual neural representations"
        },
        {
          "8": "by multimodal\nlearning of brain-visual-linguistic features,” IEEE Trans."
        },
        {
          "8": "on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10 760–"
        },
        {
          "8": "10 777, 2023."
        },
        {
          "8": "[37] B. Fu, W. Chu, C. Gu, and Y. Liu, “Cross-modal guiding neural network"
        },
        {
          "8": "for multimodal\nemotion\nrecognition\nfrom EEG and\neye movement"
        },
        {
          "8": "signals,” IEEE Journal of Biomedical and Health Informatics, vol. 28,"
        },
        {
          "8": "no. 10, pp. 5865–5876, 2024."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "[17]\nT. Zhang, W. Zheng, Z. Cui, Y. Zong,\nand Y. Li,\n“Spatial–temporal"
        },
        {
          "8": "IEEE Trans.\non\nrecurrent\nneural\nnetwork\nfor\nemotion\nrecognition,”"
        },
        {
          "8": ""
        },
        {
          "8": "Cybernetics, vol. 49, no. 3, pp. 839–847, 2018."
        },
        {
          "8": ""
        },
        {
          "8": "[18] Y. Li, L. Wang, W. Zheng, Y. Zong, L. Qi, Z. Cui, T. Zhang,\nand"
        },
        {
          "8": ""
        },
        {
          "8": "T. Song, “A novel bi-hemispheric discrepancy model\nfor EEG emotion"
        },
        {
          "8": "IEEE Trans.\non Cognitive\nrecognition,”\nand Developmental Systems,"
        },
        {
          "8": "vol. 13, no. 2, pp. 354–367, 2020."
        },
        {
          "8": ""
        },
        {
          "8": "[19] M. K. Chowdary,\nJ. Anitha, and D. J. Hemanth, “Emotion recognition"
        },
        {
          "8": "from EEG signals using recurrent neural networks,” Electronics, vol. 11,"
        },
        {
          "8": "no. 15, p. 2387, 2022."
        },
        {
          "8": ""
        },
        {
          "8": "[20]\nT. Song, W. Zheng, P. Song,\nand Z. Cui,\n“EEG emotion recognition"
        },
        {
          "8": ""
        },
        {
          "8": "using dynamical graph convolutional neural networks,” IEEE Trans. on"
        },
        {
          "8": ""
        },
        {
          "8": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018."
        },
        {
          "8": "[21]\nP. Zhong, D. Wang, and C. Miao, “EEG-based emotion recognition using"
        },
        {
          "8": "regularized graph neural networks,” IEEE Trans. on Affective Computing,"
        },
        {
          "8": "vol. 13, no. 3, pp. 1290–1301, 2020."
        },
        {
          "8": ""
        },
        {
          "8": "[22] Y. Song, Q. Zheng, B. Liu, and X. Gao, “EEG conformer: Convolutional"
        },
        {
          "8": ""
        },
        {
          "8": "transformer for EEG decoding and visualization,” IEEE Trans. on Neural"
        },
        {
          "8": "Systems and Rehabilitation Engineering, vol. 31, pp. 710–719, 2022."
        },
        {
          "8": "[23] Y. Ding, Y. Li, H. Sun, R. Liu, C. Tong, and C. Guan, “EEG-Deformer:"
        },
        {
          "8": "A dense convolutional\ntransformer for brain-computer interfaces,” IEEE"
        },
        {
          "8": ""
        },
        {
          "8": "Journal of Biomedical and Health Informatics, 2024."
        },
        {
          "8": ""
        },
        {
          "8": "[24]\nJ. Zhu, C. Yang, X. Xie, S. Wei, Y. Li, X. Li,\nand B. Hu,\n“Mutual"
        },
        {
          "8": "information based fusion model (MIBFM): Mild depression recognition"
        },
        {
          "8": "using EEG and pupil area signals,” IEEE Trans. on Affective Computing,"
        },
        {
          "8": ""
        },
        {
          "8": "vol. 14, no. 3, pp. 2102–2115, 2022."
        },
        {
          "8": ""
        },
        {
          "8": "[25] B. Fu, C. Gu, M. Fu, Y. Xia, and Y. Liu, “A novel feature fusion network"
        },
        {
          "8": ""
        },
        {
          "8": "for multimodal\nemotion\nrecognition\nfrom EEG and\neye movement"
        },
        {
          "8": ""
        },
        {
          "8": "signals,” Frontiers in Neuroscience, vol. 17, p. 1234162, 2023."
        },
        {
          "8": ""
        },
        {
          "8": "[26] Y. Lu, W.-L. Zheng, B. Li, and B.-L. Lu, “Combining eye movements"
        },
        {
          "8": ""
        },
        {
          "8": "and EEG to enhance emotion recognition,” in Proc. 24th Int’l Joint Conf."
        },
        {
          "8": ""
        },
        {
          "8": "on Artiﬁcial Intelligence, vol. 15, Buenos Aires, Argentina, Jul. 2015, pp."
        },
        {
          "8": ""
        },
        {
          "8": "1170–1176."
        },
        {
          "8": ""
        },
        {
          "8": "[27]\nJ. Zhu, S. Wei, X. Xie, C. Yang, Y. Li, X. Li, and B. Hu, “Content-based"
        },
        {
          "8": ""
        },
        {
          "8": "multiple evidence fusion on EEG and eye movements for mild depression"
        },
        {
          "8": ""
        },
        {
          "8": "recognition,” Computer Methods and Programs in Biomedicine, vol. 226,"
        },
        {
          "8": ""
        },
        {
          "8": "p. 107100, 2022."
        },
        {
          "8": ""
        },
        {
          "8": "[28] C. Fei, R. Li, L.-M. Zhao, Z. Li, and B.-L. Lu, “A cross-modality deep"
        },
        {
          "8": ""
        },
        {
          "8": "learning method for measuring decision conﬁdence from eye movement"
        },
        {
          "8": ""
        },
        {
          "8": "Int’l Conf. on IEEE Engineering in\nsignals,” in Proc. 2022 44th Annual"
        },
        {
          "8": ""
        },
        {
          "8": "Medicine and Biology Society, Glasgow, UK, Jul. 2022, pp. 3342–3345."
        },
        {
          "8": ""
        },
        {
          "8": "[29] W.-L. Zheng, B.-N. Dong, and B.-L. Lu, “Multimodal emotion recogni-"
        },
        {
          "8": "tion using EEG and eye tracking data,” in 2014 6th Annual Int’l Conf. on"
        },
        {
          "8": "IEEE Engineering in Medicine and Biology Society, Chicago,\nIL, Aug."
        },
        {
          "8": "2014, pp. 5040–5043."
        },
        {
          "8": "[30] W. Liu, W.-L. Zheng, and B.-L. Lu, “Emotion recognition using mul-"
        },
        {
          "8": "23rd\nInt’l Conf.\non Neural\nInformation\ntimodal\ndeep\nlearning,”\nin"
        },
        {
          "8": "Processing.\nKyoto, Japan: Springer, Oct. 2016, pp. 521–529."
        },
        {
          "8": "[31] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition"
        },
        {
          "8": "performance\nand robustness of multimodal deep learning models\nfor"
        },
        {
          "8": "multimodal emotion recognition,” IEEE Trans. on Cognitive and De-"
        },
        {
          "8": "velopmental Systems, vol. 14, no. 2, pp. 715–729, 2021."
        },
        {
          "8": ""
        },
        {
          "8": "[32] C. Spampinato, S. Palazzo,\nI. Kavasidis, D. Giordano, N. Souly,\nand"
        },
        {
          "8": ""
        },
        {
          "8": "M. Shah, “Deep learning human mind for automated visual classiﬁca-"
        },
        {
          "8": ""
        },
        {
          "8": "tion,” in Proc. IEEE Conf. on Computer Vision and Pattern Recognition,"
        },
        {
          "8": ""
        },
        {
          "8": "Honolulu, HI, Jul. 2017, pp. 6809–6817."
        },
        {
          "8": ""
        },
        {
          "8": "[33] H. Jiang, X. Guan, W.-Y. Zhao, L.-M. Zhao, and B.-L. Lu, “Generating"
        },
        {
          "8": ""
        },
        {
          "8": "multimodal features for emotion classiﬁcation from eye movement sig-"
        },
        {
          "8": ""
        },
        {
          "8": "nals.” Australian Journal of\nIntelligent Information Processing Systems,"
        },
        {
          "8": ""
        },
        {
          "8": "vol. 15, no. 3, pp. 59–66, 2019."
        },
        {
          "8": ""
        },
        {
          "8": "[34] X. Yan, L.-M. Zhao, and B.-L. Lu, “Simplifying multimodal emotion"
        },
        {
          "8": ""
        },
        {
          "8": "recognition with single eye movement modality,” in Proc. 29th ACM"
        },
        {
          "8": ""
        },
        {
          "8": "Int’l Conf. on Multimedia, Virtual Event, Oct. 2021, pp. 1057–1063."
        },
        {
          "8": ""
        },
        {
          "8": "[35] Y. Liu, Z.\nJia, and H. Wang, “EmotionKD: A cross-modal knowledge"
        },
        {
          "8": ""
        },
        {
          "8": "distillation framework for emotion recognition based on physiological"
        },
        {
          "8": ""
        },
        {
          "8": "signals,” in Proc. 31st ACM Int’l Conf. on Multimedia, Ottawa, Canada,"
        },
        {
          "8": ""
        },
        {
          "8": "Oct. 2023, pp. 6122–6131."
        },
        {
          "8": ""
        },
        {
          "8": "[36] C. Du, K. Fu, J. Li, and H. He, “Decoding visual neural representations"
        },
        {
          "8": "by multimodal\nlearning of brain-visual-linguistic features,” IEEE Trans."
        },
        {
          "8": "on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10 760–"
        },
        {
          "8": "10 777, 2023."
        },
        {
          "8": "[37] B. Fu, W. Chu, C. Gu, and Y. Liu, “Cross-modal guiding neural network"
        },
        {
          "8": "for multimodal\nemotion\nrecognition\nfrom EEG and\neye movement"
        },
        {
          "8": "signals,” IEEE Journal of Biomedical and Health Informatics, vol. 28,"
        },
        {
          "8": "no. 10, pp. 5865–5876, 2024."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "NIPS Deep Learning and Representation Learning Workshop, Montreal,"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Canada, Dec. 2015."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[39] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Ben-"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Information\ngio,\n“Fitnets: Hints\nfor\nthin deep nets,”\nin Proc. Neural"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Processing Systems, Montreal, Canada, Dec. 2014, pp. 1–9."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[40]\nZ. Huang and N. Wang, “Like what you like: Knowledge distill via neu-"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "ron selectivity transfer,” in Proc. Int’l Conf. on Learning Representations,"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "New Orleans, LA, May 2019."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[41] N. Passalis and A. Tefas, “Learning deep representations with probabilis-"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "tic knowledge transfer,” in Proc. European Conf. on Computer Vision,"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Munich, Germany, Sep. 2018, pp. 268–284."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[42]\nF. Tung and G. Mori, “Similarity-preserving knowledge distillation,” in"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Proc.\nIEEE/CVF Int’l Conf. on Computer Vision, Seoul, South Korea,"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Oct. 2019, pp. 1365–1374."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[43] W. Park, D. Kim, Y. Lu, and M. Cho, “Relational knowledge distillation,”"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "in Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition,"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Long Beach, CA, Jun. 2019, pp. 3967–3976."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[44] Y. Tian, D. Krishnan, and P.\nIsola, “Contrastive representation distilla-"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "tion,” in Proc.\nInt’l Conf. on Learning Representations, New Orleans,"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "LA, Apr. 2019."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[45] R. Arandjelovic and A. Zisserman, “Objects that sound,” in Proc. Eur."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Conf. on Computer Vision, Munich, Germany, Sep. 2018, pp. 435–451."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[46] Y.\nJin, X. Wang, M. Long,\nand J. Wang,\n“Minimum class confusion"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "for versatile domain adaptation,” in Proc. European Conf. on Computer"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Vision, Glasgow, UK, Aug. 2020, pp. 464–480."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[47] R.-N. Duan,\nJ.-Y. Zhu, and B.-L. Lu, “Differential entropy feature for"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "EEG-based emotion classiﬁcation,” in 2013 6th IEEE/EMBS Int’l Conf."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "on Neural Engineering, San Diego, CA, May. 2013, pp. 81–84."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[48] W.-L. Zheng and B.-L. Lu, “Investigating critical frequency bands and"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "channels for EEG-based emotion recognition with deep neural networks,”"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "IEEE Trans. on Autonomous Mental Development, vol. 7, no. 3, pp. 162–"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "175, 2015."
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "[49]\nL.\nvan\nder Maaten\nand G. Hinton,\n“Visualizing\ndata\nusing\nt-SNE,”"
        },
        {
          "[38] G. Hinton,\n“Distilling the knowledge\nin a neural network,”\nin Proc.": "Journal of Machine Learning Research, vol. 9, pp. 2579–2605, 2008."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "3",
      "title": "EEG-based emotion recognition in music listening",
      "authors": [
        "Y.-P Lin",
        "C.-H Wang",
        "T.-P Jung",
        "T.-L Wu",
        "S.-K Jeng",
        "J.-R Duann",
        "J.-H Chen"
      ],
      "year": "2010",
      "venue": "IEEE Trans. on Biomedical Engineering"
    },
    {
      "citation_id": "4",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Trans. on Cybernetics"
    },
    {
      "citation_id": "5",
      "title": "Cross-task inconsistency based active learning (CTIAL) for emotion recognition",
      "authors": [
        "Y Xu",
        "X Jiang",
        "D Wu"
      ],
      "year": "2024",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Learning multiscale active facial patches for expression analysis",
      "authors": [
        "L Zhong",
        "Q Liu",
        "P Yang",
        "J Huang",
        "D Metaxas"
      ],
      "year": "2014",
      "venue": "IEEE Trans. on Cybernetics"
    },
    {
      "citation_id": "7",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Int'l Conf. on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Inconsistencybased multi-task cooperative learning for emotion recognition",
      "authors": [
        "Y Xu",
        "Y Cui",
        "X Jiang",
        "Y Yin",
        "J Ding",
        "L Li",
        "D Wu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "M Soleymani",
        "M Pantic",
        "T Pun"
      ],
      "year": "2011",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Affective Brain-Computer Interfaces (aBCIs): A Tutorial",
      "authors": [
        "D Wu",
        "B.-L Lu",
        "B Hu",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Proc. of the IEEE"
    },
    {
      "citation_id": "14",
      "title": "Emotional state classification from EEG data using machine learning approach",
      "authors": [
        "X.-W Wang",
        "D Nie",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "15",
      "title": "Cascade and parallel convolutional recurrent neural networks on EEG-based intention recognition for brain computer interface",
      "authors": [
        "D Zhang",
        "L Yao",
        "X Zhang",
        "S Wang",
        "W Chen",
        "R Boots",
        "B Benatallah"
      ],
      "year": "2018",
      "venue": "Proc. AAAI Conf. on Artificial Ntelligence"
    },
    {
      "citation_id": "16",
      "title": "EEGNet: A compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "17",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2018",
      "venue": "IEEE Trans. on Cybernetics"
    },
    {
      "citation_id": "18",
      "title": "A novel bi-hemispheric discrepancy model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Trans. on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition from EEG signals using recurrent neural networks",
      "authors": [
        "M Chowdary",
        "J Anitha",
        "D Hemanth"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "20",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "EEG conformer: Convolutional transformer for EEG decoding and visualization",
      "authors": [
        "Y Song",
        "Q Zheng",
        "B Liu",
        "X Gao"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "23",
      "title": "EEG-Deformer: A dense convolutional transformer for brain-computer interfaces",
      "authors": [
        "Y Ding",
        "Y Li",
        "H Sun",
        "R Liu",
        "C Tong",
        "C Guan"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "24",
      "title": "Mutual information based fusion model (MIBFM): Mild depression recognition using EEG and pupil area signals",
      "authors": [
        "J Zhu",
        "C Yang",
        "X Xie",
        "S Wei",
        "Y Li",
        "X Li",
        "B Hu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "A novel feature fusion network for multimodal emotion recognition from EEG and eye movement signals",
      "authors": [
        "B Fu",
        "C Gu",
        "M Fu",
        "Y Xia",
        "Y Liu"
      ],
      "year": "2023",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "26",
      "title": "Combining eye movements and EEG to enhance emotion recognition",
      "authors": [
        "Y Lu",
        "W.-L Zheng",
        "B Li",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "Proc. 24th Int'l Joint Conf. on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Content-based multiple evidence fusion on EEG and eye movements for mild depression recognition",
      "authors": [
        "J Zhu",
        "S Wei",
        "X Xie",
        "C Yang",
        "Y Li",
        "X Li",
        "B Hu"
      ],
      "year": "2022",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "28",
      "title": "A cross-modality deep learning method for measuring decision confidence from eye movement signals",
      "authors": [
        "C Fei",
        "R Li",
        "L.-M Zhao",
        "Z Li",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "Proc. 2022 44th Annual Int'l Conf. on IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "29",
      "title": "Multimodal emotion recognition using EEG and eye tracking data",
      "authors": [
        "W.-L Zheng",
        "B.-N Dong",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "Annual Int'l Conf. on IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition using multimodal deep learning",
      "authors": [
        "W Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "23rd Int'l Conf. on Neural Information Processing"
    },
    {
      "citation_id": "31",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "32",
      "title": "Deep learning human mind for automated visual classification",
      "authors": [
        "C Spampinato",
        "S Palazzo",
        "I Kavasidis",
        "D Giordano",
        "N Souly",
        "M Shah"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Generating multimodal features for emotion classification from eye movement signals",
      "authors": [
        "H Jiang",
        "X Guan",
        "W.-Y Zhao",
        "L.-M Zhao",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Australian Journal of Intelligent Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "Simplifying multimodal emotion recognition with single eye movement modality",
      "authors": [
        "X Yan",
        "L.-M Zhao",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proc. 29th ACM Int'l Conf. on Multimedia, Virtual Event"
    },
    {
      "citation_id": "35",
      "title": "EmotionKD: A cross-modal knowledge distillation framework for emotion recognition based on physiological signals",
      "authors": [
        "Y Liu",
        "Z Jia",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Proc. 31st ACM Int'l Conf. on Multimedia"
    },
    {
      "citation_id": "36",
      "title": "Decoding visual neural representations by multimodal learning of brain-visual-linguistic features",
      "authors": [
        "C Du",
        "K Fu",
        "J Li",
        "H He"
      ],
      "year": "2023",
      "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Cross-modal guiding neural network for multimodal emotion recognition from EEG and eye movement signals",
      "authors": [
        "B Fu",
        "W Chu",
        "C Gu",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "38",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Proc. NIPS Deep Learning and Representation Learning Workshop"
    },
    {
      "citation_id": "39",
      "title": "Fitnets: Hints for thin deep nets",
      "authors": [
        "A Romero",
        "N Ballas",
        "S Kahou",
        "A Chassang",
        "C Gatta",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Proc. Neural Information Processing Systems"
    },
    {
      "citation_id": "40",
      "title": "Like what you like: Knowledge distill via neuron selectivity transfer",
      "authors": [
        "Z Huang",
        "N Wang"
      ],
      "year": "2019",
      "venue": "Proc. Int'l Conf. on Learning Representations"
    },
    {
      "citation_id": "41",
      "title": "Learning deep representations with probabilistic knowledge transfer",
      "authors": [
        "N Passalis",
        "A Tefas"
      ],
      "year": "2018",
      "venue": "Proc. European Conf. on Computer Vision"
    },
    {
      "citation_id": "42",
      "title": "Similarity-preserving knowledge distillation",
      "authors": [
        "F Tung",
        "G Mori"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Int'l Conf. on Computer Vision"
    },
    {
      "citation_id": "43",
      "title": "Relational knowledge distillation",
      "authors": [
        "W Park",
        "D Kim",
        "Y Lu",
        "M Cho"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "44",
      "title": "Contrastive representation distillation",
      "authors": [
        "Y Tian",
        "D Krishnan",
        "P Isola"
      ],
      "year": "2019",
      "venue": "Proc. Int'l Conf. on Learning Representations"
    },
    {
      "citation_id": "45",
      "title": "Objects that sound",
      "authors": [
        "R Arandjelovic",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proc. Eur. Conf. on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "Minimum class confusion for versatile domain adaptation",
      "authors": [
        "Y Jin",
        "X Wang",
        "M Long",
        "J Wang"
      ],
      "year": "2020",
      "venue": "Proc. European Conf. on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th IEEE/EMBS Int'l Conf. on Neural Engineering"
    },
    {
      "citation_id": "48",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. on Autonomous Mental Development"
    },
    {
      "citation_id": "49",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}