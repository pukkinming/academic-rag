{
  "paper_id": "2405.07260v1",
  "title": "A Supervised Information Enhanced Multi-Granularity Contrastive Learning Framework For Eeg Based Emotion Recognition",
  "published": "2024-05-12T11:51:00Z",
  "authors": [
    "Xiang Li",
    "Jian Song",
    "Zhigang Zhao",
    "Chunxiao Wang",
    "Dawei Song",
    "Bin Hu"
  ],
  "keywords": [
    "Emotion Recognition",
    "EEG",
    "Contrastive Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This study introduces a novel Supervised Info-enhanced Contrastive Learning framework for EEG based Emotion Recognition (SI-CLEER). SI-CLEER employs multi-granularity contrastive learning to create robust EEG contextual representations, potentially improving emotion recognition effectiveness. Unlike existing methods solely guided by classification loss, we propose a joint learning model combining self-supervised contrastive learning loss and supervised classification loss. This model optimizes both loss functions, capturing subtle EEG signal differences specific to emotion detection. Extensive experiments demonstrate SI-CLEER's robustness and superior accuracy on the SEED dataset compared to state-of-the-art methods. Furthermore, we analyze electrode performance, highlighting the significance of central frontal and temporal brain region EEGs in emotion detection. This study offers an universally applicable approach with potential benefits for diverse EEG classification tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition, a crucial aspect of Affective Computing, provides technology for intelligent emotional decod ing and interactions. EEG-based emotion recognition, with its non-invasive, user-friendly, and high-time-resolution attributes, has made significant strides. However, challenges persist due to EEG's sensitivity to noise and low signal-tonoise ratio (SNR), thus decrease the reliability of emotion recognition systems  [1, 2] . To overcome these challenges, there's a pressing need for developing robust feature extraction and spatio-temporal representation learning method that can effectively capture the distinctive neural signatures linked to various emotional states within EEG signals  [3] .\n\nZhigang Zhao (zhaozg@sdas.org), Chunxiao Wang (wangcx@sdas.org), Dawei Song (dwsong@bit.edu.cn) are corresponding authors. For this purpose, this study introduces a supervised information enhanced contrastive learning framework. By incorporating supervised classification information, it enhances the model's performance and generalization ability in emotion recogntion. The main contributions of this study are as follows: This study presents the first-ever end-to-end joint learning framework that merges self-supervised contrastive learning with supervised classification learning to acquire highquality EEG representations for emotion recognition. Furthermore, this innovative approach is versatile and can be applied to a wide range of EEG-based intelligent analysis tasks beyond just emotion recognition. Based on this framework, we undertake an exploration of how various brain regions and electrodes influence classification accuracy. This effort provides valuable insights to enhance our understanding of the role of different EEG components in emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Framework",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contrastive Representation Learning",
      "text": "Inspired by the idea of Contrastive Learning on time series  [4] , a self-supervised technique, our approach uses an encoder to create an unsupervised, non-parametric function t = Æ’ (s), with ğ‘“ being a deep neural network encoder specialized in extracting feature vectors from EEG signals. By training this encoder ğ‘“, our model minimizes the distance between representations of an input sample ğ‘  and a similar one ğ‘  + in the embedding space, while maximizing the distance from a dissimilar sample ğ‘  -and these points. This concept is formalized in Equation (  1 ), embedding a similarity metric. This metric empowers the model to distinguish various samples in the embedding space effectively, capturing commonalities and disparities among EEG signal samples. This approach results in more comprehensive, informative feature representations, enhancing performance in classifycation tasks.\n\nğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(ğ‘“(ğ‘ ), ğ‘“(ğ‘  + ) â‰« ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(ğ‘“(ğ‘ ), ğ‘“(ğ‘  -)). (  1 )",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Selection Of Contextual Views",
      "text": "In the context of contrastive learning, the selection of positive and negative samples presents a significant challenge. To address this, we employ a dual strategy involving 'Timestamp Masking' and 'Random Cropping' on the input EEG signals. This approach generates a range of diverse contextual perspectives. Within these perspectives, we establish representations sharing the same timestamp as positive sample pairs, contrasting with representations carrying distinct timestamps that form negative sample pairs. This mechanism serves to actively guide the model in acquiring feature representations that effectively discriminate between analogous and dissimilar samples.\n\n1) Random Cropping: Given a time series ğ‘¥ ğ‘– âˆˆ ğ‘… ğ‘‡ Ã—ğ¶ , where ğ‘‡ denotes the number of timestamps and ğ¶ signifies the count of EEG channels, our approach involves the random cropping of two context views, [a1, b1] and [a2, b2]. This cropping occurs along the time axis and adheres to the condition 0 < a1 â‰¤ a2 â‰¤ b1 â‰¤ b2 â‰¤ T . Notably, the overlapping interval [a2, b1] must exhibit consistent representations within both context views. This strategic choice ensures that the model grasps features that sustain coherence across these overlapping segments.\n\n2) Timestamp Masking: For the output vector ğ‘§ ğ‘– = { ğ‘§ ğ‘–,ğ‘¡ } from the input projection layer, we introduce timestamp masking. We use a binary encoding vector n âˆˆ{0, 1} T to independently select mask positions, following a Bernoulli distribution with ğ‘ = 0.5. Importantly, masking occurs in the latent vectors, not the original values. This approach ensures the process unfolds in a high-dimensional vector space, unconstrained by the original data value range. It also prevents the model from disproportionately focusing on specific timestamps, enhancing overall robustness.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Hierarchical And Multi-Granularity Contrast",
      "text": "This approach adopts the hierarchical and multi-granularity constrative learning to captures an array of feature insights across distinct time scales within EEG signals.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dual Contrastive Learning On Single Semantic Level",
      "text": "The Dual Contrastive learning (DCL) Loss integrates Temporal Contrastive Learning (TCL) Loss and Instance-wise Contrastive Learning (ICL) Loss:\n\nwhere ğ‘ is the total instance count, and ğ‘‡ signifies sample length in time steps. The details are as follows.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "1) Temporal Contrastive Learning:",
      "text": "In order to capture the evolving patterns within EEG signals over time, our model leverages the TCL. ğ‘§ ğ‘–,ğ‘¡ and ğ‘§ ğ‘–,ğ‘¡ â€² are positive pairs that denote the latent representations obtained from Encoder Module for the same timestamp ğ‘¡ but from two views of ğ‘¥ ğ‘–,ğ‘¡ . Conversely, representations originating from distinct timestamps are treated as negative pairs. The formulation of the temporal contrastive loss is presented below:\n\nIn this context, â„¦ signifies the collection of timestamps of the overlap part. The symbol ğ• functions as an indicator function, taking the value 1 when ğ‘¡ â‰  ğ‘¡ â€² and 0 when ğ‘¡ = ğ‘¡ â€² .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2) Instance-Wise Contrastive Learning:",
      "text": "In our approach, we define an instance as a sample of length 2 seconds with a 0.2second overlap. The latent representations obtained from Encoder Module for the same timestamp ğ‘¡ but from two views of ğ‘¥ ğ‘–,ğ‘¡ are positive pairs. Whereas, the representation ğ‘§ ğ‘—,ğ‘¡ of diverse instances sharing the same timestamp ğ‘¡ within a given batch as negative samples. The formulation of the ICL is as follows:\n\nHere, ğµ denotes the batch size, and ğ‘— corresponds to the ğ‘—-th EEG signal within the same batch. When ğ‘– â‰  ğ‘—, ğ• is set to 1; otherwise, it's set to 0. The notation ğ‘§ ğ‘—,ğ‘¡ signifies the representation of the ğ‘—-th sample from the first view at timestamp ğ‘¡, while ğ‘§ ğ‘—,ğ‘¡ â€² denotes the representation of the ğ‘—-th sample from the second view at the same timestamp ğ‘¡. These representations are integral in forming negative sample pairs for contrastive learning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Contrastive Learning On Multiple Semantic Levels",
      "text": "The above DCL loss are calculated and recorded on various semantic (resolution) levels, namely multiple max-pooling operations are performed on the input series. After each maxpooling operation, the DCL loss is calculated. Since max-pooling operations decrease the length of the series, this process continues until the series length is reduced to 1. The overall Hierarchical Contrastive Learning (HCL) Loss ğ¿ HCL is the average of the ğ¿ ğ·ğ¶ğ¿ losses calculated after multiple max-pooling operations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "Our model consists of four main components: the preprocessing module, the encoder module, the hierarchical contrastive module, and the downstream classifier, as shown in Figure  1 .\n\nWe will further describe these components in the subsequent sections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "1) Preprocessing Module:",
      "text": "We utilize the MNE  [5]  toolkit for raw data reading. EEG recordings are globally averaged according to the international 10-20 system standard. Bandpass filtering (1 to 49Hz) and notch filtering at 60Hz  [6]  effectively reduce noise. To address mixed activities and interferences from multiple sources in EEG signals, we apply Auto ICA  [7]  to eliminate artifacts. Importantly, this process doesn't necessitate manual intervention or expert knowledge.\n\n2) Encoder Module: The encoder module is a crucial component in EEG signal processing. We selected the Input Projection Layer, Timestamp Masking Module, and Dilated CNN Module as the backbone of the encoder.\n\nâ€¢ Input Projection Module: The Input Projection Layer is a fully connected layer that maps the observed values xi,t at timestamp t to a high-dimensional latent vector zi,t. The purpose of this layer is to transform the original observed values into more meaningful and learnable representations.\n\nâ€¢ Timestamp Masking Module: Randomly masks vector representations, promoting robust feature learning. Enhanced context view generated by masking encourages location-insensitive feature representation learning. Avoiding focusing too much on specific timestamps, and improve the model's ability to generalize on unseen timestamps.\n\nâ€¢ Dilated CNN Module: The dilated CNN module comprises five residual blocks, with each block housing two one-dimensional convolutional layers. This module enhance the capture of longrange dependencies and context information within time series data.\n\n3) Hierarchical Contrastive Module: Each parallelogram in Figure  1  depicts a representation vector at an instance's specific timestamp. This module captures multi-granularity contextual insights across time and instances via temporal and instance-wise contrastive learning as mentioned in Section 2.3.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "4) Classifier Module:",
      "text": "The downstream classifier comprises 1D convolutional, pooling, ReLU, and fully connected layers. While deeper networks often enhance learning performance  [8] , they can impact computation time. Hence, we meticulously crafted a network architecture for faster computation, and parameter fine-tuning during training achieves peak classification performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Optimization Objective",
      "text": "The literature  [9]  introduces a two-step supervised learning algorithm. However, training these components separately risks encoder representations ill-suited for downstream classification tasks. To address this, we propose a joint training approach, optimizing self-supervised contrastive learning and supervised classification concurrently. Guided by both contrastive and classification losses, contrastive learning adapts representations better suited for emotion detection. Specifically, we combine contrastive and classifier losses, as in equation (  4 ):\n\nFor classification, we employ cross-entropy loss as Lclass.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment And Results",
      "text": "Experimental Data and Setting: The effectiveness of our framework is validated on the recognized SEED database  [10] . The SEED database comprises EEG data obtained from 15 Chinese subjects using 62 EEG electrodes while they watched 15 Chinese film clips, each lasting approximately 4 minutes. Each subject participate the experiment 3 times on different days. Three distinct emotions-positive, neutral, and negative-were elicited. We segmented each participant's EEG data into segments of 2 seconds with a 0.2-second overlap. The encoder's input dimension corresponds to the number of EEG channels, with a hidden dimension of 128, and an output representation dimension of 900. The encoder consists of 5 residual blocks in the hidden layers. We adopt the Adam optimizer with a learning rate of 0.001 and conduct training for 50 epochs in each cross-validation fold. ReLU activation functions were applied to all layers of the classifier, while the final layer utilized a Softmax activation function for threeemotion classification task. Recognition Performance: We employ a stratified Kfold cross-validation (SKCV) approach for assessing our model's performance on each subject's data. This method ensures a balanced sampling of three-type emotional samples. The results, averaged across 5 folds, are presented in Table  1 . As demonstrated in Table  2 , we present the performance of our SI-CLEER framework alongside that of several notable works with high Google citations. Our framework has improved emotion recognition performance on the SEED dataset.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Table 2: Performance Comparison",
      "text": "Exploration of Channel Importance: The question of which channels and components in multi-channel EEG are advantageous for emotion recognition has long been a focal point of researchers' attention. Utilizing our framework, we evaluated emotion recognition performance across various channel data and visually presented the results through EEG topographic maps in Figure  3 . It is evident that EEG electrodes that significantly contribute to emotion recognition are distributed primarily in the frontal central regions and bilateral temporal regions. This conclusion is consistent with findings from several studies in emotional-cognitive neuroscience  [17, 18] . The top 5 recognition accuracy rankings ranges from 86.41% to 90.76%. Of course, we can see that each electrode contains useful information for emotion recognition. By utilizing information from all electrodes, an overall effectiveness of 95.45% can be achieved. The training loss curves for all experiments are depicted in Figure  2 (left). Furthermore, as shown in Figure  2 (reight) we employ the t-SNE method to visualize one subject's intermediate representations of EEGs from the SI-CLEER Encoder module in two-dimensional space. In the resulting visualization, green points denote neutral-emotion samples, while red points and blue points denote positive and negative emotion samples, respectively. Our framework effectively separates these samples into groups, with small intra-class distances among samples of the same class and large inter-class distances among samples of different classes. We can see that this intermediate representation is clearly advantageous for downstream classification tasks.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a novel approach that combines selfsupervised contrastive learning with supervised classifica-",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture diagram of the proposed model.",
      "page": 1
    },
    {
      "caption": "Figure 1: We will further describe these components in the subsequent",
      "page": 3
    },
    {
      "caption": "Figure 1: depicts a representation vector at an instan-",
      "page": 3
    },
    {
      "caption": "Figure 3: It is evident that EEG",
      "page": 4
    },
    {
      "caption": "Figure 2: (left). Furthermore, as shown in Figure 2(reight) we",
      "page": 4
    },
    {
      "caption": "Figure 2: Training loss curve of each experimentsâ€™ data (left)",
      "page": 4
    },
    {
      "caption": "Figure 3: Averaged performance (Accuracy) of each individual",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Detailed Recognition Performance on SEED",
      "page": 3
    },
    {
      "caption": "Table 1: As demonstrated in Table 2, we present the perfor-",
      "page": 4
    },
    {
      "caption": "Table 2: Performance Comparison",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Latent factor decoding of multi-channel eeg for emotion recognition through autoencoder-like neural networks",
      "authors": [
        "Xiang Li",
        "Zhigang Zhao",
        "Dawei Song",
        "Yazhou Zhang",
        "Jingshan Pan",
        "Lu Wu",
        "Jidong Huo",
        "Chunyang Niu",
        "Di Wang"
      ],
      "year": "2020",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Long-range correlation analysis of high frequency prefrontal electroencephalogram oscillations for dynamic emotion recognition",
      "authors": [
        "Zhilin Gao",
        "Xingran Cui",
        "Wang Wan",
        "Wenming Zheng",
        "Zhongze Gu"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "3",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "Xiang Li",
        "Yazhou Zhang",
        "Prayag Tiwari",
        "Dawei Song",
        "Bin Hu",
        "Meihong Yang",
        "Zhigang Zhao",
        "Neeraj Kumar",
        "Pekka Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "4",
      "title": "Ts2vec: Towards universal representation of time series",
      "authors": [
        "Zhihan Yue",
        "Yujing Wang",
        "Juanyong Duan",
        "Tianmeng Yang",
        "Congrui Huang",
        "Yunhai Tong",
        "Bixiong Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "MEG and EEG data analysis with MNE-Python",
      "authors": [
        "Alexandre Gramfort",
        "Martin Luessi",
        "Eric Larson",
        "Denis Engemann",
        "Daniel Strohmeier",
        "Christian Brodbeck",
        "Roman Goj",
        "Mainak Jas",
        "Teon Brooks",
        "Lauri Parkkonen",
        "Matti Ha Ma La Ãnen"
      ],
      "year": "2013",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "6",
      "title": "Eeg based brain alertness monitoring by statistical and artifi-cial neural network approach",
      "authors": [
        "Farzana Md Asadur Rahman",
        "; Khanam",
        "Ahmad Mohiuddin"
      ],
      "year": "2019",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "7",
      "title": "Iclabel: An automated electroencephalographic independent component classifier, dataset, and website",
      "authors": [
        "Luca Pion-Tonachini",
        "Ken Kreutz-Delgado",
        "Scott Makeig"
      ],
      "year": "2019",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "8",
      "title": "Deep learning for computational chemistry",
      "authors": [
        "Nathan Garrett B Goh",
        "Abhinav Hodas",
        "Vishnu"
      ],
      "year": "2017",
      "venue": "Journal of computational chemistry"
    },
    {
      "citation_id": "9",
      "title": "10-1 supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Neural Information Processing Systems,Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "11",
      "title": "Hierarchical convolutional neural networks for eeg-based emotion recognition",
      "authors": [
        "Jinpeng Li",
        "Zhaoxiang Zhang",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "12",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Peng Song",
        "Zhen Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "A novel bi-hemispheric discrepancy model for eeg emo-tion recognition",
      "authors": [
        "Yang Li",
        "Lei Wang",
        "Wenming Zheng",
        "Yuan Zong",
        "Lei Qi",
        "Zhen Cui",
        "Tong Zhang",
        "Tengfei Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "14",
      "title": "Data augmentation for enhancing eeg-based emotion recognition with deep generative models",
      "authors": [
        "Yun Luo",
        "Li-Zhen Zhu",
        "Zi-Yu Wan",
        "Bao-Liang Lu"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "15",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "Xiaobing Du",
        "Cuixia Ma",
        "Guanhua Zhang",
        "Jinyao Li",
        "Yu-Kun Lai",
        "Guozhen Zhao",
        "Xiaoming Deng",
        "Yong-Jin Liu",
        "Hongan Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Contrastive representation learning for electroencephalogram classification",
      "authors": [
        "Mostafa Neo Mohsenvand",
        "Mohammad Izadi",
        "Pattie Maes"
      ],
      "year": "2020",
      "venue": "Machine Learning for Health"
    },
    {
      "citation_id": "17",
      "title": "Mapping cerebral sites for emotion and emotional expression with direct cortical electrical stimulation and seizure discharges",
      "authors": [
        "Barry Gordon",
        "John Hart",
        "Ronald Lesser",
        "Santiago Arroyo"
      ],
      "year": "1996",
      "venue": "Progress in brain research"
    },
    {
      "citation_id": "18",
      "title": "Exploring eeg features in cross-subject emotion recognition",
      "authors": [
        "Xiang",
        "Dawei Li",
        "Song",
        "Peng",
        "Yazhou Zhang",
        "Yuexian Zhang",
        "Bin Hou",
        "Hu"
      ],
      "year": "2018",
      "venue": "Frontiers in Neuroscience"
    }
  ]
}