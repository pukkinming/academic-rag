{
  "paper_id": "2004.02355v1",
  "title": "Deep Multilayer Perceptrons For Dimensional Speech Emotion Recognition",
  "published": "2020-04-06T00:03:43Z",
  "authors": [
    "Bagus Tris Atmaja",
    "Masato Akagi"
  ],
  "keywords": [
    "Affective computing",
    "emotion recognition",
    "multilayer perceptrons",
    "neural networks",
    "speech analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Modern deep learning architectures are ordinarily performed on high-performance computing facilities due to the large size of the input features and complexity of its model. This paper proposes traditional multilayer perceptrons (MLP) with deep layers and small input size to tackle that computation requirement limitation. The result shows that our proposed deep MLP outperformed modern deep learning architectures, i.e., LSTM and CNN, on the same number of layers and value of parameters. The deep MLP exhibited the highest performance on both speaker-dependent and speaker-independent scenarios on IEMOCAP and MSP-IMPROV corpus.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition is currently gaining interest from both academia and commercial industries. Researchers in the affective computing field progressively proposed new methods to improve the accuracy of automatic emotion recognition. Commercial industries are trying to make this technology available to the market since its potential applications. Previously, researchers has attempted to implement speechbased emotion recognition for wellbeing detection  [1] , call center application  [2] , and automotive safety  [3] .\n\nOne of the common requirements in computing speech emotion recognition is the availability of high-performance computing since the dataset usually is very large in size, and the classifying methods are complicated. Graphical processing units (GPU)-based computers are often used over CPU-based computers due to its processing speed to process the data, particularly, image-like data.\n\nThis paper proposes the use of deep multilayer perceptrons (MLP) to overcome the requirement of high computing power required by modern deep learning architectures. The inputs are high-level statistical functions (HSF), which are used to reduce the dimension of input features. The outputs are emotion dimensions, i.e., degree of valence, arousal, and dominance.\n\nAccording to research in psychology, dimensional emotion is another view in emotion theories apart from categorical emotion. Russel  [4]  argued that emotion categories could be derived from this dimensional emotion, particularly in valencearousal space. Given the benefit of the ability to convert dimensional emotion to categorical emotion, but not vice versa, predicting the emotion dimension is more beneficial than predicting the emotion category. We added dominance, since it is suggested in  [5] , and the availability of those labels in the datasets. Dimensional emotion recognition are evaluated with deep MLP from speech data since the target applications are speech-based technology like call center and voice assistant applications.\n\nThe contribution of this paper is the evaluation of the classical MLP technique with deep layers compared with modern deep learning techniques, i.e., LSTM and CNN, in terms of concordance correlation coefficient (CCC). The deep neural network is an extension of the neural network with deeper layers, commonly five or more layers  [6] . Our results show that on both speaker-dependent and speaker-independent, in three datasets, deep MLP obtained higher performances than LSTM and CNN. The proposed method worked effectively on the small size of feature, in which, this may be a limitation of our proposed deep MLP method.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Data And Feature Sets",
      "text": "In this section, we describe data and feature sets used in this research.\n\na) IEMOCAP: The interactive emotional dyadic motion capture database is used in this research  [7] . Although the database consists of the measurement of speech, facial expression, head, and movements of affective dyadic sessions, only speech data are processed. The database contains approximately 12 h of data with 10039 utterances. All of those data are used. The dimensional emotion labels are given in valence, arousal, and dominance, in range  [1] [2] [3] [4] [5]  score and normalized into [-1, 1] as in  [8]  when those labels are fed into the neural network system. For speech data, two versions are available in the dataset, stereo data per dialog and mono data per sentence (utterance). We used the mono version since it is easy to process with the labels. The sampling rate of the data was 16 kHz and 16-bit PCM.\n\nWe arranged the IEMOCAP dataset into two scenarios, speaker-dependent (SD) and speaker-independent. On speakerindependent, we split the dataset with ratio 80/20 for training/test set, while in speaker-independent, the last session, i.e., session five, is left for the test set (leave one session out, LOSO). The ratio of dataset partition in the speaker-arXiv:2004.02355v1 [eess.AS] 6 Apr 2020 independent scenario is similar to speaker-dependent split which is shown in Table  I .\n\nb) MSP-IMPROV: We used the MSP-IMPROV corpus to generalize the impact of the evaluated methods. MSP-IMPROV is an acted corpus of dyadic interactions to study emotion perception. This dataset consists of speech and visual recording of 18 hours of affective dyadic sessions. Same as IEMOCAP dataset, we only used the speech data, with 8438 utterances. The same split ratio is used in speaker-dependent scenario while the last session six is used for test set in in speaker-independent scenario, with the same labels scale and normalization. While IEMOCAP labels are annotated by at least two subjects, these MSP-IMPROV labels are annotated by at least five annotators. The speech data were mono, 44 kHz, and 16-bit PCM.\n\nTable  I  shows the number of utterances allocated for each set partition for both speaker-dependent and speaker-independent, including MSP-IMPROV dataset.\n\nc) Mixed-corpus: In addition to the two datasets above, we mixed those two datasets to create a new category of dataset namely mixed-corpus. In mixed-corpus, we concatenated speaker-dependent from IEMOCAP with speakerdependent from MSP-IMPROV for each, training, development and test sets. The same rules also applied for the speakerindependent scenario. d) Acoustic Feature Set: We used high statistical functions of the low-level descriptor (LLD) from Geneva Minimalistic Acoustic Parameter Set (GeMAPS), which is developed by Eyben et al.  [9] . The HSF features are extracted per utterance depend on the given labels, while the LLDs are processed on a frame-based level with 25 ms window size and 10 ms of hop size. The use of HSF feature reduce computation complexity since the feature size decrease from (3409 × 23) to (1 × 23 features), that is, for IEMOCAP dataset. To obtain the HSF feature, however, LLDs must obtained first. Then, HSF can be calculated as statistics of those LLDs for a fixed time, in this case, per utterance.\n\nTo add those functionals, we used a silence feature, which is also extracted per utterance. Silence feature, in this paper, is defined as the portion of the silence frames compared to the total frames in an utterance. In human-human communication, this portion of silence in speaking depends on the speaker's emotion. For example, high arousal emotion category like happy may have fewer silences (or pauses) than a sad emotion category. The ratio of silence per utterance is calculated as where N s is the number of frames to be categorized as silence (silence frames), and N t is the number of total frames within an utterance. To be categorized as silence, a frame is checked whether it is less than a threshold, which is a multiplication of a factor with a root mean square (RMS) energy (X rms ). This RMS energy is formulated as\n\nand X rms is defined as\n\nwhere silence factor of 0.3 is obtained from experiments. These equations are similar to what is proposed in  [10]  and  [11] . In  [10] , the author of that paper used a fixed threshold, while we evaluated some factors of to find the best factor for silence feature in speech emotion recognition. In  [11] , the author divided the total duration of disfluency over the total utterance length on n words and counted it as a disfluency feature.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Benchmarked And Proposed Method",
      "text": "We evaluated three different methods in this paper, LSTM, CNN and MLP. LSTM and CNN are used as baselines, while MLP with deep layers is the proposed method. All evaluated methods used the same numbers of layers, units and value of parameters.\n\nA. Benchmarked Methods: LSTM and CNN LSTM and CNN are two common deep learning architectures widely used in speech emotion recognition  [12] -  [14] . We used those two architectures as the baselines due to its reported effectiveness on predicting valence, arousal, and dominance. Both LSTM and CNN evaluated here have the same five layers with the same number of units. For the size of the kernel in CNN architecture, we determined it in order that the number of trainable parameters is similar. The other parameters, like batch size, feature and label standardization, loss function, number of iterations, and callback criteria, are same for both architectures.\n\nFig.  1  shows both structures of LSTM and CNN. On the first layer, 256 neurons are used and decreased half for the next layers since the models are supposed to learn better along with those layers. Five LSTM layers used tanh as activation function, while five CNN layers used ReLU activation function. We kept all output from the last LTM layer and flatten it before splitting into three dense layers for obtaining the prediction of valence, arousal, and dominance. For the CNN architecture, the same flatten layer is used before entering three one-unit dense layers.\n\nBoth architectures used the same standardization for input features and labels. The z-score normalization is used to standardize feature, while minmax scaler is used to scale the labels into [0, 1] range. We used CCC  [15]  loss as the cost function with multitask (MTL) approach in which the prediction of valence, arousal, and dominance are done simultaneously. While CCC loss (CCCL) is used as the cost function, the following CCC is used to evaluate the performance of recognition.\n\nwhere ρ is Pearson's correlation between gold standard y and and predicted score x, σ is standard deviation, and µ is the mean score. The total loss (L T ) for three variables is then defined as the sum of CCCL for those three with corresponding weighting factors,\n\nwhere α and β are weighting factors for loss function of valence (CCCL V ) and arousal (CCCL A ), respectively. The weighting factors of loss function for dominance (CCCL D ) is obtained by subtracting 1 by the sum of those weighting factors. The CCC is used to evaluate all architectures including the proposed deep MLP method.\n\nAll architectures used a mini-batch size of 200 utterances by shuffling its orders, maximum number iteration of 180, and 10 iterations patience of early stopping criteria (callback). An adam optimizer  [16]  is used to adjust the learning rate during the training process. Both architectures run on GPU-based machine using CuDNN implementation  [17]  within Keras toolkit  [18] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Proposed Method: Deep Mlp",
      "text": "Fig.  2  shows our proposed deep MLP structure. The MLP used here similar to the definition of connectionist learning proposed by Hinton  [19] . As the benchmarked methods, deep MLP also has five layers with the same number of units as previous. The only difference of the layer structure from the previous is the absent of flatten layer since the output of the last MLP layers can be coupled directly to three oneunit dense layers. While the previous LSTM and CNN used batch normalization layer in the beginning (input) to speed-up computation process, this deep MLP structure did not use that layer since the implementation only need a minute to run on a CPU-based machine.\n\nWe used the same batch size, tolerance for early stopping criteria, optimizer, and maximum number of iteration as the benchmarked methods. While the benchmarked methods used CCC the as loss function, the proposed deep MLP method used a mean square error (MSE) as the cost function,\n\nThe total loss function is given as an average of MSE scores from valence, arousal, and dominance,\n\nThere are no weighting factors used here since we do not find a way to implement it via scikit-learn toolkit  [20] , in which the proposed deep MLP is implemented. The same reason applied for the selection of MSE over CCC for the loss function. The Python implementation codes for both proposed and benchmarked methods are available at https: //github.com/bagustris/deep mlp ser.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiment Results And Discussion",
      "text": "CCC is the standard metric used in affective computing to measure the performance of dimensional emotion recognition. We presented our results in that metric in two different groups; within-corpus and mixed-corpus evaluation. The results are shown in Table  III  and IV.\n\nTable  III  shows CCC scores of valence (V), arousal (A), dominance (D) and its average from different datasets, scenarios, and methods. The proposed deep MLP method outperforms benchmarked methods by remarkable margins. On every emotion dimensions and averaged score, the proposed deep MLP gained the highest CCC score for both speakerdependent and speaker-independent scenarios (typed in bold). On IEMOCAP dataset, the score of speaker-dependent is only slightly higher than speaker-independent due to the nature of dataset structure. The utterances in IEMOCAP dataset is already in order by its session when it is sorted by file names.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mlp (128) Mlp (32)",
      "text": "Output  (1)  Output(  1 ) Output(  1  The change from speaker-dependent to speaker-independent is done by changing the number of train/test partitions. In contrast, the file naming of utterances in MSP-IMPROV made the arrangement of the sessions not in order when utterances are sorted by its file names. We did the sorting process to assure the pair of features and labels. The gap between speaker-dependent and speaker-independent in MSP-IMPROV is larger than in IEMOCAP which may be caused by those different files organization. A case where our deep MLP method gained a lower score is on dominance part of MSP-IMPROV speaker-dependent scenario, however, the averaged CCC score, in that case, is still the highest. Table  IV  shows the results from the mixed-corpus dataset. This corpus is concatenation of IEMOCAP with MSP-IMPROV as listed in Table  I , for both speaker-dependent and speaker-independent scenarios. In this mixed-corpus, the proposed deep MLP method also outperformed LSTM and CNN in all emotion dimensions and averaged CCC scores. The score on speaker-dependent in that mixed-corpus is in between the score of speaker-dependent in IEMOCAP and MSP-IMPROV within-corpus. For speaker-independent, the score is lower than in within-corpus. This low score suggested that speaker variability (in different sessions) affected the result, even with the z-normalization process. Instead of predicting one different session (LOSO), the test set in the mixed-corpus consists of two different sessions, each from IEMOCAP and MSP-IMPROV, which made regression task more difficult.\n\nWe showed that our proposed deep MLP functioned to overcome the requirement of modern neural network architectures since it surpassed the results obtained by those architectures. Using a small dimension of feature size, i.e., 47-dimensional data, our deep MLP with five layers, excluding input and output layers, achieved the highest performance. Modern deep learning architectures require high computation hardware, e.g., GPU card, which costs expensive. We showed that using a small deep MLP architecture, which does not require high computation load, better performance can be achieved. Our proposed deep MLP method gained a higher performance than benchmarked methods not only on both within-corpus and mixed-corpus but also on both speaker-dependent and speakerindependent scenarios. Although the proposed method used the different loss function from the benchmarked methods, i.e., MSE versus CCC loss, we presumed that our proposed deep MLP will achieve higher performance if it used the CCC loss since the evaluation metric is CCC. V. CONCLUSIONS This paper demonstrated that the use of deep MLP with proper parameter choices outperformed the more modern neural network architectures with the same number of layers. For both speaker-dependent and speaker-independent, the proposed deep MLP gained the consistent highest performance among evaluated methods. The proposed deep MLP also gained the highest score on both within-corpus and mixedcorpus scenarios. Based on the results of these investigations, there is no high requirements on computing power to obtain outstanding results on dimensional speech emotion recognition. The proper choice of feature (i.e., using small size feature) and the classifier can leverage the performance of conventional neural networks. Future research can be directed to investigate the performance of the proposed method on cross-corpus evaluations, which is not evaluated in this paper.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows both structures of LSTM and CNN. On the",
      "page": 2
    },
    {
      "caption": "Figure 2: shows our proposed deep MLP structure. The MLP",
      "page": 3
    },
    {
      "caption": "Figure 1: Diagram of LSTM and CNN used for benchmarking of proposed deep",
      "page": 3
    },
    {
      "caption": "Figure 2: Diagram of proposed deep MLP with ﬁve layers; the number inside",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "bagus@jaist.ac.jp": "Abstract—Modern deep learning architectures are ordinarily",
          "akagi@jaist.ac.jp": "than predicting the emotion category. We added dominance,"
        },
        {
          "bagus@jaist.ac.jp": "performed on high-performance computing facilities due to the",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "since it\nis suggested in [5], and the availability of those labels"
        },
        {
          "bagus@jaist.ac.jp": "large\nsize\nof\nthe\ninput\nfeatures\nand complexity\nof\nits model.",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "in the datasets. Dimensional emotion recognition are evaluated"
        },
        {
          "bagus@jaist.ac.jp": "This paper proposes\ntraditional multilayer perceptrons\n(MLP)",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "with deep MLP from speech data since the target applications"
        },
        {
          "bagus@jaist.ac.jp": "with deep layers and small\ninput size to tackle that computation",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "requirement limitation. The result shows that our proposed deep",
          "akagi@jaist.ac.jp": "are speech-based technology like call center and voice assistant"
        },
        {
          "bagus@jaist.ac.jp": "MLP\noutperformed modern\ndeep\nlearning\narchitectures,\ni.e.,",
          "akagi@jaist.ac.jp": "applications."
        },
        {
          "bagus@jaist.ac.jp": "LSTM and CNN, on the\nsame number of\nlayers and value of",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "The\ncontribution\nof\nthis\npaper\nis\nthe\nevaluation\nof\nthe"
        },
        {
          "bagus@jaist.ac.jp": "parameters. The deep MLP exhibited the highest performance",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "classical MLP\ntechnique with\ndeep\nlayers\ncompared with"
        },
        {
          "bagus@jaist.ac.jp": "on both speaker-dependent and speaker-independent\nscenarios",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "modern deep learning techniques,\ni.e., LSTM and CNN,\nin"
        },
        {
          "bagus@jaist.ac.jp": "on IEMOCAP and MSP-IMPROV corpus.",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "Index Terms—Affective computing, emotion recognition, mul-",
          "akagi@jaist.ac.jp": "terms of concordance correlation coefﬁcient (CCC). The deep"
        },
        {
          "bagus@jaist.ac.jp": "tilayer perceptrons, neural networks, speech analysis",
          "akagi@jaist.ac.jp": "neural network is\nan extension of\nthe neural network with"
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "deeper\nlayers, commonly ﬁve or more layers [6]. Our\nresults"
        },
        {
          "bagus@jaist.ac.jp": "I.\nINTRODUCTION",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "show that on both speaker-dependent and speaker-independent,"
        },
        {
          "bagus@jaist.ac.jp": "Speech\nemotion\nrecognition\nis\ncurrently\ngaining\ninterest",
          "akagi@jaist.ac.jp": "in three datasets, deep MLP obtained higher performances than"
        },
        {
          "bagus@jaist.ac.jp": "from both academia and commercial\nindustries. Researchers",
          "akagi@jaist.ac.jp": "LSTM and CNN. The proposed method worked effectively on"
        },
        {
          "bagus@jaist.ac.jp": "in the affective computing ﬁeld progressively proposed new",
          "akagi@jaist.ac.jp": "the small size of feature,\nin which,\nthis may be a limitation of"
        },
        {
          "bagus@jaist.ac.jp": "methods to improve the accuracy of automatic emotion recog-",
          "akagi@jaist.ac.jp": "our proposed deep MLP method."
        },
        {
          "bagus@jaist.ac.jp": "nition. Commercial\nindustries are trying to make this technol-",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "ogy available\nto the market\nsince\nits potential\napplications.",
          "akagi@jaist.ac.jp": "II. DATA AND FEATURE SETS"
        },
        {
          "bagus@jaist.ac.jp": "Previously,\nresearchers has\nattempted to implement\nspeech-",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "In this\nsection, we describe data and feature sets used in"
        },
        {
          "bagus@jaist.ac.jp": "based emotion recognition for wellbeing detection [1],\ncall",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "",
          "akagi@jaist.ac.jp": "this research."
        },
        {
          "bagus@jaist.ac.jp": "center application [2], and automotive safety [3].",
          "akagi@jaist.ac.jp": ""
        },
        {
          "bagus@jaist.ac.jp": "One\nof\nthe\ncommon\nrequirements\nin\ncomputing\nspeech",
          "akagi@jaist.ac.jp": "a)\nIEMOCAP: The interactive emotional dyadic motion"
        },
        {
          "bagus@jaist.ac.jp": "emotion\nrecognition\nis\nthe\navailability\nof\nhigh-performance",
          "akagi@jaist.ac.jp": "capture database\nis used in this\nresearch [7]. Although the"
        },
        {
          "bagus@jaist.ac.jp": "computing since the dataset usually is very large in size, and",
          "akagi@jaist.ac.jp": "database\nconsists of\nthe measurement of\nspeech,\nfacial\nex-"
        },
        {
          "bagus@jaist.ac.jp": "the classifying methods are complicated. Graphical processing",
          "akagi@jaist.ac.jp": "pression, head, and movements of affective dyadic sessions,"
        },
        {
          "bagus@jaist.ac.jp": "units (GPU)-based computers are often used over CPU-based",
          "akagi@jaist.ac.jp": "only\nspeech\ndata\nare\nprocessed. The\ndatabase\ncontains\nap-"
        },
        {
          "bagus@jaist.ac.jp": "computers due\nto its processing speed to process\nthe data,",
          "akagi@jaist.ac.jp": "proximately 12 h of data with 10039 utterances. All of\nthose"
        },
        {
          "bagus@jaist.ac.jp": "particularly,\nimage-like data.",
          "akagi@jaist.ac.jp": "data\nare used. The dimensional\nemotion labels are given in"
        },
        {
          "bagus@jaist.ac.jp": "This paper proposes the use of deep multilayer perceptrons",
          "akagi@jaist.ac.jp": "valence,\narousal,\nand\ndominance,\nin\nrange\n[1-5]\nscore\nand"
        },
        {
          "bagus@jaist.ac.jp": "(MLP) to overcome the requirement of high computing power",
          "akagi@jaist.ac.jp": "normalized into [-1, 1]\nas\nin [8] when those\nlabels\nare\nfed"
        },
        {
          "bagus@jaist.ac.jp": "required by modern deep learning architectures. The inputs are",
          "akagi@jaist.ac.jp": "into the neural network system. For speech data,\ntwo versions"
        },
        {
          "bagus@jaist.ac.jp": "high-level statistical functions (HSF), which are used to reduce",
          "akagi@jaist.ac.jp": "are available in the dataset, stereo data per dialog and mono"
        },
        {
          "bagus@jaist.ac.jp": "the\ndimension\nof\ninput\nfeatures. The\noutputs\nare\nemotion",
          "akagi@jaist.ac.jp": "data per sentence (utterance). We used the mono version since"
        },
        {
          "bagus@jaist.ac.jp": "dimensions,\ni.e., degree of valence, arousal, and dominance.",
          "akagi@jaist.ac.jp": "it\nis easy to process with the labels. The sampling rate of\nthe"
        },
        {
          "bagus@jaist.ac.jp": "According to research in psychology, dimensional emotion",
          "akagi@jaist.ac.jp": "data was 16 kHz and 16-bit PCM."
        },
        {
          "bagus@jaist.ac.jp": "is\nanother\nview in\nemotion\ntheories\napart\nfrom categorical",
          "akagi@jaist.ac.jp": "We\narranged\nthe\nIEMOCAP dataset\ninto\ntwo\nscenarios,"
        },
        {
          "bagus@jaist.ac.jp": "emotion. Russel\n[4] argued that emotion categories could be",
          "akagi@jaist.ac.jp": "speaker-dependent (SD) and speaker-independent. On speaker-"
        },
        {
          "bagus@jaist.ac.jp": "derived from this dimensional emotion, particularly in valence-",
          "akagi@jaist.ac.jp": "independent, we split\nthe dataset with ratio 80/20 for\ntrain-"
        },
        {
          "bagus@jaist.ac.jp": "arousal\nspace. Given\nthe\nbeneﬁt\nof\nthe\nability\nto\nconvert",
          "akagi@jaist.ac.jp": "ing/test\nset, while\nin\nspeaker-independent,\nthe\nlast\nsession,"
        },
        {
          "bagus@jaist.ac.jp": "dimensional\nemotion\nto\ncategorical\nemotion,\nbut\nnot\nvice",
          "akagi@jaist.ac.jp": "i.e.,\nsession ﬁve,\nis\nleft\nfor\nthe\ntest\nset\n(leave\none\nsession"
        },
        {
          "bagus@jaist.ac.jp": "versa,\npredicting\nthe\nemotion\ndimension\nis more\nbeneﬁcial",
          "akagi@jaist.ac.jp": "out, LOSO). The\nratio\nof\ndataset\npartition\nin\nthe\nspeaker-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "amplitude, F3, and F3 amplitude."
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "IEMOCAP dataset, we only used the speech data, with 8438",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "mean (of LLDs), standard deviation (of LLDs), silence"
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "utterances. The same split\nratio is used in speaker-dependent",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "scenario while the last\nsession six is used for\ntest\nset\nin in",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "speaker-independent scenario, with the same labels scale and",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "where Ns is the number of frames to be categorized as silence"
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "normalization. While\nIEMOCAP labels\nare\nannotated by at",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "is the number of\ntotal"
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "least\ntwo subjects,\nthese MSP-IMPROV labels are annotated",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "an utterance. To be categorized as silence, a frame is checked"
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "by at\nleast ﬁve annotators. The speech data were mono, 44",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "is\nless"
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "kHz, and 16-bit PCM.",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "of a factor with a root mean square (RMS) energy (Xrms)."
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "Table I shows the number of utterances allocated for each set",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "partition for both speaker-dependent and speaker-independent,",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "including MSP-IMPROV dataset.",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "th = 0.3 × Xrms"
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "c) Mixed-corpus:\nIn addition to the two datasets above,",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "we mixed\nthose\ntwo\ndatasets\nto\ncreate\na\nnew category",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": "is deﬁned as"
        },
        {
          "recording of 18 hours of affective dyadic sessions. Same as": "of dataset namely mixed-corpus.\nIn mixed-corpus, we\ncon-",
          "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "GEMAPS FEATURE [9] AND ITS FUNCTIONALS; ONLY FUNCTIONALS"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "which is shown in Table I.",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "(HSFS) USED IN THIS DIMENSIONAL SER."
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "b) MSP-IMPROV: We used the MSP-IMPROV corpus",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "to\ngeneralize\nthe\nimpact\nof\nthe\nevaluated methods. MSP-",
          "TABLE II": "loudness, alpha ratio, hammarberg index,\nspectral\nslope"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "0-500 Hz,\nspectral\nslope 500-1500 Hz,\nspectral ﬂux, 4"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "IMPROV is an acted corpus of dyadic interactions\nto study",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "MFCCs, F0,\njitter,\nshimmer, Harmonics-to-Noise Ratio"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "emotion perception. This dataset consists of speech and visual",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "(HNR), Harmonic\ndifference H1-H2, Harmonic\ndiffer-"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "recording of 18 hours of affective dyadic sessions. Same as",
          "TABLE II": "ence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "amplitude, F3, and F3 amplitude."
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "IEMOCAP dataset, we only used the speech data, with 8438",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "mean (of LLDs), standard deviation (of LLDs), silence"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "utterances. The same split\nratio is used in speaker-dependent",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "scenario while the last\nsession six is used for\ntest\nset\nin in",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "speaker-independent scenario, with the same labels scale and",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "where Ns is the number of frames to be categorized as silence"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "normalization. While\nIEMOCAP labels\nare\nannotated by at",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "is the number of\ntotal"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "least\ntwo subjects,\nthese MSP-IMPROV labels are annotated",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "an utterance. To be categorized as silence, a frame is checked"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "by at\nleast ﬁve annotators. The speech data were mono, 44",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "is\nless"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "kHz, and 16-bit PCM.",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "of a factor with a root mean square (RMS) energy (Xrms)."
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "Table I shows the number of utterances allocated for each set",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "partition for both speaker-dependent and speaker-independent,",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "including MSP-IMPROV dataset.",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "th = 0.3 × Xrms"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "c) Mixed-corpus:\nIn addition to the two datasets above,",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "we mixed\nthose\ntwo\ndatasets\nto\ncreate\na\nnew category",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "is deﬁned as"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "of dataset namely mixed-corpus.\nIn mixed-corpus, we\ncon-",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "catenated speaker-dependent\nfrom IEMOCAP with speaker-",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "dependent\nfrom MSP-IMPROV for\neach,\ntraining, develop-",
          "TABLE II": "(cid:118)(cid:117)(cid:117)(cid:116)\nx[i]2"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "1 n\nn(cid:88) i\nXrms ="
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "ment and test sets. The same rules also applied for the speaker-",
          "TABLE II": "=1"
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "independent scenario.",
          "TABLE II": ""
        },
        {
          "independent\nscenario\nis\nsimilar\nto\nspeaker-dependent\nsplit": "",
          "TABLE II": "0.3\nfactor\nof\nis\nobtained"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "independent scenario.": ""
        },
        {
          "independent scenario.": ""
        },
        {
          "independent scenario.": ""
        },
        {
          "independent scenario.": ""
        },
        {
          "independent scenario.": ""
        },
        {
          "independent scenario.": "Scenarios"
        },
        {
          "independent scenario.": ""
        },
        {
          "independent scenario.": "IEMOCAP-SD"
        },
        {
          "independent scenario.": "IEMOCAP-LOSO"
        },
        {
          "independent scenario.": "IMPROV-SD"
        },
        {
          "independent scenario.": ""
        },
        {
          "independent scenario.": "IMPROV-LOSO"
        },
        {
          "independent scenario.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "tion, while ﬁve CNN layers used ReLU activation function. We"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "kept all output\nfrom the last LTM layer and ﬂatten it before"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "splitting into three dense layers for obtaining the prediction of"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "valence, arousal, and dominance. For\nthe CNN architecture,"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "the same ﬂatten layer\nis used before entering three one-unit"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "dense layers."
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "Both architectures used the same standardization for\ninput"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "features\nand\nlabels. The\nz-score\nnormalization\nis\nused\nto"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "standardize\nfeature, while minmax\nscaler\nis\nused\nto\nscale"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "the\nlabels\ninto\n[0,\n1]\nrange. We\nused CCC [15]\nloss\nas"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "the\ncost\nfunction with multitask (MTL)\napproach in which"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "the prediction of valence,\narousal,\nand dominance\nare done"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "simultaneously. While CCC loss\n(CCCL)\nis used as\nthe cost"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "function,\nthe following CCC is used to evaluate\nthe perfor-"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "mance of\nrecognition."
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "2ρσxσy"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "CCC =\n(4)"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "σ2\nx + σ2\ny + (µx − µy)2"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "CCCL = 1 − CCC\n(5)"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "where ρ is Pearson’s correlation between gold standard y and"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "and predicted score x, σ is\nstandard deviation, and µ is\nthe"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "mean score. The\ntotal\nloss\nfor\nthree variables\nis\nthen\n(LT )"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "deﬁned as the sum of CCCL for those three with corresponding"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "weighting factors,"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "(6)\nLT = αCCCLV + βCCCLA + (1 − α − β)CCCLD"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "where α and β\nare weighting\nfactors\nfor\nloss\nfunction\nof"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "respectively. The\nvalence (CCCLV ) and arousal\n(CCCLA),"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "weighting factors of\nloss\nfunction for dominance (CCCLD)"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "is obtained by subtracting 1 by the sum of\nthose weighting"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "factors. The CCC is used to evaluate all architectures including"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "the proposed deep MLP method."
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "All architectures used a mini-batch size of 200 utterances"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "by shufﬂing its orders, maximum number iteration of 180, and"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "10 iterations patience of early stopping criteria (callback). An"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "adam optimizer [16] is used to adjust\nthe learning rate during"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "the\ntraining\nprocess. Both\narchitectures\nrun\non GPU-based"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "machine\nusing CuDNN implementation\n[17] within Keras"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "toolkit\n[18]."
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "B. Proposed Method: Deep MLP"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": ""
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "Fig. 2 shows our proposed deep MLP structure. The MLP"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "used here\nsimilar\nto the deﬁnition of\nconnectionist\nlearning"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "proposed by Hinton [19]. As the benchmarked methods, deep"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "MLP also has ﬁve layers with the same number of units as"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "previous. The only difference of\nthe layer structure from the"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "previous\nis\nthe\nabsent\nof ﬂatten\nlayer\nsince\nthe\noutput\nof"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "the\nlast MLP layers\ncan\nbe\ncoupled\ndirectly\nto\nthree\none-"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "unit dense layers. While the previous LSTM and CNN used"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "batch normalization layer in the beginning (input) to speed-up"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "computation process,\nthis deep MLP structure did not use that"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "layer since the implementation only need a minute to run on"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "a CPU-based machine."
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "We used the same batch size,\ntolerance for early stopping"
        },
        {
          "those layers. Five LSTM layers used tanh as activation func-": "criteria, optimizer, and maximum number of\niteration as\nthe"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "The score on speaker-dependent\nin that mixed-corpus is in be-"
        },
        {
          "mean,": "silence",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "tween the score of speaker-dependent in IEMOCAP and MSP-"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "IMPROV within-corpus. For\nspeaker-independent,\nthe\nscore"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "is lower\nthan in within-corpus. This low score suggested that"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "speaker variability (in different\nsessions) affected the result,"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "even with the z-normalization process.\nInstead of predicting"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "one different session (LOSO),\nthe test set\nin the mixed-corpus"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "consists of\ntwo different sessions, each from IEMOCAP and"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "MSP-IMPROV, which made regression task more difﬁcult."
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "We showed that our proposed deep MLP functioned to over-"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "come the requirement of modern neural network architectures"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "since it surpassed the results obtained by those architectures."
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "Using a small dimension of\nfeature size,\ni.e., 47-dimensional"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "data,\nour\ndeep MLP with ﬁve\nlayers,\nexcluding\ninput\nand"
        },
        {
          "mean,": "Output(1)",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "output\nlayers, achieved the highest performance. Modern deep"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "learning architectures require high computation hardware, e.g.,"
        },
        {
          "mean,": "V",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "GPU card, which costs\nexpensive. We\nshowed that using a"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "small deep MLP architecture, which does not\nrequire high"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "computation load, better performance\ncan be\nachieved. Our"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "proposed deep MLP method gained a higher performance than"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "benchmarked methods\nnot\nonly\non\nboth within-corpus\nand"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "mixed-corpus but also on both speaker-dependent and speaker-"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "independent scenarios. Although the proposed method used the"
        },
        {
          "mean,": "the ﬁle naming of utterances in MSP-IMPROV made",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "different\nloss\nfunction from the benchmarked methods,\ni.e.,"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "MSE versus CCC loss, we presumed that our proposed deep"
        },
        {
          "mean,": "by",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "MLP will achieve higher performance if it used the CCC loss"
        },
        {
          "mean,": "the pair of",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "since the evaluation metric is CCC."
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "than in IEMOCAP which may be caused by those",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "TABLE IV"
        },
        {
          "mean,": "ﬁles",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "RESULTS OF CCC SCORES ON MIXED-CORPUS EVALUATION"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "Method\nV\nA\nD\nMean"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "speaker-dependent"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "LSTM\n0.262\n0.518\n0.424\n0.401"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "CNN\n0.198\n0.494\n0.424\n0.372"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "0.395\n0.640\n0.461\n0.499\nMLP"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "speaker-independent\n(LOSO)"
        },
        {
          "mean,": "RESULTS OF CCC SCORES ON WITHIN-CORPUS EVALUATION",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "LSTM\n0.118\n0.270\n0.242\n0.210"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "CNN\n0.073\n0.265\n0.249\n0.196"
        },
        {
          "mean,": "Dataset",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "0.212\n0.402\n0.269\n0.294\nMLP"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "IEMOCAP",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "V. CONCLUSIONS"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "This paper demonstrated that\nthe use of deep MLP with"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "proper\nparameter\nchoices\noutperformed\nthe more modern"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "neural network architectures with the same number of\nlayers."
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "For both speaker-dependent and speaker-independent,\nthe pro-"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "posed deep MLP gained the consistent highest performance"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "MSP-IMPROV",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "among\nevaluated methods.\nThe\nproposed\ndeep MLP\nalso"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "gained the highest\nscore on both within-corpus\nand mixed-"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "corpus scenarios. Based on the results of these investigations,"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": ""
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "there is no high requirements on computing power\nto obtain"
        },
        {
          "mean,": "",
          "CNN in all\nemotion dimensions\nand averaged CCC scores.": "outstanding\nresults\non\ndimensional\nspeech\nemotion\nrecog-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "CNN",
          "0.262": "0.198",
          "0.518\n0.424": "0.494\n0.424",
          "0.401": "0.372"
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "MLP",
          "0.262": "0.395",
          "0.518\n0.424": "0.640\n0.461",
          "0.401": "0.499"
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "TABLE III",
          "the highest.": "",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "",
          "0.262": "speaker-independent",
          "0.518\n0.424": "(LOSO)",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "RESULTS OF CCC SCORES ON WITHIN-CORPUS EVALUATION",
          "is still": "",
          "the highest.": "",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "LSTM",
          "0.262": "0.118",
          "0.518\n0.424": "0.270\n0.242",
          "0.401": "0.210"
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "CNN",
          "0.262": "0.073",
          "0.518\n0.424": "0.265\n0.249",
          "0.401": "0.196"
        },
        {
          "CCC score,": "Dataset",
          "in that case,": "Method",
          "is still": "V",
          "the highest.": "A",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "MLP",
          "0.262": "0.212",
          "0.518\n0.424": "0.402\n0.269",
          "0.401": "0.294"
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "speaker-dependent",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "LSTM",
          "is still": "0.222",
          "the highest.": "0.508",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "CNN",
          "is still": "0.086",
          "the highest.": "0.433",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "MLP",
          "is still": "0.335",
          "the highest.": "0.599",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "IEMOCAP",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "V. CONCLUSIONS",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "speaker-independent",
          "the highest.": "",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "LSTM",
          "is still": "0.210",
          "the highest.": "0.474",
          "LSTM": "This paper demonstrated that",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": "the use of deep MLP with"
        },
        {
          "CCC score,": "",
          "in that case,": "CNN",
          "is still": "0.113",
          "the highest.": "0.460",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "parameter",
          "0.262": "choices",
          "0.518\n0.424": "outperformed",
          "0.401": "the more modern"
        },
        {
          "CCC score,": "",
          "in that case,": "MLP",
          "is still": "0.316",
          "the highest.": "0.488",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "neural network architectures with the same number of",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": "layers."
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "speaker-dependent",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "LSTM",
          "is still": "0.392",
          "the highest.": "0.629",
          "LSTM": "For both speaker-dependent and speaker-independent,",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": "the pro-"
        },
        {
          "CCC score,": "",
          "in that case,": "CNN",
          "is still": "0.346",
          "the highest.": "0.623",
          "LSTM": "posed deep MLP gained the consistent highest performance",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "MLP",
          "is still": "0.438",
          "the highest.": "0.650",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "MSP-IMPROV",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "",
          "0.262": "evaluated methods.",
          "0.518\n0.424": "The\nproposed",
          "0.401": "deep MLP\nalso"
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "speaker-independent",
          "the highest.": "",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "LSTM",
          "is still": "0.210",
          "the highest.": "0.483",
          "LSTM": "gained the highest",
          "0.262": "",
          "0.518\n0.424": "score on both within-corpus",
          "0.401": "and mixed-"
        },
        {
          "CCC score,": "",
          "in that case,": "CNN",
          "is still": "0.216",
          "the highest.": "0.524",
          "LSTM": "corpus scenarios. Based on the results of these investigations,",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "MLP",
          "is still": "0.269",
          "the highest.": "0.551",
          "LSTM": "",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": ""
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "there is no high requirements on computing power",
          "0.262": "",
          "0.518\n0.424": "",
          "0.401": "to obtain"
        },
        {
          "CCC score,": "",
          "in that case,": "",
          "is still": "",
          "the highest.": "",
          "LSTM": "results",
          "0.262": "on",
          "0.518\n0.424": "dimensional\nspeech",
          "0.401": "emotion\nrecog-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "[1] Y.\nGao,\nZ.\nPan,\nH. Wang,\nand\nG.\nChen,\n“Alexa, My\nLove:"
        },
        {
          "REFERENCES": "2018\nIEEE SmartWorld,\nAnalyzing Reviews\nof Amazon Echo,”\nin"
        },
        {
          "REFERENCES": "Ubiquitous\nIntell.\nComput.\nAdv.\nTrust.\nComput.\nScalable\nComput."
        },
        {
          "REFERENCES": "Commun.\nCloud\nBig\nData\nComput.\nInternet\nPeople\nSmart\nCity"
        },
        {
          "REFERENCES": "Innov.\nIEEE,\noct\n2018,\npp.\n372–380.\n[Online]. Available:\nhttps:"
        },
        {
          "REFERENCES": "//ieeexplore.ieee.org/document/8560072/"
        },
        {
          "REFERENCES": "[2] V. A. Petrushin, “Emotion In Speech: Recognition And Application To"
        },
        {
          "REFERENCES": "Call Centers,” Proc. Artif. neural networks Eng., vol. 710, pp. 22–30,"
        },
        {
          "REFERENCES": "1999."
        },
        {
          "REFERENCES": "[3] C. Nass,\nI. M.\nJonsson, H. Harris, B. Reaves,\nJ. Endo, S. Brave, and"
        },
        {
          "REFERENCES": "L. Takayama, “Improving automotive safety by pairing driver emotion"
        },
        {
          "REFERENCES": "and car voice emotion,” in Conf. Hum. Factors Comput. Syst.\n- Proc.,"
        },
        {
          "REFERENCES": "2005."
        },
        {
          "REFERENCES": "[4]\nJ. A. Russell, “Affective space is bipolar,” J. Pers. Soc. Psychol., 1979."
        },
        {
          "REFERENCES": "[5]\nI. Bakker, T. van der Voordt, P. Vink, and J. de Boon, “Pleasure, Arousal,"
        },
        {
          "REFERENCES": "Dominance: Mehrabian and Russell\nrevisited,” Curr. Psychol., vol. 33,"
        },
        {
          "REFERENCES": "no. 3, pp. 405–421, 2014."
        },
        {
          "REFERENCES": "[6] B. T. Atmaja, D. Ariﬁanto,\nand M. Akagi,\n“Speech\nrecognition\non"
        },
        {
          "REFERENCES": "Indonesian language by using time delay neural network,” ASJ Spring"
        },
        {
          "REFERENCES": "Meet., pp. 1291–1294, 2019."
        },
        {
          "REFERENCES": "[7] C. Busso, M. Bulut, C.-C. C. Lee, A. Kazemzadeh, E. Mower, S. Kim,"
        },
        {
          "REFERENCES": "J. N. Chang, S. Lee,\nand S. S. Narayanan,\n“IEMOCAP:\nInteractive"
        },
        {
          "REFERENCES": "emotional dyadic motion capture database,” Lang. Resour. Eval., vol. 42,"
        },
        {
          "REFERENCES": "no. 4, pp. 335–359, 2008."
        },
        {
          "REFERENCES": "[8]\nS. Parthasarathy and C. Busso, “Jointly Predicting Arousal, Valence and"
        },
        {
          "REFERENCES": "Dominance with Multi-Task Learning,” in Interspeech, 2017, pp. 1103–"
        },
        {
          "REFERENCES": "1107."
        },
        {
          "REFERENCES": "[9]\nF. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andre, C. Busso,"
        },
        {
          "REFERENCES": "L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan, and K. P. Truong,"
        },
        {
          "REFERENCES": "“The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice"
        },
        {
          "REFERENCES": "Research and Affective Computing,” IEEE Trans. Affect. Comput., vol. 7,"
        },
        {
          "REFERENCES": "no. 2, pp. 190–202, apr 2016."
        },
        {
          "REFERENCES": "[10] G. Sahu and D. R. Cheriton, “Multimodal Speech Emotion Recognition"
        },
        {
          "REFERENCES": "and Ambiguity\nResolution,”\nTech.\nRep.\n[Online]. Available:\nhttp:"
        },
        {
          "REFERENCES": "//tinyurl.com/y55dlc3m"
        },
        {
          "REFERENCES": "[11]\nJ. D. Moore, L. Tian, and C. Lai, “Word-level emotion recognition using"
        },
        {
          "REFERENCES": "high-level features,” in Int. Conf. Intell. Text Process. Comput. Linguist."
        },
        {
          "REFERENCES": "Springer, 2014, pp. 17–31."
        },
        {
          "REFERENCES": "[12] Y. Xie, R. Liang, Z. Liang, and L. Zhao, “Attention-based dense LSTM"
        },
        {
          "REFERENCES": "for\nspeech emotion recognition,” IEICE Trans.\nInf. Syst., vol. E102D,"
        },
        {
          "REFERENCES": "no. 7, pp. 1426–1429, 2019."
        },
        {
          "REFERENCES": "[13] M. Schmitt, N. Cummins,\nand B. W. Schuller,\n“Continuous Emotion"
        },
        {
          "REFERENCES": "Recognition in Speech Do We Need Recurrence?” in Interspeech 2019."
        },
        {
          "REFERENCES": "ISCA:\nISCA, sep 2019, pp. 2808–2812."
        },
        {
          "REFERENCES": "[14] B. T. Atmaja and M. Akagi, “Speech Emotion Recognition Based on"
        },
        {
          "REFERENCES": "Speech Segment Using LSTM with Attention Model,”\nin 2019 IEEE"
        },
        {
          "REFERENCES": "Int. Conf. Signals Syst.\nIEEE,\njul 2019, pp. 40–44."
        },
        {
          "REFERENCES": "[15]\nL.\nI.-K. Lin, “A concordance correlation coefﬁcient\nto evaluate repro-"
        },
        {
          "REFERENCES": "ducibility,” Biometrics, vol. 45, no. 1, pp. 255–68, 1989."
        },
        {
          "REFERENCES": "[16] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”"
        },
        {
          "REFERENCES": "3rd Int. Conf. Learn. Represent.\nICLR 2015 - Conf. Track Proc., dec"
        },
        {
          "REFERENCES": "2014.\n[Online]. Available: http://arxiv.org/abs/1412.6980"
        },
        {
          "REFERENCES": "[17]\nS. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,"
        },
        {
          "REFERENCES": "and E. Shelhamer,\n“cuDNN: Efﬁcient Primitives\nfor Deep Learning,”"
        },
        {
          "REFERENCES": "Tech. Rep., oct 2014."
        },
        {
          "REFERENCES": "[18]\nF. Chollet and Others, “Keras,” https://keras.io, 2015."
        },
        {
          "REFERENCES": "[19] G. E. Hinton, “Connectionist\nlearning procedures,” Artif. Intell., vol. 40,"
        },
        {
          "REFERENCES": "no. 1-3, pp. 185–234, 1989."
        },
        {
          "REFERENCES": "[20]\nF.\nPedregosa, G. Varoquaux, A. Gramfort, V. Michel, B.\nThirion,"
        },
        {
          "REFERENCES": "O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-"
        },
        {
          "REFERENCES": "plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-"
        },
        {
          "REFERENCES": "nay, “Scikit-learn: Machine Learning in Python,” J. Mach. Learn. Res.,"
        },
        {
          "REFERENCES": "vol. 12, pp. 2825–2830, 2011."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Analyzing Reviews of Amazon Echo",
      "authors": [
        "Y Gao",
        "Z Pan",
        "H Wang",
        "G Chen",
        "My Alexa",
        "Love"
      ],
      "year": "2018",
      "venue": "2018 IEEE SmartWorld"
    },
    {
      "citation_id": "2",
      "title": "Emotion In Speech: Recognition And Application To Call Centers",
      "authors": [
        "V Petrushin"
      ],
      "year": "1999",
      "venue": "Proc. Artif. neural networks Eng"
    },
    {
      "citation_id": "3",
      "title": "Improving automotive safety by pairing driver emotion and car voice emotion",
      "authors": [
        "C Nass",
        "I Jonsson",
        "H Harris",
        "B Reaves",
        "J Endo",
        "S Brave",
        "L Takayama"
      ],
      "year": "2005",
      "venue": "Conf. Hum. Factors Comput. Syst. -Proc."
    },
    {
      "citation_id": "4",
      "title": "Affective space is bipolar",
      "authors": [
        "J Russell"
      ],
      "year": "1979",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "5",
      "title": "Pleasure, Arousal, Dominance: Mehrabian and Russell revisited",
      "authors": [
        "I Bakker",
        "T Van Der Voordt",
        "P Vink",
        "J De Boon"
      ],
      "year": "2014",
      "venue": "Curr. Psychol"
    },
    {
      "citation_id": "6",
      "title": "Speech recognition on Indonesian language by using time delay neural network",
      "authors": [
        "B Atmaja",
        "D Arifianto",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "ASJ Spring Meet"
    },
    {
      "citation_id": "7",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Eval"
    },
    {
      "citation_id": "8",
      "title": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning"
    },
    {
      "citation_id": "9",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E Andre",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "10",
      "title": "Multimodal Speech Emotion Recognition and Ambiguity Resolution",
      "authors": [
        "G Sahu",
        "D Cheriton"
      ],
      "venue": "Multimodal Speech Emotion Recognition and Ambiguity Resolution"
    },
    {
      "citation_id": "11",
      "title": "Word-level emotion recognition using high-level features",
      "authors": [
        "J Moore",
        "L Tian",
        "C Lai"
      ],
      "year": "2014",
      "venue": "Word-level emotion recognition using high-level features"
    },
    {
      "citation_id": "12",
      "title": "Attention-based dense LSTM for speech emotion recognition",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEICE Trans. Inf. Syst"
    },
    {
      "citation_id": "13",
      "title": "Continuous Emotion Recognition in Speech Do We Need Recurrence?",
      "authors": [
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Interspeech 2019. ISCA: ISCA"
    },
    {
      "citation_id": "14",
      "title": "Speech Emotion Recognition Based on Speech Segment Using LSTM with Attention Model",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "2019 IEEE Int. Conf. Signals Syst"
    },
    {
      "citation_id": "15",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "-K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "16",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "3rd Int. Conf. Learn. Represent. ICLR 2015 -Conf. Track Proc"
    },
    {
      "citation_id": "17",
      "title": "cuDNN: Efficient Primitives for Deep Learning",
      "authors": [
        "S Chetlur",
        "C Woolley",
        "P Vandermersch",
        "J Cohen",
        "J Tran",
        "B Catanzaro",
        "E Shelhamer"
      ],
      "year": "2014",
      "venue": "Tech. Rep"
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "F Chollet",
        "\" Others",
        "Keras"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "19",
      "title": "Connectionist learning procedures",
      "authors": [
        "G Hinton"
      ],
      "year": "1989",
      "venue": "Artif. Intell"
    },
    {
      "citation_id": "20",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "J. Mach. Learn. Res"
    }
  ]
}