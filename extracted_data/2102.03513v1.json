{
  "paper_id": "2102.03513v1",
  "title": "Privacy-Preserving Video Classification With Convolutional Neural Networks",
  "published": "2021-02-06T05:05:31Z",
  "authors": [
    "Sikha Pentyala",
    "Rafael Dowsley",
    "Martine De Cock"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Many video classification applications require access to personal data, thereby posing an invasive security risk to the users' privacy. We propose a privacy-preserving implementation of single-frame method based video classification with convolutional neural networks that allows a party to infer a label from a video without necessitating the video owner to disclose their video to other entities in an unencrypted manner. Similarly, our approach removes the requirement of the classifier owner from revealing their model parameters to outside entities in plaintext. To this end, we combine existing Secure Multi-Party Computation (MPC) protocols for private image classification with our novel MPC protocols for oblivious single-frame selection and secure label aggregation across frames. The result is an end-to-end privacy-preserving video classification pipeline. We evaluate our proposed solution in an application for private human emotion recognition. Our results across a variety of security settings, spanning honest and dishonest majority configurations of the computing parties, and for both passive and active adversaries, demonstrate that videos can be classified with state-of-the-art accuracy, and without leaking sensitive user information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Deep learning based video classification is extensively used in a growing variety of applications, such as facial recognition, activity recognition, gesture analysis, behavioral analysis, eye gaze estimation, and emotion recognition in empathy-based AI systems  [3] ,  [47] ,  [42] ,  [46] ,  [49] ,  [51] ,  [58] ,  [65] ,  [72] . Many existing and envisioned applications of video classification rely on personal data, rendering these applications invasive of privacy. This applies among other tasks to video surveillance and home monitoring systems. Similarly, empathy-based AI systems expose personal emotions, which are most private to a user, to the service provider. Video classification systems deployed in commercial applications commonly require user videos to be shared with the service provider or sent to the cloud. These videos may remain publicly available on the Internet. Users have no control over the deletion of the videos, and the data may be available for scraping, as done for instance by Clearview AI  [30] . The need to protect the privacy of individuals is widely acknowledged  [56] . Concerns regarding privacy of user data are giving rise to new laws and regulations such as the European GDPR and the California Consumer Privacy Act (CCPA), as well as a perceived tension between Sikha Pentyala is with the School of Engineering and Technology, University of Washington, Tacoma, WA, USA. Email: sikha@uw.edu Rafael Dowsley is with the Faculty of Information Technology, Monash University, Clayton, Australia. Email: rafael.dowsley@monash.edu Martine De Cock is with the School of Engineering and Technology, University of Washington, Tacoma, WA, USA and Ghent University, Ghent, Belgium. Email: mdecock@uw.edu the desire to protect data privacy on one hand, and to promote an economy based on free-flowing data on the other hand  [37] . The E.U. is for instance considering a three-to-five-year moratorium on face recognition in public places, given its significant potential for misuse  [17] .\n\nA seemingly straightforward technique to keep user videos private is to deploy the deep learning models of the service providers at the user-end instead of transferring user data to the cloud. This is not a viable solution for several reasons. First, owners of proprietary models are concerned about shielding their model, especially when it constitutes a competitive advantage. Second, in security applications such as facial recognition, or deepfake detection, revealing model details helps adversaries develop evasion strategies. Furthermore, powerful deep learning models that memorize their training examples are well known; one would not want to expose those by revealing the model. Finally, deployment of large deep learning models at the user end may be technically difficult or impossible due to limited computational resources. For these reasons, ML tasks such as video classification are commonly outsourced to a set of efficient cloud servers in a Machine-Learning-as-a-Service (MLaaS) architecture. Protecting the privacy of both the users' and the service provider's data while performing outsourced ML computations is an important challenge.\n\nPrivacy-preserving machine learning (PPML) has been hailed, even by politicians  [20] ,  [73] , as a potential solution when handling sensitive information. Substantial technological progress has been made during the last decade in the area of Secure Multi-Party Computation (MPC)  [21] , an umbrella term for cryptographic approaches that allow two or more parties to jointly compute a specified output from their private information in a distributed fashion, without revealing the private information to each other. Initial applications of MPC based privacy-preserving inference with deep learning models have been proposed for image  [2] ,  [24] ,  [36] ,  [41] ,  [52] ,  [62] ,  [61] ,  [63]  and audio classification  [7] . We build on this existing work to create the first end-to-end MPC protocol for private video classification. In our solution, videos are classified according to the well-known single-frame method, i.e. by aggregating predictions across single frames/images. Our main novel contributions are:\n\n• A protocol for selecting frames in an oblivious manner.\n\n• A protocol for secure frame label aggregation.\n\n• An evaluation of our secure video classification pipeline in an application for human emotion detection from video on the RAVDESS dataset, demonstrating that MPC based video classification is feasible today, with state-of-the-art classification accuracies, and without leaking sensitive user information. Steps 1 and 3-4 are trivial as they follow directly from the choice of the underlying MPC scheme (see Sec. III). The focus of this paper is on Step 2, in which the servers (parties) execute protocols to perform computations over the secret shared data (see Sec. IV). MPC is concerned with the protocol execution coming under attack by an adversary which may corrupt parties to learn private information or cause the result of the computation to be incorrect. MPC protocols are designed to prevent such attacks being successful. There exist a variety of MPC schemes, designed for different numbers of parties and offering various levels of security that correspond to different threat models, and coming with different computational costs. Regarding threat models, we consider settings with semi-honest as well as with malicious adversaries. While parties corrupted by semi-honest adversaries follow the protocol instructions correctly but try to obtain additional information, parties corrupted by malicious adversaries can deviate from the protocol instructions. Regarding the number of parties (servers), some of the most efficient MPC schemes have been developed for 3 parties, out of which at most one is corrupted. We evaluate the runtime of our protocols in this honest-majority 3-party computing setting (3PC), which is growing in popularity in the PPML literature, e.g.  [24] ,  [41] ,  [62] ,  [67] ,  [57] , and we demonstrate how even better runtimes can be obtained with a recently proposed MPC scheme for 4PC with one corruption  [23] . Our protocols are generic and can be used in a 2PC, dishonest-majority setting as well, i.e. where each party can only trust itself. Note that in the 2PC setting, the computation can be performed directly by Alice and Bob if they are not very limited in terms of computational resources. As known from the literature, and apparent from our results, the higher level of security offered by the 2PC setting comes with a substantial increase in runtime.\n\nAfter discussing related work in Sec. II and recalling preliminaries about MPC in Sec. III, we present our protocols for privacy-preserving video classification in Sec. IV. The MPC protocols we present in Sec. IV enable the servers to perform all these computations without accessing the video V or the convolutional neural network (ConvNet) model M in plaintext. In Sec. V we present an experimental evaluation of our method when applied to emotion recognition from videos of the RAVDESS dataset. Our ConvNet based secure video classification approach achieves accuracies at par with those in the literature for this dataset, while not requiring leakage of sensitive information. Our prototype classifies videos that are 3-5 sec in length in under 14 sec on Azure F32 machines, demonstrating that private video classification based on MPC is feasible today.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Privacy-preserving video classification. Given the invasive nature of video classification applications, it is not surprising that efforts have been made to protect the privacy of individuals. Non-cryptography based techniques such as anonymizing faces in videos  [60] , pixel randomization to hide the user's identity  [32] , compressing video frames to achieve visual shielding effect  [45] , lowering resolution of videos  [64] , using autoencoders to maintain privacy of the user's data  [27] , and changes in ways the videos are captured  [70]  do not provide any formal privacy guarantees and affect the accuracy of the inference made. Solutions based on Differential Privacy (DP)  [69]  introduce noise, or replace the original data at the user end by newly generated data, to limit the amount of information leaked, at the cost of lowering accuracy. The recently proposed \"Visor\" system requires secure hardware (trusted execution environments) for privacy-preserving video analytics  [59] .\n\nIn contrast to the approaches above, in this paper we pursue the goal of having no leakage of information during the inference phase, without requiring special secure hardware. To the best of our knowledge, our approach is the first in the open literature to achieve this goal for private video classification. To this end, we leverage prior work on cryptography based private image classification, as described below, and augment it with novel cryptographic protocols for private video frame selection and label aggregation across frames.\n\nCryptography based image classification. There are 2 main approaches within cryptography that enable computations over encrypted data, namely Homomorphic Encryption (HE) and Secure Multiparty Computation (MPC). Both have been applied to secure inference with trained neural networks, including for image classification with ConvNets  [10] ,  [29] ,  [40] ,  [41] ,  [57] ,  [16] ,  [61] ,  [62] ,  [67] ,  [68] . Neither have been applied to video classification before. While HE has a lower communication burden than MPC, it has much higher computational costs, making HE less appealing at present for use in applications where response time matters. E.g., in state-of-the-art work on private image classification with HE, Chillotti et al. (  [18] ) report a classification time of ∼ 9 sec for a 28 × 28 MNIST image on 96vCPU AWS instances with a neural network smaller in size (number of parameters) than the one we use in this paper. As demonstrated in Sec. V, the MPC based techniques for image classification based on Dalskov et al. (  [24] ) that we use, can label images (video frames) an order of magnitude faster, even when run on less powerful 32vCPU Azure instances (∼ 0.5 sec for passive 3PC; ∼ 1 sec for active 4PC). We acknowledge that this superior performance stems from the flexibility of MPC to accommodate honest-majority 3PC/4PC scenarios. HE based private image classification is by design limited to the dishonest-majority 2PC setting, in which our MPC approach is too slow for video classification in (near) real-time as well.\n\nEmotion recognition. A wide variety of applications have prompted research in emotion recognition, using various modalities and features  [6] ,  [34] ,  [35] ,  [71] , including videos  [75] ,  [31] ,  [53] ,  [54] ,  [26] . Emotion recognition from videos in the RAVDESS benchmark dataset, as we do in the use case in Sec. V, has been studied by other authors in-the-clear, i.e. without regards for privacy protection, using a variety of deep learning architectures, with reported accuracies in the 57%-82% range, depending on the number of emotion classes included in the study (6 to 8)  [5] ,  [50] ,  [9] ,  [1] . The ConvNet model that we trained for our experimental results in Sec. V is at par with these state-of-the-art accuracies. Jaiswal and Provost  [33]  have studied privacy metrics and leakages when inferring emotions from data. To the best of our knowledge, there is no existing work on privacy-preserving emotion detection from videos using MPC, as we do in Sec. V.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Preliminaries",
      "text": "Protocols for Secure Multi-Party Computation (MPC) enable a set of parties to jointly compute the output of a function over the private inputs of each party, without requiring any of the parties to disclose their own private inputs. MPC is concerned with the protocol execution coming under attack by an adversary A which may corrupt one or more of the parties to learn private information or cause the result of the computation to be incorrect. MPC protocols are designed to prevent such attacks being successful, and can be mathematically proven to guarantee privacy and correctness. We follow the standard definition of the Universal Composability (UC) framework  [11] , in which the security of protocols is analyzed by comparing a real world with an ideal world. For details, see  Evans   An adversary A can corrupt any number of parties. In a dishonest-majority setting at least half of the parties are corrupt, while in an honest-majority setting, more than half of the parties are honest (not corrupted). Furthermore, A can have different levels of adversarial power. In the semi-honest model, even corrupted parties follow the instructions of the protocol, but the adversary attempts to learn private information from the internal state of the corrupted parties and the messages that they receive. MPC protocols that are secure against semi-honest or \"passive\" adversaries prevent such leakage of information.\n\nIn the malicious adversarial model, the corrupted parties can arbitrarily deviate from the protocol specification. Providing security in the presence of malicious or \"active\" adversaries, i.e. ensuring that no such adversarial attack can succeed, comes at a higher computational cost than in the passive case.\n\nThe protocols in Sec. IV are sufficiently generic to be used in dishonest-majority as well as honest-majority settings, with passive or active adversaries. This is achieved by changing the underlying MPC scheme to align with the desired security setting. Table  I  contains an overview of the MPC schemes used in Sec. V. In these MPC schemes, all computations are done on integers modulo q, i.e., in a ring Z q = {0, 1, . . . , q -1}, with q a power of 2. The pixel values in Alice's video and the model parameters in Bob's classifier are natively real numbers represented in a floating point format. As is common in MPC, they are converted to integers using a fixed-point representation  [15] . When working with fixed-point representations with a fractional bits, every multiplication generates an extra a bits of unwanted fractional representation. To securely \"chop off\" the extra fractional bits generated by multiplication, we use the deterministic truncation protocol by Dalskov et al. (  [24] ,  [23] ) for computations over Z 2 k . Below we give a high level description of the 3PC schemes from Table  I . For more details and a description of the other MPC schemes, we refer to the papers in Table  I .\n\nReplicated sharing (3PC). After Alice and Bob have converted all their data to integers modulo q, they send secret shares of these integers to the servers in S (see Fig.  1 ). In a replicated secret sharing scheme with 3 servers (3PC), a value x in Z q is secret shared among servers (parties) S 1 , S 2 , and S 3 by picking uniformly random shares x 1 , x 2 , x 3 ∈ Z q such that x 1 + x 2 + x 3 = x mod q, and distributing (x 1 , x 2 ) to S 1 , (x 2 , x 3 ) to S 2 , and (x 3 , x 1 ) to S 3 . Note that no single server can obtain any information about x given its shares. We use  [[x] ] as a shorthand for a secret sharing of x. The servers subsequently classify Alice's video with Bob's model by computing over the secret sharings.\n\nPassive security (3PC). The 3 servers can perform the following operations through carrying out local computations on their own shares: addition of a constant, addition of secret shared values, and multiplication by a constant. For multiplying secret shared values  [[x] ] and [[y]], we have that x • y = (x 1 + x 2 + x 3 )(y 1 + y 2 + y 3 ), and so S 1 computes z\n\nNext, the servers obtain an additive secret sharing of 0 by picking uniformly random u 1 , u 2 , u 3 such that u 1 + u 2 + u 3 = 0, which can be locally done with computational security by using pseudorandom functions, and S i locally computes v i = z i + u i . Finally, S 1 sends v 1 to S 3 , S 2 sends v 2 to S 1 , and S 3 sends v 3 to S 2 , enabling the servers S 1 , S 2 and S 3 to get the replicated secret shares (v 1 , v 2 ), (v 2 , v 3 ), and (v 3 , v 1 ), respectively, of the value v = x•y. This protocol only requires each server to send a single ring element to one other server, and no expensive publickey encryption operations (such as homomorphic encryption or oblivious transfer) are required. This MPC scheme was introduced by Araki et al.  ([4] ).\n\nActive security (3PC). In the case of malicious adversaries, the servers are prevented from deviating from the protocol and gain knowledge from another party through the use of information-theoretic message authentication codes (MACs). For every secret share, an authentication message is also sent to authenticate that each share has not been tampered in each communication between parties. In addition to computations over secret shares of the data, the servers also need to update the MACs appropriately, and the operations are more involved than in the passive security setting. For each multiplication of secret shared values, the total amount of communication between the parties is greater than in the passive case. We use the MPC scheme SPDZ-wiseReplicated2k recently proposed by  Dalskov et al. ([23] ), with the option with preprocessing for generation of the multiplication triples that is available in MP-SPDZ  [39] .\n\na) MPC primitives.: The MPC schemes listed above provide a mechanism for the servers to perform cryptographic primitives through the use of secret shares, namely addition of a constant, multiplication by a constant, and addition and multiplication of secret shared values. Building on these cryptographic primitives, MPC protocols for other operations have been developed in the literature. We use:  secure comparison protocol. • Secure RELU π RELU  [24] : at the start of this protocol, the parties have a secret sharing of z; at the end of the protocol, the parties have a secret sharing of the value max(0, z). π RELU is constructed from π LT , followed by a secure multiplication to either keep the original value z or replace it by zero in an oblivious way. • Secure division π DIV : for secure division, the parties use an iterative algorithm that is well known in the MPC literature  [14] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Methodology",
      "text": "The servers perform video classification based on the single-frame method, i.e. by (1) selecting frames from the video V (Sec. IV-A); (2) labeling the selected frames with a ConvNet model M (Sec. IV-B); and (3) aggregating the labels inferred for the selected frames into a final label for the video (Sec. IV-C). The video V is owned by Alice and the model M is owned by Bob. Neither party is willing or able to reveal their video/model to other parties in an unencrypted manner.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Oblivious Frame Selection",
      "text": "We assume that Alice has prepared her video V as a 4D array (tensor) A of size N × h × w × c where N is the number of frames, h is the height and w is the width of the frame, and c represents the number of color channels of the frame. As explained in Sec. III, Alice has converted the pixel values into integers using a fixed-point representation. The values of the dimensions N, h, w, c are known to Bob and the set of servers S. All other properties of the video are kept private, including the video length, the frames per second (fps), and video capture details such as the type of camera used. Moreover, Bob and the servers S do not learn the values of the pixels, i.e. the actual contents of the frames remain hidden from Bob and S (and anyone else, for that matter). For an illustration of Alice's input, we refer to the top of Fig.  2 , where N = 4, h = 2, w = 2, and c = 1.\n\nBob samples a fixed number of frames from Alice's video, without revealing to Alice the frames he is selecting, as such knowledge might allow Alice to insert malicious frames in the video in the exact positions that Bob is sampling. We assume that Bob has a vector b of length n, with the indices of the n frames he wishes to select. These indices can for instance be 1, 1 + d, 1 + 2d, . . . for a fixed window size d that is known to Bob. In the example in Fig.  2 , n = 2, both 2nd and 4th frames are selected.\n\nThe idea behind protocol π FSELECT for oblivious frame selection, as illustrated in Fig.  2 , is to flatten A into a matrix that contains one row per frame, use a matrix B with onehot-encodings of the selected frames, multiply B with A, and finally expand the product. In more detail: Bob converts each entry i of list b (which is an index of a frame to be selected) into a vector of length N that is a one-hot-encoding of i, and inserts it as a row in matrix B of size n × N . Alice and Bob then send secret shares of their respective inputs A and B to the servers S, using a secret sharing scheme as mentioned in Sec. III. None of the servers can reconstruct the values of A or B by using only its own secret shares.\n\nNext the parties in S jointly execute protocol π FSELECT for oblivious frame selection (see Protocol 1). On line 1, the parties reorganize the shares of tensor A of size N × h × w × c into a flattened matrix A flat of size N × (h • w • c). On line 2, the parties multiply [[B]] and [[A flat ]], using protocol π DMM for secure matrix multiplication, to select the desired rows from A flat . On line 3, these selected rows are expanded again into a secret-shared tensor F of size n × h × w × c that holds the selected frames. F  [1] , F [2], . . . , F [n] are used in the remainder to denote the individual frames contained in F . Throughout this process, the servers do not learn the pixel values from A, nor which frames were selected. Output: A secret shared 4D-array F of size n × h × w × c holding the selected frames 1:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Protocol 1 Protocol Π Fselect For Oblivious Frame Selection",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Private Frame Classification",
      "text": "We assume that Bob has trained an \"MPC-friendly\" 2D-ConvNet M for classifying individual video frames (images), and that Bob secret shares the values of the model parameters with the servers S, who already have secret shares of the selected frames from Alice's video after running Protocol π FSELECT . By \"MPC-friendly\" we mean that the operations to be performed when doing inference with the trained ConvNet are chosen purposefully among operations for which efficient MPC protocols exist or can be constructed. Recall that a standard ConvNet contains one or more blocks that each have a convolution layer, followed by an activation layer, typically with RELU, and an optional pooling layer. These blocks are then followed by fully connected layers which commonly have RELU as activation function, except for the last fully connected layer which typically has a Softmax activation for multi-class classification. The operations needed for all layers, except for the output layer, boil down to comparisons, multiplicatons, and summations. All of these cryptographic primitives can be efficiently performed with state-of-the-art MPC schemes, as explained in Sec. III. Efficient protocols for convolutional, RELU activation, average pooling layers, and dense layers are known in the MPC literature  [24] . We do not repeat them in this paper for conciseness. All these operations are performed by the servers S using the secret shares of Bob's model parameters and of the selected frame from Alice's video, as obtained using π FSELECT .\n\nAs previously mentioned, Softmax is generally used as the activation function in the last layer of ConvNets that are trained to perform classification. Softmax normalizes the logits passed into it from the previous layer to a probability distribution over the class labels. Softmax is an expensive operation to implement using MPC protocols, as this involves division and exponentiation. Previously proposed workarounds include disclosing the logits and computing Softmax in an unencrypted manner  [44] , which leaks information, or replacing Softmax by Argmax  [7] ,  [24] . The latter works when one is only interested in retrieving the class label with the highest probability, as the Softmax operation does not change the ordering among the logits. In our context of video classification based on the single-frame method however, the probabilities of all class labels for each frame are required, to allow probabilities across the different frames to be aggregated to define a final label (see Sec. IV-C).\n\nWe therefore adopt the solution proposed by Mohassel and Zhang  [55]  and replace the Softmax operation by\n\nfor i = 1, . . . , C, where (u 1 , u 2 , . . . , u C ) denote the logits for each of the C class labels, and\n\nis the computed probability distribution over the class labels. Pseudocode for the corresponding MPC protocol is presented in Protocol 2. At the start of Protocol π SOFT , the servers have secret shares of a list of logits, on which they apply the secure RELU protocol in Line 1. Lines 2-5 serve to compute the sum of the RELU values, while on Line 6 the parties run a secure comparison protocol to determine if this sum is greater than 0. If Sum relu is greater than 0, then after Line 6, [[cn]] contains a secret sharing of 1; otherwise it contains a secret sharing of 0. Note that if cn = 1 then the numerator of the i th probability f (u i ) should be X relu [i] while the denominator should be Sum relu . Likewise, if cn = 0 then the numerator should be 1 and the denominator C. As is common in MPC protocols, we use multiplication instead of control flow logic for such conditional assignments. To this end, a conditional based branch operation as \"if p then q ← r else q ← s\" is rephrased as \"q ← p•r +(1-p)•s\". In this way, the number and the kind of operations executed by the parties does not depend on the actual values of the inputs, so it does not leak information that could be exploited by side-channel attacks. Such conditional\n\nA protocol π FINFER for performing secure inference with Bob's model M (which is secret shared among the servers) over a secret shared frame f from Alice's video can be straightforwardly obtained by: (1) using the cryptographic primitives defined in Sec. III to securely compute all layers except the output layer; (2) using Protocol π SOFT to compute the approximation of the Softmax for the last layer. The execution of this protocol results in the servers obtaining secret shares of the inferred probability distribution over the class labels for frame f .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Secure Label Aggregation",
      "text": "As illustrated in Fig.  3 , we aggregate the predictions across the single frames by selecting the class label with the highest sum of inferred probabilities across the frames. We implement this securely as Protocol 3. To classify a video V, the servers: (1) obliviously select the desired frames as shown in Line 2; (2) securely infer the probability distribution SM approx of all classes labels generated by the model M on a specific selected frame, as shown in Line 4; (3) add these probabilities, index-wise, to the sum of the probabilities corresponding to each class that is obtained throughout the selected frames (Line 5-6); (  4  for i = 1 to C do 6:\n\nend for 8: end for 9:\n\nV. RESULTS",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Dataset And Model Architecture",
      "text": "We demonstrate the feasibility of our privacy-preserving video classification approach for the task of emotion detection using the database 1    [48] . We use 1,248 video-only files with speech modality from this dataset, corresponding to 7 different emotions, namely neutral (96), happy (192), sad (192), angry (192), fearful (192), disgust (192), and surprised (192). The videos portray 24 actors who each read two different statements twice, with different emotions, for a total of 52 video files per actor. For all emotions except for neutral, the statements are read with alternating normal and strong intensities; this accounts for the fact that there are less \"neutral\" instances in the dataset than for the other emotion categories. As in  [9] , we leave out the calm instances, reducing the original 8 emotion categories from the RADVESS dataset to the 7 categories that are available in the FER2013 dataset  [12] , which we use for pre-training. The videos in the RAVDESS dataset have a duration of 3-5 seconds with 30 frames per second, hence the total number of frames per video is in the range of 120-150. We split the data into 1,116 videos for training and 132 videos for testing. To this end, we moved all the video recordings of the actors 8, 15 (selected randomly) and an additional randomly selected 28 video recordings to the test set, while keeping the remaining video recordings in the train set.\n\nWe used OpenCV  [8]  to read the videos into frames. Faces are detected with a confidence greater than 98% using MTCNN  [74]  on the FER 2013 data to learn to extract facial features for emotion recognition, and fine-tuned 3  the model on the RAVDESS training data. Our video classifier samples every 15th frame, classifies it with the above ConvNet, and assigns as the final class label the label that has the highest average probability across all frames in the video. The video classification accuracy on the test set is 56%. For inference with the MPC protocols, after training, we replace the Softmax function on the last layer by the approximate function discussed in Section IV-B. After this replacement, the accuracy of the video classifier is 56.8%. This is in line with state-of-the-art results in the literature on emotion recognition from RAVDESS videos, namely 57.5% with Synchronous Graph Neural Networks (8 emotions)  [50] ; 61% with ConvNet-LSTM (8 emotions)  [1] ; 59% with an RNN (7 emotions)  [9] , and 82.4% with stacked autoencoders (6 emotions)  [5] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Runtime Experiments",
      "text": "We implemented the protocols from Sec. IV in the MPC framework MP-SPDZ  [39] , and ran experiments on co-located F32s V2 Azure virtual machines. Each of the parties (servers) ran on separate VM instances (connected with a Gigabit Ethernet network), which means that the results in Table  II  cover communication time in addition to computation time. A F32s V2 virtual machine contains 32 cores, 64 GiB of memory, and network bandwidth of upto 14 Gb/s. For the ring Z 2 k , we used value k = 64.\n\nTable  II  presents the average time needed to privately classify a video. The MPC schemes for 4PC (with one corrupted party) are faster than 3PC (with one corrupted party), which are in turn substantially faster than 2PC. Furthermore, as expected, there is a substantial difference in runtime between the semihonest (passive security) and malicious (active security) settings. These findings are in line with known results from the MPC literature  [23] ,  [24] . In the fastest setting, namely a 3PC setting with a semi-honest adversary that can only corrupt one party, videos from the RAVDESS dataset are classified on average in 13.12 sec, which corresponds to approximately 0.5-0.6 sec per frame, demonstrating that privacy-preserving video classification with state-of-the-art accuracy is feasible in practice. While the presented runtime results are still too slow for video classification in real-time, there is a clear path to substantial optimization that would enable deployment of our proposed MPC solution in practical real-time applications. Indeed, MPC schemes are normally divided in two phases: the offline and online phases. The runtime results in Table  II  represent the time needed for both. The offline phase only performs computations that are independent from the specific inputs of the parties to the protocol (Alice's video and Bob's trained model parameters), and therefore can be executed long before the inputs are known. By executing the offline phase of the MPC scheme in advance, it is possible to improve the responsiveness of the final solution.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "We presented the first end-to-end solution for private video classification based on Secure Multi-Party Computation (MPC). To achieve state-of-the-art accuracy while keeping our architecture lean, we used the single-frame method for video classification with a ConvNet. To keep the videos and the model parameters hidden, we proposed novel MPC protocols for oblivious frame selection and secure label aggregation across frames. We used these in combination with existing MPC protocols for secure ConvNet based image classification, and evaluated them for the task of emotion recognition from videos in the RAVDESS dataset.\n\nOur work provides a baseline for private video classification based on cryptography. It can be improved and adapted further to align with state-of-the-art techniques in video classification in-the-clear, including the use of machine learning for intelligent frame selection. While our approach considers only spatial information in the videos, the model architecture in Sec. IV-B can be replaced by different architectures such as CONV3D, efficient temporal modeling in video  [43] , single and two stream ConvNets  [38] ,  [66]  to fuse temporal information. Many such approaches use popular ImageNet models for which efficient MPC protocols are available in the literature  [24] ,  [41] , opening up interesting directions for further research.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Privacy-preserving video classiﬁcation as an outsourced computation",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates the ﬂow of our proposed solution at a",
      "page": 2
    },
    {
      "caption": "Figure 1: , Alice and Bob",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of oblivious frame selection. The assumption is made that",
      "page": 4
    },
    {
      "caption": "Figure 2: , where N = 4, h = 2,",
      "page": 4
    },
    {
      "caption": "Figure 2: , n = 2, both 2nd and 4th frames",
      "page": 5
    },
    {
      "caption": "Figure 2: , is to ﬂatten A into a matrix",
      "page": 5
    },
    {
      "caption": "Figure 3: Illustration of label aggregation. Let us assume that n = 4 frames",
      "page": 6
    },
    {
      "caption": "Figure 3: , we aggregate the predictions across",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: PC 511.64 669.35",
      "data": [
        {
          "Avg. Time (sec)": "511.64\n13.12",
          "Avg. Comm (GB)": "669.35\n2.49"
        },
        {
          "Avg. Time (sec)": "8423.81\n48.20\n18.40",
          "Avg. Comm (GB)": "7782.74\n10.98\n4.60"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial expression recognition in videos: An CNN-LSTM based model for video classification",
      "authors": [
        "M Abdullah",
        "M Ahmad",
        "D Han"
      ],
      "year": "2020",
      "venue": "International Conference on Electronics, Information, and Communication"
    },
    {
      "citation_id": "2",
      "title": "QUOTIENT: two-party secure neural network training and prediction",
      "authors": [
        "N Agrawal",
        "A Shahin",
        "M Shamsabadi",
        "A Kusner",
        "Gascón"
      ],
      "year": "2019",
      "venue": "QUOTIENT: two-party secure neural network training and prediction"
    },
    {
      "citation_id": "3",
      "title": "Spatiotemporal attention and magnification for classification of Parkinson's disease from videos collected via the Internet",
      "authors": [
        "M Ali",
        "J Hernandez",
        "E Dorsey",
        "E Hoque",
        "D Mcduff"
      ],
      "year": "2020",
      "venue": "15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "4",
      "title": "High-throughput semi-honest secure three-party computation with an honest majority",
      "authors": [
        "T Araki",
        "J Furukawa",
        "Y Lindell",
        "A Nof",
        "K Ohara"
      ],
      "year": "2016",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "5",
      "title": "A novel model for emotion detection from facial muscles activity",
      "authors": [
        "E Bagheri",
        "A Bagheri",
        "P Esteban",
        "B Vanderborgth"
      ],
      "year": "2019",
      "venue": "Iberian Robotics conference"
    },
    {
      "citation_id": "6",
      "title": "STEP: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "34th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Private speech classification with secure multiparty computation",
      "authors": [
        "Kyle Bittner",
        "Martine Cock",
        "Rafael Dowsley"
      ],
      "year": "2020",
      "venue": "Private speech classification with secure multiparty computation"
    },
    {
      "citation_id": "8",
      "title": "Learning OpenCV: Computer vision with the OpenCV library",
      "authors": [
        "G Bradski",
        "A Kaehler"
      ],
      "year": "2008",
      "venue": "Learning OpenCV: Computer vision with the OpenCV library"
    },
    {
      "citation_id": "9",
      "title": "Improving the accuracy of automatic facial expression recognition in speaking subjects with deep learning",
      "authors": [
        "S Bursic",
        "G Boccignone",
        "A Ferrara",
        "A D'amelio",
        "R Lanzarotti"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "10",
      "title": "Flash: fast and robust framework for privacy-preserving machine learning",
      "authors": [
        "M Byali",
        "H Chaudhari",
        "A Patra",
        "A Suresh"
      ],
      "year": "2020",
      "venue": "Proceedings on Privacy Enhancing Technologies"
    },
    {
      "citation_id": "11",
      "title": "Security and composition of multiparty cryptographic protocols",
      "authors": [
        "R Canetti"
      ],
      "year": "2000",
      "venue": "Journal of Cryptology"
    },
    {
      "citation_id": "12",
      "title": "FER-2013 face database",
      "authors": [
        "P Carrier",
        "A Courville",
        "I Goodfellow",
        "M Mirza",
        "Y Bengio"
      ],
      "year": "2013",
      "venue": "FER-2013 face database"
    },
    {
      "citation_id": "13",
      "title": "Improved primitives for secure multiparty integer computation",
      "authors": [
        "O Catrina",
        "S Hoogh"
      ],
      "year": "2010",
      "venue": "International Conference on Security and Cryptography for Networks"
    },
    {
      "citation_id": "14",
      "title": "Secure multiparty linear programming using fixed-point arithmetic",
      "authors": [
        "O Catrina",
        "S Hoogh"
      ],
      "year": "2010",
      "venue": "European Symposium on Research in Computer Security"
    },
    {
      "citation_id": "15",
      "title": "Secure computation with fixed-point numbers",
      "authors": [
        "O Catrina",
        "A Saxena"
      ],
      "year": "2010",
      "venue": "14th International Conference on Financial Cryptography and Data Security"
    },
    {
      "citation_id": "16",
      "title": "Trident: Efficient 4pc framework for privacy preserving machine learning",
      "authors": [
        "H Chaudhari",
        "R Rachuri",
        "A Suresh"
      ],
      "year": "2020",
      "venue": "27th Annual Network and Distributed System Security Symposium, NDSS"
    },
    {
      "citation_id": "17",
      "title": "EU mulls five-year ban on facial recognition tech in public areas",
      "authors": [
        "F Chee"
      ],
      "year": "2020",
      "venue": "Reuters, Innovation and Technology"
    },
    {
      "citation_id": "18",
      "title": "Programmable bootstrapping enables efficient homomorphic inference of deep neural networks",
      "authors": [
        "Ilaria Chillotti",
        "Marc Joye",
        "Pascal Paillier"
      ],
      "year": "2021",
      "venue": "Cryptology ePrint Archive"
    },
    {
      "citation_id": "19",
      "title": "",
      "authors": [
        "Chollet Franc"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "20",
      "title": "Commission of Evidence-Based Policymaking. The Promise of Evidence-Based Policymaking",
      "year": "2017",
      "venue": "Commission of Evidence-Based Policymaking. The Promise of Evidence-Based Policymaking"
    },
    {
      "citation_id": "21",
      "title": "Secure Multiparty Computation and Secret Sharing",
      "authors": [
        "R Cramer",
        "I Damgård",
        "J Nielsen"
      ],
      "year": "2015",
      "venue": "Secure Multiparty Computation and Secret Sharing"
    },
    {
      "citation_id": "22",
      "title": "SPDZ 2 k : Efficient MPC mod 2 k for dishonest majority",
      "authors": [
        "Ronald Cramer",
        "Ivan Damgård",
        "Daniel Escudero",
        "Peter Scholl",
        "Chaoping Xing"
      ],
      "year": "2018",
      "venue": "Annual International Cryptology Conference"
    },
    {
      "citation_id": "23",
      "title": "Fantastic four: Honest-majority four-party secure computation with malicious security",
      "authors": [
        "A Dalskov",
        "D Escudero",
        "M Keller"
      ],
      "year": "1330",
      "venue": "Cryptology ePrint Archive"
    },
    {
      "citation_id": "24",
      "title": "Secure evaluation of quantized neural networks",
      "authors": [
        "A Dalskov",
        "D Escudero",
        "M Keller"
      ],
      "year": "2007",
      "venue": "Proceedings on Privacy Enhancing Technologies"
    },
    {
      "citation_id": "25",
      "title": "New primitives for actively-secure MPC over rings with applications to private machine learning",
      "authors": [
        "I Damgård",
        "D Escudero",
        "T Frederiksen",
        "M Keller",
        "P Scholl",
        "N Volgushev"
      ],
      "year": "2019",
      "venue": "IEEE Symposium on Security and Privacy (SP)"
    },
    {
      "citation_id": "26",
      "title": "MIMAMO Net: Integrating micro-and macro-motion for video emotion recognition",
      "authors": [
        "D Deng",
        "Z Chen",
        "Y Zhou",
        "B Shi"
      ],
      "year": "2019",
      "venue": "MIMAMO Net: Integrating micro-and macro-motion for video emotion recognition",
      "arxiv": "arXiv:1911.09784"
    },
    {
      "citation_id": "27",
      "title": "Autoencoder as a new method for maintaining data privacy while analyzing videos of patients with motor dysfunction: Proof-of-concept study",
      "authors": [
        "M Souza",
        "J Dorn",
        "A Dorier",
        "C Kamm",
        "S Steinheimer",
        "F Dahlke",
        "C Van Munster",
        "B Uitdehaag",
        "L Kappos",
        "M Johnson"
      ],
      "year": "2020",
      "venue": "Journal of Medical Internet Research"
    },
    {
      "citation_id": "28",
      "title": "A pragmatic introduction to secure multi-party computation",
      "authors": [
        "D Evans",
        "V Kolesnikov",
        "M Rosulek"
      ],
      "year": "2018",
      "venue": "Foundations and Trends in Privacy and Security"
    },
    {
      "citation_id": "29",
      "title": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy",
      "authors": [
        "R Gilad-Bachrach",
        "N Dowlin",
        "K Laine",
        "K Lauter",
        "M Naehrig",
        "J Wernsing"
      ],
      "year": "2016",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "The secretive company that might end privacy as we know it",
      "authors": [
        "K Hill"
      ],
      "year": "2020",
      "venue": "The New York Times"
    },
    {
      "citation_id": "31",
      "title": "Video facial emotion recognition based on local enhanced motion history image and CNN-CTSLSTM networks",
      "authors": [
        "M Hu",
        "H Wang",
        "X Wang",
        "J Yang",
        "R Wang"
      ],
      "year": "2019",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "32",
      "title": "Robust, efficient and privacypreserving violent activity recognition in videos",
      "authors": [
        "J Imran",
        "B Raman",
        "A Rajput"
      ],
      "year": "2020",
      "venue": "35th Annual ACM Symposium on Applied Computing"
    },
    {
      "citation_id": "33",
      "title": "Privacy enhanced multimodal neural representations for emotion recognition",
      "authors": [
        "M Jaiswal",
        "E Provost"
      ],
      "year": "2020",
      "venue": "34th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Facial emotion distribution learning by exploiting low-rank label correlations locally",
      "authors": [
        "X Jia",
        "X Zheng",
        "W Li",
        "C Zhang",
        "Z Li"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "35",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2019",
      "venue": "Real-time emotion recognition via attention gated hierarchical memory network",
      "arxiv": "arXiv:1911.09075"
    },
    {
      "citation_id": "36",
      "title": "GAZELLE: A low latency framework for secure neural network inference",
      "authors": [
        "C Juvekar",
        "V Vaikuntanathan",
        "A Chandrakasana"
      ],
      "year": "2018",
      "venue": "27th USENIX Security Symposium"
    },
    {
      "citation_id": "37",
      "title": "New European data privacy and cyber security laws -one year later",
      "authors": [
        "L Kalman"
      ],
      "year": "2019",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "38",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "A Karpathy",
        "G Toderici",
        "S Shetty",
        "T Leung",
        "R Sukthankar",
        "L Fei-Fei"
      ],
      "year": "2014",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "39",
      "title": "MP-SPDZ: A versatile framework for multi-party computation",
      "authors": [
        "Marcel Keller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "40",
      "title": "Swift: Super-fast and robust privacy-preserving machine learning",
      "authors": [
        "N Koti",
        "M Pancholi",
        "A Patra",
        "A Suresh"
      ],
      "year": "2020",
      "venue": "Swift: Super-fast and robust privacy-preserving machine learning",
      "arxiv": "arXiv:2005.10296"
    },
    {
      "citation_id": "41",
      "title": "CrypTFlow: Secure TensorFlow inference",
      "authors": [
        "N Kumar",
        "M Rathee",
        "N Chandran",
        "D Gupta",
        "A Rastogi",
        "R Sharma"
      ],
      "year": "2007",
      "venue": "41st IEEE Symposium on Security and Privacy"
    },
    {
      "citation_id": "42",
      "title": "Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison",
      "authors": [
        "D Li",
        "C Rodriguez",
        "X Yu",
        "H Li"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "43",
      "title": "Tsm: Temporal shift module for efficient video understanding",
      "authors": [
        "Ji Lin",
        "Chuang Gan",
        "Song Han"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Oblivious neural network predictions via MiniONN transformations",
      "authors": [
        "J Liu",
        "M Juuti",
        "Y Lu",
        "N Asokan"
      ],
      "year": "2017",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "45",
      "title": "Privacy-preserving video fall detection using visual shielding information",
      "authors": [
        "J Liu",
        "Y Xia",
        "Z Tang"
      ],
      "year": "2020",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "46",
      "title": "Enhancing anomaly detection in surveillance videos with transfer learning from action recognition",
      "authors": [
        "K Liu",
        "M Zhu",
        "H Fu",
        "H Ma",
        "T Chua"
      ],
      "year": "2020",
      "venue": "28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "47",
      "title": "Social relation recognition from videos via multi-scale spatial-temporal reasoning",
      "authors": [
        "X Liu",
        "W Liu",
        "M Zhang",
        "J Chen",
        "L Gao",
        "C Yan",
        "T Mei"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS One"
    },
    {
      "citation_id": "49",
      "title": "Face recognition from video using deep learning",
      "authors": [
        "S Manna",
        "S Ghildiyal",
        "K Bhimani"
      ],
      "year": "2020",
      "venue": "5th International Conference on Communication and Electronics Systems (ICCES)"
    },
    {
      "citation_id": "50",
      "title": "Synch-graph: multisensory emotion recognition through neural synchrony via graph convolutional networks",
      "authors": [
        "E Mansouri-Benssassi",
        "J Ye"
      ],
      "year": "2020",
      "venue": "34th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Activity recognition from newborn resuscitation videos",
      "authors": [
        "Ø Meinich-Bache",
        "S Austnes",
        "K Engan",
        "I Austvoll",
        "T Eftestøl",
        "H Myklebust",
        "S Kusulla",
        "H Kidanto",
        "H Ersdal"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "52",
      "title": "Delphi: A cryptographic inference service for neural networks",
      "authors": [
        "P Mishra",
        "R Lehmkuhl",
        "A Srinivasan",
        "W Zheng",
        "R Popa"
      ],
      "year": "2020",
      "venue": "29th USENIX Security Symposium"
    },
    {
      "citation_id": "53",
      "title": "M3ER: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "34th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "55",
      "title": "Secureml: A system for scalable privacypreserving machine learning",
      "authors": [
        "P Mohassel",
        "Y Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE Symposium on Security and Privacy (SP)"
    },
    {
      "citation_id": "56",
      "title": "BLAZE: Blazing fast privacy-preserving machine learning",
      "authors": [
        "A Patra",
        "A Suresh"
      ],
      "year": "2020",
      "venue": "BLAZE: Blazing fast privacy-preserving machine learning",
      "arxiv": "arXiv:2005.09042"
    },
    {
      "citation_id": "57",
      "title": "Predicting students' attention level with interpretable facial and head dynamic features in an online tutoring system",
      "authors": [
        "S Peng",
        "L Chen",
        "C Gao",
        "R Tong"
      ],
      "year": "2020",
      "venue": "34th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "58",
      "title": "Visor: Privacy-preserving video analytics as a cloud service",
      "authors": [
        "R Poddar",
        "G Ananthanarayanan",
        "S Setty",
        "S Volos",
        "R Popa"
      ],
      "year": "2020",
      "venue": "29th USENIX Security Symposium"
    },
    {
      "citation_id": "59",
      "title": "Learning to anonymize faces for privacy preserving action detection",
      "authors": [
        "Z Ren",
        "Y Lee",
        "M Ryoo"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "60",
      "title": "XONN: Xnor-based oblivious deep neural network inference",
      "authors": [
        "M Riazi",
        "M Samragh",
        "H Chen",
        "K Laine",
        "K Lauter",
        "F Koushanfar"
      ],
      "year": "2019",
      "venue": "28th USENIX Security Symposium"
    },
    {
      "citation_id": "61",
      "title": "Chameleon: A hybrid secure computation framework for machine learning applications",
      "authors": [
        "M Riazi",
        "C Weinert",
        "O Tkachenko",
        "E Songhori",
        "T Schneider",
        "F Koushanfar"
      ],
      "year": "2018",
      "venue": "Asia Conference on Computer and Communications Security"
    },
    {
      "citation_id": "62",
      "title": "DeepSecure: Scalable provably-secure deep learning",
      "authors": [
        "B Rouhani",
        "M Riazi",
        "F Koushanfar"
      ],
      "year": "2018",
      "venue": "55th Annual Design Automation Conference (DAC)"
    },
    {
      "citation_id": "63",
      "title": "Privacy-preserving human activity recognition from extreme low resolution",
      "authors": [
        "M Ryoo",
        "B Rothrock",
        "C Fleming",
        "H Yang"
      ],
      "year": "2017",
      "venue": "31st AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "64",
      "title": "Multimodal behavioral markers exploring suicidal intent in social media videos",
      "authors": [
        "A Shah",
        "V Vaibhav",
        "V Sharma",
        "M Ismail",
        "J Girard",
        "L Morency"
      ],
      "year": "2019",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "65",
      "title": "Two-stream convolutional networks for action recognition in videos",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "66",
      "title": "SecureNN: 3-party secure computation for neural network training",
      "authors": [
        "S Wagh",
        "D Gupta",
        "N Chandran"
      ],
      "year": "2019",
      "venue": "Proceedings on Privacy Enhancing Technologies"
    },
    {
      "citation_id": "67",
      "title": "Falcon: Honest-majority maliciously secure framework for private deep learning",
      "authors": [
        "S Wagh",
        "S Tople",
        "F Benhamouda",
        "E Kushilevitz",
        "P Mittal",
        "T Rabin"
      ],
      "year": "2021",
      "venue": "Proceedings on Privacy Enhancing Technologies"
    },
    {
      "citation_id": "68",
      "title": "Privacy-preserving deep visual recognition: An adversarial learning framework and a new dataset",
      "authors": [
        "H Wang",
        "Z Wu",
        "Z Wang",
        "Z Wang",
        "H Jin"
      ],
      "year": "2019",
      "venue": "Privacy-preserving deep visual recognition: An adversarial learning framework and a new dataset",
      "arxiv": "arXiv:1906.05675"
    },
    {
      "citation_id": "69",
      "title": "Privacy-preserving action recognition using coded aperture videos",
      "authors": [
        "Z Wang",
        "V Vineet",
        "F Pittaluga",
        "S Sinha",
        "O Cossairt",
        "S Kang"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "70",
      "title": "Learning visual emotion representations from web data",
      "authors": [
        "Z Wei",
        "J Zhang",
        "Z Lin",
        "J Lee",
        "N Balasubramanian",
        "M Hoai",
        "D Samaras"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "71",
      "title": "Learning actor relation graphs for group activity recognition",
      "authors": [
        "J Wu",
        "L Wang",
        "Li Wang",
        "J Guo",
        "G Wu"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "72",
      "title": "Wyden pushes for stronger security in collection of personal information",
      "authors": [
        "R Wyden"
      ],
      "year": "2017",
      "venue": "Wyden pushes for stronger security in collection of personal information"
    },
    {
      "citation_id": "73",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "74",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "34th AAAI Conference on Artificial Intelligence"
    }
  ]
}