{
  "paper_id": "2110.08020v1",
  "title": "Multimodal Emotion-Cause Pair Extraction In Conversations",
  "published": "2021-10-15T11:30:24Z",
  "authors": [
    "Fanfan Wang",
    "Zixiang Ding",
    "Rui Xia",
    "Zhaoyu Li",
    "Jianfei Yu"
  ],
  "keywords": [
    "Ohh",
    "get a room Yeah",
    "she couldn't live without the Chan Love"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion cause analysis has received considerable attention in recent years. Previous studies primarily focused on emotion cause extraction from texts in news articles or microblogs. It is also interesting to discover emotions and their causes in conversations. As conversation in its natural form is multimodal, a large number of studies have been carried out on multimodal emotion recognition in conversations, but there is still a lack of work on multimodal emotion cause analysis. In this work, we introduce a new task named Multimodal Emotion-Cause Pair Extraction in Conversations, aiming to jointly extract emotions and their associated causes from conversations reflected in multiple modalities (text, audio and video). We accordingly construct a multimodal conversational emotion cause dataset, Emotion-Causein-Friends, which contains 9,272 multimodal emotion-cause pairs annotated on 13,509 utterances in the sitcom Friends. We finally benchmark the task by establishing a baseline system that incorporates multimodal features for emotion-cause pair extraction. Preliminary experimental results demonstrate the potential of multimodal information fusion for discovering both emotions and causes in conversations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the field of textual emotion analysis, previous research mostly focused on emotion recognition. In recent years, emotion cause analysis, a new task which aimed at extracting potential causes given the emotions  (Lee et al., 2010; Chen et al., 2010; Gui et al., 2016b)  or jointly extracting emotions and the corresponding causes in pairs  (Xia and Ding, 2019; Ding et al., 2020a; Wei et al., 2020; Fan et al., 2020) , has received much attention. These studies were normally carried out based on news articles or microblogs.  Poria et al. (2021)  further introduced an interesting task to recognize emotion cause in textual dialogues.\n\nHowever, conversation in its natural form is multimodal. Multimodality is especially important for discovering both emotions and their causes in conversations. For example, we do not only rely on the speaker's voice intonation and facial expressions to perceive his emotions, but also depend on some auditory and visual scenes to speculate the potential causes that trigger the speakers' emotions beyond text. Although a large number of studies have explored multimodal emotion analysis in conversations  (Busso et al., 2008; McKeown et al., 2012; Poria et al., 2019) , to our knowledge, at present there is still a lack of research on multimodal emotion cause analysis in conversations.\n\nIn this work, we introduce a new task named Multimodal Emotion-Cause Pair Extraction in Conversations (MC-ECPE), with the goal to extract all potential pairs of emotions and their corresponding causes from a conversation in consideration of three modalities  (text, audio and video) . We accordingly construct a multimodal emotion cause dataset, Emotion-Cause-in-Friends (ECF), by using the sitcom Friends as the source. The ECF dataset contains 1,344 conversations and 13,509 utterances 1  , where 9,272 emotion-cause pairs are annotated, covering three modalities.\n\nFigure  1  displays a real conversation in the ECF dataset, where Chandler and his girlfriend Monica walked into the casino, hugging each other (they had a quarrel earlier but made up soon), and then started a conversation with Phoebe. In Utterance 1 (U 1 for short), Chandler said hello to Phoebe with a Joy emotion (the cause is greeting). Phoebe's Surprise emotion in U 2 is caused by the event that Chandler and Monica had made up (reflected by the textual modality in U 2 ). This is also the cause of Monica's Joy emotion in U 3 and Chandler's Joy emotion in U 4 . Chandler's Joy emotion in U 4 has another cause -Monica's opinion in U 3 (\"I couldn't be mad at him for too long\"). The cause for Phoebe's Disgust in U 5 is the event that Monica and Chandler were kissing in front of her. This is not explicitly expressed by the textual modality, but is mainly reflected in the visual modality of U 5 . For this example, it is expected to extract a set of six utterance-level emotion-cause pairs: {U\n\nWe finally benchmark the MC-ECPE task by establishing a baseline system adapted from a representative textual ECPE approach. We incorporate multimodal features for utterance representation, extract emotion utterances and cause utterances respectively, and finally construct emotion-cause pairs. The experimental results demonstrate the effect of multimodal information fusion for discovering both emotions and causes in conversations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion Cause Analysis: Emotion cause extraction (ECE) is a subtask of emotion analysis. It was originally proposed by  Lee et al. (2010) , with the goal to extract cause spans of a given emotion in the text. Based on the same task setting, some researchers use rule-based methods  (Neviarouskaya and Aono, 2013; Li and Xu, 2014; Gao et al., 2015a,b; Yada et al., 2017)  or machine learning methods  (Ghazi et al., 2015; Song and Meng, 2015)  to extract emotion causes in their own corpus.\n\nBy analyzing the corpus proposed by Lee et al. (  2010 ),  Chen et al. (2010)  pointed out that clause may be a more suitable unit for cause annotation, and proposed to extract emotion cause at clause granularity. After that, a lot of work based on this task setting appeared  (Russo et al., 2011; Gui et al., 2014) . Especially,  Gui et al. (2016a)  released an open Chinese emotion cause dataset. This dataset has received extensive attention and become the benchmark dataset for the ECE task. Based on this corpus, many traditional machine learning methods  (Gui et al., 2016a,b; Xu et al., 2017)  and deep learning methods  (Gui et al., 2017; Li et al., 2018; Yu et al., 2019; Xu et al., 2019; Ding et al., 2019; Xia and Ding, 2019)  were put forward.\n\nHowever, there are two shortcomings in the ECE task: 1) emotions must be manually annotated before cause extraction, which greatly limits its practical application; 2) the way of annotating emotions first and then extracting causes ignores the fact that emotions and causes are mutually indicative. To solve these problems,  Xia and Ding (2019)  proposed a new task called emotion-cause pair extraction (ECPE), aiming at extracting potential emotions and corresponding causes from documents simultaneously. They further constructed the ECPE dataset based on the benchmark corpus for ECE  (Gui et al., 2016a) . After that, a lot of work on the ECPE task has been put forward to solve the shortcomings of the existing methodology  (Ding et al., 2020a,b; Wei et al., 2020; Fan et al., 2020) .\n\nThe above studies mostly focused on emotion cause analysis in news articles  (Gui et al., 2016a; Gao et al., 2017; Bostan et al., 2020) , microblogs  (Cheng et al., 2017)  and fictions  (Gao et al., 2017; Kim and Klinger, 2018) . Recently,  Poria et al. (2021)   that conversation itself is multimodal, we further propose to jointly extract emotions and their corresponding causes from conversations based on multiple modalities, and accordingly create a multimodal conversational emotion cause dataset.\n\nEmotion Recognition in Conversations: Although there's a lack of research on multimodal emotion cause analysis, many studies have been carried out on multimodal emotion recognition using textual, acoustic, and visual modalities, especially in conversations  (Busso et al., 2008; McKeown et al., 2012; Poria et al., 2019; Hazarika et al., 2018; Jin et al., 2020) .\n\nIn recent years, due to the increasing amount of open conversation data, the Emotion Recognition in Conversations (ERC) task has received continuous attention in the field of NLP. So far, there have been some publicly available datasets for ERC. IEMOCAP  (Busso et al., 2008)  contains multimodal dyadic conversations of ten actors performing the emotional scripts. SEMAINE  (McKeown et al., 2012)  contains multimodal data of robothuman conversations (it does not provide emotion categories, but the attributes of four emotion dimensions). The above two datasets are relatively small in scale and do not contain multi-party conversations. DailyDialog  (Li et al., 2017)  is a large dataset that contains the texts of daily conversations covering 10 topics , but the neutral utterances in it account for a high proportion. EmoContext  (Chat-terjee et al., 2019)  has a large total number of utterances, but only contains two-person conversations in plain text, with only three utterances in each conversation.  EmotionLines (Hsu et al., 2018)  contains two datasets: multi-party conversations from the sitcom Friends (Friends) and private chats on Facebook Messenger (EmotionPush) where all the utterances are labeled with emotion categories.  Poria et al. (2019)  extended EmotionLines (Friends) to the multimodal dataset MELD with raw videos, audio segments and transcripts, the size of which is moderate. Recently,  Firdaus et al. (2020)  constructed a large-scale multimodal conversational dataset MEISD from 10 famous TV series, where an utterance may be labeled with multiple emotions along with their corresponding intensities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task",
      "text": "We first clarify the definitions of emotion and cause in our work:\n\n• Emotion is a psychological state associated with thought, feeling and behavioral response.\n\nIn computer science, emotions are often described as discrete emotion categories, such as Ekman's six basic emotions including Anger, Disgust, Fear, Joy, Sadness and Surprise (Ekman, 1971). In conversations, emotions are usually annotated at the utterance level  (Li et al., 2017; Hsu et al., 2018; Poria et al., 2019) .\n\n• Cause refers to the explicitly expressed event or argument that is highly linked with the corresponding emotion  (Lee et al., 2010; Chen et al., 2010; Russo et al., 2011) . In this work, we use an utterance to describe an emotion cause. Although we have annotated the textual span if the cause is reflected in the textual modality, we only consider utterance-level emotion and cause extraction in this work, in order to facilitate the representation and fusion of multimodal information.\n\nThere are two kinds of textual emotion cause analysis task: emotion cause extraction (ECE)  (Gui et al., 2016b)  and emotion-cause pair extraction (ECPE)  (Xia and Ding, 2019) . The goal of ECE is to extract the potential causes given the emotion annotation; while ECPE aims to jointly extract the emotions and the corresponding causes in pairs, which solves ECE's shortcoming of emotion annotation dependency and improves the performance of emotion and cause extraction.\n\nTherefore, in this work we directly define the task of multimodal emotion cause analysis under ECPE rather than ECE. Given a conversation D = [U 1 , . . . , U i , . . . , U |D| ], in which each utterance is represented by the text, audio and video , i.e., U i = [t i , a i , v i ], the goal of MC-ECPE is to extract a set of emotion-cause pairs:\n\nwhere U e denotes an emotion utterance and U c is the corresponding cause utterance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset Source",
      "text": "The conversations in sitcoms usually contain more emotions than other TV series and movies. Hsu et al. (  2018 ) constructed the EmotionLines dataset from the scripts of the popular sitcom Friends for the ERC task.  Poria et al. (2019)  extended Emotion-Lines to a multimodal dataset MELD, by extracting the audio-visual clip from the source episode, and re-annotating each utterance with emotion labels. We find that sitcoms also contain rich emotion causes, therefore we choose MELD 2 as the data source and further annotate the corresponding causes for the given emotion annotations. We drop a few conversations where three modalities are completely inconsistent in timestamps.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Annotation Procedure",
      "text": "Given a multimodal conversation, for the emotion (one of Ekman's six basic emotions) labeled on each utterance, the annotator should annotate the utterances containing corresponding causes, label the types of causes, and mark the textual cause spans if the causes are explicitly expressed in the textual modality.\n\nWe first develop detailed annotation instructions and then employ three annotators who have reasonable knowledge of our task to annotate the entire dataset independently. After annotation, we determine the cause utterances by majority voting and take the largest boundary (i.e., the union of the spans) as the gold annotation of textual cause span, similar as  (Gui et al., 2016a; Bostan et al., 2020) . If there are further disagreements, another expert is invited for the final decision.\n\nTo improve the annotation efficiency, we furthermore develop a multimodal emotion cause annotation toolkit 3  . It is a general toolkit for multimodal annotation in conversations, with the functions of multimodal signal alignment, quick emotion-cause pair selection, multiple users and tasks manage-   ment, distributable deployment, etc. Figure  2  displays the interface of the toolkit.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Annotation Quality Assessment",
      "text": "To evaluate the quality of annotation, we measure the inter-annotator agreement on the full set of annotations, based on Cohen's Kappa and Fleiss' Kappa. Cohen's Kappa is used to measure the consistency of any two annotators (Cohen, 1960), while Fleiss' Kappa is used to measure the overall annotation consistency among three annotators  (McHugh, 2012) . The agreement scores are reported in Table  3 .\n\nIt can be seen that the Kappa coefficients are all higher than 0.6, which indicates a substantial agreement between three annotators  (Landis and Koch, 1977) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset Statistic And Analysis",
      "text": "Overall Statistics: As shown in Table  4 , the ECF dataset contains 1,344 conversations and 13,509 utterances from three modalities, where 7,528 emo-  tion utterances and 9,272 emotion-cause pairs have been annotated. In other words, about 55.73% of the utterances are annotated with one of the six basic emotions, and 91.34% of the emotions are annotated with the corresponding causes in our dataset. The number of pairs is larger than 6,876, which indicates that one emotion may be triggered by multiple causes in different utterances.\n\nIn Table  1 , we compare our ECF dataset with the related datasets in the field of emotion cause analysis and emotion recognition in conversations, in terms of modality, scene, and size.\n\nEmotion/Cause Distribution: For each emotion category, the proportion of emotion utterances annotated with causes is shown in Figure  4 . It can be seen that the distribution of emotion categories is unbalanced, and the proportion of emotion having causes varies slightly with the emotion category.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Types Of Emotion Causes:",
      "text": "In Table  2 , we furthermore summarized the emotion causes in our dataset into four types.   • Event: The speaker's emotion is caused by something that happens in a particular situation, which is normally a fact. This type of cause may be reflected in the three modalities.\n\n• Opinion: The speaker's emotion is triggered by someone's feelings or thoughts about people or things rather than a fact. Such causes are only expressed in texts.\n\n• Emotional Influence: The speaker's emotion is sometimes induced by the counterpart's emotion. This type of cause is normally embodied in three modalities jointly.\n\n• Greeting: As an act of giving a sign of welcome or recognition in communication, the greeting is a cause for the Joy emotion in daily conversations. It can be reflected in both textual and visual modalities.\n\nWe can see that \"Event\" covers the largest per-centage of emotion causes (68.30%), followed by \"Opinion\" (25.11%), which suggests that most of the emotions in conversations are triggered by specific events or subjective opinions.\n\nIt is also worth noting that 8% of the emotion causes in our dataset are the events mainly reflected in the acoustic or visual modalities. For example, Phoebe was disgust because Monica and Chandler were kissing in front of her in the scene shown in Figure  1 , we cannot speculate on such causes only based on the textual content obviously.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Baseline",
      "text": "In this section, we benchmark our MC-ECPE task by proposing a baseline system named MC-ECPE-2steps, which is adapted from a representative ECPE-2steps  (Xia and Ding, 2019)  approach for ECPE in news articles.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Main Framework",
      "text": "The main framework of MC-ECPE-2steps is shown in Figure  3 .\n\nStep 1 aims to extract a set of emotion utterances and a set of cause utterances individually via multi-task learning. We first obtain the independent utterance representations u i through word-level encoder and then feed them into two utterance-level encoders. The hidden states r e i and r c i , which can be viewed as the emotion-specific representation and cause-specific representation of utterance U i , are used to perform emotion prediction and cause prediction respectively.\n\nStep 2 performs emotion-cause pairing and filtering. We combine all predicted emotions and causes into pairs, obtain the pair representation through BiLSTM and attention mechanism, and finally filter out the pairs that do not contain a causal relationship via a feed-forward neural network.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Features",
      "text": "Since the emotions and causes in this paper are defined on the multimodal utterances, we further extract the features from three modalities and then concatenate them to obtain the independent multimodal representation of each utterance, i.e.,\n\nText: We initialize each token with pre-trained 300-dimensional GloVe vectors  (Pennington et al., 2014) , feed them into a BiLSTM and then obtain the textual features of each utterance t i after an attention mechanism.\n\nAudio: We adopt the 1611-dimensional acoustic features a i extracted by  Poria et al. (2019)  using openSMILE in their MELD dataset.\n\nVideo: We apply 3D-CNN  (Ji et al., 2012)  to extract the 128-dimensional global scene features v i from the video of each utterance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Settings And Metrics",
      "text": "The maximum number of utterances in each conversation and the maximum number of words in each utterance are both set to 35. The dimensions of word embedding and relative position are set to 300 and 50, respectively. The hidden dimension of BiLSTM is set to 100. All models are trained based on the Adam optimizer with a batch size of 32 and a learning rate of 0.005. The dropout ratio is set to 0.5, and the weight of L 2 -norm regularization is set to 1e-5.\n\nWe divide the dataset into training, validation and testing sets in a ratio of 7:1:2 at the conversation level. In order to obtain statistically credible results, we repeat all the experiments 20 times and report the average results. The precision, recall and F 1 score defined in  Xia and Ding (2019)  are used as the evaluation metrics for the MC-ECPE task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Overall Performance",
      "text": "In addition to MC-ECPE-steps, we further design four simple statistical methods for comparison, based on the observation that there is a certain trend in the relative positions between emotion utterances and cause utterances (i.e., most cause utterances are either the emotion utterances themselves or are immediately before their corresponding emotion utterances). In the training phase, we separately train an emotion classifier, and calculate the prior probability distribution of relative positions between the cause utterances and their corresponding emotion utterances. In the testing phase, we first obtain emotion utterances in two alternative ways: 1) emotion prediction (E Pred ), which is based on the trained emotion classifier, 2) emotion annotations (E True ), which are the ground truth labels of emotion prediction. Next, the relative position is assigned to each utterance in the document, which is the position of the current utterance relative to the given emotion utterance (for example, -2, -1, 0, +1, etc.). Then we use the following two strategies to randomly select a cause utterance for each emotion utterance according to the prior probability distribution.\n\n• C Multi-Bernoulli : We independently carry out a binary decision for each relative position to determine whether its corresponding utterance is the cause utterance. The selection probability of each relative position is calculated from the training set.\n\n• C Multinomial : We randomly select a relative position from all relative positions, and its corresponding utterance is the cause utterance. The selection probability of each relative position is calculated from the training set.\n\nThe experimental results on the MC-ECPE task are reported in table 5. We can see that the statistical methods perform much poorer than our baseline system MC-ECPE-2steps, which shows that it is not enough to select the emotion cause only based on the relative position.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Effectiveness Of Multimodal Features",
      "text": "To explore the effectiveness of different multimodal features, we conduct extended experiments and also report the results in Table  5 .\n\nIt can be seen that removing the acoustic features or visual features from the baseline system MC-ECPE-2steps (-Audio/-Video) leads to a decrease in F 1 score. When both the acoustic features and visual features are removed, the F 1 score of the system even drops by about 1.9% (-Audio -Video). Specifically, by integrating multimodal features, the baseline system can predict more causes that are reflected in the auditory and visual scenes, resulting in the great improvement in the recall rate. These results illustrate that it's beneficial to introduce multimodal information into the MC-ECPE task.\n\nIn addition, we found that the improvement brought by visual features is slightly lower than that brought by acoustic features. This indicates that it is challenging to perceive and understand the complex visual scenes in conversations, hence leaving much room for extra improvement in multimodal feature representation and multimodal fusion.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments On Mc-Ecpe With Emotion Category",
      "text": "We further conduct experiments on the extended task named \"MC-ECPE with emotion category\" which needs to predict an additional emotion category for each emotion-cause pair. Specifically, we convert the binary emotion classification to multi-class emotion classification in the first step of MC-ECPE-steps. We first evaluate the emotioncause pairs of each emotion category with F 1 score separately. To evaluate the overall performance, we further calculate a weighted average of F 1 scores across different emotion categories. Considering the imbalance of emotion categories in the dataset described in Section 4.4, we also report the weighted average F 1 score of the main four emotion categories except Disgust and Fear.\n\nThe experimental results on this task are reported in Table  6 . An obvious observation is that the performance on Surprise is the best, while that on Fear is the worst. The performance for different emotion categories significantly varies with the proportion of emotion and cause annotation shown in Figure  4 . It should be noted that emotion category imbalance is actually an inherent problem in the ERC task  (Li et al., 2017; Hsu et al., 2018; Poria et al., 2019) , which is of great challenge and needs to be tackled in future work.\n\nSimilar to the conclusion drawn on MC-ECPE, the performance of the baseline system is significantly reduced if not utilizing the acoustic and visual features, which demonstrates that multimodal fusion is also helpful for the task of MC-ECPE with emotion categories. Although there is much room for further improvement, our model is still effective and feasible. The relatively poor performance on this task indicates that it is more difficult to further predict the emotion categories based on MC-ECPE.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this work, we introduce a new emotion cause analysis task named Multimodal Emotion-Cause Pair Extraction (MC-ECPE) in Conversations. Secondly, we accordingly construct a multimodal conversational emotion cause dataset, Emotion-Causein-Friends (ECF), based on the American sitcom Friends. Finally, we establish a baseline system and demonstrate the importance of multimodal information for the MC-ECPE task.\n\nMC-ECPE is a challenging task, leaving much room for future improvements. The focus of this work is the introduction of the task and datasets, and we only propose a simple baseline system to benchmark the task. In the future, the following issues are worth exploring in order to further improve the performance of the task:\n\n• How to effectively model the impact of speaker relevance on emotion recognition and emotion cause extraction in conversations?\n\n• How to better perceive and understand the visual scenes to better assist emotion cause reasoning in conversations?\n\n• How to establish a multimodal conversation representation framework to efficiently align, interact and fuse the information from three modalities?",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: displays a real conversation in the ECF",
      "page": 1
    },
    {
      "caption": "Figure 1: An example of the annotated conversation in our ECF dataset. Each arc points from the cause utterance",
      "page": 2
    },
    {
      "caption": "Figure 2: The interface of our developed annotation",
      "page": 4
    },
    {
      "caption": "Figure 3: Overview of the baseline system MC-ECPE-2steps.",
      "page": 6
    },
    {
      "caption": "Figure 4: The distribution of emotions (with/without",
      "page": 6
    },
    {
      "caption": "Figure 1: , we cannot speculate on such causes only",
      "page": 6
    },
    {
      "caption": "Figure 3: Step 1 aims to extract a set of emotion utter-",
      "page": 6
    },
    {
      "caption": "Figure 4: It should be noted that emotion category imbalance",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 3: The inter-annotator agreement for utterance-",
      "data": [
        {
          "T": "A",
          "60.14%": "0.60%",
          "[U1] Phoebe: Ohh! You made up! (Surprise)\nEmotion-Cause Pair: (U1, U1)": "[U1] Chandler: What is wrong with Emma? (Sadness)\n[U2] Monica: Oh she misunderstood, she thought she was\nmoving to Tulsa. (Neutral)\nEmotion-Cause Pair: (U1, U1)\n*Note: Chandler heard Emma crying."
        },
        {
          "T": "V",
          "60.14%": "7.56%",
          "[U1] Phoebe: Ohh! You made up! (Surprise)\nEmotion-Cause Pair: (U1, U1)": "[U1] Phoebe: Ohh, get a room. (Disgust)\nEmotion-Cause Pair: (U1, U1)\n*Note: In the video, Monica and Chandler were kissing in\nfront of Phoebe, as shown in Figure 1."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Goodnewseveryone: A corpus of news headlines annotated with emotions, semantic roles, and reader perception",
      "authors": [
        "Laura Ana",
        "Maria Bostan",
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "year": "2020",
      "venue": "Proceedings of The 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Understanding emotions in text using deep learning and big data",
      "authors": [
        "Ankush Chatterjee",
        "Umang Gupta",
        "Manoj Kumar Chinnakotla",
        "Radhakrishnan Srikanth",
        "Michel Galley",
        "Puneet Agrawal"
      ],
      "year": "2019",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "4",
      "title": "Emotion cause detection with linguistic constructions",
      "authors": [
        "Ying Chen",
        "Sophia Yat",
        "Mei Lee",
        "Shoushan Li",
        "Chu-Ren Huang"
      ],
      "year": "2010",
      "venue": "Computational Linguistics (COLING)"
    },
    {
      "citation_id": "5",
      "title": "A coefficient of agreement for nominal scales. Educational and psychological measurement",
      "authors": [
        "Xiyao Cheng",
        "Ying Chen",
        "Bixiao Cheng",
        "Shoushan Li",
        "Guodong Zhou"
      ],
      "year": "1960",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)"
    },
    {
      "citation_id": "6",
      "title": "From independent prediction to reordered prediction: Integrating relative position and global label information to emotion cause identification",
      "authors": [
        "Zixiang Ding",
        "Huihui He",
        "Mengran Zhang",
        "Rui Xia"
      ],
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "7",
      "title": "ECPE-2D: Emotion-cause pair extraction based on joint two-dimensional representation, interaction and prediction",
      "authors": [
        "Zixiang Ding",
        "Rui Xia",
        "Jianfei Yu"
      ],
      "year": "2020",
      "venue": "Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "8",
      "title": "End-toend emotion-cause pair extraction based on sliding window multi-label learning",
      "authors": [
        "Zixiang Ding",
        "Rui Xia",
        "Jianfei Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "9",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1971",
      "venue": "Nebraska Symposium on Motivation. Nebraska Symposium on Motivation"
    },
    {
      "citation_id": "10",
      "title": "Transition-based directed graph construction for emotion-cause pair extraction",
      "authors": [
        "Chuang Fan",
        "Chaofa Yuan",
        "Jiachen Du",
        "Lin Gui",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "year": "2020",
      "venue": "Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "11",
      "title": "Meisd: A multimodal multi-label emotion, intensity and sentiment dialogue dataset for emotion recognition and sentiment analysis in conversations",
      "authors": [
        "Mauajama Firdaus",
        "Hardik Chauhan",
        "Asif Ekbal",
        "Pushpak Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "Emotion cause detection for chinese micro-blogs based on ecocc model",
      "authors": [
        "Kai Gao",
        "Hua Xu",
        "Jiushuo Wang"
      ],
      "year": "2015",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)"
    },
    {
      "citation_id": "13",
      "title": "A rulebased approach to emotion cause detection for chinese micro-blogs",
      "authors": [
        "Kai Gao",
        "Hua Xu",
        "Jiushuo Wang"
      ],
      "year": "2015",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "14",
      "title": "Overview of ntcir-13 eca task",
      "authors": [
        "Qinghong Gao",
        "Jiannan Hu",
        "Ruifeng Xu",
        "Gui Lin",
        "Yulan He",
        "Qin Lu",
        "Kam-Fai Wong"
      ],
      "year": "2017",
      "venue": "Proceedings of the NTCIR-13 Conference"
    },
    {
      "citation_id": "15",
      "title": "Detecting emotion stimuli in emotion-bearing sentences",
      "authors": [
        "Diman Ghazi",
        "Diana Inkpen",
        "Stan Szpakowicz"
      ],
      "year": "2015",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "A question answering approach to emotion cause extraction",
      "authors": [
        "Lin Gui",
        "Jiannan Hu",
        "Yulan He",
        "Ruifeng Xu",
        "Qin Lu",
        "Jiachen Du"
      ],
      "year": "2017",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "17",
      "title": "2016a. Event-driven emotion cause extraction with corpus construction",
      "authors": [
        "Lin Gui",
        "Dongyin Wu",
        "Ruifeng Xu",
        "Qin Lu",
        "Yu Zhou"
      ],
      "venue": "EMNLP"
    },
    {
      "citation_id": "18",
      "title": "Emotion cause extraction, a challenging task with corpus construction",
      "authors": [
        "Lin Gui",
        "Ruifeng Xu",
        "Qin Lu",
        "Dongyin Wu",
        "Yu Zhou"
      ],
      "year": "2016",
      "venue": "Chinese National Conference on Social Media Processing"
    },
    {
      "citation_id": "19",
      "title": "Emotion cause detection with linguistic construction in chinese weibo text",
      "authors": [
        "Lin Gui",
        "Li Yuan",
        "Ruifeng Xu",
        "Bin Liu",
        "Qin Lu",
        "Yu Zhou"
      ],
      "year": "2014",
      "venue": "CCF International Conference on Natural Language Processing and Chinese Computing"
    },
    {
      "citation_id": "20",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "21",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Chao-Chun",
        "Sheng-Yeh Hsu",
        "Chuan-Chun Chen",
        "Ting-Hao Kuo",
        "Lun-Wei Huang",
        "Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "22",
      "title": "3d convolutional neural networks for human action recognition",
      "authors": [
        "Shuiwang Ji",
        "Wei Xu",
        "Ming Yang",
        "Kai Yu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "23",
      "title": "Hierarchical multimodal transformer with localness and speaker aware attention for emotion recognition in conversations",
      "authors": [
        "Xiao Jin",
        "Jianfei Yu",
        "Zixiang Ding",
        "Rui Xia"
      ],
      "year": "2020",
      "venue": "CCF International Conference on Natural Language Processing and Chinese Computing"
    },
    {
      "citation_id": "24",
      "title": "Who feels what and why? annotation of a literature corpus with semantic roles of emotions",
      "authors": [
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "The measurement of observer agreement for categorical data",
      "authors": [
        "Richard Landis",
        "Gary Koch"
      ],
      "year": "1977",
      "venue": "biometrics"
    },
    {
      "citation_id": "26",
      "title": "A text-driven rule-based system for emotion cause detection",
      "authors": [
        "Sophia Yat",
        "Mei Lee",
        "Ying Chen",
        "Chu-Ren Huang"
      ],
      "year": "2010",
      "venue": "NAACL HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text"
    },
    {
      "citation_id": "27",
      "title": "Text-based emotion classification using emotion cause extraction. Expert Systems with Applications",
      "authors": [
        "Weiyuan Li",
        "Hua Xu"
      ],
      "year": "2014",
      "venue": "Text-based emotion classification using emotion cause extraction. Expert Systems with Applications"
    },
    {
      "citation_id": "28",
      "title": "A co-attention neural network model for emotion cause analysis with emotional context awareness",
      "authors": [
        "Xiangju Li",
        "Kaisong Song",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang"
      ],
      "year": "2018",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "29",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Interrater reliability: the kappa statistic",
      "authors": [
        "Mary Mchugh"
      ],
      "year": "2012",
      "venue": "Biochemia medica"
    },
    {
      "citation_id": "31",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Extracting causes of emotions from text",
      "authors": [
        "Alena Neviarouskaya",
        "Masaki Aono"
      ],
      "year": "2013",
      "venue": "International Joint Conference on Natural Language Processing (IJCNLP)"
    },
    {
      "citation_id": "33",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "34",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "Recognizing emotion cause in conversations",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Deepanway Ghosal",
        "Rishabh Bhardwaj",
        "Samson Yu Bai Jian",
        "Pengfei Hong",
        "Romila Ghosh",
        "Abhinaba Roy",
        "Niyati Chhaya"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "36",
      "title": "Emocause: an easy-adaptable approach to emotion cause contexts",
      "authors": [
        "Irene Russo",
        "Tommaso Caselli",
        "Francesco Rubino",
        "Ester Boldrini",
        "Patricio Martínez-Barco"
      ],
      "year": "2011",
      "venue": "Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA)"
    },
    {
      "citation_id": "37",
      "title": "Detecting concept-level emotion cause in microblogging",
      "authors": [
        "Shuangyong Song",
        "Yao Meng"
      ],
      "year": "2015",
      "venue": "World Wide Web (WWW)"
    },
    {
      "citation_id": "38",
      "title": "Effective inter-clause modeling for end-to-end emotion-cause pair extraction",
      "authors": [
        "Penghui Wei",
        "Jiahao Zhao",
        "Wenji Mao"
      ],
      "year": "2020",
      "venue": "Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "39",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "Rui Xia",
        "Zixiang Ding"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "40",
      "title": "Extracting emotion causes using learning to rank methods from an information retrieval perspective",
      "authors": [
        "Bo Xu",
        "Hongfei Lin",
        "Yuan Lin",
        "Yufeng Diao",
        "Liang Yang",
        "Kan Xu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "41",
      "title": "An ensemble approach for emotion cause detection with event extraction and multikernel svms",
      "authors": [
        "Ruifeng Xu",
        "Jiannan Hu",
        "Qin Lu",
        "Dongyin Wu",
        "Lin Gui"
      ],
      "year": "2017",
      "venue": "Tsinghua Science and Technology"
    },
    {
      "citation_id": "42",
      "title": "A bootstrap method for automatic rule acquisition on emotion cause extraction",
      "authors": [
        "Shuntaro Yada",
        "Kazushi Ikeda",
        "Keiichiro Hoashi",
        "Kyo Kageura"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Data Mining Workshops"
    },
    {
      "citation_id": "43",
      "title": "Multiple level hierarchical network-based clause selection for emotion cause extraction",
      "authors": [
        "Xinyi Yu",
        "Wenge Rong",
        "Zhuo Zhang",
        "Yuanxin Ouyang",
        "Zhang Xiong"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    }
  ]
}