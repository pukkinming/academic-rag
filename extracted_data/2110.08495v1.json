{
  "paper_id": "2110.08495v1",
  "title": "Hybrid Mutimodal Fusion For Dimensional Emotion Recognition",
  "published": "2021-10-16T06:57:18Z",
  "authors": [
    "Ziyu Ma",
    "Fuyan Ma",
    "Bin Sun",
    "Shutao Li"
  ],
  "keywords": [
    "Multi-modal Fusion",
    "Long Short-Term Memory",
    "Self Attention",
    "Gated Convolutional Neural Networks",
    "Continuous Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we extensively present our solutions for the MuSe-Stress sub-challenge and the MuSe-Physio sub-challenge of Multimodal Sentiment Challenge (MuSe) 2021. The goal of MuSe-Stress sub-challenge is to predict the level of emotional arousal and valence in a time-continuous manner from audio-visual recordings and the goal of MuSe-Physio sub-challenge is to predict the level of psycho-physiological arousal from a) human annotations fused with b) galvanic skin response (also known as Electrodermal Activity (EDA)) signals from the stressed people. The Ulm-TSST dataset which is a novel subset of the audio-visual textual Ulm-Trier Social Stress dataset that features German speakers in a Trier Social Stress Test (TSST) induced stress situation is used in both subchallenges. For the MuSe-Stress sub-challenge, we highlight our solutions in three aspects: 1) the audio-visual features and the biosignal features are used for emotional state recognition. 2) the Long Short-Term Memory (LSTM) with the self-attention mechanism is utilized to capture complex temporal dependencies within the feature sequences. 3) the late fusion strategy is adopted to further boost the model's recognition performance by exploiting complementary information scattered across multimodal sequences. Our proposed model achieves CCC of 0.6159 and 0.4609 for valence and arousal respectively on the test set, which both rank in the top 3. For the MuSe-Physio sub-challenge, we first extract the audiovisual features and the bio-signal features from multiple modalities. Then, the LSTM module with the self-attention mechanism, and the Gated Convolutional Neural Networks (GCNN) as well as the",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The emotional states play a crucial role in human work and daily life through affecting psychological and physiological status. It is noted that many people in modern societies are under a high level of stress  [1] . What's worse, the long-term accumulation of negative emotions may result in depression, which has an extremely severe impact on mental health. Thus, accurately recognizing human emotions is a fundamental but profound research direction. Emotion recognition systems have various applications, such as driver fatigue monitoring  [5] , health care especially mental health monitoring  [12] , building a secure and safe society  [30]  and human-robot interaction.\n\nAlthough the discrete categorical theory of emotion is a popular computing model for facial expression recognition  [18] , the continuous dimensional theory of emotion is the most commonly used for emotion prediction in time-continuous sequences. According to  [9] , many researchers have focused on dimensional models, such as the valence-arousal model  [32] . For example, Russel et al.  [24]  proposed a computing model of emotions characterized by two essential dimensions (i.e., valence and arousal). Marsella et al.  [19]  developed a model that treated a specific emotional state as a data point in the continuous space, which was described mathematically by 3 dimensions corresponding to arousal (a measure of affective activation), valence (a measure of pleasure), and dominance (a measure of control). Therefore, the continuous dimensional theory of emotion is suitable to compute the sentiments scattered across multi-modal sequences, because it can describe more subtle, continuous and complicated emotional states.\n\nAs we know, it is significantly subjective to annotate the emotions of human beings. However, multiple professional annotators must continuously annotate multi-modal sequences, which is time-consuming and laborious. Therefore, how to produce an agreed-upon representation (gold standard) from multiple labelers still remains an open research question, with few methods available. For example, given the level of disagreement, Michael et al.  [11]  proposed to use the Evaluator Weighted Estimator to weight different annotators based on the likelihood of agreement. It is limited for the research on the fusion of perceived emotional signals with extra physiological signals and there also has been minimal research on the combination between physiological and perceived arousal gold standard. Therefore, in the 2021 edition of the Multimodal Sentiment in-the-wild (MuSe) challenge  [27]  , apart from predicting the level of emotional arousal and valence in a timecontinuous manner from people in stressed dispositions for the MuSe-Stress sub-challenge, the signal of arousal is fused with the Electrodermal Activity (EDA) and then used as a prediction target for the MuSe-Physio sub-challenge. Participants are provided with the multimodal Ulm-TSST data-base, where subjects were recorded under a highly stress-induced free speech task.\n\nIn the MuSe-Stress sub-challenge, eGeMAPS is the best audio feature for the prediction of valence and arousal, achieving 0.5018 CCC and 0.4416 CCC on the test set respectively which reported in the baseline paper. VGGface is the best vision feature for the prediction of valence and arousal, achieving 0.4529 CCC and 0.1579 CCC on the test set respectively which reported in the baseline paper. Therefore, we choose eGeMAPS and VGGface feature sets for the MuSe-Stress sub-challenge in this paper. Apart from the audio features (eGeMAPs), the visual features (VGGface), the biosignal features (Electrocardiogram (ECG), Respiration (RESP), and heart rate (BPM)) are also selected. We apply the Long Short-Term Memory (LSTM) with the self-attention module for modeling the complex temporal dependencies in the sequence from the audiovisual modalities and the LSTM network is used to capture the complex and continuous dependencies within the sequences from bio-signal modality. Finally, we concatenate all multi-modal features and subsequently send them into the regression model.\n\nIn the MuSe-Physio sub-challenge, the late fusion of the best audio (VGGish) and video (VGGface) predictions yield the best results, namely 0.4913 CCC on development data and 0.4908 CCC on test data according to the baseline paper. Therefore, we choose VGGish and VGGface feature sets for the MuSe-Physio sub-challenge in this paper. In addition, the bio-signal features (ECG, RESP, and BPM) are also the input of the model. Then, we utilize the LSTM with the self-attention mechanism module and the Gated Convolutional Neural Networks (GCNN) as well as the LSTM network for modeling the complex temporal dependencies in the sequence from the audio-visual modality respectively. The bio-signal features are concatenated and sent to the LSTM network. Finally, we concatenate all types of multi-modal features before sending them into the regression model.\n\nOur main contributions to the MuSe 2021 challenge in this paper can be summarized as follows:\n\nâ€¢ We investigate the impact of various features including audio features, visual features and bio-signal features in both sub-challenges. We also demonstrate the effectiveness of the bio-signal features, which has not been well-used in corresponding sub-challenges of previous MuSe. The rest of this paper is structured as follows: Section 2 introduces the related work. The multimodal features are presented in Section 3. The methods are described in Section 4. The experimental setting and the results are presented in Section 5. Finally, we make the conclusive remarks and indicates the possible future directions in Section.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multi-modal Features: Various multimodal features have been well investigated in the past series of AVECs, which are the predecessor of MuSe. Before the deep learning era, competitors tended to use carefully-handcrafted features for regression. For example, SÃ¡nchez-Lozano et al.  [25]  proposed to extract the Gabor feature and the Local Binary Patterns (LBP) for the individual visual modality and extract low-level features for the single audio modality. With the deep learning technique showing superior performance over carefully-handcrafted features, participants have been more likely to learn deep representations for both modalities by different variants of very deep networks. The methods  [4]    [15]  in AVEC 2017 demonstrated that utilizing the convolutional neural networks for the visual representations obtained comparable and even better results than carefully-engineered features. Furthermore, the results of  [33]  in AVEC 2018 demonstrated that the learned audio representations from the pretrained VGGish model could outperform expert-knowledge based acoustic features. Chen et al.  [2]  proposed to use 2D+1D convolutional neural networks in the very recent AVEC 2019, which also proved the superior performance of learned audio-visual features. Apart from audio-visual modalities, the linguistic features play an important part in multimodal emotion analysis  [23] . In AVEC 2017, the textual modality was first introduced to solve the recognition problem. At the very beginning, participants used the classic bag-of-words features  [15] . Subsequently, the word vectors (such as Word2Vec  [20]  and GloVe  [22] ) were widely applied because of their efficiency and effectiveness, which were pre-trained on large-scale text datasets.\n\nModel Architecture: Support vector regression (SVR) is the most commonly used as the baseline methods in previous AVECs. However, because of its non-temporal disadvantage, SVR cannot use the temporal and contextual information well to promote the performance of emotion recognition and sentiment analysis. With the increasing popularity of deep neural networks, the Recurrent Neural Network (RNN) has outperformed better in sequence modeling over other methods. Therefore, the 1st place winners of recent AVECs all adopted the LSTM network without exception, for continuous sentiment analysis. Previous methods  [31] ,  [4]  both utilized the LSTM and SVR to perform emotion regression. The experimental results demonstrated that the LSTM outperformed SVR dramatically. Compared with RNN based methods, a fully convolutional network was designed by Du et al.  [6] . It performed emotion recognition in a coarse-to-fine strategy by aggregating multi-level features with various scales. Huang et al.  [16]  utilized various types of temporal architectures to capture temporal contextual relationships within sequences. Their studies also revealed that the combination of various models resulted in better results.\n\nMulti-modal Fusion: It is indispensable for researchers to utilize multi-modal fusion to boost the performance of emotion prediction systems. Huang et al.  [15]  adopted the late fusion technique to aggregate the predictions based on different types of features in AVEC 2017, while the early fusion strategy was used by Chen et al.  [4]  to capture the inter-modal dynamics. In AVEC 2018, the detail comparison results of these methods were described by Huang et al.  [14] . The comparison demonstrated that methods with the late fusion predicted arousal and valence better than these with the early fusion. In AVEC 2019, Chen et al.  [3]  proposed to utilize both the early and the late fusion techniques in one model. They first trained the deep Bidirectional Long Short-Term Memory networks (Bi-LSTM) for unimodal features. Afterwards, Bi-LSTMs were applied for fusing corresponding bi-modal features in an early stage. Finally, the predictions of various models were fused late by a second level Bi-LSTM. Moreover, Huang et al.  [17]  proposed to fuse multimodal information by the cross-modal attention module. Their results showed the proposed cross-modal attention could outperform better over both the early and the late fusion techniques.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Features 3.1 Visual Features",
      "text": "VGGface Feature: The visual geometry group of Oxford introduced the deep CNN referred to as VGG16  [26] . As a variant of VGG16, the VGGface architecture was initially intended for supervised facial recognition purposes  [26] , which was trained on 2.6 million faces. To get rid of irrelevant background information, we first apply MTCNN to detect and align faces in videos. VGGface  [21]  is then used to extract general facial features, which can present high-level and discriminative features compared with other facial recognition models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Acoustic Features",
      "text": "eGeMAPS Feature: We use the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) feature provided by the organizers of MuSe 2021, which contains 23 acoustic low-level descriptors (LLDs  [7] . The freely and publicly available openSMILE toolkit  [8]  can be used to extract the eGeMAPS feature. Several statistical functions of the openSMILE toolkit can be directly applied to extract segment-level features with an 88-dimensional vector.\n\nVGGish Feature: VGGish  [13]  was pre-trained on AudioSet  [10] . Although VGGish was originally proposed for audio classification, previous studies, such as  [34] , demonstrated that the learned deep feature representation from trained VGGish outperformed carefullyhandcrafted acoustic features. Therefore, the pre-trained VGGish model is used to extract acoustic features in our work. We first divided the recordings into multiple 0.975 s frames with a hop size of 0.25s to match with the ground-truth labels. Afterwards, we extract the log spectrograms from these frames. Subsequently, they are sent into the VGGish model. The 128-dimensional embeddings from the output of fc2 layer are extracted as the final VGGish feature.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Biological Features",
      "text": "ECG, RESP and BPM Feature: Three biological signals (i.e., Electrocardiogram (ECG), Respiration (RESP), and heart rate (BPM)) were captured at a sampling rate of 1 kHz. We use the official biological features at a sampling rate of 2Hz as the inputs for our model. It is noted that the bio-signals are newly featured for MuSe 2021, which has not been well investigated for continuous dimensional emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods 4.1 Muse-Stress Sub-Challenge",
      "text": "In this sub-challenge, the LSTM networks with the self-attention mechanism is utilized to capture complex temporal dependencies within the feature sequences. Different from  [29]  , the bio-signal features (Electrocardiogram (ECG), Respiration (RESP), and heart rate (BPM)) are also selected. Figure  1  demonstrates the architecture of our model, which contains three modules: the LSTM networks with the self-attention module for the acoustic and visual features, the LSTM module for the bio-signal features and the late fusion module. First, we feed the eGeMAPS feature and the VGGface feature to the LSTM network with self-attention module respectively and obtain the predictions from both audio and visual modalities. Then, the bio-signal features are concatenated and sent to the LSTM module in order to get the predictions of bio-signal modality. Finally, the predictions from above modalities are concatenated and sent to the late fusion module for regression.\n\nSelf-attention Mechanism: The self-attention mechanism is utilized to transform the input sequences into high-level representations, which further captures the relationships across the sequences.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Given An Input Acoustic Sequence ğ‘¿",
      "text": "where ğ‘‘ ğ‘ is the dimension of the acoustic sequence and |ğ‘‡ | is the max time step. Q a , K a and V a represent the queries, keys and values matrices respectively which mapped by the input acoustic sequence ğ‘¿ ğ‘ . ğ‘‘ ğ‘˜ = ğ‘‘ ğ‘ /â„ is the scale factor and â„ represents the number of heads. For simplicity, we override the whole operation as a mapping ATT:\n\nIn our model, the visual feature is also sent to the self-attention module to capture the contextual information. Similarly, given an input visual sequence\n\nwhere ğ‘‘ ğ‘£ is the dimension of the visual sequence.\n\nLSTM: In our model, the outputs of the self-attention modules are sent to the LSTM to capture the complex temporal dependencies within the sequence. Given the input embedding sequence ğ‘ª ğ‘ , the hidden semantic feature is repeatedly computed at the ğ‘–-th step by:\n\nwhere ğ‘“ ğ¿ğ‘†ğ‘‡ ğ‘€ represents the LSTM function. For simplicity, we override the whole operation as a mapping LSTM 1 :\n\nwhere ğ‘¯ ğ‘ ğ‘– âˆˆ R â„ 1 and â„ 1 represents the dimension of LSTM 1 . Similarly, given the input embedding sequence ğ‘ª ğ‘£ , the LSTM output is calculated by:\n\nwhere ğ‘¯ ğ‘£ ğ‘– âˆˆ R â„ 2 and â„ 2 represents the dimensionality of LSTM 2 . Finally, ğ‘¯ ğ‘ and ğ‘¯ ğ‘£ are sent to the fully connected layers to obtain the emotion predictions respectively. For the audio modality, the emotion prediction Å¶ ğ‘ = Å¶ ğ‘ 1 , . . . , Å¶ ğ‘ |ğ‘‡ | is given by\n\nwhere\n\nwhere ğ‘‘ ğ‘’ , ğ‘‘ ğ‘Ÿ and ğ‘‘ ğ‘ represent the dimension of the ECG feature, RESP feature and BPM feature respectively. Afterwards, ğ‘¿ ğµ âˆˆ R ğ‘‡ Ã— (ğ‘‘ ğ‘’ +ğ‘‘ ğ‘Ÿ +ğ‘‘ ğ‘ ) is sent to the LSTM and the output is given by:\n\nwhere ğ‘¯ ğµ ğ‘– âˆˆ R â„ 3 and â„ 3 represents the dimension of LSTM 3 . Finally, the emotion prediction are made by a fully connected layer following the LSTM layer:\n\nwhere ğ‘¾ ğµ âˆˆ R â„3Ã—1 , and ğ’ƒ ğµ represents the bias. Late Fusion: In this sub-challenge, we fuse multi-modal features using the late fusion strategy. First, Å¶ ğ‘ , Å¶ ğ‘£ and Å¶ ğµ are concatenated by:\n\nwhere\n\nSusequently, ğ’€ ğ‘“ is sent to the LSTM model to get hidden states by:\n\nwhere ğ‘¯ ğ‘“ ğ‘– âˆˆ R â„ 4 , and â„ 4 represents the dimension of LSTM 4 . Finally, the fusion emotion prediction Å¶ ğ‘“ = Å¶ ğ‘“ 1 , . . . , Å¶ ğ‘“ |ğ‘‡ | are made through a fully connected layer following the LSTM layer:\n\nwhere ğ‘¾ ğ‘“ âˆˆ R â„4Ã—1 , and ğ’ƒ ğ‘“ represents the bias.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Muse-Physio Sub-Challenge",
      "text": "In this subsection, we will describe the details of the method for the MuSe-Physio sub-challenge. Based on the model designed for the MuSe-Stress sub-challenge, the GCNN-LSTM-based model is added and used for modeling the temporal dependencies of acoustic feature and visual feature. It is noted that we use the VGGish feature instead of eGeMAPS feature in this sub-challenge. Figure  2  shows the architecture of our method, which contains four modules: the LSTM network with self-attention mechanism module for the acoustic and visual features, the gated CNN module with the LSTM network module for the acoustic and visual features, the LSTM module for the bio-signal features and the late fusion module. Firstly, we send the eGeMAPS feature and the VGGface feature to the LSTM network with self-attention module respectively and get the predictions from audio-visual modalities. Then, the same features are also sent to the gated CNN module with the LSTM network module respectively and get the predictions of audio and video modalities. Thirdly, the bio-signal features are concatenated and sent to the LSTM module in order to get the predictions of biosignal modality. Finally, all of the predictions from above modalities are concatenated and sent to the late fusion module. GCNN-LSTM-based Model: In this sub-challenge, we first train the model consisted of stacked gated convolution blocks with a LSTM layer and a fully connected layer. For the gated block, the input acoustic sequence ğ‘¿ ğ‘ to the output layer is defined as:\n\nwhere sigm is the sigmoid activation function, conv represents the convolution operation, âŠ™ is the Hadamard product between two tensors. ğ‘® ğ‘ âˆˆ R ğ‘‡ Ã—ğ‘‘ ğ‘ ğ‘› , ğ‘‘ ğ‘ ğ‘› is the number of channels for convolution. ğ‘¾ ğ‘ âˆˆ R ğ‘‘ ğ‘ Ã—ğ‘‘ ğ‘› and ğ’ ğ‘ âˆˆ R ğ‘‘ ğ‘ Ã—ğ‘‘ ğ‘› are parameters of the convolution layers. For simplicity, we override the whole operation as a mapping GCNN 1 :\n\nğ‘® ğ‘ = GCNN 1 (ğ‘¿ ğ‘ ) (16) Similarly, given the input visual sequence ğ‘¿ ğ‘£ , the GCNN output is given by:\n\nğ‘› is the number of channels for convolution. Then, the GCNN output is sent to the LSTM to capture the temporal dependencies within the feature sequences. Given the input embedding sequence ğ‘® ğ‘ , the LSTM output is given by:\n\nwhere ğ‘¯ ğ‘1 ğ‘– âˆˆ R â„ 5 and â„ 5 represents the dimension of LSTM 5 . Similarly, given the input embedding sequence ğ‘® ğ‘£ , we can get the LSTM output ğ‘¯ ğ‘£1 âˆˆ R ğ‘‡ Ã—â„ 6 , where â„ 6 represents the dimension of LSTM 6 .\n\nFinally, ğ‘¯ ğ‘1 and ğ‘¯ ğ‘£1 are sent to a fully connected layer to make the emotion predictions respectively. According to the Eq. 7, Å¶ ğ‘1 and Å¶ ğ‘£1 are calculated. Å¶ ğ‘1 is the emotion predictions of audio modality and Å¶ ğ‘£1 is the emotion predictions of video modality.\n\nIn this sub-challenge, we also use the LSTM with self-attention mechanism to capture the complex temporal dependencies and make emotion predictions for audio and video modality. According to the Eq. 1 -8, Å¶ ğ‘2 and Å¶ ğ‘£2 are calculated. Å¶ ğ‘2 is the emotion predictions of audio modality and Å¶ ğ‘£2 is the emotion predictions of video modality. In addition, the bio-signal features are also sent to the LSTM model and make emotion predictions. According to the Eq. 9 -11, Å¶ ğµ1 is calculated. Å¶ ğµ1 is the emotion predictions of bio-signal modality.\n\nLate fusion: In this sub-challenge, we also fuse multi-modal features using late fusion strategy. Å¶ ğ‘1 , Å¶ ğ‘£1 , Å¶ ğ‘2 , Å¶ ğ‘£2 and Å¶ ğµ1 are concatenated which is given by:\n\nAccording to the Eq. 13 -14 we get the final emotion predictions Å¶ ğ‘“ 1 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ccc Loss",
      "text": "In both sub-challenges, we train our model using the Concordance Correlation Coefficient (CCC) loss, which can be formatted as follows:\n\nwhere ğœ Å¶ and ğœ ğ‘Œ are the respective standard deviations. ğœ‡ Å¶ and ğœ‡ ğ‘Œ are the average of the predicted values Å¶ and labels ğ‘Œ , correspondingly. ğœŒ is the Pearson Correlation Coefficient (PCC) between Å¶ and ğ‘Œ .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments 5.1 Dataset",
      "text": "In MuSe 2021, two datasets (MuSe_CaR  [28]  and Ulm-TSST) are provided for four different sub-challenges. The dataset we used is Ulm-TSST, a novel subset of the audio-visual textual Ulm-Trier Social Stress dataset that features German speakers in a Trier Social",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "We implement all of our models with the PyTorch toolkit in the two sub-challenges. For the MuSe-Stress sub-challenge, the proposed model consists of the self-attention layer, a unidirectional LSTM layer and the fully connected layer. The number of heads is set to 4 or 8 and the number of layers is 1, 2, or 4. The number of hidden sizes in the unidirectional LSTM layer is 64, 128, or 256 and the number of the unidirectional LSTM layer is set 2 or 4. For the MuSe-Physio sub-challenge, the proposed model consists of a gated CNN layer, a unidirectional LSTM layer and the fully connected layer. The number of channels for convolution is 64 or 128 and the number of hidden size in the unidirectional LSTM layer is 128, 256, or 1024. The Adam optimizer with a learning rate (0.0002, 0.001, 0.002, or 0.005) is used to optimize the whole networks in both sub-challenges. During training, the batch size is set to (64, 128, or 256). We train the model 100 epochs. When the loss does not decrease in 15 consecutive epochs, the learning rate will be halved.\n\nFor the late fusion model, we use a Bi-LSTM layer with 32 cells to fuse the previous predictions. The late fusion model is trained at most 20 epochs. The Adam optimizer with a learning rate of 0.001 is also applied, and the batch size is set to 64.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: demonstrates the architecture of",
      "page": 3
    },
    {
      "caption": "Figure 1: Overview of the architecture used in the MuSe-",
      "page": 4
    },
    {
      "caption": "Figure 2: Overview of the architecture used in the MuSe-",
      "page": 5
    },
    {
      "caption": "Figure 2: shows the architecture of our method, which contains four mod-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Ziyu Maâˆ—"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "College of Electrical and Information Engineering,"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Hunan University"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Key Laboratory of Visual Perception and Artificial"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Intelligence of Hunan Province"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Changsha, China"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "maziyu@hnu.edu.cn"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Bin Sun"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "College of Electrical and Information Engineering,"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Hunan University"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Key Laboratory of Visual Perception and Artificial"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Intelligence of Hunan Province"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "Changsha, China"
        },
        {
          "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition": "sunbin611@hnu.edu.cn"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Changsha, China": "shutao_li@hnu.edu.cn"
        },
        {
          "Changsha, China": "LSTM network are utilized for modeling the complex temporal"
        },
        {
          "Changsha, China": "dependencies in the sequence. Finally, the late fusion strategy is"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "used. Our proposed method also achieves CCC of 0.5412 on the test"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "set, which ranks in the top 3."
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "CCS CONCEPTS"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "â€¢ Computing methodologies â†’ Neural networks; â€¢ Informa-"
        },
        {
          "Changsha, China": "tion systems â†’ Multimedia information systems; â€¢ Human-centered"
        },
        {
          "Changsha, China": "computing â†’ HCI theory, concepts and models."
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "KEYWORDS"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "Multi-modal Fusion; Long Short-Term Memory; Self Attention;"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "Gated Convolutional Neural Networks; Continuous Emotion Recog-"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "nition"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "ACM Reference Format:"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "Ziyu Ma, Fuyan Ma, Bin Sun, and Shutao Li. 2021. Hybrid Mutimodal Fusion"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "for Dimensional Emotion Recognition . In Proceedings of the 2nd Multimodal"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "Sentiment Analysis Challenge (MuSe â€™21), October 24, 2021, Virtual Event,"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "China. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3475957."
        },
        {
          "Changsha, China": "3484457"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "1\nINTRODUCTION"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "The emotional states play a crucial role in human work and daily"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "life through affecting psychological and physiological status. It is"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "noted that many people in modern societies are under a high level"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "of stress [1]. Whatâ€™s worse, the long-term accumulation of negative"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "emotions may result\nin depression, which has an extremely se-"
        },
        {
          "Changsha, China": "vere impact on mental health. Thus, accurately recognizing human"
        },
        {
          "Changsha, China": "emotions is a fundamental but profound research direction. Emo-"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "tion recognition systems have various applications, such as driver"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "fatigue monitoring [5], health care especially mental health moni-"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "toring [12], building a secure and safe society [30] and human-robot"
        },
        {
          "Changsha, China": ""
        },
        {
          "Changsha, China": "interaction."
        },
        {
          "Changsha, China": "Although the discrete categorical theory of emotion is a popular"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "control). Therefore, the continuous dimensional theory of emotion",
          "Our main contributions to the MuSe 2021 challenge in this paper": "can be summarized as follows:"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "is suitable to compute the sentiments scattered across multi-modal",
          "Our main contributions to the MuSe 2021 challenge in this paper": ""
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "sequences, because it can describe more subtle, continuous and",
          "Our main contributions to the MuSe 2021 challenge in this paper": "â€¢ We investigate the impact of various features including"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "complicated emotional states.",
          "Our main contributions to the MuSe 2021 challenge in this paper": "audio features, visual\nfeatures and bio-signal\nfeatures in"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "As we know, it is significantly subjective to annotate the emo-",
          "Our main contributions to the MuSe 2021 challenge in this paper": "both sub-challenges. We also demonstrate the effectiveness"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "tions of human beings. However, multiple professional annota-",
          "Our main contributions to the MuSe 2021 challenge in this paper": "of the bio-signal features, which has not been well-used in"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "tors must continuously annotate multi-modal sequences, which",
          "Our main contributions to the MuSe 2021 challenge in this paper": "corresponding sub-challenges of previous MuSe."
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "is time-consuming and laborious. Therefore, how to produce an",
          "Our main contributions to the MuSe 2021 challenge in this paper": "â€¢ We also propose to augment the LSTM network with the self-"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "agreed-upon representation (gold standard) from multiple labelers",
          "Our main contributions to the MuSe 2021 challenge in this paper": "attention mechanism for continuous emotion prediction. The"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "still remains an open research question, with few methods avail-",
          "Our main contributions to the MuSe 2021 challenge in this paper": "results demonstrate the combination of these two modules"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "able. For example, given the level of disagreement, Michael et al.",
          "Our main contributions to the MuSe 2021 challenge in this paper": "can enhance the ability of modeling long-term temporal"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "[11] proposed to use the Evaluator Weighted Estimator to weight",
          "Our main contributions to the MuSe 2021 challenge in this paper": "dependencies within the sequence. Moreover, with the help"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "different annotators based on the likelihood of agreement. It is lim-",
          "Our main contributions to the MuSe 2021 challenge in this paper": "of the late fusion technique, our method achieves promising"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "ited for the research on the fusion of perceived emotional signals",
          "Our main contributions to the MuSe 2021 challenge in this paper": "results in the MuSe-Stress sub-challenge."
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "with extra physiological signals and there also has been minimal",
          "Our main contributions to the MuSe 2021 challenge in this paper": "â€¢\nThe LSTM network enhanced with the self-attention mech-"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "research on the combination between physiological and perceived",
          "Our main contributions to the MuSe 2021 challenge in this paper": "anism and Gated CNN-LSTM model is proposed for further"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "arousal gold standard. Therefore, in the 2021 edition of the Multi-",
          "Our main contributions to the MuSe 2021 challenge in this paper": "extracting the information from audio-visual modalities. The"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "modal Sentiment in-the-wild (MuSe) challenge [27] , apart from",
          "Our main contributions to the MuSe 2021 challenge in this paper": "late fusion technique is also used as the MuSe-Stress sub-"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "predicting the level of emotional arousal and valence in a time-",
          "Our main contributions to the MuSe 2021 challenge in this paper": "challenge and our model achieves competitive results in the"
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "continuous manner from people in stressed dispositions for the",
          "Our main contributions to the MuSe 2021 challenge in this paper": "MuSe-Physio sub-challenge."
        },
        {
          "tion), valence (a measure of pleasure), and dominance (a measure of": "MuSe-Stress sub-challenge, the signal of arousal is fused with the",
          "Our main contributions to the MuSe 2021 challenge in this paper": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "many researchers have focused on dimensional models, such as the": "valence-arousal model[32]. For example, Russel et al. [24] proposed",
          "the self-attention mechanism module and the Gated Convolutional": "Neural Networks (GCNN) as well as the LSTM network for mod-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "a computing model of emotions characterized by two essential di-",
          "the self-attention mechanism module and the Gated Convolutional": "eling the complex temporal dependencies in the sequence from"
        },
        {
          "many researchers have focused on dimensional models, such as the": "mensions (i.e., valence and arousal). Marsella et al. [19] developed",
          "the self-attention mechanism module and the Gated Convolutional": "the audio-visual modality respectively. The bio-signal features are"
        },
        {
          "many researchers have focused on dimensional models, such as the": "a model that treated a specific emotional state as a data point in",
          "the self-attention mechanism module and the Gated Convolutional": "concatenated and sent to the LSTM network. Finally, we concate-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "the continuous space, which was described mathematically by 3",
          "the self-attention mechanism module and the Gated Convolutional": "nate all types of multi-modal features before sending them into the"
        },
        {
          "many researchers have focused on dimensional models, such as the": "dimensions corresponding to arousal (a measure of affective activa-",
          "the self-attention mechanism module and the Gated Convolutional": "regression model."
        },
        {
          "many researchers have focused on dimensional models, such as the": "tion), valence (a measure of pleasure), and dominance (a measure of",
          "the self-attention mechanism module and the Gated Convolutional": "Our main contributions to the MuSe 2021 challenge in this paper"
        },
        {
          "many researchers have focused on dimensional models, such as the": "control). Therefore, the continuous dimensional theory of emotion",
          "the self-attention mechanism module and the Gated Convolutional": "can be summarized as follows:"
        },
        {
          "many researchers have focused on dimensional models, such as the": "is suitable to compute the sentiments scattered across multi-modal",
          "the self-attention mechanism module and the Gated Convolutional": ""
        },
        {
          "many researchers have focused on dimensional models, such as the": "sequences, because it can describe more subtle, continuous and",
          "the self-attention mechanism module and the Gated Convolutional": "â€¢ We investigate the impact of various features including"
        },
        {
          "many researchers have focused on dimensional models, such as the": "complicated emotional states.",
          "the self-attention mechanism module and the Gated Convolutional": "audio features, visual\nfeatures and bio-signal\nfeatures in"
        },
        {
          "many researchers have focused on dimensional models, such as the": "As we know, it is significantly subjective to annotate the emo-",
          "the self-attention mechanism module and the Gated Convolutional": "both sub-challenges. We also demonstrate the effectiveness"
        },
        {
          "many researchers have focused on dimensional models, such as the": "tions of human beings. However, multiple professional annota-",
          "the self-attention mechanism module and the Gated Convolutional": "of the bio-signal features, which has not been well-used in"
        },
        {
          "many researchers have focused on dimensional models, such as the": "tors must continuously annotate multi-modal sequences, which",
          "the self-attention mechanism module and the Gated Convolutional": "corresponding sub-challenges of previous MuSe."
        },
        {
          "many researchers have focused on dimensional models, such as the": "is time-consuming and laborious. Therefore, how to produce an",
          "the self-attention mechanism module and the Gated Convolutional": "â€¢ We also propose to augment the LSTM network with the self-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "agreed-upon representation (gold standard) from multiple labelers",
          "the self-attention mechanism module and the Gated Convolutional": "attention mechanism for continuous emotion prediction. The"
        },
        {
          "many researchers have focused on dimensional models, such as the": "still remains an open research question, with few methods avail-",
          "the self-attention mechanism module and the Gated Convolutional": "results demonstrate the combination of these two modules"
        },
        {
          "many researchers have focused on dimensional models, such as the": "able. For example, given the level of disagreement, Michael et al.",
          "the self-attention mechanism module and the Gated Convolutional": "can enhance the ability of modeling long-term temporal"
        },
        {
          "many researchers have focused on dimensional models, such as the": "[11] proposed to use the Evaluator Weighted Estimator to weight",
          "the self-attention mechanism module and the Gated Convolutional": "dependencies within the sequence. Moreover, with the help"
        },
        {
          "many researchers have focused on dimensional models, such as the": "different annotators based on the likelihood of agreement. It is lim-",
          "the self-attention mechanism module and the Gated Convolutional": "of the late fusion technique, our method achieves promising"
        },
        {
          "many researchers have focused on dimensional models, such as the": "ited for the research on the fusion of perceived emotional signals",
          "the self-attention mechanism module and the Gated Convolutional": "results in the MuSe-Stress sub-challenge."
        },
        {
          "many researchers have focused on dimensional models, such as the": "with extra physiological signals and there also has been minimal",
          "the self-attention mechanism module and the Gated Convolutional": "â€¢\nThe LSTM network enhanced with the self-attention mech-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "research on the combination between physiological and perceived",
          "the self-attention mechanism module and the Gated Convolutional": "anism and Gated CNN-LSTM model is proposed for further"
        },
        {
          "many researchers have focused on dimensional models, such as the": "arousal gold standard. Therefore, in the 2021 edition of the Multi-",
          "the self-attention mechanism module and the Gated Convolutional": "extracting the information from audio-visual modalities. The"
        },
        {
          "many researchers have focused on dimensional models, such as the": "modal Sentiment in-the-wild (MuSe) challenge [27] , apart from",
          "the self-attention mechanism module and the Gated Convolutional": "late fusion technique is also used as the MuSe-Stress sub-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "predicting the level of emotional arousal and valence in a time-",
          "the self-attention mechanism module and the Gated Convolutional": "challenge and our model achieves competitive results in the"
        },
        {
          "many researchers have focused on dimensional models, such as the": "continuous manner from people in stressed dispositions for the",
          "the self-attention mechanism module and the Gated Convolutional": "MuSe-Physio sub-challenge."
        },
        {
          "many researchers have focused on dimensional models, such as the": "MuSe-Stress sub-challenge, the signal of arousal is fused with the",
          "the self-attention mechanism module and the Gated Convolutional": ""
        },
        {
          "many researchers have focused on dimensional models, such as the": "Electrodermal Activity (EDA) and then used as a prediction target",
          "the self-attention mechanism module and the Gated Convolutional": "The rest of this paper is structured as follows: Section 2 intro-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "for the MuSe-Physio sub-challenge. Participants are provided with",
          "the self-attention mechanism module and the Gated Convolutional": "duces the related work. The multimodal features are presented in"
        },
        {
          "many researchers have focused on dimensional models, such as the": "the multimodal Ulm-TSST data-base, where subjects were recorded",
          "the self-attention mechanism module and the Gated Convolutional": "Section 3. The methods are described in Section 4. The experimental"
        },
        {
          "many researchers have focused on dimensional models, such as the": "under a highly stress-induced free speech task.",
          "the self-attention mechanism module and the Gated Convolutional": "setting and the results are presented in Section 5. Finally, we make"
        },
        {
          "many researchers have focused on dimensional models, such as the": "In the MuSe-Stress sub-challenge, eGeMAPS is the best audio",
          "the self-attention mechanism module and the Gated Convolutional": "the conclusive remarks and indicates the possible future directions"
        },
        {
          "many researchers have focused on dimensional models, such as the": "feature for the prediction of valence and arousal, achieving 0.5018",
          "the self-attention mechanism module and the Gated Convolutional": "in Section."
        },
        {
          "many researchers have focused on dimensional models, such as the": "CCC and 0.4416 CCC on the test set respectively which reported",
          "the self-attention mechanism module and the Gated Convolutional": ""
        },
        {
          "many researchers have focused on dimensional models, such as the": "in the baseline paper. VGGface is the best vision feature for the",
          "the self-attention mechanism module and the Gated Convolutional": ""
        },
        {
          "many researchers have focused on dimensional models, such as the": "",
          "the self-attention mechanism module and the Gated Convolutional": "2\nRELATED WORK"
        },
        {
          "many researchers have focused on dimensional models, such as the": "prediction of valence and arousal, achieving 0.4529 CCC and 0.1579",
          "the self-attention mechanism module and the Gated Convolutional": ""
        },
        {
          "many researchers have focused on dimensional models, such as the": "CCC on the test set respectively which reported in the baseline",
          "the self-attention mechanism module and the Gated Convolutional": "Multi-modal Features: Various multimodal features have been"
        },
        {
          "many researchers have focused on dimensional models, such as the": "paper. Therefore, we choose eGeMAPS and VGGface feature sets",
          "the self-attention mechanism module and the Gated Convolutional": "well investigated in the past series of AVECs, which are the prede-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "for the MuSe-Stress sub-challenge in this paper. Apart from the",
          "the self-attention mechanism module and the Gated Convolutional": "cessor of MuSe. Before the deep learning era, competitors tended"
        },
        {
          "many researchers have focused on dimensional models, such as the": "audio features (eGeMAPs), the visual features (VGGface), the bio-",
          "the self-attention mechanism module and the Gated Convolutional": "to use carefully-handcrafted features for regression. For example,"
        },
        {
          "many researchers have focused on dimensional models, such as the": "signal features (Electrocardiogram (ECG), Respiration (RESP), and",
          "the self-attention mechanism module and the Gated Convolutional": "SÃ¡nchez-Lozano et al. [25] proposed to extract the Gabor feature"
        },
        {
          "many researchers have focused on dimensional models, such as the": "heart rate (BPM)) are also selected. We apply the Long Short-Term",
          "the self-attention mechanism module and the Gated Convolutional": "and the Local Binary Patterns (LBP) for the individual visual modal-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "Memory (LSTM) with the self-attention module for modeling the",
          "the self-attention mechanism module and the Gated Convolutional": "ity and extract\nlow-level\nfeatures for the single audio modality."
        },
        {
          "many researchers have focused on dimensional models, such as the": "complex temporal dependencies in the sequence from the audio-",
          "the self-attention mechanism module and the Gated Convolutional": "With the deep learning technique showing superior performance"
        },
        {
          "many researchers have focused on dimensional models, such as the": "visual modalities and the LSTM network is used to capture the",
          "the self-attention mechanism module and the Gated Convolutional": "over carefully-handcrafted features, participants have been more"
        },
        {
          "many researchers have focused on dimensional models, such as the": "complex and continuous dependencies within the sequences from",
          "the self-attention mechanism module and the Gated Convolutional": "likely to learn deep representations for both modalities by different"
        },
        {
          "many researchers have focused on dimensional models, such as the": "bio-signal modality. Finally, we concatenate all multi-modal features",
          "the self-attention mechanism module and the Gated Convolutional": "variants of very deep networks. The methods [4] [15] in AVEC 2017"
        },
        {
          "many researchers have focused on dimensional models, such as the": "and subsequently send them into the regression model.",
          "the self-attention mechanism module and the Gated Convolutional": "demonstrated that utilizing the convolutional neural networks for"
        },
        {
          "many researchers have focused on dimensional models, such as the": "In the MuSe-Physio sub-challenge, the late fusion of the best au-",
          "the self-attention mechanism module and the Gated Convolutional": "the visual representations obtained comparable and even better"
        },
        {
          "many researchers have focused on dimensional models, such as the": "dio (VGGish) and video (VGGface) predictions yield the best results,",
          "the self-attention mechanism module and the Gated Convolutional": "results than carefully-engineered features. Furthermore, the results"
        },
        {
          "many researchers have focused on dimensional models, such as the": "namely 0.4913 CCC on development data and 0.4908 CCC on test",
          "the self-attention mechanism module and the Gated Convolutional": "of [33] in AVEC 2018 demonstrated that the learned audio repre-"
        },
        {
          "many researchers have focused on dimensional models, such as the": "data according to the baseline paper. Therefore, we choose VGGish",
          "the self-attention mechanism module and the Gated Convolutional": "sentations from the pretrained VGGish model could outperform"
        },
        {
          "many researchers have focused on dimensional models, such as the": "and VGGface feature sets for the MuSe-Physio sub-challenge in this",
          "the self-attention mechanism module and the Gated Convolutional": "expert-knowledge based acoustic features. Chen et al. [2] proposed"
        },
        {
          "many researchers have focused on dimensional models, such as the": "paper. In addition, the bio-signal features (ECG, RESP, and BPM)",
          "the self-attention mechanism module and the Gated Convolutional": "to use 2D+1D convolutional neural networks in the very recent"
        },
        {
          "many researchers have focused on dimensional models, such as the": "are also the input of the model. Then, we utilize the LSTM with",
          "the self-attention mechanism module and the Gated Convolutional": "AVEC 2019, which also proved the superior performance of learned"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "guistic features play an important part in multimodal emotion anal-",
          "high-level and discriminative features compared with other facial": "recognition models."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "ysis [23]. In AVEC 2017, the textual modality was first introduced",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "to solve the recognition problem. At the very beginning, partici-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "3.2\nAcoustic Features"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "pants used the classic bag-of-words features [15]. Subsequently, the",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "eGeMAPS Feature: We use the extended Geneva Minimalistic"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "word vectors (such as Word2Vec [20] and GloVe [22]) were widely",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "Acoustic Parameter Set (eGeMAPS) feature provided by the organiz-"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "applied because of their efficiency and effectiveness, which were",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "ers of MuSe 2021, which contains 23 acoustic low-level descriptors"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "pre-trained on large-scale text datasets.",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "(LLDs [7]. The freely and publicly available openSMILE toolkit [8]"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "Model Architecture: Support vector regression (SVR)\nis the",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "can be used to extract the eGeMAPS feature. Several statistical func-"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "most commonly used as the baseline methods in previous AVECs.",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "tions of the openSMILE toolkit can be directly applied to extract"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "However, because of its non-temporal disadvantage, SVR cannot",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "segment-level features with an 88-dimensional vector."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "use the temporal and contextual information well to promote the",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "VGGish Feature: VGGish [13] was pre-trained on AudioSet [10]."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "performance of emotion recognition and sentiment analysis. With",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "Although VGGish was originally proposed for audio classification,"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "the increasing popularity of deep neural networks, the Recurrent",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "previous studies, such as [34], demonstrated that the learned deep"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "Neural Network (RNN) has outperformed better in sequence mod-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "feature representation from trained VGGish outperformed carefully-"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "eling over other methods. Therefore, the 1st place winners of re-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "handcrafted acoustic features. Therefore, the pre-trained VGGish"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "cent AVECs all adopted the LSTM network without exception, for",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "model\nis used to extract acoustic features in our work. We first"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "continuous sentiment analysis. Previous methods [31], [4] both",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "divided the recordings into multiple 0.975 s frames with a hop size"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "utilized the LSTM and SVR to perform emotion regression. The",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "of 0.25s to match with the ground-truth labels. Afterwards, we"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "experimental results demonstrated that the LSTM outperformed",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "extract the log spectrograms from these frames. Subsequently, they"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "SVR dramatically. Compared with RNN based methods, a fully con-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "are sent into the VGGish model. The 128-dimensional embeddings"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "volutional network was designed by Du et al. [6].\nIt performed",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "from the output of\nfc2 layer are extracted as the final VGGish"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "emotion recognition in a coarse-to-fine strategy by aggregating",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "feature."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "multi-level features with various scales. Huang et al. [16] utilized",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "various types of temporal architectures to capture temporal con-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "textual relationships within sequences. Their studies also revealed",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "3.3\nBiological Features"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "that the combination of various models resulted in better results.",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "ECG, RESP and BPM Feature: Three biological signals (i.e., Elec-"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "Multi-modal Fusion: It is indispensable for researchers to uti-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "trocardiogram (ECG), Respiration (RESP), and heart rate (BPM))"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "lize multi-modal fusion to boost the performance of emotion pre-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "were captured at a sampling rate of 1 kHz. We use the official biolog-"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "diction systems. Huang et al. [15] adopted the late fusion technique",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "ical features at a sampling rate of 2Hz as the inputs for our model."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "to aggregate the predictions based on different types of features in",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "It is noted that the bio-signals are newly featured for MuSe 2021,"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "AVEC 2017, while the early fusion strategy was used by Chen et",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "which has not been well investigated for continuous dimensional"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "al. [4] to capture the inter-modal dynamics. In AVEC 2018, the de-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "emotion recognition."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "tail comparison results of these methods were described by Huang",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "et al. [14]. The comparison demonstrated that methods with the",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "late fusion predicted arousal and valence better than these with",
          "high-level and discriminative features compared with other facial": "4\nMETHODS"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "the early fusion.\nIn AVEC 2019, Chen et al. [3] proposed to uti-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "4.1\nMuSe-Stress sub-challenge"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "lize both the early and the late fusion techniques in one model.",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "In this sub-challenge, the LSTM networks with the self-attention"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "They first trained the deep Bidirectional Long Short-Term Memory",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "mechanism is utilized to capture complex temporal dependencies"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "networks (Bi-LSTM) for unimodal features. Afterwards, Bi-LSTMs",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "within the feature sequences. Different from [29] , the bio-signal fea-"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "were applied for fusing corresponding bi-modal features in an early",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "tures (Electrocardiogram (ECG), Respiration (RESP), and heart rate"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "stage. Finally, the predictions of various models were fused late by",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "(BPM)) are also selected. Figure 1 demonstrates the architecture of"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "a second level Bi-LSTM. Moreover, Huang et al. [17] proposed to",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "our model, which contains three modules: the LSTM networks with"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "fuse multimodal information by the cross-modal attention module.",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "the self-attention module for the acoustic and visual features, the"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "Their results showed the proposed cross-modal attention could out-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "LSTM module for the bio-signal features and the late fusion module."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "perform better over both the early and the late fusion techniques.",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "First, we feed the eGeMAPS feature and the VGGface feature to the"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "LSTM network with self-attention module respectively and obtain"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "3\nMULTIMODAL FEATURES",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "the predictions from both audio and visual modalities. Then, the"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "bio-signal features are concatenated and sent to the LSTM module"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "3.1\nVisual Features",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "in order to get the predictions of bio-signal modality. Finally, the"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "VGGface Feature: The visual geometry group of Oxford intro-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "predictions from above modalities are concatenated and sent to the"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "duced the deep CNN referred to as VGG16 [26]. As a variant of",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "late fusion module for regression."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "VGG16, the VGGface architecture was initially intended for super-",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "Self-attention Mechanism: The self-attention mechanism is"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "vised facial recognition purposes [26], which was trained on 2.6",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "utilized to transform the input sequences into high-level representa-"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "million faces. To get rid of irrelevant background information, we",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "tions, which further captures the relationships across the sequences."
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "first apply MTCNN to detect and align faces in videos. VGGface [21]",
          "high-level and discriminative features compared with other facial": ""
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "",
          "high-level and discriminative features compared with other facial": "(cid:110)\n(cid:111)"
        },
        {
          "audio-visual features. Apart from audio-visual modalities, the lin-": "is then used to extract general facial features, which can present",
          "high-level and discriminative features compared with other facial": ",\nâˆˆ Rğ‘‘ğ‘ |ğ‘– = 1, ..., |ğ‘‡ |\nGiven an input acoustic sequence ğ‘¿ğ‘ =\nğ‘¿ğ‘"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "calculated by:": "ğ‘£ 1\nğ‘£ |\nğ‘£ 1\nğ‘£ |\n, . . . , ğ‘¯\n, . . . , ğ‘ª"
        },
        {
          "calculated by:": "(6)\nğ‘‡ |) = LSTM2 (ğ‘ª\nğ‘‡ |)"
        },
        {
          "calculated by:": "where ğ‘¯ ğ‘£\nâˆˆ Râ„2 and â„2 represents the dimensionality of LSTM2."
        },
        {
          "calculated by:": "Finally, ğ‘¯ ğ‘ and ğ‘¯ ğ‘£ are sent to the fully connected layers to obtain"
        },
        {
          "calculated by:": "the emotion predictions respectively. For the audio modality, the"
        },
        {
          "calculated by:": "(cid:110) Ë†ğ‘Œ ğ‘\n, . . . ,\nğ‘Œ ğ‘\nemotion prediction Ë†ğ‘Œ ğ‘ =\nis given by"
        },
        {
          "calculated by:": "1\n|ğ‘‡ |"
        },
        {
          "calculated by:": "(7)\nğ‘Œ ğ‘ = ğ‘¯\nğ‘¾\nğ‘ + ğ’ƒ"
        },
        {
          "calculated by:": "ğ‘"
        },
        {
          "calculated by:": "represents the bias. For the video modal-\nwhere ğ‘¾ğ‘ âˆˆ Râ„1Ã—1, and ğ’ƒ"
        },
        {
          "calculated by:": "(cid:110) Ë†ğ‘Œ ğ‘£\n, . . . ,\nğ‘Œ ğ‘£\nity, the emotion prediction Ë†ğ‘Œ ğ‘£ =\nis given by"
        },
        {
          "calculated by:": "1\n|ğ‘‡ |"
        },
        {
          "calculated by:": "(8)\nğ‘Œ ğ‘£ = ğ‘¯\nğ‘¾\nğ‘£ + ğ’ƒ"
        },
        {
          "calculated by:": "ğ‘£"
        },
        {
          "calculated by:": "represents the bias.\nwhere ğ‘¾ ğ‘£ âˆˆ Râ„2Ã—1, and ğ’ƒ"
        },
        {
          "calculated by:": "Moreover, the bio-signal features are applied for the predictions"
        },
        {
          "calculated by:": "of valence and arousal. In our model, we use the LSTM to model de-"
        },
        {
          "calculated by:": "(cid:111)\n(cid:110)"
        },
        {
          "calculated by:": "pendencies among these features. Let ğ‘¿ğ‘’ =\nğ‘¿ğ‘’\nğ‘– âˆˆ Rğ‘‘ğ‘’ |ğ‘– = 1, ..., |ğ‘‡ |"
        },
        {
          "calculated by:": ""
        },
        {
          "calculated by:": "(cid:110)\n(cid:111)"
        },
        {
          "calculated by:": "repre-\nrepresents the ECG feature, ğ‘¿ğ‘Ÿ =\nğ‘¿ğ‘Ÿ\nğ‘– âˆˆ Rğ‘‘ğ‘Ÿ |ğ‘– = 1, ..., |ğ‘‡ |"
        },
        {
          "calculated by:": "(cid:110)\n(cid:111)"
        },
        {
          "calculated by:": "âˆˆ Rğ‘‘ğ‘ |ğ‘– = 1, ..., |ğ‘‡ |\nrepre-\nsents the RESP feature and ğ‘¿ğ‘ =\nğ‘¿ğ‘"
        },
        {
          "calculated by:": ""
        },
        {
          "calculated by:": "sents the BPM feature sequence. First, these features are concate-"
        },
        {
          "calculated by:": ""
        },
        {
          "calculated by:": "nated by:"
        },
        {
          "calculated by:": "(cid:16)\nğ‘ (cid:17)"
        },
        {
          "calculated by:": "ğµ = concat\n(9)\nğ‘¿\nğ‘¿\nğ‘’, ğ‘¿\nğ‘Ÿ , ğ‘¿"
        },
        {
          "calculated by:": ""
        },
        {
          "calculated by:": ""
        },
        {
          "calculated by:": "where ğ‘‘ğ‘’ , ğ‘‘ğ‘Ÿ and ğ‘‘ğ‘ represent the dimension of the ECG feature,"
        },
        {
          "calculated by:": "RESP feature and BPM feature respectively. Afterwards, ğ‘¿ ğµ âˆˆ Rğ‘‡ Ã—"
        },
        {
          "calculated by:": "(ğ‘‘ğ‘’ +ğ‘‘ğ‘Ÿ +ğ‘‘ğ‘ )"
        },
        {
          "calculated by:": "is sent to the LSTM and the output is given by:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "layers. For simplicity, we override the whole operation as a mapping": "GCNN1:"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ‘)\n(16)\nğ‘®\nğ‘ = GCNN1 (ğ‘¿"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "Similarly, given the input visual sequence ğ‘¿ ğ‘£, the GCNN output is"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "given by:"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ‘£)\n(17)\nğ‘®\nğ‘£ = GCNN2 (ğ‘¿"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ‘› , ğ‘‘ ğ‘£\nğ‘®ğ‘£ âˆˆ Rğ‘‡ Ã—ğ‘‘ ğ‘£\nğ‘› is the number of channels for convolution. Then,"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "the GCNN output is sent to the LSTM to capture the temporal depen-"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "dencies within the feature sequences. Given the input embedding"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "sequence ğ‘®ğ‘, the LSTM output is given by:"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ‘1\nğ‘1"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ‘ 1\nğ‘ |\n, . . . , ğ‘¯\n, . . . , ğ‘®"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "(18)\n1\n|ğ‘‡ |) = LSTM5 (ğ‘®\nğ‘‡ |)"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "where ğ‘¯ ğ‘1\nâˆˆ Râ„5 and â„5 represents the dimension of LSTM5. Simi-"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "larly, given the input embedding sequence ğ‘®ğ‘£, we can get the LSTM"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "output ğ‘¯ ğ‘£1 âˆˆ Rğ‘‡ Ã—â„6 , where â„6 represents the dimension of LSTM6."
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "Finally, ğ‘¯ ğ‘1 and ğ‘¯ ğ‘£1 are sent to a fully connected layer to make"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ‘Œ ğ‘1\nthe emotion predictions respectively. According to the Eq. 7,"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "and Ë†ğ‘Œ ğ‘£1 are calculated.\nis the emotion predictions of audio"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "modality and Ë†ğ‘Œ ğ‘£1 is the emotion predictions of video modality."
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "In this sub-challenge, we also use the LSTM with self-attention"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "mechanism to capture the complex temporal dependencies and"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "make emotion predictions for audio and video modality. According"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "to the Eq. 1 â€“ 8,\nğ‘Œ ğ‘2 and Ë†ğ‘Œ ğ‘£2 are calculated.\nğ‘Œ ğ‘2 is the emotion"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "predictions of audio modality and Ë†ğ‘Œ ğ‘£2 is the emotion predictions"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "of video modality. In addition, the bio-signal features are also sent"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "to the LSTM model and make emotion predictions. According to"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "the Eq. 9 â€“ 11,\nğ‘Œ ğµ1 is calculated.\nğ‘Œ ğµ1 is the emotion predictions of"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "bio-signal modality."
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "Late fusion:\nIn this sub-challenge, we also fuse multi-modal"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "features using late fusion strategy.\nğ‘Œ ğ‘1,\nğ‘Œ ğ‘£1,\nğ‘Œ ğ‘2,\nğ‘Œ ğ‘£2 and Ë†ğ‘Œ ğµ1 are"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "concatenated which is given by:"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "(cid:16) Ë†ğ‘Œ ğ‘1,\nğ‘Œ ğ‘£1,\nğ‘Œ ğ‘2,\nğ‘Œ ğ‘£2,\nğ‘Œ ğµ1(cid:17)\nğ‘“ 1 = concat\n(19)\nğ’€"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "According to the Eq. 13 â€“ 14 we get the final emotion predictions"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ‘Œ ğ‘“ 1."
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "4.3\nCCC Loss"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "In both sub-challenges, we train our model using the Concordance"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "Correlation Coefficient (CCC) loss, which can be formatted as fol-"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "lows:"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "L = 1 âˆ’ ğ¶ğ¶ğ¶\n(20)"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "2ğœŒğœ Ë†ğ‘Œ ğœğ‘Œ"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ¶ğ¶ğ¶ =\n(21)"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "(cid:17)2\n(cid:16)"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğœ2"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğœ‡ Ë†ğ‘Œ âˆ’ ğœ‡ğ‘Œ\nğ‘Œ +"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ğ‘Œ"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "where ğœ Ë†ğ‘Œ and ğœğ‘Œ are the respective standard deviations. ğœ‡ Ë†ğ‘Œ and ğœ‡ğ‘Œ"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "are the average of the predicted values\nğ‘Œ and labels ğ‘Œ , correspond-"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "ingly. ğœŒ is the Pearson Correlation Coefficient (PCC) between Ë†ğ‘Œ"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "and ğ‘Œ ."
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "5\nEXPERIMENTS"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": ""
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "5.1\nDataset"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "In MuSe 2021, two datasets (MuSe_CaR [28] and Ulm-TSST) are"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "provided for four different sub-challenges. The dataset we used"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "is Ulm-TSST, a novel subset of the audio-visual textual Ulm-Trier"
        },
        {
          "layers. For simplicity, we override the whole operation as a mapping": "Social Stress dataset that features German speakers in a Trier Social"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: The CCC performance on the development set of",
      "data": [
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "the MuSe-Stress sub-challenge."
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "ity. â€˜Aâ€™, â€˜Vâ€™ and â€˜Bâ€™ represent the audio, video, and bio-signal"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "Number",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "41",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "14",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "M"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "14",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "A"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "69",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "A"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "A"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "V"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "V"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "V"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": "B"
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": ""
        },
        {
          "Table 1: Statistics information of the Ulm-TSST dataset.": "",
          "Table 2: The CCC performance on the development set of": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: The CCC performance on the development set of",
      "data": [
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "ity. â€˜Aâ€™, â€˜Vâ€™ and â€˜Bâ€™ represent the audio, video, and bio-signal"
        },
        {
          "5.2\nExperimental Setup": "We implement all of our models with the PyTorch toolkit in the two",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "modality."
        },
        {
          "5.2\nExperimental Setup": "sub-challenges. For the MuSe-Stress sub-challenge, the proposed",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "model consists of the self-attention layer, a unidirectional LSTM",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "layer and the fully connected layer. The number of heads is set",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "Feature"
        },
        {
          "5.2\nExperimental Setup": "to 4 or 8 and the number of\nlayers is 1, 2, or 4. The number of",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGish"
        },
        {
          "5.2\nExperimental Setup": "hidden sizes in the unidirectional LSTM layer is 64, 128, or 256 and",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGish"
        },
        {
          "5.2\nExperimental Setup": "the number of the unidirectional LSTM layer is set 2 or 4. For the",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGish"
        },
        {
          "5.2\nExperimental Setup": "MuSe-Physio sub-challenge, the proposed model consists of a gated",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "CNN layer, a unidirectional LSTM layer and the fully connected",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGish"
        },
        {
          "5.2\nExperimental Setup": "layer. The number of channels for convolution is 64 or 128 and the",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGish"
        },
        {
          "5.2\nExperimental Setup": "number of hidden size in the unidirectional LSTM layer is 128, 256,",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGface"
        },
        {
          "5.2\nExperimental Setup": "or 1024. The Adam optimizer with a learning rate (0.0002, 0.001,",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGface"
        },
        {
          "5.2\nExperimental Setup": "0.002, or 0.005)\nis used to optimize the whole networks in both",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGface"
        },
        {
          "5.2\nExperimental Setup": "sub-challenges. During training, the batch size is set to (64, 128,",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGface"
        },
        {
          "5.2\nExperimental Setup": "or 256). We train the model 100 epochs. When the loss does not",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "VGGface"
        },
        {
          "5.2\nExperimental Setup": "decrease in 15 consecutive epochs, the learning rate will be halved.",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        },
        {
          "5.2\nExperimental Setup": "For the late fusion model, we use a Bi-LSTM layer with 32 cells to",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": "ECG+RESP+BPM"
        },
        {
          "5.2\nExperimental Setup": "fuse the previous predictions. The late fusion model is trained at",
          "the MuSe-Physio sub-challenge. â€˜Mâ€™ denotes the used modal-": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Stress sub-": "Feature"
        },
        {
          "Table 4: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Stress sub-": "eGeMAPS + VGGface"
        },
        {
          "Table 4: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Stress sub-": "eGeMAPS +ECG+RESP+BPM"
        },
        {
          "Table 4: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Stress sub-": "VGGface + ECG+RESP+BPM"
        },
        {
          "Table 4: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Stress sub-": "eGeMAPS + VGGface+ECG+RESP+BPM"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Physio sub-": "Feature"
        },
        {
          "Table 5: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Physio sub-": "VGGish + VGGface"
        },
        {
          "Table 5: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Physio sub-": "VGGish + VGGface"
        },
        {
          "Table 5: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Physio sub-": "VGGish + VGGface"
        },
        {
          "Table 5: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Physio sub-": "VGGish + VGGface+ECG+RESP+BPM"
        },
        {
          "Table 5: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Physio sub-": "VGGish + VGGface+ECG+RESP+BPM"
        },
        {
          "Table 5: CCC performance of different modalities using late fusion strategy on the development set in the MuSe-Physio sub-": "VGGish + VGGface+ECG+RESP+BPM"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6: The best submission results of our proposed method in the two sub-challenges.": "Model"
        },
        {
          "Table 6: The best submission results of our proposed method in the two sub-challenges.": "Baseline"
        },
        {
          "Table 6: The best submission results of our proposed method in the two sub-challenges.": "Proposed Model"
        },
        {
          "Table 6: The best submission results of our proposed method in the two sub-challenges.": "Baseline"
        },
        {
          "Table 6: The best submission results of our proposed method in the two sub-challenges.": "Proposed Model"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Baseline\nMuSe-Physio\nTest": "Proposed Model\nMuSe-Physio\nTest",
          "-\n-\n-\n0.4908": "-\n-\n-\n0.5412"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "anno12_EDA is 0.6257, which outperforms the baseline (0.4913) and",
          "-\n-\n-\n0.4908": "the model performance. For the MuSe-Physio sub-challenge, the"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "the best uni-modal result (0.5050).",
          "-\n-\n-\n0.4908": "bio-signal features are explored and proved to be useful. Besides,"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "the LSTM network as well as the self-attention mechanism and"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "5.5\nSubmission Results",
          "-\n-\n-\n0.4908": "the GCNN-LSTM based model are used for this task and achieve"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "good performance. The late fusion strategy is also applied and"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "Table 6 shows the best submission results of the proposed method",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "contributes mostly to the model performance."
        },
        {
          "Baseline\nMuSe-Physio\nTest": "in the two sub-challenges. For the MuSe-Stress sub-challenge, our",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "The proposed method shows promising perspectives of future"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "proposed method outperforms the official baseline by 0.0047 with",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "improvements. First, more multi-modal\nfeatures can be used in"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "arousal and 0.0545 with valence. It is noticed that this sub-challenge",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "both sub-challenge. Then, many advanced and robust models can"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "is an extremely challenging task. There is a heavy overfitting prob-",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "be explored in the next step. Finally, the early fusion strategy is"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "lem here, especially predicting arousal. For the MuSe-Physio sub-",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "proved useful, and we can use it to the two sub-challenges."
        },
        {
          "Baseline\nMuSe-Physio\nTest": "challenge, the proposed method outperforms the official baseline",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "by 0.0504. Before submitting the best results, we retrain the data",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "from the development set using the optimal parameters in this sub-",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "7\nACKNOWLEDGMENTS"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "challenge, in order to further enhance the generalization ability of",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "This work is supported by the National Key Research and Develop-"
        },
        {
          "Baseline\nMuSe-Physio\nTest": "the model.",
          "-\n-\n-\n0.4908": ""
        },
        {
          "Baseline\nMuSe-Physio\nTest": "",
          "-\n-\n-\n0.4908": "ment Project of China (2018YFB1305200) and the National Natural"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Hichem Sahli. 2019. Efficient spatial temporal convolutional features for audio-",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "transformer fusion for continuous emotion recognition. In ICASSP 2020-2020 IEEE"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "visual continuous affect recognition. In Proceedings of the 9th International on",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Audio/Visual Emotion Challenge and Workshop. 19â€“26.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "3507â€“3511."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[4]\nShizhe Chen, Qin Jin, Jinming Zhao, and Shuai Wang. 2017. Multimodal multi-",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[18]\nFuyan Ma, Bin Sun, and Shutao Li. 2021. Robust facial expression recognition"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "task learning for dimensional and continuous emotion recognition. In Proceedings",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "with convolutional visual transformers. arXiv preprint arXiv:2103.16854 (2021)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "of the 7th Annual Workshop on Audio/Visual Emotion Challenge. 19â€“26.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[19]\nStacy Marsella and Jonathan Gratch. 2014. Computationally modeling human"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[5]\nSilvia De Nadai, Massimo Dâ€™IncÃ , Francesco Parodi, Mauro Benza, Anita Trotta,",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "emotion. Commun. ACM 57, 12 (2014), 56â€“67."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Enrico Zero, Luca Zero, and Roberto Sacile. 2016. Enhancing safety of transport",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "by road by on-line monitoring of driver emotions. In 2016 11th System of Systems",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Distributed representations of words and phrases and their compositionality. In"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Engineering Conference (SoSE). IEEE, 1â€“4.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Advances in Neural Information Processing Systems. 3111â€“3119."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[6] Zhengyin Du, Suowei Wu, Di Huang, Weixin Li, and Yunhong Wang. 2019.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[21] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. 2015. Deep face"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Spatio-temporal encoder-decoder fully convolutional network for video-based",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "recognition.\n(2015)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "dimensional emotion recognition.\nIEEE Transactions on Affective Computing",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[22]\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "(2019).",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Global vectors for word representation. In Proceedings of the 2014 conference on"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[7]\nFlorian Eyben, Klaus R Scherer, BjÃ¶rn W Schuller, Johan Sundberg, Elisabeth",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "empirical methods in natural language processing (EMNLP). 1532â€“1543."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "AndrÃ©, Carlos Busso, Laurence Y Devillers, Julien Epps, Petri Laukka, Shrikanth S",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[23]\nSoujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. 2017. A review of"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Narayanan, et al. 2015. The Geneva minimalistic acoustic parameter set (GeMAPS)",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "affective computing: From unimodal analysis to multimodal fusion.\nInformation"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "for voice research and affective computing.\nIEEE Transactions on Affective Com-",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Fusion 37 (2017), 98â€“125."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "puting 7, 2 (2015), 190â€“202.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[24]\nJames A Russell. 1980. A circumplex model of affect. Journal of Personality and"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[8]\nFlorian Eyben, Martin WÃ¶llmer, and BjÃ¶rn Schuller. 2010. Opensmile: the munich",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Social Psychology 39, 6 (1980), 1161."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "versatile and fast open-source audio feature extractor. In Proceedings of the 18th",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Enrique SÃ¡nchez-Lozano, Paula Lopez-Otero, Laura Docio-Fernandez, Enrique\n[25]"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "ACM international conference on Multimedia. 1459â€“1462.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Argones-RÃºa, and JosÃ© Luis Alba-Castro. 2013. Audiovisual three-level fusion for"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[9]\nJohnny RJ Fontaine, Klaus R Scherer, Etienne B Roesch, and Phoebe C Ellsworth.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "continuous estimation of russellâ€™s emotion circumplex. In Proceedings of the 3rd"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "2007. The world of emotions is not two-dimensional. Psychological Science 18, 12",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "ACM international workshop on Audio/visual emotion challenge. 31â€“40."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "(2007), 1050â€“1057.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[26] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[10]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "R Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[27]\nLukas Stappen, Alice Baird, Lukas Christ, Lea Schumann, Benjamin Sertolli,"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "and human-labeled dataset for audio events. In IEEE International Conference on",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Eva-Maria Messner, Erik Cambria, Guoying Zhao, and BjÃ¶rn W Schuller. 2021."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Acoustics, Speech and Signal Processing (ICASSP). IEEE, 776â€“780.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emo-"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[11] Michael Grimm and Kristian Kroschel. 2005. Evaluation of natural emotions using",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "tion, Physiological-Emotion, and Stress. In Proceedings of the 2nd International"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "self assessment manikins. In IEEE Workshop on Automatic Speech Recognition and",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "on Multimodal Sentiment Analysis Challenge and Workshop (Chengdu, China)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Understanding. IEEE, 381â€“385.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Association for Computing Machinery, New York, NY, USA."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[12] Rui Guo, Shuangjiang Li, Li He, Wei Gao, Hairong Qi, and Gina Owens. 2013.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Lukas Stappen, Alice Baird, Lea Schumann, and BjÃ¶rn Schuller. 2021. The Mul-\n[28]"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Pervasive and unobtrusive emotion sensing for human mental health. In 2013",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "timodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection,"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "7th International Conference on Pervasive Computing Technologies for Healthcare",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Insights and Improvements.\nIEEE Transactions on Affective Computing (2021)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "and Workshops. IEEE, 436â€“439.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[29]\nLicai Sun, Zheng Lian, Jianhua Tao, Bin Liu, and Mingyue Niu. 2020. Multi-modal"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[13]\nShawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "continuous dimensional emotion recognition using recurrent neural network and"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "self-attention mechanism. In Proceedings of the 1st International on Multimodal"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Seybold, et al. 2017. CNN architectures for large-scale audio classification. In",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Sentiment Analysis in Real-life Media Challenge and Workshop. 27â€“34."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "International Conference on Acoustics, Speech and Signal Processing(ICASSP). IEEE,",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[30] Bruno Verschuere, Geert Crombez, Ernst Koster, and Katarzyna Uzieblo. 2006."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "131â€“135.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Psychopathy and physiological detection of concealed information: A review."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[14]\nJian Huang, Ya Li, Jianhua Tao, Zheng Lian, Mingyue Niu, and Minghao Yang.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Psychologica Belgica 46, 1-2 (2006)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "2018. Multimodal continuous emotion recognition with data augmentation using",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[31] Martin WÃ¶llmer, Moritz Kaiser, Florian Eyben, BjÃ¶rn Schuller, and Gerhard"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "recurrent neural networks. In Proceedings of the 2018 on Audio/Visual Emotion",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Rigoll. 2013. LSTM-Modeling of continuous emotions in an audiovisual affect"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Challenge and Workshop. 57â€“64.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "recognition framework.\nImage and Vision Computing 31, 2 (2013), 153â€“163."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[15]\nJian Huang, Ya Li, Jianhua Tao, Zheng Lian, Zhengqi Wen, Minghao Yang, and",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[32] Michelle SM Yik, James A Russell, and Lisa Feldman Barrett. 1999. Structure of"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Jiangyan Yi. 2017. Continuous multimodal emotion prediction based on long",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "self-reported current affect: Integration and beyond. Journal of Personality and"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "short term memory recurrent neural network. In Proceedings of the 7th Annual",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Social Psychology 77, 3 (1999), 600."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Workshop on Audio/Visual Emotion Challenge. 11â€“18.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[33]\nJinming Zhao, Ruichen Li, Shizhe Chen, and Qin Jin. 2018. Multi-modal multi-"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Jian Huang, Jianhua Tao, Bin Liu, Zhen Lian, and Mingyue Niu. 2019. Efficient\n[16]",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "cultural dimensional continues emotion recognition in dyadic interactions. In"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "modeling of long temporal contexts for continuous emotion recognition. In 2019",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop. 65â€“72."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "8th International Conference on Affective Computing and Intelligent Interaction",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[34]\nJinming Zhao, Ruichen Li, Shizhe Chen, and Qin Jin. 2018. Multi-modal multi-"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "(ACII). IEEE, 185â€“191.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "cultural dimensional continues emotion recognition in dyadic interactions. In"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop. 65â€“72."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Hichem Sahli. 2019. Efficient spatial temporal convolutional features for audio-",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "transformer fusion for continuous emotion recognition. In ICASSP 2020-2020 IEEE"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "visual continuous affect recognition. In Proceedings of the 9th International on",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Audio/Visual Emotion Challenge and Workshop. 19â€“26.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "3507â€“3511."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[4]\nShizhe Chen, Qin Jin, Jinming Zhao, and Shuai Wang. 2017. Multimodal multi-",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[18]\nFuyan Ma, Bin Sun, and Shutao Li. 2021. Robust facial expression recognition"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "task learning for dimensional and continuous emotion recognition. In Proceedings",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "with convolutional visual transformers. arXiv preprint arXiv:2103.16854 (2021)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "of the 7th Annual Workshop on Audio/Visual Emotion Challenge. 19â€“26.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[19]\nStacy Marsella and Jonathan Gratch. 2014. Computationally modeling human"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[5]\nSilvia De Nadai, Massimo Dâ€™IncÃ , Francesco Parodi, Mauro Benza, Anita Trotta,",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "emotion. Commun. ACM 57, 12 (2014), 56â€“67."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Enrico Zero, Luca Zero, and Roberto Sacile. 2016. Enhancing safety of transport",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "by road by on-line monitoring of driver emotions. In 2016 11th System of Systems",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Distributed representations of words and phrases and their compositionality. In"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Engineering Conference (SoSE). IEEE, 1â€“4.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Advances in Neural Information Processing Systems. 3111â€“3119."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[6] Zhengyin Du, Suowei Wu, Di Huang, Weixin Li, and Yunhong Wang. 2019.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[21] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. 2015. Deep face"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Spatio-temporal encoder-decoder fully convolutional network for video-based",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "recognition.\n(2015)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "dimensional emotion recognition.\nIEEE Transactions on Affective Computing",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[22]\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "(2019).",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Global vectors for word representation. In Proceedings of the 2014 conference on"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[7]\nFlorian Eyben, Klaus R Scherer, BjÃ¶rn W Schuller, Johan Sundberg, Elisabeth",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "empirical methods in natural language processing (EMNLP). 1532â€“1543."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "AndrÃ©, Carlos Busso, Laurence Y Devillers, Julien Epps, Petri Laukka, Shrikanth S",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[23]\nSoujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. 2017. A review of"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Narayanan, et al. 2015. The Geneva minimalistic acoustic parameter set (GeMAPS)",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "affective computing: From unimodal analysis to multimodal fusion.\nInformation"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "for voice research and affective computing.\nIEEE Transactions on Affective Com-",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Fusion 37 (2017), 98â€“125."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "puting 7, 2 (2015), 190â€“202.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[24]\nJames A Russell. 1980. A circumplex model of affect. Journal of Personality and"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[8]\nFlorian Eyben, Martin WÃ¶llmer, and BjÃ¶rn Schuller. 2010. Opensmile: the munich",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Social Psychology 39, 6 (1980), 1161."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "versatile and fast open-source audio feature extractor. In Proceedings of the 18th",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Enrique SÃ¡nchez-Lozano, Paula Lopez-Otero, Laura Docio-Fernandez, Enrique\n[25]"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "ACM international conference on Multimedia. 1459â€“1462.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Argones-RÃºa, and JosÃ© Luis Alba-Castro. 2013. Audiovisual three-level fusion for"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[9]\nJohnny RJ Fontaine, Klaus R Scherer, Etienne B Roesch, and Phoebe C Ellsworth.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "continuous estimation of russellâ€™s emotion circumplex. In Proceedings of the 3rd"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "2007. The world of emotions is not two-dimensional. Psychological Science 18, 12",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "ACM international workshop on Audio/visual emotion challenge. 31â€“40."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "(2007), 1050â€“1057.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[26] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[10]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "R Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[27]\nLukas Stappen, Alice Baird, Lukas Christ, Lea Schumann, Benjamin Sertolli,"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "and human-labeled dataset for audio events. In IEEE International Conference on",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Eva-Maria Messner, Erik Cambria, Guoying Zhao, and BjÃ¶rn W Schuller. 2021."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Acoustics, Speech and Signal Processing (ICASSP). IEEE, 776â€“780.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emo-"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[11] Michael Grimm and Kristian Kroschel. 2005. Evaluation of natural emotions using",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "tion, Physiological-Emotion, and Stress. In Proceedings of the 2nd International"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "self assessment manikins. In IEEE Workshop on Automatic Speech Recognition and",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "on Multimodal Sentiment Analysis Challenge and Workshop (Chengdu, China)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Understanding. IEEE, 381â€“385.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Association for Computing Machinery, New York, NY, USA."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[12] Rui Guo, Shuangjiang Li, Li He, Wei Gao, Hairong Qi, and Gina Owens. 2013.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Lukas Stappen, Alice Baird, Lea Schumann, and BjÃ¶rn Schuller. 2021. The Mul-\n[28]"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Pervasive and unobtrusive emotion sensing for human mental health. In 2013",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "timodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection,"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "7th International Conference on Pervasive Computing Technologies for Healthcare",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Insights and Improvements.\nIEEE Transactions on Affective Computing (2021)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "and Workshops. IEEE, 436â€“439.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[29]\nLicai Sun, Zheng Lian, Jianhua Tao, Bin Liu, and Mingyue Niu. 2020. Multi-modal"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[13]\nShawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "continuous dimensional emotion recognition using recurrent neural network and"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "self-attention mechanism. In Proceedings of the 1st International on Multimodal"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Seybold, et al. 2017. CNN architectures for large-scale audio classification. In",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Sentiment Analysis in Real-life Media Challenge and Workshop. 27â€“34."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "International Conference on Acoustics, Speech and Signal Processing(ICASSP). IEEE,",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[30] Bruno Verschuere, Geert Crombez, Ernst Koster, and Katarzyna Uzieblo. 2006."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "131â€“135.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Psychopathy and physiological detection of concealed information: A review."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[14]\nJian Huang, Ya Li, Jianhua Tao, Zheng Lian, Mingyue Niu, and Minghao Yang.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Psychologica Belgica 46, 1-2 (2006)."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "2018. Multimodal continuous emotion recognition with data augmentation using",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[31] Martin WÃ¶llmer, Moritz Kaiser, Florian Eyben, BjÃ¶rn Schuller, and Gerhard"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "recurrent neural networks. In Proceedings of the 2018 on Audio/Visual Emotion",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Rigoll. 2013. LSTM-Modeling of continuous emotions in an audiovisual affect"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Challenge and Workshop. 57â€“64.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "recognition framework.\nImage and Vision Computing 31, 2 (2013), 153â€“163."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "[15]\nJian Huang, Ya Li, Jianhua Tao, Zheng Lian, Zhengqi Wen, Minghao Yang, and",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[32] Michelle SM Yik, James A Russell, and Lisa Feldman Barrett. 1999. Structure of"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Jiangyan Yi. 2017. Continuous multimodal emotion prediction based on long",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "self-reported current affect: Integration and beyond. Journal of Personality and"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "short term memory recurrent neural network. In Proceedings of the 7th Annual",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Social Psychology 77, 3 (1999), 600."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Workshop on Audio/Visual Emotion Challenge. 11â€“18.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[33]\nJinming Zhao, Ruichen Li, Shizhe Chen, and Qin Jin. 2018. Multi-modal multi-"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "Jian Huang, Jianhua Tao, Bin Liu, Zhen Lian, and Mingyue Niu. 2019. Efficient\n[16]",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "cultural dimensional continues emotion recognition in dyadic interactions. In"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "modeling of long temporal contexts for continuous emotion recognition. In 2019",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop. 65â€“72."
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "8th International Conference on Affective Computing and Intelligent Interaction",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "[34]\nJinming Zhao, Ruichen Li, Shizhe Chen, and Qin Jin. 2018. Multi-modal multi-"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "(ACII). IEEE, 185â€“191.",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "cultural dimensional continues emotion recognition in dyadic interactions. In"
        },
        {
          "[3] Haifeng Chen, Yifan Deng, Shiwen Cheng, Yixuan Wang, Dongmei Jiang, and": "",
          "[17]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. 2020. Multimodal": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop. 65â€“72."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Stress detection in daily life scenarios using smart phones and wearable sensors: A survey",
      "authors": [
        "Yekta Said Can",
        "Bert Arnrich",
        "Cem Ersoy"
      ],
      "year": "2019",
      "venue": "Journal of biomedical informatics"
    },
    {
      "citation_id": "2",
      "title": "Efficient spatial temporal convolutional features for audiovisual continuous affect recognition",
      "authors": [
        "Haifeng Chen",
        "Yifan Deng",
        "Shiwen Cheng",
        "Yixuan Wang",
        "Dongmei Jiang",
        "Hichem Sahli"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "3",
      "title": "Efficient spatial temporal convolutional features for audiovisual continuous affect recognition",
      "authors": [
        "Haifeng Chen",
        "Yifan Deng",
        "Shiwen Cheng",
        "Yixuan Wang",
        "Dongmei Jiang",
        "Hichem Sahli"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "4",
      "title": "Multimodal multitask learning for dimensional and continuous emotion recognition",
      "authors": [
        "Shizhe Chen",
        "Qin Jin",
        "Jinming Zhao",
        "Shuai Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "5",
      "title": "Enhancing safety of transport by road by on-line monitoring of driver emotions",
      "authors": [
        "Silvia De Nadai",
        "D' Massimo",
        "Francesco IncÃ ",
        "Mauro Parodi",
        "Anita Benza",
        "Enrico Trotta",
        "Luca Zero",
        "Roberto Zero",
        "Sacile"
      ],
      "year": "2016",
      "venue": "2016 11th System of Systems Engineering Conference (SoSE)"
    },
    {
      "citation_id": "6",
      "title": "Spatio-temporal encoder-decoder fully convolutional network for video-based dimensional emotion recognition",
      "authors": [
        "Zhengyin Du",
        "Suowei Wu",
        "Di Huang",
        "Weixin Li",
        "Yunhong Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "BjÃ¶rn Schuller",
        "Johan Sundberg",
        "Elisabeth AndrÃ©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "The world of emotions is not two-dimensional",
      "authors": [
        "Johnny Rj Fontaine",
        "Klaus Scherer",
        "Etienne Roesch",
        "Phoebe Ellsworth"
      ],
      "year": "2007",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "10",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Evaluation of natural emotions using self assessment manikins",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel"
      ],
      "year": "2005",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "12",
      "title": "Pervasive and unobtrusive emotion sensing for human mental health",
      "authors": [
        "Rui Guo",
        "Shuangjiang Li",
        "Li He",
        "Wei Gao",
        "Hairong Qi",
        "Gina Owens"
      ],
      "year": "2013",
      "venue": "2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops"
    },
    {
      "citation_id": "13",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Multimodal continuous emotion recognition with data augmentation using recurrent neural networks",
      "authors": [
        "Jian Huang",
        "Ya Li",
        "Jianhua Tao",
        "Zheng Lian",
        "Mingyue Niu",
        "Minghao Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "15",
      "title": "Continuous multimodal emotion prediction based on long short term memory recurrent neural network",
      "authors": [
        "Jian Huang",
        "Ya Li",
        "Jianhua Tao",
        "Zheng Lian",
        "Zhengqi Wen",
        "Minghao Yang",
        "Jiangyan Yi"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "16",
      "title": "Efficient modeling of long temporal contexts for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zhen Lian",
        "Mingyue Niu"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "17",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Robust facial expression recognition with convolutional visual transformers",
      "authors": [
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2021",
      "venue": "Robust facial expression recognition with convolutional visual transformers",
      "arxiv": "arXiv:2103.16854"
    },
    {
      "citation_id": "19",
      "title": "Computationally modeling human emotion",
      "authors": [
        "Stacy Marsella",
        "Jonathan Gratch"
      ],
      "year": "2014",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "20",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeff Dean"
      ],
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "Deep face recognition",
      "authors": [
        "Andrea Omkar M Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "Deep face recognition"
    },
    {
      "citation_id": "22",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "23",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "24",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "25",
      "title": "Audiovisual three-level fusion for continuous estimation of russell's emotion circumplex",
      "authors": [
        "Enrique SÃ¡nchez-Lozano",
        "Paula Lopez-Otero",
        "Laura Docio-Fernandez",
        "Enrique Argones-RÃºa",
        "JosÃ© Luis"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM"
    },
    {
      "citation_id": "26",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "27",
      "title": "The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lukas Christ",
        "Lea Schumann",
        "Benjamin Sertolli",
        "Eva-Maria Messner",
        "Erik Cambria",
        "Guoying Zhao",
        "BjÃ¶rn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd International on Multimodal Sentiment Analysis Challenge and Workshop"
    },
    {
      "citation_id": "28",
      "title": "The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lea Schumann",
        "BjÃ¶rn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "30",
      "title": "Psychopathy and physiological detection of concealed information: A review",
      "authors": [
        "Bruno Verschuere",
        "Geert Crombez",
        "Ernst Koster",
        "Katarzyna Uzieblo"
      ],
      "year": "2006",
      "venue": "Psychologica Belgica"
    },
    {
      "citation_id": "31",
      "title": "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework",
      "authors": [
        "Martin WÃ¶llmer",
        "Moritz Kaiser",
        "Florian Eyben",
        "BjÃ¶rn Schuller",
        "Gerhard Rigoll"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "32",
      "title": "Structure of self-reported current affect: Integration and beyond",
      "authors": [
        "Michelle Sm Yik",
        "James Russell",
        "Lisa Barrett"
      ],
      "year": "1999",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "33",
      "title": "Multi-modal multicultural dimensional continues emotion recognition in dyadic interactions",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Shizhe Chen",
        "Qin Jin"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "34",
      "title": "Multi-modal multicultural dimensional continues emotion recognition in dyadic interactions",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Shizhe Chen",
        "Qin Jin"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop"
    }
  ]
}