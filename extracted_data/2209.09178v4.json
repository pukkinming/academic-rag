{
  "paper_id": "2209.09178v4",
  "title": "Vit-Dd: Multi-Task Vision Transformer For Semi-Supervised Driver Distraction Detection",
  "published": "2022-09-19T16:56:51Z",
  "authors": [
    "Yunsheng Ma",
    "Ziran Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Ensuring traffic safety and mitigating accidents in modern driving is of paramount importance, and computer vision technologies have the potential to significantly contribute to this goal. This paper presents a multi-modal Vision Transformer for Driver Distraction Detection (termed ViT-DD), which incorporates inductive information from training signals related to both distraction detection and driver emotion recognition. Additionally, a self-learning algorithm is developed, allowing for the seamless integration of driver data without emotion labels into the multi-task training process of ViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existing stateof-the-art methods for driver distraction detection by 6.5% and 0.9% on the SFDDD and AUCDD datasets, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "According to National Highway Traffic Safety Administration (NHTSA), there were 38,824 people killed in motor vehicle crashes on U.S. roadways during 2020. Among these cases, 3,142 (or 8.1%) are distraction-affected crashes, i.e., a crash involving at least one driver who was distracted. Distracted driving is defined by NHTSA as any activity that diverts attention away from safe driving, such as talking or texting on a cell phone, eating and drinking, chatting with others in the car, and fiddling with the audio, entertainment, or navigation system  [1] .\n\nDuring the past decade, rapid development has been witnessed worldwide in intelligent vehicle technology, where advancements in perception  [2] -  [5] , communication  [6] -  [12] , and computation introduced numerous emerging applications on intelligent vehicles  [13] -  [17] . As a core element of intelligent vehicles, driving automation systems, such as Advanced Driver-Assistance Systems (ADAS) and Automated Driving Systems (ADS), have been designed to support human drivers either by providing warnings to reduce risk exposure, or by assisting the vehicle actuation to relieve drivers' burden on some of the driving tasks  [14] ,  [18] . When functioning, these systems can help the driver safely navigate the vehicle through tricky traffic scenarios when he/she is distracted by some other tasks.\n\nHowever, a driver can also over-trust the driving automation system, especially when the system is categorized as SAE Level 3 (i.e., conditional driving automation): When the automated driving features are engaged, the driver is allowed to take his/her hands off the steering wheel and feet off the pedals, but he/she needs to stay alert and get ready to take over the driving task when the system requests  [19] . Due to human nature, the attention from the driver on road conditions Y. Ma and Z. Wang are with College of Engineering, Purdue University, West Lafayette, IN 47907, USA. Emails: {yunsheng, ziran}@purdue.edu can get diminished when he/she is not in charge of driving, and the involvement of distracted behaviors can decrease the driver's capability of taking over, which in turn leads to traffic accidents.\n\nIt can be envisioned in future transportation systems that intelligent vehicles can detect and identify driver distractions, then warn the driver against them or take precautionary measures. Therefore, in this paper, a multi-modal Vision Transformer (termed ViT-DD) is proposed to exploit inductive information contained in the training signals of both emotion recognition and distraction detection, along with a novel pseudo-labeled multi-task training algorithm that leverages the knowledge in an independent emotion recognition teacher model to train a student ViT-DD.\n\nIn summary, the contributions of this paper are threefold:\n\n• This paper explores the detection of driver distractions using a pure Transformer-based architecture and a semisupervised learning setup, which, to the best of the authors' knowledge, has not been previously investigated.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Background",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Vision Transformer",
      "text": "Since AlexNet  [20] , convolutional neural networks (CNNs) have been the dominant methodology for learning visual representations of images in computer vision (CV)  [21] ,  [22] . Vision Transformer (ViT)  [23] , on the other hand, has recently achieved state-of-the-art performances on a variety of CV tasks and garnered significant interest from the CV community  [24] ,  [25] .\n\nViT seldom employs convolution kernels (i.e. the core of CNNs). Instead, it relies on the self-attention mechanism  [26]  to provide context information for input visual tokens, which is inspired by tasks in natural language processing. In addition to the initial patch extraction process, ViT does not introduce image-specific inductive biases into its architecture.\n\nThere have been some attempts to apply Transformer architecture to the detection of driver distractions. For example, DDR-ViT  [27]  is a distracted driving recognition model based on a fine-tuned Vision Transformer, proposed to solve the problem of accurate and timely recognition of driving behavior. TransDARC  [28]  presented a vision-based framework for recognizing secondary driver behaviors based on visual transformers and an additional augmented feature distribution calibration module. However, none of these works consider driver emotion as an additional modality that can significantly improve performance in detecting distracted behaviors.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Multi-Task Learning And Self-Training",
      "text": "Multi-Task Learning (MTL) is an inductive transfer mechanism with the primary objective of enhancing generalization performance. Learning one task at a time is the standard for machine learning. However, Caruana  [29]  contends that this strategy is sometimes ineffective since it disregards a potentially rich source of information accessible in many realworld problems, i.e., the information contained in the training signals of other tasks drawn from the same domain. If the tasks can share what they learn, it may be preferable to require the learner to learn many capabilities simultaneously. In computer vision, a popular method for MTL is to employ a single encoder to learn a shared representation, followed by numerous task-specific decoders  [30] . In this paper, a similar strategy is employed by training one main backbone model together with several small task-specific heads.\n\nSelf-training is an approach for incorporating unlabeled data into a supervised learning task  [31] . It is one of the earliest semi-supervised learning approaches, which generates pseudo labels for unlabeled data using a supervised model. Recently, Ghiasi et al.  [32]  proposes multi-task self-training (MuST), an approach for generating generalized visual representations using multi-task learning with pseudo labels. This method differs from the approach presented in this paper in that the multi-task learning strategy is employed for both output and various input modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Facial Expression Recognition And Driver Distraction Detection",
      "text": "Facial expression recognition (FER) is an image classification problem that recognizes the emotion state of individuals  [33] . AffectNet-7  [34]  is currently the largest publicly available FER dataset, which comprises images labeled with Ekman's six fundamental emotions  [35] , namely happy, sad, surprise, fear, disgust, and anger, plus an additional neutral category. In this paper, FER is employed to drivers' face images to evaluate his or her emotion state, therefore acquiring additional information to detect driver distractions through multi-task learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, a novel multi-task ViT for semi-supervised driver distraction detection is proposed, where the overall framework is shown in Fig.  1 . Specifically, ViT-DD has two input modalities, i.e. driver and face, to exploit information contained in the training signals of both distraction detection and emotion recognition. The input images from both modalities are separated into patches, linearly projected to fixed-dimensional visual tokens, and encoded using a Transformer encoder. Task-specific classification heads are applied to the output sequence of the Transformer encoder to generate the prediction results. The training of ViT-DD is conducted through a novel multi-task multi-modal self-training technique.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Model Overview",
      "text": "The backbone of ViT-DD is a ViT  [23] , with different patch projection layers applied to each input modality (driver and face). Specially, the input space for each modality is defined by X (i) , where i ∈ {0, 1}. The input image x (i) ∈ X (i) ⊆ R C×Hi×Wi from modality i is sliced into patches and then flattened to v (i) ∈ R Ni×(P 2 •C) , where (P, P ) is the patch size, C is the number of channels of the input image, and N i = H i W i /P 2 is the number of patches for each modality. Next, the flattened patches are linear projected to D dimensional tokens with the projection matrix E (i) ∈ R (P 2 •C)×D , followed by position embedding E pos ∈ R Ni×D . Additionally, class tokens (t\n\nclass ) for each modality with a learnable embedding are prepended to the beginning of the input sequence. All input tokens are then concatenated into a combined sequence z 0 (Eq.2) and sent to the same Transformer Encoder as input.\n\nThe Transformer encoder then learns the fused driver behavior and emotion representation by stacking L Transformer blocks . A Multilayer Perceptron (MLP) module and a Multihead Self-attention (MSA) module are included in each block. Additionally, LayerNorm (LN)  [36]  is also adopted prior to each module. Self-attention (SA) is the key component of Transformer blocks, in which, the input vector z ℓ-1 is first transformed into three separate vectors: the query vector q, the key vector k, and the value vector v, all of the same dimension q, k, v ∈ R D . After that, the attention scores are constructed by the following function:\n\nwhere\n\nare learnable parameters of three linear projections and D H = D.\n\nMSA is an extension of SA with H self-attention heads, which can be formulated as:\n\nwhere W P ∈ R D×(H•D H ) , and D H is typically set to D/H. To obtain the final multi-task prediction probabilities of the driver's distractions and emotions, the states of the class tokens at the output of the Transformer encoder (z i L ) are fed into the respective classification heads, with the standard cross-entropy loss adopted. The final loss is the weighed sum of the loss of each head.\n\nFor all experiments, a ViT-B  [23]  pretrained on ImageNet  [37]  with a patch size of 16 × 16 pixels is employed as the backbone. Specially, the latent vector size D is 768, the patch size P is 16, the layer depth L is 12, and the number of attention heads H is 12.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Pseudo-Labeled Multi-Task Training",
      "text": "Pseudo labeling offers the benefit of not requiring a large multi-task dataset with all required labels. Having access to a well-trained neural network, that can produce pseudo labels of other domains on the dataset we wish to work on, can be effective. Pseudo labeling is a one-time preprocessing method applicable to RGB datasets of variable size. Compared to the training cost, this phase is computationally inexpensive  [38] .\n\nThe proposed multi-task multi-modal self-training algorithm has four steps. First a teacher ViT is trained on AffectNet-7  [34] , a large facial emotion recognition dataset, to enable it to recognize the facial expressions of drivers. Second, RetinaFace  [39] , a face detector, is used to detect and crop face images in the driver distraction detection datasets. Next, the FER teacher model is used to label the unlabeled drivers' face images to create a multi-task pseudo-labeled driver dataset. Finally, the driver dataset, which now contains both supervised labels for distraction detection and pseudo labels from the teacher model for emotion recognition, is then employed to train a student ViT-DD model with multi-task multi-modal learning. To manage the situation in which the driver's face cannot be detected, an additional Non-Face label is added to the emotion classification task, and in such case, a blank image is fed to the face input.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "In this section, the performance of the proposed ViT-DD model in detecting driver distractions is assessed. The employed benchmarks and baselines are described first. The major results are then reported along with some empirical analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Benchmarks",
      "text": "The performance of ViT-DD is evaluated on two publicly available distracted driver detection datasets: State Farm Distracted Driver Detection (SFDDD) and the American University in Cairo Distracted Driver Dataset (AUCDD). The SFDDD dataset is comprised of 22,424 labeled images of 26 drivers captured by a constant-placed 2D dashboard camera with 640 × 480 pixels in RGB  [40]  There are two commonly used train/test split methods for the SFDDD dataset. One option is to directly split the dataset by images for training or testing  [27] ,  [43] -  [45] , but it will result in a strong correlation between the training and testing data. In particular, it is possible that consecutive video frames are divided into training and testing sets, so it simplifies the problem of distraction detection. The other one is to divide the dataset by drivers such that images of the same driver do not appear in both training and test data  [46] .\n\nIn this paper, results of both split approaches for the SFDDD dataset are reported. For the first split method, 70%/30% of images are used for training and testing, respectively. While for the second, 18/6 of the total 28 drivers are randomly selected for training/testing. The AUCDD dataset adheres to the original split-by-driver setup.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Baselines",
      "text": "To compare ViT-DD with the state-of-the-art approaches for distracted driver detection, the following methods are selected as baselines: (1) GA-Weighted Ensemble  [41] , (2) ADNet  [47] ,\n\n(3) C-SLSTM  [48] , (4) DD-RCNN  [46] , (5) ViTConv  [43] , (6) Inception+ResNet+HRNN  [44] ,  (7)  LWANet  [45]   1  , and (8) DDR-ViT-finetuned  [27] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Implementation Details",
      "text": "The AdamW optimizer  [49]  was used with weight decay set to 0.1 for all experiments. The base learning rates were set to 0.0003 and 0.0006 for the SFDDD and AUCDD datasets, respectively. The learning rate was warmed up for 5 epochs, starting with a learning rate of 10 -6 and then decaying to 0 using the cosine scheduler. To prepare the data, the driver's images were resized to 224 × 224 and the face images to 32 × 32. A patch size of 16 × 16 was used, and a total of 200 patches were extracted. The ViT-DD model achieved an inference FPS of 15.56 with this input size on an NVIDIA 3090ti GPU, which is suitable for real-time applications.\n\nFor data augmentation, the Simple Random Crop strategy introduced by Touvron et al.  [50]  with random horizontal flip was used. For the SFDDD dataset, the 3-Augment introduced in  [50]  was also applied.\n\nAs the datasets used in this study are not very large, only the multi-head self-attention layers in the Transformer encoder were fine-tuned, as suggested by Touvron et al.  [51] . The ViT-DD model was trained for 20 epochs on 1 NVIDIA A100 GPU with a batch size of 256.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Comparison With State-Of-The-Art",
      "text": "The results of the performance comparisons between the proposed ViT-DD and the state-of-the-art approaches are presented in Table  I . It is worth noting that the accuracy scores are calculated in an unbiased manner that treats all categories equally, regardless of their size. Based on the results, we have the following observations:\n\n(1) Splitting-by-image produces a significant correlation between training and testing data. In comparison to other stateof-the-art results on SFDDD with this setting, such as LWANet  [45] , ViT-DD achieves an accuracy improvement of 0.26%. This demonstrates the excellent fitting capability of ViT-DD.\n\n(2) When adopting a more challenging and realistic split strategy, namely, separate by driver, ViT-DD can respectively obtain 6.5% and 0.9% performance gains over the reported state-of-the-art results, i.e., DD-RCNN  [46]  on SFDDD and C-SLSTM  [48]  on AUCDD. This result shows the superior generalization ability of ViT-DD. The performance improvements benefit from the advantages of ViT-DD. First the stateof-the-art ViT is employed as the backbone network, which can provide high generalization performance if pretrained on a large-scale dataset  [52] . Second, the novel multi-task multimodal self-training method enables ViT-DD to leverage addi-",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Ablation Study",
      "text": "The proposed ViT-DD is trained utilizing a novel multi-task multi-modal self-training procedure. To further validate the effectiveness of this strategy, an ablation study is conducted in comparison to the standard ViT trained with supervised driver distraction detection labels. The average accuracy and NLL on SFDDD split-by-driver and AUCDD datasets are shown in Table  II . The confusion matrices on the AUCDD dataset is shown in Fig.  3 .\n\nFrom Table  II , it is clear that for both datasets, ViT-DD performs better. The average accuracy improvements of ViT-DD over the standard ViT are 2.2% and 2.7% on the SFDDD and AUCDD datasets, respectively. This demonstrates that ViT-DD successfully leverages additional sources of information from the emotion recognition to improve the performance of learning on the task of distraction detection. From the confusion matrices, we have the following observations: (1) ViT-DD performs significantly better in detecting safe driving (C0) and talking to passenger (C9) with 13% and 9% increases in accuracy, respectively. This is because certain emotion states correlate strongly with these two driving behaviors. Specifically, in most cases, drivers have a neutral emotion when driving safely and tend to be happy when talking to passengers. Standard ViT tends to misclassify safe driving as phone right (C1). This can be resolved with the support of drivers' emotion information, as talking on the phone corresponds to all kinds of emotion status, not just neutral. Also, in standard ViT, 13% of talking to passenger scenarios are misclassified as hair or makeup (C7), compared to 0% in ViT-DD due to the inclusion of emotion information.\n\n(2) However, ViT-DD suffers a performance loss when detecting phone left (C2) and reaching behind (C8). Specifically, phone left is occasionally interpreted as hair or makeup (C7), and reaching behind is occasionally interpreted as drinking (C6). In both of these cases, emotion information may mislead the detection of driver distractions: The driver's emotion state can vary when phoning, so emotion cannot provide useful information for detecting this behavior; For reaching behind, it is difficult to identify the driver's emotion, so the emotion data may not be accurate.\n\n(3) It is worth noting that ViT-DD has a much higher detection accuracy on phone right (C1) than phone left (C2), which is due to the bias present in the dataset. The dashboard camera is positioned in front of the passenger's seat and photographs the driver from the right-hand side. As a result, detection of the phone on the right is much simpler than on the left, since the phone on the right is completely visible.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "F. Visualization",
      "text": "In order to show the interpretability of our model, the attention maps during inference on the AUCDD dataset are visualized in Fig.  2 , which shows the interactions between the distraction token and visual tokens of various Transformer encoder layers. The attention scores are used to generate the attention maps. For visualization purposes, the 1D sequence of attention scores is reshaped according to their original spatial positions in the driver or face images.\n\nAs seen in Fig.  2 , as the network becomes deeper, the distraction token gathers more precise local cues rather than the whole driver or face image signals. In the first few layers, the whole in-cabin scene provides interference cues, but a well-trained ViT-DD can gradually concentrate on critical areas of input images. For instance, in the first safe driving scenario, the model successfully focuses on the driving wheel region of the driver's image, which is the most informative area of the whole picture. For the second phone left scenario, the model effectively pays the most attention to the phone region. For both face images, the model attends to the eye region, which is the most distinguishable part of the face for recognizing the facial expressions of drivers.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion And Future Work",
      "text": "In this paper, a pure Transformer architecture-based method for detecting driver distractions is proposed. The developed ViT-DD trained with the novel pseudo-labeled multi-task learning algorithm can leverage information from emotion recognition to improve the performance of learning on distraction detection. Extensive experiments conducted on SFDDD and AUCDD benchmarks with the challenging split-by-driver setting demonstrate that ViT-DD achieves 6.5% and 0.9% performance improvements as compared to the best state-ofthe-art driver distraction detection approaches. As the next step, additional training signals available from in-cabin camera, such as gaze tracking and head pose tracking, can be incorporated into distraction detection or other driver behavior prediction tasks.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Specifically, ViT-DD has two",
      "page": 2
    },
    {
      "caption": "Figure 1: (Left) The framework of the proposed ViT-DD: First, a face detector is applied to the input signal from an in-cabin camera to acquire the driver’s",
      "page": 3
    },
    {
      "caption": "Figure 2: This figure displays the attention maps generated by a well-trained ViT-DD model during inference on the AUCDD dataset. The attention maps depict",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrices of the standard ViT and ViT-DD on AUCDD",
      "page": 5
    },
    {
      "caption": "Figure 3: From Table II, it is clear that for both datasets, ViT-DD per-",
      "page": 5
    },
    {
      "caption": "Figure 2: , which shows the interactions between",
      "page": 6
    },
    {
      "caption": "Figure 2: , as the network becomes deeper, the",
      "page": 6
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Overview of motor vehicle crashes in 2020",
      "year": "2022",
      "venue": "Overview of motor vehicle crashes in 2020"
    },
    {
      "citation_id": "2",
      "title": "Object-Level Data-Driven Sensor Simulation for Automotive Environment Perception",
      "authors": [
        "L Lindenmaier",
        "S Aradi",
        "T Bécsi",
        "O Törő",
        "P Gáspár"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "3",
      "title": "Surround-View Fisheye BEV-Perception for Valet Parking: Dataset, Baseline and Distortion-Insensitive Multi-Task Framework",
      "authors": [
        "Z Wu",
        "Y Gan",
        "X Li",
        "Y Wu",
        "X Wang",
        "T Xu",
        "F Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "4",
      "title": "Learning for Vehicle-to-Vehicle Cooperative Perception Under Lossy Communication",
      "authors": [
        "J Li",
        "R Xu",
        "X Liu",
        "J Ma",
        "Z Chi",
        "J Ma",
        "H Yu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "5",
      "title": "HYDRO-3D: Hybrid Object Detection and Tracking for Cooperative Perception Using 3D LiDAR",
      "authors": [
        "Z Meng",
        "X Xia",
        "R Xu",
        "W Liu",
        "J Ma"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "6",
      "title": "Intelligent Connectivity Through RIS-Assisted Wireless Communication: Exact Performance Analysis With Phase Errors and Mobility",
      "authors": [
        "V Chapala",
        "S Zafaruddin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "7",
      "title": "Network-Induced Asynchronous Fuzzy Control for Vehicle Steering Using Switching Event-Triggered Communication Mechanism",
      "authors": [
        "Z Gao",
        "D Zhang",
        "S Zhu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "8",
      "title": "CADer: A Deep Reinforcement Learning Approach for Designing the Communication Architecture of System of Systems",
      "authors": [
        "M Lin",
        "T Chen",
        "B Ren",
        "H Chen",
        "M Zhang",
        "D Guo"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "9",
      "title": "Memory-Anticipation Strategy to Compensate for Communication and Actuation Delays for Strings-Stable Platooning",
      "authors": [
        "Y Zhang",
        "Y Bai",
        "J Hu",
        "D Cao",
        "M Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "10",
      "title": "Robustly String Stable Longitudinal Control for Vehicle Platoons Under Communication Failures: A Generalized Extended State Observer-Based Control Approach",
      "authors": [
        "Q Chen",
        "Y Zhou",
        "S Ahn",
        "J Xia",
        "S Li",
        "S Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "11",
      "title": "SLAPS: Simultaneous Localization and Phase Shift for a RIS-Equipped UAV in 5G/6G Wireless Communication Networks",
      "authors": [
        "M Eskandari",
        "A Savkin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "12",
      "title": "Conflict Analysis for Cooperative Maneuvering With Status and Intent Sharing via V2X Communication",
      "authors": [
        "H Wang",
        "S Avedisov",
        "T Molnár",
        "A Sakr",
        "O Altintas",
        "G Orosz"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "13",
      "title": "The AD4CHE Dataset and Its Application in Typical Congestion Scenarios of Traffic Jam Pilot Systems",
      "authors": [
        "Y Zhang",
        "C Wang",
        "R Yu",
        "L Wang",
        "W Quan",
        "Y Gao",
        "P Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "14",
      "title": "Evaluation of Thermal Imaging on Embedded GPU Platforms for Application in Vehicular Assistance Systems",
      "authors": [
        "M Farooq",
        "W Shariff",
        "P Corcoran"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "15",
      "title": "Uncertainty-Aware Model-Based Reinforcement Learning: Methodology and Application in Autonomous Driving",
      "authors": [
        "J Wu",
        "Z Huang",
        "C Lv"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "16",
      "title": "ACP-Based Parallel Railway Traffic Management for High-Speed Trains in Case of Emergencies",
      "authors": [
        "M Zhou",
        "W Xu",
        "X Liu",
        "Z Zhang",
        "H Dong",
        "D Wen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "17",
      "title": "Software-Defined Active LiDARs for Autonomous Driving: A Parallel Intelligence-Based Adaptive Model",
      "authors": [
        "Y Liu",
        "B Sun",
        "Y Tian",
        "X Wang",
        "Y Zhu",
        "R Huai",
        "Y Shen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "18",
      "title": "Development and Testing of Advanced Driver Assistance Systems Through Scenario-Based Systems Engineering",
      "authors": [
        "X Li",
        "R Song",
        "J Fan",
        "M Liu",
        "F.-Y Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "19",
      "title": "Sae levels of driving automation™ refined for clarity and international audience",
      "authors": [
        "Sae"
      ],
      "year": "2022",
      "venue": "Sae levels of driving automation™ refined for clarity and international audience"
    },
    {
      "citation_id": "20",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "21",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "22",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "23",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "24",
      "title": "A Survey on Multimodal Large Language Models for Autonomous Driving",
      "authors": [
        "C Cui",
        "Y Ma",
        "X Cao",
        "W Ye",
        "Y Zhou",
        "K Liang",
        "J Chen",
        "J Lu",
        "Z Yang",
        "K.-D Liao",
        "T Gao",
        "E Li",
        "K Tang",
        "Z Cao",
        "T Zhou",
        "A Liu",
        "X Yan",
        "S Mei",
        "J Cao",
        "Z Wang",
        "C Zheng"
      ],
      "venue": "A Survey on Multimodal Large Language Models for Autonomous Driving"
    },
    {
      "citation_id": "25",
      "title": "MACP: Efficient Model Adaptation for Cooperative Perception",
      "authors": [
        "Y Ma",
        "J Lu",
        "C Cui",
        "S Zhao",
        "X Cao",
        "W Ye",
        "Z Wang"
      ],
      "year": "2024",
      "venue": "MACP: Efficient Model Adaptation for Cooperative Perception"
    },
    {
      "citation_id": "26",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Distracted driving recognition using vision transformer for human-machine co-driving",
      "authors": [
        "H Chen",
        "H Liu",
        "X Feng",
        "H Chen"
      ],
      "year": "2021",
      "venue": "2021 5th CAA International Conference on Vehicular Control and Intelligence (CVCI)"
    },
    {
      "citation_id": "28",
      "title": "Transdarc: Transformer-based driver activity recognition with latent space feature calibration",
      "authors": [
        "K Peng",
        "A Roitberg",
        "K Yang",
        "J Zhang",
        "R Stiefelhagen"
      ],
      "venue": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "29",
      "title": "Multitask learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1997",
      "venue": "Machine learning"
    },
    {
      "citation_id": "30",
      "title": "REDFormer: Radar Enlightens the Darkness of Camera Perception with Transformers",
      "authors": [
        "C Cui",
        "Y Ma",
        "J Lu",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "31",
      "title": "Self-Supervised Monocular Depth Estimation With Geometric Prior and Pixel-Level Sensitivity",
      "authors": [
        "J Liu",
        "Z Cao",
        "X Liu",
        "S Wang",
        "J Yu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "32",
      "title": "Multitask self-training for learning general representations",
      "authors": [
        "G Ghiasi",
        "B Zoph",
        "E Cubuk",
        "Q Le",
        "T.-Y Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "33",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "36",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "37",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "38",
      "title": "Multimae: Multi-modal multi-task masked autoencoders",
      "authors": [
        "R Bachmann",
        "D Mizrahi",
        "A Atanov",
        "A Zamir"
      ],
      "year": "2022",
      "venue": "Multimae: Multi-modal multi-task masked autoencoders",
      "arxiv": "arXiv:2204.01678"
    },
    {
      "citation_id": "39",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "40",
      "title": "State farm distracted driver detection",
      "year": "2016",
      "venue": "State farm distracted driver detection"
    },
    {
      "citation_id": "41",
      "title": "Driver distraction identification with an ensemble of convolutional neural networks",
      "authors": [
        "H Eraqi",
        "Y Abouelnaga",
        "M Saad",
        "M Moustafa"
      ],
      "year": "2019",
      "venue": "Journal of Advanced Transportation"
    },
    {
      "citation_id": "42",
      "title": "Real-time distracted driver posture classification",
      "authors": [
        "Y Abouelnaga",
        "H Eraqi",
        "M Moustafa"
      ],
      "year": "2017",
      "venue": "Real-time distracted driver posture classification",
      "arxiv": "arXiv:1706.09498"
    },
    {
      "citation_id": "43",
      "title": "Distracted driving detection by combining vit and cnn",
      "authors": [
        "Y Li",
        "L Wang",
        "W Mi",
        "H Xu",
        "J Hu",
        "H Li"
      ],
      "year": "2022",
      "venue": "2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)"
    },
    {
      "citation_id": "44",
      "title": "Distracted driver classification using deep learning",
      "authors": [
        "M Alotaibi",
        "B Alotaibi"
      ],
      "year": "2020",
      "venue": "Signal, Image and Video Processing"
    },
    {
      "citation_id": "45",
      "title": "A lightweight attentionbased network towards distracted driving behavior recognition",
      "authors": [
        "Y Lin",
        "D Cao",
        "Z Fu",
        "Y Huang",
        "Y Song"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "46",
      "title": "Driver action recognition using deformable and dilated faster r-cnn with optimized region proposals",
      "authors": [
        "M Lu",
        "Y Hu",
        "X Lu"
      ],
      "year": "2020",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Attention-based deep neural network for driver behavior recognition",
      "authors": [
        "W Xiao",
        "H Liu",
        "Z Ma",
        "W Chen"
      ],
      "year": "2022",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "48",
      "title": "A hybrid deep learning approach for driver distraction detection",
      "authors": [
        "J Mase",
        "P Chapman",
        "G Figueredo",
        "M Torres"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Information and Communication Technology Convergence (ICTC)"
    },
    {
      "citation_id": "49",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "50",
      "title": "Deit iii: Revenge of the vit",
      "authors": [
        "H Touvron",
        "M Cord",
        "H Jégou"
      ],
      "year": "2022",
      "venue": "Deit iii: Revenge of the vit",
      "arxiv": "arXiv:2204.07118"
    },
    {
      "citation_id": "51",
      "title": "Three things everyone should know about vision transformers",
      "authors": [
        "H Touvron",
        "M Cord",
        "A El-Nouby",
        "J Verbeek",
        "H Jégou"
      ],
      "year": "2022",
      "venue": "Three things everyone should know about vision transformers",
      "arxiv": "arXiv:2203.09795"
    },
    {
      "citation_id": "52",
      "title": "Plex: Towards reliability using pretrained large model extensions",
      "authors": [
        "D Tran",
        "J Liu",
        "M Dusenberry",
        "D Phan",
        "M Collier",
        "J Ren",
        "K Han",
        "Z Wang",
        "Z Mariet",
        "H Hu"
      ],
      "year": "2022",
      "venue": "Plex: Towards reliability using pretrained large model extensions",
      "arxiv": "arXiv:2207.07411"
    }
  ]
}