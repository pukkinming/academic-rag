{
  "paper_id": "2406.14014v1",
  "title": "Feature Fusion Based On Mutual-Cross-Attention Mechanism For Eeg Emotion Recognition",
  "published": "2024-06-20T06:08:52Z",
  "authors": [
    "Yimin Zhao",
    "Jin Gu"
  ],
  "keywords": [
    "Emotion Recognition",
    "Attention Feature Fusion",
    "3D-CNN",
    "EEG Feature"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "An objective and accurate emotion diagnostic reference is vital to psychologists, especially when dealing with patients who are difficult to communicate with for pathological reasons. Nevertheless, current systems based on Electroencephalography (EEG) data utilized for sentiment discrimination have some problems, including excessive model complexity, mediocre accuracy, and limited interpretability. Consequently, we propose a novel and effective feature fusion mechanism named Mutual-Cross-Attention (MCA). Combining with a specially customized 3D Convolutional Neural Network (3D-CNN), this purely mathematical mechanism adeptly discovers the complementary relationship between timedomain and frequency-domain features in EEG data. Furthermore, the new designed Channel-PSD-DE 3D feature also contributes to the high performance. The proposed method eventually achieves 99.49% (valence) and 99.30% (arousal) accuracy on DEAP dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Autism and depression are serious psychological problem, potentially leading to detrimental outcomes. A recent study indicated that dysarthria, mood disorders, rumination, literal understanding of problems or communication difficulties make their assessment difficult  [3] . Therefore, it is essential for psychological therapist to examine more reliable indicators such as EEG data from patients. By promptly integrating the emotion judgements derived from these signals into the diagnostic process, psychologists are better equipped to formulate tailored treatment strategies for their patients.\n\nIn recent years, the academic community has achieved some advances in emotion recognition through various methods  [2, 10, 16] . Initially, the focus was on singular traditional EEG features, such as Differential Entropy (DE)  [17]  and Power Spectral Density (PSD)  [1] . Subsequently, approaches involving feature fusion and deep learning were adopted to enhance recognition accuracy. With the ⋆ Corresponding author.\n\napplication of these new technologies, the performance of the classifier has been improved, but there are still some problems. Currently, the mainstream fusion methods implemented different learnable models to extract feature mappings, and then concatenated them directly  [8, 7, 4, 13] . Some projects  [16, 2]  appended extra neural networks to further process the feature mappings. These strategies, which train the networks to autonomously concentrate on significant aspects of the signal, escalates the burden of model training and diminishes the efficiency of the model's outputs. Considering that sentiment classification systems require instantaneous output in practical applications, current increasingly complicated neural networks are not beneficial. In addition, this is not conducive to the interpretability of the task, potentially resulting in moral hazards.\n\nFurthermore, the latest study  [6]  indicated an emerging trend of utilizing 3D data inputs for models. The review identified two predominant structures of Channel-Time-Frame  [10]  and Channel-Topology-Time  [15] . However, the final results were unsatisfactory. As shown in Table  4 , the accuracy of 2D-Topology-DE structure of Yang et al.  [18]  is only 90.24%, which could be the SOTA 3D input feature structure of the other projects that use 3D-CNN network models. Our analysis suggests that the limited spatial information provided by the channel topology map may contribute to this situation.\n\nIn that case, to achieve an instant well-performing emotion justification system based on EEG analysis, this project introduces a novel solution that has been experimentally validated as the new state-of-the-art (SOTA) method. It encompasses two primary contributions:\n\nMutual-Cross-Attention Mechanism. Inspired by the self-attention mechanism proposed by Vaswani et al.  [14] , we introduce a purely mathematical method named Mutual-Cross-Attention (MCA) for it applies Attention Mechanism from each directions of two features. In the field of EEG emotion analysis, we are the first to propose a pure mathematical fusion method, coupled with customized 3D-CNN, to accomplish the task of feature fusion.\n\nNew 3D feature presentation. By analyzing existing projects, it is found that spectral information might be more prominent than spacial information (presented by channel topology). Hence, we develop a unique Channel-Frequency-Time 3D feature structure. This innovative feature presents spectral and temporal information simultaneously.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "To evaluate the proposed MCA, we designed a complete experimental pipeline with five steps: Data Acquisition, Pre-process, Feature Extraction, Feature Fusion, and Classification. In terms of the feature fusing procedure, the complementarity between multiple features and the ability of the fusion mechanism to find important information are both crucial. Finding the optimal combination is challenging. It is widely recognized that DE and PSD complement each other  [9] . Hence, these two features are selected for further feature fusion research. The accuracy results, based on the Circumplex Model of Affect that concentrates on arousal and valence, are compared with other SOTA methods to demonstrate the validity of our methodology",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Acquisition",
      "text": "The DEAP dataset from Queen Mary University was selected for our experimental setup. In the study, 32 individuals were monitored using electroencephalogram (EEG) and peripheral physiological signals as they viewed 40 one-minute music video clips. Participants rated each video on a scale of 1 to 9 in terms of arousal, valence, likeability, dominance, and familiarity  [5] . The data acquisition equipment has 32 channels and work with 512 Hz sampling frequency.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Process",
      "text": "The dataset is pre-processed guided by the well-known steps of Steve Luck  [9] . It is cleaned by filtering wave and excluding noise components. Firstly, Notch Filter is implemented to eliminate 50 Hz signal, commonly associated with interference from AC power sources. Additionally, considering measurement tool inaccuracies and environmental interferences, a 4-45 Hz band-pass filter is set. Following this, Independent Component Analysis (ICA) is applied to the filtered EEG to cancel noise elements like Electrooculogram, Electrocardiogram, Electromyography. The final step involved downsampling the original 512 Hz data to 128 Hz. These operations improve data quality, reduce data volume, and accelerate computation speed.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "The DE and PSD extractions are adopted across five distinct frequency bands to enhance feature representation and prevent information from influencing each other. The categories include θ (4-7 Hz), α (8-10 Hz), slow α (8-13 Hz), β (14-29 Hz), and γ (30-45 Hz).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "De Extraction.",
      "text": "There are several methods to calculate DE. If the signal fits the Gaussian distribution, which is performed as N (µ, ǫ 2 ). The mathematical formulation is equal to the following one: where ǫ is the standard deviation of f (x). It has been proven that the EEG data filtered by a 4-45 or similar band-pass filter fits a Gaussian distribution every 2 Hz  [11] . The formulation is applied every 2 seconds of data. Then these segments of DEs are collected and constructed as a DE trial array. Finally, the band-pass filter is applied to consider the DE feature separately according to that 5 different frequency bands mentioned in Sect. 2.3.\n\nPSD extraction. The Welch's method is used to extract power spectra density (PSD). The first step to acquiring the PSD value is dividing the whole signal into K batches and calculating for each of them. The mathematical presentation of the k th PSD value on frequency f is  [12] :\n\nwhere W is related to the Hanning window and F k (f ) is a windowed fast Fourier transform (FFT) at a specific frequency f , which is set as 128 Hz according to the analysis above. The window size is 2 seconds. Finally, the estimation of PSD with the Welch method is combined with the results from all segments:\n\nTo preliminarily evaluate the validity of the PSD, a corresponding diagram is plotted. Fig.  1  indicates that the local value of the PSD fluctuates in the frequency range of about 5-7 Hz and 9-11 Hz, which suggests there might be emotion presentation in these ranges. That is the reason for the Sect. 2.3 indicating two frequency bands (slow α and α) in 8-13 Hz range. Finally, the PSD's 4-45 Hz spectrum is categorically divided into five bands for further analysis.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Feature Fusion",
      "text": "The MCA mechanism is applied across each selected frequency band to fuse DE and PSD. Initially, respectively consider DE and PSD as key and query vector. Then, designate PSD as value and implement basic Scaled Dot-Product Attention, which is presented by:\n\nwhere Q, K, V respectively represent query, key, and value. And d k is the size of the query's last dimension.\n\nThat is one direction calculation in MCA. After that, PSD is used as Q, DE as K and V. Again, the Scaled Dot-Product Attention operation is implemented. The results from both directions are added together to get the new feature. Fig.  2  illustrates the entire process, and its mathematical presentation is:\n\nwhere f 1 is the first feature (DE) and f 2 is the second feature (PSD).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification",
      "text": "After those feature extraction operations are implemented, the final single feature is denoted as F f ∈ IR 32×5×60 . However, it takes too long to perform classification tasks. Therefore, every F f is split into 20 F s ∈ IR 32×5×3 . This operation allows the model to output a sentiment prediction every 3 seconds. A special 3D-CNN structure is proposed to process the feature F s . As shown in Fig.  3 , the network begins with a 3D convolutional layer, configured with one input channel and 32 output channels, utilizing a 3x3x3 kernel. This layer is followed by another 3D convolutional layer, which maintains the same number of output channels and kernel size. Using two consecutive convolutional layers with the same number of channels enhances the network. The trick deepens the network's capacity to extract features without immediately reducing the spatial dimensions of the input data. After the initial convolutional stage, a 2x2x2 kernel Max Pooling layer is employed with a (0, 1, 1) asymmetric padding.\n\nSubsequently, the network extends into another set of convolutional layers, where the number of output channels is doubled to 64. Following this, the second Max Pooling layer further downsamples the feature maps. Finally, the network transitions to a fully connected layer, which classifies the extracted features into two categories.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussions",
      "text": "The performance of the proposed model is demonstrated through various evaluation metrics as detailed in Table  1 . It is clear that all indices exceed 99%. Additionally, this section includes not only ablation experiments but also comparisons with other SOTA results. All these experiments and comparisons are conducted using the DEAP dataset. Ultimately, the proposed methodology is proven to be effective.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Experiments",
      "text": "The experiment primarily examines the impact of both singular and fused features on the results. For comparison with our proposed MCA method, a baseline configuration labeled \"DE+PSD\" is established, which is based on the summation of 3D-DE and 3D-PSD. According to the Table  2 , the accuracy results for single DE and single PSD are almost the same to those of \"DE+PSD\". However, the valence at 99.49% and arousal at 99.30% achieved by the proposed method are significantly higher than those of \"DE+PSD\". This proves that the proposed method has advantages in the complementary integration of DE and PSD information. The innovation of this project focuses mainly on the design of the new feature structure and the way of fusing the new features. Hence, the comparisons with other SOTA methods in these two aspects are carried out. Feature fusion comparison. There are various other fusion methods between different features as detailed in Table  3 . Gao et al.  [2]  integrated DE, PSD, Hjorth, and Sample Entropy (SE). Our project, however, utilizes a narrower range of features. Additionally, similar to the work  [16] , our project fuses both frequency domain and time domain features. This demonstrates that our proposed method is effective when working with similar features, whether they are the same or fewer in categories.\n\nIn the study by Liu et al.  [8] , the accuracy results are around 90%, showing commendable performance. However, their approach relies on pre-trained features, which might limit its ability to instantly output results compared to our proposed method. Sun et al.  [13]  developed TSFFN to fuse EEG features with high accuracy. Comprising a 3D-CNN and a transformer, the TSFFN might be too complex for efficient computation. This highlights the advantages of our proposed purely mathematical MCA to feature fusion. Feature structure comparison. For 3D feature presentations, the majority of emotion recognition projects based on EEG analysis have inclined towards using topology to expand a 1D channel into a 2D format. Subsequently, data on other dimensions are combined with the topological channel map. As indicated in Table  4 , the performance of our proposed feature structure outperforms all 3D-CNN methods that employ features of the topology and the Channel-Time-Frame. This validates the rationality and effectiveness of the structure we have designed.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "The proposed MCA mechanism, 3D feature Channel-PSD-DE, and customized 3D-CNN show excellent capabilities in EEG-based emotion recognition. The whole system effectively overcomes the limitations of existing systems in terms of instantaneity, accuracy, and interpretability. By integrating DE and PSD features through the MCA mechanism, the ability of mathematical fusion methods to extract meaningful information from EEG data is highlighted. The emotional discrimination system developed using this solution has great potential for practical clinical psychotherapy. In the future, we will further explore the transformer implementing MCA as a core, which could fuse features of larger and more complex datasets.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: PSD diagram of subject 01.",
      "page": 4
    },
    {
      "caption": "Figure 1: indicates that the local value of the PSD ﬂuctuates in the",
      "page": 4
    },
    {
      "caption": "Figure 2: Overview of mutual-cross-attention mechanism.",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates the entire process, and its mathematical presentation is:",
      "page": 5
    },
    {
      "caption": "Figure 3: , the network begins with a 3D convolutional layer, conﬁgured with one",
      "page": 5
    },
    {
      "caption": "Figure 3: Structure of new designed 3D-CNN.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: , the accuracy results for",
      "data": [
        {
          "Feature": "Channel-Frequency-DE 89.88\nChannel-PSD-Time\nDE+PSD(baseline)\nProposed MCA",
          "Valence(%) Arousal(%)": "91.88\n90.90\n99.49"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: , the accuracy results for",
      "data": [
        {
          "Features": "ResNet, LFCC\nDE, PSD, Hjorth, SE\nPSD, temporal statistics\nTime, 2D-Topology-Time TSFFN [13]",
          "Fusion Method(s)": "Concat & KNN [8]\nCNN & SVM [2]\nSTFFNN [16]",
          "Valence(%) Arousal(%)": "89.06\n80.52\n86.20\n98.53"
        },
        {
          "Features": "DE, PSD",
          "Fusion Method(s)": "MCA & 3D-CNN (Ours) 99.49",
          "Valence(%) Arousal(%)": "99.30"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Compare with results based on othersingular 3D feature presentations.",
      "data": [
        {
          "Feature": "2D-Topology-DE [18]\n2D-Topology-Time [15]\nChannel-Time-Frame [10]",
          "Network Valence(%) Arousal(%)": ""
        },
        {
          "Feature": "Channel-PSD-Time (Ours)",
          "Network Valence(%) Arousal(%)": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Eeg-based emotion recognition in listening music by using support vector machine and linear dynamic system",
      "authors": [
        "R Duan",
        "X Wang",
        "B Lu"
      ],
      "year": "2012",
      "venue": "Neural Information Processing",
      "doi": "10.1007/978-3-642-34478-7_57"
    },
    {
      "citation_id": "2",
      "title": "Eeg-based emotion recognition with feature fusion networks",
      "authors": [
        "Q Gao",
        "Y Yang",
        "Q Kang",
        "Z Tian",
        "Y Song"
      ],
      "year": "2022",
      "venue": "International Journal of Machine Learning and Cybernetics",
      "doi": "10.1007/s13042-021-01414-5"
    },
    {
      "citation_id": "3",
      "title": "Autism and depression: clinical presentation, evaluation and treatment",
      "authors": [
        "A Hervás"
      ],
      "year": "2023",
      "venue": "Medicina"
    },
    {
      "citation_id": "4",
      "title": "Sst-emotionnet: Spatial-spectral-temporal based attention 3d dense network for eeg emotion recognition",
      "authors": [
        "Z Jia",
        "Y Lin",
        "X Cai",
        "H Chen",
        "H Gou",
        "J Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia",
      "doi": "10.1145/3394171.3413724"
    },
    {
      "citation_id": "5",
      "title": "Deap: A database for emotion analysis ;using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "6",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys",
      "doi": "10.1145/3524499"
    },
    {
      "citation_id": "7",
      "title": "Positional-spectral-temporal attention in 3d convolutional neural networks for eeg emotion recognition",
      "authors": [
        "J Liu",
        "Y Zhao",
        "H Wu",
        "D Jiang"
      ],
      "year": "2021",
      "venue": "Positional-spectral-temporal attention in 3d convolutional neural networks for eeg emotion recognition",
      "doi": "10.48550/arXiv.2110.09955"
    },
    {
      "citation_id": "8",
      "title": "Multiple feature fusion for automatic emotion recognition using eeg signals",
      "authors": [
        "N Liu",
        "Y Fang",
        "L Li",
        "L Hou",
        "F Yang",
        "Y Guo"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2018.8462518"
    },
    {
      "citation_id": "9",
      "title": "An Introduction to the Event-Related Potential Technique",
      "authors": [
        "S Luck"
      ],
      "year": "2014",
      "venue": "An Introduction to the Event-Related Potential Technique"
    },
    {
      "citation_id": "10",
      "title": "Eeg-based emotion recognition using 3d convolutional neural networks",
      "authors": [
        "E Salama",
        "A El-Khoribi",
        "E Shoman",
        "A Wahby"
      ],
      "year": "2018",
      "venue": "International Journal of Advanced Computer Science and Applications",
      "doi": "10.14569/IJACSA.2018.090843"
    },
    {
      "citation_id": "11",
      "title": "Differential entropy feature for eeg-based vigilance estimation",
      "authors": [
        "L Shi",
        "Y Jiao",
        "B Lu"
      ],
      "year": "2013",
      "venue": "2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)",
      "doi": "10.1109/EMBC.2013.6611075"
    },
    {
      "citation_id": "12",
      "title": "Psd computations using welch's method",
      "authors": [
        "O Solomon"
      ],
      "venue": "Psd computations using welch's method"
    },
    {
      "citation_id": "13",
      "title": "SAND-91-1533",
      "authors": [
        "Tech",
        "Rep"
      ],
      "year": "1991",
      "venue": "SNL-NM)",
      "doi": "10.2172/5688766"
    },
    {
      "citation_id": "14",
      "title": "Multi-channel eeg emotion recognition based on parallel transformer and 3d-convolutional neural network",
      "authors": [
        "J Sun",
        "X Wang",
        "K Zhao",
        "S Hao",
        "T Wang"
      ],
      "year": "2022",
      "venue": "Mathematics",
      "doi": "10.3390/math10173131"
    },
    {
      "citation_id": "15",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Emotionet: A 3-d convolutional neural network for eeg-based emotion recognition",
      "authors": [
        "Y Wang",
        "Z Huang",
        "B Mccane",
        "P Neo"
      ],
      "year": "2018",
      "venue": "International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN.2018.8489715"
    },
    {
      "citation_id": "17",
      "title": "Spatialtemporal feature fusion neural network for eeg-based emotion recognition",
      "authors": [
        "Z Wang",
        "Y Wang",
        "J Zhang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "doi": "10.1109/TIM.2022.3165280"
    },
    {
      "citation_id": "18",
      "title": "Eeg-based emotion classification based on bidirectional long short-term memory network",
      "authors": [
        "J Yang",
        "X Huang",
        "H Wu",
        "X Yang"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science",
      "doi": "10.1016/j.procs.2020.06.117"
    },
    {
      "citation_id": "19",
      "title": "Continuous convolutional neural network with 3d input for eeg-based emotion recognition",
      "authors": [
        "Y Yang",
        "Q Wu",
        "Y Fu",
        "X Chen"
      ],
      "year": "2018",
      "venue": "Neural Information Processing",
      "doi": "10.1007/978-3-030-04239-4_39"
    }
  ]
}