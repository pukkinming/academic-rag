{
  "paper_id": "2404.12979v2",
  "title": "Trnet: Two-Level Refinement Network Leveraging Speech Enhancement For Noise Robust Speech Emotion Recognition",
  "published": "2024-04-19T16:09:17Z",
  "authors": [
    "Chengxin Chen",
    "Pengyuan Zhang"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Noise robustness",
    "Speech enhancement",
    "Feature compensation",
    "Representation calibration"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "One persistent challenge in Speech Emotion Recognition (SER) is the ubiquitous environmental noise, which frequently results in deteriorating SER performance in practice. In this paper, we introduce a Two-level Refinement Network, dubbed TRNet, to address this challenge. Specifically, a pre-trained speech enhancement module is employed for front-end noise reduction and noise level estimation. Later, we utilize clean speech spectrograms and their corresponding deep representations as reference signals to refine the spectrogram distortion and representation shift of enhanced speech during model training. Experimental results validate that the proposed TRNet substantially promotes the robustness of the proposed system in both matched and unmatched noisy environments, without compromising its performance in noise-free environments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) has become a hot topic in the speech processing field due to its wide applications, such as mental health care, personal voice assistants, and user preference analysis  [1] . The goal of SER is to categorize human speech into a predetermined set of discrete emotion labels, and numerous deep learning-based approaches have been proposed to break the upper limit of SER performance  [2, 3, 4, 5] . Most of these works focused on SER in relatively ideal experimental scenarios. In realworld applications, however, speech signals are frequently contaminated by miscellaneous noises, leading to a prominent drop in SER performance.\n\nTo increase the robustness of SER in noisy environments, one strategy involves focusing on feature engineering, exploring the design of feature sets that are insensitive to noise contamination  [6, 7, 8] . For example, Leem et al.  [8]  proposed a feature selection framework that automatically assessed the robustness of each acoustic feature against environmental noise. However, directly transferring these methods to current deep learning-based emotion recognition models presents challenges. An alternative approach is to investigate from the model's perspective, studying the application of data augmentation techniques to expose the model to target noise during training  [9, 10] . For instance, Lakomkin et al.  [9]  introduced random background noise addition and simulated reverberation to contaminate clean training data, while Tiwari et al.  [10]  utilized parametric generative models to generate noisy data. Nevertheless, these methods may not be sufficiently effective on testing data contaminated with unseen noise types. Recent research has explored methods that integrate speech enhancement (SE) with SER models  [11, 12, 13] , aiming to increase the robustness of back-end SER models in noisy environments through noise reduction pre-processing. Although frontend SE modules can improve SER performance to some extent, the enhanced speech signals often exhibit nonlinear distortions such as artifacts in practical applications  [11] . Therefore, further research is necessary to optimize the utilization of SE modules for noise robust SER.\n\nMotivated by the above observations, this paper proposes TRNet, a Twolevel Refinement Network for robust SER in noisy environments. TRNet initially estimates a coefficient with respect to the noise level, enabling low-level feature compensation and high-level representation calibration. The low-level feature compensation is designed to approximate target speech spectrograms from pairs of noisy and enhanced spectrograms, while the high-level representation calibration is tailored to align the deep representations extracted from both target and approximated spectrograms. Experimental results demonstrate that TRNet can effectively couple SE and SER modules, increasing the robustness of the system in both matched and unmatched environments while maintaining SER performance in noise-free environments.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Preliminary",
      "text": "Assuming the observed signal x ∈ R N is a mixture of the target speech signal x s and noise signal x n , i.e., x = x s + x n , the objective of SE is to recover x s from x. SE serves as an indispensable front-end module in most speech tasks, such as improving the intelligibility of phone conversations  [14]  and the accuracy of automatic speech recognition  [15] . In recent years, deep learning-based SE has become mainstream, with a plethora of high-performance SE algorithms emerging  [16, 17] . At a higher signal-tonoise ratio (SNR), the gains from noise reduction may be overwhelmed by losses due to signal distortion. One strategy to address this issue dynamically adjusts the importance of the front-end SE module based on the estimated SNR  [13, 18] . Inspired by the idea of SNR estimation, this paper aims to better couple SE and SER modules from the perspectives of both low-level feature compensation and high-level representation calibration.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Method",
      "text": "As illustrated in Figure  1 , the overall workflow of our proposed TRNet comprises five key modules: an SE module, an SNR-aware module, a bridge module and a pair of SER modules. In the following subsections, we elaborate on the details of each module.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Se Module",
      "text": "The SE module employs one of the state-of-the-art architectures, namely the Conformer-based Metric Generative Adversarial Network (CMGAN)  [19] . CMGAN was trained on the Voice Bank and DEMAND datasets and the project code of the pre-trained model is publicly available 1  . Previous research has indicated that the joint training of both SE and back-end models often leads to further improved performance in downstream tasks  [20] . However, during the training process of TRNet, the parameters of CMGAN are fixed for two reasons: (1) Considering real-world applications, SE serves as a general-purpose front-end module that needs to adapt to multiple downstream tasks. Joint training for the specific SER task may potentially impact the generalization of the SE module for other tasks or unmatched noisy environments;\n\n(2) Treating the SE module as a fixed-weight \"black box\" eliminates the need to consider the internal structures of SE models, thus saving computational resources required for training. Note that CMGAN can be replaced by other SE models, such as FSI-Net  [21]  or DPT-FSNet  [22] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Snr-Aware Module",
      "text": "Given an observed signal x and the corresponding enhanced signal x e , the SNR-aware module aims to dynamically adjust the importance of the SE module. Firstly, the Log Mel-scale Filter Bank (LMFB) spectrograms X and X e are computed, where X, X e ∈ R T ×F . Intuitively, the difference between X and X e increases as the SNR decreases. Hence, we calculate the cosine similarity to measure such difference. Let x i and x e i be the ith columns of X and X e , we have\n\nwhere d i is the ith element of the similarity vector d ∈ R F , and ∥•∥ 2 is the L 2 norm. Afterwards, d is passed through a fully connected (FC) layer and a Hardtanh activation function:\n\nwhere c ∈ [0, 1] is an estimated coefficient positively correlated with the SNR. Finally, the compensated feature X is given by\n\nAt higher SNRs, c is anticipated to approach 1, indicating that X closely resembles the original input X. As the SNR decreases, the noisy signal corresponds to smaller c, resulting in a higher weight for X e . During model training, the target speech spectrogram X s is utilized to guide the low-level feature compensation, and the loss function can be calculated as\n\n.\n\n(5)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ser Module",
      "text": "The SER module consists of two major parts. Firstly, the acoustic encoder is employed to convert X into an utterance-level representation, denoted as h. Within this encoder, multiple convolutional layers are initially stacked with residual connections to capture local spectrogram characteristics. Subsequently, an average pooling is conducted along the frequency axis, while an attention pooling  [23]  is performed along the time axis to aggregate global information. Finally, a classifier composed of FC and Softmax layers is employed to project h into a predicted emotion label, denoted as ŷ.\n\nAs shown in Figure  1 , we introduce another identical SER module, which is first pre-trained on the clean emotion dataset and then utilized to extract the utterance-level representation of the target speech, denoted as h s . During model training, the parameters of the pre-trained SER module are fixed.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Bridge Module",
      "text": "While X is expected to be closer to X s compared to X e , there still exists a certain level of distortion introduced by the SE module. Generally, such distortion becomes more pronounced with increasing SNRs. In this section, the bridge module is developed to adjust h according to different SNRs, aiming to directly align the distributions of h and h s in the emotion space.\n\nInspired by Feature-wise Linear Modulation (FiLM)  [24] , we adopt the estimated SNR coefficient c as a constraint to perform an affine transformation on h:\n\nwhere h represents the calibrated representation, while FC 1 and FC 2 denote different fully connected layers. Therefore, the loss function for the high-level representation calibration can be calculated as",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Objectives",
      "text": "The main task of emotion recognition adopts standard cross-entropy loss function, denoted as L task . Eventually, the overall loss function can be computed as\n\nWe empirically find that α = β = 0.5 will suffice in our evaluation. All the trainable modules of TRNet are jointly optimized by minimizing L.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "4.1. Dataset description IEMOCAP  [25]  is a well-benchmarked dataset for SER, which contains 5 sessions and each session is performed by one female and one male actor. In this research, we considered 4 dominant emotion categories: Angry, Happy, Neutral, and Sad. Following previous works, we merged the utterances labeled Excited into the Happy category, yielding 5531 speech samples.\n\nTo simulate noisy environments, we adopted another two noise datasets: ESC-50  [26]  and MUSAN  [27] . ESC-50 comprises 2000 5-second environmental recordings spanning 50 categories, including animal sounds, natural ambiances, urban noises, etc. MUSAN contains approximately 6 hours of noise recordings, encompassing mechanical (e.g., dial tones, fax machine noises) and environmental noises (e.g., thunder, rustling paper). We utilized all ESC-50 samples directly, while the noise recordings from MUSAN were segmented into 5-second intervals, resulting in a total of 3860 samples.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "The speech samples were resampled at 16 kHz, then the 80-dimensional LMFB features were extracted with 25 ms frame length and 10 ms frame shift. For parallel computing, the input features of the same minibatch were truncated or padded to a maximum length of 500 frames, and the batch size was set to 32. For the encoder of SER modules, the filter numbers of the convolutional layers were set as {32, 64, 128, 256}, with the kernel sizes of  (5, 5)  and the strides of (2, 2). An Adam optimizer with a learning rate of 10 -3 was adopted to optimize the model parameters. The models were trained for a maximum of 80 epochs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Settings",
      "text": "In this research, the clean speech dataset is denoted as IEM, while the datasets contaminated by ESC-50 and MUSAN are denoted as IEM-ESC and IEM-MUSAN, respectively. To ensure diverse noise exposure across the 5 sessions of IEMOCAP, noise samples were carefully selected to avoid originating from the same source recording. The speech samples were contaminated at 5 different SNRs (20 dB, 15 dB, 10 dB, 5 dB, and 0 dB) by random noise samples. The training data comprised IEM and IEM-ESC, while the testing data encompassed IEM, IEM-ESC, and IEM-MUSAN.\n\nFollowing previous works, leave-one-speaker-out cross-validation (LOSO CV) was employed in the experiments, where 4 sessions were used for training, while utterances from the remaining two speakers were used for validating and testing, respectively. As for performance evaluation, we used the officially recommended metrics, namely unweighted average recall (UAR) and weighted average recall (WAR).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparative Models",
      "text": "To validate the effectiveness of our proposed method, we implemented three representative models as baselines: (  1",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance Comparison",
      "text": "First, we perform the quantitative experiments to evaluate the model performance. According to the results shown in Table  1 , we conclude the following observations:\n\n(1) In comparison to Baseline-c, both Baseline-n and Baseline-e exhibit a notable enhancement in overall performance across the two noisy environments. However, there is a performance decline in the noise-free environment, indicating a trade-off where Baseline-n and Baseline-e prioritize robustness over performance in the noise-free environment. Additionally, Baseline-ne surpasses both Baseline-n and Baseline-e in the noisy and noise-free environments, but still falls short of TRNet.\n\n(2) The two variants of TRNet perform slightly worse than the original TRNet , yet they consistently outperform Baseline-e. It can be concluded that both low-level feature compensation and high-level representation calibration are beneficial for coupling the SE and SER modules. Moreover, comparing the two variant models reveals that the gain from high-level representation calibration is more pronounced. Interestingly, TRNet can achieve even better performance than Baseline-c. This may be attributed to the effective prior knowledge provided by the pre-trained SER module.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Computational Complexity Analysis",
      "text": "We further compare the computational complexity of different SE-based methods via the metrics of model parameters and multiply-accumulate operations (MACs)  2  . Note that we exclude the computational complexity of the SE module. From Table  2 , we find that the MACs of Baseline-ne are twice those of Baseline-e during both the training and inference phases due to the parallel input of noisy and enhanced speech. By contrast, the extra model parameters and MACs of TRNet are introduced only in the training phase by the pre-trained SER module.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Effect Of Snr Estimation",
      "text": "In this section, we further investigate the role of the estimated c in TRNet and its two variant models. As illustrated in Figure  2 , we conclude the following observations:\n\n1. As expected, c decreases as the SNR decreases, signifying an increase in the weights of enhanced signals. Moreover, similar trends are noted in both matched and unmatched noisy environments, showcasing TRNet's capability to adapt to unseen noise. 2. In comparison to TRNet w/o L high , TRNet w/o L low appears to be less sensitive to c when the SNR varies. We speculate the reason is that c functions directly as a linear combination coefficient in the low-level feature compensation, whereas it undergoes an affine transformation in the high-level representation calibration.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Visualization Analysis",
      "text": "Figure  3  visualizes the distribution of deep emotion representations at different SNRs using t-SNE  [28] . As shown in the first row, varying SNRs cause significant shifts in the distributional centers for Baseline-c. In contrast, Baseline-e mitigates this phenomenon to a certain extent, and TRNet further aligns the distributional centers at different SNRs. This suggests that TRNet can adeptly adjust the deep emotion representations in various noisy environments and approximate the representations of clean utterances more closely. In the second row of Figure  3 , an evident increase in distance between the centers of diverse emotion categories can be observed for TRNet, which leads to a stronger emotional discrimination within the deep representations.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose TRNet, a noise robust SER algorithm that leverages a pre-trained SE module for front-end noise reduction and noise level estimation. Based on the estimated SNR coefficient, the low-level feature compensation and high-level representation calibration are performed, jointly enhancing the robustness of the system against both seen and unseen environmental noise. Quantitative experimental results demonstrate that TRNet can achieve superior performance in both noisy and noise-free environments. We further validate the roles of SNR estimation and the characteristics of deep representations through ablation study and visualization analysis. In future work, we plan to extend the applicability of TRNet to more complex acoustic environments.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the overall workflow of our proposed TRNet",
      "page": 3
    },
    {
      "caption": "Figure 1: Diagram of our proposed TRNet. The red arrows are activated during both the",
      "page": 4
    },
    {
      "caption": "Figure 1: , we introduce another identical SER module, which",
      "page": 5
    },
    {
      "caption": "Figure 2: , we conclude the",
      "page": 9
    },
    {
      "caption": "Figure 2: Estimated SNR coefficients in different noisy environments. (a) IEM-ESC; (b)",
      "page": 10
    },
    {
      "caption": "Figure 3: visualizes the distribution of deep emotion representations at",
      "page": 10
    },
    {
      "caption": "Figure 3: , an evident increase in distance between",
      "page": 10
    },
    {
      "caption": "Figure 3: T-SNE visualization of deep emotion representations. 500 clean utterances were",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "T",
          "T\nT": "T",
          "RNet\nRNet w/o Llow": "RNet w/o Lhig",
          "Column_7": "h"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "T",
          "T\nT": "T",
          "RNet\nRNet w/o Llow": "RNet w/o Lhig",
          "Column_7": "h"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Kaya Mehmet Berkehan Akçay",
        "Oguz"
      ],
      "year": "2020",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "2",
      "title": "Attention guided 3D CNN-LSTM model for accurate speech based emotion recognition",
      "authors": [
        "Orhan Atila",
        "S Abdulkadir",
        "¸engür"
      ],
      "year": "2021",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "3",
      "title": "CTA-RNN: Channel and Temporalwise Attention RNN leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition",
      "authors": [
        "Chengxin Chen",
        "Pengyuan Zhang"
      ],
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "4",
      "title": "MPAF-CNN: Multiperspective aware and fine-grained fusion strategy for speech emotion recognition",
      "authors": [
        "Guoyan Li",
        "Junjie Hou",
        "Yi Liu",
        "Jianguo Wei"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "5",
      "title": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin-Cheng Wen",
        "Yujie Wei",
        "Yong Xu",
        "Kunhong Liu",
        "Hongming Shan"
      ],
      "venue": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition"
    },
    {
      "citation_id": "6",
      "title": "Emotion Recognition in the Noise Applying Large Acoustic Feature Sets",
      "authors": [
        "Björn Schuller",
        "Dejan Arsić",
        "Frank Wallhoff",
        "Gerhard Rigoll"
      ],
      "year": "2006",
      "venue": "Emotion Recognition in the Noise Applying Large Acoustic Feature Sets"
    },
    {
      "citation_id": "7",
      "title": "Speech Emotion Recognition using non-linear Teager energy based features in noisy environments",
      "authors": [
        "Alexandros Georgogiannis",
        "Vassilis Digalakis"
      ],
      "venue": "EUSIPCO"
    },
    {
      "citation_id": "8",
      "title": "Not All Features are Equal: Selection of Robust Features for Speech Emotion Recognition in Noisy Environments",
      "authors": [
        "Seong-Gyun Leem",
        "Daniel Fulford",
        "Jukka-Pekka Onnela",
        "David Gard",
        "Carlos Busso"
      ],
      "venue": "ICASSP"
    },
    {
      "citation_id": "9",
      "title": "On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks",
      "authors": [
        "Egor Lakomkin",
        "Mohammad-Ali Zamani",
        "Cornelius Weber",
        "Sven Magg",
        "Stefan Wermter"
      ],
      "venue": "IROS"
    },
    {
      "citation_id": "10",
      "title": "Multi-Conditioning and Data Augmentation Using Generative Noise Model for Speech Emotion Recognition in Noisy Conditions",
      "authors": [
        "Upasana Tiwari",
        "H Meet",
        "Rupayan Soni",
        "Ashish Chakraborty",
        "Sunil Panda",
        "Kumar Kopparapu"
      ],
      "venue": "ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Towards Robust Speech Emotion Recognition Using Deep Residual Networks for Speech Enhancement",
      "authors": [
        "Andreas Triantafyllopoulos",
        "Gil Keren",
        "Johannes Wagner",
        "Ingmar Steiner",
        "Björn Schuller"
      ],
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "12",
      "title": "Using Speech Enhancement Preprocessing for Speech Emotion Recognition in Realistic Noisy Conditions",
      "authors": [
        "Hengshun Zhou",
        "Jun Du",
        "Yanhui Tu",
        "Chin-Hui Lee"
      ],
      "venue": "Using Speech Enhancement Preprocessing for Speech Emotion Recognition in Realistic Noisy Conditions"
    },
    {
      "citation_id": "13",
      "title": "Noise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement",
      "authors": [
        "Yu-Wen Chen",
        "Julia Hirschberg",
        "Yu Tsao"
      ],
      "year": "2023",
      "venue": "Noise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement",
      "arxiv": "arXiv:2309.01164"
    },
    {
      "citation_id": "14",
      "title": "Sriram Srinivasan. ICASSP 2021 Deep Noise Suppression Challenge",
      "authors": [
        "K Chandan",
        "Harishchandra Reddy",
        "Vishak Dubey",
        "Ross Gopal",
        "Sebastian Cutler",
        "Hannes Braun",
        "Robert Gamper",
        "Aichner"
      ],
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Improving Noise Robust Automatic Speech Recognition with Single-Channel Time-Domain Enhancement Network",
      "authors": [
        "Keisuke Kinoshita",
        "Tsubasa Ochiai",
        "Marc Delcroix",
        "Tomohiro Nakatani"
      ],
      "venue": "ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Supervised Speech Separation Based on Deep Learning: An Overview",
      "authors": [
        "Deliang Wang",
        "Jitong Chen"
      ],
      "year": "2018",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "17",
      "title": "Sixty Years of Frequency-Domain Monaural Speech Enhancement: From Traditional to Deep Learning Methods",
      "authors": [
        "Chengshi Zheng",
        "Huiyong Zhang",
        "Wenzhe Liu",
        "Xiaoxue Luo",
        "Andong Li",
        "Xiaodong Li",
        "Brian Moore"
      ],
      "year": "2023",
      "venue": "Trends in Hearing"
    },
    {
      "citation_id": "18",
      "title": "SNRi Target Training for Joint Speech Enhancement and Recognition",
      "authors": [
        "Yuma Koizumi",
        "Shigeki Karita",
        "Arun Narayanan",
        "Sankaran Panchapagesan",
        "Michiel Bacchiani"
      ],
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "19",
      "title": "CMGAN: Conformer-Based Metric-GAN for Monaural Speech Enhancement",
      "authors": [
        "Ruizhe Sherif Abdulatif",
        "Bin Cao",
        "Yang"
      ],
      "year": "2024",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "20",
      "title": "ESPnet-SE: End-To-End Speech Enhancement and Separation Toolkit Designed for ASR Integration",
      "authors": [
        "Chenda Li",
        "Jing Shi",
        "Wangyou Zhang",
        "Aswin Shanmugam Subramanian",
        "Xuankai Chang",
        "Naoyuki Kamo",
        "Moto Hira",
        "Tomoki Hayashi",
        "Christoph Böddeker",
        "Zhuo Chen",
        "Shinji Watanabe"
      ],
      "venue": "ESPnet-SE: End-To-End Speech Enhancement and Separation Toolkit Designed for ASR Integration"
    },
    {
      "citation_id": "21",
      "title": "FSI-Net: A dual-stage full-and sub-band integration network for full-band speech enhancement",
      "authors": [
        "Guochen Yu",
        "Hui Wang",
        "Andong Li",
        "Wenzhe Liu",
        "Yuan Zhang",
        "Yutian Wang",
        "Chengshi Zheng"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "22",
      "title": "DPT-FSNet: Dual-Path Transformer Based Full-Band and Sub-Band Fusion Network for Speech Enhancement",
      "authors": [
        "Feng Dang",
        "Hangting Chen",
        "Pengyuan Zhang"
      ],
      "venue": "ICASSP"
    },
    {
      "citation_id": "23",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "venue": "ICASSP"
    },
    {
      "citation_id": "24",
      "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
      "authors": [
        "Ethan Perez",
        "Florian Strub",
        "Vincent Harm De Vries",
        "Aaron Dumoulin",
        "Courville"
      ],
      "venue": "AAAI"
    },
    {
      "citation_id": "25",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "S Shrikanth",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "26",
      "title": "ESC: Dataset for Environmental Sound Classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "year": "2015",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "27",
      "title": "MUSAN: A Music, Speech, and Noise Corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey"
      ],
      "year": "2015",
      "venue": "MUSAN: A Music, Speech, and Noise Corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "28",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maate",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "J. Mach. Learn. Res"
    }
  ]
}