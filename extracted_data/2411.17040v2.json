{
  "paper_id": "2411.17040v2",
  "title": "Multimodal Alignment And Fusion: A Survey",
  "published": "2024-11-26T02:10:27Z",
  "authors": [
    "Songtao Li",
    "Hao Tang"
  ],
  "keywords": [
    "Multimodal Alignment",
    "Multimodal Fusion",
    "Multimodality",
    "Machine Learning",
    "Survey"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This survey provides a comprehensive overview of recent advances in multimodal alignment and fusion within the field of machine learning, driven by the increasing availability and diversity of data modalities such as text, images, audio, and video. Unlike previous surveys that often focus on specific modalities or limited fusion strategies, our work presents a structure-centric and method-driven framework that emphasizes generalizable techniques. We systematically categorize and analyze key approaches to alignment and fusion through both structural perspectives-data-level, feature-level, and output-level fusion-and methodological paradigmsincluding statistical, kernel-based, graphical, generative, contrastive, attention-based, and large language model (LLM)-based methods, drawing insights from an extensive review of over 260 relevant studies. Furthermore, this survey highlights critical challenges such as crossmodal misalignment, computational bottlenecks, data quality issues, and the modality gap, along with recent efforts to address them. Applications ranging from social media analysis and medical imaging to emotion recognition and embodied AI are explored to illustrate the real-world impact of robust multimodal systems. The insights provided aim to guide future research toward optimizing multimodal learning systems for improved scalability, robustness, and generalizability across diverse domains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Rapid advancement in technology has led to an exponential increase in the generation of multimodal data, including images, text, audio, and video  [15] . This abundance of data presents opportunities and challenges for researchers and practitioners in diverse fields, such as computer vision and natural language processing. Integrating information from multiple modalities can significantly enhance the performance of machine learning models, improving their ability to understand complex scenarios in the real world  [20, 93, 94, 253, 12, 49] .\n\nAt the core of multimodal learning lie two interdependent problems: alignment and fusion. Alignment aims to establish semantic correspondences across modalities so that their representations occupy a shared space, while fusion merges these aligned features into unified predictions or embeddings. The combination of modalities is generally pursued with two main objectives: (i) Different data modalities can complement each other, thus improving the precision and effectiveness of models for specific tasks  [102, 247, 121] . (ii) Some modalities may have limited data availability or may be challenging to collect in large quantities; therefore, training in an LLM-based model can leverage knowledge transfer to achieve satisfactory performance in tasks with sparse data  [104, 121] .\n\nFor example, in social media analysis, combining textual content with related images or videos offers a more comprehensive understanding of user sentiment and behavior  [15, 113] . Beyond social networks, multimodal methods have shown promising results in applications such as automated caption generation for medical images, video summarization, and emotion recognition  [55, 123, 33, 223, 259, 214] . Despite these advancements, two major technical challenges remain in effectively leveraging multimodal data: alignment-ensuring semantic consistency across modalities-and fusion-integrating complementary cues to enhance downstream performance.\n\nTo illustrate how modern architectures approach these challenges, Figure  1  provides an overview of three typical multimodal model structures: (a) Two-Tower, which processes modalities separately and combines embeddings via simple operations; (b) Two-Leg, which introduces a dedicated fusion network on top of separate encoders; and (c) One-Tower, which jointly encodes all modalities in a unified network.\n\nWhile prior surveys often categorize fusion either by the stage of integration (e.g., data-level, featurelevel, output-level fusion)  [15, 18] , our work complements rather than replaces existing taxonomies. We retain the traditional classification while introducing a new organization based on the core methods they employ-ranging from statistical and kernel-based techniques to generative, contrastive, attention-based, and LLM-driven frameworks. This approach foregrounds methodological innovations and highlights how each paradigm contributes to deeper and more flexible multimodal integration. Additionally, unlike prior surveys that focus on specific modalities or models-such as vision-language models-and often treats alignment and fusion methods as a part of their survey, our work adopts a structure-centric and method-driven perspective, emphasizing general-purpose alignment and fusion methods  [240, 258, 103] . Note that while vision-text research dominates the current literature due to data availability and historical development patterns, the structural and methodological frameworks we present are designed to be transferable across modality combinations.\n\nThe organization of this survey is as follows. Section 2 presents an overview of the foundational concepts in multimodal learning, including recent advances in LLMs and vision models, laying the groundwork for discussions on fusion and alignment. Section 3 focuses on why to conduct a survey on alignment and fusion. Section 4 examines multimodal alignment and fusion approaches, presenting a dual perspective that combines structural categorizations-such as data-level, feature-level, and output-level fusion-with classifications based on the core methods and features involved in modern models. The section heavily focuses later classification from traditional strategies to recent advances, including statistical, kernel-based, graphical, generative, contrastive, attention-based and llm-based fusion frameworks, highlighting how these techniques enable deeper inter-modal integration and more flexible modeling of complex relationships. Section 5 addresses key challenges in multimodal fusion and alignment, including feature alignment, computational efficiency, data quality, and scalability. Finally, section 7 outlines the potential directions for future research and discusses practical implications, with the aim of guiding further innovation in the field.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Preliminaries",
      "text": "This section provides a brief overview of key topics and concepts to enhance the understanding of our work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mllm",
      "text": "Recently, both natural language processing (NLP) and computer vision (CV) have experienced rapid development, especially since the introduction of attention mechanisms and the Transformer  [194, 69, 249, 107, 248, 135, 83, 173, 45] . Building on this framework, numerous large language models (LLMs) have emerged, such as OpenAI's GPT series  [149, 24, 138 ] and Meta's Llama series  [50] . Similarly, in the vision domain, large vision models (LVMs) have been proposed, including Segment Anything  [80] , DINO  [238] , and DINOv2  [139] .\n\nHowever, these LLMs struggle to understand visual information and handle other modalities, such as audio or sensor inputs, while LVMs have limitations in reasoning  [225] . Given their complementary strengths, LLMs and LVMs are increasingly being combined, leading to the emergence of a new field called multimodal large language models (MLLMs). To extend the strong performance of LLMs in text processing to tasks involving other modalities, significant research efforts have been dedicated to developing large-scale multimodal models.\n\nTo extend the strong performance of LLMs in text processing to tasks involving other modalities, significant research efforts have focused on the development of large-scale multimodal models  [53] . Kosmos-2  [143]  introduces grounding capabilities by linking textual descriptions with visual contexts, allowing more accurate object detection and phrase recognition. PaLM-E  [48]  further integrates these capabilities into real-world applications, using sensor data for embodied tasks in robotics, such as sequential planning and visual question answering. Additionally, models like ContextDET  [234]  excel in contextual object detection, overcoming previous limitations in visual-language association by directly linking visual elements to language inputs.\n\nSeveral models have adopted a hierarchical approach to managing the complexity of multimodal data. For example, SEED-Bench-2 benchmarks hierarchical MLLM   [148, 72, 39, 193, 218, 170, 111, 28, 54, 211, 189, 231] : processes images and text separately, combining embeddings through simple operations (add, multiple, dot product and concatenate); (b) Two-Leg  [5, 13, 42, 61, 71, 100, 92, 124, 126, 128, 153, 169, 190, 210, 79] : combines separate image and text embeddings using a Fusion Network; (c) One-Tower  [17, 96, 95, 40, 30, 259, 209, 201, 14, 4] : utilizes a unified network to jointly embed image and text inputs.\n\ncapabilities, providing a structured framework to evaluate and improve model performance in both perception and cognition tasks  [87] . Furthermore, X-LLM enhances multimodal alignment by treating each modality as a \"foreign language\", allowing a more effective alignment of audio, visual, and textual inputs with large language models  [29] .\n\nAs MLLMs continue to evolve, foundational frameworks such as UnifiedVisionGPT enable the integration of multiple vision models into a unified platform, accelerating advancements in multimodal AI  [76] . These frameworks demonstrate the potential of MLLMs to not only leverage vast multimodal datasets but also adapt to a wide range of tasks, representing a significant step toward achieving artificial general intelligence.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Dataset",
      "text": "Different modalities offer unique characteristics. For example, images provide visual information, but are susceptible to variations in lighting and viewpoint  [255] . The text data are linguistically diverse and may contain ambiguities  [225] . Audio data conveys emotional content and other non-verbal cues  [15] .\n\nMultimodal datasets are foundational for training vision-language models (VLMs) by providing large-scale paired image-text data that enable model learning across various tasks, such as image captioning, text-to-image retrieval, and zero-shot classification. Key datasets include LAION-5B, WIT, and newer specialized datasets like RS5M, which target specific domains or challenges within multimodal learning. Table  1  summarizes the commonly used datasets and their characteristics.\n\nFor example, the LAION-5B dataset contains more than 5 billion CLIP-filtered image-text pairs, enabling researchers to fine-tune models such as CLIP (Contrastive Language-Image Pretraining) and GLIDE, supporting open-domain generation and robust zero-shot classification tasks  [157] . The WIT (Wikipedia-based Image Text) dataset, with more than 37 million image-text pairs in 108 languages, is designed to support multilingual and diverse retrieval tasks, focusing on cross-lingual understanding  [166] . The RS5M dataset, which consists of 5 million remote sensing image-text pairs, is optimized for domain-specific learning tasks such as semantic localization and vision-language retrieval in geospatial data  [254] . Furthermore, fine-grained datasets like ViLLA are tailored to capture complex region-attribute relationships, which are critical for tasks such as object detection in medical or synthetic imagery  [192] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Characteristics And Targets",
      "text": "Each modality in multimodal learning presents unique challenges. For example, image data often face issues such as lighting variations, occlusions, and perspective distortions, which can affect a model's ability to recognize objects and scenes under varying conditions  [256] . Text data bring complexities due to the variability of natural language, including ambiguity, slang, and pol- ysemy, which complicate accurate interpretation and alignment with other modalities  [225] . Similarly, audio data is susceptible to background noise, reverberation, and environmental interference, which can distort the intended signal and reduce model accuracy  [150] .\n\nTo address these challenges, specific loss functions are employed in multimodal learning to optimize both representations and alignments. These losses define how features from different modalities should be related or transformed to achieve meaningful alignment or fusion. Notable examples include:\n\n-Contrastive Loss (and Variants), commonly used in tasks such as image-text matching, aims to pull together semantically similar pairs while pushing apart dissimilar ones in the embedding space. This objective supports better alignment and discrimination across modalities. The basic contrastive loss is defined as:\n\nwhere d is the distance between embeddings of a pair, y = 1 indicates a negative pair, and m is a margin hyperparameter. Supervised contrastive loss extends this idea by leveraging class labels to construct positive and negative pairs within a batch. It encourages samples from the same class to cluster tightly while separating those from different classes. Its formulation is:\n\nwhere P(i) denotes indices of positive samples for anchor i, sim(•, •) is cosine similarity, τ is a temperature parameter, and K includes all samples in the batch. CLIP  [148]  shares a similar objective, using contrastivestyle learning with image-text pairs as positives and cross-modal negatives.\n\n-Sigmoid Loss  [235]  proposes a simplified and more efficient alternative to the softmax-based contrastive loss used in models like CLIP. Instead of normalizing similarities across the entire batch via softmax, which couples the loss computation to the global batch structure, the sigmoid loss treats each image-text pair independently as a binary classification problem: matching (positive) or non-matching (negative). It applies the sigmoid function to the scaled similarity score (with a learnable temperature t and bias b) and computes a binary cross-entropy loss. Formally, for a batch of size n, the loss is:\n\nwhere\n\n) + b, and\n\nThis formulation significantly improves training efficiency and scalability. First, it eliminates the need for costly all-to-all batch synchronization for softmax normalization, enabling a highly memory-efficient \"chunked\" implementation. Second, it decouples the batch size from the loss definition, leading to superior performance at small batch sizes and allowing stable training at extremely large batch sizes (e.g., one million). Furthermore, the introduction of a learnable bias term b stabilizes training by counteracting the initial extreme imbalance between positive and negative pairs, ensuring the model starts training from a more reasonable prior. These improvements make language-image pre-training more accessible and efficient, especially with limited computational resources.\n\n-Cross-Entropy Loss, a widely used classification loss, calculates the divergence between predicted and true probability distributions, enabling label-driven learning across modalities. It is fundamental in supervised classification tasks, and variants such as set crossentropy offer greater flexibility for multimodal tasks by handling multiple target answers  [257, 8] .\n\nCross-entropy loss is particularly effective when the goal is to map multimodal inputs into a shared label space. Given predicted logits p and ground truth y, it is defined as:\n\nwhere C is the number of classes. In multimodal settings, cross-entropy can be applied after fusing modalities or used independently per modality to encourage consistency.\n\n-Reconstruction Loss, used in autoencoders and multimodal fusion tasks, aims to reconstruct input data or mask noise, making models more resilient to modalityspecific distortions. This type of loss is essential for multimodal tasks requiring robust feature alignment and noise resilience, such as visual-textual and audiovisual fusion  [141] .\n\nReconstruction loss serves the purpose of preserving information during modality transformation or fusion. It ensures that no critical semantic content is lost during projection into a shared space. A common form is mean squared error (MSE):\n\nwhere x i is the original input and xi is the reconstructed version. This loss is especially useful in unsupervised or semi-supervised multimodal architectures.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Why Multimodal Alignment And Fusion",
      "text": "Alignment and fusion are two fundamental concepts in multimodal learning that, while distinct, are deeply interconnected and often mutually reinforce  [3, 15] . Alignment involves ensuring that the different modalities are properly matched and synchronized, making the information they convey coherent and suitable for integration. Fusion, on the other hand, refers to the combination of information from different modalities to create a unified representation that captures the essence of the data in a comprehensive way  [15, 184, 158] . Furthermore, many recent methods find it challenging to fusion without an alignment process  [97] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Enhancing Comprehensiveness And Robustness",
      "text": "Alignment ensures that data from different sources are synchronized in terms of time, space, or context, enabling a meaningful combination. Without proper alignment, the fusion process can result in misinterpretations or loss of crucial information  [18] . Once alignment is achieved, fusion utilizes the aligned data to produce a more robust and comprehensive representation  [97, 180] . By integrating multiple perspectives, fusion mitigates the weaknesses of individual modalities, leading to improved accuracy and reliability.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Addressing Data Sparsity And Imbalance",
      "text": "In many real-world applications, data from certain modalities may be scarce or difficult to obtain. Alignment helps to synchronize the available data, even if limited, to ensure that it can be used effectively  [165, 197] .\n\nThe fusion then enables the transfer of knowledge between modalities, allowing the model to leverage the strengths of one modality to compensate for the weaknesses of another. This is particularly beneficial in scenarios where one modality has abundant data, while another is limited.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Improving Model Generalization And Adaptability",
      "text": "Alignment ensures that the relationships between different modalities are well understood and accurately modeled, which is crucial for the model's ability to generalize across various contexts and applications  [15, 18] .\n\nFusion improves the model's adaptability by creating a unified representation that captures the nuances of the data more effectively. This unified representation can be more easily adapted to new tasks or environments, enhancing the overall flexibility of the model  [15, 18] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Enabling Advanced Applications",
      "text": "Alignment and fusion together enable advanced applications such as cross-modal retrieval, where information from one modality (e.g., text) is used to search for relevant information in another modality (e.g., images)  [198] . These processes are also crucial for tasks like emotion recognition  [137] , where combining visual and auditory cues provides a more accurate understanding of human emotions compared to using either modality alone.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal Alignment And Fusion",
      "text": "Multimodal data involves the integration of various types of information, such as images, text, and audio, which can be processed by machine learning models to improve performance across numerous tasks  [104, 15, 18, 228] . In this context, multimodal alignment and fusion are essential techniques that aim to effectively combine information from different modalities. While early approaches often processed modalities separately with only basic integration, recent methods have evolved to better capture semantic correspondences and interactions among modalities.\n\nAt a high level, these processes involve establishing meaningful relationships between heterogeneous data sources, either explicitly or implicitly, to construct representations that reflect shared semantics  [133, 146] . Explicit strategies may rely on similarity matrices to directly measure correspondences, while implicit approaches often operate in latent spaces, learning joint representations through intermediate steps such as translation or prediction  [15, 120] . These mechanisms are not strictly confined to one task or another but instead contribute to the broader objective of integrating diverse signals into a coherent model.\n\nFusion methods, in particular, play a critical role in how modalities interact within the architecture. Traditional classifications distinguish between early fusion, which combines data at the feature level  [163] , late fusion, which merges outputs at the decision level  [130] , and hybrid approaches that integrate both strategies  [255] . However, as technology evolves, the boundaries between these categories have become increasingly blurred. Modern architectures-such as CLIP  [148] -utilize dual encoders with relatively shallow interaction mechanisms, which are effective for retrieval-based tasks  [155, 182] , but fall short in more complex scenarios requiring nuanced understanding  [236, 179] .\n\nFor tasks like visual question answering and reasoning, deeper integration is essential to capture the interdependencies between modalities, going beyond simple concatenation or independent encoding  [167, 247] . This has led to the development of advanced fusion techniques that operate simultaneously at multiple levels of abstraction, challenging traditional stage-based categorizations. As a result, there is a growing need for a classification framework based on the core characteristics of current fusion technologies, rather than rigid temporal distinctions. Notably, attention-based mechanisms have emerged as powerful tools in this space, warranting separate treatment due to their unique contributions and rapid evolution in recent years  [1]  two classification approaches: one follows the traditional structural categorization, while the other is based on the core methods and features involved in the models.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Structural Perspectives: A Three-Level Taxonomy",
      "text": "From structural perspectives, a multimodal model typically involves an encoder that captures essential features from the input data and compresses them into a compact form, while the decoder reconstructs the output from this compressed representation  [13] .\n\nIn this architecture, the system is primarily composed of two major components: the encoder and the decoder. The encoder typically functions as a high-level feature extractor, transforming the input data into a latent space of significant features  [13, 190] . In other words, the encoding process preserves important semantic information while reducing redundancy. Once the encoding step is complete, the decoder generates a corresponding \"reconstructed\" output based on the latent representation  [13, 92] . In tasks like semantic segmentation, the decoder's output is usually a semantic label map that matches the size of the input.\n\nIn this section, we review models based on architecture. The encoder-decoder framework is an intuitive approach in which an encoder first extracts features, and then these more expressive representations are used to learn the correlations, enabling interactions between different modalities and integrating features from diverse sources. Increasingly, researchers are exploring hybrid ways to integrate features from different modalities to better reveal the relationships among them. To provide a summary, detailed information on representative models is presented in Table  2 .\n\nThis encoder-decoder architecture typically takes three forms: (1) Data-level fusion, where raw data from different modalities is concatenated and fed into a shared encoder; (2) Feature-level fusion, where features are extracted separately from each modality, possibly including intermediate layers, and then combined before being input into the decoder; and (3) Output-level fusion, where outputs of individual modality-specific models are concatenated after processing. Figure  2  illustrates these three types of encoder-decoder fusion structures. Feature-level fusion is often the most effective, as it considers the relationships between different modalities, enabling deeper integration rather than a superficial combination.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Data-Level Methods",
      "text": "In this method, data from each modality or processed data from each modality's unique preprocessing steps are combined at the input level  [42] . After this integration, the unified input from all modalities is passed through a single encoder to extract higher-level features. Essentially, data from different modalities is merged at the input stage, and a single encoder is used to extract comprehensive features from the multimodal information.\n\nRecent research has focused on data-level fusion to improve object detection and perception in autonomous vehicles. Studies have explored fusing camera and Li-DAR data at the early stages of neural network architectures, demonstrating enhanced 3D object detection accuracy, particularly for cyclists in sparse point clouds  [153] . A YOLO-based framework that jointly processes raw camera and LiDAR data showed a 5% improvement in vehicle detection compared to traditional decisionlevel fusion  [42] . Additionally, an open hardware and software platform for low-level sensor fusion, specifically leveraging raw radar data, has been developed to facilitate research in this area  [169] . These studies highlight the potential of raw-data-level fusion to exploit inter-sensor synergies and improve overall system performance.\n\nTable  2 : Summary of models from structural perspective.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Year Category Modality Explanation",
      "text": "TFN  [233]  2017 Output-level Text, Audio, Image First to use tensor fusion for high-order modality interactions at decision level. Enables end-to-end learning without intermediate fusion, capturing complex multimodal dynamics.\n\nMFAS  [188]  2018 Hybrid Text, Audio, Image Combines feature-and output-level fusion via factorized attention and modality gating. Dynamically weights modalities, outperforming static fusion strategies.\n\nViLBERT  [116]  2019 Feature-level Text, Image Employs a two-stream architecture with co-attentional transformer layers for separate vision and language processing. Innovates by pretraining on masked multimodal modeling and alignment prediction, enabling deep cross-modal interaction while preserving modalityspecific processing depths.\n\nUNITER  [35]  2020 Feature-level Text, Image\n\nEarly unified Transformer for vision-language pretraining. Pioneered MLM, MRM, and ITM tasks, setting the standard for subsequent VLP models.\n\nPerceiver  [70]  2021 Enhances CLIP with generative captioning for data refinement. Jointly optimizes understanding and generation, improving performance on both retrieval and captioning tasks.\n\nFLAVA  [162]  2022 Feature-level Text, Image\n\nUses three separate encoders with multimodal fusion and joint unimodal/multimodal pretraining. Unique in supporting comprehensive unimodal and multimodal tasks within one model.\n\nImageBind  [58]  2023 Feature-level Text, Image, Audio, Depth, Thermal, IMU Aligns six modalities in joint embedding space using image as binding medium via contrastive learning. Achieves \"emergent alignment\" for unseen modality pairs without direct training, enabling zero-shot crossmodal retrieval and classification.\n\nProVLA  [68]  2023 Output-level Text, Image\n\nUses two-stage Transformer with hard negative mining for progressive fusion. Improves retrieval accuracy through iterative cross-modal refinement.\n\nTextBind  [91]  2024 Hybrid Text, Image\n\nCombines feature-level fusion (Q-Former mapping) and output-level fusion (LM to Stable Diffusion). Supports interleaved multimodal inputs/outputs for comprehensive instruction following with understanding and generation capabilities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Feature-Level Methods",
      "text": "The concept behind this fusion technique is to combine data from multiple levels of abstraction, allowing features extracted at different layers of hierarchical deep networks to be utilized, ultimately enhancing model performance. Many applications have implemented this fusion strategy  [171, 156, 100, 124, 126] .\n\nFeature-level fusion has emerged as a powerful approach in various computer vision tasks. It involves combining features at different levels of abstraction to improve performance. For instance, in gender classification, a two-level hierarchy that fused local patches proved effective  [156] . For salient object detection, a network that hierarchically fused features from different VGG levels preserved both semantic and edge information  [100] . In multimodal affective computing, a \"divide, conquer, and combine\" strategy explored both local and global interactions, achieving state-of-the-art performance  [124] . For adaptive visual tracking, a hierarchical model fusion framework was developed to update object models hierarchically, guiding the search in parameter space and reducing computational complexity  [126] . These approaches demonstrate the versatility of hierarchical feature fusion across various domains, showcasing its ability to capture both fine-grained and high-level information for improved performance in complex visual tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Output-Level Methods",
      "text": "Output-level fusion is a technique that improves accuracy in various applications by integrating the outputs from multiple models. For example, in landmine detection using ground penetrating radar (GPR), Missaoui et al.  [128]  demonstrated that fusing Edge Histogram Descriptors and Gabor Wavelets through a multi-stream Continuous hidden markov model (HMM) outperformed individual features and equal-weight combinations.\n\nIn multimodal object detection, Guo and Zhang  [61]  applied fusion methods such as averaging, weighting, cascading, and stacking to combine the results from models processing images, speech, and video, thereby improving performance in complex environments. For facial action unit (AU) detection, Jaiswal et al.  [71]  found that output-level fusion using artificial neural networks (ANNs) was more effective than simple featurelevel approaches.\n\nAdditionally, for physical systems involving multifidelity computer models, Allaire and Willcox  [5]  developed a fusion methodology that uses model inadequacy information and synthetic data, resulting in better estimates compared to individual models. In quality control and predictive maintenance, a novel output-level fusion approach outperformed traditional methods, reducing prediction variance and increasing accuracy  [210] . These studies demonstrate the effectiveness of output-level fusion across various domains.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Methodological Approaches: Classification Based On Core Techniques",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Statistical Methods",
      "text": "In the early stages, \"alignment\" often referred to the process of mapping vectors from different modalities into a shared vector space, while \"fusion\" typically involved a simple summation of the aligned modality vectors, followed by feeding the summed result into a neural network to obtain the final fused representation. Such alignment methods frequently relied on statistical techniques such as dynamic time warping (DTW)  [191, 85]  and canonical correlation analysis (CCA)  [67] .\n\nDTW measures the similarity between two sequences by finding an optimal match through time warping, which involves inserting frames to align the sequences  [191] . However, the original DTW formulation requires a predefined similarity metric, so it has been extended with CCA, introduced by Harold Hotelling in 1936  [67] , to project two different spaces into a common space through linear transformations. The goal of CCA is to maximize the correlation between the two spaces by optimizing the projection. CCA facilitates both alignment (through DTW) and joint learning of the mapping between modalities in an unsupervised manner, as seen in multimodal applications such as video-text and videoaudio alignment. Figure  3  visualizes the CCA method. Specifically, the objective function of CCA can be expressed as:\n\nwhere:\n\n-X and Y are the data matrices from two different spaces; u and v are the linear transformation vectors (or canonical vectors) that project X and Y into the common space; ρ is the correlation coefficient between the projections u T X and v T Y ; -The goal is to find u and v that maximize the correlation ρ between the projected data. However, CCA can only capture linear relationships between two modalities, limiting its applicability in complex scenarios involving non-linear relationships. To address this limitation, kernel canonical correlation analysis (KCCA) was introduced to handle non-linear dependencies by mapping the original data into a higherdimensional feature space using kernel methods  [11, 65] . Extensions such as multi-label KCCA and deep canonical correlation analysis (DCCA) further improved upon the original CCA method  [2, 127, 11, 65, 6] .\n\nAdditionally, Verma and Jawahar demonstrated that multimodal retrieval could be achieved using support vector machines (SVMs)  [195] . Furthermore, methods such as linear mapping between feature modalities for image alignment have been developed to address multimodal alignment through complex spatial transformations  [74] . Fig.  3 : Canonical Correlation Analysis (CCA), a classic alignment method, aligns different sample matrices with varying feature dimensions using a shared weight matrix to produce a unified representation. X ,Y and Z are the data matrices from three different spaces.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Kernel-Based Methods",
      "text": "Kernel-based techniques have gained prominence across various domains for their ability to handle nonlinear relationships and effectively integrate heterogeneous data sources. These methods leverage the kernel trick to map data into higher-dimensional spaces, enabling improved feature representation and analysis  [7, 131] . By selecting appropriate kernel functions, such as polynomial kernels or radial basis function kernels, these methods can achieve computational efficiency while maintaining model complexity and accuracy.\n\nKernel cross-modal factor analysis has been introduced as a novel approach for multimodal fusion, particularly for bimodal emotion recognition  [208] . This technique identifies optimal transformations to represent coupled patterns between different feature subsets. In drug discovery, integrating multiple data sources through kernel functions within SVMs enhances drugprotein interaction predictions  [207] . For audio-visual voice activity detection, kernel-based fusion with optimized bandwidth selection outperforms traditional approaches in noisy environments  [47] . In multimedia semantic indexing, kernel-based normalized early fusion and contextual late fusion schemes demonstrate improvements over standard fusion methods  [10] . For drug repositioning, kernel-based data fusion effectively integrates heterogeneous information sources, outperforming rank-based fusion and providing a unique solution for identifying new therapeutic applications of existing drugs  [7] . Fig.  4 : In graph-based alignment, different data modalities can form graphs with distinct meanings, where the interpretation of edges and nodes may vary. For example, in  [82] , the interpretation of vertices and edges depends on the type of biological networks being compared.\n\nThrough the use of the kernel trick, these methods achieve computational efficiency and improve prediction accuracy by better representing patterns. However, challenges exist, including difficulty in selecting the right kernel and tuning parameters, potential scalability issues with large datasets, reduced interpretability due to higher-dimensional projections, and the risk of overfitting if not properly regularized.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Graphical Model-Based Methods",
      "text": "The integration of graph structures allows for better modeling of complex relationships between different modalities, enabling more accurate and efficient processing of multimodal data. Such methods are commonly applied in aligning images with text or images with signals.\n\nFor instance, certain models enable few-shot in-context imitation learning by aligning graph representations of objects, allowing robots to perform tasks on new objects without prior training  [196] . The GraphAlignment algorithm, based on an explicit evolutionary model, demonstrates robust performance in identifying homologous vertices and resolving paralogs, outperforming alternatives in specific scenarios  [82] . Figure  4  illustrates how graphs are used in alignment.\n\nA significant challenge in these tasks is aligning implicit information across modalities, where multimodal signals do not always correspond directly to one another. Graph-based models have proven effective in addressing this challenge by representing complex relationships be-tween modalities as graphs, where nodes represent data elements (e.g., words, objects, or frames) and edges represent relationships (e.g., semantic, spatial, or temporal) between them.\n\nRecent studies have explored various aspects of multimodal alignment using graph structures. For instance, Tang et al.  [178]  introduced a graph-based multimodal sequential embedding approach to improve sign language translation. By embedding multimodal data into a unified graph structure, their model better captures complex relationships.\n\nAnother application is in sentiment analysis, where implicit multimodal alignment plays a crucial role. Yang et al.  [224]  proposed a multimodal graph-based alignment model (MGAM) that jointly models explicit aspects (e.g., objects, sentiment) and implicit multimodal interactions (e.g., image-text relations).\n\nIn the domain of embodied AI, Song et al.  [164]  explored how scene-driven knowledge graphs can be constructed to model implicit relationships in complex multimodal tasks. Their work integrates both textual and visual information into a knowledge graph, where multimodal semantics are aligned through graph-based reasoning. Aligning implicit cues, such as spatial and temporal relationships between objects in a scene, is crucial for improving decision-making and interaction in embodied AI systems.\n\nFor named entity recognition (NER), Zhang et al.  [252]  proposed a token-wise graph-based approach that incorporates implicit visual information from images associated with text. This method leverages spatial relations in the visual domain to improve the identification of named entities, which are often ambiguous when using isolated textual data.\n\nIn tasks such as image captioning and visual question answering (VQA), scene graphs also play a crucial role. Xiong et al.  [215]  introduced a scene graph-based model for semantic alignment across modalities. By representing objects and their relationships as nodes and edges in a graph, the model improves the alignment of visual and textual modalities.\n\nBesides, graphical models provide a powerful approach for representing and fusing multimodal data, effectively capturing complex relationships between different modalities  [261] . These models are particularly useful for handling incomplete multimodal data. For example, the heterogeneous graph-based multimodal Fusion (HGMF) method  [31]  constructs a heterogeneous hypernode graph to model and fuse incomplete multimodal data. HGMF leverages hypernode graphs to accommodate diverse data combinations without requiring data imputation, enabling robust representations Fig.  5 : Illustration from  [31] , demonstrating how graph models can effectively fuse modalities, even when some data is missing. across various modalities  [31] . Figure  5  illustrates the construction of hypernodes in  [31] .\n\nGraphical fusion methods are increasingly used to combine data from multiple modalities for various applications, such as Alzheimer's disease (AD) diagnosis and target tracking  [19, 160] . For example, in AD diagnosis, heterogeneous graph-based models integrate neuroimaging modalities like MRI and PET, capturing complex brain network structures to improve prediction accuracy  [161] . In recommendation systems, heterogeneous graphs enable the effective integration of text, image, and social media data, enhancing the quality of recommendations by capturing multimodal relationships  [216] . However, traditional linear combination approaches for multimodal fusion face limitations in capturing complementary information and are often sensitive to modality weights  [186] .\n\nTo address these issues, researchers have developed nonlinear graph fusion techniques that efficiently exploit multimodal complementarity  [186, 187] . These techniques, such as early fusion operators in heterogeneous graphs, outperform linear approaches by capturing intermodal interactions and have demonstrated improvements in one-class learning and multimodal classification tasks  [63] . For instance, nonlinear fusion methods have shown enhanced classification accuracy for AD and its prodromal stage, mild cognitive impairment (MCI)  [187] .\n\nRecent advancements include adversarial representation learning and graph fusion networks, which aim to learn modality-invariant embedding spaces and explore multi-stage interactions between modalities  [125] . These approaches have demonstrated state-of-the-art performance in multimodal fusion tasks and provide improved visualization of fusion results  [19, 125] .\n\nIn summary, graph-based methods provide a powerful framework for representing diverse data types and capturing complex, high-order interactions across modalities, making them highly effective for applications in medical diagnosis, social recommendation, and sentiment analysis. With ongoing advancements, graph-based methods hold great promise for handling incomplete, heterogeneous data and driving innovation in AI-powered multimodal applications. However, this flexibility also presents significant challenges. The sparsity and dynamic nature of graph structures complicate optimization. Unlike matrices or vectors, graphs have irregular unstructured connections, leading to high computational complexity and memory constraints. These issues persist even with advanced hardware platforms. Additionally, graph neural networks (GNNs) are particularly sensitive to hyperparameters. Choices related to network architecture, graph sampling, and loss function optimization directly impact performance, increasing the difficulty of GNN design and practical deployment.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Generative Methods",
      "text": "Generative methods have shown remarkable promise in learning cross-modal relationships by synthesizing and aligning high-dimensional data from different modalities. Generative adversarial networks (GANs) remain a foundational model in this domain  [175, 172, 174, 176] , offering effective solutions for complex mappings between modalities. For example, DMF-GAN integrates multihead attention and recurrent semantic fusion networks to achieve fine-grained text-to-image synthesis, significantly improving semantic alignment between modalities  [221] . Similarly, GAN-based frameworks have been used for multimodal MRI synthesis, where a single generator learns unified mappings across image modalities  [41] .\n\nVariational autoencoders (VAEs) also play a key role in multimodal alignment. By projecting data into shared latent spaces, VAEs enable the fusion of semantic information across modalities. This technique has proven effective in compositional tasks like image-text representation learning  [213] , and cross-modal quantization using VAEs has further demonstrated success in aligning text and image representations  [86] .\n\nA significant recent development in generative multimodal modeling is the adoption of diffusion models. These models offer a robust alternative to GANs and VAEs, especially in terms of stability, mode diversity, and representation fidelity  [26] . Diffusion-driven fusion techniques have enabled tasks such as face image generation from both visual prompts and text, showing strong cross-modal alignment by integrating latent representations from both GANs and diffusion models  [77] . Furthermore, semi-supervised methods like diffusion transport alignment (DTA) leverage diffusion processes for manifold alignment using minimal supervision, effectively capturing geometric similarities across modalities  [51] .\n\nMore broadly, diffusion models have been successfully applied in multimodal generative frameworks like Stable Diffusion and DALL-E, enabling synthesis tasks such as image-to-audio and text-to-video generation by iteratively denoising representations conditioned on multimodal inputs  [21, 26] . These approaches not only enhance the quality and coherence of generated outputs but also support flexible alignment by embedding conditional semantics at multiple stages of the generation process.\n\nIn summary, the evolution from GANs and VAEs to diffusion models marks a paradigm shift in generative multimodal fusion and alignment, offering better performance, interpretability, and multimodal coherence.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Contrastive Methods",
      "text": "Contrastive learning has become a cornerstone in multimodal alignment and fusion due to its ability to bring semantically related modalities closer in a shared embedding space. One of the most influential contrastive architectures is CLIP  [148] , which aligns text and image embeddings through large-scale pretraining on paired image-text data. CLIP and its variants have set new benchmarks in vision-language tasks, and inspired a wide range of fusion strategies.\n\nAt its core, CLIP learns aligned representations by training two encoders-one for images and one for text-so that the embeddings of matching image-text pairs are pulled closer together, while those of mismatched pairs are pushed apart. This is achieved by contrasting each sample against a batch of negatives, encouraging the model to capture semantic correspondences without requiring explicit annotations. The result is a powerful zero-shot transfer capability, where the model can generalize to unseen categories simply by encoding their textual descriptions  [148] .\n\nThe original CLIP model aligns global representations of images and text using a simple contrastive loss, achieving strong performance on zero-shot classification and retrieval tasks  [148] . Recent advances have extended CLIP's architecture to improve representation granularity and domain adaptability. For instance, HiCLIP enhances CLIP by incorporating hierarchy-aware attention in both visual and textual branches to better model fine-grained semantic relationships  [57] . Similarly, Set-CLIP reformulates alignment as a manifold matching problem and introduces semantic density loss to improve contrastive alignment even in low-alignment settings without paired data  [165] .\n\nOther works, such as ComKD-CLIP, use contrastive distillation to transfer alignment knowledge from large CLIP models into smaller networks using image-text fusion attention mechanisms  [36] . SyCoCa further augments contrastive captioners by introducing bidirectional attention flows and text-guided masked image modeling to unify global and local cross-modal alignments  [122] . Furthermore, models like Domain-Aligned CLIP explore few-shot adaptation through intra-and inter-modal contrastive learning without full finetuning  [59] .\n\nThe influence of CLIP-based models extends beyond standard vision-language domains. Applications such as image-guided editing  [212]  and 3D representation alignment  [88]  demonstrate the adaptability of contrastive alignment for diverse multimodal scenarios.\n\nTogether, these advances highlight how CLIP and its extensions form a central framework for contrastivebased multimodal alignment, driving both theoretical insight and practical performance gains.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Attention-Based Methods",
      "text": "Before the widespread adoption of attention mechanisms, earlier methods such as OSCAR  [99] , UNITER  [35] , VILA  [106] , and VinVL  [244]  relied on object detectors to extract modality features, followed by relatively shallow fusion strategies. These pipelines lacked dynamic alignment and adaptive weighting capabilities, which are now addressed by attention-based models. Later models such as CLIP  [148]  significantly advanced image-text representation learning through contrastive pretraining.\n\nHowever, the cross-modal interaction in CLIP was limited to a dot product between global embeddings, lacking fine-grained alignment or deep fusion at the token level  [79] . This shallow attention interaction hindered the model's ability to fully capture complex semantic relationships across modalities, motivating the development of more integrated fusion mechanisms.\n\nTo address this limitation, methods focusing on deeper inter-modal interactions were developed, often employing Transformer encoders or other complex architectures to achieve higher-level modality integration  [15] . The introduction of the Vision Transformer (ViT) marked a significant shift in multimodal learning.\n\nViLT  [79]  demonstrates the feasibility of performing multimodal tasks without convolutional networks or region supervision, using Transformers exclusively for feature extraction and processing. However, the simplistic structure of ViLT led to performance issues, particularly when compared to methods that emphasized deeper inter-modal interactions and fusion  [97, 15, 222] . ViLT lagged behind these methods in many tasks, possi-bly due to dataset bias or the inherent need for stronger visual capabilities  [97] . Generally, visual models need to be larger than text models to achieve better results, and the performance degradation was not primarily caused by the lightweight visual embedding strategy.\n\nSubsequent works, such as ALBEF  [97] , introduced more sophisticated model designs. ALBEF emphasized aligning image and text representations before their fusion using a contrastive loss. By employing momentum distillation, it generated pseudo-labels to mitigate challenges posed by noisy datasets. Following this, BLIP  [96]  adopted a bootstrapping mechanism, using initially generated captions from the model to filter out dataset noise, thereby improving the quality of subsequent training.\n\nAttention-based mechanisms gained prominence with the introduction of the Transformer architecture  [194]  and following works  [44, 260, 60, 241] , where the attention function takes queries (Q), keys (K), and values (V) and computes the relevance of each key to a given query. The scaled dot-product attention is given by:\n\nThis operation dynamically weighs each input feature according to its contextual importance, enabling the model to capture long-range dependencies across sequences or modalities. The use of attention mechanisms enables decoders to focus selectively on specific subcomponents of the source input. This contrasts with traditional encoderdecoder models that treat all source features as a single representation. Attention modules guide decoders to highlight task-relevant regions-such as specific image patches, words in a sentence, or audio frames-during output generation. For instance, in image captioning, attention allows decoders to attend to relevant visual regions when generating each word, instead of encoding the entire image as a static vector  [75] .\n\nConsidering the inherent connection between attention score and similarity score, attention mechanisms are widely applied for alignment, i.e., learning correspondences between semantically similar elements across modalities. For example, the Att-Sinkhorn method uses the Sinkhorn distance in conjunction with attention to model cross-modal optimal transport, aligning features from different distributions  [120] . The AbFTNet model emphasizes \"alignment-before-fusion\" by first synchronizing modality-specific features via a Transformerbased mechanism and then integrating them through a cross-modal aggregation module  [136] . In the domain of knowledge representation, DSEA applies a dynamic self-attention network to evaluate the importance of   [116] , UNITER  [35] ; 2020: OSCAR  [99] ; 2021: ViLT  [79] , ALBEF  [97] , Perceiver  [70] , AudioCLIP  [62] ; 2022: Unified-IO  [117] , BEIT-3  [204] , BLIP  [96] , VLMo  [17] , CoCa  [227] , MMIM  [64] , SimVLM  [209] ; 2023: ImageBind  [58] , VAST  [34] , ONE-PEACE  [202] , AudioLDM  [112] , InstructBLIP  [40] , BLIP-2  [95] , InstructBLIP2  [30] ; 2024: TextBind  [91] , CAFuser  [23] , LongMIL  [89] , Qwen-Audio  [38] , Yi-VL  [226] , Moshi  [52] ; 2025: LLaVA-Video  [251] , Qwen-Omini  [217] . structural and attribute information, improving entity alignment across multimodal knowledge graphs  [145] .\n\nAttention-based fusion integrates multimodal information by learning how much to attend to each modality. This selective integration helps manage noise, modality imbalance, and complementary cues. For instance, BMAN applies multi-head attention to align and fuse audio-text features with learnable weights, improving sentiment prediction across unaligned datasets  [245] . ProVLA employs a cross-attention fusion encoder that progressively aligns vision and language representations for compositional image retrieval  [68] .\n\nAttention-based fusion is particularly effective in multimodal tasks because it supports flexible integration and handles modality-specific uncertainties  [177, 200] . However, this flexibility comes at the cost of increased computational complexity and often demands large-scale annotated datasets.\n\nRecent advances such as ProVLA  [68]  and AbFTNet  [136]  take Transformer-based fusion further by introducing progressive alignment and structured attention mechanisms prior to fusion. ProVLA employs a two-stage alignment-fusion paradigm, leveraging cross-attention for robust semantic integration and using momentumbased hard negative mining to enhance alignment ro-bustness. Similarly, AbFTNet introduces a CAP (Crossmodal Aggregation Promoting) module, aligning unimodal features through self-attention before cross-modal integration, thus addressing modality-specific information disparities.\n\nFusion mechanisms like TokenFusion  [129]  also explore token-level replacement and residual alignment to balance modality contributions and avoid attention dilution. These designs allow Transformers to retain unimodal strengths while gaining inter-modal awareness through informed token substitution and dynamic fusion.\n\nCoCa  [227]  combined contrastive loss with captioning loss, achieving remarkable performance. In particular, CoCa excelled not only in multimodal tasks, but also performed well on single-modal tasks such as Im-ageNet classification. BEIT-3  [204]  further advanced multimodal learning with the implementation of Multiway Transformers, enabling the simultaneous processing of images, text, and image-text pairs. By applying masked data modeling to these inputs, BEIT-3 achieved state-of-the-art performance across various visual and vision-language tasks. Figure  6  illustrates the relationships among major works related to attention mechanisms and transform-ers and Table  3  provides a summary of representative models.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Llm-Based Methods",
      "text": "As mentioned in Section 4.2.6 and Figure  6 , the trend has shifted to LLM-based models from 2022. Figure  7  illustrates a common scenario of LLM-based method. After the encoder extracts features from each modality, a connector maps these features into the text space, where they are processed together by the LLM. Previously, this connector was often a simple MLP, but it can now be a more complex attention mechanism. Recently, researchers have proposed various architectures and techniques aimed at enhancing cross-modal capabilities. They embed adapters into frozen LLMs to facilitate interactions between modalities  [118] . Figure  8  shows the typical structure of this approach. The key difference from previous methods is that adapters are embedded directly into the LLMs, allowing for end-toend training with alignment included. For example, the Qwen-VL series models  [14]  advanced cross-modal learning through the design of visual receptors, input-output interfaces, and a multi-stage training pipeline, achieving notable performance in image and text understanding, localization, and text reading. In video understanding, the ViLA network  [205]  introduced a learnable textguided Frame-Prompter and a cross-modal distillation module (QFormer-Distiller) optimized for key frame selection, improving both accuracy and efficiency in video-language alignment. Additionally, CogVLM  [203]  incorporated visual expertise into pretrained language models using Transformers. In emotion recognition tasks, COLD Fusion added an uncertainty-aware component for multimodal emotion recognition  [181] .\n\nVarious pre-training strategies have been developed to facilitate multimodal fusion. For example, BLIP-2  [95]  introduced a bootstrapping approach that used frozen image encoders and large language models for vision-language pre-training, reducing the number of parameters while enhancing zero-shot learning performance. Similarly, the VAST model  [34]  explored a comprehensive multimodal setup involving vision, audio, subtitles, and text, constructing a large-scale dataset and training a foundational model capable of perceiving and processing all these modalities. Furthermore, the ONE-PEACE model  [202]  employed a modular adapter design and shared self-attention layers to provide a flexible and scalable architecture that could be extended to more modalities. The research by Zhang et al.  [239]  used Transformers for end-to-end anatomical and functional image fusion, leveraging self-attention to incorporate global contextual information. Despite these advances, the field still faces several challenges. One of the main challenges is data bias, where inherent biases in training datasets limit model performance. Another concern is maintaining consistency across modalities to ensure coherent information integration without loss or inconsistency. Additionally, as models grow in scale, there is an increasing demand for computational resources, necessitating more efficient algorithms and hardware support. Table  4  summarizes some state-of-the-art (SOTA) or popular LLM-based models.\n\nIn conclusion, multimodal fusion remains a dynamic and evolving area of research, driven by advances in attention-based mechanisms and model architectures. Although significant progress has been made in developing models that effectively integrate information from multiple modalities, ongoing challenges such as data bias, modality consistency, and computational demands persist. Continued exploration of new theoretical frameworks and technical solutions is necessary to achieve more intelligent and adaptable multimodal systems, advancing artificial intelligence technologies, and providing powerful tools for practical applications.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Challenges In Multimodal Alignment And Fusion",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Multimodal Misalignment And Modality Gap",
      "text": "Multimodal misalignment and modality gap are two critical challenges in multimodal representation learning that significantly affect model performance. Misalignment refers to the mismatch between different modalities, such as images and their corresponding textual descriptions, which can arise due to noisy or incorrect annotations  [185] . For instance, Ma et al.  [121]  identified modality misalignment as a significant barrier to transferring knowledge across different modalities, emphasizing that pre-trained models frequently struggle with knowledge transfer when there is a substantial semantic gap between modalities.\n\nThe modality gap, on the other hand, describes the disparity in embedding distributions of different modalities within a shared space, leading to suboptimal crossmodal interactions  [105] . The modality gap in multimodal contrastive representation learning arises due to a combination of factors. First, deep neural networks inherently create a \"cone effect\", where embeddings from a single modality are restricted to a narrow region of the embedding space. This geometric bias is amplified by nonlinear activation functions and network depth. Second, different random initializations for separate encoders in multimodal models result in distinct cones",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Model Year Modality Training Modules",
      "text": "ViLBERT  [116]  2019 Vision, Text\n\nDual-stream co-attentional transformer, pretrained on Conceptual Captions, for taskagnostic visiolinguistic representation in VQA, VCR, referring expressions, and image retrieval.\n\nUNITER  [35]  2019 Vision, Text MLM, MRM, ITM, and WRA via Optimal Transport for fine-grained alignment.\n\nOscar  [99]  2020 Vision, Text Object-Semantics Aligned Pre-training using detected object tags as anchor points.\n\nViLT  [79]  2021 Multi-task learning with unobserved multimodal context for sentiment analysis.\n\nImageBind  [58]  2023 Vision, Text, Audio, Depth, Thermal, IMU Zero-shot alignment across 6 modalities via image-centric binding. VAST  [34]  2023 Vision, Text OM-VCC; OM-VCM; OM-VCG ONE-PEACE  [202]  2023 Vision, Audio, Text Masked Contrastive Learning; supports multimodal fusion through self-attention layers. Coupled Mamba  [98]  2024 Vision, Text, Audio Enhanced multimodal fusion with State Space Models (SSMs) for non-LLM architectures. CAFuser  [23]  2024 Vision, Text Image-Text Contrastive Loss LongMIL  [89]  2024 Medical Images, Text\n\nLocal-Global Hybrid Transformer architecture for long-context multiple instance learning; Self-supervised contrastive learning for medical image-text alignment.\n\nin the shared embedding space, causing a separation between modalities even at initialization. Finally, during training, contrastive learning preserves this gap. The contrastive loss, especially at low temperatures, maintains a repulsive structure in the optimization landscape, preventing the gap from closing. Together, these factors explain the persistent separation between modalities observed in multimodal models like CLIP  [105] .\n\nMany approaches seek to mitigate the modality gap through various architectural and training innovations. For instance, noise-injected embeddings, such as those used in CapDec, improve generalization by perturbing CLIP embeddings to reduce overfitting to specific modality characteristics and enhance alignment in low-resource settings  [9] . Meanwhile, VT-CLIP improves modality alignment by incorporating visual-guided text generation to highlight image regions that correspond to key linguistic cues  [147] . Finite discrete tokens (FDT) aim to resolve the granularity gap between visual patches and textual tokens by embedding both into a unified semantic space  [37] . Modality knowledge alignment (MoNA) introduces a meta-learning paradigm to learn target data transformations that reduce modality knowledge discrepancies prior to transfer  [121] . Recently, Tong et al.  [185]  investigate the gap between the visual embedding space of CLIP and vision-only self-supervised learning, proposing a mixture of features (MoF) approach to enhance visual grounding capabilities.\n\nTo advance this field, future research should focus on developing more sophisticated embedding space modeling techniques, dynamically adjusting modality gaps during training, and improving data quality through better annotation practices. Additionally, enhancing the compositional understanding of VLMs by incorporating syntactic structure and word order sensitivity-such as generating targeted negative captions through linguistic element swapping-could further improve model capabilities  [232] . These directions aim to bridge existing gaps in multimodal learning and push models toward a more human-like comprehension of multimodal input.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Computational Efficiency Challenge",
      "text": "Early multimodal models faced significant computational demands due to their reliance on object detectors, particularly during inference. The introduction of vision transformers (ViTs) enabled patch-based visual representations instead of bounding boxes, reducing computational complexity. However, simply tokenizing textual and visual features remains insufficient for effective multimodal processing. Recent work has proposed several efficient fusion mechanisms to mitigate this challenge.\n\nTokenFusion, for instance, dynamically replaces less informative tokens with fused inter-modal features to reduce token redundancy in transformer architectures  [129] . Attention bottlenecks allow selective modality interaction through shared tokens, enabling low-rank communication between modalities with minimal overhead  [132] . Prompt-based multimodal fusion (PMF) introduces deeplayer prompts that interact across pretrained unimodal Transformers, achieving comparable performance to full fine-tuning  [101] . Other notable methods include sparse fusion transformers (SFT), which sparsify unimodal tokens prior to fusion  [46] , and dynamic multimodal fusion (DynMM), which adaptively determines forward com- putation paths using data-dependent gating  [219] . Lowrank tensor fusion approaches have also been explored, leveraging compact representations to avoid exponential parameter growth  [115] . Recently, GeminiFusion proposed a pixel-wise fusion method with linear complexity by combining intra-and inter-modal attention, controlled by layer-adaptive noise modulation  [73] .\n\nDespite these advances, fusion remains the computational bottleneck in large-scale multimodal models. Future research should focus on developing adaptive, scalable, and resource-efficient fusion architectures to meet the growing demand of real-world multimodal tasks.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Data Quality And Availability",
      "text": "One of the primary obstacles is the scarcity of largescale, high-quality multimodal datasets, which are essential for training robust multimodal language models (MLLMs)  [142, 22] . Complex and unbiased data that accurately reflect the richness of reality are necessary to train these models effectively. This challenge is particularly pronounced in specialized fields, such as nuclear medicine, where access to sufficient clinical data for model refinement is limited  [22] . Furthermore, the need for task-specific and domain-specific datasets adds to the complexity of data collection and integration processes  [142] .\n\nLarge-scale multimodal datasets obtained from the Internet, such as image-caption pairs, often contain mismatches or irrelevant content between images and their corresponding texts. This issue arises mainly because these image-text pairs are optimized for search engines rather than for precise multimodal alignment. Consequently, models trained on such noisy data may struggle to generalize effectively. Tong et al.  [185]  identifies specific instances where advanced systems like GPT-4V struggle with VQA due to inaccurate visual grounding. To address this problem, several approaches have been proposed to improve data quality.\n\nKim et al.  [78]  introduced a novel methodology called hyperbolic entailment filtering (HYPE) , which goes beyond traditional CLIP-based filtering by incorporating both alignment and specificity metrics. HYPE leverages hyperbolic embeddings and entailment cones to filter out samples with underspecified or meaningless semantics, ensuring better cross-modal alignment and modalitywise meaningfulness.\n\nOther notable efforts include Nguyen et al.  [134] , who tackled noise in web-scraped datasets by using synthetic captions generated through image captioning models. By integrating synthetic descriptions with the original captions, they achieved improvements in data utility across multiple benchmark tasks, demonstrating that improved caption quality can significantly benefit model performance. Similarly, CapsFusion  [229]  introduced a framework that leverages large language models to refine synthetic and natural captions in multimodal datasets, thus improving caption quality and sample efficiency for large-scale models. Furthermore, the LAION-5B dataset  [157]  provides a large collection of CLIP-filtered imagetext pairs, showing that combining high data volume with effective filtering can enhance the robustness and zero-shot capabilities of vision language models. Despite these improvements, challenges remain in scalable data filtering and maintaining diversity. For example, DataComp  [56]  has shown that even with effective filtering, achieving high-quality and diverse representation in large multimodal datasets is complex. It requires ongoing innovation in data pruning and quality assessment to ensure that models trained on these datasets generalize effectively across domains. HYPE's ability to consider both cross-modal alignment and intramodal specificity offers a promising direction for addressing these limitations, especially in large-scale settings.\n\nIn summary, while synthetic captioning and largescale filtering methods have improved the quality of multimodal datasets, further advances in scalable filtering techniques and diversity retention are needed to fully address the challenges associated with web-scraped multimodal datasets.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Scale Of Training Datasets Challenge",
      "text": "Another significant challenge in multimodal learning is acquiring sufficiently large and high-quality datasets for model training, particularly for combining vision and language tasks. There is a pressing need for extensive and reliable datasets that can be used to train models effectively across a variety of tasks. For instance, the introduction of the LAION-5B dataset, comprising billions of CLIP-filtered image-text pairs, has provided a scalable, open-source dataset that supports training and fine-tuning large-scale vision-language models, helping democratize access to high-quality data  [157] . Similarly, the WIT dataset enables multimodal, multilingual learning by offering a curated, entity-rich dataset sourced from Wikipedia, featuring a high degree of concept and language diversity, which has proven beneficial for downstream retrieval tasks  [166] .\n\nAlthough these datasets represent substantial progress, scalability and data quality remain challenging. For example,  [199]  proposes compressing vision-language pretraining (VLP) datasets to retain essential information while reducing redundancy and misalignment, resulting in a smaller but higher-quality training set. Additionally, scaling techniques like sparse mixture of experts (MoE)  [159]  aim to improve the efficiency of large models by training specialized sub-models within a unified framework, balancing compute costs and performance. While these innovations are steps toward addressing data scale and quality challenges, efficient access to diverse and large datasets for multimodal learning remains a difficulty for the research community.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Ethical Bias",
      "text": "Ethical bias represents a critical and under-addressed challenge in multimodal alignment and fusion. These systems often inherit and even amplify biases present in individual modalities, such as language, vision, or speech. Studies have demonstrated that fusion processes may introduce new forms of unfairness not present in unimodal inputs, due to disparate representations and unequal weighting of modalities  [220, 152] . For example,  [220]  revealed that different modalities in personality assessment contribute uniquely to bias, and simple fusion can worsen demographic disparities. Similarly,  [250]  developed a theoretical framework showing that multimodal networks often suffer from unimodal bias, where a dominant modality suppresses others, leading to skewed outputs. Ethical alignment frameworks, such as RoBERTa-based classifiers trained on human ethical feedback, have been proposed to better detect and mitigate bias in multimodal responses  [152] . Moreover, novel alignment datasets specifically designed to reduce gender bias in language models are also being explored to guide fairer training processes  [246] . These findings underscore the need for standardized bias auditing protocols and ethically-aware training pipelines when developing multimodal systems.\n\n6 Discussion and Future Directions Multimodal alignment and fusion techniques have made significant strides in enabling complex reasoning capabilities across tasks such as visual question answering, spatial localization, and semantic composition. Recent developments demonstrate that the effectiveness of these techniques is highly task-dependent, with distinct strategies outperforming in compositional versus spatial reasoning scenarios.\n\nFor spatial reasoning, recent research underscores the continued value of incorporating explicit spatial alignment even in deep learning architectures. Wang et al. demonstrated that spatially aligning medical images before feeding them into deep fusion networks leads to notable improvements in diagnostic accuracy. Their addition of a spatial transformer network (STN) module further enhanced this effect, providing adaptive alignment during training  [206] . Similarly, the MulFS-CAP framework bypasses traditional registration by jointly learning alignment and fusion in a unified network, which proves especially effective for unregistered infrared-visible image pairs  [90] . These results suggest that both explicit and implicit spatial alignment strategies are essential for spatially sensitive fusion tasks.\n\nIn contrast, compositional reasoning benefits more from methods emphasizing feature-level integration and cross-modality contextual alignment. ST-Align introduces a foundation model for spatial transcriptomics that aligns image and gene data across spatial scales, enabling nuanced cross-level semantic understanding  [110] . Meanwhile, Set-CLIP addresses the low-alignment data challenge by learning modality alignment through distribution-level semantic constraints, demonstrating that strong performance can be achieved even in semisupervised settings  [165] . This highlights the importance of representation space regularization and contrastive learning in achieving effective compositional fusion.\n\nFrom these studies, several generalizable lessons emerge:\n\n-Implicit fusion architectures with built-in alignment (e.g., STN, shared encoders) are increasingly favored for practical deployment due to reduced preprocessing demands. -Cross-level fusion frameworks, such as those combining local and global spatial cues, significantly enhance performance in hierarchical reasoning tasks. -Modality-specific feature preservation, via dictionaries or attention-based fusion, allows models to better retain complementary information during integration. -Distribution-aware alignment strategies improve robustness in data-scarce or weakly aligned settings. Future directions include developing unified multimodal architectures that balance task-specific performance with generalization. More interpretable fusion methods are also critical, especially in clinical or scientific domains. Finally, the field would benefit from standardized multimodal benchmarks that isolate spatial and compositional reasoning tasks, enabling more consistent evaluation of fusion techniques.\n\nOverall, effective multimodal fusion requires a deliberate choice of alignment and integration strategies, tailored to the reasoning demands of the target application. Spatial tasks benefit from precise alignment techniques, while compositional reasoning favors context-aware and semantically structured fusion methods.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Conclusion",
      "text": "Multimodal alignment and fusion are fundamental to advancing artificial intelligence systems capable of understanding and interacting with complex real-world data. Our survey has provided a comprehensive review of over 200 studies, categorizing existing techniques from both structural and methodological perspectives. Despite notable progress-particularly in contrastive learning, attention-based models, and LLM-driven architectures-achieving robust and scalable integration across modalities remains a significant challenge.\n\nCurrent approaches continue to grapple with critical issues, including modality misalignment, inconsistent data quality, and high computational overhead. Additionally, the modality gap and the scarcity of large-scale, high-quality datasets hinder the full realization of effective multimodal learning. Although many techniques have advanced the field, limitations persist in aligning heterogeneous signals and managing data noise.\n\nTo bridge these gaps, future research should prioritize the development of adaptive, noise-resilient frameworks, capable of dynamically adjusting to diverse modalities while maintaining interpretability. Promising directions include efficient token-level fusion mechanisms, cross-modal graph reasoning, and alignment-aware training objectives that explicitly mitigate the modality gap. Furthermore, continued innovation in dataset construction, annotation quality, and filtering strategies-such as hyperbolic entailment filtering and synthetic captioning-will be essential for improving model robustness.\n\nBy addressing these challenges, the next generation of multimodal systems can become more versatile, efficient, and generalizable, paving the way for broader deployment in domains such as healthcare, autonomous systems, and human-computer interaction.",
      "page_start": 20,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: provides an overview of three",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of multimodal model architectures: (a) Two-Tower [148,72,39,193,218,170,111,28,54,211,189,",
      "page": 3
    },
    {
      "caption": "Figure 2: Three types of fusion with structural perspective:",
      "page": 7
    },
    {
      "caption": "Figure 2: illustrates",
      "page": 7
    },
    {
      "caption": "Figure 3: visualizes the CCA method.",
      "page": 9
    },
    {
      "caption": "Figure 3: Canonical Correlation Analysis (CCA), a classic",
      "page": 10
    },
    {
      "caption": "Figure 4: In graph-based alignment, different data modali-",
      "page": 10
    },
    {
      "caption": "Figure 4: illustrates how",
      "page": 10
    },
    {
      "caption": "Figure 5: Illustration from [31], demonstrating how graph",
      "page": 11
    },
    {
      "caption": "Figure 5: illustrates the",
      "page": 11
    },
    {
      "caption": "Figure 6: Timeline of multimodal attention-based models from 2019 to 2025. From 2019 to 2022, researches heavily",
      "page": 14
    },
    {
      "caption": "Figure 6: illustrates the relationships among major",
      "page": 14
    },
    {
      "caption": "Figure 6: , the trend",
      "page": 15
    },
    {
      "caption": "Figure 7: illustrates a common scenario of LLM-based method.",
      "page": 15
    },
    {
      "caption": "Figure 8: shows the typical structure of this approach. The key",
      "page": 15
    },
    {
      "caption": "Figure 7: This pipeline demonstrates multimodal fusion using a large language model (LLM). Text inputs are embedded",
      "page": 17
    },
    {
      "caption": "Figure 8: Adapters embedded in LLM. Each modality (text,",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Overview of different datasets’ characteristics.",
      "data": [
        {
          "SBU Captions [140]": "MS-COCO [108]",
          "1M": "1.64M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Created by having crowd workers provide\ncaptions\nfor\nimages."
        },
        {
          "SBU Captions [140]": "YFCC-100M [183]",
          "1M": "100M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Contains 100 million image-text pairs, unclear average\nmatch degree between text and image."
        },
        {
          "SBU Captions [140]": "Flickr30k [144]",
          "1M": "30k",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Created by having crowd workers provide\ncaptions\nfor\napproximately 30,000 images."
        },
        {
          "SBU Captions [140]": "Visual Genome [84]",
          "1M": "5.4M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Includes structured image concepts such as region descrip-\ntions, object instances, relationships, etc."
        },
        {
          "SBU Captions [140]": "RedCaps [43]",
          "1M": "12.01M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Distributed across 350 subreddits with a long-tail distribu-\ntion. Contains the distribution of visual concepts encoun-\ntered by humans in everyday life without predefined object\nclass ontologies. Higher\nlinguistic diversity compared to\nother datasets like CC-3M and SBU."
        },
        {
          "SBU Captions [140]": "CC-12M [27]",
          "1M": "12.4M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Lower linguistic diversity compared to RedCaps."
        },
        {
          "SBU Captions [140]": "WIT [166]",
          "1M": "37.6M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Subset of multilingual Wikipedia image-text dataset."
        },
        {
          "SBU Captions [140]": "TaiSu [114]",
          "1M": "166M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "TaiSu is a large-scale, high-quality Chinese cross-modal\ndataset containing 166 million images and 219 million Chi-\nnese captions, designed for vision-language pre-training."
        },
        {
          "SBU Captions [140]": "COYO-700M [25]",
          "1M": "700M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Collection of 700 million informative image-alt text pairs\nfrom HTML documents."
        },
        {
          "SBU Captions [140]": "LAION-5B [157]",
          "1M": "5.85B",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "LAION-5B is a publicly available,\nlarge-scale dataset con-\ntaining over 5.8 billion image-text pairs filtered by CLIP,\ndesigned for\ntraining the next generation of\nimage-text\nmodels."
        },
        {
          "SBU Captions [140]": "DATACOMP-1B [56]",
          "1M": "1.4B",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "Collected from Common Crawl using simple filtering. Mod-\nels trained on this dataset achieve higher accuracy using\nfewer MACs compared to previous results."
        },
        {
          "SBU Captions [140]": "RS5M [254]",
          "1M": "5M",
          "Image, Text": "Image, Text",
          "More unique words than CC-3M but fewer captions.": "The RS5M dataset is a large-scale remote sensing image-\ntext paired dataset, containing 5 million remote sensing\nimages alongside corresponding English descriptions."
        },
        {
          "SBU Captions [140]": "DEAP [81]",
          "1M": "2122 samples",
          "Image, Text": "EEG, ECG, GSR",
          "More unique words than CC-3M but fewer captions.": "Contains 40 one-minute music video excerpts with con-\ntinuous affect\nratings\nfrom 32 participants\nfor\nemotion\nrecognition."
        },
        {
          "SBU Captions [140]": "PAMAP2 [151]",
          "1M": "3850505\nsam-\nples",
          "Image, Text": "IMU, Heart Rate",
          "More unique words than CC-3M but fewer captions.": "Multi-sensor time-series data for 18 activities with high-\nresolution physiological and motion data collected from 9\nsubjects."
        },
        {
          "SBU Captions [140]": "MHEALTH [16]",
          "1M": "120 samples",
          "Image, Text": "ECG, EMG, Motion",
          "More unique words than CC-3M but fewer captions.": "Biometric\nsensor\nstreams\ncapturing high-intensity func-\ntional movements for medical activity monitoring from 10\nsubjects."
        },
        {
          "SBU Captions [140]": "CH-SIMS [230]",
          "1M": "2,281 videos",
          "Image, Text": "Text, Audio, Video",
          "More unique words than CC-3M but fewer captions.": "Tri-modal dataset featuring multi-modal annotations across\ntext, audio, and video modalities for sentiment analysis,\nwith higher\nlinguistic diversity compared to audio-only\napproaches."
        },
        {
          "SBU Captions [140]": "MuSe-CaR [168]",
          "1M": "40 hours video",
          "Image, Text": "Text, Audio, Video",
          "More unique words than CC-3M but fewer captions.": "Multimodal dataset for sentiment analysis in car reviews\ncontaining 40 hours of user-generated video material with\nmore than 350 reviews, combining spoken language, vocal\nqualities, and visual cues for comprehensive sentiment un-\nderstanding."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Summary of models from structural perspective.",
      "data": [
        {
          "TFN [233]": "MFAS [188]",
          "2017": "2018",
          "Output-level": "Hybrid",
          "Text,\nAudio,\nImage": "Text,\nAudio,\nImage",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Combines feature- and output-level fusion via factorized\nattention and modality gating. Dynamically weights\nmodalities, outperforming static fusion strategies."
        },
        {
          "TFN [233]": "ViLBERT [116]",
          "2017": "2019",
          "Output-level": "Feature-level",
          "Text,\nAudio,\nImage": "Text, Image",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Employs a two-stream architecture with co-attentional\ntransformer\nlayers\nfor\nseparate vision and language\nprocessing. Innovates by pretraining on masked multi-\nmodal modeling and alignment prediction,\nenabling\ndeep cross-modal\ninteraction while preserving modality-\nspecific processing depths."
        },
        {
          "TFN [233]": "UNITER [35]",
          "2017": "2020",
          "Output-level": "Feature-level",
          "Text,\nAudio,\nImage": "Text, Image",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Early unified Transformer for vision-language pretrain-\ning. Pioneered MLM, MRM, and ITM tasks, setting\nthe standard for subsequent VLP models."
        },
        {
          "TFN [233]": "Perceiver [70]",
          "2017": "2021",
          "Output-level": "Data-level",
          "Text,\nAudio,\nImage": "Images,\nPoint\nClouds, Audio,\nVideo",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Uses asymmetric\ncross-attention to fuse\nraw inputs\ninto a latent bottleneck. Innovates by enabling a single\nTransformer to handle arbitrary modalities, avoiding\nmodality-specific architectures."
        },
        {
          "TFN [233]": "VX2TEXT [109]",
          "2017": "2021",
          "Output-level": "Data-level",
          "Text,\nAudio,\nImage": "Video,\nAudio,\nText",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Employs learnable tokenizers to convert all modalities\ninto embeddings for unified Transformer encoding. First\nto unify diverse modalities end-to-end for generation,\neliminating preprocessing disparities."
        },
        {
          "TFN [233]": "CLIP [148]",
          "2017": "2021",
          "Output-level": "Feature-level",
          "Text,\nAudio,\nImage": "Text, Image",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Aligns\nimage\nand text\nvia\ncontrastive\nlearning\non\nlarge-scale data.\nInnovates with scalable pretraining\nand strong zero-shot transfer, surpassing fine-tuning-\ndependent models."
        },
        {
          "TFN [233]": "BLIP [96]",
          "2017": "2022",
          "Output-level": "Feature-level",
          "Text,\nAudio,\nImage": "Text, Image",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Enhances CLIP with generative captioning for data\nrefinement. Jointly optimizes understanding and gen-\neration,\nimproving performance on both retrieval and\ncaptioning tasks."
        },
        {
          "TFN [233]": "FLAVA [162]",
          "2017": "2022",
          "Output-level": "Feature-level",
          "Text,\nAudio,\nImage": "Text, Image",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Uses three separate encoders with multimodal\nfusion\nand joint unimodal/multimodal pretraining. Unique in\nsupporting comprehensive unimodal and multimodal\ntasks within one model."
        },
        {
          "TFN [233]": "ImageBind [58]",
          "2017": "2023",
          "Output-level": "Feature-level",
          "Text,\nAudio,\nImage": "Text,\nImage,\nAudio, Depth,\nThermal, IMU",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Aligns\nsix modalities\nin joint\nembedding\nspace us-\ning image as binding medium via contrastive learning.\nAchieves ”emergent alignment” for unseen modality\npairs without direct training, enabling zero-shot cross-\nmodal retrieval and classification."
        },
        {
          "TFN [233]": "ProVLA [68]",
          "2017": "2023",
          "Output-level": "Output-level",
          "Text,\nAudio,\nImage": "Text, Image",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Uses two-stage Transformer with hard negative min-\ning for progressive fusion. Improves retrieval accuracy\nthrough iterative cross-modal refinement."
        },
        {
          "TFN [233]": "TextBind [91]",
          "2017": "2024",
          "Output-level": "Hybrid",
          "Text,\nAudio,\nImage": "Text, Image",
          "First to use tensor fusion for high-order modality inter-\nactions at decision level. Enables end-to-end learning\nwithout intermediate fusion, capturing complex multi-\nmodal dynamics.": "Combines feature-level\nfusion (Q-Former mapping) and\noutput-level\nfusion (LM to Stable Diffusion). Supports\ninterleaved multimodal\ninputs/outputs for comprehen-\nsive instruction following with understanding and gen-\neration capabilities."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Summary of attention-based multimodal models.",
      "data": [
        {
          "ViLBERT [116]": "UNITER [35]",
          "2019": "2019",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "MLM, MRM,\nITM,\nand WRA via Optimal\nTransport for fine-grained alignment."
        },
        {
          "ViLBERT [116]": "Oscar [99]",
          "2019": "2020",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Object-Semantics Aligned Pre-training using\ndetected object tags as anchor points."
        },
        {
          "ViLBERT [116]": "ViLT [79]",
          "2019": "2021",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Masked Language Modeling; Image-Text Match-\ning; Word-Patch Alignment."
        },
        {
          "ViLBERT [116]": "ALBEF [97]",
          "2019": "2021",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Image-Text\nContrastive\nLoss;\nImage-Text\nMatching\nLoss; Masked-Language-Modeling\nLoss."
        },
        {
          "ViLBERT [116]": "Perceiver [70]",
          "2019": "2021",
          "Vision, Text": "Arbitrary Modalities",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "General-purpose\narchitecture\nhandling\nse-\nquences of varying modalities."
        },
        {
          "ViLBERT [116]": "AudioCLIP [62]",
          "2019": "2021",
          "Vision, Text": "Audio, Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Contrastive\nlearning extended from CLIP to\naudio domain."
        },
        {
          "ViLBERT [116]": "Unified-IO [117]",
          "2019": "2022",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Object Segmentation; Visual Question Answer-\ning; Depth Estimation; Object Localization."
        },
        {
          "ViLBERT [116]": "BEIT-3 [204]",
          "2019": "2022",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Masked ”language” modeling for images, texts,\nand image-text pairs."
        },
        {
          "ViLBERT [116]": "BLIP [96]",
          "2019": "2022",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Image-Text\nContrastive\nLoss;\nImage-Text\nMatching Loss; Language Modeling Loss."
        },
        {
          "ViLBERT [116]": "VLMo [17]",
          "2019": "2022",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Image-Text Contrastive Learning; Masked Lan-\nguage Modeling; Image-Text Matching."
        },
        {
          "ViLBERT [116]": "CoCa [227]",
          "2019": "2022",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Captioning Loss; Contrastive Loss."
        },
        {
          "ViLBERT [116]": "MMIM [64]",
          "2019": "2022",
          "Vision, Text": "Audio,\nText,\nNon-\nVerbal Context",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Multi-task learning with unobserved multimodal\ncontext for sentiment analysis."
        },
        {
          "ViLBERT [116]": "ImageBind [58]",
          "2019": "2023",
          "Vision, Text": "Vision, Text, Audio,\nDepth, Thermal, IMU",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Zero-shot\nalignment\nacross\n6 modalities\nvia\nimage-centric binding."
        },
        {
          "ViLBERT [116]": "VAST [34]",
          "2019": "2023",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "OM-VCC; OM-VCM; OM-VCG"
        },
        {
          "ViLBERT [116]": "ONE-PEACE\n[202]",
          "2019": "2023",
          "Vision, Text": "Vision, Audio, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Masked Contrastive Learning; supports multi-\nmodal\nfusion through self-attention layers."
        },
        {
          "ViLBERT [116]": "Coupled Mamba\n[98]",
          "2019": "2024",
          "Vision, Text": "Vision, Text, Audio",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Enhanced multimodal\nfusion with State Space\nModels (SSMs) for non-LLM architectures."
        },
        {
          "ViLBERT [116]": "CAFuser [23]",
          "2019": "2024",
          "Vision, Text": "Vision, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Image-Text Contrastive Loss"
        },
        {
          "ViLBERT [116]": "LongMIL [89]",
          "2019": "2024",
          "Vision, Text": "Medical Images, Text",
          "Dual-stream co-attentional\ntransformer,\npre-\ntrained\non\nConceptual\nCaptions,\nfor\ntask-\nagnostic visiolinguistic representation in VQA,\nVCR, referring expressions, and image retrieval.": "Local-Global Hybrid Transformer architecture\nfor\nlong-context multiple\ninstance\nlearning;\nSelf-supervised contrastive\nlearning for medi-\ncal\nimage-text alignment."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 4: Summary of LLM-based multimodal models with diverse modalities.",
      "data": [
        {
          "MiniGPT-4 [259]": "Qwen-VL [14]",
          "2023": "2023",
          "Text, Image": "Text, Image",
          "Vicuna": "Qwen-7B",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Stage 1: Image caption generation; Stage 2: Multi-\ntask pretraining; Stage 3: Supervised finetuning"
        },
        {
          "MiniGPT-4 [259]": "BLIP-2 [95]",
          "2023": "2023",
          "Text, Image": "Text, Image",
          "Vicuna": "OPT, FlanT5",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Stage 1: Vision-Language Representation Learning;\nStage 2: Vision-to-Language Generation Learning"
        },
        {
          "MiniGPT-4 [259]": "LLaVA [113]",
          "2023": "2023",
          "Text, Image": "Text, Image",
          "Vicuna": "GPT-3, GPT-\n3.5, LLaMA",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Visual Instruction Tuning"
        },
        {
          "MiniGPT-4 [259]": "LaVIN [118]",
          "2023": "2023",
          "Text, Image": "Text, Image",
          "Vicuna": "LLaMA",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Fine-tuning with MoE adapter"
        },
        {
          "MiniGPT-4 [259]": "MiniGPT-v2 [32]",
          "2023": "2023",
          "Text, Image": "Text, Image",
          "Vicuna": "Vicuna\n(7B/13B)",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Multitask learning"
        },
        {
          "MiniGPT-4 [259]": "InstructBLIP [40]",
          "2023": "2023",
          "Text, Image": "Text, Image",
          "Vicuna": "Vicuna\n(7B/13B)",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Visual Instruction Tuning"
        },
        {
          "MiniGPT-4 [259]": "InternLM-\nXComposer\n[242]",
          "2023": "2023",
          "Text, Image": "Text, Image",
          "Vicuna": "InternLM-\nChat-7B",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Pre-training, Multi-task Training, Instruction Fine-\ntuning"
        },
        {
          "MiniGPT-4 [259]": "Macaw-LLM [119]",
          "2023": "2023",
          "Text, Image": "Text,\nImage, Au-\ndio, 3D",
          "Vicuna": "LLaMA",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Multimodal\nlanguage modeling with unified repre-\nsentation"
        },
        {
          "MiniGPT-4 [259]": "3D-MMLM [66]",
          "2023": "2024",
          "Text, Image": "Text, Image, 3D",
          "Vicuna": "LLaMA-3",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "3D understanding, point cloud processing, cross-\nmodal alignment"
        },
        {
          "MiniGPT-4 [259]": "Qwen2-VL [201]",
          "2023": "2024",
          "Text, Image": "Text, Image",
          "Vicuna": "Qwen-2",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Visual Instruction Tuning"
        },
        {
          "MiniGPT-4 [259]": "Moshi\n[52]",
          "2023": "2024",
          "Text, Image": "Text, Audio",
          "Vicuna": "LLaMA-2",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Text-to-speech, speech-to-text, audio understanding"
        },
        {
          "MiniGPT-4 [259]": "MM-LLMs [237]",
          "2023": "2024",
          "Text, Image": "Text,\nImage,\nVideo, Audio, 3D",
          "Vicuna": "Various",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Handling different modalities where X can be image,\nvideo, audio, 3D, etc."
        },
        {
          "MiniGPT-4 [259]": "AudioPaLM [154]",
          "2023": "2023",
          "Text, Image": "Text, Audio",
          "Vicuna": "PaLM-2",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Speech recognition, speech synthesis, multilingual\naudio understanding"
        },
        {
          "MiniGPT-4 [259]": "Qwen-Audio [38]",
          "2023": "2024",
          "Text, Image": "Text, Audio",
          "Vicuna": "Qwen-1.5",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Audio instruction tuning, audio-text alignment"
        },
        {
          "MiniGPT-4 [259]": "Yi-VL [226]",
          "2023": "2024",
          "Text, Image": "Text, Image",
          "Vicuna": "Yi-Chat",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Three-stage training: 1. Train ViT and projection\nmodule; 2. Increase image resolution and retrain; 3.\nTrain entire model"
        },
        {
          "MiniGPT-4 [259]": "InternLM-\nXComposer-2.5\n[243]",
          "2023": "2024",
          "Text, Image": "Text, Image",
          "Vicuna": "InternLM2-7B",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Pre-training, Multi-task Training, Instruction Fine-\ntuning"
        },
        {
          "MiniGPT-4 [259]": "CogVLM [203]",
          "2023": "2024",
          "Text, Image": "Text, Image",
          "Vicuna": "LLaMA-2",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Pre-training + Supervised Fine-tuning on vision-\nlanguage tasks"
        },
        {
          "MiniGPT-4 [259]": "ViLA [205]",
          "2023": "2024",
          "Text, Image": "Text, Image",
          "Vicuna": "Supports\nFrozen\nand\nFinetuned\n(LoRA) usage\nof LLM",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Distillation loss; Visual Question Answering loss"
        },
        {
          "MiniGPT-4 [259]": "LLaVA-Video\n[251]",
          "2023": "2025",
          "Text, Image": "Text,\nImage,\nVideo",
          "Vicuna": "LLaMA-2",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Video instruction tuning, video question answering"
        },
        {
          "MiniGPT-4 [259]": "Qwen2.5-Omni\n[217]",
          "2023": "2025",
          "Text, Image": "Text,\nImage, Au-\ndio, Video, 3D",
          "Vicuna": "Qwen-2.5",
          "Two-stage training: Stage 1: Freeze visual\nfeature\nextractor, train projection layer to align visual\nfea-\ntures with Vicuna; Stage 2: Instruction finetuning\non dialogue data": "Video and audio branches with cross-attention for\nmultimodal understanding"
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Leveraging efficient training and feature fusion in transformers for multimodal classification",
      "authors": [
        "E Kenan",
        "Gary Ak",
        "Yan Lee",
        "Mingwei Xu",
        "Shen"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "2",
      "title": "A kernel method for canonical correlation analysis",
      "authors": [
        "S Akaho"
      ],
      "year": "2001",
      "venue": "Proceedings of the International Meeting on Psychometric Society"
    },
    {
      "citation_id": "3",
      "title": "Research of spatial alignment techniques for multimodal image fusion",
      "authors": [
        "A Akhmerov",
        "A Vasilev",
        "A Vasileva"
      ],
      "year": "2019",
      "venue": "Proceedings of the SPIE"
    },
    {
      "citation_id": "4",
      "title": "Flamingo: A visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katie Millican",
        "Malcolm Reynolds",
        "Roman Ring",
        "Eliza Rutherford",
        "Serkan Cabi",
        "Tengda Han",
        "Zhitao Gong",
        "Sina Samangooei",
        "Marianne Monteiro",
        "Jacob Menick",
        "Sebastian Borgeaud",
        "Andrew Brock",
        "Aida Nematzadeh",
        "Sahand Sharifzadeh",
        "Mikolaj Binkowski",
        "Ricardo Barreira",
        "Oriol Vinyals",
        "Andrew Zisserman",
        "Karen Simonyan"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "5",
      "title": "Fusing information from multifidelity computer models of physical systems",
      "authors": [
        "Douglas Allaire",
        "Karen Willcox"
      ],
      "year": "2012",
      "venue": "15th International Conference on Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "Galen Andrew",
        "Raman Arora",
        "Jeff Bilmes",
        "Karen Livescu"
      ],
      "year": "2013",
      "venue": "Proceedings of the 30th International Conference on Machine Learning"
    },
    {
      "citation_id": "7",
      "title": "Multi-aspect candidates for repositioning: data fusion methods using heterogeneous information sources",
      "authors": [
        "Adam Arany",
        "Bence Bolgár",
        "Balázs Balogh",
        "Péter Antal",
        "Péter Mátyus"
      ],
      "year": "2012",
      "venue": "Current medicinal chemistry"
    },
    {
      "citation_id": "8",
      "title": "Set cross entropy: Likelihood-based permutation invariant loss function for probability distributions",
      "authors": [
        "Masataro Asai"
      ],
      "year": "2018",
      "venue": "Set cross entropy: Likelihood-based permutation invariant loss function for probability distributions"
    },
    {
      "citation_id": "9",
      "title": "Text-only training for image captioning using noise-injected clip",
      "year": "2022",
      "venue": "Text-only training for image captioning using noise-injected clip"
    },
    {
      "citation_id": "10",
      "title": "Classifier fusion for svm-based multimedia semantic indexing",
      "authors": [
        "S Ayache",
        "Georges Quénot",
        "Jérôme Gensel"
      ],
      "year": "2007",
      "venue": "European Conference on Information Retrieval"
    },
    {
      "citation_id": "11",
      "title": "Kernel independent component analysis",
      "authors": [
        "F Bach",
        "M Jordan"
      ],
      "year": "2002",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "12",
      "title": "An any-to-any vision model for tens of tasks and modalities",
      "authors": [
        "Roman Bachmann",
        "Oguzhan Fatih Kar",
        "David Mizrahi",
        "Ali Garjani",
        "Mingfei Gao",
        "David Griffiths",
        "Jiaming Hu",
        "Afshin Dehghan",
        "Amir Zamir"
      ],
      "year": "2024",
      "venue": "An any-to-any vision model for tens of tasks and modalities"
    },
    {
      "citation_id": "13",
      "title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation",
      "authors": [
        "Vijay Badrinarayanan",
        "Alex Kendall",
        "Roberto Cipolla"
      ],
      "year": "2015",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "14",
      "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
    },
    {
      "citation_id": "15",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrusaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "16",
      "title": "Alejandro Saez, and Claudia Villalonga. mhealthdroid: A novel framework for agile development of mobile health applications",
      "authors": [
        "Oresti Banos",
        "Rafael Garcia",
        "Juan Holgado-Terriza",
        "Miguel Damas",
        "Hector Pomares",
        "Ignacio Rojas"
      ],
      "year": "2014",
      "venue": "Ambient Assisted Living and Daily Activities"
    },
    {
      "citation_id": "17",
      "title": "Vlmo: Unified vision-language pretraining with mixture-of-modality-experts",
      "authors": [
        "Hangbo Bao",
        "Wenhui Wang",
        "Li Dong",
        "Qiang Liu",
        "Owais Khan Mohammed",
        "Kriti Aggarwal",
        "Subhojit Som",
        "Furu Wei"
      ],
      "year": "2022",
      "venue": "Vlmo: Unified vision-language pretraining with mixture-of-modality-experts"
    },
    {
      "citation_id": "18",
      "title": "Mobyen Uddin Ahmed, and Shahina Begum. A systematic literature review on multimodal machine learning: Applications, challenges, gaps and future directions",
      "authors": [
        "Arnab Barua"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "19",
      "title": "Visualization of graphical information fusion results",
      "authors": [
        "Erik Blasch",
        "M Georgiy",
        "Gennady Levchuk",
        "Dustin Staskevich",
        "Alex Burke",
        "Aved"
      ],
      "year": "2014",
      "venue": "Defense + Security Symposium"
    },
    {
      "citation_id": "20",
      "title": "Harnessing multimodal data integration to advance precision oncology",
      "authors": [
        "K Boehm",
        "P Khosravi",
        "R Vanguri",
        "Jianjiong Gao",
        "S Shah"
      ],
      "year": "2021",
      "venue": "Nature Reviews Cancer"
    },
    {
      "citation_id": "21",
      "title": "Generative ai for vision: A comprehensive study of frameworks and applications",
      "authors": [
        "Fouad Bousetouane"
      ],
      "year": "2025",
      "venue": "Generative ai for vision: A comprehensive study of frameworks and applications"
    },
    {
      "citation_id": "22",
      "title": "Large language models and large multimodal models in medical imaging: A primer for physicians",
      "authors": [
        "J Tyler",
        "Xin Bradshaw",
        "Joshua Tie",
        "Junjie Warner",
        "Quanzheng Hu",
        "Xiang Li",
        "Li"
      ],
      "year": "2025",
      "venue": "Journal of Nuclear Medicine"
    },
    {
      "citation_id": "23",
      "title": "Condition-aware multimodal fusion for robust semantic perception of driving scenes",
      "authors": [
        "Tim Broedermann",
        "Christos Sakaridis",
        "Yuqian Fu",
        "Luc Van Gool"
      ],
      "year": "2024",
      "venue": "Condition-aware multimodal fusion for robust semantic perception of driving scenes"
    },
    {
      "citation_id": "24",
      "title": "Language models are few-shot learners",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Dhariwal"
      ],
      "venue": "Language models are few-shot learners"
    },
    {
      "citation_id": "25",
      "title": "Coyo-700m: Image-text pair dataset",
      "authors": [
        "Minwoo Byeon",
        "Beomhee Park",
        "Haecheon Kim",
        "Sungjun Lee",
        "Woonhyuk Baek",
        "Saehoon Kim"
      ],
      "year": "2022",
      "venue": "Coyo-700m: Image-text pair dataset"
    },
    {
      "citation_id": "26",
      "title": "Generative ai models: Theoretical foundations and algorithmic practices",
      "authors": [
        "Yongnian Cao",
        "Xuechun Yang",
        "Rui Sun"
      ],
      "year": "2025",
      "venue": "Journal of Industrial Engineering and Applied Science"
    },
    {
      "citation_id": "27",
      "title": "Conceptual 12m: Pushing web-scale imagetext pre-training to recognize long-tail visual concepts",
      "authors": [
        "Soravit Changpinyo",
        "Piyush Sharma",
        "Nan Ding",
        "Radu Soricut"
      ],
      "year": "2021",
      "venue": "Conceptual 12m: Pushing web-scale imagetext pre-training to recognize long-tail visual concepts"
    },
    {
      "citation_id": "28",
      "title": "Mix-tower: Light visual question answering framework based on exclusive self-attention mechanism",
      "authors": [
        "D Chen",
        "J Chen",
        "L Yang",
        "F Shang"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "29",
      "title": "X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages",
      "authors": [
        "Feilong Chen",
        "Minglun Han",
        "Haozhi Zhao",
        "Qingyang Zhang",
        "Jing Shi",
        "Shuang Xu",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages"
    },
    {
      "citation_id": "30",
      "title": "Instructblip 2: Extending visionlanguage models with fine-grained instruction tuning",
      "authors": [
        "H Chen",
        "T Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Hgmf: Heterogeneous graph-based fusion for multimodal data with incompleteness",
      "authors": [
        "Jiayi Chen",
        "Aidong Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "32",
      "title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "authors": [
        "Jun Chen",
        "Deyao Zhu",
        "Xiaoqian Shen",
        "Xiang Li",
        "Zechun Liu",
        "Pengchuan Zhang",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra",
        "Yunyang Xiong",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning"
    },
    {
      "citation_id": "33",
      "title": "Llava-med: Medical image understanding with large language models",
      "authors": [
        "K Chen",
        "Y Sun"
      ],
      "year": "2023",
      "venue": "IEEE TNNLS"
    },
    {
      "citation_id": "34",
      "title": "Vast: A visionaudio-subtitle-text omni-modality foundation model and dataset",
      "authors": [
        "Sihan Chen",
        "Handong Li",
        "Qunbo Wang",
        "Zijia Zhao",
        "Mingzhen Sun",
        "Xinxin Zhu",
        "Jing Liu"
      ],
      "year": "2023",
      "venue": "Vast: A visionaudio-subtitle-text omni-modality foundation model and dataset"
    },
    {
      "citation_id": "35",
      "title": "Uniter: Universal image-text representation learning",
      "authors": [
        "Yen-Chun Chen",
        "Linjie Li",
        "Licheng Yu",
        "Ahmed Kholy",
        "Faisal Ahmed",
        "Zhe Gan",
        "Yu Cheng",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "Uniter: Universal image-text representation learning"
    },
    {
      "citation_id": "36",
      "title": "Comkd-clip: Comprehensive knowledge distillation for contrastive language-image pre-traning model",
      "authors": [
        "Yifan Chen",
        "Xiaozhen Qiao",
        "Zhe Sun",
        "Xuelong Li"
      ],
      "year": "2024",
      "venue": "Comkd-clip: Comprehensive knowledge distillation for contrastive language-image pre-traning model"
    },
    {
      "citation_id": "37",
      "title": "Revisiting multimodal representation in contrastive learning: From patch and token embeddings to finite discrete tokens",
      "authors": [
        "Yuxiao Chen",
        "Jianbo Yuan",
        "Yu Tian",
        "Shijie Geng",
        "Xinyu Li",
        "Ding Zhou",
        "Dimitris Metaxas",
        "Hongxia Yang"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models"
    },
    {
      "citation_id": "39",
      "title": "Multimodal representation learning for tourism recommendation with two-tower architecture",
      "authors": [
        "Y Cui",
        "S Liang",
        "Zhang"
      ],
      "year": "2024",
      "venue": "PLoS One"
    },
    {
      "citation_id": "40",
      "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "authors": [
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Meng",
        "Huat Tiong",
        "Junqi Zhao",
        "Weisheng Wang",
        "Boyang Li",
        "Pascale Fung",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Instructblip: Towards general-purpose vision-language models with instruction tuning"
    },
    {
      "citation_id": "41",
      "title": "Multimodal mri synthesis using unified generative adversarial networks",
      "authors": [
        "X Dai",
        "Y Lei",
        "Y Fu",
        "W Curran",
        "T Liu",
        "H Mao",
        "Xiaofeng Yang"
      ],
      "year": "2020",
      "venue": "Medical Physics"
    },
    {
      "citation_id": "42",
      "title": "Sensor fusion of camera and lidar raw data for vehicle detection",
      "authors": [
        "Gokulesh Danapal",
        "Giovanni Santos",
        "João Paulo",
        "C Da Costa",
        "J Bruno",
        "Praciano",
        "P Gabriel",
        "Pinheiro"
      ],
      "year": "2020",
      "venue": "2020 Workshop on Communication Networks and Power Systems (WCNPS)"
    },
    {
      "citation_id": "43",
      "title": "Redcaps: web-curated image-text data created by the people, for the people",
      "authors": [
        "Karan Desai",
        "Gaurav Kaul",
        "Zubin Aysola",
        "Justin Johnson"
      ],
      "year": "2021",
      "venue": "Redcaps: web-curated image-text data created by the people, for the people"
    },
    {
      "citation_id": "44",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "45",
      "title": "Looking outside the window: Wide-context transformer for the semantic segmentation of high-resolution remote sensing images",
      "authors": [
        "Lei Ding",
        "Dong Lin",
        "Shaofu Lin",
        "Jing Zhang",
        "Xiaojie Cui",
        "Yuebin Wang",
        "Hao Tang",
        "Lorenzo Bruzzone"
      ],
      "year": "2022",
      "venue": "IEEE TGRS"
    },
    {
      "citation_id": "46",
      "title": "Sparse fusion for multimodal transformers",
      "authors": [
        "Yi Ding",
        "Alex Rich",
        "Mason Wang",
        "Noah Stier",
        "Matthew Turk",
        "Pradeep Sen",
        "Tobias Höllerer"
      ],
      "year": "2021",
      "venue": "Sparse fusion for multimodal transformers"
    },
    {
      "citation_id": "47",
      "title": "Kernelbased sensor fusion with application to audio-visual voice activity detection",
      "authors": [
        "David Dov",
        "Ronen Talmon",
        "Israel Cohen"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "",
      "authors": [
        "Danny Driess",
        "Fei Xia",
        "S Mehdi",
        "Corey Sajjadi",
        "Aakanksha Lynch",
        "Brian Chowdhery",
        "Ayzaan Ichter",
        "Jonathan Wahid",
        "Quan Tompson",
        "Tianhe Vuong",
        "Wenlong Yu",
        "Yevgen Huang",
        "Pierre Chebotar",
        "Daniel Sermanet",
        "Sergey Duckworth",
        "Vincent Levine",
        "Karol Vanhoucke",
        "Marc Hausman",
        "Klaus Toussaint",
        "Andy Greff",
        "Igor Zeng",
        "Pete Mordatch",
        "Florence"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "49",
      "title": "Audio-visual event localization via recursive fusion by joint co-attention",
      "authors": [
        "Bin Duan",
        "Hao Tang",
        "Wei Wang",
        "Ziliang Zong",
        "Guowei Yang",
        "Yan Yan"
      ],
      "year": "2021",
      "venue": "Audio-visual event localization via recursive fusion by joint co-attention"
    },
    {
      "citation_id": "50",
      "title": "The llama 3 herd of models",
      "authors": [
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman"
      ],
      "venue": "The llama 3 herd of models"
    },
    {
      "citation_id": "51",
      "title": "Diffusion transport alignment",
      "authors": [
        "Andres Duque",
        "Guy Wolf",
        "Kevin Moon"
      ],
      "year": "2022",
      "venue": "Diffusion transport alignment"
    },
    {
      "citation_id": "52",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "year": "2024",
      "venue": "Moshi: a speech-text foundation model for real-time dialogue"
    },
    {
      "citation_id": "53",
      "title": "Onellm: One framework to align all modalities with language",
      "authors": [
        "Han"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "54",
      "title": "Towards artificial general intelligence via a multimodal foundation model",
      "authors": [
        "N Fei",
        "Z Lu",
        "Y Gao",
        "G Yang",
        "Y Huo",
        "J Wen",
        "H Lu"
      ],
      "year": "2022",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "55",
      "title": "Multi-modal transformer for video retrieval",
      "authors": [
        "Valentin Gabeur",
        "Chen Sun",
        "Karteek Alahari",
        "Cordelia Schmid"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "56",
      "title": "",
      "authors": [
        "Yitzhak Samir",
        "Gabriel Gadre",
        "Alex Ilharco",
        "Jonathan Fang",
        "Georgios Hayase",
        "Thao Smyrnis",
        "Ryan Nguyen",
        "Mitchell Marten",
        "Dhruba Wortsman",
        "Jieyu Ghosh",
        "Eyal Zhang",
        "Rahim Orgad",
        "Giannis Entezari",
        "Sarah Daras",
        "Vivek Pratt",
        "Yonatan Ramanujan",
        "Kalyani Bitton",
        "Stephen Marathe",
        "Richard Mussmann",
        "Mehdi Vencu",
        "Ranjay Cherti",
        "Pang Krishna",
        "Olga Wei Koh",
        "Alexander Saukh",
        "Shuran Ratner",
        "Hannaneh Song",
        "Ali Hajishirzi",
        "Romain Farhadi",
        "Sewoong Beaumont",
        "Alex Oh",
        "Jenia Dimakis",
        "Yair Jitsev",
        "Vaishaal Carmon",
        "Ludwig Shankar",
        "Schmidt"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "57",
      "title": "Hiclip: Contrastive language-image pretraining with hierarchy-aware attention",
      "authors": [
        "Shijie Geng",
        "Jianbo Yuan",
        "Yu Tian",
        "Yuxiao Chen",
        "Yongfeng Zhang"
      ],
      "year": "2023",
      "venue": "Hiclip: Contrastive language-image pretraining with hierarchy-aware attention"
    },
    {
      "citation_id": "58",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "Rohit Girdhar",
        "Alaaeldin El-Nouby",
        "Zhuang Liu",
        "Mannat Singh",
        "Kalyan Vasudev Alwala",
        "Armand Joulin",
        "Ishan Misra"
      ],
      "year": "2023",
      "venue": "Imagebind: One embedding space to bind them all"
    },
    {
      "citation_id": "59",
      "title": "Domain aligned clip for fewshot classification",
      "authors": [
        "Waleed Muhammad",
        "Jochen Gondal",
        "Inigo Gast",
        "Richard Ruiz",
        "Tommaso Droste",
        "Suren Macri",
        "Luitpold Kumar",
        "Staudigl"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "60",
      "title": "Globalizing BERT-based transformer architectures for long document summarization",
      "authors": [
        "Quentin Grail",
        "Julien Perez",
        "Eric Gaussier"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"
    },
    {
      "citation_id": "61",
      "title": "A model-level fusion-based multi-modal object detection and recognition method",
      "authors": [
        "Chen Guo",
        "Li Zhang"
      ],
      "year": "2023",
      "venue": "2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)"
    },
    {
      "citation_id": "62",
      "title": "Audioclip: Extending clip to image, text and audio",
      "authors": [
        "Andrey Guzhov",
        "Federico Raue",
        "Jörn Hees",
        "Andreas Dengel"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "63",
      "title": "On the use of early fusion operators on heterogeneous graph neural networks for one-class learning",
      "authors": [
        "M Gôlo",
        "Marcelo Isaias De Moraes",
        "R Goularte",
        "R Marcacini"
      ],
      "year": "2023",
      "venue": "Proceedings of the 29th Brazilian Symposium on Multimedia and the Web"
    },
    {
      "citation_id": "64",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "65",
      "title": "Canonical correlation analysis: An overview with application to learning methods",
      "authors": [
        "D Hardoon",
        "S Szedmak",
        "J Shawe-Taylor"
      ],
      "year": "2004",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "66",
      "title": "",
      "authors": [
        "Yining Hong",
        "Haoyu Zhen",
        "Peihao Chen",
        "Shuhong Zheng",
        "Yilun Du",
        "Zhenfang Chen",
        "Chuang Gan"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "67",
      "title": "Relations between two sets of variates",
      "authors": [
        "Harold Hotelling"
      ],
      "year": "1936",
      "venue": "Biometrika"
    },
    {
      "citation_id": "68",
      "title": "Provla: Compositional image search with progressive vision-language alignment and multimodal fusion",
      "authors": [
        "Zhizhang Hu",
        "Xinliang Zhu",
        "Son Tran",
        "René Vidal",
        "Arnab Dhua"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "69",
      "title": "A comprehensive survey on applications of transformers for deep learning tasks",
      "authors": [
        "Saidul Islam",
        "Hanae Elmekki",
        "Ahmed Elsebai",
        "Jamal Bentahar",
        "Najat Drawel",
        "Gaith Rjoub",
        "Witold Pedrycz"
      ],
      "venue": "A comprehensive survey on applications of transformers for deep learning tasks"
    },
    {
      "citation_id": "70",
      "title": "General perception with iterative attention",
      "authors": [
        "Andrew Jaegle",
        "Felix Gimeno",
        "Andrew Brock",
        "Andrew Zisserman",
        "Oriol Vinyals",
        "Joao Carreira",
        "Perceiver"
      ],
      "year": "2021",
      "venue": "General perception with iterative attention"
    },
    {
      "citation_id": "71",
      "title": "Learning to combine local models for facial action unit detection",
      "authors": [
        "Shashank Jaiswal",
        "Brais Martínez",
        "Michel Valstar"
      ],
      "year": "2015",
      "venue": "11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "72",
      "title": "Scaling up visual and visionlanguage representation learning with noisy text supervision",
      "authors": [
        "Chao Jia",
        "Yinfei Yang",
        "Ye Xia",
        "Yi-Ting Chen",
        "Zarana Parekh",
        "Hieu Pham",
        "Quoc Le",
        "Yunhsuan Sung",
        "Zhen Li",
        "Tom Duerig"
      ],
      "year": "2021",
      "venue": "Scaling up visual and visionlanguage representation learning with noisy text supervision"
    },
    {
      "citation_id": "73",
      "title": "Geminifusion: Efficient pixel-wise multimodal fusion for vision transformer",
      "authors": [
        "Ding Jia",
        "Jianyuan Guo",
        "Kai Han",
        "Han Wu",
        "Chao Zhang",
        "Chang Xu",
        "Xinghao Chen"
      ],
      "year": "2024",
      "venue": "Geminifusion: Efficient pixel-wise multimodal fusion for vision transformer"
    },
    {
      "citation_id": "74",
      "title": "Multimodal image alignment via linear mapping between feature modalities",
      "authors": [
        "Yanyun Jiang",
        "Yuanjie Zheng",
        "Sujuan Hou",
        "Yuchou Chang",
        "J Gee"
      ],
      "year": "2017",
      "venue": "Journal of Healthcare Engineering"
    },
    {
      "citation_id": "75",
      "title": "Deep visual-semantic alignments for generating image descriptions",
      "authors": [
        "Andrej Karpathy",
        "Fei-Fei Li"
      ],
      "year": "2015",
      "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "76",
      "title": "Unifiedvisiongpt: Streamlining vision-oriented ai through generalized multimodal framework",
      "authors": [
        "Chris Kelly",
        "Luhui Hu",
        "Cindy Yang",
        "Yu Tian",
        "Deshun Yang",
        "Bang Yang",
        "Zaoshan Huang",
        "Zihao Li",
        "Yuexian Zou"
      ],
      "year": "2023",
      "venue": "Unifiedvisiongpt: Streamlining vision-oriented ai through generalized multimodal framework"
    },
    {
      "citation_id": "77",
      "title": "Diffusion-driven gan inversion for multi-modal face image generation",
      "authors": [
        "Jihyun Kim",
        "Changjae Oh",
        "Hoseok Do",
        "Soohyun Kim",
        "Kwanghoon Sohn"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "78",
      "title": "Hype: Hyperbolic entailment filtering for underspecified images and texts",
      "authors": [
        "Wonjae Kim",
        "Sanghyuk Chun",
        "Taekyung Kim",
        "Dongyoon Han",
        "Sangdoo Yun"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "79",
      "title": "ViLT: Visionand-language transformer without convolution or region supervision",
      "authors": [
        "Wonjae Kim",
        "Bokyung Son",
        "Ildoo Kim"
      ],
      "venue": "ViLT: Visionand-language transformer without convolution or region supervision"
    },
    {
      "citation_id": "80",
      "title": "",
      "authors": [
        "Alexander Kirillov",
        "Eric Mintun",
        "Nikhila Ravi",
        "Hanzi Mao",
        "Chloe Rolland",
        "Laura Gustafson",
        "Tete Xiao",
        "Spencer Whitehead",
        "Alexander Berg",
        "Wan-Yen Lo",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "81",
      "title": "Deap: A database for emotion analysis ;using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "82",
      "title": "Graphalignment: Bayesian pairwise alignment of biological networks",
      "authors": [
        "M Kolář",
        "J Meier",
        "V Mustonen"
      ],
      "year": "2012",
      "venue": "BMC Syst Biol"
    },
    {
      "citation_id": "83",
      "title": "Autovit: Achieving real-time vision transformers on mobile via latency-aware coarse-to-fine search",
      "authors": [
        "Zhenglun Kong",
        "Dongkuan Xu",
        "Zhengang Li",
        "Peiyan Dong",
        "Hao Tang",
        "Yanzhi Wang",
        "Subhabrata Mukherjee"
      ],
      "year": "2025",
      "venue": "Autovit: Achieving real-time vision transformers on mobile via latency-aware coarse-to-fine search"
    },
    {
      "citation_id": "84",
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "authors": [
        "Ranjay Krishna",
        "Yuke Zhu",
        "Oliver Groth",
        "Justin Johnson",
        "Kenji Hata",
        "Joshua Kravitz",
        "Stephanie Chen",
        "Yannis Kalantidis",
        "Li-Jia Li",
        "David Shamma",
        "Michael Bernstein",
        "Fei-Fei Li"
      ],
      "year": "2016",
      "venue": "Visual genome: Connecting language and vision using crowdsourced dense image annotations"
    },
    {
      "citation_id": "85",
      "title": "An overview of sequence comparison: Time warps, string edits, and macromolecules",
      "authors": [
        "Joseph Kruskal"
      ],
      "year": "1983",
      "venue": "SIAM Rev"
    },
    {
      "citation_id": "86",
      "title": "Unconditional image-text pair generation with multimodal cross quantizer",
      "authors": [
        "Hyungyu Lee",
        "Sungjin Park",
        "E Choi"
      ],
      "year": "2022",
      "venue": "ArXiv"
    },
    {
      "citation_id": "87",
      "title": "Seed-bench-2: Benchmarking multimodal large language models",
      "authors": [
        "Bohao Li",
        "Yuying Ge",
        "Yixiao Ge",
        "Guangzhi Wang",
        "Rui Wang",
        "Ruimao Zhang",
        "Ying Shan"
      ],
      "year": "2023",
      "venue": "Seed-bench-2: Benchmarking multimodal large language models"
    },
    {
      "citation_id": "88",
      "title": "Gs-clip: Gaussian splatting for contrastive language-image-3d pretraining from real-world data",
      "authors": [
        "Haoyuan Li",
        "Yanpeng Zhou",
        "Yihan Zeng",
        "Hang Xu",
        "Xiaodan Liang"
      ],
      "year": "2024",
      "venue": "Gs-clip: Gaussian splatting for contrastive language-image-3d pretraining from real-world data"
    },
    {
      "citation_id": "89",
      "title": "Rethinking transformer for long contextual histopathology whole slide image analysis",
      "authors": [
        "Honglin Li",
        "Yunlong Zhang",
        "Pingyi Chen",
        "Zhongyi Shui",
        "Chenglu Zhu",
        "Lin Yang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "90",
      "title": "Mulfs-cap: Multimodal fusion-supervised cross-modality alignment perception for unregistered infrared-visible image fusion",
      "authors": [
        "Huafeng Li",
        "Zengyi Yang",
        "Yafei Zhang",
        "Wei Jia",
        "Zhengtao Yu",
        "Yu Liu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "91",
      "title": "TextBind: Multi-turn interleaved multimodal instructionfollowing in the wild",
      "authors": [
        "Huayang Li",
        "Siheng Li",
        "Deng Cai",
        "Longyue Wang",
        "Lemao Liu",
        "Taro Watanabe",
        "Yujiu Yang",
        "Shuming Shi"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2024"
    },
    {
      "citation_id": "92",
      "title": "Densefuse: A fusion approach to infrared and visible images",
      "authors": [
        "Hui Li",
        "Xiaojun Wu"
      ],
      "year": "2018",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "93",
      "title": "Self-distillation for robust LiDAR semantic segmentation in autonomous driving",
      "authors": [
        "Jiale Li",
        "Hang Dai",
        "Yong Ding"
      ],
      "year": "2022",
      "venue": "ECCV"
    },
    {
      "citation_id": "94",
      "title": "Mseg3d: Multi-modal 3d semantic segmentation for autonomous driving",
      "authors": [
        "Jiale Li",
        "Hang Dai",
        "Hao Han",
        "Yong Ding"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "95",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
    },
    {
      "citation_id": "96",
      "title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Caiming Xiong",
        "Steven Hoi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning"
    },
    {
      "citation_id": "97",
      "title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "authors": [
        "Junnan Li",
        "Ramprasaath Selvaraju",
        "Akhilesh Gotmare",
        "Shafiq Joty",
        "Caiming Xiong",
        "Steven Chu",
        "Hong Hoi"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "98",
      "title": "Coupled mamba: Enhanced multi-modal fusion with coupled state space model",
      "authors": [
        "Wenbing Li",
        "Hang Zhou",
        "Junqing Yu",
        "Zikai Song",
        "Wei Yang"
      ],
      "year": "2024",
      "venue": "Coupled mamba: Enhanced multi-modal fusion with coupled state space model"
    },
    {
      "citation_id": "99",
      "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "authors": [
        "Xiujun Li",
        "Xi Yin",
        "Chunyuan Li",
        "Pengchuan Zhang",
        "Xiaowei Hu",
        "Lei Zhang",
        "Lijuan Wang",
        "Houdong Hu",
        "Li Dong",
        "Furu Wei",
        "Yejin Choi",
        "Jianfeng Gao"
      ],
      "year": "2020",
      "venue": "Oscar: Object-semantics aligned pre-training for vision-language tasks"
    },
    {
      "citation_id": "100",
      "title": "Hierarchical feature fusion network for salient object detection",
      "authors": [
        "Xuelong Li",
        "Dawei Song",
        "Yongsheng Dong"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "101",
      "title": "Efficient multimodal fusion via interactive prompting",
      "authors": [
        "Yaowei Li",
        "Ruijie Quan",
        "Linchao Zhu",
        "Yi Yang"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "102",
      "title": "Data processing techniques for modern multimodal models",
      "authors": [
        "Yinheng Li",
        "Han Ding",
        "Hang Chen"
      ],
      "year": "2024",
      "venue": "Data processing techniques for modern multimodal models",
      "arxiv": "arXiv:2407.19180"
    },
    {
      "citation_id": "103",
      "title": "A survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges",
      "authors": [
        "Zongxia Li",
        "Xiyang Wu",
        "Hongyang Du",
        "Fuxiao Liu",
        "Huy Nghiem",
        "Guangyao Shi"
      ],
      "year": "2025",
      "venue": "A survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges"
    },
    {
      "citation_id": "104",
      "title": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2024",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "105",
      "title": "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
      "authors": [
        "Yuhui Victor Weixin Liang",
        "Yongchan Zhang",
        "Serena Kwon",
        "James Yeung",
        "Zou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "106",
      "title": "On pre-training for visual language models",
      "authors": [
        "Ji Lin",
        "Hongxu Yin",
        "Wei Ping",
        "Yao Lu",
        "Pavlo Molchanov",
        "Andrew Tao",
        "Huizi Mao",
        "Jan Kautz",
        "Mohammad Shoeybi",
        "Song Han",
        "Vila"
      ],
      "year": "2024",
      "venue": "On pre-training for visual language models"
    },
    {
      "citation_id": "107",
      "title": "A survey of transformers",
      "authors": [
        "Tianyang Lin",
        "Yuxin Wang",
        "Xiangyang Liu",
        "Xipeng Qiu"
      ],
      "year": "2022",
      "venue": "AI Open"
    },
    {
      "citation_id": "108",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "Lubomir Bourdev",
        "Ross Girshick",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "C Zitnick",
        "Piotr Dollár"
      ],
      "year": "2015",
      "venue": "Microsoft coco: Common objects in context"
    },
    {
      "citation_id": "109",
      "title": "Vx2text: End-toend learning of video-based text generation from multimodal inputs",
      "authors": [
        "Xudong Lin",
        "Gedas Bertasius",
        "Jue Wang",
        "Shih-Fu Chang",
        "Devi Parikh",
        "Lorenzo Torresani"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "110",
      "title": "St-align: A multimodal foundation model for image-gene alignment in spatial transcriptomics",
      "authors": [
        "Yuxiang Lin",
        "Ling Luo",
        "Ying Chen",
        "Xushi Zhang",
        "Zihui Wang",
        "Wenxian Yang",
        "Mengsha Tong",
        "Rongshan Yu"
      ],
      "year": "2024",
      "venue": "St-align: A multimodal foundation model for image-gene alignment in spatial transcriptomics"
    },
    {
      "citation_id": "111",
      "title": "Touchformer: A transformer-based two-tower architecture for tactile temporal signal classification",
      "authors": [
        "C Liu",
        "H Liu",
        "H Chen",
        "W Du"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "112",
      "title": "Audioldm: Text-to-audio generation with latent diffusion models",
      "authors": [
        "Haohe Liu",
        "Zehua Chen",
        "Yi Yuan",
        "Xinhao Mei",
        "Xubo Liu",
        "Danilo Mandic",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "year": "2023",
      "venue": "Audioldm: Text-to-audio generation with latent diffusion models"
    },
    {
      "citation_id": "113",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "114",
      "title": "Taisu: A 166m large-scale highquality dataset for chinese vision-language pre-training",
      "authors": [
        "Yulong Liu",
        "Guibo Zhu",
        "Bin Zhu",
        "Qi Song",
        "Guojing Ge",
        "Haoran Chen",
        "Guanhui Qiao",
        "Ru Peng",
        "Lingxiang Wu",
        "Jinqiao Wang"
      ],
      "venue": "Taisu: A 166m large-scale highquality dataset for chinese vision-language pre-training"
    },
    {
      "citation_id": "115",
      "title": "Advances in Neural Information Processing Systems",
      "authors": [
        "In Koyejo",
        "S Mohamed",
        "A Agarwal",
        "D Belgrave",
        "K Cho",
        "A Oh"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "116",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "117",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks"
    },
    {
      "citation_id": "118",
      "title": "Unified-io: A uni-fied model for vision, language, and multi-modal tasks",
      "authors": [
        "Jiasen Lu",
        "Christopher Clark",
        "Rowan Zellers",
        "Roozbeh Mottaghi",
        "Aniruddha Kembhavi"
      ],
      "year": "2022",
      "venue": "Unified-io: A uni-fied model for vision, language, and multi-modal tasks"
    },
    {
      "citation_id": "119",
      "title": "Cheap and quick: Efficient vision-language instruction tuning for large language models",
      "authors": [
        "Gen Luo",
        "Yiyi Zhou",
        "Tianhe Ren",
        "Shengxin Chen",
        "Xiaoshuai Sun",
        "Rongrong Ji"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "120",
      "title": "Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration",
      "authors": [
        "Chenyang Lyu",
        "Minghao Wu",
        "Longyue Wang",
        "Xinting Huang",
        "Bingshuai Liu",
        "Zefeng Du",
        "Shuming Shi",
        "Zhaopeng Tu"
      ],
      "year": "2023",
      "venue": "Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration"
    },
    {
      "citation_id": "121",
      "title": "Att-sinkhorn: Multimodal alignment with sinkhornbased deep attention architecture",
      "authors": [
        "Qianxia Ma",
        "Ming Zhang",
        "Yan Tang",
        "Zhen Huang"
      ],
      "venue": "2023 28th International Conference on Automation and Computing (ICAC)"
    },
    {
      "citation_id": "122",
      "title": "Learning modality knowledge alignment for crossmodality transfer",
      "authors": [
        "Wenxuan Ma",
        "Shuang Li",
        "Lincan Cai",
        "Jingxuan Kang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 41st International Conference on Machine Learning"
    },
    {
      "citation_id": "123",
      "title": "Sycoca: Symmetrizing contrastive captioners with attentive masking for multimodal alignment",
      "authors": [
        "Ziping Ma",
        "Furong Xu",
        "Jian Liu",
        "Ming Yang",
        "Qingpei Guo"
      ],
      "year": "2024",
      "venue": "Sycoca: Symmetrizing contrastive captioners with attentive masking for multimodal alignment"
    },
    {
      "citation_id": "124",
      "title": "Applications of deep learning and reinforcement learning to biological data",
      "authors": [
        "M Mahmud",
        "M Kaiser",
        "A Hussain",
        "Stefano Vassanelli"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "125",
      "title": "Divide, conquer and combine: Hierarchical feature fusion network with local and global perspectives for multimodal affective computing",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2019",
      "venue": "Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "126",
      "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2020",
      "venue": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion"
    },
    {
      "citation_id": "127",
      "title": "A hierarchical feature fusion framework for adaptive visual tracking",
      "authors": [
        "Alexandros Makris",
        "Dimitrios Kosmopoulos",
        "Stavros Perantonis",
        "Sergios Theodoridis"
      ],
      "year": "2011",
      "venue": "Image Vis. Comput"
    },
    {
      "citation_id": "128",
      "title": "Nonlinear feature extraction using generalized canonical correlation analysis",
      "authors": [
        "T Melzer",
        "M Reiter",
        "H Bischof"
      ],
      "year": "2001",
      "venue": "Proceedings of the International Conference on Artificial Neural Networks (ICANN)"
    },
    {
      "citation_id": "129",
      "title": "Model level fusion of edge histogram descriptors and gabor wavelets for landmine detection with ground penetrating radar",
      "authors": [
        "Oualid Missaoui",
        "Hichem Frigui",
        "Paul Gader"
      ],
      "year": "2010",
      "venue": "IEEE International Geoscience and Remote Sensing Symposium"
    },
    {
      "citation_id": "130",
      "title": "Multimodal fusion methods with vision transformers for remote sensing semantic segmentation",
      "authors": [
        "Grazia Veronica",
        "Mirko Morelli",
        "Paolo Barbato",
        "Flavio Piccoli",
        "Paolo Napoletano"
      ],
      "year": "2023",
      "venue": "2023 13th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)"
    },
    {
      "citation_id": "131",
      "title": "Majority vote of diverse classifiers for late fusion",
      "authors": [
        "Emilie Morvant",
        "Amaury Habrard",
        "Stéphane Ayache"
      ],
      "year": "2014",
      "venue": "Structural, Syntactic, and Statistical Pattern Recognition"
    },
    {
      "citation_id": "132",
      "title": "Functional learning of kernels for information fusion purposes",
      "authors": [
        "Alberto Muñoz",
        "Javier González"
      ],
      "year": "2008",
      "venue": "Iberoamerican Congress on Pattern Recognition"
    },
    {
      "citation_id": "133",
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "Arsha Nagrani",
        "Shan Yang",
        "Anurag Arnab",
        "Aren Jansen",
        "Cordelia Schmid",
        "Chen Sun"
      ],
      "year": "2022",
      "venue": "Attention bottlenecks for multimodal fusion"
    },
    {
      "citation_id": "134",
      "title": "Multimodal network alignment",
      "authors": [
        "Huda Nassar",
        "David Gleich"
      ],
      "year": "2017",
      "venue": "ArXiv"
    },
    {
      "citation_id": "135",
      "title": "Improving multimodal datasets with image captioning",
      "authors": [
        "Thao Nguyen",
        "Yitzhak Samir",
        "Gabriel Gadre",
        "Sewoong Ilharco",
        "Ludwig Oh",
        "Schmidt"
      ],
      "year": "2023",
      "venue": "Improving multimodal datasets with image captioning"
    },
    {
      "citation_id": "136",
      "title": "A survey on multimodal wearable sensor-based human action recognition",
      "authors": [
        "Jianyuan Ni",
        "Hao Tang",
        "Tousiful Syed",
        "Yan Haque",
        "Anne Yan",
        "Ngu"
      ],
      "year": "2024",
      "venue": "A survey on multimodal wearable sensor-based human action recognition",
      "arxiv": "arXiv:2404.15349"
    },
    {
      "citation_id": "137",
      "title": "Abftnet: An efficient transformer network with alignment before fusion for multimodal automatic modulation recognition",
      "authors": [
        "Meng Ning",
        "Fan Zhou",
        "Wei Wang",
        "Shaoqiang Wang",
        "Peiying Zhang",
        "Jian Wang"
      ],
      "venue": "Electronics"
    },
    {
      "citation_id": "138",
      "title": "Audiovisual emotion recognition in video clips",
      "authors": [
        "Fatemeh Noroozi",
        "Marina Marjanovic",
        "Angelina Njegus",
        "Sergio Escalera",
        "Gholamreza Anbarjafari"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "139",
      "title": "GPT-4 technical report",
      "authors": [
        "Josh Openai",
        "Steven Achiam",
        "Sandhini Adler",
        "Lama Agarwal",
        "Ilge Ahmad",
        "Akkaya"
      ],
      "venue": "GPT-4 technical report"
    },
    {
      "citation_id": "140",
      "title": "Dinov2: Learning robust visual features without supervision",
      "authors": [
        "Maxime Oquab",
        "Timothée Darcet",
        "Théo Moutakanni",
        "Huy Vo",
        "Marc Szafraniec",
        "Vasil Khalidov"
      ],
      "year": "2024",
      "venue": "Dinov2: Learning robust visual features without supervision"
    },
    {
      "citation_id": "141",
      "title": "Im2text: Describing images using 1 million captioned photographs",
      "authors": [
        "Vicente Ordonez",
        "Girish Kulkarni",
        "Tamara Berg"
      ],
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "142",
      "title": "Weakly supervised representation learning for audio-visual scene analysis",
      "authors": [
        "Sanjeel Parekh",
        "Slim Essid",
        "Alexey Ozerov",
        "Q Ngoc",
        "Patrick Duong",
        "Gaël Pérez",
        "Richard"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "143",
      "title": "Survey of large multimodal model datasets, application categories and taxonomy",
      "authors": [
        "Priyaranjan Pattnayak",
        "Laxmichand Hitesh",
        "Bhargava Patel",
        "Amit Kumar",
        "Ishan Agarwal",
        "Srikant Banerjee",
        "Tejaswini Panda",
        "Kumar"
      ],
      "year": "2024",
      "venue": "Survey of large multimodal model datasets, application categories and taxonomy"
    },
    {
      "citation_id": "144",
      "title": "Kosmos-2: Grounding multimodal large language models to the world",
      "authors": [
        "Zhiliang Peng",
        "Wenhui Wang",
        "Li Dong",
        "Yaru Hao",
        "Shaohan Huang",
        "Shuming Ma",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Kosmos-2: Grounding multimodal large language models to the world"
    },
    {
      "citation_id": "145",
      "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "authors": [
        "Bryan Plummer",
        "Liwei Wang",
        "Chris Cervantes",
        "Juan Caicedo",
        "Julia Hockenmaier",
        "Svetlana Lazebnik"
      ],
      "year": "2016",
      "venue": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models"
    },
    {
      "citation_id": "146",
      "title": "Leveraging multimodal features for knowledge graph entity alignment based on dynamic selfattention networks",
      "authors": [
        "Ye Qian",
        "Li Pan"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "147",
      "title": "Alternative telescopic displacement: An efficient multimodal alignment method",
      "authors": [
        "Jiahao Qin",
        "Yitao Xu",
        "Zihong Luo",
        "Chengzhi Liu",
        "Zong Lu",
        "Xiaojun Zhang"
      ],
      "year": "2023",
      "venue": "ArXiv"
    },
    {
      "citation_id": "148",
      "title": "Vt-clip: Enhancing vision-language models with visual-guided texts",
      "authors": [
        "Longtian Qiu",
        "Renrui Zhang",
        "Ziyu Guo",
        "Ziyao Zeng",
        "Zilu Guo",
        "Yafeng Li",
        "Guangnan Zhang"
      ],
      "year": "2023",
      "venue": "Vt-clip: Enhancing vision-language models with visual-guided texts"
    },
    {
      "citation_id": "149",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "150",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeff Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Language models are unsupervised multitask learners"
    },
    {
      "citation_id": "151",
      "title": "Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions",
      "authors": [
        "Anil Rahate",
        "Rahee Walambe",
        "Sheela Ramanna",
        "Ketan Kotecha"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "152",
      "title": "Introducing a new benchmarked dataset for activity monitoring",
      "authors": [
        "Attila Reiss",
        "Didier Stricker"
      ],
      "year": "2012",
      "venue": "2012 16th International Symposium on Wearable Computers"
    },
    {
      "citation_id": "153",
      "title": "Towards ethical multimodal systems",
      "authors": [
        "Alexis Roger",
        "Esma Aïmeur",
        "Irina Rish"
      ],
      "year": "2024",
      "venue": "Towards ethical multimodal systems"
    },
    {
      "citation_id": "154",
      "title": "Towards raw sensor fusion in 3d object detection",
      "authors": [
        "András Rövid",
        "Viktor Remeli"
      ],
      "year": "2019",
      "venue": "IEEE 17th World Symposium on Applied Machine Intelligence and Informatics (SAMI)"
    },
    {
      "citation_id": "155",
      "title": "",
      "authors": [
        "Paul Rubenstein",
        "Chulayuth Asawaroengchai",
        "Dung Duc",
        "Ankur Nguyen",
        "Zalán Bapna",
        "Félix Borsos",
        "De Chaumont",
        "Peter Quitry",
        "Dalia Chen",
        "Wei Badawy",
        "Eugene Han",
        "Hannah Kharitonov",
        "Dirk Muckenhirn",
        "James Padfield",
        "Danny Qin",
        "Tara Rozenberg",
        "Johan Sainath",
        "Matt Schalkwyk",
        "Michelle Sharifi",
        "Marco Ramanovich",
        "Alexandru Tagliasacchi",
        "Mihajlo Tudor",
        "Damien Velimirović",
        "Jiahui Vincent",
        "Yongqiang Yu",
        "Vicky Wang",
        "Neil Zayats",
        "Yu Zeghidour",
        "Zhishuai Zhang",
        "Lukas Zhang",
        "Christian Zilka",
        "Frank"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "156",
      "title": "Deep learning-based multimodal image retrieval combining image and text",
      "authors": [
        "Imran Md",
        "M Sarker",
        "Milanova"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Computational Science and Computational Intelligence (CSCI)"
    },
    {
      "citation_id": "157",
      "title": "Feature fusion hierarchies for gender classification",
      "authors": [
        "F Scalzo",
        "George Bebis",
        "Mircea Nicolescu",
        "Leandro Loss",
        "A Tavakkoli"
      ],
      "year": "2008",
      "venue": "th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "158",
      "title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
      "authors": [
        "Christoph Schuhmann",
        "Romain Beaumont",
        "Richard Vencu",
        "Cade Gordon",
        "Ross Wightman",
        "Mehdi Cherti",
        "Theo Coombes",
        "Aarush Katta",
        "Clayton Mullis",
        "Mitchell Wortsman",
        "Patrick Schramowski",
        "Srivatsa Kundurthy",
        "Katherine Crowson",
        "Ludwig Schmidt",
        "Robert Kaczmarczyk",
        "Jenia Jitsev"
      ],
      "year": "2022",
      "venue": "Laion-5b: An open large-scale dataset for training next generation image-text models"
    },
    {
      "citation_id": "159",
      "title": "Progressive fusion for multimodal integration",
      "authors": [
        "Shiv Shankar",
        "Laure Thompson",
        "Madalina Fiterau"
      ],
      "year": "2022",
      "venue": "Progressive fusion for multimodal integration"
    },
    {
      "citation_id": "160",
      "title": "Scaling vision-language models with sparse mixture of experts",
      "authors": [
        "Sheng Shen",
        "Zhewei Yao",
        "Chunyuan Li",
        "Trevor Darrell",
        "Kurt Keutzer",
        "Yuxiong He"
      ],
      "year": "2023",
      "venue": "Scaling vision-language models with sparse mixture of experts"
    },
    {
      "citation_id": "161",
      "title": "Charformer: A glyph fusion based attentive framework for high-precision character image denoising",
      "authors": [
        "Daqian Shi",
        "Xiaolei Diao",
        "Lida Shi",
        "Hao Tang",
        "Yang Chi",
        "Chuntao Li",
        "Hao Xu"
      ],
      "year": "2022",
      "venue": "ACM MM"
    },
    {
      "citation_id": "162",
      "title": "Heterogeneous graph-based multimodal brain network learning",
      "authors": [
        "Gen Shi",
        "Yifan Zhu",
        "Wenjin Liu",
        "Quanming Yao",
        "Xuesong Li"
      ],
      "year": "2022",
      "venue": "Heterogeneous graph-based multimodal brain network learning"
    },
    {
      "citation_id": "163",
      "title": "Flava: A foundational language and vision alignment model",
      "authors": [
        "Amanpreet Singh",
        "Ronghang Hu",
        "Vedanuj Goswami",
        "Guillaume Couairon",
        "Wojciech Galuba",
        "Marcus Rohrbach",
        "Douwe Kiela"
      ],
      "year": "2022",
      "venue": "Flava: A foundational language and vision alignment model"
    },
    {
      "citation_id": "164",
      "title": "Early versus late fusion in semantic video analysis",
      "authors": [
        "G Cees",
        "Marcel Snoek",
        "Arnold Worring",
        "Smeulders"
      ],
      "year": "2005",
      "venue": "Early versus late fusion in semantic video analysis"
    },
    {
      "citation_id": "165",
      "title": "Scene-driven multimodal knowledge graph construction for embodied ai",
      "authors": [
        "Yiming Song",
        "Zhen Li",
        "Wei Song"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Robotics"
    },
    {
      "citation_id": "166",
      "title": "Set-clip: Exploring aligned semantic from low-alignment multimodal data through a distribution view",
      "authors": [
        "Zijia Song",
        "Zelin Zang",
        "Yelin Wang",
        "Guozheng Yang",
        "Kaicheng Yu",
        "Wanyu Chen",
        "Miaoyu Wang",
        "Stan Li"
      ],
      "year": "2024",
      "venue": "Set-clip: Exploring aligned semantic from low-alignment multimodal data through a distribution view"
    },
    {
      "citation_id": "167",
      "title": "Wit: Wikipediabased image text dataset for multimodal multilingual machine learning",
      "authors": [
        "Krishna Srinivasan",
        "Karthik Raman",
        "Jiecao Chen",
        "Michael Bendersky",
        "Marc Najork"
      ],
      "year": "2021",
      "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21"
    },
    {
      "citation_id": "168",
      "title": "Multimodal learning with deep boltzmann machines",
      "authors": [
        "Nitish Srivastava",
        "Ruslan Salakhutdinov"
      ],
      "year": "2012",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "169",
      "title": "The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lea Schumann",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements"
    },
    {
      "citation_id": "170",
      "title": "Design of a low-level radar and timeof-flight sensor fusion framework",
      "authors": [
        "Josef Steinbaeck",
        "Christian Steger",
        "Gerald Holweg",
        "Norbert Druml"
      ],
      "year": "2018",
      "venue": "21st Euromicro Conference on Digital System Design (DSD)"
    },
    {
      "citation_id": "171",
      "title": "Beyond two-tower matching: Learning sparse retrievable crossinteractions for recommendation",
      "authors": [
        "L Su",
        "F Yan",
        "J Zhu",
        "X Xiao",
        "H Duan"
      ],
      "year": "2023",
      "venue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "172",
      "title": "Fast and robust dynamic hand gesture recognition via key frames extraction and feature fusion",
      "authors": [
        "Hao Tang",
        "Hong Liu",
        "Wei Xiao",
        "Nicu Sebe"
      ],
      "year": "2019",
      "venue": "Fast and robust dynamic hand gesture recognition via key frames extraction and feature fusion"
    },
    {
      "citation_id": "173",
      "title": "Attentiongan: Unpaired image-to-image translation using attention-guided generative adversarial networks",
      "authors": [
        "Hao Tang",
        "Hong Liu",
        "Dan Xu",
        "Philip Hs Torr",
        "Nicu Sebe"
      ],
      "year": "1972",
      "venue": "IEEE TNNLS"
    },
    {
      "citation_id": "174",
      "title": "Graph transformer gans with graph masked modeling for architectural layout generation",
      "authors": [
        "Hao Tang",
        "Ling Shao",
        "Nicu Sebe",
        "Luc Van Gool"
      ],
      "year": "2024",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "175",
      "title": "Enhanced multi-scale cross-attention for person image generation",
      "authors": [
        "Hao Tang",
        "Ling Shao",
        "Nicu Sebe",
        "Luc Van Gool"
      ],
      "year": "2025",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "176",
      "title": "Multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation",
      "authors": [
        "Hao Tang",
        "Dan Xu",
        "Nicu Sebe",
        "Yanzhi Wang",
        "Jason Corso",
        "Yan Yan"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "177",
      "title": "Local class-specific and global image-level generative adversarial networks for semantic-guided scene generation",
      "authors": [
        "Hao Tang",
        "Dan Xu",
        "Yan Yan",
        "Philip Hs Torr",
        "Nicu Sebe"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "178",
      "title": "Bafn: Bidirection attention based fusion network for multimodal sentiment analysis",
      "authors": [
        "Jiajia Tang",
        "Dongjun Liu",
        "Xuanyu Jin",
        "Yong Peng",
        "Qianchuan Zhao",
        "Yu Ding",
        "Wanzeng Kong"
      ],
      "year": "1966",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "179",
      "title": "Graph-based multimodal sequential embedding for sign language translation",
      "authors": [
        "Shengeng Tang",
        "Dixin Guo",
        "Rui Hong",
        "Min Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "180",
      "title": "Matr: Multimodal medical image fusion via multiscale adaptive transformer",
      "authors": [
        "Wei Tang",
        "Fazhi He",
        "Yefeng Liu",
        "Ying Duan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "181",
      "title": "Df-gan: A simple and effective baseline for text-to-image synthesis",
      "authors": [
        "Ming Tao",
        "Hao Tang",
        "Fei Wu",
        "Xiao-Yuan Jing",
        "Bing-Kun Bao",
        "Changsheng Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)"
    },
    {
      "citation_id": "182",
      "title": "COLD fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition",
      "authors": [
        "Mani Kumar Tellamekala",
        "Shahin Amiriparian",
        "Björn Schuller",
        "Elisabeth André",
        "Timo Giesbrecht",
        "Michel Valstar"
      ],
      "year": "2024",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "183",
      "title": "Uit-saviors at medvqa-gi 2023: Improving multimodal learning with image enhancement for gastrointestinal visual question answering",
      "authors": [
        "T Thai",
        "A Vo",
        "K Hao",
        "Linh Tieu",
        "T Bui",
        "Nguyen"
      ],
      "year": "2023",
      "venue": "Uit-saviors at medvqa-gi 2023: Improving multimodal learning with image enhancement for gastrointestinal visual question answering"
    },
    {
      "citation_id": "184",
      "title": "Yfcc100m: the new data in multimedia research",
      "authors": [
        "Bart Thomee",
        "David Shamma",
        "Gerald Friedland",
        "Benjamin Elizalde",
        "Karl Ni",
        "Douglas Poland",
        "Damian Borth",
        "Li-Jia Li"
      ],
      "year": "2016",
      "venue": "Yfcc100m: the new data in multimedia research"
    },
    {
      "citation_id": "185",
      "title": "Multimodal deep representation learning for video classification",
      "authors": [
        "Haiman Tian",
        "Yudong Tao",
        "Samira Pouyanfar",
        "Shu-Ching Chen",
        "Mei-Ling Shyu"
      ],
      "year": "2019",
      "venue": "World Wide Web"
    },
    {
      "citation_id": "186",
      "title": "Eyes wide shut? exploring the visual shortcomings of multimodal llms",
      "authors": [
        "Shengbang Tong",
        "Zhuang Liu",
        "Yuexiang Zhai",
        "Yi Ma",
        "Yann Lecun",
        "Saining Xie"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "187",
      "title": "Nonlinear graph fusion for multi-modal classification of alzheimer's disease",
      "authors": [
        "Tong Tong",
        "Katherine Gray",
        "Qinquan Gao",
        "Liang Chen",
        "Daniel Rueckert"
      ],
      "year": "2015",
      "venue": "Machine Learning for Multimodal Interaction"
    },
    {
      "citation_id": "188",
      "title": "Multi-modal classification of alzheimer's disease using nonlinear graph fusion",
      "authors": [
        "Tong Tong",
        "Katherine Gray",
        "Qinquan Gao",
        "Liang Chen",
        "Daniel Rueckert"
      ],
      "year": "2017",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "189",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Learning factorized multimodal representations"
    },
    {
      "citation_id": "190",
      "title": "Differentiable cross-modal hashing via multimodal transformers",
      "authors": [
        "J Tu",
        "X Liu",
        "R Lin",
        "Z Hong",
        "M Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "191",
      "title": "Guided deep decoder: Unsupervised image pair fusion",
      "authors": [
        "Tatsumi Uezato",
        "Danfeng Hong",
        "Naoto Yokoya",
        "Wei He"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "192",
      "title": "Dynamic time warping",
      "authors": [
        "Unknown"
      ],
      "year": "2007",
      "venue": "Information Retrieval for Music and Motion"
    },
    {
      "citation_id": "193",
      "title": "Villa: Finegrained vision-language representation learning from realworld data",
      "authors": [
        "Maya Varma",
        "Jean-Benoit Delbrouck",
        "Sarah Hooper",
        "Akshay Chaudhari",
        "Curtis Langlotz"
      ],
      "year": "2023",
      "venue": "Villa: Finegrained vision-language representation learning from realworld data"
    },
    {
      "citation_id": "194",
      "title": "I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "year": "2024",
      "venue": "I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition"
    },
    {
      "citation_id": "195",
      "title": "",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "venue": ""
    },
    {
      "citation_id": "196",
      "title": "Im2text and text2im: Associating images and texts for cross-modal retrieval",
      "authors": [
        "Y Verma",
        "C Jawahar"
      ],
      "year": "2014",
      "venue": "Proceedings of the British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "197",
      "title": "Few-shot in-context imitation learning via implicit graph alignment",
      "authors": [
        "Vitalis Vosylius",
        "Edward Johns"
      ],
      "year": "2023",
      "venue": "Proceedings of The 7th Conference on Robot Learning"
    },
    {
      "citation_id": "198",
      "title": "Dataefficient multimodal fusion on a single gpu",
      "authors": [
        "Noël Vouitsis",
        "Zhaoyan Liu",
        "Krishna Satya",
        "Valentin Gorti",
        "Jesse Villecroze",
        "Guangwei Cresswell",
        "Gabriel Yu",
        "Maksims Loaiza-Ganem",
        "Volkovs"
      ],
      "year": "2024",
      "venue": "Dataefficient multimodal fusion on a single gpu"
    },
    {
      "citation_id": "199",
      "title": "Cross-modal feature alignment and fusion for composed image retrieval",
      "authors": [
        "Yongquan Wan",
        "Wenhai Wang",
        "Guobing Zou",
        "Bofeng Zhang"
      ],
      "year": "2024",
      "venue": "CVPRW"
    },
    {
      "citation_id": "200",
      "title": "Too large; data reduction for vision-language pre-training",
      "authors": [
        "Alex Jinpeng Wang",
        "Kevin Lin",
        "David Zhang",
        "Stan Lei",
        "Mike Shou"
      ],
      "year": "2023",
      "venue": "Too large; data reduction for vision-language pre-training"
    },
    {
      "citation_id": "201",
      "title": "Mutually beneficial transformer for multimodal data fusion",
      "authors": [
        "Jinping Wang",
        "Xiaojun Tan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "202",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "Peng Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Shijie Wang",
        "Zhihao Fan",
        "Jinze Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge",
        "Yang Fan",
        "Kai Dang",
        "Mengfei Du",
        "Xuancheng Ren",
        "Rui Men",
        "Dayiheng Liu",
        "Chang Zhou",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
    },
    {
      "citation_id": "203",
      "title": "One-peace: Exploring one general representation model toward unlimited modalities",
      "authors": [
        "Peng Wang",
        "Shijie Wang",
        "Junyang Lin",
        "Shuai Bai",
        "Xiaohuan Zhou",
        "Jingren Zhou",
        "Xinggang Wang",
        "Chang Zhou"
      ],
      "year": "2023",
      "venue": "One-peace: Exploring one general representation model toward unlimited modalities"
    },
    {
      "citation_id": "204",
      "title": "CogVLM: Visual expert for pretrained language models",
      "authors": [
        "Weihan Wang",
        "Qingsong Lv",
        "Wenmeng Yu",
        "Wenyi Hong",
        "Ji Qi",
        "Yan Wang",
        "Junhui Ji",
        "Zhuoyi Yang",
        "Lei Zhao",
        "Xixuan Song",
        "Jiazheng Xu",
        "Bin Xu",
        "Juanzi Li",
        "Yuxiao Dong",
        "Ming Ding",
        "Jie Tang"
      ],
      "venue": "CogVLM: Visual expert for pretrained language models"
    },
    {
      "citation_id": "205",
      "title": "Image as a foreign language: BEiT pretraining for all vision and vision-language tasks",
      "authors": [
        "Wenhui Wang",
        "Hangbo Bao",
        "Li Dong",
        "Johan Bjorck",
        "Zhiliang Peng",
        "Qiang Liu",
        "Kriti Aggarwal",
        "Owais Khan Mohammed",
        "Saksham Singhal",
        "Subhojit Som",
        "Furu Wei"
      ],
      "venue": "Image as a foreign language: BEiT pretraining for all vision and vision-language tasks"
    },
    {
      "citation_id": "206",
      "title": "Vila: Efficient video-language alignment for video question answering",
      "authors": [
        "Xijun Wang",
        "Junbang Liang",
        "Chun-Kai Wang",
        "Kenan Deng",
        "Yu Lou",
        "Ming Lin",
        "Shan Yang"
      ],
      "year": "2024",
      "venue": "Vila: Efficient video-language alignment for video question answering"
    },
    {
      "citation_id": "207",
      "title": "The role of spatial alignment in multimodal medical image fusion using deep learning for diagnostic problems",
      "authors": [
        "Xingyue Wang",
        "Kuang Shu",
        "Haowei Kuang",
        "Shiwei Luo",
        "Richu Jin",
        "Jiang Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2021 International Conference on Intelligent Medicine and Health, ICIMH '21"
    },
    {
      "citation_id": "208",
      "title": "Kernel-based data fusion improves the drugprotein interaction prediction",
      "authors": [
        "Yong-Cui Wang",
        "Chunhua Zhang",
        "Naiyang Deng",
        "Yong Wang"
      ],
      "year": "2011",
      "venue": "Computational biology and chemistry"
    },
    {
      "citation_id": "209",
      "title": "Venetsanopoulos. Kernel cross-modal factor analysis for information fusion with application to bimodal emotion recognition",
      "authors": [
        "Yongjin Wang",
        "Ling Guan"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "210",
      "title": "Simvlm: Simple visual language model pretraining with weak supervision",
      "authors": [
        "Zirui Wang",
        "Jiahui Yu",
        "Adams Yu",
        "Zihang Dai",
        "Yulia Tsvetkov",
        "Yuan Cao"
      ],
      "year": "2022",
      "venue": "Simvlm: Simple visual language model pretraining with weak supervision"
    },
    {
      "citation_id": "211",
      "title": "Decision-level data fusion in quality control and predictive maintenance",
      "authors": [
        "Yupeng Wei",
        "Dazhong Wu",
        "Janis Terpenny"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Automation Science and Engineering"
    },
    {
      "citation_id": "212",
      "title": "Multimodal reranking for knowledge-intensive visual question answering",
      "authors": [
        "Haoyang Wen",
        "Honglei Zhuang",
        "Hamed Zamani",
        "Alexander Hauptmann",
        "Michael Bendersky"
      ],
      "year": "2024",
      "venue": "Multimodal reranking for knowledge-intensive visual question answering"
    },
    {
      "citation_id": "213",
      "title": "Clip with mapping-fusion embedding for text-guided image editing",
      "authors": [
        "Fei Wu",
        "Yongheng Ma",
        "Hao Jin",
        "Xiao-Yuan Jing",
        "Guo-Ping Jiang",
        "Mfeclip"
      ],
      "year": "2024",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "214",
      "title": "Multimodal generative models for compositional representation learning",
      "authors": [
        "Mike Wu",
        "Noah Goodman"
      ],
      "year": "2019",
      "venue": "Multimodal generative models for compositional representation learning"
    },
    {
      "citation_id": "215",
      "title": "TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots",
      "authors": [
        "Qinhua Xie",
        "Hao Tang"
      ],
      "year": "2025",
      "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "216",
      "title": "Scene graphbased semantic alignment for multimodal tasks",
      "authors": [
        "Wei Xiong",
        "Yifan Zhang",
        "Wei Li"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "217",
      "title": "Multimodal data fusion in text-image heterogeneous graph for social media recommendation",
      "authors": [
        "Yu Xiong",
        "Daling Wang",
        "Yifei Zhang",
        "Shi Feng",
        "Guoren Wang"
      ],
      "year": "2014",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "218",
      "title": "",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang",
        "Bin Zhang",
        "Xiong Wang",
        "Yunfei Chu",
        "Junyang Lin"
      ],
      "year": "2025",
      "venue": ""
    },
    {
      "citation_id": "219",
      "title": "Building bridges between encoders in visionlanguage representation learning",
      "authors": [
        "X Xu",
        "C Wu",
        "S Rosenman",
        "V Lal",
        "W Che",
        "Bridgetower"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "220",
      "title": "Dynamic multimodal fusion",
      "authors": [
        "Zihui Xue",
        "Radu Marculescu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "221",
      "title": "Mitigating biases in multimodal personality assessment",
      "authors": [
        "Shen Yan",
        "Di Huang",
        "Mohammad Soleymani"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "222",
      "title": "Dmf-gan: Deep multimodal fusion generative adversarial networks for text-to-image synthesis",
      "authors": [
        "Bing Yang",
        "Xueqin Xiang",
        "Wangzeng Kong",
        "Jianhai Zhang",
        "Yong Peng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "223",
      "title": "Continual attentive fusion for incremental learning in semantic segmentation",
      "authors": [
        "Guanglei Yang",
        "Enrico Fini",
        "Dan Xu",
        "Paolo Rota",
        "Mingli Ding",
        "Hao Tang",
        "Xavier Alameda-Pineda",
        "Elisa Ricci"
      ],
      "year": "2022",
      "venue": "IEEE TMM"
    },
    {
      "citation_id": "224",
      "title": "Videochat: Conversational agents in video understanding",
      "authors": [
        "H Yang",
        "S Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "225",
      "title": "Macsa: A multimodal aspect-category sentiment analysis dataset with multimodal fine-grained aligned annotations",
      "authors": [
        "Haoyan Yang",
        "Yifan Wu",
        "Zhenyu Si",
        "Yijun Zhao",
        "Jinfeng Liu",
        "Bing Qin"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "226",
      "title": "A survey on multimodal large language models",
      "authors": [
        "Shukang Yin",
        "Chaoyou Fu",
        "Sirui Zhao",
        "Ke Li",
        "Xing Sun",
        "Tong Xu",
        "Enhong Chen"
      ],
      "venue": "A survey on multimodal large language models"
    },
    {
      "citation_id": "227",
      "title": "",
      "authors": [
        "Alex Young",
        "Bei Chen",
        "Chao Li",
        "Chengen Huang",
        "Ge Zhang",
        "Guanwei Zhang",
        "Heng Li",
        "Jiangcheng Zhu",
        "Jianqun Chen",
        "Jing Chang",
        "Kaidong Yu",
        "Peng Liu",
        "Qiang Liu",
        "Shawn Yue",
        "Senbin Yang",
        "Shiming Yang",
        "Tao Yu",
        "Wen Xie",
        "Wenhao Huang",
        "Xiaohui Hu",
        "Xiaoyi Ren",
        "Xinyao Niu",
        "Pengcheng Nie",
        "Yuchi Xu",
        "Yudong Liu",
        "Yue Wang",
        "Yuxuan Cai",
        "Zhenyu Gu",
        "Zhiyuan Liu",
        "Zonghong Dai",
        "Yi"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "228",
      "title": "CoCa: Contrastive captioners are image-text foundation models",
      "authors": [
        "Jiahui Yu",
        "Zirui Wang",
        "Vijay Vasudevan",
        "Legg Yeung",
        "Mojtaba Seyedhosseini",
        "Yonghui Wu"
      ],
      "venue": "CoCa: Contrastive captioners are image-text foundation models"
    },
    {
      "citation_id": "229",
      "title": "Q-tempfusion: Quantization-aware temporal multi-sensor fusion on bird's-eye view representation",
      "authors": [
        "Pinrui Yu",
        "Zhenglun Kong",
        "Pu Zhao",
        "Peiyan Dong",
        "Hao Tang",
        "Fei Sun",
        "Xue Lin",
        "Yanzhi Wang"
      ],
      "year": "2025",
      "venue": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "230",
      "title": "Capsfusion: Rethinking image-text data at scale",
      "authors": [
        "Qiying Yu",
        "Quan Sun",
        "Xiaosong Zhang",
        "Yufeng Cui",
        "Fan Zhang",
        "Yue Cao",
        "Xinlong Wang",
        "Jingjing Liu"
      ],
      "year": "2024",
      "venue": "Capsfusion: Rethinking image-text data at scale"
    },
    {
      "citation_id": "231",
      "title": "CH-SIMS: A Chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "232",
      "title": "Towards user friendly medication mapping using entityboosted two-tower neural network",
      "authors": [
        "S Yuan",
        "P Bhatia",
        "B Celikkaya",
        "H Liu",
        "K Choi"
      ],
      "year": "2021",
      "venue": "International Workshop on Deep Learning for Human Activity Recognition"
    },
    {
      "citation_id": "233",
      "title": "When and why visionlanguage models behave like bags-of-words, and what to do about it?",
      "authors": [
        "Mert Yuksekgonul",
        "Federico Bianchi",
        "Pratyusha Kalluri",
        "Dan Jurafsky",
        "James Zou"
      ],
      "year": "2023",
      "venue": "When and why visionlanguage models behave like bags-of-words, and what to do about it?"
    },
    {
      "citation_id": "234",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "235",
      "title": "Contextual object detection with multimodal large language models",
      "authors": [
        "Yuhang Zang",
        "Wei Li",
        "Jun Han",
        "Kaiyang Zhou",
        "Chen Loy"
      ],
      "year": "2024",
      "venue": "Contextual object detection with multimodal large language models"
    },
    {
      "citation_id": "236",
      "title": "Sigmoid loss for language image pretraining",
      "authors": [
        "Xiaohua Zhai",
        "Basil Mustafa",
        "Alexander Kolesnikov",
        "Lucas Beyer"
      ],
      "year": "2023",
      "venue": "Sigmoid loss for language image pretraining"
    },
    {
      "citation_id": "237",
      "title": "Multimodal intelligence: Representation learning, information fusion, and applications",
      "authors": [
        "Chao Zhang",
        "Zichao Yang",
        "Xiaodong He",
        "Li Deng"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "238",
      "title": "MM-LLMs: Recent advances in MultiModal large language models",
      "authors": [
        "Duzhen Zhang",
        "Yahan Yu",
        "Jiahua Dong",
        "Chenxing Li",
        "Dan Su",
        "Chenhui Chu",
        "Dong Yu"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2024"
    },
    {
      "citation_id": "239",
      "title": "Detr with improved denoising anchor boxes for end-toend object detection",
      "authors": [
        "Hao Zhang",
        "Feng Li",
        "Shilong Liu",
        "Lei Zhang",
        "Hang Su",
        "Jun Zhu",
        "Lionel Ni",
        "Heung-Yeung Shum",
        "Dino"
      ],
      "year": "2022",
      "venue": "Detr with improved denoising anchor boxes for end-toend object detection"
    },
    {
      "citation_id": "240",
      "title": "Transformer-based end-to-end anatomical and functional image fusion",
      "authors": [
        "Jing Zhang",
        "Aiping Liu",
        "Dan Wang",
        "Yu Liu",
        "Z Wang",
        "Xun Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "241",
      "title": "Vision-language models for vision tasks: A survey",
      "authors": [
        "Jingyi Zhang",
        "Jiaxing Huang",
        "Sheng Jin",
        "Shijian Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "242",
      "title": "Neural attention: Enhancing qkv calculation in self-attention mechanism with neural networks",
      "authors": [
        "Muhan Zhang"
      ],
      "year": "2023",
      "venue": "Neural attention: Enhancing qkv calculation in self-attention mechanism with neural networks"
    },
    {
      "citation_id": "243",
      "title": "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
      "authors": [
        "Pan Zhang",
        "Xiaoyi Dong",
        "Bin Wang",
        "Yuhang Cao",
        "Chao Xu",
        "Linke Ouyang",
        "Zhiyuan Zhao",
        "Haodong Duan",
        "Songyang Zhang",
        "Shuangrui Ding",
        "Wenwei Zhang",
        "Hang Yan",
        "Xinyue Zhang",
        "Wei Li",
        "Jingwen Li",
        "Kai Chen",
        "Conghui He",
        "Xingcheng Zhang",
        "Yu Qiao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "year": "2023",
      "venue": "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition"
    },
    {
      "citation_id": "244",
      "title": "",
      "authors": [
        "Pan Zhang",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Rui Qian",
        "Lin Chen",
        "Qipeng Guo",
        "Haodong Duan",
        "Bin Wang",
        "Linke Ouyang",
        "Songyang Zhang",
        "Wenwei Zhang",
        "Yining Li",
        "Yang Gao",
        "Peng Sun",
        "Xinyue Zhang",
        "Wei Li",
        "Jingwen Li",
        "Wenhai Wang",
        "Hang Yan",
        "Conghui He",
        "Xingcheng Zhang",
        "Kai Chen",
        "Jifeng Dai",
        "Yu Qiao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "245",
      "title": "Revisiting visual representations in visionlanguage models",
      "authors": [
        "Pengchuan Zhang",
        "Xiujun Li",
        "Xiaowei Hu",
        "Jianwei Yang",
        "Lei Zhang",
        "Lijuan Wang",
        "Yejin Choi",
        "Jianfeng Gao",
        "Vinvl"
      ],
      "year": "2021",
      "venue": "Revisiting visual representations in visionlanguage models"
    },
    {
      "citation_id": "246",
      "title": "Bimodal fusion network with multi-head attention for multimodal sentiment analysis",
      "authors": [
        "Rui Zhang",
        "Chengrong Xue",
        "Qingfu Qi",
        "Liyuan Lin",
        "Jing Zhang",
        "Lun Zhang"
      ],
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "247",
      "title": "Genderalign: An alignment dataset for mitigating gender bias in large language models",
      "authors": [
        "Tao Zhang",
        "Ziqian Zeng",
        "Yuxiang Xiao",
        "Huiping Zhuang",
        "Cen Chen",
        "James Foulds",
        "Shimei Pan"
      ],
      "year": "2024",
      "venue": "Genderalign: An alignment dataset for mitigating gender bias in large language models"
    },
    {
      "citation_id": "248",
      "title": "Multimodal deep fusion for image question answering",
      "authors": [
        "Weifeng Zhang",
        "Jing Yu",
        "Yuxia Wang",
        "Wei Wang"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "249",
      "title": "From redundancy to relevance: Enhancing explainability in multimodal large language models",
      "authors": [
        "Xiaofeng Zhang",
        "Chen Shen",
        "Xiaosong Yuan",
        "Shaotian Yan",
        "Liang Xie",
        "Wenxiao Wang",
        "Chaochen Gu",
        "Hao Tang",
        "Jieping Ye"
      ],
      "year": "2024",
      "venue": "From redundancy to relevance: Enhancing explainability in multimodal large language models",
      "arxiv": "arXiv:2406.06579"
    },
    {
      "citation_id": "250",
      "title": "Enlightenyour-voice: When multimodal meets zero-shot low-light image enhancement",
      "authors": [
        "Xiaofeng Zhang",
        "Zishan Xu",
        "Hao Tang",
        "Chaochen Gu",
        "Wei Chen",
        "Shanying Zhu",
        "Xinping Guan"
      ],
      "year": "2023",
      "venue": "Enlightenyour-voice: When multimodal meets zero-shot low-light image enhancement",
      "arxiv": "arXiv:2312.10109"
    },
    {
      "citation_id": "251",
      "title": "Understanding unimodal bias in multimodal deep linear networks",
      "authors": [
        "Yedi Zhang",
        "Peter Latham",
        "Andrew Saxe"
      ],
      "year": "2024",
      "venue": "Understanding unimodal bias in multimodal deep linear networks"
    },
    {
      "citation_id": "252",
      "title": "Llava-video: Video instruction tuning with synthetic data",
      "authors": [
        "Yuanhan Zhang",
        "Jinming Wu",
        "Wei Li",
        "Bo Li",
        "Zejun Ma",
        "Ziwei Liu",
        "Chunyuan Li"
      ],
      "year": "2025",
      "venue": "Llava-video: Video instruction tuning with synthetic data"
    },
    {
      "citation_id": "253",
      "title": "A token-wise graph-based framework for multimodal named entity recognition",
      "authors": [
        "Zhiwei Zhang",
        "Wenyu Mai",
        "Heng Xiong",
        "Cheng Wu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "254",
      "title": "Risurconv: Rotation invariant surface attention-augmented convolutions for 3d point cloud classification and segmentation",
      "authors": [
        "Zhiyuan Zhang",
        "Licheng Yang",
        "Zhiyu Xiang"
      ],
      "year": "2024",
      "venue": "Risurconv: Rotation invariant surface attention-augmented convolutions for 3d point cloud classification and segmentation"
    },
    {
      "citation_id": "255",
      "title": "Rs5m and georsclip: A large-scale vision-language dataset and a large vision-language model for remote sensing",
      "authors": [
        "Zilun Zhang",
        "Tiancheng Zhao",
        "Yulong Guo",
        "Jianwei Yin"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing"
    },
    {
      "citation_id": "256",
      "title": "Deep multimodal data fusion",
      "authors": [
        "Fei Zhao",
        "Chengcui Zhang",
        "Baocheng Geng"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "257",
      "title": "Deep multimodal learning with vision, audio, and text: Challenges and innovations",
      "authors": [
        "Lihong Zhao",
        "Huan Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "258",
      "title": "Visual-textual sentiment analysis enhanced by hierarchical cross-modality interaction",
      "authors": [
        "Tao Zhou",
        "Jiuxin Cao",
        "Xueling Zhu",
        "Bo Liu",
        "Shancang Li"
      ],
      "year": "2021",
      "venue": "IEEE Systems Journal"
    },
    {
      "citation_id": "259",
      "title": "Vision + language applications: A survey",
      "authors": [
        "Yutong Zhou",
        "Nobutaka Shimada"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "260",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "261",
      "title": "Svp-t: A shape-level variable-position transformer for multivariate time series classification",
      "authors": [
        "Rui Zuo",
        "Guoqing Li",
        "Bongshin Choi",
        "Sourav Bhowmick",
        "Daphne Mah",
        "Gary Wong"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "262",
      "title": "Distributed fusion in sensor networks: a graphical models perspective",
      "authors": [
        "Lei Müjdat C ¸etin",
        "John Chen",
        "Alexander Fisher",
        "Randolph Ihler",
        "Martin Moses",
        "Alan Wainwright",
        "Willsky"
      ],
      "year": "2006",
      "venue": "Distributed fusion in sensor networks: a graphical models perspective"
    }
  ]
}