{
  "paper_id": "2103.03934v1",
  "title": "An Ensemble With Shared Representations Based On Convolutional Networks For Continually Learning Facial Expressions",
  "published": "2021-03-05T20:40:52Z",
  "authors": [
    "Henrique Siqueira",
    "Pablo Barros",
    "Sven Magg",
    "Stefan Wermter"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Social robots able to continually learn facial expressions could progressively improve their emotion recognition capability towards people interacting with them. Semisupervised learning through ensemble predictions is an efficient strategy to leverage the high exposure of unlabelled facial expressions during human-robot interactions. Traditional ensemble-based systems, however, are composed of several independent classifiers leading to a high degree of redundancy, and unnecessary allocation of computational resources. In this paper, we proposed an ensemble based on convolutional networks where the early layers are strong low-level feature extractors, and their representations shared with an ensemble of convolutional branches. This results in a significant drop in redundancy of low-level features processing. Training in a semi-supervised setting, we show that our approach is able to continually learn facial expressions through ensemble predictions using unlabelled samples from different data distributions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Deep learning has made remarkable advances in the scientific community by improving state-of-the-art results in many domains such as speech recognition and image classification, and for the modern society by assisting us in our daily activities including web search and language translation  [2] . The success of deep learning is usually attributed to the boost in computational power with parallel computing and the continuous release of large labelled datasets.\n\nIn the context of social affective robotics, however, gathering a large high-quality labelled dataset of emotions is tremendously difficult due to the intrinsic subjective perception of emotions  [3] ,  [4] . Russell et al.  [3]  suggest that emotion perception is more than a simple and universal decoding process, and may vary according to the internal emotional state of the receiver. Another factor is the influence of the context on emotion perception, as studied by Calbi et al.  [4] . In their psychological experiments, they show evidence that a neutral facial expression cross-cut by a happy or fear content event is often perceived accordingly. This subjectivity on emotion perception reflects in high disagreement between annotators on categorizing emotional samples for a dataset, forcing authors to discard many samples or all of them of a given emotional category  [5] .\n\nAll of the authors are with Knowledge Technology, Department of Informatics, University of Hamburg, Vogt-Koelln-Str. 30, 22527 Hamburg, Germany {siqueira, barros, magg, wermter}@informatik.uni-hamburg.de Therefore, different learning strategies have been explored for enhancing emotional capabilities on social affective robots including learning through observation  [6] , and human rewards  [7] . In the former, an autoencoder is used for learning appropriate facial expression responses by tracking facial landmarks from human-human conversations. In the latter, the authors have proposed a hybrid neural network architecture to teach facial expressions to a robot in an on-line fashion based on human feedbacks. However, a social robot would leverage the great exposure of continual information from the environment  [8] , which could be very expensive through human feedback, or hard to evaluate the effectiveness in an auto-associative setting.\n\nA semi-supervised learning setting based on ensemble predictions could be a potential candidate, for instance, to exploit the great volume of unlabelled facial expressions exposed to a robot during human-robot interactions (HRIs) in order to improve its recognition capability, as illustrated in Figure  1 . It is well-known that the generalization performance of an ensemble prediction from several classifiers is usually superior to the performance of a single classifier  [9] . This principle is adopted for training an ensemble composed of weak classifiers using a small amount of labelled data, and continually re-training the classifiers with plenty of unlabelled data through the ensemble predictions  [10] . As a result, a semi-supervised system can progressively improve its ability to solve a given task  [11] . However, the majority of ensemble-based systems are composed of several independent classifiers, which leads to an unnecessary great redundancy of low-level features processing, and consequently, high computational cost  [12] .\n\nIn this paper, we propose an ensemble based convolutional network for continually learning facial expressions through semi-supervised learning. Rather than traditional ensembles composed of several independent classifiers, the proposed ensemble consists of strong low-level feature extractors sharing convolutional branches for high-level feature extraction and classification designed as a single architecture. The use of shared representations results in a substantial drop in redundancy of low-level features processing, which reduces the computational cost of the ensemble, and makes the proposed approach suitable for social robots.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Approach",
      "text": "Our approach relies on the hierarchical representational nature of convolutional neural networks (CNNs)  [2]  to design an ensemble architecture where early layers are strong lowlevel feature extractors, and their feature maps are shared with an ensemble of N weak independent classifiers designed as branches of convolutional layers followed by fullyconnected layers for specific high-level feature extraction and classification, as shown in Figure  2 . Using shared representational layers for low-level feature extraction rather than fully independent architectures results in a significant drop of redundant filters for detecting oriented lines, edges, colors, and textures, as well as simple parts of objects, especially for combinatorial-based methods like CNNs as investigated in our previous work  [13] . In contrast to ensemble methods that apply hand-engineered features and several basic classifiers like SVMs  [12] , our approach is not limited only to the generalization of different classification boundaries. Instead, it can adapt its representational space by re-training the shared layers, taking into account the learning signal from each high-level branch in the ensemble, leading to both novel features and novel classification boundaries.\n\nAiming to leverage unlabelled facial expressions that a social robot could gather during interactions with people for continually improving its automatic facial expression recognition system, the proposed architecture is trained in a semisupervised learning setting. Firstly, we use a limited number of labelled samples for training the shared representational convolutional layers as strong low-level feature detectors, and each branch as a weak classifier of the ensemble. Subsequently, the trained network is ready to continually learn facial expressions by re-training the entire architecture using ensemble predictions from unlabelled samples. These two phases are detailed in the following sections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Training With Labelled Samples",
      "text": "The first step of a semi-supervised learning setting based on ensemble predictions is to adopt a learning strategy that drives each weak classifier to develop specific representations from the labelled training data, such that the weak classifiers complement each other when exposed to unlabelled data. Otherwise, no additional knowledge can be acquired if all the weak classifiers succeed and fail on the same samples.\n\nTherefore, we foster the development of different representations by separately training each of N convolutional branches with a different perspective of the labelled training data. For each training epoch, we iteratively train a given branch b with a random portion from the labelled samples. Additionally, we also apply a random form of data augmentation before feeding the network to ensure that even if the same sample is presented to different convolutional branches, each branch processes a different version of such a sample.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Re-Training With Unlabelled Samples",
      "text": "In this phase, the proposed architecture can be continually trained in parallel, i.e. as a single architecture, using ensemble predictions from unlabelled samples. The target output for a given unlabelled facial expression is obtained by a voting scheme, where each branch classification c b computes one vote of a voting vector v. Rather than taking the most voted category as target output, we apply the softmax function in order to re-train the network with a softer target probability distribution. According to Hinton et al.  [14] , a soft target usually carries more distributional information about the task than a hard target output (e.g. one-hot output vector) when the data structure presents similarities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiments And Results",
      "text": "Our objective is to propose a solution that is able to continually learn facial expression from unlabelled samples, therefore, in our experiments we are interested not only in reporting the overall generalization performance achieved by the proposed approach on a given dataset, but also on analysis of its learning behaviour when re-training with unlabelled facial expressions from unseen subjects. For instance, the latter experiment gives us evidence on how a social robot could progressively improve its facial expression recognition ability using images collected during humanrobot interactions. In the following sections, we describe the datasets of facial expressions of emotion, the adopted methodology, the proposed architecture, the parameters used for training and re-training our approach, and finally, our results in multiple experimental conditions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Datasets",
      "text": "Our results are reported on two datasets of facial expressions of emotion exemplified in Figure  3 , the wellestablished Extended Cohn-Kanade (CK+) dataset  [15]  and the Karolinska Directed Emotional Faces (KDEF) dataset  [16] . Both of the datasets are categorized in terms of discrete emotions based on the universal facial expressions according to Ekman  [17] : Anger (An), Contempt (Co), Disgust (Di), Fear (Fe), Happiness (Ha), Sadness (Sa) and Surprise (Su), as well as neutral faces (Ne). In the KDEF dataset, however, the contempt category is not presented.\n\nThe CK+ dataset contains sequences of images from 123 subjects eliciting facial expressions, starting from a neutral face. In our experiments, we use the first frame as the neutral category and the last three frames as one of the seven emotional categories for training the network as a frame-based classifier. The KDEF dataset consists of static images from 70 subjects taken from different angles, however, only frontal face images are used in order to extend our experiments using CK+. Each subject provided two samples for each category.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Methodology",
      "text": "We adopt the subject-independent 10-fold cross-validation as our experimental methodology on the CK+ dataset. With this methodology, we ensure that the subjects selected to compose the test set are different from the ones in the validation and training sets. To populate each fold, initially, we sort the subjects in ascending order according to the subject's id provided by the dataset. Subsequently, with a step size of ten, all of the images from a given subject are sampled. As a result, each fold has around 12 subjects and 130.8 facial expression samples on average. In each trial, we pick Fold-t for the test set, and Fold-t + 1 for the validation set. Among the remaining eight folds, the first four folds are used as labelled samples and the last four folds as unlabelled samples. Thus, if Fold-9 is selected for testing, then Fold-10 is selected for validation, the folds from 1 to 4 are considered as labelled samples for the first training phase, and the folds from 5 to 8 are considered unlabelled samples for re-training the proposed approach with the ensemble predictions.\n\nAll of the samples from KDEF are reserved for investigating the learning behaviour when a different data distribution is used for re-training the proposed approach with unlabelled facial expressions. This experiment better represents realistic HRI scenarios, where a network is trained in advance using a public dataset and deployed in a real robot. Afterwards, the network should be able to improve facial expression recognition from facial expressions captured in different conditions, such as camera resolution, background, and so forth. Finally, the faces from both of the datasets are cropped by the Viola and Jones's algorithm for face detection  [18] , and re-scaled to 96 by 96 pixels.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Architecture Details",
      "text": "After assessing multiple architectures derived from the work of Khorrami et al.  [19]  on the CK+ dataset to define a baseline, we fixed the overall configuration and only varied the number of branches that compose the ensemble in order to examine its influence on the generalization performance. As presented in Figure  2 , the low-level shared representations layers are three convolutional layers with 32, 64 and 64 filters each, and a filter size of 5 by 5 and stride of 1. On top, each high-level representational branch is composed of one convolutional layer with 32 filters, filter size of 5 by 5 and stride of 1, followed by a fully-connected layer with 30 neurons and the output layer with 8 neurons. A max-pooling layer of 2 by 2 with a stride of 2 follows each convolutional layer. Batch-normalization is applied to regularize the convolutional layers, allowing the use of higher learning rates to increase training speed  [20] , which is a desirable property for on-line learning. As activation function, we adopted the hyperbolic tangent function except for the softmax function at the output layer.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Training Details",
      "text": "In the training phase using labelled data, our goal is to aid each convolutional branch to learn different and, more importantly, complementary representations from the training data. Therefore, we apply random forms of data augmentation to decrease the chance of the exact same input image being presented to different branches. The data augmentation consists of rescaling, translation, rotation, pixel intensity augmentation, and blurring. We train the network with the stochastic gradient descent (SGD) with a learning rate of 0.001, a decay of 10 -5 and a momentum of 0.9.\n\nIn the re-training phase using unlabelled samples, the entire architecture is trained in parallel as a single architecture using SGD with a learning rate ten times smaller than in the previous phase and a decay of 2 × 10 -5 . A learning rate equal or higher than in the previous phase had caused a critical forgetting problem, where facial expressions correctly recognized by the network became to be misclassified.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Overall Generalization Performance On Ck+",
      "text": "First, the proposed approach is evaluated in terms of overall generalization performance by varying the number As expected, the lowest average recognition rate has been reached by the standard CNN with one branch, confirming that an ensemble of classifiers generally leads to a better generalization performance than a single classifier. The retraining with unlabelled samples has increased the average recognition rates on both of the validation and test sets for an ensemble with five branches to 87.92% and 86.85%, respectively, but with a relatively high standard deviation. Therefore, to verify if there is a statistical difference after retraining the network with unlabelled samples, we performed a paired t-test which has given us a p-value of 0.1.\n\nTable  II  shows the recognition rate obtained by the ensemble predictions for each trial, from which we can identify individual cases of improvement and decrement. In five of ten trials, re-training the network with unlabelled data has increased the recognition rate on the test set by more than one percent. Among them, three show a significant improvement of more than five percent. In the unsuccessful cases, two of ten trials have decreased the recognition rate by more than one percent. This analysis gives evidence that the use of unlabelled facial expressions for re-training the proposed architecture could lead to significant improvement on accuracy, which is suitable for a social robot that is continually interacting with people during social events.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "F. Learning Behaviour Analysis",
      "text": "To analyse the ensemble's impact on learning, we repeated the experiment for the best (Fold-8) and worst (Fold-7) cases over the ten trials from the previous experiment. In the best trial, the network has presented a significant improvement on generalization performance after the re-training phase and the highest recognition rate on the test set. Figure  4  shows the average recognition rate curves (i.e. the labelled training, validation and test sets) after 10 runs using Fold-8 for the test on CK+ during the re-training phase with ensemble predictions. Therefore, the origin of the graph corresponds to the recognition rate of the ensemble prediction immediately after the training phase with labelled data.\n\nObserve that the generalization performance on CK+ has improved for both the validation and test sets, and the forgetting problem has not affected the network in this fold. Moreover, the learning behaviour is consistent over the 10 trials, by keeping the same improvement behaviour with a low standard deviation. Note that, all of the branches have improved the generalization performance on CK+ through the ensemble prediction, as shown by the confusion matrix for each branch and the ensemble classification in Figure  5 .\n\nThrough the analysis of the confusion matrices, we are able to investigate what promotes the improvement on generalization performance and how each branch's expertise complements each other. Let's consider branch 3 before the re-training phase: it has achieved the best recognition performance for the fear expressions due to its high truepositive and low false-positive rates for the classification of such expression. In other words, branch 3 correctly classifies six fear facial expressions from a total of six on the test set and misclassified no other expressions as fear. The same explanation is valid for branch 2, which is the best branch for sadness compared to the other branches of the ensemble.\n\nThese findings suggest that the improvement in the overall generalization performance on CK+, in this case, has been caused by the high complementary expertise between these branches. While a given branch is a strong classifier for a given category such as fear but weaker for sadness, another branch can provide the complementary expertise. This yields to an improvement on generalization performance on Fold-8 by re-training the network with additional unlabelled data through ensemble predictions. Branch 5 becomes a stronger classifier for fear facial expressions, likely throwing a correct teaching signal from the branch 3 on such expression. Finally, the overall generalization performance has improved from 93.10% to 98.28% after the re-training phase.\n\nIn the worst trial (using Fold-7 for the test set), both of the validation and test sets have decreased around 2% on average, and the forgetting problem subtly affected the network during the re-training phase, as shown in Figure  6 . Despite such undesirable behaviour, after 20 epochs the generalization performance on the test set stops degenerating and stays in a stationary condition.\n\nExamining the confusion matrix from the ensemble predictions before the re-training phase in Figure  7 , the significantly high number of facial expressions misclassified as neutral indicates a strong bias of the ensemble. The bias for the neutral category becomes even stronger after the retraining phase, where strong branches for disgust and sad facial expressions might have been encouraged to wrongly",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "G. Continually Learning Facial Expressions On Kdef",
      "text": "In general, re-training the proposed architecture with unlabelled data from unseen subjects has shown to be beneficial by improving overall generalization performance on CK+, however, it is also necessary to investigate the learning behaviour using facial expressions from a different data distribution. It is especially needed for facial expression recognition since people usually express themselves in many different ways  [21] , as well as for evaluating the proposed approach using images recorded in a different setting, which happens in real HRI where images are captured by the robot's camera. These differences can be observed in Figure  3 .\n\nFigure  8  shows the average recognition rate as a function of epoch using facial expressions from the KDEF dataset for re-training the ensemble. The large recognition gap between CK+ and KDEF is expected when a trained network is tested in a different data distribution. Nevertheless, the learning behaviour on KDEF is stable and consistent with a continually boost on generalization performance over epochs on KDEF, low standard deviation after ten trials, and an increment of around 4% on average at the end. Finally, although the forgetting problem affected the network for expressions from CK+, our major goal is to increase recognition performance towards people who interact with a social robot instead of keeping generalization performance on a specific dataset, which is simulated in this experiment by using a dataset with a different data distribution.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Conclusions And Future Work",
      "text": "A social robot capable of continually learning facial expressions from its previous knowledge using unlabelled facial expressions collected during daily HRIs could enhance its emotional responses towards people who share the same environment with such a robot. In order to address this problem, we proposed an ensemble approach with shared representations based on convolutional networks. Using shared representational layers, we decreased the number of redundant low-level overall processing compared to traditional ensembles composed of independent classifiers, which make our ensemble approach suitable for robotic platforms with limited computational resources.\n\nBy re-training the proposed approach with unlabelled samples from unseen people, we improved the generalization performance on the CK+ and KDEF datasets. Analysing the confusion matrices of the ensemble, we demonstrated that the more divergent the branches' expertise, the higher is the improvement on generalization performance. However, at least one branch has to succeed in the prediction of a given sample for an effective learning. In psychology, this is called desirable difficult examples, where a learner has sufficient knowledge to succeed. As a result, the accomplished task stimulates learning, comprehension, and remembering  [22] .\n\nAs future work, strategies for increasing the ensemble diversity will be explored in order to prevent degeneration on generalization performance. Our promising results motivate us to extend the proposed architecture to deal with multimodal inputs including audio and gestures for two main reasons: (i) the processing of multimodal inputs usually leads to improvement on emotional expression recognition  [23] , and (ii) a different view from the same output task can assist the learning process by constraining the problem as suggested by Mitchell et al.  [11] . In the accompanying video (https://goo.gl/qFXhcK), we demonstrate how our approach can be used to improve recognition performance under realworld conditions. However, the progressive learning behaviour of the proposed approach will also be investigated in an HRI scenario using a NAO robot as illustrated in Figure  1 , where it should give the most appropriate response according to facial expressions, and continually improve its emotion recognition capability using unlabelled facial expressions captured from close communications with humans.\n\nV. ACKNOWLEDGEMENT",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An HRI illustration where a NAO robot will continually learn facial",
      "page": 1
    },
    {
      "caption": "Figure 1: It is well-known that the generalization perfor-",
      "page": 1
    },
    {
      "caption": "Figure 2: Using shared represen-",
      "page": 2
    },
    {
      "caption": "Figure 3: , the well-",
      "page": 2
    },
    {
      "caption": "Figure 2: The proposed approach consists of convolutional layers for low-level feature extraction (on the left) shared with an ensemble of multiple",
      "page": 3
    },
    {
      "caption": "Figure 3: Fe, An, Di, Ha, Sa, and Su facial expressions from the CK+ (on",
      "page": 3
    },
    {
      "caption": "Figure 2: , the low-level shared representations",
      "page": 3
    },
    {
      "caption": "Figure 4: Fold-8 on CK+. Average recognition rate curves during the re-",
      "page": 4
    },
    {
      "caption": "Figure 5: Through the analysis of the confusion matrices, we are",
      "page": 4
    },
    {
      "caption": "Figure 6: Despite such undesirable behaviour, after 20 epochs the",
      "page": 4
    },
    {
      "caption": "Figure 7: , the signif-",
      "page": 4
    },
    {
      "caption": "Figure 5: Confusion matrices with the recognition rates for each branch prediction in the best learning behaviour case (Fold-8) using the ensemble of 5",
      "page": 5
    },
    {
      "caption": "Figure 6: Fold-7 on CK+. Average recognition rate curves during the re-",
      "page": 5
    },
    {
      "caption": "Figure 3: Figure 8 shows the average recognition rate as a function",
      "page": 5
    },
    {
      "caption": "Figure 7: Confusion matrices with the recognition rates in the worst learning",
      "page": 5
    },
    {
      "caption": "Figure 8: Average recognition rate curves on CK+ and KDEF during the",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Shared Representations Ensemble\nBranch 1\nBranch N\n96x96x1 32@5x5 Max Max 64@5x5 Max\nPooling 64@5x5 Pooling Pooling 32@5x5 Max\n30 8\nPooling": "",
          "Ensemble": "",
          "Column_3": "N\nx5 Max\n30 8\nPooling",
          "Column_4": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "3\n3(US)",
          "99.67±0.21": "99.67±0.19\n99.66±0.28",
          "83.67±5.45": "85.31±4.86\n86.69±4.70",
          "82.38±4.99": "85.09±3.96\n84.99±3.40"
        },
        {
          "1": "5\n5(US)",
          "99.67±0.21": "99.81±0.17\n99.76±0.18",
          "83.67±5.45": "86.23±5.53\n87.92±4.82",
          "82.38±4.99": "84.75±5.72\n86.58±5.62"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Folds": "5Branches(US)",
          "1": "85.60",
          "2": "82.69",
          "3": "86.25",
          "4": "82.14",
          "5": "89.16",
          "6": "77.02",
          "7": "86.2",
          "8": "0 98.28",
          "9": "93.18 85"
        },
        {
          "Folds": "5Branches",
          "1": "82.57",
          "2": "83.33",
          "3": "86.25",
          "4": "82.14",
          "5": "87.50",
          "6": "70.27",
          "7": "88.7",
          "8": "9 93.10",
          "9": "87.12 86"
        },
        {
          "Folds": "Difference",
          "1": "+3.03",
          "2": "-0.64",
          "3": "0.00",
          "4": "0.00",
          "5": "+1.66",
          "6": "+6.75",
          "7": "-2.5",
          "8": "8 +5.17",
          "9": "+6.06 -1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "94.83%\ntegraT": "",
          "Column_2": "95.69% (U",
          "9": "S)",
          "Column_4": "93",
          "1.38%": ".10% (U",
          "Column_6": "S) 9",
          "Column_7": "",
          "93.97%": "3.97% (US",
          "Column_9": ")",
          "Column_10": "",
          "93.10%": "95.69% (US",
          "90.5": ") 95.69",
          "Column_13": "",
          "2%": "% (US)",
          "Column_15": "98.28%(US)",
          "Column_16": ""
        },
        {
          "94.83%\ntegraT": "tegraT",
          "Column_2": "",
          "9": "",
          "Column_4": "",
          "1.38%": "",
          "Column_6": "",
          "Column_7": "",
          "93.97%": "",
          "Column_9": "",
          "Column_10": "",
          "93.10%": "",
          "90.5": "",
          "Column_13": "",
          "2%": "",
          "Column_15": "",
          "Column_16": ""
        },
        {
          "94.83%\ntegraT": "",
          "Column_2": "",
          "9": "",
          "Column_4": "",
          "1.38%": "",
          "Column_6": "",
          "Column_7": "",
          "93.97%": "",
          "Column_9": "Prediction",
          "Column_10": "",
          "93.10%": "",
          "90.5": "",
          "Column_13": "",
          "2%": "",
          "Column_15": "",
          "Column_16": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "79%": "Prediction",
          "Column_3": "",
          "85.34%": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An ensemble with shared representations based on convolutional networks for continually learning facial expressions",
      "authors": [
        "H Siqueira",
        "P Barros",
        "S Magg",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "2",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "3",
      "title": "Facial and vocal expressions of emotion",
      "authors": [
        "J Russell",
        "J.-A Bachorowski",
        "J.-M Fernández-Dols"
      ],
      "year": "2003",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "4",
      "title": "How context influences our perception of emotional faces: A behavioral study on the kuleshov effect",
      "authors": [
        "M Calbi",
        "K Heimann",
        "D Barratt",
        "F Siri",
        "M Umiltà",
        "V Gallese"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "5",
      "title": "Emoreact: A multimodal approach and dataset for recognizing emotional responses in children",
      "authors": [
        "B Nojavanasghari",
        "T Baltrušaitis",
        "C Hughes",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM ICMI, ser. ICMI 2016"
    },
    {
      "citation_id": "6",
      "title": "Learn2smile: Learning non-verbal interaction through observation",
      "authors": [
        "W Feng",
        "A Kannan",
        "G Gkioxari",
        "C Zitnick"
      ],
      "year": "2017",
      "venue": "RSJ IROS"
    },
    {
      "citation_id": "7",
      "title": "Teaching emotion expressions to a human companion robot using deep neural architectures",
      "authors": [
        "N Churamani",
        "M Kerzel",
        "E Strahl",
        "P Barros",
        "S Wermter"
      ],
      "year": "2017",
      "venue": "Teaching emotion expressions to a human companion robot using deep neural architectures"
    },
    {
      "citation_id": "8",
      "title": "Continual lifelong learning with neural networks: A review",
      "authors": [
        "G Parisi",
        "R Kemker",
        "J Part",
        "C Kanan",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "Continual lifelong learning with neural networks: A review",
      "arxiv": "arXiv:1802.07569"
    },
    {
      "citation_id": "9",
      "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
      "authors": [
        "Y Freund",
        "R Schapire"
      ],
      "year": "1997",
      "venue": "Journal of Computer and System Sciences"
    },
    {
      "citation_id": "10",
      "title": "Combining labeled and unlabeled data with co-training",
      "authors": [
        "A Blum",
        "T Mitchell"
      ],
      "year": "1998",
      "venue": "Proceedings of the Eleventh Annual Conference on Computational Learning Theory, ser. COLT' 98"
    },
    {
      "citation_id": "11",
      "title": "Never ending learning",
      "authors": [
        "T Mitchell",
        "W Cohen",
        "E Hruschka",
        "P Talukdar",
        "J Betteridge",
        "A Carlson",
        "B Mishra",
        "M Gardner",
        "B Kisiel",
        "J Krishnamurthy"
      ],
      "year": "2015",
      "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Ensemble of exemplarsvms for object detection and beyond",
      "authors": [
        "T Malisiewicz",
        "A Gupta",
        "A Efros"
      ],
      "year": "2011",
      "venue": "2011 International Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Understanding how deep neural networks learn face expressions",
      "authors": [
        "N Mousavi",
        "H Siqueira",
        "P Barros",
        "B Fernandes",
        "S Wermter"
      ],
      "year": "2016",
      "venue": "2016 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "14",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "15",
      "title": "The Extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE CVPRW"
    },
    {
      "citation_id": "16",
      "title": "The Karolinska Directed Emotional Faces (KDEF)",
      "authors": [
        "D Lundqvist",
        "A Flykt",
        "A Öhman"
      ],
      "year": "1998",
      "venue": "Dept. of Clinical Neuroscience"
    },
    {
      "citation_id": "17",
      "title": "The argument and evidence about universals in facial expressions",
      "authors": [
        "P Ekman"
      ],
      "year": "1989",
      "venue": "The argument and evidence about universals in facial expressions"
    },
    {
      "citation_id": "18",
      "title": "Robust real-time face detection",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2004",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "19",
      "title": "Do deep neural networks learn facial action units when doing expression recognition?",
      "authors": [
        "P Khorrami",
        "T Paine",
        "T Huang"
      ],
      "year": "2015",
      "venue": "IEEE ICCVW"
    },
    {
      "citation_id": "20",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "21",
      "title": "Solving the emotion paradox: Categorization and the experience of emotion",
      "authors": [
        "L Barrett"
      ],
      "year": "2006",
      "venue": "Personality and Social Psychology Review"
    },
    {
      "citation_id": "22",
      "title": "Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning",
      "authors": [
        "E Bjotk",
        "R Bjork"
      ],
      "year": "2011",
      "venue": "Psychology and the Real World: Essays Illustrating Fundamental Contributions to Society"
    },
    {
      "citation_id": "23",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    }
  ]
}