{
  "paper_id": "2203.00236v2",
  "title": "Trillsson: Distilled Universal Paralinguistic Speech Representations",
  "published": "2022-03-01T05:22:57Z",
  "authors": [
    "Joel Shor",
    "Subhashini Venugopalan"
  ],
  "keywords": [
    "speech",
    "representations",
    "on-device",
    "paralinguistic speech"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advances in self-supervision have dramatically improved the quality of speech representations. However, deployment of state-of-the-art embedding models on devices has been restricted due to their limited public availability and large resource footprint. Our work addresses these issues by publicly releasing a collection of paralinguistic speech models 1 that are small and near state-of-the-art performance. Our approach is based on knowledge distillation, and our models are distilled on public data only. We explore different architectures and thoroughly evaluate our models on the Non-Semantic Speech (NOSS) benchmark. Our largest distilled model is less than 15% the size of the original model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7 tasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB) and achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the open source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model outperforms the open source Wav2Vec 2.0 on both emotion recognition tasks despite being 7% the size.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Self-supervised learning for audio has improved the quality of speech representations, resulting in huge performance gains on downstream tasks  [1, 2, 3] . However, a few obstacles prevent these representations from being widely adopted on devices, particularly for paralinguistic speech tasks which focus on aspects of speech other than the textual meaning. First, recent self-supervised models are often extremely large, making them challenging to use in resource-constrained environments like mobile phones (e.g. HuggingFace's Wav2Vec 2.0  [3]  at 360 MB). Second, due to the private nature of most speech data, high-performance models are often not publicly released (e.g. CAP12  [4] ). In this work, we overcome both constraints using public data and knowledge distillation  [5] . Specifically, we knowledge distill a recent state-of-the-art paralinguistic speech representation \"Conformer Applied to Paralinguistics\" (CAP12  [4] ) into a series of models, which we call TRILLsson.\n\nOur approach primarily relies on \"knowledge distillation\"  [5] . We train small student models on several fixedlength input architectures, including ResNets, EfficientNets, and Transformers, to match the arbitrary-length input CAP12 (teacher) Transformer embeddings. Our architectures explore the model size versus performance tradeoff. To our knowledge, this is the first successful cross-architecture distillation from a Transformer to non-Transformer model using a different dataset.\n\nWe use nearly 58K hours of publicly available speech data from Libri-light  [6]  and Audioset  [7]  for distillation. For evaluation, we use the \"Non-Semantic Speech Benchmark\" (NOSS)  [8] . NOSS includes 7 tasks such as emotion recognition and speaker identification that require slow-time features. Additionally, we demonstrate the superior performance of TRILLsson models by comparing them with existing publicly available representations such as TRILL  [8]  and Wav2Vec2.0  [3] . Our contributions are:\n\n1. Create generally-useful paralinguistic models that are small enough to run on-device.\n\n2. Demonstrate successful cross-architecture knowledge distillation from Transformers to fixed-context convolutional networks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "3.",
      "text": "Publicly release models at different points of the model size and performance trade-off curve.\n\n4. Identify the best paralinguistic representation in the public Wav2Vec2.0 model and demonstrate that our models outperform it.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Related Works",
      "text": "Self-supervised representation learning has shown remarkable success in vision  [9]  and speech recognition  [3] . The Wav2Vec2.0  [3]  and Conformer models  [10]  are most relevant to our work.  [3]  was one of the first frameworks to successfully combine Transformers  [11]   layers, producing further improvements in semi-supervised speech recognition applications  [1] . Recently,  [4]  developed Conformer-based models (CAP12) that created representations for non-ASR speech analysis and paralinguistics tasks. Linear models on time-averaged CAP12 representations performed at or above state-of-the-at across several tasks simultaneously. However, as with most self-supervised models, their resource footprint (memory and compute) makes them less suitable for on-device applications. In this work, we distill the CAP12 model from  [4]  to several \"lite\" architectures for use on mobile devices, and we release them publicly. Distillation  [5]  has been popular for transferring knowledge from large models to smaller ones. We distill the Conformer model to a variety of smaller, fixed-length input architectures that have been used in audio classification, such as ResNets  [7] , EfficientNets  [12] , and ASTs  [13] . Other works have applied distillation to speech representations  [14] , but to our knowledge this work is the first successful speech embedding distillation from arbitrary-length input Transformers to fixed-length input models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Approach",
      "text": "Shor et. al.  [4]  demonstrated that the 1024 dimension representation of the 12th layer of the CAP Conformer model (referred to as \"CAP12\") achieved at or near state-of-theart performance across all tasks in the paralinguistic Non-Semantic Speech Benchmark (NOSS)  [8] . CAP12 is a 606M parameter (2.2GB) Conformer model trained via a modified Wav2Vec2.0 self-supervised training loss on a 900M+ hour speech dataset derived from YouTube (YT-U  [1] , Tab. 1). In this work, we use CAP12 as the teacher and distill this model to several \"lite\" architectures based on the teacher-student distillation approach  [5] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Student Architectures.",
      "text": "We explore 3 different student architectures of varying sizes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio Spectrogram Transformer (Ast) [13] Is A",
      "text": "Transformer-based model for audio classification. We train student models with different depths and widths.\n\n2. EfficientNetv2  [12]  was designed by neural architecture search on image classification. The architecture is mobile friendly. Different versions of this architecture vary in terms of depths and filters.\n\n3. Resnetish  [7]  are modified ResNet-50 architectures designed to take audio spectral features as input. Different versions of this architecture include different depths and different number of filters per layer.\n\nBased on the Figure  1  lower in  [4]  and the fact that some benchmark datasets have audio that's mostly less than 3 seconds (Tab. 2) we focus on student architectures that process 2 seconds of audio at a time. When operating on audio less than 2 seconds, we symmetrically pad the audio around the end.\n\nFor the frontend, we use window length of 25ms and a hop length of 10ms. We use log-magnitude mel-frequency spectrograms with 80 bins ranging from 125 Hz to 7500 Hz. The frame width for all models is 2 seconds of audio, and we treat the frame advance between successive model patches as a hyperparameter. Since the model sees audio of exactly 2 seconds during training (see Sec 3.2), the frame advance hyperparameter can be explored after the model is fully trained.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Distillation Targets: Global Vs Local Matching",
      "text": "There are two paradigms for generating targets from a teacher that borrow ideas from distilling large vision models  [24] . In both cases the audio is first chunked into context windows which the student processes. Then the student is trained to match a target embedding generated using the teacher model. In \"global matching\", the target is the average teacher embedding from the entire un-chunked audio clip. In the \"local matching\" paradigm, the target is generated by averaging the teacher's output on the same context window that the student sees. In this work, we focus on \"local matching.\"\n\nTable  3 : Test performance on the NOSS Benchmark and extended tasks. \"Prev. SoTA\" are usually domain specific, but all other rows are linear models on time-averaged input. TRILLsson model sizes are shown without frontends. ↑ indicates higher values are better, and ↓ indicates lower is better. † We use a filtered subset of Voxceleb1 according to YouTube's privacy guidelines. We omit previous SoTA results on this dataset, since they used the entire dataset. * * ASVSpoof uses equal error rate  [18] . We report the best single-model performance (as compared to model ensembles). # Euphonia is the only non-public dataset. We use a larger dataset than was reported on in  [4] . * We use the public Wav2Vec 2.0 model from Hugging Face  [",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Datasets",
      "text": "We perform distillation on two open source datasets which together contain about 58K hours of speech data (Tab. 1).\n\nAudioset  [25]  clips are collection from YouTube, so it represents a variety of settings and acoustic environments. We use the speech subset of this data, which yields approximately 5K hours. Libri-light  [6]  contains 60k hours of audio derived from open-source audio books in the LibriVox project. It is the largest publicly available, unlabeled semi-supervised dataset to date. We note that the CAP12 teacher model  [4]  is trained on YT-U  [1] , which is a 900K hour dataset derived from YouTube.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Representation Evaluation",
      "text": "We compare and evaluate the distilled representations on several tasks including detection of speaker, language, command, synthetic speech, dysarthria, and emotion. Table  2  provides an overview of the 7 datasets these tasks come from  [8, 4, 26] .\n\nAs depicted in Fig  1 , for each (model, eval dataset) pair, we first generate the candidate embeddings for the train, dev, and test splits. We then train three types of linear models on the train set embeddings. We take the model that performs best on the dev set, and report the model's performance on the test set as the score for that (model, eval dataset) pair.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Aggregation Metric: Equivalent D-Prime (D )",
      "text": "To fairly compare with previous results, we report on the typical performance metric for each dataset (accuracy or equal error rate). However, to aggregate performance into a single scalar, for the purpose of comparing embedding quality, we average the \"equivalent d-prime\" (d ) metric, defined as follows:\n\nwhere \"AUC\" is the \"Area Under the Receiver Operating Characteristic Curve\" (ROC AUC), and \"Z()\" is the inverse Cumulative Distribution Function for the standard normal distribution. d is better suited to aggregate performance on multiple datasets for two reasons. First, ROC AUC and d , unlike accuracy, take into account performance at various levels along the accuracy/recall curve. This makes it more reflective of the overall performance of a particular representation. Second, unlike ROC AUC, d doesn't saturate in the highly performant regime. Thus, 1 unit of d is in some sense more equivalent, so averaging d values across datasets is more natural than averaging AUC values. We use the average d scores to sort and select models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Models For Comparison",
      "text": "To contextualize performance on the NOSS benchmark, we compare our models to 1) previous state-of-the-art (SoTA) results, 2) publicly available speech representation models, and 3) CAP12 with different input sizes. Previous SoTA models are been mostly domain-specific. The public models we compare to are:\n\n1. Hugging Face's Wav2Vec 2.0  [3, 22] : This model was trained on the approximately 1K hour Librispeech  [27] .\n\nWe use the TensorFlow model from the Hugging Face library. We compute the performances for each layer, and find that layer 6 of 11 performs the best overall.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Trill [8]",
      "text": ": A Resnet triplet-loss model trained on Au-dioSet. We access this model from TensorFlow Hub.\n\n3. YAMNet  [7] : This is a supervised audio classification network, also trained on AudioSet. We use layer 19 as in  [8] . We access this model through TensorFlow Hub.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Table  3  shows the sizes and performances of 5 TRILLsson models as compared to CAP12 and other publicly available models. We see that the two largest TRILLsson models outperform all publicly available models on all datasets except Wav2Vec2 layer 6 on speech commands, and that even TRILLsson1, the smallest model, outperforms previously available embeddings on emotion recognition tasks. In particular, TRILLsson2 outperforms the best Wav2Vec 2.0 representation on 5 of 6 public tasks despite being less than 12% the size.\n\nCompared to CAP12, the distilled models maintain performance on language identification and dysarthria detection. TRILLsson models suffer minor degradation on speech emotion recognition tasks, and major degradation on speaker identification, speech commands, and fake speech detection.\n\nFig  2  shows the relationship between size and performance across different architectures. We see that Resnetish models perform best at the low end, EfficientNets perform best at sizes between 40-230MB, and AST performs best when larger then 230MB.\n\nTraining on AudioSet improves performance on emotion recognition tasks: Using a dependent t-test on paired samples between models trained on Libri-light and AudioSet versus just Libri-light, we observed statistically significant improvements in dev and test set accuracies on the speech emotion recognition tasks (CREMA-D and IEMOCAP, n = 86, p < 0.03). Interestingly, training on the combined dataset hurt performance on ASVSpoof2019 test set compared to either dataset on its own (n = 86, p < 10 -3 ), but the paired t-test did not reject the null hypothesis on the ASVSpoof2019 dev set.\n\nBest model curve robust to removing a single benchmark task: Removing any single task keeps the size versus are the best models with any single task removed. TRILLs-son3 and 2 appear in every optimal curve. TRILLsson1 appears in every optimal curve unless 'voxforge' is removed. d orderings: Table  4  justifies our choice of d on the dev set as our ordering criteria. The Kendall rank coefficient, a correlation statistic for different orderings, shows that average d on the dev sets more closely correlates to average accuracy and average d than average accuracy on the dev set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we demonstrate that it's possible to distill huge models trained on large datasets to obtain much smaller models that perform well on paralinguistic speech tasks. The distillation uses only 7% of the training data and is entirely from public sources. The models we obtain are between 22MB and 314MB, and achieve between 90% and 96% of the larger CAP12 accuracy on 6 of 7 tasks. These models are between 1% and 15% the size of the original model. We release the model to allow the research community to benefit from the practical applications of self-supervised representations for paralinguistic speech.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Depiction of the evaluation process.",
      "page": 2
    },
    {
      "caption": "Figure 1: lower in [4] and the fact that some",
      "page": 2
    },
    {
      "caption": "Figure 1: , for each (model, eval dataset) pair, we",
      "page": 3
    },
    {
      "caption": "Figure 2: Comparison of “average d′” vs. “model size” for various student model architectures and sizes. Performance in this ﬁgure is across test sets, although",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the relationship between size and perfor-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 3: shows the sizes and performances of 5 TRILLsson",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "ast\neffic\nresn\nCAP",
          "Column_4": "ientn\netish\n12",
          "Column_5": "etv",
          "Column_6": "2b",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "H\nH\nT\nY",
          "Column_11": "F w2v2 layer\nF w2v2 layer\nRILL\nAMNet",
          "Column_12": "6\n12",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "BigSSL: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition",
      "authors": [
        "Y Zhang"
      ],
      "year": "2021",
      "venue": "BigSSL: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition"
    },
    {
      "citation_id": "3",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "A Baevski"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for selfsupervised learning of speech representations"
    },
    {
      "citation_id": "5",
      "title": "Universal paralinguistic speech representations using self-supervised conformers",
      "authors": [
        "J Shor"
      ],
      "year": "2021",
      "venue": "Universal paralinguistic speech representations using self-supervised conformers"
    },
    {
      "citation_id": "6",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "NIPS Deep Learning and Representation Learning Workshop"
    },
    {
      "citation_id": "7",
      "title": "Libri-Light: A benchmark for ASR with limited or no supervision",
      "authors": [
        "J Kahn"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey"
      ],
      "year": "2017",
      "venue": "Cnn architectures for large-scale audio classification"
    },
    {
      "citation_id": "9",
      "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
      "authors": [
        "J Shor"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020",
      "doi": "10.21437/Interspeech.2020-1242"
    },
    {
      "citation_id": "10",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen"
      ],
      "year": "2020",
      "venue": "ICML, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "11",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati"
      ],
      "venue": "Conformer: Convolution-augmented transformer for speech recognition"
    },
    {
      "citation_id": "12",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "13",
      "title": "Efficientnetv2: Smaller models and faster training",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2021",
      "venue": "Efficientnetv2: Smaller models and faster training"
    },
    {
      "citation_id": "14",
      "title": "Ast: Audio spectrogram transformer",
      "authors": [
        "Y Gong",
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "Ast: Audio spectrogram transformer"
    },
    {
      "citation_id": "15",
      "title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices",
      "authors": [
        "J Peplinski"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "16",
      "title": "Voxceleb: a large-scale speaker identification dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: a large-scale speaker identification dataset",
      "arxiv": "arXiv:1706.08612"
    },
    {
      "citation_id": "17",
      "title": "Ken MacLean",
      "authors": [
        "K Maclean"
      ],
      "year": "2018",
      "venue": "Ken MacLean"
    },
    {
      "citation_id": "18",
      "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition",
      "authors": [
        "P Warden"
      ],
      "year": "2018",
      "venue": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition"
    },
    {
      "citation_id": "19",
      "title": "Asvspoof 2019: Future horizons in spoofed and fake audio detection",
      "authors": [
        "M Todisco"
      ],
      "year": "2019",
      "venue": "Asvspoof 2019: Future horizons in spoofed and fake audio detection"
    },
    {
      "citation_id": "20",
      "title": "Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia",
      "authors": [
        "R Macdonald"
      ],
      "year": "2021",
      "venue": "Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia"
    },
    {
      "citation_id": "21",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "22",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "23",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "24",
      "title": "Wav2kws: Transfer learning from speech representations for keyword spotting",
      "authors": [
        "D Seo",
        "H.-S Oh",
        "Y Jung"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "25",
      "title": "Emerging properties in self-supervised vision transformers",
      "authors": [
        "M Caron"
      ],
      "venue": "Proceedings of the International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "26",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "27",
      "title": "Comparing Supervised Models and Learned Speech Representations for Classifying Intelligibility of Disordered Speech on Selected Phrases",
      "authors": [
        "S Venugopalan"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}