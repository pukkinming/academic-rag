{
  "paper_id": "2402.13589v1",
  "title": "Affective Computing For Healthcare: Recent Trends, Applications, Challenges, And Beyond",
  "published": "2024-02-21T07:43:18Z",
  "authors": [
    "Yuanyuan Liu",
    "Ke Wang",
    "Lin Wei",
    "Jingying Chen",
    "Yibing Zhan",
    "Dapeng Tao",
    "Zhe Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing, which aims to recognize, interpret, and understand human emotions, provides benefits in healthcare, such as improving patient care and enhancing doctor-patient communication. However, there is a noticeable absence of a comprehensive summary of recent advancements in affective computing for healthcare, which could pose difficulties for researchers entering this field. To address this, our paper aims to provide an extensive literature review of related studies published in the last five years. We begin by analyzing trends, benefits, and limitations of recent datasets and affective computing methods devised for healthcare. Subsequently, we highlight several healthcare application hotspots of current technologies that could be promising for real-world deployment. Through our analysis, we identify and discuss some ongoing challenges in the field as evidenced by the literature. Concluding with a thorough review, we further offer potential future research directions and hope our findings and insights could guide related researchers to make better contributions to the evolution of affective computing in healthcare.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing refers to the process of utilizing computer and artificial intelligent technologies to analyze and recognize human emotions  [Hossain and Muhammad, 2019] . Human emotions, the intuitive reflections of the body's state, can convey useful information for treatment processes. By accurately identifying patient emotions through affective computing, doctors can obtain more informative patient statistics and provide more appropriate or customized treatments for patients. As a result, affective computing could play a pivotal role in aiding doctors with various tasks like mental health monitoring, personalized treatment, and patient support. Due to promising applications, affective computing for healthcare has attracted increasing attention  [Alelaiwi, 2019; Yildirim-Celik et al., 2022; Chen and Luo, 2023] .\n\nIn particular, thanks to the significant modeling capabilities brought by deep learning advancements, affective com-puting has shown outstanding capacities of representing and identifying various human-related information  [Rejaibi et al., 2019a; Smith et al., 2020] . Diverse deep learning models, such as Convolutional Neural Networks (CNN), Recurrent Neural Network (RNN), and Transformer, have been introduced to extract emotional features from either unimodal or multimodal data, resulting in accurate emotion recognition and analysis. Meanwhile, breakthroughs in affective computing have also led to impressive progress in complex healthcare scenarios and applications. Despite promising progress, we found that existing related reviews mostly summarise the affective computing techniques in a relatively narrow application area like depression recognizing  [Giuntini et al., 2020] . We believe that the lack of a comprehensive and insightful review of affective computing in healthcare may create unnecessary barriers for researchers to enter/understand the field.\n\nTo bridge the gap mentioned above and summarize the progress achieved in recent deep learning-related advancements, we perform a comprehensive literature review of the recent progress in affective computing for healthcare in this paper. An overview of our work is shown in Figure  1 . In general, we attempt to thoroughly review the recently published datasets and approaches, covering a broad area related to affective computing for healthcare. The contributions of this paper and differences from other surveys are as follows:\n\n1) We identify 3 major research directions based on methodologies, including behavior-based, physiologicalbased, and behavior-physiological-based research. Under this taxonomy, we attempt to perform a comprehensive analysis of the recent development trends of datasets and approaches, illustrating their details, benefits, as well as their limitations.\n\n2) We highlight a few most frequently focused medical application directions of affective computing, i.e., depression diagnosis  [Othmani et al., 2021] , autism recognition and intervention  [Li et al., 2021] , pain level recognition  [Phan et al., 2023] , and other related medical applications  [Ayata et al., 2020] , establishing synergies between affective computing and clinical applications.\n\n3) In addition to existing achievements, we present significant challenges that still pose difficulties for developing and applying affective computing in healthcare. Despite the challenges, we also conclude potential directions for future research, hoping to provide valuable insights for researchers who would like to contribute to this field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets And Approaches",
      "text": "We first perform a literature review on affective computing research, mainly covering the related datasets and approaches applied in healthcare in the last five years. Tables  1  and 2  list the overview of databases and typical approaches, respectively. We find that affective computing research varies according to the type of data utilized. Hence, we classify related research into three categories: behavior data-based, physiological data-based, and behavior-physiological data-based research. Each category of research is detailed below.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Behavior Data-Based Research",
      "text": "In the realm of healthcare, behavior data-based research entails employing affective computing technology to analyze patients' emotions through their behavior data, such as language, facial expressions, and medical records. Datasets Behavioral affective datasets can be discussed based on three categories: vision-based datasets, non-visionbased unimodal datasets, and multimodal datasets, depending on their popularity and diverse modalities.\n\nVision-based datasets are the mainstream dataset format, partly due to the widespread applications of vision techniques. Current visual affective databases are generally datasets with patients' facial expressions. TIAD collected thermal imaging of the faces of autistic children  [Melinda et al., 2023] , FEMP collected 11,000 facial images of newborns  [Yan et al., 2020] , and PEMF consists of 272 microclips with facial images  [Fernandes-Magalhaes et al., 2023] . Through the analysis of facial expressions, important emotional clues can be obtained to help doctors judge the emotional state of patients and improve the diagnosis of disease.\n\nNon-vision unimodal datasets contain text or audio data modalities. Text: D4 records the conversations between doctors and patients during depression diagnosis. The diagnosis results and symptom summaries given in each conversa-tion  [Yao et al., 2022] . Audio: SAD is an audio dataset of English depression that contains 64 recordings of individuals with and without depression  [Alghifari et al., 2023] .\n\nMultimodal datasets primarily combine two or three modalities of text, audio, and visual data, providing richer cues to patients' emotional behaviour, considering the emotional bias inherent in single behavioral data. D-vlog comprises 961 vlogs collected from YouTube that encompass both audio and videos  [Yoon et al., 2022] . CALMED consists of audio and video features extracted from conference transcript files of children diagnosed with autism  [Sousa et al., 2023] . MMDA is the largest mental disorder dataset including visual, auditory, and textual data, where all subjects in MMDA are diagnosed by professional psychologists using de-identified original interview videos  [Jiang et al., 2022] .\n\nApproaches As shown in Table  2 , deep learning models have become the dominant approaches for affective computing in recent years and are gradually evolving from unimodal approaches towards multimodal approaches.\n\nVision-based affective approaches primarily employ CNN and attention methods to extract emotional features from facial images and videos, identifying emotion categories or scores. Typical methods include:  [Liu et al., 2023]  built a facial expression recognition model based on the self-attention mechanism for depression detection.  [Lu et al., 2023]   that sounds reasonable but is not faithful or meaningless.\n\nAudio-based unimodal affective approaches usually adopt CNN and LSTM to capture the long-term dependence and high-level affective feature representation of audio sequences.  [Han et al., 2023]  developed a self-supervised learning framework that combines integrate causal and dilated convolution to continuously enlarge the receptive domain for capturing multi-scale emotion-contextual information, and employed a hierarchical contrast loss to predict depression by exploring the long temporal emotion dependencies of audio.\n\nMultimodal affective approaches have emerged as a trend in healthcare affective computing. This approach combines visual, audio, and text signals to more accurately identify patients' emotions, surpassing the results of unimodal affective analysis. For example,  [Li et al., 2021]  used pre-trained ResNet-18 and ResNet-50 to construct a two-stage network for children autism prediction from facial expression videos and audio.  [Wang et al., 2023b ] first used the pre-trained models ALBERT and AGG16 to construct a visual-language model to extract facial expression image features, text, and behavioral features, and then used the early fusion method to fuse these features to form multimodal features for depression degree detection. [Zhao and  Wang, 2022]  proposed a cross-modal attention mechanism-based Generative Adversarial Network, and used an attention-based fusion approach to integrate facial expression videos, text, and audio for automatically depression severity assessment.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Physiological Data-Based Research",
      "text": "Physiological data-based affective computing research focuses on integrating physiological signals into computational models for robust sentiment analysis. Physiological signals record and measure an individual's affective state, affective experience, or affective response, such as Electroencephalography (EEG), offering a more objective reflection of the genuine emotional state  [Bajestani et al., 2019] .\n\nDatasets As shown in Table  1 , physiological affective datasets mainly contain emotion-related physiological signals, i.e., EEG, Electrodermal activity (EDA), and other related signals (e.g., Electrocardiogram (ECG), Electromyography (EMG), Heart Rate (HR), Eye-tracking (ET), Accelerometer (ACC), Inter-Beat Interval (IBI), Blood Vol-ume Pulse (BVP), and Skin Temperature (ST)). They also can be divided into unimodal physiological datasets and multimodal physiological datesets. DDLES, an EEG-based depression recognition dataset, is a typical unimodal physiological dataset that contains EEG signals of 60 depressed and non-depressed individuals  [Mohammadi et al., 2019] . SRDR is a multimodal physiological dataset that monitors specific physiological variables and synchronously collects EEG and EM signals from subjects to provide a more accurate detection dataset in a clinical environment  [Zhu et al., 2019] . Since the acquisition of physiological signals requires expensive specialised medical equipment, most of the existing relevant datasets are small.\n\nApproaches Physiological-based affective analysis methods usually use CNN and RNN to process temporal physiological data, which can extract the dynamic time-frequency features of physiological signals, facilitating a better understanding of dynamic affective states. Depending on the signal data, these methods also can be classified into two categories: unimodal approaches and multimodal approaches. Uimodal approaches focus on mining emotional states from one type of emotional physiological signal.  [Xia et al., 2023]  is a typical unimodal approach that uses multi-head selfattention mechanism and double-branch CNN to construct an end-to-end model for depression recognition from single EEG signals. In contrast, multimodal approaches explore multiple types of physiological signals in the emotion recognition process, thus improving the prediction accuracy.  [Phan et al., 2023]  first used CNN and BiLSTM to extract the lowlevel features and time information in the sequence from ECG and EDA signals, and then combined ECG and EDA features in an early fusion manner for pain recognition.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Behavior-Physiological Data-Based Research",
      "text": "Due to the directness and interpretability of behavioural data and the high objectivity of physiological data, the research integrating both behavior and psychological data is a natural derivation for the application in healthcare.\n\nDatasets Behavior-Physiological datasets integrate physiological data like EEG, EDA, and EMG with behavioral data such as facial expressions and audio. This integration aims to capture more comprehensive emotion-related cues of patients. M-MS is a multimodal autism emotion dataset, containing ECG and therapy videos to support the study of synchronization in autism recognition  [Calabrò et al., 2021] . PPAD is the first multimodal neonatal pain dataset containing facial expression videos, sound, and physiological responses (vital signs and cortical activity), which was collected from 58 neonates during their hospitalization in the neonatal intensive care unit  [Salekin et al., 2021] . Approaches Behavior-Physiological affective analysis needs to mine both the external manifestations of behavioural data and the internal changes of physiological signals, aiming to achieve more accurate affective analysis performance. For example,  [Han et al., 2022]  proposed a multimodal diagnostic framework that uses an early fusion approach combining EEG and ET data for unsupervised training and supervised fine-tuning to identify children with Autism Spectrum Disorder (ASD). Recently,  [Qayyum et al., 2023]  employed ViT, CNN, and LSTM models for the extraction of audio and EEG features. Subsequently, these features were concatenated by early fusion to improve the diagnostic performance of depression.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Discussion",
      "text": "Tendency In general, in the healthcare field, in contrast to unimodal technology, there is a growing research trend towards multimodal affective computing technology that analyzes both behavior and physiological signals. This allows for the exploration of both external manifestations of behavioral data and internal changes in physiological signals. Moreover, affective computing approaches have also shifted from early CNN-based models to Transfomer-based models, and then to hybrid large-scale ones, which can better focus on sentiment changes across various modality sequences. Both facilitate a more comprehensive analysis of a patient's emotional state, assisting doctors in tasks such as mental health monitoring, personalized treatment, and patient support. Limitation Despite the progress made, we find that there are still limitations in the current technology, mainly including: For datasets: Behavioural datasets are easy to collect but subjective and deceptive, and physiological datasets are highly reliable but costly and difficult to collect. As a result, existing multimodal fusion datasets remain limited in size, hindering their ability to fully support complex healthcare and medical applications; For approaches: As shown in Table  2 , although higher recognition rates are achieved in current studies, most existing affective models are trained and designed independently on smaller and more limited set datasets. This lack of interoperability makes it challenging to reuse these models across different healthcare systems and applications.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Applications On Healthcare",
      "text": "In this section, the affective approaches for healthcare applications are presented. Affective computing holds significant potential in various healthcare applications, offering new techniques for diagnosis, therapy, and treatment of emotionrelated diseases by identifying patients' emotions. We collect the studies of affective computing in healthcare applica-  tions that has been proposed in the last five years, and Figure  2  provides the statistics. In general, the application of affective computing technology in the field of healthcare involves depression diagnosis, autism recognition, pain level recognition, elderly dementia monitoring, stress monitoring, and intelligent medical systems. For simplicity, we review the top three most popular healthcare applications related to brain disorders with representative methods, including depression diagnosis, autism recognition, and pain level recognition. These applications can cover the major populations of current brain disorders, including children, adults, and the elderly. After the review of 3 popular applications, we also provide a brief review of the rest of the applications.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Depression Diagnosis",
      "text": "Depression, a widespread mental disorder, results from a complex interplay of social, psychological, and biological factors, leading to prolonged periods of low mood or reduced interest. Traditional self-report-based diagnosis is subjective and prone to inaccuracies, often resulting in delays in treatment. To tackle this, affective computing technology is applied for depression diagnosis, improving diagnosis objectivity and accuracy. Currently, affective computing-based depression diagnosis involves two categories: classificationbased and regression-based depression diagnosis.\n\nAffective classification-based depression diagnosis aids healthcare workers and doctors in distinguishing between individuals with and without depression.  [Alsharif et al., 2022]  used mel-frequency cepstral coefficients to extract patients' audio features and CNN to build a classification model to detect depression in Arabic audio data.  [Cai et al., 2020a]  employed KNN as a depression classification detection model to distinguish depression patients from normal people by integrating different EEG data obtained under neutral, negative, and positive audio stimulation.  [Wang et al., 2023b]  proposed a multi-modal depression detection model with emotional knowledge graph, which integrated text, facial expressions, and other behaviors (e.g., the number of user posts, blog length, etc.) to address the depression detection task.  [Pan et al., 2023]    severity of depressive symptoms in patients.  [Zhou et al., 2020]  proposed a deep regression network called DepressNet with facial depression data to predict depression degree. This significantly improves the latest performance of visual-based complex depression recognition. [Zhao and  Wang, 2022]  employed a cross-modal affective regression model to facilitate the learning of more accurate multimodal representations from text, audio, and facial expression videos for automatic depression severity assessment.\n\nClinical Application With significant advancements in affective computing technology, depression diagnosis is now being employed in clinical monitoring. To prevent the recurrence of patients with depression,  [Yin et al., 2022]  designed an intelligent monitoring system based on a hybrid affective computing model (namely CNN-LSTM), aiming to provide recurrence monitoring for patients with depression within their home and daily environments. Additionally, it can be applied to assess new patients. The system includes user input, depression testing, intelligent monitoring, and connectivity to external wearables like dedicated voice acquisition devices and EEG devices. It also supports communication with online doctors and integration with external systems.\n\nLimitation Overall, depression diagnosis with affective computing is evolving from rough classification to continuous regression analyses, for dynamic clinical monitoring. However, it still faces challenges including subjectivity and heterogeneity of depression, longitudinal monitoring, and comprehensive assessment integrating genetic, psychosocial, and environmental factors. In addition, current affective com-puting technologies rely mainly on behavioural and physiological indicators while lacking clear biological indicators to support an objective diagnosis of depression, affecting diagnostic accuracy.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Autism Recognition And Intervention",
      "text": "Autism, a neurodevelopmental disorder, is typically characterized by emotional and social difficulties. Early diagnosis is crucial for facilitating intervention and treatment. Affective computing can aid experts in promptly assessing the emotional state of individuals with autism during interactions, enhancing diagnostic accuracy and positive emotional interventions. As a result, affective computing-based autism diagnosis and intervention are gaining increasing attention.\n\nCurrently, autism diagnosis primarily employs emotion classification models to identify Autism spectrum disorder (ASD).  [Negin et al., 2021]  proposed a non-invasive visual assistance method with human action classification to facilitate the diagnosis of autism.  [Rahman and Booma, 2022]  employed the MobileNet to detect childhood autism through their facial expressions in a transfer learning manner.  [Wei et al., 2023]  proposed a lightweight, conventional classification model to recognize autism-related behaviors in facial videos.  [Han et al., 2022]  build a stacked denoising autoencoder to identify ASD in children from EEG and ET fusion data.  [Li et al., 2021]  combined audio and facial expression images to diagnose ASD in children, and used ResNet50 and ResNet18 as classifiers for a two-stage emotion classification, thus improving the accuracy of autism diagnosis Clinical Application In clinical applications of autism intervention, leveraging affective computing technology enhances virtual reality (VR) for human-computer interaction (HCI) scenarios addressing challenges caused by a shortage of autism treatment professionals.  [Manju et al., 2023]  employ VR-assisted system combined with wearable multimodel sensing technologies, to collect physiological signals and game performance data during HCI training. Then, it employs a machine learning model to identify ASD children, assessing the diagnosis, severity, social behavioral intervention, and treatment of ASD with multiple assessment scales.\n\nLimitation Despite some progress in autism diagnosis and intervention, current approaches are still in their infancy and face challenges such as data privacy concerns in children, difficulties in data collection, assessment complexity, and emotion model bias in autism. In addition, Autism may have other co-existing mental health disorders, such as attention deficit hyperactivity disorder (ADHD), complicating affective computing models for accurate diagnosis.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Pain Level Recognition",
      "text": "Pain is the body's intricate physical and psychological emotional response to underlying injury or illness. Accurate identification of pain is crucial in medicine, enabling medicine professionals to formulate effective treatment for enhancing a patient's quality of life. Research on pain level recognition through affective computing has been a prominent and challenging issue. The field of pain level recognition employs affective computing techniques for classification and regression tasks, aimed at diagnosing pain patients and obtaining their pain intensity, respectively.\n\nAffective classification-based pain level recognition can assist medical professionals in accurately determining the patient's pain location, thereby significantly optimizing consultation time. For instance,  [Vallez et al., 2022]  identified joint pain from facial expression images with the help of a pretrained CNN classification model.  [Chen et al., 2022]  used multi-layer CNN classifies the EEG singals in resting and pain states during daily activities.  [Lu et al., 2023]  identified newborns' pain levels based on their facial expression videos with the help of Softmax. Similar to  [Lu et al., 2023] ,  [Phan et al., 2023]  employed Softmax to recognition pain levels from EDA and ECG signals.\n\nTo obtain a continuous numerical output for the pain degree of patients, pain level recognition based on affective regression is developed.  [Thiam et al., 2020]  employed feedforward neural networks as a regression model for discerning pain level intensity based on emotion-related physiological signals (i.e., EDA, EMG, and ECG). Besides,  [Jiang et al., 2024]  used a non-linear neural network with Sigmoid for both classification and regression tasks, distinguishing between pain and non-pain in patients and detecting pain intensity based on ECG, EDA, and ECG.\n\nClinical Application In a clinical setting, pain analysis is critical to a patient's recovery.  [Ghosh et al., 2023]  proposed an emotion analysis system based on deep learning and statistical learning, to analyze facial expressions images of patients for detecting pain levels of patients. In addition, the system has the ability to perform pain detection and recognition on resource-constrained devices, which provides a strong support for the intelligent healthcare field.\n\nLimitation Pain level recognition has developed from initial classification of pain versus non-pain to the recognition of pain intensity, from clinical diagnosis to real-time monitoring. However, pain recognition and assessment still faces significant challenges such as individual differences, emotional differences, standard calibration, and so on. In addition, some pain symptoms may lack distinct behavioural and physiological features, further complicating identification.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Other Related Healthcare Applications",
      "text": "Affective computing has also been applied to other clinical applications such as Bipolar disorder  [Baki et al., 2022] , elderly companionship and monitoring  [Meng et al., 2021] , smart medicine  [Ayata et al., 2020]  etc. For convenience, we combine some of them into one section for overview.\n\nBipolar disorder is a mental health disorder that causes mood swings ranging from depression to mania.  [Baki et al., 2022 ] created a multimodal decision system for three level mania classification based on recordings of patients' audio, text, and facial expression videos. Elderly monitoring and dementia diagnosis are crucial in an aging population, especially during illness. To accurately monitor the emotion state of the elderly,  [Meng et al., 2021]  introduced an emotion-aware medicine monitoring system based on brain waves. Intelligent medicine systems offer an alternative to doctor shortages.  [Ayata et al., 2020]  proposed an emotion recognition-based intelligent medicine system for emotional care by collecting and analyzing multiple physiological signals from patients.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Challenges And Opportunities",
      "text": "Despite breakthroughs, several challenges remain, yet there are also related opportunities for future development.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Patient Data Privacy And Ethics",
      "text": "Challenge Data privacy issues have been well-known in the big data era, and it is particularly important for the healthcare sector. This might be attributed to the fact that patients have to share extremely sensitive information about their own bodies. As a result, the privacy of patient clinical data in affective computing is a crucial ethical concern, especially for children's information. Ensuring confidentiality involves implementing robust measures in data transmission, storage, and usage. The challenge is to retain critical emotion-related information while adhering to ethical and moral regulations. Therefore, a proper balance needs to be found to ensure privacy and the effectiveness of data analysis.\n\nOpportunities Some potential future opportunities may include the exploration of advanced privacy-protecting techniques such as federated learning  [Rieke et al., 2020]  and secure multi-party computing  [Liu et al., 2020] . Federated learning allows for model training without sharing raw data, and aggregating models without exposing individual data. Secure multi-party computing allows calculations to be performed between multiple parties while maintaining the pri-vacy of the data. These technologies can ensure that patients' clinical data is adequately protected.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emotion Bias And Fairness In Clinical Data",
      "text": "Challenge In clinical and other healthcare settings, the collection and annotation of sentiment data exhibit a natural bias due to population sentiment expression heterogeneity and annotator subjectivity. Different from data bias issues in common large datasets, the data from healthcare suffers from low data volume and larger diversities of biased factors. For example, environment, age, occupation, and race can all affect the expression and the labelling of emotions  [Liu et al., 2022] . Furthermore, preferences and concerns during data collection and labeling also vary significantly among doctors. These factors could lead to high emotion biases and unfairness in training AI models. As a result, it is needed to address and reduce the bias for a fair and unbiased understanding of emotions across diverse populations. Opportunities To lower the biases in affective computing under limited labelled databases, some research is exploring unsupervised/self-supervised learning algorithms to reduce reliance on affective labels. These algorithms can learn emotional representations from unlabeled data, reducing the need for large-scale labeled datasets  [Han et al., 2023; Han et al., 2022] . Furthermore, the introduction of domain adaptive learning techniques can improve the generality of affective computing models and mitigate affective biases between different cultures and demographic groups.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fine-Grained Health-Related Emotions",
      "text": "Challenge Most existing healthcare applications rely on single, simplistic affective models like six-class or three-class emotion classification models  [Ameer et al., 2023] . These models fall short of simulating rich emotions from real patients who may be undergoing complicated treatments, making it challenging for doctors to make accurate judgments. As a result, developing fine-grained health-related emotion models for clinical applications remains a key unresolved issue. Opportunities Recently, several researchers have proposed composite face expression models based on linguistic descriptions  [Liu et al., 2022] . We believe that this approach facilitates the description of changes in emotional details, thereby guiding doctors to make more informed medical diagnoses. Consequently, constructing multimodal fine-grained emotion models in healthcare applications emerges as a future development direction.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Real-Time Diagnosis With Affective Computing",
      "text": "Challenge Some medicine applications demand real-time emotional analysis and diagnosis, involving the rapid processing and analysis of large data sets while ensuring accurate emotion recognition, such as mental health monitoring or emergency response systems. Developing efficient algorithms and infrastructure for real-time processing without compromising accuracy is a significant challenge. Opportunities Exploring adaptive algorithms and edge computing systems facilitates the capability of real-time emotional analysis with minimal latency. Adaptive algorithms can be directly adjusted and optimized according to different needs to improve the efficiency and accuracy of sentiment analysis. Edge computing systems can discretize computing tasks to edge devices, reducing latency in data transmission and processing for faster real-time sentiment analysis.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Large Foundation Model-Related Applications",
      "text": "Challenge With the development of visual-linguistic large foundation models, such as  GPT-4 [Rathje et al., 2023] , the significance of large foundation models has been demonstrated across various application domains. Consequently, constructing an affective large foundation model specifically for healthcare could be beneficial to enhance a wider range of clinical applications. However, the acquisition and annotation of specified large foundation models for healthcare and clinical data remain challenging, posing a significant hurdle in the development of healthcare affective foundation model. Opportunities Leveraging existing visual-language foundation models to construct the affective large-scale model through transfer learning and cross-modal prompt learning could reduce the dependence on large amounts of training data, thus enhancing the reusability of these models for diverse application tasks  [Liu et al., 2024] . This method can not only improve the effect and generalization ability of affective models but also provide a common basis for emotion recognition in different domains and tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper provides a comprehensive survey of the application of affective computing in the field of healthcare. Specifically, we provide an overview of the developments in affective computing for healthcare, covering behavior databased, psychological data-based, and behavior-psychological data-based datasets and approaches. Next, we introduce key healthcare applications, highlighting the top three most frequently used, as well as other related applications. Finally, we summarize the most potential challenges and opportunities in the development of affective computing in healthcare. We believe that this review helps to provide academic and industrial researchers with a comprehensive understanding of the latest advances in affective computing-based healthcare and provides them with guidance.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework of learning-based affective computing for healthcare.",
      "page": 2
    },
    {
      "caption": "Figure 2: The statistic distribution of publications on various health-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Central China Normal University, China": "and 4Yunnan University, China"
        },
        {
          "2Central China Normal University, China": "5The School of Computing, Engineering and Mathematical Sciences, La Trobe University, Australia."
        },
        {
          "2Central China Normal University, China": "puting has shown outstanding capacities of representing and"
        },
        {
          "2Central China Normal University, China": "identifying various human-related information [Rejaibi et al.,"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "2019a; Smith et al., 2020]. Diverse deep learning models,"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "such as Convolutional Neural Networks\n(CNN), Recurrent"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "Neural Network (RNN), and Transformer, have been intro-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "duced to extract emotional features from either unimodal or"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "multimodal data,\nresulting in accurate emotion recognition"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "and analysis. Meanwhile, breakthroughs in affective comput-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "ing have also led to impressive progress in complex health-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "care scenarios and applications. Despite promising progress,"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "we found that existing related reviews mostly summarise the"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "affective computing techniques in a relatively narrow applica-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "tion area like depression recognizing [Giuntini et al., 2020]."
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "We believe that the lack of a comprehensive and insightful re-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "view of affective computing in healthcare may create unnec-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "essary barriers for researchers to enter/understand the field."
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "To bridge the gap mentioned above and summarize the"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "progress achieved in recent deep learning-related advance-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "ments, we perform a comprehensive literature review of the"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "recent progress in affective computing for healthcare in this"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "paper. An overview of our work is shown in Figure 1. In gen-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "eral, we attempt\nto thoroughly review the recently published"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "datasets and approaches, covering a broad area related to af-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "fective computing for healthcare.\nThe contributions of\nthis"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "paper and differences from other surveys are as follows:"
        },
        {
          "2Central China Normal University, China": "1) We\nidentify\n3 major\nresearch\ndirections\nbased\non"
        },
        {
          "2Central China Normal University, China": "methodologies,\nincluding\nbehavior-based,\nphysiological-"
        },
        {
          "2Central China Normal University, China": "based, and behavior-physiological-based research. Under this"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "taxonomy, we attempt\nto perform a comprehensive analysis"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "of the recent development trends of datasets and approaches,"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "illustrating their details, benefits, as well as their limitations."
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "2) We highlight a few most frequently focused medical ap-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "plication directions of affective computing,\ni.e., depression"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "diagnosis [Othmani et al., 2021], autism recognition and in-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "tervention [Li et al., 2021], pain level\nrecognition [Phan et"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "al., 2023], and other\nrelated medical applications [Ayata et"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "al., 2020], establishing synergies between affective comput-"
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "ing and clinical applications."
        },
        {
          "2Central China Normal University, China": ""
        },
        {
          "2Central China Normal University, China": "3)\nIn addition to existing achievements, we present\nsig-"
        },
        {
          "2Central China Normal University, China": "nificant challenges that still pose difficulties for developing"
        },
        {
          "2Central China Normal University, China": "and applying affective computing in healthcare. Despite the"
        },
        {
          "2Central China Normal University, China": "challenges, we also conclude potential directions for\nfuture"
        },
        {
          "2Central China Normal University, China": "research, hoping to provide valuable insights for researchers"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 2: , deep learning models",
      "data": [
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "Figure 1: Framework of learning-based affective computing for healthcare."
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "who would like to contribute to this field.\ntion [Yao et al., 2022]. Audio: SAD is an audio dataset of"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "English depression that contains 64 recordings of individuals"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "with and without depression [Alghifari et al., 2023].\n2\nDatasets and Approaches"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "primarily\ncombine\ntwo\nor\nthree\nMultimodal datasets"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "We first perform a literature review on affective computing re-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "modalities of\ntext, audio, and visual data, providing richer"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "search, mainly covering the related datasets and approaches"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "cues to patients’ emotional behaviour, considering the emo-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "applied in healthcare in the last five years.\nTables 1 and 2"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "tional bias inherent\nin single behavioral data. D-vlog com-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "list the overview of databases and typical approaches, respec-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "prises 961 vlogs collected from YouTube that encompass both"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "tively. We find that affective computing research varies ac-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "audio and videos\n[Yoon et al., 2022].\nCALMED consists"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "cording to the type of data utilized. Hence, we classify related"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "of audio and video features extracted from conference tran-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "research into three categories: behavior data-based, physio-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "script files of children diagnosed with autism [Sousa et al.,"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "logical data-based, and behavior-physiological data-based re-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "2023]. MMDA is the largest mental disorder dataset\ninclud-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "search. Each category of research is detailed below."
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "ing visual, auditory, and textual data, where all subjects in"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "MMDA are diagnosed by professional psychologists using\n2.1\nBehavior Data-based Research"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "de-identified original interview videos [Jiang et al., 2022]."
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "In the realm of healthcare, behavior data-based research en-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "tails employing affective computing technology to analyze"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "Approaches\nAs\nshown in Table 2, deep learning models"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "patients’ emotions through their behavior data, such as lan-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "have become the dominant approaches for affective comput-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "guage, facial expressions, and medical records."
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "ing in recent years and are gradually evolving from unimodal"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "approaches towards multimodal approaches.\nDatasets\nBehavioral\naffective\ndatasets\ncan\nbe\ndiscussed"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "Vision-based affective approaches primarily employ CNN\nbased on three categories: vision-based datasets, non-vision-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "and attention methods to extract emotional features from fa-\nbased unimodal datasets, and multimodal datasets, depending"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "cial\nimages\nand videos,\nidentifying emotion categories or\non their popularity and diverse modalities."
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "scores. Typical methods include:\n[Liu et al., 2023] built a fa-\nformat,\nVision-based datasets are the mainstream dataset"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "cial expression recognition model based on the self-attention\npartly due\nto the widespread applications of vision tech-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "mechanism for depression detection.\n[Lu et al., 2023] used a\nniques.\nCurrent\nvisual\naffective\ndatabases\nare\ngenerally"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "two-stream CNN network with cross-stream attention mecha-\ndatasets with patients’\nfacial expressions.\nTIAD collected"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "nism to integrate spatial and temporal information in newborn\nthermal\nimaging of\nthe faces of autistic children [Melinda"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "facial expression videos for pain level recognition.\net al., 2023], FEMP collected 11,000 facial\nimages of new-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "borns [Yan et al., 2020], and PEMF consists of 272 micro-\nlan-\nText-based unimodal affective approaches use natural"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "clips with facial\nimages [Fernandes-Magalhaes et al., 2023].\nguage processing technology to analyze text data to identify"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "Through the analysis of\nfacial expressions,\nimportant emo-\nemotions. Typical methods are as follows:\n[Dessai and Us-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "tional clues can be obtained to help doctors judge the emo-\ngaonkar, 2022] used CNN and Long Short Term Memory"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "tional state of patients and improve the diagnosis of disease.\n(LSTM) to analyze Twitter users’ tweets to discover factors"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "contain text or\naudio data\nNon-vision unimodal datasets\nrelated to depression.\nIn [Ji et al., 2023], the authors utilized"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "modalities. Text: D4 records the conversations between doc-\nLarge Language Models’ (LLM) interactivity and multitask-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "tors and patients during depression diagnosis.\nThe diagno-\ning ability to investigate “hallucination” in medical question-"
        },
        {
          "Affective Databases\nApplications\nAffective Computing Approaches": "sis results and symptom summaries given in each conversa-\nanswering systems. That is,\nthe model produces information"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: The most recent affective computing datasets in the medicine field. Depre, Nondepre, Anx, and Nonanx are abbreviations for",
      "data": [
        {
          "Class": "",
          "Datasets": "FENP [Yan et al., 2020]",
          "#Subjects": "106",
          "#Samples": "11000",
          "Modalities": "Visual",
          "Applications": "Pain",
          "Labels": "Pain, No Pain"
        },
        {
          "Class": "",
          "Datasets": "PersionSIChASD [Alizadeh and Tabibian, 2021]",
          "#Subjects": "38",
          "#Samples": "418",
          "Modalities": "Audio",
          "Applications": "Autism",
          "Labels": "Autistic, Normal, Phonetic units"
        },
        {
          "Class": "",
          "Datasets": "D4 [Yao et al., 2022]",
          "#Subjects": "201",
          "#Samples": "1339",
          "Modalities": "Text",
          "Applications": "Depression",
          "Labels": "Risk, Non Risk"
        },
        {
          "Class": "",
          "Datasets": "MMDA [Jiang et al., 2022]",
          "#Subjects": "962",
          "#Samples": "1025",
          "Modalities": "Text, Audio, Visual",
          "Applications": "Depre,Anx",
          "Labels": "Depre,Nondepre,Anx,Nonanx"
        },
        {
          "Class": "Behaviour",
          "Datasets": "D-Vlog [Yoon et al., 2022]",
          "#Subjects": "816",
          "#Samples": "961",
          "Modalities": "Audio, Visual",
          "Applications": "Depression",
          "Labels": "Depre, Nondepre"
        },
        {
          "Class": "",
          "Datasets": "SAD [Alghifari et al., 2023]",
          "#Subjects": "64",
          "#Samples": "64",
          "Modalities": "Audio",
          "Applications": "Depression",
          "Labels": "Depre, Nondepre"
        },
        {
          "Class": "",
          "Datasets": "TIAD [Melinda et al., 2023]",
          "#Subjects": "34",
          "#Samples": "6120",
          "Modalities": "Visual",
          "Applications": "Autism",
          "Labels": "Autistic, Normal"
        },
        {
          "Class": "",
          "Datasets": "PEMF [Fernandes-Magalhaes et al., 2023]",
          "#Subjects": "68",
          "#Samples": "272",
          "Modalities": "Visual",
          "Applications": "Pain",
          "Labels": "Pain, No Pain"
        },
        {
          "Class": "",
          "Datasets": "CALMED [Sousa et al., 2023]",
          "#Subjects": "4",
          "#Samples": "57012",
          "Modalities": "Audio, Visual",
          "Applications": "Autism",
          "Labels": "Autistic, Normal"
        },
        {
          "Class": "",
          "Datasets": "DDLES [Mohammadi et al., 2019]",
          "#Subjects": "60",
          "#Samples": "60",
          "Modalities": "EEG",
          "Applications": "Depression",
          "Labels": "Minimal,Mild,Moderate,Severe"
        },
        {
          "Class": "Physiological",
          "Datasets": "",
          "#Subjects": "",
          "#Samples": "",
          "Modalities": "",
          "Applications": "",
          "Labels": ""
        },
        {
          "Class": "",
          "Datasets": "SADR [Zhu et al., 2019]",
          "#Subjects": "39",
          "#Samples": "1170",
          "Modalities": "EEG,EM",
          "Applications": "Depression",
          "Labels": "Depre, Nondepre"
        },
        {
          "Class": "",
          "Datasets": "EMCASD [Duan et al., 2019]",
          "#Subjects": "28",
          "#Samples": "300",
          "Modalities": "Visual, Other",
          "Applications": "Autism",
          "Labels": "Autistic, Normal"
        },
        {
          "Class": "Behaviour",
          "Datasets": "MMOD [Cai et al., 2020b]",
          "#Subjects": "160",
          "#Samples": "160",
          "Modalities": "Audio, EEG",
          "Applications": "Depression",
          "Labels": "Depre, Nondepre"
        },
        {
          "Class": "-Physiological",
          "Datasets": "PPAD [Salekin et al., 2021]",
          "#Subjects": "58",
          "#Samples": "58",
          "Modalities": "Audio, Visual, Other",
          "Applications": "Pain",
          "Labels": "Pain, No Pain"
        },
        {
          "Class": "",
          "Datasets": "M-Ms\n[Calabr`o et al., 2021]",
          "#Subjects": "15",
          "#Samples": "37127",
          "Modalities": "Visual, EEG",
          "Applications": "Autism",
          "Labels": "Good, Poor"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: The most recent affective computing datasets in the medicine field. Depre, Nondepre, Anx, and Nonanx are abbreviations for",
      "data": [
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "Behaviour\nMMOD [Cai et al., 2020b]\n160\n160\nAudio, EEG\nDepression\nDepre, Nondepre"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "-Physiological\nPPAD [Salekin et al., 2021]\n58\n58\nAudio, Visual, Other\nPain\nPain, No Pain"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "M-Ms\n[Calabr`o et al., 2021]\n15\n37127\nVisual, EEG\nAutism\nGood, Poor"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "Table 1: The most\nrecent affective computing datasets in the medicine field. Depre, Nondepre, Anx, and Nonanx are abbreviations for"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "depression, nondepression, anxiety, and nonanxiety, respectively."
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "that sounds reasonable but is not faithful or meaningless.\nume Pulse (BVP), and Skin Temperature (ST)). They also"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "and\ncan\nbe\ndivided\ninto\nunimodal physiological datasets\nAudio-based unimodal affective approaches usually adopt"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "multimodal physiological datesets.\nDDLES, an EEG-based\nCNN and LSTM to capture the long-term dependence and"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "depression recognition dataset,\nis a typical unimodal physio-\nhigh-level affective feature representation of audio sequences."
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "logical dataset that contains EEG signals of 60 depressed and\n[Han et al., 2023] developed a self-supervised learning frame-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "non-depressed individuals [Mohammadi et al., 2019]. SRDR\nwork that combines integrate causal and dilated convolution"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "is a multimodal physiological dataset\nthat monitors specific\nto continuously enlarge the receptive domain for capturing"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "physiological variables and synchronously collects EEG and\nmulti-scale emotion-contextual information, and employed a"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "EM signals from subjects to provide a more accurate detec-\nhierarchical contrast\nloss to predict depression by exploring"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "tion dataset in a clinical environment [Zhu et al., 2019]. Since\nthe long temporal emotion dependencies of audio."
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "the acquisition of physiological\nsignals\nrequires expensive\nMultimodal affective approaches have emerged as a trend"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "specialised medical equipment, most of the existing relevant\nin healthcare affective computing. This approach combines"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "datasets are small.\nvisual,\naudio,\nand text\nsignals\nto more accurately identify"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "patients’ emotions, surpassing the results of unimodal affec-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "Approaches\nPhysiological-based affective analysis meth-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "tive analysis. For example, [Li et al., 2021] used pre-trained"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "ods usually use CNN and RNN to process temporal physi-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "ResNet-18 and ResNet-50 to construct a two-stage network"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "ological data, which can extract\nthe dynamic time-frequency"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "for children autism prediction from facial expression videos"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "features of physiological signals,\nfacilitating a better under-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "and audio.\n[Wang et al., 2023b] first used the pre-trained"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "standing of dynamic affective states. Depending on the sig-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "models ALBERT and AGG16 to construct a visual-language"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "nal data,\nthese methods also can be classified into two cat-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "model\nto extract\nfacial expression image features,\ntext, and"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "egories:\nunimodal approaches and multimodal approaches."
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "behavioral\nfeatures, and then used the early fusion method"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "Uimodal approaches focus on mining emotional states from"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "to fuse these features\nto form multimodal\nfeatures\nfor de-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "one type of emotional physiological signal. [Xia et al., 2023]"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "pression degree detection.\n[Zhao and Wang, 2022] proposed"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "is\na\ntypical unimodal\napproach that uses multi-head self-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "a cross-modal attention mechanism-based Generative Adver-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "attention mechanism and double-branch CNN to construct"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "sarial Network, and used an attention-based fusion approach"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "an end-to-end model\nfor depression recognition from sin-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "to integrate facial expression videos, text, and audio for auto-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "gle EEG signals. In contrast, multimodal approaches explore"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "matically depression severity assessment."
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "multiple types of physiological signals in the emotion recog-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "nition process, thus improving the prediction accuracy. [Phan"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "2.2\nPhysiological Data-based Research"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "et al., 2023] first used CNN and BiLSTM to extract the low-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "Physiological data-based affective\ncomputing research fo-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "level features and time information in the sequence from ECG"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "cuses on integrating physiological signals into computational"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "and EDA signals, and then combined ECG and EDA features"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "models for\nrobust sentiment analysis.\nPhysiological signals"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "in an early fusion manner for pain recognition."
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "record and measure an individual’s affective state, affective"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "experience, or affective response, such as Electroencephalog-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "2.3\nBehavior-Physiological Data-based Research"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "raphy (EEG), offering a more objective reflection of the gen-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "Due to the directness and interpretability of behavioural data\nuine emotional state [Bajestani et al., 2019]."
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "and the high objectivity of physiological data,\nthe research"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "Datasets\nAs\nshown\nin Table\n1,\nphysiological\naffective"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "integrating both behavior and psychological data is a natural"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "datasets mainly contain emotion-related physiological\nsig-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "derivation for the application in healthcare."
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "nals,\ni.e., EEG, Electrodermal activity (EDA), and other re-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "Datasets\nBehavior-Physiological datasets integrate physio-\nlated signals\n(e.g., Electrocardiogram (ECG), Electromyo-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "logical data like EEG, EDA, and EMG with behavioral data\ngraphy (EMG), Heart Rate\n(HR), Eye-tracking (ET), Ac-"
        },
        {
          "EMCASD [Duan et al., 2019]\n28\n300\nVisual, Other\nAutism\nAutistic, Normal": "such as facial expressions and audio. This integration aims\ncelerometer\n(ACC),\nInter-Beat\nInterval\n(IBI), Blood Vol-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Depression": "Autism"
        },
        {
          "Depression": "Pain"
        },
        {
          "Depression": "Sleep deprivation"
        },
        {
          "Depression": ""
        },
        {
          "Depression": "Heart disease"
        },
        {
          "Depression": "Other"
        },
        {
          "Depression": ""
        },
        {
          "Depression": ""
        },
        {
          "Depression": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Autism": "Pain"
        },
        {
          "Autism": "Sleep deprivation"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "Heart disease"
        },
        {
          "Autism": "Other"
        },
        {
          "Autism": ""
        },
        {
          "Autism": ""
        },
        {
          "Autism": ""
        },
        {
          "Autism": "Depression"
        },
        {
          "Autism": "Autism"
        },
        {
          "Autism": "Pain"
        },
        {
          "Autism": "6.30%, 197\n32.01%, 1001"
        },
        {
          "Autism": "Dementia"
        },
        {
          "Autism": "Bipolar disorder"
        },
        {
          "Autism": "3.71%, 116"
        },
        {
          "Autism": "Anxiety"
        },
        {
          "Autism": "Other"
        },
        {
          "Autism": "6.46%, 202"
        },
        {
          "Autism": "28.78%, 900"
        },
        {
          "Autism": "8.44%, 264"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "14.29%, 447"
        },
        {
          "Autism": ""
        },
        {
          "Autism": ""
        },
        {
          "Autism": "Figure 2: The statistic distribution of publications on various health-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "care applications of affective computing from 2019 to 2023 based"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "on Keyword search on web of science. We emphasize the top three"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "most frequently explored applications."
        },
        {
          "Autism": ""
        },
        {
          "Autism": ""
        },
        {
          "Autism": "tions that has been proposed in the last five years, and Fig-"
        },
        {
          "Autism": "ure 2 provides the statistics.\nIn general,\nthe application of"
        },
        {
          "Autism": "affective computing technology in the field of healthcare in-"
        },
        {
          "Autism": "volves depression diagnosis, autism recognition, pain level"
        },
        {
          "Autism": "recognition, elderly dementia monitoring, stress monitoring,"
        },
        {
          "Autism": "and intelligent medical systems.\nFor simplicity, we review"
        },
        {
          "Autism": "the top three most popular healthcare applications related to"
        },
        {
          "Autism": "brain disorders with representative methods,\nincluding de-"
        },
        {
          "Autism": "pression diagnosis, autism recognition, and pain level recog-"
        },
        {
          "Autism": "nition.\nThese applications can cover\nthe major populations"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "of current brain disorders, including children, adults, and the"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "elderly. After the review of 3 popular applications, we also"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "provide a brief review of the rest of the applications."
        },
        {
          "Autism": ""
        },
        {
          "Autism": ""
        },
        {
          "Autism": "3.1\nDepression Diagnosis"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "Depression,\na widespread mental disorder,\nresults\nfrom a"
        },
        {
          "Autism": "complex interplay of\nsocial, psychological,\nand biological"
        },
        {
          "Autism": "factors,\nleading to prolonged periods of\nlow mood or\nre-"
        },
        {
          "Autism": "duced interest. Traditional self-report-based diagnosis is sub-"
        },
        {
          "Autism": "jective and prone to inaccuracies, often resulting in delays"
        },
        {
          "Autism": "in treatment. To tackle this, affective computing technology"
        },
        {
          "Autism": "is applied for depression diagnosis,\nimproving diagnosis ob-"
        },
        {
          "Autism": "jectivity and accuracy. Currently, affective computing-based"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "depression diagnosis involves two categories: classification-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "based and regression-based depression diagnosis."
        },
        {
          "Autism": ""
        },
        {
          "Autism": "Affective\nclassification-based\ndepression\ndiagnosis\naids"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "healthcare workers and doctors in distinguishing between in-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "dividuals with and without depression. [Alsharif et al., 2022]"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "used mel-frequency cepstral coefficients to extract patients’"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "audio features and CNN to build a classification model to de-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "tect depression in Arabic audio data.\n[Cai et al., 2020a] em-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "ployed KNN as a depression classification detection model"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "to distinguish depression patients from normal people by in-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "tegrating different EEG data obtained under neutral, nega-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "tive, and positive audio stimulation. [Wang et al., 2023b] pro-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "posed a multi-modal depression detection model with emo-"
        },
        {
          "Autism": "tional knowledge graph, which integrated text, facial expres-"
        },
        {
          "Autism": ""
        },
        {
          "Autism": "sions, and other behaviors\n(e.g.,\nthe number of user posts,"
        },
        {
          "Autism": "blog length, etc.)\nto address the depression detection task."
        },
        {
          "Autism": "[Pan et al., 2023] used Transformer to classify depression and"
        },
        {
          "Autism": "non-depression from audio signals and facial expressions."
        },
        {
          "Autism": "Since\nregression\nallows\naccess\nto\ncontinuous\naffective"
        },
        {
          "Autism": "states,\naffective\nregression-based depression diagnosis has"
        },
        {
          "Autism": "been proposed to predict\nthe degree of depression or\nthe"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Performance": ""
        },
        {
          "Performance": "Pre."
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "93.00"
        },
        {
          "Performance": "85.18"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "65.00"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "92.52"
        },
        {
          "Performance": "80.00"
        },
        {
          "Performance": "89.00"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "93.50"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "85.71"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": "-"
        },
        {
          "Performance": ""
        },
        {
          "Performance": "97.71"
        },
        {
          "Performance": "-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "authors. Due to space constraints, we only list representative methods.": "severity of depressive symptoms\nin patients.\n[Zhou et al.,"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "2020] proposed a deep regression network called DepressNet"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "with facial depression data to predict depression degree. This"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "significantly improves the latest performance of visual-based"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "complex depression recognition. [Zhao and Wang, 2022] em-"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "ployed a\ncross-modal\naffective\nregression model\nto facili-"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "tate the learning of more accurate multimodal representations"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": ""
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "from text, audio, and facial expression videos for automatic"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": ""
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "depression severity assessment."
        },
        {
          "authors. Due to space constraints, we only list representative methods.": ""
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "Clinical Application With significant advancements in af-"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "fective computing technology, depression diagnosis\nis now"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "being employed in clinical monitoring. To prevent the recur-"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "rence of patients with depression,\n[Yin et al., 2022] designed"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "an intelligent monitoring system based on a hybrid affective"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "computing model\n(namely CNN-LSTM), aiming to provide"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "recurrence monitoring for patients with depression within"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "their home and daily environments. Additionally,\nit can be"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "applied to assess new patients. The system includes user in-"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "put, depression testing,\nintelligent monitoring, and connec-"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "tivity to external wearables like dedicated voice acquisition"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "devices and EEG devices.\nIt also supports communication"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "with online doctors and integration with external systems."
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "Limitation Overall,\ndepression diagnosis with affective"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "computing is evolving from rough classification to contin-"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "uous\nregression analyses,\nfor dynamic clinical monitoring."
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "However,\nit still\nfaces challenges including subjectivity and"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "heterogeneity of depression,\nlongitudinal monitoring,\nand"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "comprehensive assessment integrating genetic, psychosocial,"
        },
        {
          "authors. Due to space constraints, we only list representative methods.": "and environmental factors. In addition, current affective com-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Clinical Application In clinical applications of autism in-": "tervention,\nleveraging affective\ncomputing technology en-",
          "resource-constrained devices, which provides a strong sup-": "port for the intelligent healthcare field."
        },
        {
          "Clinical Application In clinical applications of autism in-": "hances virtual\nreality (VR)\nfor human-computer\ninteraction",
          "resource-constrained devices, which provides a strong sup-": "Limitation Pain level recognition has developed from ini-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "(HCI)\nscenarios\naddressing challenges\ncaused by a\nshort-",
          "resource-constrained devices, which provides a strong sup-": "tial classification of pain versus non-pain to the recognition"
        },
        {
          "Clinical Application In clinical applications of autism in-": "age of autism treatment professionals.\n[Manju et al., 2023]",
          "resource-constrained devices, which provides a strong sup-": "of pain intensity, from clinical diagnosis to real-time monitor-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "employ VR-assisted system combined with wearable multi-",
          "resource-constrained devices, which provides a strong sup-": "ing. However, pain recognition and assessment still faces sig-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "model sensing technologies,\nto collect physiological signals",
          "resource-constrained devices, which provides a strong sup-": "nificant challenges such as individual differences, emotional"
        },
        {
          "Clinical Application In clinical applications of autism in-": "and game performance data during HCI\ntraining.\nThen,\nit",
          "resource-constrained devices, which provides a strong sup-": "differences, standard calibration, and so on. In addition, some"
        },
        {
          "Clinical Application In clinical applications of autism in-": "employs a machine learning model to identify ASD children,",
          "resource-constrained devices, which provides a strong sup-": "pain symptoms may lack distinct behavioural and physiolog-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "assessing the diagnosis, severity, social behavioral\ninterven-",
          "resource-constrained devices, which provides a strong sup-": "ical features, further complicating identification."
        },
        {
          "Clinical Application In clinical applications of autism in-": "tion, and treatment of ASD with multiple assessment scales.",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "Limitation Despite some progress in autism diagnosis and",
          "resource-constrained devices, which provides a strong sup-": "3.4\nOther Related Healthcare Applications"
        },
        {
          "Clinical Application In clinical applications of autism in-": "intervention, current approaches are still\nin their infancy and",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "Affective computing has also been applied to other clinical"
        },
        {
          "Clinical Application In clinical applications of autism in-": "face challenges such as data privacy concerns in children, dif-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "applications such as Bipolar disorder [Baki et al., 2022], el-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "ficulties in data collection, assessment complexity, and emo-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "derly companionship and monitoring [Meng et al., 2021],"
        },
        {
          "Clinical Application In clinical applications of autism in-": "tion model bias in autism. In addition, Autism may have other",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "smart medicine [Ayata et al., 2020] etc. For convenience, we"
        },
        {
          "Clinical Application In clinical applications of autism in-": "co-existing mental health disorders, such as attention deficit",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "combine some of them into one section for overview."
        },
        {
          "Clinical Application In clinical applications of autism in-": "hyperactivity disorder (ADHD), complicating affective com-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "Bipolar disorder\nis\na mental health disorder\nthat\ncauses"
        },
        {
          "Clinical Application In clinical applications of autism in-": "puting models for accurate diagnosis.",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "mood\nswings\nranging\nfrom depression\nto mania.\n[Baki"
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "et\nal.,\n2022]\ncreated\na\nmultimodal\ndecision\nsystem"
        },
        {
          "Clinical Application In clinical applications of autism in-": "3.3\nPain Level Recognition",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "for\nthree\nlevel mania\nclassification\nbased\non\nrecordings"
        },
        {
          "Clinical Application In clinical applications of autism in-": "Pain is the body’s intricate physical and psychological emo-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "of\npatients’\naudio,\ntext,\nand\nfacial\nexpression\nvideos."
        },
        {
          "Clinical Application In clinical applications of autism in-": "tional response to underlying injury or illness. Accurate iden-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "Elderly monitoring and dementia diagnosis are crucial\nin an"
        },
        {
          "Clinical Application In clinical applications of autism in-": "tification of pain is crucial\nin medicine, enabling medicine",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "aging population,\nespecially during illness.\nTo accurately"
        },
        {
          "Clinical Application In clinical applications of autism in-": "professionals to formulate effective treatment plans for en-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "monitor the emotion state of the elderly, [Meng et al., 2021]"
        },
        {
          "Clinical Application In clinical applications of autism in-": "hancing a patient’s quality of\nlife.\nResearch on pain level",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "introduced an emotion-aware medicine monitoring system"
        },
        {
          "Clinical Application In clinical applications of autism in-": "recognition through affective computing has been a promi-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "based on brain waves.\nIntelligent medicine systems offer an"
        },
        {
          "Clinical Application In clinical applications of autism in-": "nent and challenging issue. The field of pain level recogni-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "alternative to doctor shortages.\n[Ayata et al., 2020] proposed"
        },
        {
          "Clinical Application In clinical applications of autism in-": "tion employs affective computing techniques for classifica-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "an emotion recognition-based intelligent medicine system for"
        },
        {
          "Clinical Application In clinical applications of autism in-": "tion and regression tasks, aimed at diagnosing pain patients",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "emotional care by collecting and analyzing multiple physio-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "and obtaining their pain intensity, respectively.",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "logical signals from patients."
        },
        {
          "Clinical Application In clinical applications of autism in-": "Affective\nclassification-based pain level\nrecognition can",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "assist medical professionals in accurately determining the pa-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "tient’s pain location, thereby significantly optimizing consul-",
          "resource-constrained devices, which provides a strong sup-": "4\nChallenges and Opportunities"
        },
        {
          "Clinical Application In clinical applications of autism in-": "tation time. For instance,\n[Vallez et al., 2022] identified joint",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "Despite breakthroughs, several challenges remain, yet\nthere"
        },
        {
          "Clinical Application In clinical applications of autism in-": "pain from facial expression images with the help of a pre-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "are also related opportunities for future development."
        },
        {
          "Clinical Application In clinical applications of autism in-": "trained CNN classification model.\n[Chen et al., 2022] used",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "multi-layer CNN classifies\nthe EEG singals\nin resting and",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "4.1\nPatient Data Privacy and Ethics"
        },
        {
          "Clinical Application In clinical applications of autism in-": "pain states during daily activities.\n[Lu et al., 2023]\nidenti-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "Challenge\nData privacy issues have been well-known in the"
        },
        {
          "Clinical Application In clinical applications of autism in-": "fied newborns’ pain levels based on their\nfacial expression",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "big data era, and it\nis particularly important\nfor\nthe health-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "videos with the help of Softmax. Similar to [Lu et al., 2023],",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "care sector. This might be attributed to the fact\nthat patients"
        },
        {
          "Clinical Application In clinical applications of autism in-": "[Phan et al., 2023] employed Softmax to recognition pain lev-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "have to share extremely sensitive information about their own"
        },
        {
          "Clinical Application In clinical applications of autism in-": "els from EDA and ECG signals.",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "bodies.\nAs a result,\nthe privacy of patient clinical data in"
        },
        {
          "Clinical Application In clinical applications of autism in-": "To obtain a continuous numerical output\nfor\nthe pain de-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "affective computing is a crucial ethical concern, especially"
        },
        {
          "Clinical Application In clinical applications of autism in-": "gree of patients, pain level recognition based on affective re-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "for children’s information. Ensuring confidentiality involves"
        },
        {
          "Clinical Application In clinical applications of autism in-": "gression is developed.\n[Thiam et al., 2020] employed feed-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "implementing robust measures in data transmission, storage,"
        },
        {
          "Clinical Application In clinical applications of autism in-": "forward neural networks as a regression model\nfor discern-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "and usage. The challenge is to retain critical emotion-related"
        },
        {
          "Clinical Application In clinical applications of autism in-": "ing pain level\nintensity based on emotion-related physiolog-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "information while adhering to ethical and moral regulations."
        },
        {
          "Clinical Application In clinical applications of autism in-": "ical signals (i.e., EDA, EMG, and ECG). Besides,\n[Jiang et",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "Therefore, a proper balance needs to be found to ensure pri-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "al., 2024] used a non-linear neural network with Sigmoid",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "",
          "resource-constrained devices, which provides a strong sup-": "vacy and the effectiveness of data analysis."
        },
        {
          "Clinical Application In clinical applications of autism in-": "for both classification and regression tasks, distinguishing be-",
          "resource-constrained devices, which provides a strong sup-": ""
        },
        {
          "Clinical Application In clinical applications of autism in-": "tween pain and non-pain in patients and detecting pain inten-",
          "resource-constrained devices, which provides a strong sup-": "Opportunities\nSome potential future opportunities may in-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "sity based on ECG, EDA, and ECG.",
          "resource-constrained devices, which provides a strong sup-": "clude the exploration of advanced privacy-protecting tech-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "Clinical Application In a clinical setting, pain analysis is",
          "resource-constrained devices, which provides a strong sup-": "niques\nsuch as\nfederated learning [Rieke et al., 2020] and"
        },
        {
          "Clinical Application In clinical applications of autism in-": "critical to a patient’s recovery.\n[Ghosh et al., 2023] proposed",
          "resource-constrained devices, which provides a strong sup-": "secure multi-party computing [Liu et al., 2020].\nFederated"
        },
        {
          "Clinical Application In clinical applications of autism in-": "an emotion analysis system based on deep learning and statis-",
          "resource-constrained devices, which provides a strong sup-": "learning allows for model\ntraining without sharing raw data,"
        },
        {
          "Clinical Application In clinical applications of autism in-": "tical learning, to analyze facial expressions images of patients",
          "resource-constrained devices, which provides a strong sup-": "and aggregating models without\nexposing individual data."
        },
        {
          "Clinical Application In clinical applications of autism in-": "for detecting pain levels of patients.\nIn addition,\nthe system",
          "resource-constrained devices, which provides a strong sup-": "Secure multi-party computing allows calculations to be per-"
        },
        {
          "Clinical Application In clinical applications of autism in-": "has the ability to perform pain detection and recognition on",
          "resource-constrained devices, which provides a strong sup-": "formed between multiple parties while maintaining the pri-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vacy of the data. These technologies can ensure that patients’": "clinical data is adequately protected.",
          "can be directly adjusted and optimized according to differ-": "ent needs to improve the efficiency and accuracy of sentiment"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "analysis. Edge computing systems can discretize computing"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "4.2\nEmotion Bias and Fairness in Clinical Data",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "tasks to edge devices,\nreducing latency in data transmission"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "Challenge\nIn clinical and other healthcare settings, the col-",
          "can be directly adjusted and optimized according to differ-": "and processing for faster real-time sentiment analysis."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "lection and annotation of sentiment data exhibit a natural bias",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "4.5\nLarge Foundation Model-related Applications"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "due to population sentiment expression heterogeneity and an-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "notator subjectivity. Different from data bias issues in com-",
          "can be directly adjusted and optimized according to differ-": "Challenge\nWith the development of visual-linguistic large"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "mon large datasets, the data from healthcare suffers from low",
          "can be directly adjusted and optimized according to differ-": "foundation models, such as GPT-4 [Rathje et al., 2023],\nthe"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "data volume and larger diversities of biased factors. For ex-",
          "can be directly adjusted and optimized according to differ-": "significance of\nlarge\nfoundation models has been demon-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "ample, environment, age, occupation, and race can all affect",
          "can be directly adjusted and optimized according to differ-": "strated across various application domains.\nConsequently,"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "the expression and the labelling of emotions [Liu et al., 2022].",
          "can be directly adjusted and optimized according to differ-": "constructing an affective large foundation model specifically"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "Furthermore, preferences and concerns during data collection",
          "can be directly adjusted and optimized according to differ-": "for healthcare could be beneficial\nto enhance a wider\nrange"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "and labeling also vary significantly among doctors.\nThese",
          "can be directly adjusted and optimized according to differ-": "of clinical applications. However,\nthe acquisition and anno-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "factors could lead to high emotion biases and unfairness in",
          "can be directly adjusted and optimized according to differ-": "tation of specified large foundation models for healthcare and"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "training AI models. As a result,\nit\nis needed to address and",
          "can be directly adjusted and optimized according to differ-": "clinical data remain challenging, posing a significant hurdle"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "reduce the bias for a fair and unbiased understanding of emo-",
          "can be directly adjusted and optimized according to differ-": "in the development of healthcare affective foundation model."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "tions across diverse populations.",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "Opportunities\nLeveraging existing visual-language foun-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "Opportunities\nTo lower\nthe biases\nin affective\ncomput-",
          "can be directly adjusted and optimized according to differ-": "dation models\nto construct\nthe affective large-scale model"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "ing under\nlimited labelled databases,\nsome research is ex-",
          "can be directly adjusted and optimized according to differ-": "through transfer\nlearning and cross-modal prompt\nlearning"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "ploring unsupervised/self-supervised learning algorithms\nto",
          "can be directly adjusted and optimized according to differ-": "could reduce the dependence on large amounts of\ntraining"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "reduce reliance on affective labels.\nThese algorithms can",
          "can be directly adjusted and optimized according to differ-": "data,\nthus enhancing the reusability of\nthese models for di-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "learn emotional\nrepresentations from unlabeled data,\nreduc-",
          "can be directly adjusted and optimized according to differ-": "verse application tasks [Liu et al., 2024]. This method can"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "ing the need for large-scale labeled datasets [Han et al., 2023;",
          "can be directly adjusted and optimized according to differ-": "not only improve the effect and generalization ability of af-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "Han et al., 2022].\nFurthermore,\nthe introduction of domain",
          "can be directly adjusted and optimized according to differ-": "fective models but also provide a common basis for emotion"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "adaptive learning techniques can improve the generality of",
          "can be directly adjusted and optimized according to differ-": "recognition in different domains and tasks."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "affective computing models and mitigate affective biases be-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "tween different cultures and demographic groups.",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "5\nConclusion"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "This paper provides a comprehensive survey of the applica-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "4.3\nFine-grained Health-related Emotions",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "tion of affective computing in the field of healthcare. Specif-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "Challenge\nMost\nexisting healthcare\napplications\nrely on",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "ically, we provide an overview of\nthe developments\nin af-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "single, simplistic affective models like six-class or three-class",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "fective\ncomputing for healthcare,\ncovering behavior data-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "emotion classification models [Ameer et al., 2023].\nThese",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "based, psychological data-based, and behavior-psychological"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "models fall short of simulating rich emotions from real pa-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "data-based datasets and approaches. Next, we introduce key"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "tients who may be undergoing complicated treatments, mak-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "healthcare applications, highlighting the top three most\nfre-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "ing it challenging for doctors to make accurate judgments. As",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "quently used, as well as other\nrelated applications.\nFinally,"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "a result, developing fine-grained health-related emotion mod-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "we summarize the most potential challenges and opportuni-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "els for clinical applications remains a key unresolved issue.",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "ties in the development of affective computing in healthcare."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "Opportunities\nRecently, several researchers have proposed",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "We believe that this review helps to provide academic and in-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "composite\nface\nexpression models based on linguistic de-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "dustrial\nresearchers with a comprehensive understanding of"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "scriptions [Liu et al., 2022]. We believe that\nthis approach",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "the latest advances in affective computing-based healthcare"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "facilitates\nthe description of\nchanges\nin emotional details,",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "and provides them with guidance."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "thereby guiding doctors to make more informed medical di-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "agnoses. Consequently, constructing multimodal fine-grained",
          "can be directly adjusted and optimized according to differ-": "References"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "emotion models in healthcare applications emerges as a future",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "[Abdolzadegan et al., 2020] D. Abdolzadegan, MH. Moattar,\nand"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "development direction.",
          "can be directly adjusted and optimized according to differ-": "M. Ghoshuni.\nA robust method for early diagnosis of autism"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "spectrum disorder from eeg signals based on feature selection and"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "4.4\nReal-time Diagnosis with Affective Computing",
          "can be directly adjusted and optimized according to differ-": "dbscan method. Biocybern Biomed Eng, 2020."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "Challenge\nSome medicine applications demand real-time",
          "can be directly adjusted and optimized according to differ-": "[Alelaiwi, 2019] A. Alelaiwi.\nMultimodal\npatient\nsatisfaction"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "recognition for smart healthcare.\nIEEE Access, 2019."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "emotional analysis and diagnosis,\ninvolving the rapid pro-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "cessing and analysis of\nlarge data sets while ensuring ac-",
          "can be directly adjusted and optimized according to differ-": "[Alghifari et al., 2023] MF. Alghifari, TS. Gunawan, and M. Kar-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "tiwi. Development of sorrow analysis dataset for speech depres-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "curate emotion recognition, such as mental health monitor-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "sion prediction.\nIn I2MTC, 2023."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "ing or emergency response systems. Developing efficient al-",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "gorithms and infrastructure for real-time processing without",
          "can be directly adjusted and optimized according to differ-": "[Alizadeh and Tabibian, 2021] M. Alizadeh and S. Tabibian.\nA"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "persian speaker-independent dataset\nto diagnose autism infected"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "compromising accuracy is a significant challenge.",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "children based on speech processing techniques.\nICSPIS, 2021."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "Opportunities\nExploring\nadaptive\nalgorithms\nand\nedge",
          "can be directly adjusted and optimized according to differ-": ""
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "",
          "can be directly adjusted and optimized according to differ-": "[Alsharif et al., 2022] Z. Alsharif, S. Elhag, and S. Alfakeh. De-"
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "computing systems facilitates the capability of real-time emo-",
          "can be directly adjusted and optimized according to differ-": "pression detection in arabic using speech language recognition."
        },
        {
          "vacy of the data. These technologies can ensure that patients’": "tional analysis with minimal\nlatency.\nAdaptive algorithms",
          "can be directly adjusted and optimized according to differ-": "In CDMA, 2022."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "et al. Multi-label emotion classification in texts using transfer",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "tegration of deep learning for improved diagnosis of depression"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "learning. Expert Syst. Appl, 2023.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "using eeg and facial features. Mater. Today., 2023."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Anekar et al., 2023] D. Anekar, Y. Deshpande, R. Suryawanshi,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Han et al., 2022]\nJ. Han, G. Jiang, G. Ouyang, and X. Li. A multi-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "R. Waman, V. Divekar, and R. Salunke. Exploring emotion and",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "modal approach for identifying autism spectrum disorders in chil-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "sentiment\nlandscape of depression: A multimodal analysis ap-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "dren.\nIEEE Trans. Neural Syst. Rehab. Eng., 2022."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "proach. GCAT, 2023.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Han et al., 2023] Z. Han, Y. Shang, Z. Shao, J. Liu, G. Guo, T. Liu,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Ayata et al., 2020] D. Ayata, Y. Yaslan, and ME. Kamasak. Emo-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "H. Ding, and Q. Hu. Spatial-temporal feature network for speech-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "tion recognition from multimodal physiological signals for emo-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "based depression recognition.\nIEEE Trans Cogn Dev Syst, 2023."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "tion aware healthcare systems. J Med Biol Eng, 2020.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Hossain and Muhammad, 2019] MS. Hossain and G. Muhammad."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Bajestani et al., 2019] GS. Bajestani, M. Behrooz, AG. Khani,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "Emotion recognition using secure edge and cloud computing. In-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "M. Nouri-Baygi, and A. Mollaei. Diagnosis of autism spectrum",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "formation Sciences, 2019."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "disorder based on complex network features. Comput Methods",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Ji et al., 2023] Z. Ji, T. Yu, Y. Xu, N. Lee, E. Ishii, and P. Fung. To-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Programs Biomed, 2019.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "wards mitigating llm hallucination via self reflection. In EMNLP,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Baki et al., 2022] P. Baki, H. Kaya, E .C¸ iftc¸i, and H. G¨ulec¸. A mul-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "2023."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "timodal approach for mania level prediction in bipolar disorder.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Jiang et al., 2022] Y. Jiang, Z. Zhang, and X. Sun. Mmda: A mul-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "IEEE Transactions on Affective Computing, 2022.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "timodal dataset\nfor depression and anxiety detection.\nIn ICPR,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Baygin et al., 2021] M. Baygin, S. Dogan, T. Tuncer, PD. Barua,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "2022."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "O. Faust, N. Arunkumar, EW. Abdulhay, EE Palmer, and UR.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Jiang et al., 2024] M. Jiang, R. Rosio, S. Salanter¨a, AM. Rahmani,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Acharya. Automated asd detection using hybrid deep lightweight",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "and other.\nPersonalized and adaptive neural networks for pain"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "features extracted from eeg signals. Comput. Biol. Med, 2021.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "detection from multi-modal physiological features. Expert Syst."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "Appl, 2024."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Cai et al., 2020a] H. Cai, Z. Qu, Z. Li, Y. Zhang, X. Hu, and B. Hu.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Feature-level fusion approaches based on multimodal eeg data for",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Lan et al., 2023] YT. Lan, D. Peng, W. Liu, Y. Luo, Z. Mao, WL."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "depression recognition.\nInf Fusion, 2020.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "Zheng, and BL. Lu.\nInvestigating emotion eeg patterns for de-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "pression detection with attentive simple graph convolutional net-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Cai et al., 2020b] H. Cai, Z. Yuan, Y. Gao, S. Sun, N. Li, F. Tian,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "H. Xiao, J. Li, Z. Yang, X. Li, Q. Zhao, Z. Liu, Z. Yao, et al. A",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "work.\nIn EMBC, 2023."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "multi-modal open dataset for mental-disorder analysis. Sci. Data,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Li et al., 2021]\nJ. Li, A. Bhat, and R. Barmaki. A two-stage multi-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "2020.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "modal affect analysis framework for children with autism spec-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Calabr`o et al., 2021] G.\nCalabr`o,\nA.\nBizzego,\nS.\nCainelli,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "trum disorder. arXiv preprint arXiv:2106.09199, 2021."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "C. Furlanello, and P. Venuti. M-ms: A multi-modal synchrony",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Liu et al., 2020]\nJ. Liu, Y. Tian, Y. Zhou, Y. Xiao, and N. Ansari."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Progresses\nin\ndataset\nto\nexplore\ndyadic\ninteraction\nin\nasd.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "Privacy preserving distributed data mining based on secure multi-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Artificial Intelligence and Neural Systems, 2021.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "party computation. Computer Communications, 2020."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Chen and Luo, 2023] X. Chen and T. Luo.\nCatching elusive de-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Liu et al., 2022] Y. Liu, W. Dai, C. Feng, W. Wang, G. Yin,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "pression via facial micro-expression recognition. IEEE Commun.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "J. Zeng, and S. Shan. Mafw: A large-scale and multi-modal"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Mag., 2023.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "and compound affective database for dynamic facial expression"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Chen and Zhao, 2019] S. Chen\nand Q. Zhao.\nAttention-based",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "recognition in the wild. In Proc. 30th ACM Int. Conf. Multimedia,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "autism spectrum disorder screening with privileged modality.\nIn",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "2022."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "ICCV, 2019.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Liu et al., 2023] Z. Liu, X. Yuan, Y. Li, Z. Shangguan, L. Zhou,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Chen et al., 2022] D. Chen, H. Zhang, PT. Kavitha, FL. Loy, SH.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "et al. Pra-net: Part-and-relation attention network for depression"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Ng, C. Wang, KS. Phua, SY. Tjan, SY. Yang,\nand C. Guan.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "recognition from facial expression. Comput. Biol. Med, 2023."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Scalp eeg-based pain detection using convolutional neural net-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Liu et al., 2024] Z. Liu, K. Yang, T. Zhang, Q. Xie, Z. Yu, and"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "work.\nIEEE Trans. Neural Syst. Rehab. Eng., 2022.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "S. Ananiadou.\nEmollms: A series of emotional\nlarge language"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Dessai and Usgaonkar, 2022] S. Dessai and SS. Usgaonkar.\nDe-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "models and annotation tools for comprehensive affective analy-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "pression detection on social media using text mining.\nIn INCET,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "sis. arXiv preprint arXiv:2401.08508, 2024."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "2022.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Lu et al., 2023] G. Lu, H. Chen, J. Wei, X. Li, X. Zheng, H. Leng,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Duan et al., 2019] H. Duan, G. Zhai, X. Min, Z. Che, Y. Fang,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "Y\n. Lou, and J. Yan. Video-based neonatal pain expression recog-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "X. Yang, J. Guti´errez, and PL. Callet. A dataset of eye move-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "nition with cross-stream attention. Multimed. Tools. Appl, 2023."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "ments for\nthe children with autism spectrum disorder.\nIn ACM",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Mallol-Ragolta et al., 2020] A. Mallol-Ragolta, S. Liu, N. Cum-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "MM, 2019.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "mins, and B. Schuller. A curriculum learning approach for pain"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Fernandes-Magalhaes et al., 2023] R.\nFernandes-Magalhaes,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "intensity recognition from facial expressions.\nIn FG, 2020."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "A. Carpio, D. Ferrera, D. Van Ryckeghem,\nI. Pel´aez, P. Bar-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "jola,\net al.\nPain emotion faces database (pemf):\nPain-related",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Manju et al., 2023] T. Manju, Magesh, S. Padmavathi,\nand Du-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "micro-clips for emotion research. Behav Res Methods, 2023.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "rairaj.\nIncreasing the social interaction of autism child using vir-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "tual reality intervention (vri). TALLIP, 2023."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Ghosh et al., 2023] A. Ghosh, S. Umer, MK. Khan, RK. Rout, and",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "BC. Dhara.\nSmart sentiment analysis system for pain detection",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Melinda et al., 2023] M. Melinda,\nA. Ahmadiar, M. Oktiana,"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "using cutting edge techniques in a smart healthcare framework.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "M. ShadiqAdiNugraha, MAL. Qadrillah, and Y. Yunidar. A novel"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Cluster Computing, 2023.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "autism spectrum disorder children dataset based on thermal imag-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "ing.\nIn ICCCE, 2023."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Giuntini et al., 2020] FT. Giuntini, MT. Cazzolato, MJD. dos Reis,",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": ""
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "et al.\nA review on recognizing depression in social networks:",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Meng et al., 2021] W. Meng, Y. Cai, LT. Yang, and WY. Chiu. Hy-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Journal of Ambient\nIntelligence\nchallenges and opportunities.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "brid emotion-aware monitoring system based on brainwaves for"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "and Humanized Computing, 2020.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "internet of medical things.\nIEEE Internet Things J, 2021."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "[Hadoush et al., 2019] H. Hadoush, M. Alafeef, and E. Abdulhay.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "[Mohammadi et al., 2019] Y. Mohammadi, M. Hajian,\nand MH."
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "Eeg analysis using empirical mode decomposition and second or-",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "Moradi. Discrimination of depression levels using machine learn-"
        },
        {
          "[Ameer et al., 2023]\nI. Ameer, N. B¨ol¨uc¨u, MHF. Siddiqui, B. Can,": "der difference plot. Behavioural Brain Research, 2019.",
          "[Hamid et al., 2023] DSBA. Hamid, SB. Goyal, and P. Bedi.\nIn-": "ing methods on eeg signals.\nIn ICEE, 2019."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "and AY. Prasad.\nEdge artificial\nintelligence-based facial pain",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "son, et al. Evaluation of interpretability for deep learning algo-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "recognition during myocardial infarction. JAMRIS, 2022.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "rithms in eeg emotion recognition: A case study in autism. Arti-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "ficial Intelligence in Medicine, 2023."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Negin et al., 2021] F. Negin, B. Ozyer, S. Agahian, S. Kacdioglu,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "and GT. Ozyer. Vision-assisted recognition of stereotype behav-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Vallez et al., 2022] N. Vallez,\nJ. Ruiz-Santaquiteria, O. Deniz,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "iors for early diagnosis of autism spectrum disorders. Neurocom-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "J. Hughes, S. Robertson, K. Hoti, and G. Bueno. Adults’ pain"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "puting, 2021.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "recognition via facial expressions using cnn-based au detection."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "In ICIAP, 2022."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Othmani et al., 2021] A. Othmani, D. Kadoch, K. Bentounes,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "E. Rejaibi, R. Alfred, and A. Hadid. Towards robust deep neural",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Wadhera and Kakkarl, 2021] T. Wadhera and D. Kakkarl.\nSocial"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "networks for affect and depression recognition from speech.\nIn",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "cognition and functional brain network in autism spectrum dis-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "Pattern Recognition.\nICPR International Workshops and Chal-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "order:\nInsights from eeg graph-theoretic measures. Biomedical"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "lenges: Virtual Event and January 10–15 and 2021 and Proceed-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "Signal Processing and Control, 2021."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "ings and Part II, 2021.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Wang et al., 2021] X. Wang, S. Zhao, and Y. Wang. Bimodal emo-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Pan et al., 2023] Y. Pan, Y. Shang, Z. Shao, T. Liu, G. Guo, and",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "tion recognition for the patients with depression. In ICSIP, 2021."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "H. Ding. Integrating deep facial priors into landmarks for privacy",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Wang et al., 2023a] X. Wang, X. Wan, Z. Ning, Z. Qie, J. Li, and"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "preserving multimodal depression recognition. IEEE Trans Affect",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "Y\n. Xiao.\nA multimodal\nfusion depression recognition assisted"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "Comput., 2023.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "decision-making system based on eeg and speech signals.\nIn"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Peng et al., 2020] X. Peng, D. Huang, and H. Zhang. Pain intensity",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "CCCI, 2023."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "recognition via multi-scale deep network. IET Image Processing,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Wang et al., 2023b] Z. Wang, B. Deng, X. Shu, and J. Shu. Mul-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "2020.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "timodal depression detection model\nfusing emotion knowledge"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Pham et al., 2020] TH. Pham,\nJ. Vicnesh,\nJKE. Wei,\nSL. Oh,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "graph.\nIn ICAIBD, 2023."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "N. Arunkumar, EW. Abdulhay, EJ. Ciaccio, and UR. Acharya.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Wei et al., 2023] P. Wei, D. Ahmedt-Aristizabal, H. Gammulle,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "Autism spectrum disorder diagnostic system using hos bispec-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "S. Denman, and MA. Armin. Vision-based activity recognition"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "International Journal of Environmental\ntrum with eeg signals.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "in children with autism-related behaviors. Heliyon, 2023."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "Research and Public Health, 2020.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Xia et al., 2023] M. Xia, Y. Zhang, Y. Wu, and X. Wang. An end-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Phan et al., 2023] KN. Phan, NK.\nIyortsuun, S. Pant, HJ. Yang,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "to-end deep learning model for eeg-based major depressive dis-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "and SH. Kim. Pain recognition with physiological signals using",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "order classification.\nIEEE Access, 2023."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "multi-level context information.\nIEEE Access, 2023.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Xu et al., 2023] X. Xu, G. Zhang, Q. Lu, and X. Mao. Multimodal"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Qayyum et al., 2023] A.\nQayyum,\nI.\nRazzak,\nM.\nTanveer,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "depression recognition that integrates audio and text.\nIn ISCEIC,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "M. Mazher, and B. Alhaqbani. High-density electroencephalog-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "2023."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "raphy and speech signal based deep framework for clinical de-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "pression diagnosis.\nIEEE ACM Trans Comput Bi, 2023.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Yan et al., 2020]\nJ. Yan, G. Lu, X. Li, W. Zheng, C. Huang, Z. Cui,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "Y\n. Zong, M. Chen, Q. Hao, Y. Liu, J. Zhu, and H. Li.\nFenp:\na"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Rahman and Booma, 2022] LA. Rahman and PM. Booma.\nThe",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "IEEE\ndatabase of neonatal\nfacial expression for pain analysis."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "early detection of autism within children through facial\nrecog-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "Trans Affect Comput., 2020."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "nition; a deep transfer learning approach.\nIn NTIC, 2022.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Yao et al., 2022] B. Yao,\nC.\nShi,\nL.\nZou,\nL. Dai, M. Wu,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Rathje et al., 2023] S. Rathje, DM. Mirea, I. Sucholutsky, R. Mar-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "L. Chen,\nZ. Wang,\nand K. Yu.\nD4:\na\nchinese\ndialogue"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "jieh, et al. Gpt is an effective tool for multilingual psychological",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "arXiv preprint\ndataset\nfor depression-diagnosis-oriented chat."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "text analysis. 2023.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "arXiv:2205.11764, 2022."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Rejaibi et al., 2019a] E. Rejaibi, D. Kadoch, K. Bentounes, R. Al-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Yildirim-Celik et al., 2022] H. Yildirim-Celik,\nS.\nEroglu,\nand"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "fred, M. Daoudi, et al. Clinical depression and affect recognition",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "K. Oguz...\nEmotional context effect on recognition of varying"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "with emoaudionet. arXiv preprint arXiv:1911.00310, 2019.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "Journal of\nfacial emotion expression intensities in depression."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Rejaibi et al., 2019b] E. Rejaibi, D. Kadoch, K. Bentounes, R. Al-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "Affective Disorders, 2022."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "fred, M. Daoudi, et al. Clinical depression and affect recognition",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Yin et al., 2022] W. Yin, C. Yu, P. Wu, W. Jiang, Y. Liu, T. Ren,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "with emoaudionet. ArXiv, 2019.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "and W. Dai. An intelligent mobile system for monitoring relapse"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Rieke et al., 2020] N. Rieke, J. Hancox, W. Li, F. Milletari, HR.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "of depression.\nIn CSCW, 2022."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "Roth, et al. The future of digital health with federated learning.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Yoon et al., 2022]\nJ. Yoon, C. Kang, S. Kim, and J. Han. D-vlog:"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "NPJ digital medicine, 2020.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "Multimodal vlog dataset for depression detection. In Proceedings"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Salekin et al., 2021] MS. Salekin, G. Zamzmi, J. Hausmann, and",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "of the AAAI Conference on Artificial Intelligence, 2022."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "D. Goldgof. . . . Multimodal neonatal procedural and postopera-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Yuan et al., 2022] X. Yuan, S. Zhang, C. Zhao, X. He, B. Ouyang,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "tive pain assessment dataset. Data in Brief, 2021.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "and S. Yang.\nPain intensity recognition from masked facial ex-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Shen et al., 2023]\nJ. Shen, Y. Zhang, H. Liang, Z. Zhao, K. Zhu,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "pressions using swin-transformer.\nIn ROBIO, 2022."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "K. Qian, Q. Dong, X. Zhang, and B. Hu. Depression recognition",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "from eeg signals using an adaptive channel\nfusion method via",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Zhang et al., 2023] F. Zhang, M. Wang, J. Qin, Y. Zhao, X. Sun,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "and W. Wen. Depression recognition based on electrocardiogram."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "improved focal loss.\nIEEE J. Biomed. Health. Inf., 2023.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "ICCCS, 2023."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Smith et al., 2020] M. Smith, BJ. Dietrich, E. Bai, and HJ. Bock-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "holt. Vocal pattern detection of depression among older adults.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Zhao and Wang, 2022] Z. Zhao and K. Wang.\nUnaligned mul-"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "International journal of mental health nursing, 2020.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "timodal\nsequences\nfor depression assessment\nfrom speech.\nIn"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "EMBC, 2022."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Sousa et al., 2023] A. Sousa, K. Young, M. D’aquin, M. Zarrouk,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "and J. Holloway.\nIntroducing calmed: Multimodal annotated",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Zhou et al., 2020] X. Zhou, K. Jin, Y. Shang, and G. Guo. Visually"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "dataset for emotion detection in children with autism. In Interna-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "interpretable representation learning for depression recognition"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "tional Conference on Human-Computer Interaction, 2023.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "from facial\nimages.\nIEEE Transactions on Affective Computing,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "2020."
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "[Thiam et al., 2020] P. Thiam, HA. Kestler, and F. Schwenker. Mul-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": ""
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "timodal deep denoising convolutional autoencoders for pain in-",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "[Zhu et al., 2019]\nJ. Zhu, Y. Wang, R. La, J. Zhan, J. Niu, S. Zeng,"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "tensity classification based on physiological signals. In ICPRAM,",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "and X. Hu. Multimodal mild depression recognition based on"
        },
        {
          "[Mohan et al., 2022] HM. Mohan, HCS. Kumara, SH. Mallikarjun,": "2020.",
          "[Torres et al., 2023]\nJMM. Torres, S. Medina-DeVilliers, T. Clark-": "eeg-em synchronization acquisition network. IEEE Access, 2019."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A robust method for early diagnosis of autism spectrum disorder from eeg signals based on feature selection and dbscan method",
      "authors": [
        "M Al. ; D. Abdolzadegan",
        "M Moattar",
        "Ghoshuni"
      ],
      "year": "2020",
      "venue": "Biocybern Biomed Eng"
    },
    {
      "citation_id": "2",
      "title": "Alizadeh and Tabibian, 2021] M. Alizadeh and S. Tabibian. A persian speaker-independent dataset to diagnose autism infected children based on speech processing techniques",
      "authors": [
        "; Alelaiwi",
        "Alelaiwi",
        "Alghifari"
      ],
      "year": "2019",
      "venue": "CDMA"
    },
    {
      "citation_id": "3",
      "title": "Exploring emotion and sentiment landscape of depression: A multimodal analysis approach",
      "authors": [
        "Ameer"
      ],
      "year": "2023",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from multimodal physiological signals for emotion aware healthcare systems",
      "authors": [
        "Ayata"
      ],
      "year": "2019",
      "venue": "Comput Methods Programs Biomed"
    },
    {
      "citation_id": "5",
      "title": "Automated asd detection using hybrid deep lightweight features extracted from eeg signals",
      "authors": [
        "Baki"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "M-ms: A multi-modal synchrony dataset to explore dyadic interaction in asd. Progresses in Artificial Intelligence and Neural Systems",
      "authors": [
        "Cai"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "7",
      "title": "Dessai and Usgaonkar, 2022] S. Dessai and SS. Usgaonkar. Depression detection on social media using text mining",
      "year": "2019",
      "venue": "ACM MM"
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Fernandes-Magalhaes"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "Smart sentiment analysis system for pain detection using cutting edge techniques in a smart healthcare framework",
      "authors": [
        "A Carpio",
        "D Ferrera",
        "D Van Ryckeghem",
        "I Peláez",
        "P Barjola"
      ],
      "year": "2023",
      "venue": "Cluster Computing"
    },
    {
      "citation_id": "10",
      "title": "A review on recognizing depression in social networks: challenges and opportunities",
      "authors": [
        "Giuntini"
      ],
      "year": "2020",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "11",
      "title": "Integration of deep learning for improved diagnosis of depression using eeg and facial features",
      "authors": [
        "Hadoush"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Neural Syst. Rehab. Eng"
    },
    {
      "citation_id": "12",
      "title": "Personalized and adaptive neural networks for pain detection from multi-modal physiological features",
      "year": "2022",
      "venue": "EMNLP"
    },
    {
      "citation_id": "13",
      "title": "Investigating emotion eeg patterns for depression detection with attentive simple graph convolutional network",
      "year": "2023",
      "venue": "EMBC"
    },
    {
      "citation_id": "14",
      "title": "A two-stage multimodal affect analysis framework for children with autism spectrum disorder",
      "authors": [
        "Li"
      ],
      "year": "2020",
      "venue": "Computer Communications",
      "arxiv": "arXiv:2106.09199"
    },
    {
      "citation_id": "15",
      "title": "Mafw: A large-scale and multi-modal and compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Liu"
      ],
      "year": "2022",
      "venue": "Proc. 30th ACM Int. Conf. Multimedia, 2022",
      "arxiv": "arXiv:2401.08508"
    },
    {
      "citation_id": "16",
      "title": "Increasing the social interaction of autism child using virtual reality intervention (vri)",
      "authors": [
        "Lu"
      ],
      "year": "2020",
      "venue": "ICCCE"
    },
    {
      "citation_id": "17",
      "title": "Hybrid emotion-aware monitoring system based on brainwaves for internet of medical things",
      "authors": [
        "Meng"
      ],
      "year": "2019",
      "venue": "ICEE"
    },
    {
      "citation_id": "18",
      "title": "Vision-assisted recognition of stereotype behaviors for early diagnosis of autism spectrum disorders. Neurocomputing",
      "year": "2021",
      "venue": "Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event and January 10-15 and 2021 and Proceedings and Part II"
    },
    {
      "citation_id": "19",
      "title": "Autism spectrum disorder diagnostic system using hos bispectrum with eeg signals",
      "authors": [
        "Peng"
      ],
      "year": "2020",
      "venue": "International Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "20",
      "title": "Rahman and Booma, 2022] LA. Rahman and PM. Booma. The early detection of autism within children through facial recognition; a deep transfer learning approach",
      "authors": [
        "Phan"
      ],
      "year": "2019",
      "venue": "NTIC",
      "arxiv": "arXiv:1911.00310"
    },
    {
      "citation_id": "21",
      "title": "Depression recognition from eeg signals using an adaptive channel fusion method via improved focal loss",
      "authors": [
        "Salekin"
      ],
      "year": "2020",
      "venue": "International journal of mental health nursing"
    },
    {
      "citation_id": "22",
      "title": "Evaluation of interpretability for deep learning algorithms in eeg emotion recognition: A case study in autism. Artificial Intelligence in Medicine",
      "authors": [
        "Sousa"
      ],
      "year": "2020",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "23",
      "title": "Social cognition and functional brain network in autism spectrum disorder: Insights from eeg graph-theoretic measures",
      "authors": [
        "Kakkarl Wadhera",
        "T Wadhera",
        "D Kakkarl"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "24",
      "title": "A multimodal fusion depression recognition assisted decision-making system based on eeg and speech signals",
      "authors": [
        "Wang"
      ],
      "year": "2021",
      "venue": "ICSIP"
    },
    {
      "citation_id": "25",
      "title": "Multimodal depression detection model fusing emotion knowledge graph",
      "authors": [
        "Wang"
      ],
      "year": "2023",
      "venue": "Multimodal depression detection model fusing emotion knowledge graph"
    },
    {
      "citation_id": "26",
      "title": "An endto-end deep learning model for eeg-based major depressive disorder classification",
      "authors": [
        "Wei"
      ],
      "year": "2023",
      "venue": "An endto-end deep learning model for eeg-based major depressive disorder classification"
    },
    {
      "citation_id": "27",
      "title": "Multimodal depression recognition that integrates audio and text",
      "authors": [
        "Xu"
      ],
      "year": "2020",
      "venue": "ISCEIC"
    },
    {
      "citation_id": "28",
      "title": "D4: a chinese dialogue dataset for depression-diagnosis-oriented chat",
      "authors": [
        "Yao"
      ],
      "year": "2022",
      "venue": "D4: a chinese dialogue dataset for depression-diagnosis-oriented chat",
      "arxiv": "arXiv:2205.11764"
    },
    {
      "citation_id": "29",
      "title": "Emotional context effect on recognition of varying facial emotion expression intensities in depression",
      "authors": [
        "Yildirim-Celik"
      ],
      "year": "2022",
      "venue": "CSCW"
    },
    {
      "citation_id": "30",
      "title": "Pain intensity recognition from masked facial expressions using swin-transformer",
      "authors": [
        "Yoon"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Visually interpretable representation learning for depression recognition from facial images",
      "authors": [
        "Zhou"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}