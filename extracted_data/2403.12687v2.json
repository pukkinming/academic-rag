{
  "paper_id": "2403.12687v2",
  "title": "Audio-Visual Compound Expression Recognition Method Based On Late Modality Fusion And Rule-Based Decision",
  "published": "2024-03-19T12:45:52Z",
  "authors": [
    "Elena Ryumina",
    "Maxim Markitantov",
    "Dmitry Ryumin",
    "Heysem Kaya",
    "Alexey Karpov"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents the results of the SUN team for the Compound Expression (CE) Recognition Challenge of the 6th ABAW Competition. We propose a novel audio-visual method for compound expression recognition. Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules. Notably, our method does not use any training data specific to the target task. Thus, the problem is a zeroshot classification task. The method is evaluated in multicorpus training and cross-corpus validation setups. Using our proposed method is achieved an F1-score value equals to 22.01% on the C-EXPR-DB test subset. Our findings from the challenge demonstrate that the proposed method can potentially form a basis for developing intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The Compound Expression Recognition (CER) as a part of affective computing is a novel task in intelligent human-computer interaction and multimodal user interfaces. It entails the automated identification of compound emotional states in individuals, which may include combinations of two or more basic emotions such as: Fear, Happiness, Sadness, Anger, Surprise, and Disgust.\n\nOver the last two decades, research efforts in the field of automatic expression analysis have predominantly focused on identifying six basic emotions  [9, 30] . However, these methods fail to fully capture the complexity of everyday emotional expressions. Individuals often exhibit Compound Expressions (CEs), such as Fearfully Surprised, Happily Surprised, Sadly Surprised, Disgustedly Surprised, Angrily Surprised, Sadly Fearful, Sadly Angry, which are combinations of basic emotions. This CEs underscores the necessity for more comprehensive models capable of capturing the subtleties inherent in human's emotional expressions.\n\nExisting methods for CER predominantly focus on the visual modality  [21, 31] . These methods use both dynamic  [21, 31, 32]  and static  [9, 19, 21]  deep models, relying on facial action units  [9, 19, 20] . The audio models used in the first two audio-visual methods are based on spectrograms  [9, 32] . However, to train a model for CER, it is necessary to have relevant data comprising balanced samples for each class, collected under uncontrolled conditions, containing multimodal data, and large enough to train deep neural network models. Nevertheless, challenges in annotating CEs  [9]  contribute to the scarcity of such corpora. An exception is the C-EXPR-DB corpus  [7] [8] [9] 17] , a part of which is presented as a Test subset in the 6th Workshop and Competition on Affective Behavior Analysis inthe-Wild (ABAW)  [18] . Another corpus along with the CEs is a Multi-modal compound Affective database for facial expression recognition in the Wild (MAFW)  [21] , which has limited access.\n\nIn this paper, we present a novel method for audio-visual CER as a part of the 6th ABAW CE Recognition Challenge. Our method does not utilize the CER challenge data as training data; rather, it includes models trained for basic emotion recognition. Decisions regarding predicted CEs are determined by the proposed rule-based method. It enables us to address the problem of insufficient publicly available data with CEs. Moreover, CEs comprise various pair combinations of basic emotions, rendering the proposed method particularly valuable to the research community. The versatility of the proposed method allows for use as a software tool for annotating new emotionally colored data.\n\nIn summary, our main contributions are as follows: • We introduce a novel audio-visual CER method based on basic emotion recognition and analysis of emotion probability distribution through multimodal fusion. • We present a method for audio-visual emotion recognition based on both multi-corpus and cross-corpus research. • We propose a rule-based decision-making method for CER that explains which modality is responsible for predicting specific CEs. • We provide new baseline performance measures for the recognition task of the seven basic emotions on the Validation subsets of the AffWild2 and Acted Facial Expressions in The Wild (AFEW) corpora.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "A pipeline of the proposed audio-visual CER method is shown in Figure  1 . The method accepts a multimedia file as an input. Then, it performs the necessary pre-processing for each modality, including face region detection and audio data extraction. The pre-processed data are used as an input for three models, which predict the probability distribution of recognized emotions. A hierarchical probability weighting is then applied. The final weighted probabilities are utilized for rule-based CEs prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Video Models",
      "text": "We use the RetinaFace model  [3] , as implemented by 1 , for face region detection. However, relying only on a single detector is insufficient; it is necessary to perform post-   [6]  pre-trained on face recognition  2  . The model extracts discriminative features from faces that are useful for transfer learning in our task. We initialize the model with pre-trained weights and subsequently fine-tune it to recognize affective states without freezing its layers. For feature extraction and classification, we extend the model by adding two Fully Connected Layers (FCLs) comprising 512 and 7 neurons, respectively. In the proposed method, we use a static model to detect affective states in each frame and extract features from every N frame (where N is the frame sampling step). We utilize these features as inputs for the dynamic model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dynamic Visual Model (Vd).",
      "text": "The model designed for analyzing dynamically changing affective states operates on 2-second segments or 10 frames. To produce 10 frames within two seconds, the Frame Per Second (FPS) rate is reduced to five FPS. The model proposed comprises two Long Short-Term Memory (LSTM) layers, with 512 and 256 neurons, respectively. It also includes a classification layer consisting of 7 neurons.\n\nTo enhance the generalizability of the video models, several augmentation techniques are applied, namely MixUp  [35]  and Label Smoothing  [26] . These techniques help to reduce the confidence level of the models in their basic emotion predictions, thereby enabling them to identify multiple emotions with varying degrees of certainty in the frames. All the models are trained using the Adam optimizer with a learning rate of 1e-4 for 30 epochs and the Cosine Annealing Cold Restart Learning Scheduler  [23]  with five rate restart cycles.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio Model",
      "text": "Before training an audio model, in addition to extracting audio signals from multimedia files, we detect voice activity. Two approaches are used for this purpose, depending on the corpus used. The first one employs an audio-based Voice Activity Detection (VAD)  3  . The second one relies on the video modality, analyzing video data frame by frame. We extract facial landmarks using MediaPipe  [25] , then mouth landmarks are identified and the region of interest is ex- tracted. It is used to determine whether the target speaker's mouth is open or closed. We employ this method due to the specificity of the training acoustic data, which may include background noises, making it challenging to identify the target speaker. Then, 4-second segments with a step of two seconds are formed on the detected segments of voice activity. In addition, to obtain the target label of a window, we compute the most frequent frame-wise label.\n\nSequence-to-One acoustic model. We proposed two slightly different models. The backbone of both models is based on the pre-trained emotional model Wav2vec2  [33] . This model was pre-trained using the regression emotion dimensions (arousal, valence, and dominance) from the MSP-Podcast corpus  [24] . On top of the model, we stack two transformer layers with self-attention mechanisms, each with 32 and 16 heads. After the last transformer layer, we aggregate the information along the time axis and apply a FCL with seven or eight neurons, depending on the number of classes. We fine-tune all the layers from the top to the last two (W2V2-7cl) or four (W2V2-8cl) encoding layers of the backbone model for models with seven and eight neurons, respectively.\n\nSimilar to the video model, Label Smoothing  [26]  is also used for the audio model to reduce the confidence of the model. The remaining training parameters are identical to those of the video model.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Modality Fusion",
      "text": "The proposed modality fusion method uses three models to represent emotion probability distributions. Each model exhibits varying prediction confidences for different emotions. Therefore, we employ a hierarchical probability weighting before predicting CEs. The importance of models and probabilities is considered in the first weighting. The weight matrix W is generated using the Dirichlet distribution:\n\nwhere W ∈ R M ×C , M is the number of models and C is the number of emotional classes. The weight matrix is generated such that the sum of the weights for the three models of each class equals one. After the first weighting, new probabilities for each model are calculated using the following formula:\n\nwhere P M = [p M 1 , p M 2 , ..., p M C ] is the probability vector for the model M , w M = [w M 1 , w M 2 , ..., w M C ] is the weight vector for the model M . In the second weighting, only the importance of the models is taken into account. A weight vector V of size M is generated with one value for each model. The vector values are generated in the range of [0.01; 0.5] with an increment of 0.005. A final probability vector P is obtained using the following formula:\n\nwhere v 1 ∈ V . W and V are weights that remain consistent across all test samples. This hierarchical weighting enhances performance measures for basic emotion recognition by considering the contribution of each model. The final probability vector is then utilized for CER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Rule-Based Decision-Making Method",
      "text": "We utilize two rules to make decisions regarding the predicted CEs. The first rule (Rule 1) allow for certain emotion predictions and is based on masking probabilities that fall below the minimum threshold (1/7) for emotion prediction. This rule is exclusively applied to the outputs of Dirichlet-based fusion method, e.g. P probability vectors. The probability vector is updated according to the following condition:\n\nwhere pE is the probability of the emotion E. For the Rule 1, according to the probabilities of the basic emotions Neutral (Ne), Anger (An), Disgust (Di), Fear (Fe), Happiness (Ha), Sadness (Sa), and Surprise (Su), the probability value for the Fearfully Surprised cp F eSu is calculated using the following formula:\n\nwhere pF e and pSu are the probability values for Fe and Su emotions, respectively. The probabilities for the remaining CEs are calculated similarly. The predicted class is the class with the maximal probability.\n\nCE The second rule (Rule 2) is based on weighting the frequency of emotions occurring in CEs. In the CE Recognition Challenge, we aim to develop a method for recognizing seven CEs: Fearfully Surprised, Happily Surprised, Sadly Surprised, Disgustedly Surprised, Angrily Surprised, Sadly Fearful, Sadly Angry. The occurrence frequency of each emotion differs among these pairs. For example, the emotion of Surprise is more frequent than the others; therefore, this emotion does not allow you to distinguish CEs. The use of emotion weights can enhance the importance of the probability of less represented emotions. The weight vectors, CW 1 and CW 2 , determine the weights of the first and second emotions in pairs of CEs. The weight vectors, whose paired values sum to one, are experimentally determined for our method and are shown in Table  1 .\n\nFor the Rule 2, the probability value for the Fearfully Surprised cp F eSu is calculated based on the established weights (see Table  1 ) and on the probabilities of the basic emotions using the formula:\n\nwhere cw F eSu 1 ∈ CW 1 and cw F eSu 2 ∈ CW 2 . This rule is applied to the outputs of both Dirichlet-based ( P ) and the hierarchical modality fusion ( P ) methods.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Research Corpora",
      "text": "We propose the audio-visual CER method based on the models designed for recognizing six basic emotions and a neutral state.\n\nWe use several corpora for training, validating and testing the developed emotion recognition models. To train a static video model, we use the AffectNet corpus  [27] . This corpus comprises an extensive collection of static facial images displaying spontaneous emotions. We use the RA-MAS  [28] , RAVDESS  [22] , CREMA-D  [2] , IEMOCAP  [1]  and SAVEE  [5]  corpora to train the dynamic visual model. In contrast to AffectNet, these corpora were collected in office conditions, but contain dynamically changing expressions. Therefore, the annotation quality is considered reliable, the facial images are noiseless, and the multi-corpus training provides the model with a high generalization ability to new data.\n\nTo train the audio model, we conduct a multi-corpus training using the AffWild2  [10] [11] [12] [13] [14] [15] [16] 34]  and MELD  [29]  corpora. These corpora comprise recordings collected in uncontrolled conditions and include various paralinguistic elements in speech (such as laughter, shouting, etc.), making the data more relevant to real-world scenarios in comparison to the aforementioned corpora.\n\nValidation of the audio and video models, as well as optimization of the modality fusion weights, is conducted on the Validation subset of the AffWild2 corpus (with corpus annotations dated 2024). To ensure that we do not overfit the models and fusion weights for each corpus, we use the AFEW Validation subset  [4]  as an additional validation corpus. Finally, the CER method is tested on the non-annotated Test subset of the C-EXPR-DB corpus.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "The experimental results are presented in Table  2 . Depending on the corpus, the visual models show different performance; the dynamic model outperforms the static model for AffWild2, and vice versa for AFEW. When the two visual models are combined, the hierarchical weighting fusion outperforms the Dirichlet-based weighting. Therefore, in the first submission to the CE Recognition Challenge, we use Model ID 4 in two versions: with and without the Rule 2 to predict CE. Using Rule 2 reduces the performance measure of CER by 1.54% compared to using the rule-free method. This suggests that increasing the importance of the less represented emotions in CE pairs does not improve CER performance.  We then evaluate the performance of the audio models on two corpora, AffWild2 and AFEW. It is impossible to say which model is more efficient. However, since these models show lower performance than the video models, we do not test only the audio models on C-EXPR-DB. Finally, we fuse the audio and video models by comparing various weighting fusions. The results show that the hierarchical weighting fusion outperforms the Dirichlet-based fusion. However, for comparison with previous models tested on C-EXPR-DB, we submit Model ID 8 with the Rule 2 for CE prediction. Additionally, we evaluate Model ID 9 based on Dirichlet fusion with both rules. Combining the visual models with the audio model results in a 2.57% improvement in F1. However, employing hierarchical weighting for model fusion shows a performance decrease of 4.45% compared to Dirichlet-based weighting. Additionally, Rule 1 exhibits a 0.87% lower performance than Rule 2.\n\nTo understand the significance of each model in the final CE predictions, the fusion model weights are presented in Figure  2 . The analysis of the fusion weights involving only the two visual models indicates a preference towards the dynamic model for predicting Di, Ha, and Sa expressions, while favoring the static model for the predicting Fe and Su expressions. The hierarchical weighting reduces the contribution of the dynamic model. This weight distribution suggests that considering the CE weighting (see Table  1 ), the method bases its decision on the dynamic model for predicting, for example, the Disgustedly Surprised and Happily Surprised classes, whereas it relies on the static model for predicting classes like Fearfully Surprised and Sadly Fearful.\n\nThe acoustic model trained on seven classes contributes less to the final prediction than the model trained on eight classes. For example, the first model holds greater in predicting only the Sa emotion, whereas the second model demonstrates a higher contribution in predicting An and competes with the dynamic video model in predicting Sa. Nevertheless, in both cases, the hierarchical weighting leads to a complete disregard of the acoustic model, consequently leading to a decrease in emotion recognition performance. Thus, when combining three models, using hierarchical weighting of emotion probability distribution proves ineffective. The latter fusion (see Figure  2 , bottom subfigure) demonstrates that the method bases its decision on the acoustic model when predicting, for example, the Angrily Surprised and Sadly Angry classes, while relying on the static model to predict the Happily Surprised class. The contribution of the dynamic model is considered when predicting other CEs.  We also show an example of CER in Figure  3 . From the frames depicted, it is clear that the woman is experiencing negative CEs; none of the models make a mistake in predicting Happily Surprised. All models, except the VS model, detect a change in expression at frame 100. Moreover, it is the audio model that recognizes the An emotion, which also influences the correct prediction of Sadly Angry by the audio-visual models. In general, according to the predictions of all models, the frames presented contain CEs such as: Fearfully Surprised, Sadly Surprised, Disgustedly Surprised, Sadly Fearful, and Sadly Angry. Subjectively, we have a tendency to assume that the video represents CEs such as: Sadly Fearful, Sadly Angry, Angrily Disgusted (this CE is not in the target classes). Thus, the most accurate predictor of CEs in this video is the audio-visual model with the Rule 1 (a purple line).\n\nTherefore, we have developed a method comprising three models, each assigned responsibility for predicting its respective class during CER.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel audio-visual method for CER. The method integrates three models, including the static and dynamic visual models, as well as the audio model. Each model predicts the emotion probabilities for six basic emotions and the neutral state. The emotional probabilities are then weighted utilizing the Dirichlet distribution. Finally, two rules are applied to determine CE. Additionally, We provide new baselines for recognizing seven emotions on the Validation subsets of the AffWild2 and AFEW corpora.\n\nThe experimental results demonstrate that each model is responsible for predicting specific CEs. For example, the audio model is responsible for predicting the Angry Surprised and Sadly Angry, the static visual model is responsi-ble for predicting the Happily Surprised class, and the dynamic visual model well predicts other CEs. The results obtained show that the proposed method can potentially lead to intelligent software tools for fast annotation of data containing both basic and compound emotional expressions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Acknowledgements",
      "text": "This study was supported by the Russian Science Foundation (project No. 22-11-00321).",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The method accepts a multimedia file",
      "page": 2
    },
    {
      "caption": "Figure 1: Pipeline of the proposed audio-visual CER method. PD refers to probability distribution.",
      "page": 3
    },
    {
      "caption": "Figure 2: Weights for different modality fusion. VS, VD, and A",
      "page": 5
    },
    {
      "caption": "Figure 2: The analysis of the fusion weights involving only",
      "page": 5
    },
    {
      "caption": "Figure 2: , bottom sub-",
      "page": 5
    },
    {
      "caption": "Figure 3: An example of CEs prediction using video from the C-EXPR-DB corpus.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VS\nVD\nA\nAV using Rule 1": "AV using Rule 2"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "IEMO-CAP: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael K Keutmann"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Evangelos Ververas"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "4",
      "title": "Automatic emotion, engagement and cohesion prediction tasks",
      "authors": [
        "Abhinav Dhall",
        "Emotiw"
      ],
      "year": "2019",
      "venue": "International Commission on Mathematical Instruction (ICMI)"
    },
    {
      "citation_id": "5",
      "title": "Audiovisual feature selection and reduction for emotion classification",
      "authors": [
        "Sanaul Haq",
        "Philip Jb Jackson",
        "James Edge"
      ],
      "year": "2008",
      "venue": "Int. Conf. on Auditory-Visual Speech Processing (AVSP)"
    },
    {
      "citation_id": "6",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "7",
      "title": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "8",
      "title": "ABAW: Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "ECCV"
    },
    {
      "citation_id": "9",
      "title": "Multi-label compound expression recognition: C-EXPR database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "11",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "12",
      "title": "Analysing affective behavior in the second ABAW2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "14",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond. IJCV",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Mihalis Nicolaou"
      ],
      "year": "2019",
      "venue": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond. IJCV"
    },
    {
      "citation_id": "15",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "16",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "17",
      "title": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "The 6th affective behavior analysis in-the-wild (ABAW) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (ABAW) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "19",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "Compound expression recognition in-the-wild with AU-assisted meta multi-task learning",
      "authors": [
        "Ximan Li",
        "Weihong Deng",
        "Shan Li",
        "Yong Li"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "21",
      "title": "MAFW: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng"
      ],
      "year": "2022",
      "venue": "ACM MM"
    },
    {
      "citation_id": "22",
      "title": "The ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "23",
      "title": "SGDR: stochastic gradient descent with warm restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "SGDR: stochastic gradient descent with warm restarts",
      "arxiv": "arXiv:1608.03983"
    },
    {
      "citation_id": "24",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "A framework for building perception pipelines",
      "authors": [
        "Camillo Lugaresi",
        "Jiuqiang Tang",
        "Hadon Nash"
      ],
      "year": "2019",
      "venue": "A framework for building perception pipelines",
      "arxiv": "arXiv:1906.08172"
    },
    {
      "citation_id": "26",
      "title": "Does label smoothing mitigate label noise?",
      "authors": [
        "Michal Lukasik",
        "Srinadh Bhojanapalli",
        "Aditya Menon",
        "Sanjiv Kumar"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "27",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "RAMAS: Russian multimodal corpus of dyadic interaction for affective computing",
      "authors": [
        "Olga Perepelkina",
        "Evdokia Kazimirova",
        "Maria Konstantinova"
      ],
      "year": "2018",
      "venue": "International Conference on Speech and Computer"
    },
    {
      "citation_id": "29",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "30",
      "title": "In search of a robust facial expressions recognition model: A large-scale visual cross-corpus study",
      "authors": [
        "Elena Ryumina",
        "Denis Dresvyanskiy",
        "Alexey Karpov"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "31",
      "title": "MAE-DFER: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "ACM MM"
    },
    {
      "citation_id": "32",
      "title": "HiC-MAE: Hierarchical contrastive masked autoencoder for selfsupervised audio-visual emotion recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "HiC-MAE: Hierarchical contrastive masked autoencoder for selfsupervised audio-visual emotion recognition",
      "arxiv": "arXiv:2401.05698"
    },
    {
      "citation_id": "33",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf"
      ],
      "year": "2023",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "34",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "Mihalis Nicolaou"
      ],
      "year": "2017",
      "venue": "CVPRW"
    },
    {
      "citation_id": "35",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cisse",
        "David Yann N Dauphin",
        "Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    }
  ]
}