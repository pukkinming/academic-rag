{
  "paper_id": "2402.07326v1",
  "title": "Persian Speech Emotion Recognition By Fine-Tuning Transformers",
  "published": "2024-02-11T23:23:31Z",
  "authors": [
    "Minoo Shayaninasab",
    "Bagher Babaali"
  ],
  "keywords": [
    "Persian Speech Emotion Recognition",
    "shEMO",
    "Self-Supervised Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Given the significance of speech emotion recognition, numerous methods have been developed in recent years to create effective and efficient systems in this domain. One of these methods involves the use of pretrained transformers, fine-tuned to address this specific problem, resulting in high accuracy. Despite extensive discussions and global-scale efforts to enhance these systems, the application of this innovative and effective approach has received less attention in the context of Persian speech emotion recognition. In this article, we review the field of speech emotion recognition and its background, with an emphasis on the importance of employing transformers in this context. We present two models, one based on spectrograms and the other on the audio itself, fine-tuned using the shEMO dataset. These models significantly enhance the accuracy of previous systems, increasing it from approximately 65% to 80% on the mentioned dataset. Subsequently, to investigate the effect of multilinguality on the fine-tuning process, these same models are fine-tuned twice. First, they are fine-tuned using the English IEMOCAP dataset, and then they are fine-tuned with the Persian shEMO dataset. This results in an improved accuracy of 82% for the Persian emotion recognition system.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is a psychophysical process that plays a crucial role in human decision-making, interaction, and cognitive processes. With advancements in our comprehension of emotions and their functions, there is a growing demand for automatic emotion recognition systems. These systems are extensively studied and applied using speech, text, facial cues, and physiological signals, either individually or through multimodal approaches. As everyday intelligent devices proliferate, one of the challenges in this field is to make interactions with these devices more human-centric, moving away from traditional machine-human interactions  [1] .\n\nWith the growth of audio and image datasets in the past decade, developing systems for emotion recognition from text, speech, and images and fusing them has become a significant research area in human-computer interactions. Among unimodal studies, speech as a modality is particularly promising, given the long history of audio processing, the availability of suitable hardware and software infrastructure  [3] .\n\nResearch has shown that, in human communication, 7%, 38%, and 55% of information is conveyed through text (content of speech), voice, and facial expressions (images), respectively  [2] . Notably, there is a significant sensory load distinction between speech and its text form, creating an opportunity for focusing on speech-based emotion recognition systems. Developing such systems is intriguing because most studies and models focused on speech often emphasize the textual content and speaker-related features, neglecting information that conveys emotions regardless of words and speakers, considering it as additional, and even intrusive.\n\nIn the Persian language, due to a lack of diverse datasets, less research has been conducted on speech emotion recognition. Recent years have seen some progress, but there is still much room for research and development.\n\nThe diversity of modeling tools in artificial intelligence and the vast digital data available in recent years allow the pursuit of emotion recognition systems through various methods. One of these methods is deep learning, specifically using self-supervised learning with large unlabeled datasets to train models that can later be used for feature extraction and finetuning in various tasks. These self-supervised models were initially used for text and achieved State of the Art performance using the transformer architecture. This architecture was quickly adapted for various data types, especially in audio and speech processing, as common and robust tools  [3] .\n\nIn this article, after reviewing research on speech emotion recognition, self-supervised models using transfer learning are employed to construct a Persian speech emotion recognition system. Two models, one based on spectrograms and the other on speech audio, will be fine-tuned and compared with prior work. Then, these models will be fine-tuned once with emotional speech from the IEMOCAP dataset in English and again with Persian emotional speech. The impact of cross-lingual learning will be investigate using these results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Approaches to speech emotion recognition involve two stages: feature extraction and classification. In the first stage of speech processing, features such as source-based arousal features, prosodic features, and vocal tract articulations are extracted. In the second stage, classifiers like Support Vector Machines or neural networks are used for the classification of the extracted features  [10] .\n\nRecent unimodal studies on speech emotion recognition have focused on identifying relevant audio features, such as fundamental frequency (pitch), speech intensity, bandwidth, and duration. Prior to the widespread use of deep learning, speech emotion recognition employed methods like Hidden Markov Models, Gaussian Mixture Models, and Support Vector Machines. These approaches required significant feature engineering, and any changes in features often necessitated the reconstruction of the entire system. The advent of deep learning in this field significantly increased the accuracy of results in controlled environments, raising it from around 70% to over 90%  [4] .\n\nThe diversity of extracted speech features, as well as the variety of tools and architectures used, follows the field of speech emotion recognition. Recurrent neural networks, especially bidirectional long short-term memory networks, have demonstrated good performance due to their ability to model temporal features. This approach employs direct speech and acoustic features.\n\nLe et. al  [9]  introduced the idea of using uncertainty in emotional labels and an efficient learning algorithm. They employed a bidirectional long short-term memory (BiLSTM) recurrent network architecture and reported a weighted accuracy of 62.85 on the IEMOCAP dataset. This number represents a 12% improvement compared to the baseline by extreme learning machines.\n\nAnother architecture in this field involves Convolutional Neural Networks (CNNs). CNN-LSTMs have gained recent attention. This approach utilizes the audio spectrogram and its visual features. Satt et. al  [12]  presented a new implementation of speech emotion recognition. They segmented speech into smaller units of less than 3 seconds and employed the audio spectrogram as features. Ultimately, they improved the performance of the CNN network from 64 to 68 on the IEMOCAP dataset by incorporating long short-term memory.\n\nOne of the recent developments in the field of Persian speech emotion recognition is the work of Yezdani  [10] , in which deep learning models, including LSTM-CNN and LSTM-RNN with and without attention mechanisms, were applied to low-level and high-level signal features from the emotional speech dataset shEMO. This research reported an unweighted accuracy of 65.2 on this dataset, whereas the baseline accuracy using Support Vector Machines was reported as 58.2 in the paper introducing the dataset  [11] .\n\nIn recent years, self-supervised architectures like HuBert, Wav2vec2.0, and Whisper have shown promising results in speech emotion recognition. Kakouris et. al  [5] , through fine-tuning WavLM on the IEMOCAP dataset, reported an accuracy of 75, which has been the best performance so far.\n\nSelf-supervised methods for speech emotion recognition provides room for further research. Numerous experiments have been undertaken to enhance the robustness of emotion recognition systems, covering various aspects such as speech features, speaker-related features, and architectural choices. The fusion of speech features with text or facial expressions has been explored to improve system accuracy, referred to as multi-modal emotion recognition  [6] [7] [8] . One of the advantages of these architectures is the ability to fine-tune pretrained models for a new language, which provides flexibility for multilingual or crosslingual models. However, the incorporation of pre-trained self-supervised models into Persian language emotion recognition has not been implemented to date.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset And Transformer Models",
      "text": "One of the large-scale datasets for Persian emotion recognition is the shEMO emotional speech dataset, introduced in 2020  [11] . shEMO comprises 3,000 semi-natural utterances, equivalent to 3 hours and 25 minutes of speech data extracted from online radio scripts. The labels cover five basic emotions, including anger, fear, joy, sadness, and surprise, as well as a neutral state. The labeling was performed by 12 separate annotators, and the final labels were determined based on the aggregation of their judgments. There are a total of 87 Persian speakers, including 31 females and 56 males, reflecting a gender imbalance among the speakers. The duration of the utterances varies between 0.35 to 33.32 seconds, with an average of 4.11 and a standard deviation of 3.41 reported. The highest average is observed in the neutral class, and the lowest in the surprise class. In terms of label frequency, there is a significant imbalance in the data (Figure  1 ). All six labels participated in the following experiments, despite the general imbalance among labels and the insufficient data for the fear class.\n\nThe first model used is the base model of wav2vec2.0  [13] . This powerful model is built on a transformer architecture with a self-attention. The second model used is AST (Audio Spectrogram Transformer), which was introduced for the first time in the research  [14] . This model is essentially a vision transformer  [15] . The vision transformer eliminated the necessity for using convolutional networks for image data. The audio spectrogram transformer has a similar architecture, and it has been pretrained on spectrogram images, making it available open-source for fine-tuning purposes. This model has demonstrated superior performance compared to other models on many sound classification tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments And Results",
      "text": "The 3000 speech segments, as described earlier, are randomly divided into training, validation, test sets in a ratio of 80%, 10%, and 10%, respectively. This categorization remains consistent throughout all experiments. Feature extraction, preprocessing, and data representation are entirely based on the mentioned models, and the duration of speech input is set to 5 seconds. In the first part of the experiment, the two introduced transformer models are fine-tuned on the training set. The accuracy of each model on the test data is reported in Table  1 . In the second part of the experiment, two models are employed, which are the results of modifying the previously introduced models. These two models have been fine-tuned on IEMOCAP speech data for 4-class emotion recognition. By changing the last layer from 4 to 6 classes, they are fine-tuned once again on the shEMO data as in the previous section. The accuracy on the test set is reported before and after this fine-tuning (Table  2 ).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Table 1: Performance Of Fine-Tuned Models",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Discussion",
      "text": "In introducing the shEMO dataset for emotion recognition, machine learning methods were used. In this classification, the fear emotion group was entirely removed from the classification problem due to insufficient data. Over the remaining 5 groups, a Support Vector Machine (SVM) model achieved an average accuracy of 58.2  [11] . In  [10] , a more recent classification using audio features and neural networks improved this accuracy to 65.2. In the first experiment in this article, both models, regardless of their working method and the features they extract, significantly outperformed the results mentioned earlier. Although the results in Table  1  are not averaged over different data splits, the notable difference in performance demonstrates the capabilities of these two models. The wav2vec2.0 model has performed slightly better.\n\nThe second experiment answers the question of whether fine-tuning on speech from another language and then fine-tuning with Persian data can improve the final model's accuracy or not. Both of these models perform better before learning Persian speech from random initialization. After fine-tuning on shEMO, the accuracy for the wav2vec2.0 model improves by   1 , it can be concluded that there is a cross-lingual relationship among emotional speech features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, the best accuracy for Persian speech emotion recognition has been achieved using transformers compared to previous methods. The superiority of self-supervised pretrained models for this task has also been demonstrated. Furthermore, given the positive impact of emotional data from other languages on the performance of, there is room for improving Persian speech recognition models, even though diverse datasets for Persian language is insufficient. It is important to continue developing rich datasets for Persian speech.\n\nThe best model developed in this paper, which is currently used for Persian speech emotion recognition, has an accuracy of 82% on shEMO. However, due to the close performance of the two final models presented, cross-validation is necessary to select the superior model, which would require more time and computational resources and will be considered in the future.\n\nFor practical applications, it's essential to report and compare other metrics such as weighted accuracy and f-score in addition to accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "7.",
      "text": "",
      "page_start": 1,
      "page_end": 1
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Label Frequencies in the shEMO",
      "page": 3
    },
    {
      "caption": "Figure 1: ). All six labels",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Accuracy of fine-tuned models on",
      "data": [
        {
          "model": "Finetuned-\nWav2vec2.0",
          "Accuracy \nbefore fine-\ntuning on \nshEMO": "25.3",
          "Accuracy \nafter fine-\ntuning on \nshEMO": "82"
        },
        {
          "model": "Finetuned-AST",
          "Accuracy \nbefore fine-\ntuning on \nshEMO": "25",
          "Accuracy \nafter fine-\ntuning on \nshEMO": "81.7"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "S Abdullah",
        "S Ameen"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multimodal emotion recognition using deep learning",
      "authors": [
        "A Sadeeq",
        "S Zeebaree"
      ],
      "year": "2021",
      "venue": "Journal of Applied Science and Technology Trends"
    },
    {
      "citation_id": "3",
      "title": "Facial emotion recognition using deep learning: review and insights",
      "authors": [
        "W Mellouk",
        "W Handouzi"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "4",
      "title": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "Shamane Siriwardhana"
      ],
      "year": "2020",
      "venue": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "5",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "Babak Abbaschian",
        "Daniel Joze",
        "Adel Sierra-Sosa",
        "Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "6",
      "title": "Speech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing",
      "authors": [
        "Kakouros",
        "Sofoklis"
      ],
      "year": "2022",
      "venue": "Speech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing",
      "arxiv": "arXiv:2211.01756"
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika"
      ],
      "venue": ""
    },
    {
      "citation_id": "8",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "A Cambria",
        "A Gelbukh",
        "Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "9",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "Samarth Tripathi",
        "Sarthak Tripathi",
        "Homayoon Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "10",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition in persian speech using deep neural networks",
      "authors": [
        "Ali Yazdani",
        "Hossein Simchi",
        "Yasser Shekofteh"
      ],
      "year": "2021",
      "venue": "11th International Conference on Computer Engineering and Knowledge (ICCKE)"
    },
    {
      "citation_id": "13",
      "title": "",
      "authors": [
        "Mohamad Nezami",
        "O Jamshid Lou"
      ],
      "venue": ""
    },
    {
      "citation_id": "14",
      "title": "ShEMO: a large-scale validated database for Persian speech emotion detection. Language Resources and Evaluation",
      "authors": [
        "M Karami"
      ],
      "year": "2019",
      "venue": "ShEMO: a large-scale validated database for Persian speech emotion detection. Language Resources and Evaluation"
    },
    {
      "citation_id": "15",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "16",
      "title": "",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed"
      ],
      "venue": ""
    },
    {
      "citation_id": "17",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Ast: Audio spectrogram transformer",
      "authors": [
        "Yuan Gong",
        "Yu-An Chung",
        "James Glass"
      ],
      "year": "2021",
      "venue": "Ast: Audio spectrogram transformer",
      "arxiv": "arXiv:2104.01778"
    },
    {
      "citation_id": "19",
      "title": "An image is worth 16x16 words: Transformers image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    }
  ]
}