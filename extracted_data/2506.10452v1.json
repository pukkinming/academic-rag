{
  "paper_id": "2506.10452v1",
  "title": "Towards Robust Multimodal Emotion Recognition Under Missing Modalities And Distribution Shifts",
  "published": "2025-06-12T07:58:17Z",
  "authors": [
    "Guowei Zhong",
    "Ruohong Huan",
    "Mingzhen Wu",
    "Ronghua Liang",
    "Peng Chen"
  ],
  "keywords": [
    "Missing modalities",
    "out-of-distribution",
    "robust multimodal emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advancements in Multimodal Emotion Recognition (MER) face challenges in addressing both modality missing and Out-Of-Distribution (OOD) data simultaneously. Existing methods often rely on specific models or introduce excessive parameters, which limits their practicality. To address these issues, we propose a novel robust MER framework, Causal Inference Distiller (CIDer), and introduce a new task, Random Modality Feature Missing (RMFM), to generalize the definition of modality missing. CIDer integrates two key components: a Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal Inference (MACI) module. MSSD enhances robustness under the RMFM task through a weight-sharing selfdistillation approach applied across low-level features, attention maps, and high-level representations. Additionally, a Word-level Self-aligned Attention Module (WSAM) reduces computational complexity, while a Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion. To tackle OOD challenges, MACI employs a tailored causal graph to mitigate label and language biases using a Multimodal Causal Module (MCM) and finegrained counterfactual texts. Notably, MACI can independently enhance OOD generalization with minimal additional parameters. Furthermore, we also introduce the new repartitioned MER OOD datasets. Experimental results demonstrate that CIDer achieves robust performance in both RMFM and OOD scenarios, with fewer parameters and faster training compared to state-ofthe-art methods. The implementation of this work is publicly accessible at https://github.com/gw-zhong/CIDer.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "I N real-world scenarios, emotions are conveyed through multiple modalities, including language, pitch, facial expressions, and body movements. Multimodal Emotion Recognition (MER) leverages these modalities to interpret emotional states. While earlier research primarily focused on fusing word-level aligned multimodal sequences  [1] -  [5] , recent advancements aim to apply MER in practical, real-world con-texts, addressing challenges such as modality missing and outof-distribution (OOD)  1  generalization.\n\nTo address modality missing in MER, some researchers employ modality translation to infer missing data  [7] -  [10] , while others reconstruct missing features to guide learning without requiring complete multimodal sequences  [11] -  [13] . However, these approaches have notable limitations: 1) They often assume that training and test data are independently and identically distributed (IID), resulting in poor performance on OOD data. 2) Their definitions of modality missing are inadequate. Current methods typically handle two missing scenarios: missing features across modalities with equal probability (Traditional Random Modality Feature Missing, Traditional RMFM) or random missing modalities across samples (Random Modality Missing, RMM). These definitions fail to account for more complex cases, such as partial frame loss in video clips while retaining language and audio data. Neither traditional RMFM nor RMM can adequately describe such scenarios, as illustrated in Fig.  1 .\n\nTo address OOD generalization in MER, researchers have employed causal inference to mitigate bias  [6] ,  [14] ,  [15]  and utilized probabilistic or model-based approaches  [16] ,  [17]  to enhance generalization. However, these methods exhibit notable limitations: 1) They often perform poorly when modalities are missing. 2) Many require specific debiasing models, complicating the enhancement of OOD generalization in existing MER frameworks. Even model-agnostic methods tend to introduce excessive learnable parameters, increasing computational overhead. 3) Current word-based MER OOD datasets present critical flaws: they provide only word-level alignment, mix OOD and IID data in test sets (thereby reducing the effective OOD sample size and increasing variance), and frequently include mixed sentences, which limits their utility.\n\nTo address these challenges, we propose Causal Inference Distiller (CIDer), a robust MER framework that effectively handles both modality missing and OOD issues. Additionally, we introduce a novel RMFM task, which generalizes the concept of modality missing by using a missing rate to describe the loss of features across all three modalities, encompassing traditional RMFM and RMM as special cases. To address RMFM, we propose a Model-Specific Self-Distillation (MSSD) module. For OOD challenges, we introduce a Model-Agnostic Causal Inference (MACI) module, which introduces minimal learnable parameters and can be independently applied to enhance OOD generalization in existing MER models. Furthermore, we repartitioned the original IID datasets to create OOD datasets with training, validation, and test sample counts almost consistent with the IID dataset, providing both word-level aligned and unaligned versions to facilitate future research.\n\nIn summary, the contributions of this paper are as follows:\n\n• We introduce CIDer, a robust MER framework that addresses both modality missing and OOD challenges. • We propose a generalized RMFM task and the MSSD module to address it. MSSD employs a hierarchical structure for knowledge self-distillation within a weightsharing twin network. To reduce computational complexity, we introduce the Word-level Self-aligned Attention Module (WSAM) for word-level alignment of non-linguistic sequences. Additionally, we propose the Multimodal Composite Transformer (MCT) to facilitate efficient multimodal fusion through intra-and inter-modal interactions using shared attention matrices. • We propose a tailored causal graph and the MACI module to address the OOD challenge. Within MACI, we introduce the Multimodal Causal Module (MCM) to mitigate label bias during training and construct fine-grained counterfactual texts to reduce language bias during testing. MACI introduces minimal learnable parameters and can be independently applied to enhance OOD generalization in existing MER models. Furthermore, we repartitioned the IID datasets to create new OOD datasets for MER.\n\n• Extensive experiments on two MER datasets demonstrate that CIDer achieves superior robustness in addressing RMFM and OOD challenges, with fewer parameters and faster training compared to state-of-the-art methods.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A. Incomplete Data Traditional Random Modality Feature Missing (Traditional RMFM): This research area focuses on how models should address scenarios where each modality feature in the input data is randomly missing with equal probability. Yuan et al.  [11]  introduced a feature reconstruction network to enable the model to extract as much semantic information as possible from incomplete features. Sun et al.  [12]  proposed the Efficient Multimodal Transformer (EMT), which facilitates interactions between global multimodal and local unimodal representations. Additionally, they introduced Dual-Level Feature Restoration (DLFR) to reconstruct features from a lowdimensional space, encouraging the model to learn semantic information from incomplete data. Yuan et al.  [18]  developed a noise-hint-based adversarial training framework to enhance the model's robustness against various potential defects during inference. Furthermore, Yuan et al.  [19]  proposed the Meta Noise Adaptation (Meta-NA) strategy, enabling robust training in the presence of noisy instances. Zhang et al.  [20]  introduced the Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust MER. The LNLN incorporates two key components: the Dominant Modality Correction (DMC) Although this line of research has addressed the traditional RMFM problem to some extent, most studies rely on the assumption that features are missing with equal probability across modalities -an assumption that may not hold true in real-world scenarios.\n\nRandom Modality Missing (RMM): This research area focuses on addressing how models should handle scenarios where each modality in the input data is randomly missing with a certain probability. Zeng et al.  [21] ,  [22]  introduced a method to tackle the issue of RMM by leveraging tags to assist in different modality-missing situations. Furthermore, Zeng et al.  [23]  proposed a backbone encoder-decoder network to learn joint representations of available modalities and assess semantic consistency to determine whether the missing modality is critical for overall MER. Lian et al.  [13]  introduced two graph neural networks (GNNs) -Speaker GNN and Temporal GNN -which aim to jointly optimize classification and reconstruction tasks to fully utilize both complete and incomplete data in an end-to-end manner. Wang et al.  [24] , to alleviate potential distribution inconsistency between recovered and real data, proposed transferring the distribution of available modalities to missing modalities to preserve distribution consistency in the recovered data. Lin et al.  [25]  employed constraints such as geometric contrastive loss, distribution distance loss, and sentiment semantic loss to align the representations of modality-missing and modalitycomplete data, thereby enhancing the model's robustness to missing modalities without compromising the recognition of complete modalities. Liu et al.  [26] , to fully leverage the effectiveness of the language modality in MER, proposed a Modality Translation-based Multimodal Sentiment Analysis model (MTMSA), which exhibits robustness in uncertain modality-missing situations. Guo et al.  [27] , addressing the RMM problem, introduced three types of prompts: generation prompts, missing signal prompts, and missing type prompts. These prompts facilitate the generation of missing modality features and promote learning within and across modalities.\n\nAlthough this line of research has addressed the RMM problem, there may also be real-world scenarios where a sequence of a specific modality is partially missing -a situation referred to as RMFM. In such cases, these approaches often fail to perform effectively.\n\nIn summary, although the aforementioned works have enhanced the models' robustness in scenarios involving missing modalities, their performance significantly deteriorates when faced with OOD data. Furthermore, the definitions of modality missing in these two types of works exhibit certain limitations and fail to encompass all possible scenarios of modality absence.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Out-Of-Distribution Data",
      "text": "In this category, researchers have primarily focused on enhancing the robustness of models when handling OOD data inputs. Sun et al.  [6]  were the first to introduce the OOD Multimodal Sentiment Analysis (MSA) task and construct a corresponding word-based OOD dataset. By employing counterfactual reasoning to model the text modality independently, they derived single-modality predictions for text. During the testing phase, subtracting the text-only prediction from the multimodal prediction yields a debiased prediction result. Furthermore, Sun et al.  [16]  investigated biases introduced by non-text modalities and proposed a general debiasing framework based on inverse probability weighting, which adaptively assigns lower weights to samples with greater biases. Ma et al.  [17]  introduced the Bilateral Cross-modal Debias Multimodal sentiment analysis Model (BCD-MM), which improves the model's generalization ability in OOD scenarios by enhancing the extraction of low-redundancy cross-modal features and reducing reliance on non-causal associations. Yang et al.  [14]  proposed a causal-based Multimodal Counterfactual Inference Sentiment (MCIS) analysis framework, which imagines two counterfactual scenarios during the reasoning phase to mitigate utterance-level label bias and word-level context bias. Huan et al.  [15]  achieved debiasing from both multimodal bias and label bias perspectives by leveraging front-door adjustment and counterfactual reasoning methods.\n\nAlthough the aforementioned works have enhanced the generalization of models for OOD data, these methods often fall short in addressing modality-missing scenarios. Furthermore, such approaches typically necessitate the construction of specialized models to achieve debiasing, which is highly inconvenient for improving the OOD generalization of existing MER models. Although some model-agnostic approaches have emerged, they still require the introduction of a significant number of additional learnable parameters. Additionally, current word-based MER OOD datasets remain limited by several notable shortcomings.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we provide a detailed overview of the proposed CIDer framework. The overall architecture of CIDer is illustrated in Fig.  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Preliminaries",
      "text": "Data Preparation: MER typically involves three modalities: text ( S ∈ R T l ×1 ), audio ( Å ∈ R Ta×da ), and vision ( V ∈ R Tv×dv ), where T {l,a,v} denote the sequence lengths and d {a,v} represent the feature dimensions.\n\nFor incomplete data, we define:\n\nwhere Missing(•) applies modality missing methods such as our proposed RMFM, Traditional RMFM, RMM, Temporal Modality Feature Missing (TMFM), Structural Temporal Modality Feature Missing (STMFM)  [18]  or Specific Modality Missing (SMM).\n\nFor our RMFM, given a missing rate r%, we generate a total missing mask M all ∈ R T l +Ta+Tv , which is split into\n\nindicates that the i-th feature in modality k is missing. The implementation details for other cases of missing modalities can be found in the supplementary materials and the opensource code.\n\nDuring training, we apply a random missing rate within [0%, 100%) to robustly handle various scenarios.\n\nTask Definition: MER aims to leverage information from ( S, Å, V ) or ( S, A, V ) to identify shared modality expressions. For classification, the objective is to predict the emotion y ′ ∈ {1, . . . , cls}, where cls represents the total number of categories.\n\nFor simplicity, the absence of the ˚or indicators in the following text implies that both complete and incomplete data are handled in the same manner.\n\nB. Model-Specific Self-Distillation 1) Unimodal Compression: For the text sequence S, we first extract features using BERT [28]:\n\nwhere d l denotes the dimension of the text features. To facilitate subsequent attention computations, we map each modality sequence into a shared feature space using 1D convolution:\n\nwhere d represents the common feature dimension across modalities. To reduce computational complexity and training time, we propose the Word-level Self-aligned Attention Module (WSAM), which aligns long sequences X a and X v at the word level.\n\nWord-level Self-aligned Attention Module: As illustrated in Fig.  3 , for the visual sequence X v , WSAM uses X l as the Fig.  3 . The Word-level Self-aligned Attention Module (WSAM), taking the alignment of the visual sequence to the language sequence as an example.\n\nQuery and X v as the Key and Value, performing alignment through attention:\n\nwhere W {Q,K,V } ∈ R d×d are learnable weight matrices, and MASK v ∈ R T l ×Tv is used to mask irrelevant tokens. Similarly, the same method aligns the audio sequence X a , yielding\n\nFinally, a single-layer GRU  [29]  extracts temporal features for each modality:\n\n2) Multimodal Fusion: After obtaining the temporal features H {l,a,v} for each modality through intra-modal interaction, the next step involves inter-modal interaction. First, the temporal features of the three modalities are concatenated along the time dimension:\n\nThe concatenated multimodal sequence H m is then fed into the Multimodal Composite Transformer (MCT), whose core  component is the Multimodal Composite Attention (MCA), as illustrated in Fig.  4 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Composite Transformer:",
      "text": "The MCA matrix is computed as:\n\nSix mask matrices are applied to Attn tri to form unimodal and bimodal interactions, yielding Attn bi ∈ {Attn l↔a , Attn l↔v , Attn a↔v } and Attn uni ∈ {Attn l , Attn a , Attn v }. After applying softmax, these attention matrices are multiplied with H {tri,bi,uni} W V 2  to produce seven multimodal representations:\n\nThese representations are mapped through modality-specific linear layers:\n\nH{tri,bi,uni} = Linear {tri,bi,uni} H ′ {tri,bi,uni}  (9)  For each modality, the valid parts of H{tri,bi,uni} are concatenated along the feature dimension:\n\nwhere Hk lav tri represents the portion of Htri associated with modality k lav (k lav ∈ {l, a, v}), while Hk la l↔a , Hk lv l↔v , and Hkav a↔v respectively denote the portions of Hl↔a , Hl↔v , and Ha↔v corresponding to modality k {la,lv,av} (with k la ∈ {l, a}, k lv ∈ {l, v}, and k av ∈ {a, v}, respectively). Additionally, Hl l , Ha a , and Hv v represent the entirety of Hl , Ha , and Hv , respectively, as they pertain to their respective modalities. These are then reduced in dimension through linear layers:\n\nFinally, H{l,a,v} are concatenated and passed through a linear layer to obtain the MCA output:\n\nFor the i-th layer of MCT (i = 1, . . . , N ), the process is defined as follows:\n\nwhere LN denotes layer normalization  [30] , and FFN represents a feed-forward network. After N layers, H\n\n[N ] m ∈ R (3×T l )×d is obtained. Next, we separately extract each modality component to obtain H\n\n{l,a,v} ∈ R T l ×d . The utterancelevel representation for each modality is computed using naive attention (NA)  [12] :\n\nThese representations are concatenated and passed through two linear layers to produce the final multimodal joint representation:\n\n(15) 3) Hierarchical Distillation: To enhance the model's robustness in modality-missing scenarios without increasing the training load or compromising its lightweight design, we employ weight-sharing twin networks to perform hierarchical self-distillation across three dimensions-low-level features, attention maps, and high-level joint representations-in a bottom-up manner. Feature Distillation: First, we reconstruct the outputs of the MCT, H\n\n[N ] {l,a,v} , under incomplete data inputs:\n\nWe then use SmoothL1 loss to align {L fake , A fake , V fake } with the original features { L, Å, V }:\n\nAttention Distillation: We extract the attention matrices from the last MCT layer under complete Åttn\n\ntri , Åttn\n\n[N ] tri  (18)  Joint Representation Distillation: We extract the multimodal joint representations under complete hm and incomplete h m data inputs. Cosine similarity is used to align h m with hm :",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Model-Agnostic Causal Inference",
      "text": "The imbalance in the distribution of word categories often leads to an implicit imbalance in the distribution of category labels. To achieve robust predictions, we design a novel causal graph for MER that addresses both label bias and language bias, as illustrated in Fig.  5 . In the graph, L, A, and V represent language, audio, and visual inputs, respectively; M denotes the fused multimodal representation; Y represents the label; and C indicates label bias.\n\nLabel Bias Mitigation: During training, we employ backdoor adjustment to mitigate label bias. The traditional prediction P (Y |L) = P (Y |M = f (L, A, V )) is adjusted to the interventional form:  (20)  This is implemented through the Multimodal Causal Module (MCM), as shown in Fig.  6 . For each class i (i ∈ {1, . . . , cls}), we compute the mean features L i 3  , A i , and V i , and encode them using modality-shared MLPs:\n\nwhere MLP {l,a,v} (•) consists of three linear layers interspersed with two ReLU activation. Then, we concatenate them with the multimodal joint representation h m to form class-specific causal representations h c i :\n\nNext, h c i is passed through a class-specific classifier (a fully connected layer) to produce the classification result y ′ i for category i, corresponding to the term P (Y |M = f (L, A, V ), c):\n\nSubsequently, it is multiplied by the pre-calculated prior probability P (i) of each category's label distribution and summed to produce the robust classification output  4  :\n\nLanguage Bias Mitigation: During testing, we use counterfactual reasoning to mitigate language bias. Instead of relying on additional unimodal modeling, we directly adjust P (Y |do(L)) by subtracting counterfactual predictions:\n\nwhere l represents the full text feature, l cf denotes the counterfactual text feature (l cf ⊆ l), and τ controls the debiasing strength.\n\nNext, we will detail the process of constructing the finegrained counterfactual text S cf . First, we compute the frequency of each word w within each class in the dataset to obtain the set N w :\n\nNext, we compute the coefficient of variation for the word w using the set N w :\n\nwhere σ(•) represents the standard deviation, and µ(•) represents the mean. We define that when CV w ≥ 0.1, the word w exhibits a significant inter-class distribution difference and is therefore retained in the counterfactual text S cf . To prevent words with uneven inter-class distributions but low overall frequency in the dataset from being categorized as counterfactual, we restrict to the top 100 most frequent words for constructing S cf . For words with CV w < 0.1 or those with low frequency, we replace them with the [MASK] token.\n\nThis results in counterfactual texts such as\n\n[MASK], . . . , w T l ⟩, which are processed through BERT to obtain l cf and reduce language bias.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Two-Stage Optimization",
      "text": "To fully leverage information from both complete and incomplete modalities during training, we adopt a two-stage optimization approach. First, the model weights are updated using complete modalities through a conventional classification task:\n\nwhere CE(•) denotes the cross-entropy loss. Subsequently, incomplete modalities are fed into the network for classification:\n\nTo enable multi-layer self-distillation, we jointly optimize L task alongside three distillation losses-L recon , L attn , and L joint -resulting in the overall optimization objective for incomplete modality inputs:\n\nwhere α, β, and γ are hyperparameters that control the contribution of each distillation component.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Setup A. Datasets",
      "text": "CMU-MOSI  [31] : A dataset consisting of 2,199 YouTube movie review videos annotated with sentiment scores ranging from -3 (strongly negative) to +3 (strongly positive). The dataset includes 93 speakers and is divided into  So the answer to the question, can I contribute stock to an IRA is absolutely correct but always be mindful of the security that your using it, make sure it's safe. this is financial",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sun Et Al.'S Dataset Original Dataset Sample 1",
      "text": "So the answer to the question, can I contribute stock to an IRA is absolutely correct but always be mindful of the security that your using it, make sure it's safe.  To construct the OOD datasets with unaligned multimodal sequences, we repartitioned the original CMU-MOSI and CMU-MOSEI datasets. The statistics for the original MER IID dataset, the MER OOD dataset partitioned by Sun et al.  [6] , and our repartitioned MER OOD dataset are presented in Table  I .\n\nAdditionally, we provide three examples to illustrate the phenomenon of mixed sentences in the MER OOD dataset partitioned by Sun et al.  [6] , as depicted in Fig.  7 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Feature Extraction",
      "text": "Language: Text features are extracted using a fine-tuned BERT model  [28] , with a feature dimension of 768. The sequences are padded to a maximum length of 50 tokens. Audio: Audio features are extracted using COVAREP  [33] , with dimensions of 5 for CMU-MOSI and 74 for CMU-MOSEI. The sequences are padded to lengths of 375 for CMU-MOSI and 500 for CMU-MOSEI. Visual: Visual features are extracted using Facet  5  , with dimensions of 20 for CMU-MOSI and 35 for CMU-MOSEI. The sequences are padded to a uniform length of 500 for both datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Evaluation Metrics",
      "text": "We evaluate performance using three metrics: Acc2 (binary sentiment classification), F1 score, and Acc7 (7-class sentiment classification). For incomplete data, we calculate the Area Under Indicators Line Chart (AUILC)  [11]  to assess overall performance across increasing missing rates. The AUILC is computed as follows:\n\nwhere {v 0 , v 1 , . . . , v t } (v ∈ {Acc2, F1, Acc7}) represents the model's performance under different missing rates {p 0 , p 1 , . . . , p t }. To maintain consistency with prior works  [11] ,  [12] , all calculations involving AUILC in this paper are performed over a missing rate range of 0.0 to 1.0.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Implementation Details",
      "text": "The experiments were conducted on an NVIDIA Quadro RTX 8000 GPU, using Python 3.10, PyTorch 2.5.1, and Adam  [34]  as the optimizer. Hyperparameter optimization was performed using Optuna  [35] . The fixed hyperparameters and those requiring tuning via Optuna are detailed in Table  II .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Baselines",
      "text": "We compare our proposed CIDer with state-of-the-art methods, categorized into four groups based on the data types they handle: 1) Full modality input: DMD  [36] , GLoMo  [37] , DLF  [38] . 2) Traditional RMFM: EMT-DLFR  [12] , LNLN  [20] . 3) RMM: DiCMoR  [24] , MPLMM  [27] . 4) OOD data: CLUE [6] (using MAG-BERT  [39]  as the base model), GEAR  [16] .\n\nFor DiCMoR and MAG-BERT, multimodal sequences were aligned using Connectionist Temporal Classification (CTC)  [40]  prior to input. Table  III  summarizes the data types handled by each baseline and CIDer. GLoMo  [37]  DLF  [38]  EMT-DLFR  [12]  one-to-one LNLN  [20]  one-to-all\n\nDiCMoR  [24]  one-to-one MPLMM  [27]  one-to-all CLUE  [6]  GEAR  [16]  CIDer (Ours) one-to-all V. RESULTS AND DISCUSSION",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Comparison To State-Of-The-Art",
      "text": "The RMFM scenario: To evaluate CIDer's performance under modality-missing scenarios, we conducted experiments on the unaligned CMU-MOSI and CMU-MOSEI datasets with RMFM as the missing type. As shown on the left side of Table  IV , among the six metrics across the two datasets, CIDer achieved the best performance in four of them. It fell behind LNLN  [20]  in terms of Acc2 and F1 scores on CMU-MOSEI. This is likely because LNLN has a significantly larger number of model parameters compared to CIDer, enabling it to exhibit more robust performance across varying missing rates.\n\nThe OOD Scenario: To assess CIDer's performance in OO) scenarios, we conducted experiments on the same two datasets, as shown in the middle section of Table  IV . The results demonstrate that under the OOD condition with complete modality input, CIDer achieved the best performance on all metrics for both datasets, except for Acc7 on CMU-MOSEI, where it was only 0.2% lower than the optimal result obtained by EMT-DLFR  [12] .\n\nThe RMFM and OOD scenario: To further validate CIDer's robustness when addressing both RMFM and OOD challenges simultaneously, we conducted experiments on the above datasets with RMFM as the modality-missing type. As shown on the right side of Table  IV , the results indicate that CIDer effectively handles both RMFM and OOD challenges, outperforming various state-of-the-art methods on both datasets.\n\nTo validate the effectiveness of the MACI module when integrated into different MER methods, we conducted experiments on DLF  [38]  and MPLMM  [27] . As demonstrated in Table  IV , incorporating MACI improved the performance of both DLF and MPLMM across most scenarios. These results further confirm the plug-and-play portability and efficacy of MACI.\n\nExtensive experiments demonstrate that CIDer, by integrating self-distillation and causal inference, maintains high robustness even in the presence of both RMFM and OOD challenges. Although CIDer does not outperform LNLN in some scenarios, its parameter count is significantly lower, comprising approximately only 3.8% of LNLN's parameters, as detailed in Subsection V-C. This makes CIDer a highly efficient and practical solution for real-world MER applications.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Ablation Study",
      "text": "To evaluate the performance of each component in the proposed CIDer framework, we conducted experiments on the CMU-MOSI dataset, as shown in Table  V . The results indicate that both self-distillation and causal inference are crucial for addressing RMFM and OOD challenges. The absence of either module leads to a noticeable decline in performance.\n\nFurthermore, the handling of lengthy non-linguistic sequences and the fusion of multimodal information also have a significant impact on the model's performance. Row 9 (w/o WSAM) illustrates that even with the full architecture, the model's performance is still compromised when processing lengthy non-linguistic sequences. This suggests that such sequences often contain redundant information, which can adversely affect the model's judgment. Row 10 (w/o MCT) demonstrates that replacing MCT with a vanilla transformer results in performance degradation. This highlights that MCT improves the model's efficiency by constructing distinct attention matrices, enabling effective fusion of multimodal information in a simple yet powerful manner.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Complexity Analysis",
      "text": "To further evaluate the spatial and temporal complexity of the proposed CIDer, we computed the model parameters, GPU memory usage, and training time for both baseline methods and CIDer on the CMU-MOSI dataset, as presented in Table  VI . The results indicate that CIDer has the fewest parameters among all state-of-the-art methods, with approximately 248K parameters. Furthermore, CIDer's GPU memory usage ranks second only to DiCMoR  [24]  and CLUE  [6]  optimized with CTC, consuming only 6.8 GiB. We attribute the significantly lower memory usage of DiCMoR and CLUE to the fact that CTC performs word-level alignment of nonlinguistic sequences outside the network at a relatively low computational cost. However, CTC-based training remains unstable, with the CTC loss frequently reaching magnitudes on the order of 10 4 .\n\nAdditionally, CIDer demonstrates a relatively short training time, surpassed only by GEAR  [16] , which requires fine-tuning two BERT BASE models, and is comparable to GLoMo  [37] , averaging 10.4 seconds per epoch.\n\nWe also validated the number of parameters in the introduced debiasing module MACI, which amounts to approximately 37K. This indicates that MACI can be integrated as an auxiliary module into existing MER methods with minimal additional parameters while enhancing their generalization capabilities in OOD environments.\n\nIn summary, despite incorporating multiple modules, CIDer remains a compact model characterized by a low parameter count, efficient memory usage, and fast training speed.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Cross-Dataset Generalization",
      "text": "To investigate the generalization capability of CIDer in cross-dataset scenarios (domain generalization, which is a generalized form of OOD), we conducted experiments on the CMU-MOSI and CMU-MOSEI datasets. These datasets were re-partitioned by Guo et al.  [41]  to ensure compatibility in feature dimensions. Specifically, CMU-MOSI → CMU-   [20]  outperformed it. We attribute this to LNLN's larger parameter count, which may enhance its generalization in scenarios involving missing modalities.\n\nOverall, despite not being explicitly designed for domain generalization, CIDer demonstrated superior performance in most cases while maintaining a lower parameter count. This further underscores its robustness and efficiency.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Other Modality Missing Scenarios",
      "text": "To evaluate the performance of CIDer in handling other types of modality missing scenarios (Traditional RMFM, RMM, Temporal Modality Feature Missing (TMFM), Structural Temporal Modality Feature Missing (STMFM)) at different missing rates, we conducted experiments on the CMU-MOSI dataset, as shown in Table  VIII . Additionally, we evaluated the performance of CIDer in the special case of RMM, specifically the scenario where a Specific Modality is    Missing (SMM), as shown in Table  IX . The detailed results for each method at various missing rates under different modalitymissing scenarios, along with the final AUILC scores, are provided in the supplementary material.\n\nAs shown in Table  VIII  and Table  IX , CIDer maintains robust performance across most cases, even when confronted with various modality-missing scenarios such as Traditional RMFM, RMM, TMFM, STMFM, and SMM. This further highlights the effectiveness and necessity of the hierarchical self-distillation mechanism in CIDer. More importantly, CIDer employs a one-to-all training strategy, which not only ensures strong robustness but also significantly reduces training costs. This characteristic holds substantial practical significance for real-world MER applications.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "F. Qualitative Analysis 1) Embedding Space Of Joint Representation:",
      "text": "To evaluate the effectiveness of self-distillation, we conducted experiments on both the CMU-MOSI and CMU-MOSEI datasets. Scatter plots were generated for missing rates of 0.1, 0.3, and 0.5, as shown in Fig.  8 .\n\nFrom the figures, it is evident that on both the CMU-MOSI and CMU-MOSEI datasets, when self-distillation is applied, the model effectively distinguishes between negative and positive samples despite modality missing. In contrast, when self-distillation is not utilized (as illustrated on the right side of Fig.  8 ), the separability of features for negative and positive samples is significantly reduced, leading to increased overlap in the embedding space. This overlap adversely affects the final recognition performance.\n\n2) Prediction Correction by MACI: To evaluate the effectiveness of the MACI module, we present an example from the CMU-MOSI OOD test set to illustrate its prediction correction process. As shown in Fig.  9 , for a sample labeled as \"positive\", the model-relying solely on the statistical information of the dataset-tends to predict it as \"negative\" with a high probability of 87.1%. To enhance the robustness of the model's predictions and mitigate the influence of label bias, the incorporation of the MCM module significantly reduces the model's prediction probability for the \"negative\" class. However, overall, the model still leans toward predicting the emotion expressed by the sample as negative. To further refine the prediction, the model subtracts the results obtained from fine-grained counterfactual text during the test phase, ultimately correctly classifying the sample as positive.\n\nThis example demonstrates that, in the context of OOD  scenarios, the MACI module effectively alleviates label bias and language bias through the synergy of the MCM and finegrained counterfactual text. This integration enables accurate predictions and enhances CIDer's generalization capabilities in OOD scenarios.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In summary, this paper introduces a novel and robust MER paradigm named CIDer, designed to simultaneously address the challenges of modality missing and OOD data. The paper first defines a more generalized RMFM task and proposes the MSSD module to tackle the RMFM challenge. Specifically, within MSSD, knowledge distillation is performed in a bottom-up manner across three levels: low-level features, attention maps, and high-level representations. To maintain a streamlined overall framework, weight sharing is implemented between the teacher and student models. Additionally, the WSAM module for word-level alignment is introduced to compress lengthy non-linguistic sequences, while the MCT module is incorporated to enable efficient multimodal fusion.\n\nThe paper also proposes the MACI module to address OOD challenges. In MACI, a specific causal graph is designed for MER, accounting for the potential impacts of label bias and language bias. The MCM is introduced to mitigate label bias during training. And fine-grained counterfactual texts are constructed to alleviate language bias during testing. Notably, MACI introduces only a small number of learnable parameters and can be independently applied to enhance the OOD generalization capabilities of existing MER models.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Appendix Definition Of Various Modality Missing Scenarios",
      "text": "In this section, we mainly provide the mathematical definitions of other modality missing scenarios, which mainly include Traditional Random Modality Feature Missing (Traditional RMFM) and Random Modality Missing (RMM).\n\nAdditionally, according to the research by Yuan et al.  [18] , Traditional RMFM can be further divided into: Temporal Modality Feature Missing (TMFM) and Structural Temporal Modality Feature Missing (STMFM). And RMM can be further divided into Specific Modality Missing (SMM).\n\nTraditional RMFM: For Traditional RMFM, given a missing rate of r%, we can generate missing masks for each modality, M L ∈ R T l , M A ∈ R Ta , and M V ∈ R Tv , respectively. Thus, M (i) k ∈ R 1 = 0 (k ∈ {L, A, V }) indicates that the i-th feature in the modality sequence is missing.\n\nRMM: For RMM, given a missing rate of r%, we can generate modality-level missing masks for the three modalities, M all = [M L , M A , M V ] ∈ R 3 , corresponding to the language, audio, and visual modalities, respectively. Thus, M k ∈ R 1 = 0 (k ∈ {L, A, V }) indicates that all features in the k-th modality sequence are completely missing.\n\nA. Special Cases of Traditional RMFM TMFM (requires aligned multimodal sequences): For TMFM, given a missing rate of r%, we can generate a missing mask M ∈ R T that is applied to each modality, where T is the unified sequence length after multimodal sequence alignment. Thus, M (i) ∈ R 1 = 0 indicates that the i-th feature is missing in all modality sequences. STMFM (requires aligned multimodal sequences): STMFM is an extension of TMFM, where information is lost not just at a single time step but over a period. Specifically, for STMFM, given a missing rate of r%, the starting time step for loss is i, and the length of the loss period is t. Thus, (M (i) , . . . , M (i+t) ) = 0 indicates that all features from the i-th to the (i + t)-th are missing in all modality sequences, where t = T × r%. B. Special Case of RMM SMM: SMM is a special case of RMM, where a specific modality is entirely missing in all samples while the other modalities are retained. For SMM, retaining only the language modality can be described as M L = 1 for all samples, while M {A,V } = 0. Similarly, retaining only the audio modality can be described as M A = 1 for all samples, while M {L,V } = 0; retaining only the visual modality can be described as M V = 1 for all samples, while M {L,A} = 0; retaining only the language and audio modalities can be described as M {L,A} = 1 for all samples, while M V = 0; retaining only the language and visual modalities can be described as M {L,V } = 1 for all samples, while M A = 0; retaining only the audio and visual modalities can be described as M {A,V } = 1 for all samples, while M L = 0.\n\nFor detailed definitions of Traditional RMFM, TMFM, and STMFM, please refer to  [18] .",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Experimental Results Of The Complete Modality Scenario",
      "text": "In this section, we mainly present the experimental results under the IID environment with complete modality input. As can be seen from Table  X  and Tabel XI, the proposed CIDer maintains good performance on both the CMU-MOSI and CMU-MOSEI datasets. It is important to note that even under the conditions of complete modality input and IID environment, CIDer can still retain the MSSD and the MACI modules, and achieve good performance without any processing, which further demonstrates its excellent robustness when facing different data inputs.\n\nAs shown in Table  XII , when facing domain generalization scenarios, CIDer still achieved the best performance in six metrics under complete modality input on both datasets, even without further special treatment for domain generalization. This further proves the effectiveness of the MACI module in dealing with data distribution shifts.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Experimental Details Of Various Modality Missing Scenarios",
      "text": "To more clearly present the performance of CIDer and various state-of-the-art methods under different modality missing scenarios and different missing rates, we report the specific numerical results in tabular form in this section.\n\n• RMFM/UA/IID: Table  XIII",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: To address OOD generalization in MER, researchers have",
      "page": 1
    },
    {
      "caption": "Figure 1: Definition of modality missing. 1) Traditional RMFM: each feature",
      "page": 2
    },
    {
      "caption": "Figure 2: A. Preliminaries",
      "page": 3
    },
    {
      "caption": "Figure 3: , for the visual sequence Xv, WSAM uses Xl as the",
      "page": 3
    },
    {
      "caption": "Figure 2: The overall framework of CIDer. Specifically, CIDer consists of two primary modules: the MSSD module and the MACI module. These modules are",
      "page": 4
    },
    {
      "caption": "Figure 3: The Word-level Self-aligned Attention Module (WSAM), taking the",
      "page": 4
    },
    {
      "caption": "Figure 4: The Multimodal Composite Transformer (MCT), which is primarily",
      "page": 4
    },
    {
      "caption": "Figure 4: Multimodal Composite Transformer: The MCA matrix is",
      "page": 4
    },
    {
      "caption": "Figure 5: The causal graph for MER. During training, label bias is mitigated",
      "page": 5
    },
    {
      "caption": "Figure 5: In the graph, L, A, and V",
      "page": 5
    },
    {
      "caption": "Figure 6: For each class i (i ∈{1, . . . , cls}),",
      "page": 5
    },
    {
      "caption": "Figure 6: The Multimodal Causal Module (MCM).",
      "page": 6
    },
    {
      "caption": "Figure 7: The phenomenon of mixed sentences in the MER OOD datasets",
      "page": 7
    },
    {
      "caption": "Figure 7: B. Feature Extraction",
      "page": 7
    },
    {
      "caption": "Figure 8: From the figures, it is evident that on both the CMU-",
      "page": 10
    },
    {
      "caption": "Figure 8: ), the separability of features for negative and",
      "page": 10
    },
    {
      "caption": "Figure 9: , for a sample labeled",
      "page": 10
    },
    {
      "caption": "Figure 8: The distribution of joint representations in the embedding space. Red dots correspond to negative samples, green dots represent positive samples, and",
      "page": 11
    },
    {
      "caption": "Figure 9: An example of MACI correcting prediction. The category highlighted",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Complete View\n[CLS] any ##how it was\nBERT\nreally good [SEP]": "",
          "WSAM\nMCT\nWSAM": "Attentio\nDistillati"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "So the answer to the question, can I contribute stock\nto an IRA is absolutely correct but always be\nmindful of the security that your using it, make sure\nit's safe. this is financial": "very aware of. Many people are not until they get to\nbe a older age and they realize that gosh they\nwasted a lot of' time and money in placing bets on\nstock that's ultimately lost value",
          "Column_2": "",
          "So the answer to the question, can I contribute stock\nto an IRA is absolutely correct but always be\nmindful of the security that your using it, make sure\nit's safe.": "Many people are not until they get to be a older age\nand they realize that gosh they wasted a lot of time\nand money in placing bets on stock that's ultimately\nlost value."
        },
        {
          "So the answer to the question, can I contribute stock\nto an IRA is absolutely correct but always be\nmindful of the security that your using it, make sure\nit's safe. this is financial": "future. And it's a retirement future that can\nultimately turned in to an income for you when you\nno longer have an income and you're fully retired.",
          "Column_2": "",
          "So the answer to the question, can I contribute stock\nto an IRA is absolutely correct but always be\nmindful of the security that your using it, make sure\nit's safe.": "And it's a retirement future that can ultimately\nturned in to an income for you when you no longer\nhave an income and you're fully retired."
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proc. Conf. Empirical Methods Natural Lang. Process"
    },
    {
      "citation_id": "2",
      "title": "Efficient low-rank multimodal fusion with modalityspecific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "3",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Y Wang",
        "Y Shen",
        "Z Liu",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "4",
      "title": "Trisat: Trimodal representation learning for multimodal sentiment analysis",
      "authors": [
        "R Huan",
        "G Zhong",
        "P Chen",
        "R Liang"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "5",
      "title": "Distinguishing visually similar images: Triplet contrastive learning framework for image-text retrieval",
      "authors": [
        "P Ouyang",
        "J Chen",
        "Q Ma",
        "Z Wang",
        "C Bai"
      ],
      "year": "2024",
      "venue": "Proc. IEEE Int. Conf. Multimedia Expo"
    },
    {
      "citation_id": "6",
      "title": "Counterfactual reasoning for out-of-distribution multimodal sentiment analysis",
      "authors": [
        "T Sun",
        "W Wang",
        "L Jing",
        "Y Cui",
        "X Song",
        "L Nie"
      ],
      "venue": "Proc. ACM Int. Conf. Multimedia, 2022"
    },
    {
      "citation_id": "7",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "H Pham",
        "P Liang",
        "T Manzini",
        "L.-P Morency",
        "B Póczos"
      ],
      "year": "2019",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "8",
      "title": "Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis",
      "authors": [
        "Z Wang",
        "Z Wan",
        "X Wan"
      ],
      "year": "2020",
      "venue": "Proc. Web Conf"
    },
    {
      "citation_id": "9",
      "title": "Ctfn: Hierarchical learning for multimodal sentiment analysis using coupledtranslation fusion network",
      "authors": [
        "J Tang",
        "K Li",
        "X Jin",
        "A Cichocki",
        "Q Zhao",
        "W Kong"
      ],
      "venue": "Proc. Assoc. Comput. Linguistics, 2021"
    },
    {
      "citation_id": "10",
      "title": "Unimf: A unified multimodal framework for multimodal sentiment analysis in missing modalities and unaligned multimodal sequences",
      "authors": [
        "R Huan",
        "G Zhong",
        "P Chen",
        "R Liang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "W Li",
        "H Xu",
        "W Yu"
      ],
      "venue": "Proc. ACM Int. Conf. Multimedia, 2021"
    },
    {
      "citation_id": "12",
      "title": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "13",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Z Lian",
        "L Chen",
        "L Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "14",
      "title": "Towards multimodal sentiment analysis debiasing via bias purification",
      "authors": [
        "D Yang",
        "M Li",
        "D Xiao",
        "Y Liu",
        "K Yang",
        "Z Chen",
        "Y Wang",
        "P Zhai",
        "K Li",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Proc. Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "15",
      "title": "Muldef: A model-agnostic debiasing framework for robust multimodal sentiment analysis",
      "authors": [
        "R Huan",
        "G Zhong",
        "P Chen",
        "R Liang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "16",
      "title": "General debiasing for multimodal sentiment analysis",
      "authors": [
        "T Sun",
        "J Ni",
        "W Wang",
        "L Jing",
        "Y Wei",
        "L Nie"
      ],
      "venue": "Proc. ACM Int. Conf. Multimedia, 2023"
    },
    {
      "citation_id": "17",
      "title": "Bcd-mm: Mul-timodal sentiment analysis model with dual-bias-aware feature learning and attention mechanisms",
      "authors": [
        "L Ma",
        "J Li",
        "D Shao",
        "J Yan",
        "J Wang",
        "Y Yan"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Noise imitation based adversarial training for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "Y Liu",
        "H Xu",
        "K Gao"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Meta noise adaption framework for multimodal sentiment analysis with feature noise",
      "authors": [
        "Z Yuan",
        "B Zhang",
        "H Xu",
        "K Gao"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Towards robust multimodal sentiment analysis with incomplete data",
      "authors": [
        "H Zhang",
        "W Wang",
        "T Yu"
      ],
      "year": "2024",
      "venue": "Proc. Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "21",
      "title": "Tag-assisted multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "J Zeng",
        "T Liu",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "Proc. Int. ACM SIGIR Conf. Res. Develop Inf. Ret"
    },
    {
      "citation_id": "22",
      "title": "Robust multimodal sentiment analysis via tag encoding of uncertain missing modalities",
      "authors": [
        "J Zeng",
        "J Zhou",
        "T Liu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Mitigating inconsistencies in multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "J Zeng",
        "J Zhou",
        "T Liu"
      ],
      "year": "2022",
      "venue": "Proc. Conf. Empirical Methods Natural Lang. Process"
    },
    {
      "citation_id": "24",
      "title": "Distribution-consistent modal recovering for incomplete multimodal learning",
      "authors": [
        "Y Wang",
        "Z Cui",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Proc. IEEE Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "25",
      "title": "Missmodal: Increasing robustness to missing modality in multimodal sentiment analysis",
      "authors": [
        "R Lin",
        "H Hu"
      ],
      "year": "2023",
      "venue": "Trans. Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Modality translationbased multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Z Liu",
        "B Zhou",
        "D Chu",
        "Y Sun",
        "L Meng"
      ],
      "year": "2024",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "27",
      "title": "Multimodal prompt learning with missing modalities for sentiment analysis and emotion recognition",
      "authors": [
        "Z Guo",
        "T Jin",
        "Z Zhao"
      ],
      "year": "2024",
      "venue": "Proc. Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "28",
      "title": "",
      "authors": [
        "Dicmor"
      ],
      "venue": ""
    },
    {
      "citation_id": "29",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. Conf. North Amer. Chapter Assoc"
    },
    {
      "citation_id": "30",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "31",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "32",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intell. Syst"
    },
    {
      "citation_id": "33",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "34",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "35",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "36",
      "title": "Optuna: A next-generation hyperparameter optimization framework",
      "authors": [
        "T Akiba",
        "S Sano",
        "T Yanase",
        "T Ohta",
        "M Koyama"
      ],
      "year": "2019",
      "venue": "Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining"
    },
    {
      "citation_id": "37",
      "title": "Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Y Li",
        "Y Wang",
        "Z Cui"
      ],
      "year": "2023",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "38",
      "title": "Glomo: Global-local modal fusion for multimodal sentiment analysis",
      "authors": [
        "Y Zhuang",
        "Y Zhang",
        "Z Hu",
        "X Zhang",
        "J Deng",
        "F Ren"
      ],
      "venue": "Proc. ACM Int. Conf. Multimedia, 2024"
    },
    {
      "citation_id": "39",
      "title": "Dlf: Disentangledlanguage-focused multimodal sentiment analysis",
      "authors": [
        "P Wang",
        "Q Zhou",
        "Y Wu",
        "T Chen",
        "J Hu"
      ],
      "year": "2025",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "40",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Zadeh",
        "C Mao",
        "L.-P Morency",
        "E Hoque"
      ],
      "year": "2020",
      "venue": "Proc. Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "42",
      "title": "Bridging the gap for testtime multimodal sentiment analysis",
      "authors": [
        "Z Guo",
        "T Jin",
        "W Xu",
        "W Lin",
        "Y Wu"
      ],
      "year": "2025",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "43",
      "title": "Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD",
      "venue": "Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD"
    },
    {
      "citation_id": "44",
      "title": "Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD",
      "venue": "Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD"
    },
    {
      "citation_id": "45",
      "title": "Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD",
      "venue": "Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD"
    },
    {
      "citation_id": "46",
      "title": "Acc2 F1 Acc7 Acc2 F1 Acc7 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD",
      "venue": "Acc2 F1 Acc7 Acc2 F1 Acc7 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD"
    },
    {
      "citation_id": "47",
      "title": "Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD",
      "venue": "Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 Acc2 F1 Acc7 DMD"
    }
  ]
}