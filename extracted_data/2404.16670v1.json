{
  "paper_id": "2404.16670v1",
  "title": "Emovit: Revolutionizing Emotion Insights With Visual Instruction Tuning",
  "published": "2024-04-25T15:15:36Z",
  "authors": [
    "Hongxia Xie",
    "Chu-Jun Peng",
    "Yu-Wen Tseng",
    "Hung-Jen Chen",
    "Chan-Feng Hsu",
    "Hong-Han Shuai",
    "Wen-Huang Cheng"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions. This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding. In this work, we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts. Initially, we identify key visual clues critical to visual emotion recognition. Subsequently, we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data, effectively addressing the scarcity of annotated instruction data in this domain. Expanding on the groundwork established by InstructBLIP, our proposed EmoVIT architecture incorporates emotion-specific instruction data, leveraging the powerful capabilities of Large Language Models to enhance performance. Through extensive experiments, our model showcases its proficiency in emotion classification, adeptness in affective reasoning, and competence in comprehending humor. The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs, providing valuable insights and opening avenues for future exploration in this domain. Our code is available at https://github.com/aimmemotion/EmoVIT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Visual emotion recognition, a key area within artificial intelligence and computer vision, aims to predict human emotions based on visual cues such as facial expressions and body language. This technology is essential in bridging the gap between human affective states and machine understanding. Its diverse applications  [10, 13, 22, 39] , spanning from improving human-computer interaction to aiding in mental health assessment, underscore its significance. Accurate emotion recognition is vital for enhancing user expe- rience and ensuring information security, as it helps prevent emotional manipulation and misinformation  [32] . Developing robust emotion recognition models is not only a technical challenge but also a step towards more empathetic and intuitive AI systems, paving the way for more efficient and natural human-computer interactions.\n\nThe AI community has recently shown a growing interest in developing foundational vision models, e.g., Flamingo  [8] , LLaVA  [7] , BLIP2  [14] . These models excel in open-world visual understanding, tackling several vision tasks such as classification, detection, segmentation, and captioning. In contrast, current large-scale multimodal models are still in its infancy when it comes to emotion perception  [20] . As illustrated in Fig.  1 , when directly query the GPT-4  [29]  about the emotional category of an image, the model tends to provide incorrect responses. However, the model delivers accurate responses when provided with revised instructions. To fully leverage the potential of existing vision-based large models, our approach is based on the concept of Instruction Tuning. This effective strategy is aimed at teaching language models to follow natural language instructions, a technique proven to enhance their generalization performance across unseen tasks  [7, 9, 21] .\n\nIn this work, we focus on developing the model's proficiency in understanding and following instructions related to emotional contexts. This approach highlights the importance of fine-tuning the model's instruction-following capabilities, enabling it to interpret and respond to emotional content effectively. This is achieved by leveraging its preexisting knowledge base, thereby eliminating the necessity for an emotion-specific architectural framework.\n\nTo address the notable challenges encountered in Instruction Tuning for visual emotion recognition, especially the lack of specific instruction data, we introduce a novel self-generation pipeline explicitly crafted for visual emotion recognition by using GPT-4  [29] . This innovative pipeline excels in generating a diverse array of (image, instruction, output) instances, thereby notably enhancing the dataset with a more extensive and task-oriented variety of examples. This approach not only overcomes the challenge of limited data availability but also reduces the dependence on human labor. Therefore, it streamlines the process, enabling more efficient and effective emotion recognition.\n\nAdditionally, Instruction Tuning has been criticized for its emphasis on surface-level features like output patterns and styles, rather than achieving a profound comprehension and assimilation of tasks  [23] . To tackle this issue and enhance the diversity and creativity of instruction data, our dataset includes instructions that demand complex reasoning, going beyond basic question-and-answer formats. This is further enriched by incorporating visual cues such as brightness, colorfulness, scene type, object class, facial expressions, and human actions. These aspects are pivotal in fostering a nuanced comprehension of visual emotions, thus allowing the model to generate more precise and contextually appropriate interpretations  [13] .\n\nAfter generating the emotion visual instruction data, we propose an Emotion Visual Instruction Tuning (EmoVIT) framework, leveraging the foundation of In-structBLIP  [9] . This framework incorporates an emotioncentric, instruction-aware module that proficiently guides Large Language Models (LLMs) in assimilating the nuances of emotion instructions. Our work signifies a paradigm shift, presenting a new era of instruction-based learning for visual emotion understanding that relies less on explicit training data. Remarkably, as shown in Fig.  2 , our approach requires almost 50% of the training data typically needed yet exceeds the performance of previous visual emotion recognition methods and popular Visual Instruction Tuning methods.\n\nOur contributions can be summarized as follows: • We explore the potential of the Visual Instruction Tuning paradigm for emotion comprehension and introduce the concept of Emotion Visual Instruction Tuning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Visual Emotion Recognition",
      "text": "A key challenge in visual emotion recognition is bridging the gap between an image's visual cues and the emotions it portrays  [11, 12, 35] . While traditional efforts, e.g., Xu et al.'s multi-level dependent attention network  [12] , focus on visual models for emotional feature learning, recent advancements like EmoSet  [13]  offer rich emotion-laden datasets with 3.3 million images. The rise of multimodal models, such as the GPT series  [29] , has further propelled Vision-Language Recognition. However, fully leveraging these models in emotion recognition is an area ripe for exploration. Our work leads the way in utilizing large-scale models for Emotion Visual Instruction Tuning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Visual Instruction Tuning",
      "text": "Current Large Language Models (LLMs) have extensive knowledge bases, but their effectiveness depends on accurately interpreting human instructions due to a mismatch between training goals and user expectations. LLMs are trained to minimize prediction errors, whereas users expect helpful and safe instruction-following. Instruction Tuning addresses this by teaching models to follow natural language instructions, enhancing generalization to new tasks. FLAN  [21]  demonstrated that training a large model on instruction-based datasets improves zero-shot performance. This approach has extended to vision-language tasks, with BLIP2  [14]  and LLaVA  [7]  adapting instructiontuned LLMs for visual inputs. InstructBLIP  [9]  introduces instruction-aware visual feature extraction and the Q-Former, enabling more flexible, instruction-driven feature extraction.\n\nAs a novel area, visual emotion instruction tuning lacks benchmarks or guidelines for creating emotion instruction data. Our work pioneers the use of large-scale models to develop an emotion instruction data pipeline, overcoming the limitations of manual annotation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preliminary Of Visual Instruction Tuning",
      "text": "In the deep learning era, visual tuning has experienced significant paradigm shifts, as depicted in Fig.  3 .\n\nIn Fig.  3 (a), conventional tuning methodologies encompass Full fine-tuning, Head-oriented, and Backboneoriented techniques, capitalizing on large-scale pre-trained models. Predominantly, thoroughly fine-tuning these models for specific tasks, conducted end-to-end, is recognized as a highly effective strategy. However, this method requires maintaining separate copies of the backbone parameters for each distinct task, posing challenges in storage and deployment.\n\nAlternatively, Visual Prompt Tuning (VPT)  [24] , presents an efficient substitute for full fine-tuning within large-scale vision Transformer models. It achieves this by employing a minimal fraction of trainable parameters in the input space while maintaining a frozen backbone model. The objective function for Visual Prompt Tuning is given by:\n\nwhere min θP is the minimization over the prompt parameters P , L is the loss function, f represents the model function with input image X, prompt parameters P , and learnable model parameters θ P as input, and Y is the target output.\n\nVisual Prompt Tuning focuses on optimizing LLMs using a small set of parameters, whereas Visual Instruction Tuning (VIT) aims to improve the model's comprehension of instructions, thereby addressing the model's shortcomings in specific domains. This type of method aims to enhance the model's proficiency in following instructions, leveraging the capabilities of the latest foundation models, e.g., Llama  [25] , and BLIP2  [14] . Instructions serve as guiding constraints, shaping the model's outputs to conform to specific response characteristics and domainrelevant knowledge. This approach enables human monitoring of the model's behavior, thereby assuring alignment with the desired outcomes. Moreover, Instruction Tuning is computationally efficient, allowing LLMs to swiftly adapt to particular domains without extensive retraining or architectural alterations.\n\nThe objective function for Visual Instruction Tuning is given by:\n\nwhere min θtunable denotes the minimization over the tunable parameters θ tunable in the Instruction Tuning Module, L is the loss function, g is the model function with instruction I, image X, other contexts C, and tunable parameters θ tunable , and Y denotes the target output. The optional context C is not just raw data; it encompasses descriptive or directive information guiding the model on how to process input or which task to execute, e.g., image caption. It's integral to the model's understanding and execution of tasks based on specific instructions or guidelines.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Gpt-Assisted Emotion Visual Instruction Data Generation",
      "text": "Previous methodologies commonly employed a consistent template-based set of instructions for every image within a dataset across various specific tasks  [9] . For instance, a standard instruction such as \"Briefly describe the content of the image\" was employed uniformly across all images for Image Captioning. In this way, the model may not be able to adequately capture the unique characteristics of each image. Moreover, this one-size-fits-all approach often leads to suboptimal performance in emotion recognition tasks that require nuanced perception and differentiation of ambiguous emotion classes. Since the topic of Emotion Visual Instruction Tuning is still in its infancy, no benchmarks or guidelines have been proposed so far for constructing emotion instruction data. Based on the recent successes of machine-generated instructions demonstrated in LLaVA  [7] , our work pioneers the use of existing LLMs to create a pipeline for self-generating emotion instructions. Different from previous template-based and one-size-fits-all instruction data, we propose an instance-wise and LLM-assisted visual emotion instruction data pipeline. This methodology transcends the constraints of manual annotation by employing GPT-4  [29]  to generate instance-wise, tailored instruction data that dynamically corresponds to visual content.\n\nPrior to the development of instructional data for the visual emotion recognition task, it is imperative to confront a fundamental academic problem: What types of visual clues are pivotal in identifying emotions? This necessitates a careful consideration of the unique characteristics inherent to the task, along with a comprehensive understanding of the potential visual cues associated with human emotions. In this work, we propose a novel visual instruction data mechanism to remove the inherent subjectivity and ambiguity in emotional interpretation. Specifically, we integrate a broad spectrum of emotion attributes across multiple levels: low-level attributes (e.g., brightness, colorfulness), midlevel attributes (e.g., scene type and object class), and highlevel attributes (e.g., facial expressions and human actions), building upon insights from previous work  [13] . This comprehensive strategy not only aligns with the intricate nature of emotions but also significantly enhances the model's capability to interpret and understand visual emotional cues more accurately and holistically.\n\nThe overall pipeline of our proposed emotion visual instruction data is shown in Fig.  4 (a) . For an image X img , three types of image-related contexts are essential for GPT-4 to generate emotion instruction data: (i) a caption X c , (ii) an emotion attribute list X attr , which includes emotion class, brightness, colorfulness, scene type, object class, facial expression, and human action, and (iii) the system prompt, designed to enable GPT-4 to comprehend the specific task requirement  1  .\n\nWe first manually design a few examples which are used as seed examples for in-context learning to query GPT-4. This operation leverages the model's ability to extrapolate from given examples, enhancing its understanding and response accuracy based on the principles of few-shot learning  [7] . Our generated emotion instruction data includes three types: Categorical, Conversation, and Reasoning. Building upon previous research  [7] , our generated instruction data adheres to the dialogue format, exemplified in Fig.  5 .\n\nOur strategy for generating emotion instruction data adopts a progressive approach from simple to complex. Initially, for the Categorical data, we transform the associated emotion class of the image into a structured format. This process serves as the foundational component of our emotion instruction data.\n\nFor the Conversation data, our framework is designed to create dialogues in which the GPT assistant interacts with an inquirer, focusing on the emotion attributes of the image. In this setup, the assistant's responses are tailored to interpret and describe the image as though it were within its own visual field, thereby providing insights from an observational viewpoint. The scope of questions posed is comprehensive, encompassing the types of objects depicted, their actions, and the dynamics of their interrelationships. The dialogues we generate fall into two categories: (i) Basic Interaction, focusing on the provided emotion attribute list with simple, direct characteristics, and (ii) Advanced Interaction, which builds on the first type to reach greater conversational complexity and sophistication.\n\nFor the Reasoning data, our approach extends beyond mere visual content, prompting the model to generate indepth reasoning questions. To enhance the dialogue's credibility and structure, detailed examples are incorporated alongside logical reasoning steps, ensuring that the discourse convincingly captures the intricacies of the visual content.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Visual Instruction Tuning",
      "text": "After acquiring the emotion visual instruction data as detailed in Sec. 3.2, our goal is to employ this data in enhancing the existing Visual Instruction Tuning model. This enhancement aims to align the LLMs' existing knowledge with the emotion understanding domain.\n\nAs shown in Fig.  4  b, we have developed an Emotion Visual Instruction Tuning (EmoVIT) architecture based on InstructBLIP  [9] . This architecture specifically leverages its Instruction-aware Q-Former Module, as depicted in Fig.  4  c, for emotion-centric instructional tasks. We note that the data generated by our approach is not confined to a single model but can also be applied to other Visual Instruction Tuning models, such as LLaVA  [25] . Notably, when LLaVA is fine-tuned with our data, it exhibits a significant enhancement in emotion recognition capabilities, as detailed in Sec. 4.2. In this way, we demonstrate not only the effectiveness but also the transferability of our generated data, showing its broad applicability and impact.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implemental Details",
      "text": "Our implementation is based on the LAVIS library  [31] . Our EmoVIT starts with a pre-trained InstructBLIP baseline and proceeds to fine-tune exclusively the Q-Former module, whilst keeping both the image encoder and the language model frozen. The parameters for our training adhere to the default settings established by InstructBLIP. Datasets. We evaluate our framework on ten benchmark datasets annotated under different scenarios and class number, namely EmoSet  [13] , WEBEmo  [11] , Emo-tion6  [34] , the Flickr and Instagram (FI)  [35] , Artphoto  [36] , IAPS  [37] , Abstract  [36] , EmotionROI  [38] , UnbiasedEmo  [11] , and OxfordTVG-HIC  [33] . Held-in Pretraining. Following previous work  [9] , we divide our dataset into two categories: held-in for pretraining and held-out for evaluation 2 . Considering the EmoSet dataset's comprehensive inclusion of emotion attributes for each image, it has been chosen as the primary resource for our held-in pretraining phase. Simultaneously, for a broader assessment, we perform held-out evaluations using the test sets from various other datasets.\n\nFor the generation of emotion visual instruction data, we initially employ the BLIP2 model for image captioning, followed by leveraging the GPT-4 API to generate emotion instruction data. In total, our collection comprises Categorical, Conversation, and Reasoning instruction data, derived from 51,200 unique images. This represents less than 50% of the entire EmoSet.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Held-Out Evaluation",
      "text": "As shown in Tab. 1, our proposed methodology exhibits a marked superiority in performance relative to the burgeoning Visual Instruction Tuning Methods. Since they have been pre-trained on dozens of large-scale datasets, it is evident that our generated emotion visual instruction data is particularly effective for emotional understanding Our results signify a paradigm shift, heralding a new era of model training that relies less on explicit supervision and more on the robustness of emotion instruction-driven learning.\n\nThe Effectiveness of Our Proposed Emotion Visual Instruction Data. As the first to introduce the concept of emotion visual instruction data, our study seeks to evaluate the generalizability of this newly generated instruction data. Our goal is to test its efficacy not only with InstructBLIP but also across other Visual Instruction Tuning model, to understand its broader applicability. As depicted in Fig.  6 , we employ two Visual Instruction Tuning models, LLaVA and InstructBLIP, which were fine-tuned on our specially gen- 2 Unlike the setup in InstructBLIP, our dataset exclusively comprises emotion-related content. Consequently, our held-out evaluation does not constitute a strict zero-shot evaluation in the conventional sense. Figure  6 . The improvement of our proposed emotion visual instruction tuning data tuning on LLaVA  [7]  and InstructBLIP  [9] . erated emotion visual instruction data. Subsequent testing across five distinct datasets reveals notable improvements in both models, substantiating the efficacy of our generated data. Notably, InstructBLIP demonstrated a more substantial overall enhancement compared to LLaVA. This can be attributed to InstructBLIP's specialized Instruction-aware Q-Former Module, which adeptly extracts the salient features of our emotion instructions and synergizes them effectively with the corresponding images, thereby yielding improved performance.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Effectiveness Of Different Instruction Data",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study Of Different Instruction Data",
      "text": "The ablation study outlined in Tab. 2 provides a comprehensive analysis of the impact that different instructional data types have on model performance, specifically concerning accuracy metrics on the EmoSet test set. Initially, the model, referred to as InstructBLIP  [9] , operates without the integration of the three types of instructional data and attains a baseline accuracy of 42.20%. This foundational performance is significantly enhanced with the inclusion of Categorical data, which alone contributes to a substantial increase in accuracy. The introduction of Conversation data further amplifies this effect, underscoring the value of conversational context in improving the model's predictive capabilities. The addition of Reasoning data notably boosts performance, achieving a peak accuracy of 83.36%. This indicates that the model significantly benefits from the nuanced cues in reasoning, aiding in understanding complex emotional instructions. The gradual improvements with each data type support the idea that a diverse approach to instructional data markedly enhances model comprehension and performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Instruction Sensitivity",
      "text": "This work is dedicated to the creation of a varied corpus of visual emotion instruction data, alongside the development of a robust instruction-based model. Our objective is for the model to demonstrate stability, producing consistent results in the face of minor variations in instruction phrasing, provided the core objective of the task persists unchanged. To this end, we employ the Sensitivity evaluation metric, as introduced by  [30] , to assess the model's fidelity in generating uniform outcomes irrespective of instructional nuances. We employ two semantically similar instructions as input prompts for the model, testing their impact on the Sensitivity score across three visual emotion datasets for different Visual Instruction Tuning models. The first instruction is: \"From the given options: cls 1, cls 2, cls 3, etc., identify the emotion that most accurately reflects the image. Ensure your selection is confined to the listed options. Respond in the format: Predicted emotion:\" The second one states: \"Please choose the emotion that best corresponds to the image from the following options: cls 1, cls 2, cls 3, etc. (Do not provide answers beyond the provided candidates.) Please reply in the following format: Predict emotion:\"\n\nAs illustrated in Fig.  7 , our approach, along with BLIP2, exhibited exceptionally low Sensitivity values, demonstrating robustness in understanding the instructions. Conversely, Flamingo and InstructBLIP displayed a higher degree of sensitivity, indicating a relative susceptibility to variations in instruction wording.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Robustness",
      "text": "Given that current emotion recognition datasets often exhibit category imbalances and labeling biases, our aim is to evaluate the generalization ability of various learning strategies more impartially. Hence, we selected the Un-BiasedEmo test set  [11] , which is uniquely suited for recognizing intricate emotions, such as those associated with identical objects or scenes, e.g., landscapes, crowds, families, babies, and animals, where the emotional undertones can be particularly subtle and complex.\n\nAs depicted in Tab. 3, our proposed methodology demonstrates superior performance when benchmarked against conventional supervised emotion recognition techniques, thereby underscoring the efficacy of our approach in more accurately discerning complex emotional contexts.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Method",
      "text": "Accuracy (%)\n\nDirect Learning  [11]  71.64 Self-Directed Learning  [11]  72.45 Joint Learning  [11]  71.64 Curriculum Learning  [11]  74.27 Ours* 74.72",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Affective Reasoning",
      "text": "In the domain of visual emotion recognition, where ambiguity and subjectivity are pervasive, the advent of an interpretable model is of considerable value. Such a model elucidates its cognitive processes, enhancing its trustworthiness and practicality in scenarios requiring a delicate grasp of emotional subtleties. Leveraging Visual Instruction Tuning, our model transcends mere categorization of emotions; it articulates the underlying rationale for its classifications. The executing commands for identifying emotions and elucidating the decision basis is illustrated below: Our model delineates the visual features influencing its determinations, thereby addressing the complexities inherent in discerning and explaining emotion-related nuances.\n\nThe explanations provide us with visual clues contained within the images, as exemplified in Fig.  8 . It provides interpretable visual indicators that inform the model's outputs, as demonstrated in our example, by disambiguating the often abstract emotional categories.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Scaling Law",
      "text": "Pretraining data. As demonstrated in Tab. 4, there is a clear correlation between the size of the pre-training dataset and improved performance. Consequently, we anticipate that an increase in training data in the future could enhance the effectiveness of Emotion Visual Instruction Tuning.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Humour Caption Generation",
      "text": "The comprehension of humor is intricately linked to the understanding of emotions. Leveraging our generative language model, we conduct a caption generation task without  modifying the model's architecture, specifically testing the model's proficiency in generating humorous captions. For this purpose, we select 50 images from the OxfordTVG-HIC dataset  [33]  and generate corresponding captions using our model. Subsequently, the captions produced by our model are compared with manually annotated captions from the dataset in a user study. Thirty participants were asked to vote on which captions were more humorous. Our modelgenerated captions receive 60% of the votes, demonstrating its effective humor generation capabilities. One sample is visualized in Fig.  9 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In our study, drawing upon the distinctive visual cues key to visual emotion recognition, we present a GPT-assisted pipeline specifically designed for generating emotion visual instruction data. The developed EmoVIT model incorporates emotion-specific instructions, leveraging LLMs for enhanced performance. Our comprehensive experiments validate its effectiveness in emotion classification, affective reasoning, and humor understanding. This comparative analysis sets a benchmark for Emotion Visual Instruction Tuning with LLMs, providing valuable insights and directions for future research in this field.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emovit: Revolutionizing Emotion Insights With Visual Instruction Tuning",
      "text": "Supplementary Material Figure  10 . The sample of our generated visual emotion instruction data.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "More Emotion Visual Instruction Data Sample",
      "text": "Additional samples from our Emotion Visual Instruction Data collection are presented in Figures  10  and 11 . Upon acceptance, the complete dataset will be made available on our project webpage.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implemental Details",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Our Experiment Settings",
      "text": "Held-out vs supervised learning. We adopt the terminology held-in and held-out as defined in the work of Instruct-BLIP  [9] . For the held-in, we utilize the training subset of the EmoSet dataset for Emotion Visual Instruction Tuning, with its corresponding test subset serving the purpose of held-in evaluation. The outcomes of this evaluation are depicted in Fig.  1  of the main manuscript. In our held-out evaluation, we focus on determining how instruction tuning bolsters the model's ability to transfer learning to new and unseen data. It's crucial to highlight that our methodology sets a distinct path from In-structBLIP's framework. Our dataset is specifically curated with emotion-centric content, presenting unique categories such as cheerfulness and enthrallment found in WEBEmo, which are not typically included in other datasets. Conversely, common emotional categories like anger and fear are shared with other collections, such as FI and Emotion6. This distinctive mix in our dataset implies that our held-out evaluation operates on a cross-domain level, examining the model's ability to interpret and adapt to diverse emotional contexts not strictly confined to zero-shot scenarios.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "System Prompt",
      "text": "The system prompt inputted into ChatGPT for the purpose of gathering instruction-based data is presented below.\n\nYou are an AI visual assistant, and you are seeing a single image. What you see are provided with one caption and some emotion related attributes, describing the same image you are looking at. Answer all questions as you are seeing the image. The range of brightness is from 0 (darkest) to 1 (brightest), and the range of colorfulness is from 0 (black-and-white) to 1 (the most colorful).\n\nDesign two questions for a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers.\n\nInclude questions asking about the visual content of the image, including the object types, object actions, relationship among objects, etc. Only include questions that have definite answers: (1) one can see the content in the image that the question asks about and can answer confidently; (2) one can determine confidently from the image that it is not in the image. Do not ask any question that cannot be answered confidently. Please answer with the format Question: Answer: Also include one complex question that is relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Details Of The Q-Former",
      "text": "Similar to the approach in InstructBLIP, Q-Former is a lightweight transformer architecture that utilizes a collection of trainable query vectors to distill visual features from a static image encoder. The Q-Former acts as the trainable module to bridge the gap between a frozen image encoder and a frozen LLM. Its role is to curate and present the most pertinent visual information, thereby enabling the LLM to generate the targeted textual output efficiently. Following the default setting, in our experimental setup, we employ 32 distinct queries, each with a dimensionality of 768.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Sensitivity Formula",
      "text": "As mentioned in Sec.4.3.2 in the main paper, we employ the Sensitivity evaluation metric, as introduced by  [30] , to assess the model's fidelity in generating uniform outcomes irrespective of instructional nuances. Specifically, for each task t ∈ T , given its associated instances with task instruc-tions: D t = {(I t j , x t j , y t j ) ∈ T × X t × Y t } N j=1 , sensitivity is defined as:",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Gpt-4 Vs Gpt-4 Turbo",
      "text": "We conducted a comparative analysis of conversational datasets derived from GPT-4 (the model name is gpt-4 in the API) against the recently released GPT-4 Turbo (the model name is gpt-4-1106-preview in the API). The comparative metrics yielded negligible differences between the two models (83.36% vs 82.96% on EmoSet test set).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Adding In-Context Samples In Held-Out Evaluation",
      "text": "Recent LLMs are capable of in-context learning when provided with a limited number of examples in a few-shot manner. In this work, we have also embarked on such an exploration. For instance, Tab. 6 presents the in-context samples utilized within the EmotionROI dataset. During our heldout evaluation, we incorporated three in-context samples for each category, consisting of a caption paired with its corresponding emotion class. Nevertheless, in our experimental observations, we did not witness any enhancement in performance attributable to furnishing the LLM with these incontext examples. Consequently, our finalized methodology did not incorporate in-context samples during the heldout evaluation phase.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the importance of instruction-following abil-",
      "page": 1
    },
    {
      "caption": "Figure 1: , when directly query",
      "page": 1
    },
    {
      "caption": "Figure 2: Performance comparison on EmoSet test set [13] (Accu-",
      "page": 2
    },
    {
      "caption": "Figure 3: The comparison of different visual tuning paradigms.",
      "page": 3
    },
    {
      "caption": "Figure 3: In Fig. 3(a), conventional tuning methodologies en-",
      "page": 3
    },
    {
      "caption": "Figure 4: The overall architecture of our proposed method. The Emotion Instruction data generated by (a) will be used for Emotion",
      "page": 4
    },
    {
      "caption": "Figure 4: (a). For an image Ximg,",
      "page": 4
    },
    {
      "caption": "Figure 5: Our strategy for generating emotion instruction data",
      "page": 5
    },
    {
      "caption": "Figure 4: b, we have developed an Emotion",
      "page": 5
    },
    {
      "caption": "Figure 4: c, for emotion-centric instructional tasks.",
      "page": 5
    },
    {
      "caption": "Figure 5: The sample of our generated visual emotion instruction",
      "page": 5
    },
    {
      "caption": "Figure 6: The improvement of our proposed emotion visual instruc-",
      "page": 6
    },
    {
      "caption": "Figure 7: , our approach, along with BLIP2,",
      "page": 7
    },
    {
      "caption": "Figure 7: The sensitivity score comparison (the lower the better).",
      "page": 7
    },
    {
      "caption": "Figure 8: The sample of our generated explanation.",
      "page": 8
    },
    {
      "caption": "Figure 8: It provides inter-",
      "page": 8
    },
    {
      "caption": "Figure 9: The sample of our generated humour caption vs human",
      "page": 8
    },
    {
      "caption": "Figure 9: 5. Conclusion",
      "page": 8
    },
    {
      "caption": "Figure 10: The sample of our generated visual emotion instruction",
      "page": 9
    },
    {
      "caption": "Figure 1: of the main manuscript.",
      "page": 9
    },
    {
      "caption": "Figure 11: The sample of our generated visual emotion instruction",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "Sup",
          "Column_5": "e\nV",
          "Column_6": "rvised\nisual",
          "Column_7": "E\nIn",
          "Column_8": "motio\nstructi",
          "Column_9": "n\no",
          "Column_10": "Reco\nnTun",
          "Column_11": "g\nin",
          "Column_12": "nition\ngMet",
          "Column_13": "M\nh",
          "Column_14": "ethod\nods",
          "Column_15": "s",
          "Column_16": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Cross\nAttention",
          "Column_2": "",
          "Feed\nForward": ""
        },
        {
          "Column_1": "Self",
          "Column_2": "",
          "Feed\nForward": ""
        },
        {
          "Column_1": "",
          "Column_2": "Atte",
          "Feed\nForward": "ntion"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Basic Interaction": "Advanced Interaction"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "2014, face and Gesture submission ID 324",
      "authors": [
        "F Lastname"
      ],
      "venue": "2014, face and Gesture submission ID 324"
    },
    {
      "citation_id": "2",
      "title": "Frobnication tutorial",
      "year": "2014",
      "venue": "Frobnication tutorial"
    },
    {
      "citation_id": "3",
      "title": "Frobnication",
      "authors": [
        "F Alpher"
      ],
      "year": "2002",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "4",
      "title": "Frobnication revisited",
      "authors": [
        "F Alpher",
        "F Fotheringham-Smythe"
      ],
      "year": "2003",
      "venue": "Journal of Foo"
    },
    {
      "citation_id": "5",
      "title": "Can a machine frobnicate?",
      "authors": [
        "F Alpher",
        "F Fotheringham-Smythe",
        "F Gamow"
      ],
      "year": "2004",
      "venue": "Journal of Foo"
    },
    {
      "citation_id": "6",
      "title": "Can a computer frobnicate",
      "authors": [
        "F Alpher",
        "F Gamow"
      ],
      "year": "2005",
      "venue": "CVPR"
    },
    {
      "citation_id": "7",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2007",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "J.-B Alayrac",
        "J Donahue",
        "P Luc",
        "A Miech",
        "I Barr",
        "Y Hasson",
        "K Lenc",
        "A Mensch",
        "K Millican",
        "M Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Instructblip: Towards generalpurpose vision-language models with instruction tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2007",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "Stimuli-aware visual emotion analysis",
      "authors": [
        "J Yang",
        "J Li",
        "X Wang",
        "Y Ding",
        "X Gao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "11",
      "title": "Contemplating visual emotions: Understanding and overcoming dataset bias",
      "authors": [
        "R Panda",
        "J Zhang",
        "H Li",
        "J.-Y Lee",
        "X Lu",
        "A Roy-Chowdhury"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Mdan: Multi-level dependent attention network for visual emotion analysis",
      "authors": [
        "L Xu",
        "Z Wang",
        "B Wu",
        "S Lui"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Emoset: A large-scale visual emotion dataset with rich attributes",
      "authors": [
        "J Yang",
        "Q Huang",
        "T Ding",
        "D Lischinski",
        "D Cohen-Or",
        "H Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2007",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Wscnet: Weakly supervised coupled networks for visual sentiment classification and detection",
      "authors": [
        "D She",
        "J Yang",
        "M.-M Cheng",
        "Y.-K Lai",
        "P Rosin",
        "L Wang"
      ],
      "year": "2019",
      "venue": "IEEE Trans-actions on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Pdanet: Polarity-consistent deep attention network for finegrained visual emotion regression",
      "authors": [
        "S Zhao",
        "Z Jia",
        "H Chen",
        "L Li",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Weakly supervised emotion intensity prediction for recognition of emotions in images",
      "authors": [
        "H Zhang",
        "M Xu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Exploring discriminative representations for image emotion recognition with cnns",
      "authors": [
        "W Zhang",
        "X He",
        "W Lu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Z Lian",
        "L Sun",
        "M Xu",
        "H Sun",
        "K Xu",
        "Z Wen",
        "S Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "21",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "J Wei",
        "M Bosma",
        "V Zhao",
        "K Guu",
        "A Yu",
        "B Lester",
        "N Du",
        "A Dai",
        "Q Le"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "22",
      "title": "Most important person-guided dual-branch cross-patch attention for group affect recognition",
      "authors": [
        "H Xie",
        "M.-X Lee",
        "T.-J Chen",
        "H.-J Chen",
        "H.-I Liu",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "S Zhang",
        "L Dong",
        "X Li",
        "S Zhang",
        "X Sun",
        "S Wang",
        "J Li",
        "R Hu",
        "T Zhang",
        "F Wu"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: A survey",
      "arxiv": "arXiv:2308.10792"
    },
    {
      "citation_id": "24",
      "title": "Visual prompt tuning",
      "authors": [
        "M Jia",
        "L Tang",
        "B.-C Chen",
        "C Cardie",
        "S Belongie",
        "B Hariharan",
        "S.-N Lim"
      ],
      "year": "2022",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "25",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "26",
      "title": "Scaling vision transformers to 22 billion parameters",
      "authors": [
        "M Dehghani",
        "J Djolonga",
        "B Mustafa",
        "P Padlewski",
        "J Heek",
        "J Gilmer",
        "A Steiner",
        "M Caron",
        "R Geirhos",
        "I Alabdulmohsin"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "27",
      "title": "Auassisted graph attention convolutional network for microexpression recognition",
      "authors": [
        "H.-X Xie",
        "L Lo",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "28",
      "title": "An overview of facial micro-expression analysis: Data, methodology and challenge",
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2004",
      "venue": "Tech. Rep"
    },
    {
      "citation_id": "30",
      "title": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning",
      "authors": [
        "Z Xu",
        "Y Shen",
        "L Huang"
      ],
      "year": "2022",
      "venue": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning",
      "arxiv": "arXiv:2212.10773"
    },
    {
      "citation_id": "31",
      "title": "Lavis: A library for language-vision intelligence",
      "authors": [
        "D Li",
        "J Li",
        "H Le",
        "G Wang",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2022",
      "venue": "Lavis: A library for language-vision intelligence",
      "arxiv": "arXiv:2209.09019"
    },
    {
      "citation_id": "32",
      "title": "Eu ai act: first regulation on artificial intelligence",
      "authors": [
        "E Parliament"
      ],
      "year": "2023",
      "venue": "Eu ai act: first regulation on artificial intelligence"
    },
    {
      "citation_id": "33",
      "title": "Oxfordtvg-hic: Can machine make humorous captions from images",
      "authors": [
        "R Li",
        "S Sun",
        "M Elhoseiny",
        "P Torr"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "34",
      "title": "A mixed bag of emotions: Model, predict, and transfer emotion distributions",
      "authors": [
        "K.-C Peng",
        "T Chen",
        "A Sadovnik",
        "A Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2006",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "36",
      "title": "Affective image classification using features inspired by psychology and art theory",
      "authors": [
        "J Machajdik",
        "A Hanbury"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Emotional category data on images from the international affective picture system",
      "authors": [
        "J Mikels",
        "B Fredrickson",
        "G Larkin",
        "C Lindberg",
        "S Maglio",
        "P Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "38",
      "title": "Where do emotions come from? predicting the emotion stimuli map",
      "authors": [
        "K.-C Peng",
        "A Sadovnik",
        "A Gallagher",
        "T Chen"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on image processing (ICIP"
    },
    {
      "citation_id": "39",
      "title": "Learning to prompt for vision-language emotion recognition",
      "authors": [
        "H Xie",
        "H Chung",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    }
  ]
}