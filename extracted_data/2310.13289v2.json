{
  "paper_id": "2310.13289v2",
  "title": "Salmonn: Towards Generic Hearing Abili-Ties For Large Language Models",
  "published": "2023-10-20T05:41:57Z",
  "authors": [
    "Changli Tang",
    "Wenyi Yu",
    "Guangzhi Sun",
    "Xianzhao Chen",
    "Tian Tan",
    "Wei Li",
    "Lu Lu",
    "Zejun Ma",
    "Chao Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditoryinformation-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Text-based large language models (LLMs)  (Brown et al., 2020; Touvron et al., 2023; Chiang et al., 2023; Anil et al., 2023; Du et al., 2022)  have demonstrated remarkable and even human-level performance in many natural language processing (NLP) tasks  (OpenAI, 2023) . Meanwhile, instruction tuning  (Wei et al., 2022a; Chung et al., 2022; Ouyang et al., 2022; Peng et al., 2023) , where data is organised as pairs of user instruction (or prompt) and reference response, has emerged as an LLM training paradigm that allows LLMs to follow open-ended user instructions. There is a burgeoning research interest in empowering LLMs with multimodal perception abilities. Recent studies focus on connecting LLMs with either the encoder of one additional type of input, such as image  (Li et al., 2023a; Alayrac et al., 2022; Dai et al., 2023) , silent video  (Maaz et al., 2023; Chen et al., 2023b; Zhao et al., 2022) , audio events  (Gong et al., 2023b; Lyu et al., 2023)  or speech  (Chen et al., 2023a) , or the encoders of multiple input types together  (Su et al., 2023; Zhang et al., 2023b) . A connection module and LLM adaptors can be used to align the encoder output spaces with the LLM input space, which are often trained by cross-modal pre-training and instruction tuning.\n\nIn this paper, we propose a speech audio language music open neural network (SALMONN), which is a single audio-text multimodal LLM that can perceive and understand three basic types of sounds including speech, audio events, and music. To enhance the performance on both speech and nonspeech audio tasks, SALMONN uses a dual encoder structure consisting of a speech encoder from the Whisper speech model  (Radford et al., 2023)  and a BEATs audio encoder  (Chen et al., 2023c) . A window-level query Transformer (Q-Former)  (Li et al., 2023a)  is used as the connection module to convert a variable-length encoder output sequence to a variable number of augmented audio tokens input to the Vicuna LLM  (Chiang et al., 2023)  and can achieve audio-text alignment with high temporal resolution. The low-rank adaptation (LoRA) approach  (Hu et al., 2022)  is applied to Vicuna as a cross-modal adaptor to align Vicuna's augmented input space with its output space and further improve its performance. A number of speech, audio, and music tasks are used in the cross-modal pre-training and instruction tuning stages of the window-level Q-Former and LoRA. The resulting multimodal LLMs can be confined to the specific types of tasks used in instruction tuning, particularly speech recognition and audio captioning, and exhibit limited or no cross-modal emergent abilities, which we refer to as the task over-fitting issue. In this paper, cross-modal emergent abilities refer to the abilities to perform cross-modal tasks unseen in training, which are essentially the emergent abilities of LLMs  (Wei et al., 2022b)  that are lost during instruction tuning. As a solution, we propose an extra few-shot activation tuning stage so that SALMONN regains the emergent abilities of LLMs and alleviates the considerable catastrophic forgetting to the trained tasks.\n\nIn order to evaluate SALMONN's cognitive hearing abilities, a wide range of speech, audio events, and music benchmarks are used. The tasks can be divided into three levels. The first level benchmarks eight tasks that are trained in instruction tuning, such as speech recognition, translation and audio captioning, while the other two levels benchmark untrained tasks. The second level includes five speech-based NLP tasks, such as translation to untrained languages and slot filling, which relies on multilingual and high-quality alignments between speech and text tokens. The last level of tasks, including audio-based storytelling and speech audio co-reasoning etc, requires understanding not only speech but also non-speech auditory information. Experimental results show that SALMONN as a single model can perform all these tasks and achieve competitive performance on standard benchmarks, which reveals the feasibility of building artificial intelligence (AI) that can \"hear\" and understand general audio inputs consisting of mixtures of speech, audio events, and music.\n\nThe main contribution of this paper can be summarised as follows.\n\n• We propose SALMONN, the first multimodal LLM that can perceive and understand general audio inputs with speech, audio events, and music, to the best of our knowledge.\n\n• We study the presence of cross-modal emergent abilities by playing with the LoRA scaling factor, and propose a cheap activation tuning method as an extra training stage that can activate the crossmodal emergent abilities and alleviate catastrophic forgetting to tasks seen in training.\n\n• We evaluate SALMONN on a range of tasks reflecting a degree of generic hearing abilities, and propose two novel tasks, audio-based storytelling and speech audio co-reasoning.",
      "page_start": 1,
      "page_end": 9
    },
    {
      "section_name": "Related Work",
      "text": "LLMs, as text-based dialogue models, have a fundamental connection with speech, and several studies have attempted to extend LLMs to support direct speech inputs with a connection module  (Chen et al., 2023a; Wu et al., 2023; Fathullah et al., 2023; Yu et al., 2023; Huang et al., 2023a) .\n\nTo avoid the LLMs having overly long input speech token sequences caused by long-form speech inputs, different frame-rate reduction approaches have been developed, including stacking-based fixed-rate reduction approach  (Fathullah et al., 2023; Yu et al., 2023) , speech-recognition-based variable frame-rate reduction approach  (Wu et al., 2023; Chen et al., 2023a) , and Q-Former-based approach with a fixed number of output frames  (Yu et al., 2023)  etc. When LLM-based speech synthesis is also considered, the LLM output space can be augmented with speech tokens as well, such as SpeechGPT  (Zhang et al., 2023a)  and AudioPaLM  (Rubenstein et al., 2023) .\n\nUnlike speech, audio event inputs are often treated as fixed-sized spectrogram images that can be processed using visual-language LLM methods without explicitly modelling temporal correlations  (Gong et al., 2023a; b; Zhang et al., 2023b) . These methods are therefore unable to handle speech.\n\nAlthough  Lyu et al. (2023)  uses the speech encoder from the Whisper model, only audio event inputs are supported, which indicates the difficulty of the joint modelling of speech and audio events. Without using LLMs,  Narisetty et al. (2022)  studies achieving speech recognition and audio captioning separately using the same model. Regarding music inputs,  Liu et al. (2023)  integrates the MERT music encoder  (Li et al., 2023b)  with an LLM for music understanding tasks. AudioGPT allows a text-based LLM to process speech, audio events, and music by interacting with other models in a pipeline based on a set of pre-defined tasks  (Huang et al., 2023b) . Compared with AudioGPT, SALMONN is an end-to-end model with cross-modal emergent abilities for open-ended tasks.\n\nIn addition to audio, multimodal LLMs are more widely studied in visual modalities, such as image  (Zhu et al., 2023; Li et al., 2023a) , video  (Maaz et al., 2023; Chen et al., 2023b)  and audio-visual  (Su et al., 2023; Lyu et al., 2023; Sun et al., 2023) . Modality alignment in those models is often achieved via either a fully connected layer or an attention-based module. In particular, the Q-Former structure  (Li et al., 2023a)  used by SALMONN is commonly applied to visual modalities, such as in  MiniGPT-4 (Zhu et al., 2023) , InstructBLIP  (Dai et al., 2023) , Video-LLaMA  (Zhang et al., 2023b) .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "The model architecture of SALMONN is introduced in Section 3.1. Our training method is presented in Section 3.2, which includes the pre-training and fine-tuning stages, and the proposed activation tuning stage as a solution to the task over-fitting issue.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "The model architecture of SALMONN is shown in Fig.  1 . The output features of the two complementary auditory encoders are synchronised and combined. Q-Former is used as the connection module and applied at the frame level, whose output sequence is integrated with the text instruction prompt and fed into the LLM with LoRA adaptors to generate the text response.\n\nDual Auditory Encoders: A speech encoder from OpenAI's Whisper model  (Radford et al., 2023)  and a non-speech BEATs audio encoder  (Chen et al., 2023c)  are used. The Whisper model is trained for speech recognition and translation based on a large amount of weakly supervised data, whose encoder output features are suitable to model speech and include information about the background noises  (Gong et al., 2023a) . BEATs is trained to extract high-level non-speech audio semantics information using iterative self-supervised learning. The input audio is first tokenised then masked and predicted in training. The tokeniser is updated by distilling the semantic knowledge of the audio tokens  (Chen et al., 2023c) . Therefore, the resulting auditory features of these two encoders are complementary and suitable for general audio inputs with both speech and non-speech information.\n\nSince both encoders have the same output frame rate of 50Hz, the concatenated output features are\n\nwhere X is a variable-length general audio input sequence, Encoder whisper (•) and Encoder beats (•) are the Whisper and BEATs encoder, Concat(•) is the frame-by-frame concatenation operation along the feature dimension, Z is the concatenated encoder output sequence with T frames.\n\nWindow-level Q-Former: The Q-Former structure is commonly used to convert the output of an image encoder into a fixed number of textual input tokens of an LLM  (Li et al., 2023a) , which requires modification when applied to handle audio inputs of variable lengths. Specifically, regarding the encoder output of an input image l as a Z l , Q-Former employs a fixed number of N trainable queries Q to transform Z l into N textual tokens H l using a number of stacked Q-Former blocks. A Q-Former block is similar to a Transformer decoder block  (Vaswani et al., 2017) , apart from the use of a fixed number of trainable static queries Q in the first block and the removal of the causal masks from the self-attention layers. In this way, Q-Former allows the queries in Q to refer to each other first using a self-attention layer and then interact with Z l using cross-attention layers.\n\nRegarding a variable-length general audio input with Z = [Z t ] T t=1 , by segmenting Z into L-sized windows and padding the last window with zeros, it becomes [{Z t } l×L t=(l-1)×L+1 ]\n\n⌈T /L⌉ l=1 , instead of using Q-Former at the sequence level to convert the entire Z into N textual tokens, SALMONN uses Q-Former at the window level as if the encoder output frames stacked in each window were an image. As a result, the textual token sequence H becomes\n\nwhere Q-Former(•) is the Q-Former function and H has ⌈T /L⌉ × N textual tokens. The windowlevel Q-Former uses a variable number of textual tokens and is more efficient for variable-length sequences. In addition, H is enforced to have a monotonic alignment with Z, resulting in better temporal resolution which is important for speech recognition.\n\nLLM and LoRA: A pre-trained Vicuna LLM is used in this work  (Chiang et al., 2023)  which is a LLaMA LLM  (Touvron et al., 2023)  fine-tuned to follow instructions.  LoRA (Hu et al., 2022)  is a widely used parameter-efficient fine-tuning method for LLM adaptation, which is used in SALMONN to adapt the query and value weight matrices in the self-attention layers of Vicuna.\n\nIn this work, LoRA is trainable while Vicuna is not.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Training Method",
      "text": "A three-stage cross-modal training method of SALMONN is introduced in this section. Besides the pre-training and instruction tuning stages used by recent visual LLMs  (Dai et al., 2023; Zhang et al., 2023b) , an additional activation tuning stage is proposed to resolve the issue of over-fitting to the speech recognition and audio captioning tasks in instruction tuning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pre-Training Stage:",
      "text": "To mitigate the gap between the pre-trained parameters (LLM and encoders) and the randomly initialised parameters (connection module and adaptor), a large amount of speech recognition and audio captioning data is used to pre-train the window-level Q-Former and LoRA. These two tasks contain key auditory information about the contents of speech and non-speech audio events, and both do not require complex reasoning and understanding and therefore can help SALMONN to learn high-quality alignment between the auditory and textual information.\n\nInstruction Tuning Stage: Similar to NLP  (Wei et al., 2022a)  and visual-language  (Dai et al., 2023) , audio-text instruction tuning with a list of supervised speech, audio event, and music tasks, as shown in Section 4.2 and Table  1 , is used as the second stage of SALMONN training. The tasks are selected based on their importance (e.g. speech recognition and audio captioning) and the indispensability of having such ability in tests (e.g. overlapping speech recognition, phone recognition and music captioning). The instruction prompts are generated based on the texts paired with the audio data.\n\nTask Over-fitting: Although SALMONN built with only the first two stages of training can produce competitive results on the tasks trained in instruction tuning, it exhibits limited or almost no ability to perform untrained cross-modal tasks, especially the tasks that require cross-modal co-reasoning abilities. In particular, the model sometimes violates the instruction prompts and generates irrelevant responses as if it received an instruction related to a task commonly seen in training (e.g. speech recognition). We refer to this phenomenon as task over-fitting. A theoretical analysis of the issue is presented here and detailed experimental verification is provided in Sections 5.3 and 5.4.\n\nWe attribute task over-fitting to two reasons. First, compared to the text-only data used in LLM training, only simpler instruction prompts are used in our cross-modal instruction tuning  (Wei et al., 2022a)  and the resulting responses are not as complex and diverse. Meanwhile, some tasks included in instruction tuning, in particular speech recognition and audio captioning, have more deterministic outputs than the other tasks, such as speech and audio question answering. These two reasons combined to cause the intrinsic conditional language model (LM) to bias to an ill-formed distribution with poor generalisation ability, which prohibits SALMONN from performing untrained crossmodal tasks. More specifically, at test time the response text sequence Ŷ of a test input X given a new instruction prompt I can be generated according to Ŷ = arg max Y P Λ (Y|X, I), which is also the objective to maximise in training. Using the Bayes' Rule, there is\n\nSince only limited text responses are seen in SALMONN training, the intrinsic conditional LM P Λ (Y|X) is biased towards the Y sequences that are strongly-aligned with X, such as the transcriptions of automatic speech recognition (ASR) and automatic audio captioning (AAC) tasks, especially simple and short transcriptions. From Eqn.\n\n(3), this causes I ′ , zero-shot instructions with more diverse responses, to have small P Λ (Y|X, I ′ ).\n\nActivation Tuning Stage: An effective approach to alleviating task over-fitting is to regularise the intrinsic conditional LM P Λ (Y|X). An easy way to achieve this is to fine-tune SALMONN on tasks with longer and more diverse responses, such as auditory-information-based question answering and storytelling. Paired training data for such tasks can be generated based on the text paired with speech recognition or audio and music caption data, either manually by human annotators or automatically by prompting a text-based LLM.\n\nWe use an efficient approach that can enable SALMONN to generate long and diverse responses for zero-shot instructions by simply reducing the scaling factor of the LoRA adaptor. This is an alternative way to regularise P Λ (Y|X) since the intrinsic conditional LM can only be learned with the window-level Q-Former and LoRA, since they are the only modules that are updated in training. The effect of reducing the LoRA scaling factor can be found in Section 5.2, which can indeed activate question-answering and storytelling abilities and produce long and diversified responses but also considerably degrade the results on the trained tasks. To retain competitive results while restoring the cross-modal emergent abilities, we propose to use the responses generated by SALMONN with a discounted LoRA scaling factor to perform the third stage of fine-tuning termed activation tuning.\n\nExperimental results showed later in Section 5.4 demonstrate that activation tuning is an efficient and effective few-shot self-supervised training approach.\n\n4 EXPERIMENTAL SETUP   (Panayotov et al., 2015)  and 1000-hour GigaSpeech M-set  (Chen et al., 2021)  for speech recognition, as well as 2800-hour WavCaps  (Mei et al., 2023)  (with audio clips longer than 180 seconds removed), AudioCaps  (Kim et al., 2019)  and Clotho  (Drossos et al., 2020)  datasets for audio captioning.\n\nThe second instruction tuning stage involves multiple tasks, including ASR, automatic speech translation (AST), AAC, phone recognition (PR), emotion recognition (ER), music captioning (MC), overlapped speech recognition (OSR), speaker verification (SV), gender recognition (GR) and speech question answering (SQA), audio question answering (AQA) and music question answering (MQA). In the SQA, AQA and MQA tasks, the questions are generated based on the text caption labels using ChatGPT, and the model needs to provide answers based on the general audio input and the text prompt with a question. The data used in this stage is listed in Table  1 , where \"En2Zh\" refers to AST from English to Chinese.\n\nFor the final activation tuning stage, twelve stories were written based on the audio clips by SALMONN with a reduced LoRA. Then the model is trained using teacher-forcing-based crossentropy training for 12 steps, with each step using only one story sample, when activating the crossmodal emergent abilities of SALMONN.   (Thickstun et al., 2017)  400 + 3 48K + 0.3K Total -∼4400 ∼2.3M",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task Specifications",
      "text": "Since text LLMs have the abilities of zero-shot learning via instruction tuning  (Wei et al., 2022a) , the emergence of such abilities is expected when high-quality cross-modal alignment is achieved when connecting the backbone text-based LLM with multimodal encoders. To evaluate the zeroshot cross-modal emergent abilities of SALMONN, 15 types of speech, audio, and music tasks are selected and divided into three different levels.\n\nTask level 1 consists of the tasks used in instruction tuning and are therefore easiest for SALMONN to perform. The list of such tasks and their training data are given in Section 4.2, and the evaluation metrics for each task are presented in Table  2 .\n\nTask level 2 includes untrained tasks and is therefore more difficult than level 1. The level 2 tasks are speech-based NLP tasks including speech keyword extracting (KE), which evaluates the accuracy of the keywords extracted based on the speech content; spoken-query-based question answering (SQQA), which evaluates the common sense knowledge retrieved based on the question in speech; speech-based slot filling (SF) that evaluates the accuracy of the required slot values, usually named entities, obtained from the speech content. Two AST tasks, En2De (English to German) and En2Ja (English to Japanese) are also included, which are also considered as cross-modal emergent abilities since only En2Zh is trained in instruction tuning. Vicuna, the backbone LLM of SALMONN, can perform all level 2 tasks based on speech transcriptions. Therefore, SALMONN is to achieve such tasks based on speech in a fully end-to-end way without requiring any explicit speech recognition.\n\nTask level 3 has the most difficult tasks including audio-based storytelling (Story) and speech audio co-reasoning (SAC). Audio-based storytelling is to write a meaningful story based on the auditory information from the general audio inputs. SAC requires the model to understand a spoken question embedded in the input audio clip, find evidence from the background audio events or music, and reason from it to answer the question. Both level 3 tasks are new tasks that are first proposed in this paper, to the best of our knowledge, which requires SALMONN to perceive speech, audio, and music, and to understand and reason based on the auditory information in a fully end-to-end way.\n\nTable 2 lists all test sets and their evaluation metrics. Following rate (FR) is an extra metric used for some level 2 and level 3 tasks, which measures the percentage that SALMONN can successfully follow the instructions. FR is considered since the selected tasks are complex and easier to suffer from the violation of instructions caused by task over-fitting. It is worth noting that we only calculate the diversity metric of the Story task by counting the number of different words in the story, which simply represents the richness of the story instead of the quality.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Full Results On All 15 Tasks",
      "text": "The results on all 15 tasks produced by SALMONN are shown in Table  3 . From the results, SALMONN, without or with activation tuning, can produce competitive results on all level 1 tasks. However, the model without activation tuning suffers severely from task over-fitting and can barely perform level 2 and level 3 tasks. In particular, in SQQA, Story, and SAC, where multimodal interactions are emphasised, SALMONN without activation tuning can hardly follow the instructions. The FRs of performing SQQA, SF, Story and SAC tasks improve considerably with activation tuning. More detailed analysis about the results are discussed in Appendix B due to page limit.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discounting Lora Scaling Factor",
      "text": "This section explores the influence of the use of test-time discounting of the LoRA scaling factor for alleviating the task over-fitting issue without activation tuning. As shown in Fig.  3 , when the LoRA scaling factor decreases to around 2.0, i.e. to half of its original value, the model suddenly emerges  with cross-modal reasoning abilities, which, together with the drops in %PER, proves the existence of the intrinsic conditional LM embedded in LoRA.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis Of Task Over-Fitting And Activation Tuning",
      "text": "To verify the influences of the intrinsic conditional LM, we calculate the perplexities (PPLs) of P Λ (Y|X, I) and P Λ (Y|X) of each step of the activation tuning stage. In detail, for a given audio X, the probability of the Y sequence corresponding to the probed task is calculated using teacher forcing based on the text instruction prompt I of a certain task or without using any I.\n\nAs shown in Fig.  4 , PPLs were compared between the ground-truth responses of the task of Story/SAC (generated by discounting the LoRA scaling factor) and the responses of the task that the model performed incorrectly due to task over-fitting (AAC in this example). In sub-figures (a) and (b), no text instruction prompt was used when computing P Λ (Y|X), showing that without activation tuning the PPL of the Y of AAC was obviously lower than that of the Y of Story/SAC. During activation tuning, the PPL gaps between these tasks were mitigated, meaning the bias to the dominant AAC task was considerably reduced. In sub-figures (c) and (d), we probed P Λ (Y|X, I), the PPLs with the instructions of Story and SAC respectively. It is revealed that before activation tuning, the PPL of AAC is lower than that of Story/SAC even if the text instruction prompt is to perform Story/SAC, which explains the failure of the model to follow the instruction. During activation tuning, the PPL values of the Y of Story/SAC gradually become lower than those of AAC, and the model can eventually perform the instructed task.    ) , can. This lies in the fact that the ASR task emphasises improving high-quality cross-modal alignments that make the distribution even more biased towards the intrinsic conditional LM. As for text-based fine-tuning on stories, this actually affect P (Y|T x , I) instead of P (Y|X, I), where T x is the reference text of the speech. Therefore, text-based training cannot alleviate task over-fitting. QA data with long answers can activate the LLM, but the performance of the activated model is worse than that activated using the Story task, which especially results in a higher repeat rate of the response generation. That is possibly due to the fact that the paired answer to the QA task is less diverse than the Story task, which leads to less effective regularisation of the intrinsic conditional LM P Λ (Y|X).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This work proposes SALMONN, a speech audio language music open neural network that can be regarded as a step towards generic hearing abilities for LLMs. Equipped with dual auditory encoders, SALMONN achieved competitive performances on trained tasks including speech recognition, audio captioning and speech translation etc., while generalising to a range of untrained understanding tasks such as slot filling, speech translation for untrained languages and keyword extracting. Moreover, a proposed activation tuning stage enables SALMONN with remarkable emergent abilities, such as audio-based storytelling and speech audio co-reasoning. As a result, with thorough and comprehensive experimental evaluations, SALMONN has demonstrated a promising direction for developing generic hearing AI in the future.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Salmonn",
      "text": "Please describe the audio.\n\nA person is speaking while gunshots are being fired in the background.\n\n(A man asks \"Can you guess where I am right now\", and explosions and gunshots can be heard in the background. )\n\nUSER Based on the background sound, it seems like the speaker is in a war zone or a combat situation. The sound of gunfire and explosions can be heard in the background. The speaker is asking if the listener can guess where they are. It was a beautiful day in the countryside. The sun was shining, the birds were singing, and the wind was blowing gently through the fields. In the midst of all this natural beauty, a group of ducks were swimming in a pond.\n\nAs they swam, one of the ducks began to quack in a unique way. It was as if the duck was trying to say something important. The other ducks noticed this and began to listen.\n\nSuddenly, a man appeared on the edge of the pond. He had heard the duck's The music is emotional and evokes feelings of love and affection. The reason for this is that the music is a love song, with lyrics that express the deep emotions and feelings of love and affection. The music is also instrumental, which means that it relies solely on the melody and harmony of the instruments to convey these emotions. This creates a more intimate and personal experience for the listener, allowing them to interpret the music in their own way and connect with it on a deeper level.   3 , with activation tuning 2  , SALMONN 1. performs close to the state-of-the-art results on the trained tasks including ASR, En2Zh, AAC and MC; 2. learns to handle PR and OSR tasks which are difficult for the cascaded approach of \"Vicuna + Whisper\"; 3. generalises well on a wide range of speech-grounded NLP tasks, especially such as En2De and En2Ja where SALMONN performs better than the \"Whisper+Vicuna\" cascaded system, which indicates the superior of the audio-grounding system to cascade system on avoiding error propagation and the loss of non-linguistic information (e.g. prosody); 4. tackles tasks such as audio-based storytelling and SAC which existing models can not handle to our best knowledge. The underlying reason for using the cascaded Whisper + Vicuna system for reference values of the level 2 tasks lies in the fact that all level 2 tasks are zero-shot and there is no other audiogrounding system apart from SALMONN that can perform such tasks as zero-shot. An advantage of SALMONN over the cascaded system is that SALMONN can leverage both linguistic and paralinguistic information required for spoken language understanding.\n\nDespite such advantages, SALMONN has performance limitations on some tasks.\n\n1. First, PR is achieved by extending the LLM to consider phonemes as a new writing system. Since recognising phonemes requires finer-grained modelling of pronunciation information than recognising the word pieces used by the original Whisper ASR, it is not easy for the SALMONN model built upon an existing Whisper speech encoder to perform as well as a specialised model on the PR task.\n\n2. Similarly, SALMONN has a relatively high WER on OSR since the Whisper ASR was not able to perform OSR.\n\n3. The success of SQQA mainly relies on the understanding of the spoken questions (e.g. \"What is the highest mountain in the world\") and answering the questions based on the commonsense knowledge stored in the text-based LLM. The drop in SQQA performance indicates that the use of LoRA cross-modal adaptation may cause the LLM to \"forget\" some text-based commonsense knowledge.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C Statistics Of Task Over-Fitting",
      "text": "In this section, we analyse the changes of perplexity (PPL) in the three training stages to shed light on the underlying principle of activation tuning. As shown in      words in the story to represent the diversity metric for storytelling. For the SAC task, we use GPT-3.5 to determine whether our model follows the instruction or answers the question correctly based on the background audio caption and the question.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "F Prompt Template For Salmonn",
      "text": "To train SALMONN to generate responses to a text instruction given an audio input, we utilize a prompt template as follows:\n\nUSER: [Auditory Tokens] Text Prompt \\n ASSISTANT:\n\nThe \"[Auditory Tokens]\" are the output variable length text-like tokens of the window-level Q-Former. Noted that this template matches the one used for training Vicuna  (Chiang et al., 2023) .",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "G Prompts Interacting With Gpt3.5",
      "text": "In this work, we utilize GPT3.5 to help us generate some data and evaluate the quality of the model output automatically. We list our prompts for different purposes in Table  6 . Words with all capital letters in the prompt will be replaced with the appropriate description before being fed into GPT3.5.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "H Potential To Extend Salmonn To Speech And Audio Generation",
      "text": "Although SALMONN is designed to focus on enabling LLMs with hearing abilities, it is possible to extend SALMONN to speech generation.  3  The human speech production mechanism is related to auditory perception. A well-known phenomenon attributed to the speech chain is the \"Lombard reflex\" which describes the effect where individuals raise their voice level to be heard more clearly while speaking in noisy environments  (Lane & Tranel, 1971) . This reveals the importance of having generic hearing abilities when empowering AI with fully human-like speaking abilities, and the opportunities in text-to-speech (TTS) synthesis enabled by SALMONN. This also matches the recent development in TTS that the text and audio contexts from the surrounding utterances are useful to achieve more natural prosody modelling and enable the use of more natural and casual speech data  (Xu et al., 2021; Guo et al., 2021; Oplustil-Gallegos et al., 2021; Zhang et al., 2023c) .",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Purposes Prompts",
      "text": "To generate audio QA data given audio caption text.\n\nBelow I will give you some sentences that you will need to help me generate **only one** question, and its corresponding answer. These sentences are captions of some audio. Your question should be highly related to the audio caption, and your answer must be **correct**, and should be simple and clear. \\n Your response should strictly follow the format below: \\n {\"Question\": \"xxx\", \"Answer\": \"xxx\"} \\n Here are the sentences:\n\nTo generate speech QA data given speech recognition text.\n\nBelow I will give you some sentences that you will need to help me generate **only one** question, and its corresponding answer. Your question should be highly related to the sentences, and your answer must be **correct**, and should be simple and clear. \\n Your response should strictly follow the format below: \\n {\"Question\": \"xxx\", \"Answer\": \"xxx\"} \\n Here are the sentences:\n\nTo evaluate answers of the model of spoken-query-based question answering (SQQA).\n\nNext I will give you a question and give you the corresponding standard answer and the answer I said. You need to judge whether my answer is correct or not based on the standard answer to the question. I will give you the question and the corresponding answer in the following form: {'Question': 'xxx', 'Standard Answer': 'xxx', 'My Answer': 'xxx'} \\n You need to judge the correctness of my answer, as well as state a short justification. Your responses need to follow the Python dictionary format: \\n {\"Correct\": True / False, \"Reason\": \"xxx\"} \\n Now, I will give you the following question and answer: SENTENCEHERE \\n Your response is:\n\nTo evaluate whether the model attempts to do the speech audio co-reasoning (SAC) task.\n\nThere is an audio clip, and there is a person in the audio asking questions. I now have an AI model that needs to go and answer the speaker's question based on the background audio. I'll tell you the question the speaker is asking the output of my AI model, and what you need to determine: whether my AI model is trying to answer the question and why. You need to be especially careful that my model may just be describing the audio without hearing your question and answering it. You don't need to care about the correctness of the answer. All you need to focus on is whether the model is trying to answer the question. Your response needs to follow the format of the python dictionary: {\"Response\": \"Yes/No\", \"Reason\": \"xxx\"}.\\n Question in audio: <QUESTION> \\n Model Output: <OUTPUT> \\n Your Response:\n\nTo evaluate whether the model successfully completes the SAC task.\n\nThere is an audio clip, and there is a person in the audio asking questions. I now have an AI model that needs to go and answer the speaker's question based on the background audio. I'll tell you the question asked by the speaker, some description of the background audio, and the output of my AI model, and you need to decide whether my AI model answered it correctly, and why. Your response needs to follow the format of the python dictionary: {\"Response\": \"Yes/No\", \"Reason\": \"xxx\"}.\\n Question in audio: <QUESTION> \\n Background Audio: <AUDIO> \\n Model Output: <OUTPUT> \\n Your Response:\n\nTable 6: Purposes and prompts of using GPT3.5.",
      "page_start": 23,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The output features of the two comple-",
      "page": 3
    },
    {
      "caption": "Figure 1: The model architecture of SALMONN. A window-level Q-Former is used as the con-",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates the trends of model performance change on ASR & PR, SQQA, Story, and SAC",
      "page": 7
    },
    {
      "caption": "Figure 3: , when the LoRA",
      "page": 7
    },
    {
      "caption": "Figure 2: Performance changes on ASR & PR (a), SQQA (b), Story (c) and SAC (d) along with the",
      "page": 8
    },
    {
      "caption": "Figure 3: Performance changes on ASR & PR (a), SQQA (b), Story (c) and SAC (d) together with",
      "page": 8
    },
    {
      "caption": "Figure 4: , PPLs were compared between the ground-truth responses of the task of",
      "page": 8
    },
    {
      "caption": "Figure 4: Changes in PPL during the activation tuning stage. (a) shows −ln PΛ(Y|X) for AAC and",
      "page": 9
    },
    {
      "caption": "Figure 5: Automatic Speech Recognition",
      "page": 14
    },
    {
      "caption": "Figure 6: Automatic Speech Translation (En2De)",
      "page": 14
    },
    {
      "caption": "Figure 7: Phone Recognition",
      "page": 14
    },
    {
      "caption": "Figure 8: Automatic Audio Captioning",
      "page": 15
    },
    {
      "caption": "Figure 9: Joint Speech and Audio Captioning",
      "page": 15
    },
    {
      "caption": "Figure 10: Speech Audio Coreasoning",
      "page": 15
    },
    {
      "caption": "Figure 11: Audio Event Detection",
      "page": 16
    },
    {
      "caption": "Figure 12: Audio Story Telling",
      "page": 16
    },
    {
      "caption": "Figure 13: Speaker Recognition",
      "page": 16
    },
    {
      "caption": "Figure 14: Emotion Recognition",
      "page": 17
    },
    {
      "caption": "Figure 15: Spoken-query-based Question Answering",
      "page": 17
    },
    {
      "caption": "Figure 16: Keywords Extracting",
      "page": 17
    },
    {
      "caption": "Figure 17: Slot Filling",
      "page": 18
    },
    {
      "caption": "Figure 18: Music Captioning",
      "page": 18
    },
    {
      "caption": "Figure 19: Music Emotion Recognition",
      "page": 18
    },
    {
      "caption": "Figure 20: Lyrics Writing",
      "page": 19
    },
    {
      "caption": "Figure 21: Titling Music",
      "page": 19
    },
    {
      "caption": "Figure 22: Changes in perplexity during the three-stage training. S1 is cross-modal pre-training",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Whisper\nEncoder ❄": "BEATs\nEncoder ❄"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ASR\nPR": "",
          "Column_2": "ASR\nPR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SQQA Acc\nSQQA FR": "",
          "Column_2": "SQQA Acc\nSQQA FR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Story Div.\nStory FR": "",
          "Column_2": "Story Div.\nStory FR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SAC Acc\nSAC FR": "",
          "Column_2": "SAC Acc\nSAC FR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ASR\nPR": "",
          "Column_2": "ASR\nPR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SQQA Acc\nSQQA FR": "",
          "Column_2": "SQQA Acc\nSQQA FR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Story Div.\nStory FR": "",
          "Column_2": "Story Div.\nStory FR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SAC Acc\nSAC FR": "",
          "Column_2": "SAC Acc\nSAC FR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nRecognize the speech and give me the transcription.\nUSER\nCan you guess where I am right now?\nSALMONN\nFigure5: AutomaticSpeechRecognition": "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nListen to the speech and translate it into German."
        },
        {
          "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nRecognize the speech and give me the transcription.\nUSER\nCan you guess where I am right now?\nSALMONN\nFigure5: AutomaticSpeechRecognition": "USER\nKannst du erraten, wohin ich gerade fahre?\nSALMONN\nFigure6: AutomaticSpeechTranslation(En2De)"
        },
        {
          "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nRecognize the speech and give me the transcription.\nUSER\nCan you guess where I am right now?\nSALMONN\nFigure5: AutomaticSpeechRecognition": "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nProvide the phonetic transcription for the speech."
        },
        {
          "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nRecognize the speech and give me the transcription.\nUSER\nCan you guess where I am right now?\nSALMONN\nFigure5: AutomaticSpeechRecognition": "USER\nsil K AH0 N Y UW1 G EH1 S W EH1 R AY1 EY1 EH1 M R AY1 T N AW1 sil\nSALMONN\nFigure7: PhoneRecognition\n14"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nPlease describe the audio.\nUSER\nA person is speaking while gunshots are being fired in the background.\nSALMONN\nFigure8: AutomaticAudioCaptioning": "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )"
        },
        {
          "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nPlease describe the audio.\nUSER\nA person is speaking while gunshots are being fired in the background.\nSALMONN\nFigure8: AutomaticAudioCaptioning": "Recognize what the speaker says and describe the background audio at the same\ntime.\nUSER\nThe speaker says \"Can you guess where I am right now?\" while the background\naudio has gunshots and explosions.\nSALMONN\nFigure9: JointSpeechandAudioCaptioning"
        },
        {
          "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nPlease describe the audio.\nUSER\nA person is speaking while gunshots are being fired in the background.\nSALMONN\nFigure8: AutomaticAudioCaptioning": "(A man asks \"Can you guess where I am right now\", and explosions and"
        },
        {
          "(A man asks \"Can you guess where I am right now\", and explosions and\ngunshots can be heard in the background. )\nPlease describe the audio.\nUSER\nA person is speaking while gunshots are being fired in the background.\nSALMONN\nFigure8: AutomaticAudioCaptioning": "gunshots can be heard in the background. )\nPlease answer the speaker's question in detail based on the background sound.\nUSER\nBased on the background sound, it seems like the speaker is in a war zone or a\ncombat situation. The sound of gunfire and explosions can be heard in the\nbackground. The speaker is asking if the listener can guess where they are.\nSALMONN\nFigure10: SpeechAudioCoreasoning\n15"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Birds chirping, a woman singing, a man speaking and a duck quacking.)\nPlease list each event in the audio in order.\nUSER\n1. Birds chirping in the distance.\n2. A woman singing.\n3. A man speaking.\n4. A duck quacking.\nSALMONN\nFigure11: AudioEventDetection": "(Birds chirping, a woman singing, a man speaking and a duck quacking.)\nBased on the audio, write a story in detail. Your story should be highly related to\nthe audio."
        },
        {
          "(Birds chirping, a woman singing, a man speaking and a duck quacking.)\nPlease list each event in the audio in order.\nUSER\n1. Birds chirping in the distance.\n2. A woman singing.\n3. A man speaking.\n4. A duck quacking.\nSALMONN\nFigure11: AudioEventDetection": "USER\nIt was a beautiful day in the countryside. The sun was shining, the birds were\nsinging, and the wind was blowing gently through the fields. In the midst of all this\nnatural beauty, a group of ducks were swimming in a pond.\nAs they swam, one of the ducks began to quack in a unique way. It was as if the\nduck was trying to say something important. The other ducks noticed this and began\nto listen.\nSuddenly, a man appeared on the edge of the pond. He had heard the duck's\nquacking and had come to see what was going on. The duck continued to quack,\nand the man began to speak ......\nSALMONN\nFigure12: AudioStoryTelling"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(A man says excitedly: “All right, let’s do”.)\nDescribe the emotion of the speaker.\nUSER\nThe speaker is excited.\nSALMONN\nFigure14: EmotionRecognition": "(A person asks “What is the highest mountain in the world”.)\nPlease answer the question in detail.\nUSER"
        },
        {
          "(A man says excitedly: “All right, let’s do”.)\nDescribe the emotion of the speaker.\nUSER\nThe speaker is excited.\nSALMONN\nFigure14: EmotionRecognition": "The highest mountain in the world is Mount Everest, located in the Himalayas\nbetween Nepal and Tibet. It stands at a height of 8,848 meters (29,029 feet) above\nsea level.\nSALMONN\nFigure15: Spoken-query-basedQuestionAnswering"
        },
        {
          "(A man says excitedly: “All right, let’s do”.)\nDescribe the emotion of the speaker.\nUSER\nThe speaker is excited.\nSALMONN\nFigure14: EmotionRecognition": "(A part of Steve Jobs' speech: “Your time is limited so don’t waste it living\nsomeone else’s life. Don’t be trapped by dogma which is living with the\nresults of other people’s thinking. Don’t let the noise of other’s opinions\ndrown out your own inner voice. And most important have the courage to"
        },
        {
          "(A man says excitedly: “All right, let’s do”.)\nDescribe the emotion of the speaker.\nUSER\nThe speaker is excited.\nSALMONN\nFigure14: EmotionRecognition": "follow your heart and intuition. They somehow already know what you truly\nwant to become everything else is secondary. When I was young there was an\namazing publication called the whole earth catalog, which was one of the\nbibles of my”)\nGive me only three keywords of the text. Explain your reason.\nUSER\nThe keywords are 'time', 'limited', and 'courage'.\nThe reason for choosing these keywords is that the speaker emphasizes the\nimportance of not wasting time living someone else's life, not being trapped by\ndogma, and having the courage to follow one's own inner voice.\nSALMONN\nFigure16: KeywordsExtracting\n17"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(A girl says: “Schedule a meeting with the accounting department on half past\ntwo on Friday”)\nWhat is the time mentioned in the speech?\nUSER\nHalf past two on Friday\nSALMONN\nFigure17: SlotFilling": "(It's a soothing, elegant piano piece. At the end of the audio a woman begins\nto sing.)\nPlease describe the music in detail.\nUSER"
        },
        {
          "(A girl says: “Schedule a meeting with the accounting department on half past\ntwo on Friday”)\nWhat is the time mentioned in the speech?\nUSER\nHalf past two on Friday\nSALMONN\nFigure17: SlotFilling": "This is a beautiful piano and vocal piece. The piano plays a simple melody while\nthe female vocalist sings in a high register. The atmosphere is romantic and dreamy.\nThe piece has a soft and mellow feel to it. This piece could be used in a romantic\nmovie scene or as a lullaby.\nSALMONN\nFigure18: MusicCaptioning"
        },
        {
          "(A girl says: “Schedule a meeting with the accounting department on half past\ntwo on Friday”)\nWhat is the time mentioned in the speech?\nUSER\nHalf past two on Friday\nSALMONN\nFigure17: SlotFilling": "(It's a soothing, elegant piano piece. At the end of the audio a woman begins\nto sing.)"
        },
        {
          "(A girl says: “Schedule a meeting with the accounting department on half past\ntwo on Friday”)\nWhat is the time mentioned in the speech?\nUSER\nHalf past two on Friday\nSALMONN\nFigure17: SlotFilling": "What is the emotion of the music? Explain the reason in detail.\nUSER\nThe music is emotional and evokes feelings of love and affection. The reason for\nthis is that the music is a love song, with lyrics that express the deep emotions and\nfeelings of love and affection. The music is also instrumental, which means that it\nrelies solely on the melody and harmony of the instruments to convey these\nemotions. This creates a more intimate and personal experience for the listener,\nallowing them to interpret the music in their own way and connect with it on a\ndeeper level.\nSALMONN\nFigure19: MusicEmotionRecognition\n18"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(It's a soothing, elegant piano piece. At the end of the audio a woman begins\nto sing.)\nCan you write some lyrics of the song?\nUSER\nSure, here are some lyrics for the song:\nVerse 1:\nYou’re the one I’ve been waiting for,\nMy heart beats faster when I’m near you,\nYou’re the one I’ve been searching for,\nMy love for you will never fade.\nChorus:\nYou’re perfect, you’re flawless, ...\nSALMONN\nFigure20: LyricsWriting": "(It's a soothing, elegant piano piece. At the end of the audio a woman begins\nto sing.)\nGive me a title of the music based on its rhythm and emotion.\nUSER\nThe music is a beautiful piano piece with a slow and emotional rhythm. It is a love\nsong with a female vocalist singing in a soft and gentle tone. The title of the music\ncould be something like ‘Forever Love’ or ‘Endless Emotion’.\nSALMONN\nFigure21: TitlingMusic\nB PERFORMANCE ANALYSIS\nAsshowninTable3,withactivationtuning2,SALMONN\n1. performs close to the state-of-the-art results on the trained tasks including ASR, En2Zh,\nAACandMC;\n2. learnstohandlePRandOSRtaskswhicharedifficultforthecascadedapproachof“Vicuna"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "w/ LoRA\nw/o LoRA": "S2 S3-2\nS",
          "Column_3": "3-4\nS\nckpt",
          "Column_4": "3-6\nS",
          "Column_5": "3-8\nS3",
          "Column_6": "-10 S3-1"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "w/ LoRA\nw/o LoRA": "S2 S3-2\nS",
          "Column_3": "3-4\nS\nckpt",
          "Column_4": "3-6\nS",
          "Column_5": "3-8\nS3",
          "Column_6": "-10 S3-1"
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Generating music from text",
      "authors": [
        "Andrea Agostinelli",
        "I Timo",
        "Zalán Denk",
        "Jesse Borsos",
        "Mauro Engel",
        "Antoine Verzetti",
        "Qingqing Caillon",
        "Aren Huang",
        "Adam Jansen",
        "Marco Roberts",
        "Tagliasacchi"
      ],
      "year": "2023",
      "venue": "Generating music from text",
      "arxiv": "arXiv:2301.11325"
    },
    {
      "citation_id": "2",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr"
      ],
      "year": "2022",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "3",
      "title": "PaLM 2 technical report",
      "authors": [
        "Rohan Anil",
        "Andrew Dai",
        "Orhan Firat",
        "Melvin Johnson",
        "Dmitry Lepikhin",
        "Alexandre Passos",
        "Siamak Shakeri",
        "Emanuel Taropa",
        "Paige Bailey",
        "Zhifeng Chen"
      ],
      "year": "2023",
      "venue": "PaLM 2 technical report",
      "arxiv": "arXiv:2305.10403"
    },
    {
      "citation_id": "4",
      "title": "SLURP: A spoken language understanding resource package",
      "authors": [
        "Emanuele Bastianelli",
        "Andrea Vanzo",
        "Pawel Swietojanski",
        "Verena Rieser"
      ],
      "year": "2020",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "5",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "6",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Bootstrapping advanced large language models by treating multi-modalities as foreign languages",
      "authors": [
        "Feilong Chen",
        "Minglun Han",
        "Haozhi Zhao",
        "Qingyang Zhang",
        "Jing Shi",
        "Shuang Xu",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "Bootstrapping advanced large language models by treating multi-modalities as foreign languages",
      "arxiv": "arXiv:2305.04160"
    },
    {
      "citation_id": "8",
      "title": "Modeling video sequence with large language models",
      "authors": [
        "Guo Chen",
        "Yin-Dong Zheng",
        "Jiahao Wang",
        "Jilan Xu",
        "Yifei Huang",
        "Junting Pan",
        "Yi Wang",
        "Yali Wang",
        "Yu Qiao",
        "Tong Lu"
      ],
      "year": "2023",
      "venue": "Modeling video sequence with large language models",
      "arxiv": "arXiv:2305.13292"
    },
    {
      "citation_id": "9",
      "title": "GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio",
      "authors": [
        "Guoguo Chen",
        "Shuzhou Chai",
        "Guanbo Wang",
        "Jiayu Du",
        "Wei-Qiang Zhang",
        "Chao Weng",
        "Dan Su",
        "Daniel Povey",
        "Jan Trmal",
        "Junbo Zhang"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "10",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "BEATs: Audio pre-training with acoustic tokenizers",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "12",
      "title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph Gonzalez",
        "Ion Stoica",
        "Eric Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality"
    },
    {
      "citation_id": "13",
      "title": "Scaling instructionfinetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay"
      ],
      "year": "2022",
      "venue": "Scaling instructionfinetuned language models",
      "arxiv": "arXiv:2210.11416"
    },
    {
      "citation_id": "14",
      "title": "LibriMix: An open-source dataset for generalizable speech separation",
      "authors": [
        "Joris Cosentino",
        "Manuel Pariente",
        "Samuele Cornell",
        "Antoine Deleforge",
        "Emmanuel Vincent"
      ],
      "year": "2020",
      "venue": "LibriMix: An open-source dataset for generalizable speech separation",
      "arxiv": "arXiv:2005.11262"
    },
    {
      "citation_id": "15",
      "title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning",
      "authors": [
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Meng",
        "Huat Tiong",
        "Junqi Zhao"
      ],
      "year": "2023",
      "venue": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning",
      "arxiv": "arXiv:2305.06500"
    },
    {
      "citation_id": "16",
      "title": "LLM-based pseudo music captioning",
      "authors": [
        "Seungheon Doh",
        "Keunwoo Choi",
        "Jongpil Lee",
        "Juhan Nam",
        "Lp-Musiccaps"
      ],
      "year": "2023",
      "venue": "LLM-based pseudo music captioning",
      "arxiv": "arXiv:2307.16372"
    },
    {
      "citation_id": "17",
      "title": "Clotho: An audio captioning dataset",
      "authors": [
        "Konstantinos Drossos",
        "Samuel Lipping",
        "Tuomas Virtanen"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "18",
      "title": "GLM: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "19",
      "title": "Prompting large language models with speech recognition abilities",
      "authors": [
        "Yassir Fathullah",
        "Chunyang Wu",
        "Egor Lakomkin",
        "Junteng Jia",
        "Yuan Shangguan",
        "Ke Li",
        "Jinxi Guo",
        "Wenhan Xiong",
        "Jay Mahadeokar",
        "Ozlem Kalinli"
      ],
      "year": "2023",
      "venue": "Prompting large language models with speech recognition abilities",
      "arxiv": "arXiv:2307.11795"
    },
    {
      "citation_id": "20",
      "title": "Whisper-AT: Noise-robust automatic speech recognizers are also strong general audio event taggers",
      "authors": [
        "Yuan Gong",
        "Sameer Khurana",
        "Leonid Karlinsky",
        "James Glass"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Listen, think, and understand",
      "authors": [
        "Yuan Gong",
        "Hongyin Luo",
        "Alexander Liu",
        "Leonid Karlinsky",
        "James Glass"
      ],
      "year": "2023",
      "venue": "Listen, think, and understand",
      "arxiv": "arXiv:2305.10790"
    },
    {
      "citation_id": "22",
      "title": "Conversational end-to-end TTS for voice agents",
      "authors": [
        "Haohan Guo",
        "Shaofei Zhang",
        "Frank Soong",
        "Lei He",
        "Lei Xie"
      ],
      "year": "2021",
      "venue": "Proc. SLT"
    },
    {
      "citation_id": "23",
      "title": "LoRA: Low-Rank Adaptation of large language models",
      "authors": [
        "J Edward",
        "Phillip Hu",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2022",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "24",
      "title": "Dynamic-SUPERB: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech",
      "authors": [
        "Chien-Yu Huang",
        "Ke-Han Lu",
        "Shih-Heng Wang",
        "Chi-Yuan Hsiao",
        "Chun-Yi Kuan",
        "Haibin Wu",
        "Siddhant Arora",
        "Kai-Wei Chang",
        "Jiatong Shi",
        "Yifan Peng"
      ],
      "year": "2023",
      "venue": "Dynamic-SUPERB: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech",
      "arxiv": "arXiv:2309.09510"
    },
    {
      "citation_id": "25",
      "title": "AudioGPT: Understanding and generating speech, music, sound, and talking head",
      "authors": [
        "Rongjie Huang",
        "Mingze Li",
        "Dongchao Yang",
        "Jiatong Shi",
        "Xuankai Chang",
        "Zhenhui Ye",
        "Yuning Wu",
        "Zhiqing Hong",
        "Jiawei Huang",
        "Jinglin Liu"
      ],
      "year": "2023",
      "venue": "AudioGPT: Understanding and generating speech, music, sound, and talking head",
      "arxiv": "arXiv:2304.12995"
    },
    {
      "citation_id": "26",
      "title": "Adapting self-supervised models to multi-talker speech recognition using speaker embeddings",
      "authors": [
        "Zili Huang",
        "Desh Raj",
        "Paola García",
        "Sanjeev Khudanpur"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "27",
      "title": "Improved automatic keyword extraction given more linguistic knowledge",
      "authors": [
        "Anette Hulth"
      ],
      "year": "2003",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "28",
      "title": "AudioCaps: Generating captions for audios in the wild",
      "authors": [
        "Chris Dongjoo Kim",
        "Byeongchang Kim",
        "Hyunmin Lee",
        "Gunhee Kim"
      ],
      "year": "2019",
      "venue": "Proc. NAACL-HLT"
    },
    {
      "citation_id": "29",
      "title": "The Lombard sign and the role of hearing in speech",
      "authors": [
        "Harlen Lane",
        "Bernard Tranel"
      ],
      "year": "1971",
      "venue": "Journal of Speech Hearing Research"
    },
    {
      "citation_id": "30",
      "title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "31",
      "title": "MERT: Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "Yizhi Li",
        "Ruibin Yuan",
        "Ge Zhang",
        "Yinghao Ma",
        "Xingran Chen",
        "Hanzhi Yin",
        "Chenghua Lin",
        "Anton Ragni",
        "Emmanouil Benetos",
        "Norbert Gyenge"
      ],
      "year": "2023",
      "venue": "MERT: Acoustic music understanding model with large-scale self-supervised training",
      "arxiv": "arXiv:2306.00107"
    },
    {
      "citation_id": "32",
      "title": "Music understanding LLaMA: Advancing text-to-music generation with question answering and captioning",
      "authors": [
        "Shansong Liu",
        "Atin Sakkeer Hussain",
        "Chenshuo Sun",
        "Ying Shan"
      ],
      "year": "2023",
      "venue": "Music understanding LLaMA: Advancing text-to-music generation with question answering and captioning",
      "arxiv": "arXiv:2308.11276"
    },
    {
      "citation_id": "33",
      "title": "Multi-modal language modeling with image, audio, video, and text integration",
      "authors": [
        "Chenyang Lyu",
        "Minghao Wu",
        "Longyue Wang",
        "Xinting Huang",
        "Bingshuai Liu"
      ],
      "year": "2023",
      "venue": "Multi-modal language modeling with image, audio, video, and text integration",
      "arxiv": "arXiv:2306.09093"
    },
    {
      "citation_id": "34",
      "title": "Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan",
        "Video-Chatgpt"
      ],
      "year": "2023",
      "venue": "Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "35",
      "title": "WavCaps: A ChatGPT-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "authors": [
        "Xinhao Mei",
        "Chutong Meng",
        "Haohe Liu",
        "Qiuqiang Kong",
        "Tom Ko",
        "Chengqi Zhao",
        "Mark Plumbley",
        "Yuexian Zou",
        "Wenwu Wang"
      ],
      "year": "2023",
      "venue": "WavCaps: A ChatGPT-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "arxiv": "arXiv:2303.17395"
    },
    {
      "citation_id": "36",
      "title": "Voxceleb: Large-scale speaker verification in the wild",
      "authors": [
        "Arsha Nagrani",
        "Son Joon",
        "Weidi Chung",
        "Andrew Xie",
        "Zisserman"
      ],
      "year": "2019",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "37",
      "title": "Joint speech recognition and audio captioning",
      "authors": [
        "Chaitanya Narisetty",
        "Emiru Tsunoo",
        "Xuankai Chang",
        "Yosuke Kashiwagi",
        "Michael Hentschel",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP, Singapore, 2022. OpenAI. GPT-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "38",
      "title": "Comparing acoustic and textual representations of previous linguistic context for improving text-to-speech",
      "authors": [
        "Pilar Oplustil-Gallegos",
        "Johannah O' Mahony",
        "Simon King"
      ],
      "year": "2021",
      "venue": "Proc. SSW"
    },
    {
      "citation_id": "39",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "40",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "41",
      "title": "Instruction tuning with GPT-4",
      "authors": [
        "Baolin Peng",
        "Chunyuan Li",
        "Pengcheng He",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Instruction tuning with GPT-4",
      "arxiv": "arXiv:2304.03277"
    },
    {
      "citation_id": "42",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "43",
      "title": "A large language model that can speak and listen",
      "authors": [
        "Paul Rubenstein",
        "Chulayuth Asawaroengchai",
        "Dung Duc",
        "Ankur Nguyen",
        "Zalán Bapna",
        "Félix Borsos",
        "De Chaumont",
        "Peter Quitry",
        "Dalia Chen",
        "Wei Badawy",
        "Eugene Han",
        "Kharitonov"
      ],
      "year": "2023",
      "venue": "A large language model that can speak and listen",
      "arxiv": "arXiv:2306.12925"
    },
    {
      "citation_id": "44",
      "title": "One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai",
        "Pandagpt"
      ],
      "year": "2023",
      "venue": "One model to instruction-follow them all",
      "arxiv": "arXiv:2305.16355"
    },
    {
      "citation_id": "45",
      "title": "Fine-grained audio-visual joint representations for multimodal large language models",
      "authors": [
        "Guangzhi Sun",
        "Wenyi Yu",
        "Changli Tang",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Fine-grained audio-visual joint representations for multimodal large language models",
      "arxiv": "arXiv:2310.05863"
    },
    {
      "citation_id": "46",
      "title": "Learning features of music from scratch",
      "authors": [
        "John Thickstun",
        "Zaid Harchaoui",
        "Sham Kakade"
      ],
      "year": "2017",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "47",
      "title": "LLaMA: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "48",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "49",
      "title": "CoVoST 2 and massively multilingual speech translation",
      "authors": [
        "Changhan Wang",
        "Anne Wu",
        "Juan Pino"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "50",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Y Vincent",
        "Kelvin Zhao",
        "Adams Guu",
        "Brian Yu",
        "Nan Lester",
        "Andrew Du",
        "Quoc V Dai",
        "Le"
      ],
      "year": "2022",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "51",
      "title": "Emergent abilities of large language models",
      "authors": [
        "Jason Wei",
        "Yi Tay",
        "Rishi Bommasani",
        "Colin Raffel",
        "Barret Zoph",
        "Sebastian Borgeaud",
        "Dani Yogatama",
        "Maarten Bosma",
        "Denny Zhou",
        "Donald Metzler"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "52",
      "title": "On decoder-only architecture for speech-to-text and large language model integration",
      "authors": [
        "Jian Wu",
        "Yashesh Gaur",
        "Zhuo Chen",
        "Long Zhou",
        "Yimeng Zhu",
        "Tianrui Wang",
        "Jinyu Li",
        "Shujie Liu",
        "Bo Ren",
        "Linquan Liu"
      ],
      "year": "2023",
      "venue": "On decoder-only architecture for speech-to-text and large language model integration",
      "arxiv": "arXiv:2307.03917"
    },
    {
      "citation_id": "53",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "54",
      "title": "Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis",
      "authors": [
        "Guanghui Xu",
        "Wei Song",
        "Zhengchen Zhang",
        "Chao Zhang",
        "Xiaodong He",
        "Bowen Zhou"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "55",
      "title": "WikiQA: A challenge dataset for open-domain question answering",
      "authors": [
        "Yi Yang",
        "Wen-Tau Yih",
        "Christopher Meek"
      ],
      "year": "2015",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "56",
      "title": "Connecting speech encoder and large language model for asr",
      "authors": [
        "Wenyi Yu",
        "Changli Tang",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Connecting speech encoder and large language model for asr",
      "arxiv": "arXiv:2309.13963"
    },
    {
      "citation_id": "57",
      "title": "Empowering large language models with intrinsic cross-modal conversational abilities",
      "authors": [
        "Dong Zhang",
        "Shimin Li",
        "Xin Zhang",
        "Jun Zhan",
        "Pengyu Wang",
        "Yaqian Zhou",
        "Xipeng Qiu",
        "Speechgpt"
      ],
      "year": "2023",
      "venue": "Empowering large language models with intrinsic cross-modal conversational abilities",
      "arxiv": "arXiv:2305.11000"
    },
    {
      "citation_id": "58",
      "title": "Video-LLaMA: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-LLaMA: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "59",
      "title": "Prosody modelling with pre-trained cross-utterance representations for improved speech synthesis",
      "authors": [
        "Ya-Jie Zhang",
        "Chao Zhang",
        "Wei Song",
        "Zhengchen Zhang",
        "Yonghui Wu",
        "Xiaodong He"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "60",
      "title": "Learning video representations from large language models",
      "authors": [
        "Yue Zhao",
        "Ishan Misra",
        "Philipp Krähenbühl",
        "Rohit Girdhar"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "61",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}