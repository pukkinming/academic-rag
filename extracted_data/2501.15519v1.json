{
  "paper_id": "2501.15519v1",
  "title": "Fuzzy-Aware Loss For Source-Free Domain Adaptation In Visual Emotion Recognition",
  "published": "2025-01-26T13:20:52Z",
  "authors": [
    "Ying Zheng",
    "Yiyi Zhang",
    "Yi Wang",
    "Lap-Pui Chau"
  ],
  "keywords": [
    "Source-free domain adaptation",
    "visual emotion recognition",
    "fuzzy-aware learning",
    "loss function"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Source-free domain adaptation in visual emotion recognition (SFDA-VER) is a highly challenging task that requires adapting VER models to the target domain without relying on source data, which is of great significance for data privacy protection. However, due to the unignorable disparities between visual emotion data and traditional image classification data, existing SFDA methods perform poorly on this task. In this paper, we investigate the SFDA-VER task from a fuzzy perspective and identify two key issues: fuzzy emotion labels and fuzzy pseudo-labels. These issues arise from the inherent uncertainty of emotion annotations and the potential mispredictions in pseudolabels. To address these issues, we propose a novel fuzzyaware loss (FAL) to enable the VER model to better learn and adapt to new domains under fuzzy labels. Specifically, FAL modifies the standard cross entropy loss and focuses on adjusting the losses of non-predicted categories, which prevents a large number of uncertain or incorrect predictions from overwhelming the VER model during adaptation. In addition, we provide a theoretical analysis of FAL and prove its robustness in handling the noise in generated pseudo-labels. Extensive experiments on 26 domain adaptation sub-tasks across three benchmark datasets demonstrate the effectiveness of our method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "V ISUAL emotion recognition (VER) has emerged as a crucial research direction in computer vision and affective computing [1]-  [3] . With the explosive growth of social media and digital content, VER has shown significant potential for applications such as human-robot interaction  [4]  [5], social media analytics  [6] , and enhancing entertainment experiences  [7] . By interpreting the emotional cues embedded in visual data like images and videos, VER enables machines to more effectively comprehend and react to human emotions, ultimately enhancing user experiences. However, despite significant progress in recent years, VER still encounters several issues in real-world scenarios. One of the main issues is the diversity of visual data distribution. Visual emotional data Twitter I (T1)  [10]  75.  6  24.4 60.2 39.8 Instagram (In)  [11]  49.5 50.5 43.1 56.9 Flickr (Fl)  [11]  44.8 55.  2 31.6 68.4  is usually collected from various sources and environments, leading to substantial differences in data distribution. For example, users from different social platforms may come from diverse cultural backgrounds and use different shooting conditions, resulting in distinct emotional expression styles and features. This distribution difference makes it difficult for models trained on one dataset to perform well on new datasets. Furthermore, visual emotional data often raises privacy concerns, and the difficulty and cost associated with acquiring large-scale annotated data further limit the generalization ability of VER models  [8] .\n\nTo address the aforementioned issues, source-free domain adaptation (SFDA) presents a promising solution. SFDA seeks to facilitate a trained model to learn and adapt to the data distribution of an unlabeled target domain without relying on source domain data  [9] . This approach allows the model to achieve strong performance in the target domain while alleviating privacy concerns related to the use of source domain data. By leveraging SFDA, models can effectively perform emotion recognition in the target domain, thereby enhancing the practicality and reliability of VER systems. However, existing SFDA methods are primarily designed for general image classification tasks and do not perform well when directly applied to VER. This is mainly due to the specific complexity of VER. Firstly, the diversity and subtlety of emotional expressions make the emotion recognition task more complex than general classification tasks. Secondly, the subjectivity of emotional data increases the difficulty of model adaptation. Existing SFDA methods usually do not consider these specific emotional characteristics, leading to poor performance in the SFDA-VER task.\n\nTo tackle the challenges of SFDA-VER, we first delve into this task from a fuzzy perspective. Specifically, through the analysis of visual emotion data, we identify two aspects  of fuzzy problems in SFDA-VER. (1) The fuzzy emotion label problem, refers to the inherent uncertainty in emotion labels. The annotation of emotional data is usually subjective, so existing VER datasets generally determine the emotional category labels of images through voting, which results in a certain degree of ambiguity in the labels themselves. Table  I  shows the percentage of certain and uncertain samples across three datasets that contain two emotional categories, i.e., positive and negative. Uncertainty in emotional labels is prevalent across these datasets, particularly in the negative category of the Flickr  [11]  dataset, where 68.4% of the samples have inconsistent annotations. (  2 ) The fuzzy pseudolabel problem, which means the inaccuracy of the model's prediction. In existing SFDA work, many approaches rely on pseudo-labels generated by the source-pretrained model for adaptation. The pseudo-labels may be incorrect, which also contributes to fuzzy problems. We alternate between using EmoSet and FI as the source and target domain, and test the classification performance of the source model on the target dataset, as shown in Table  II . The accuracy of each category reflects the reliability of the pseudo-labels generated by the source model. Overall, the reliability of pseudo-labels is not very high, especially the accuracy of the fear category is only 26.2% when F→E. Potential incorrect pseudo-labels can be viewed as noise, which may lead to overfitting on noisy data during adaptation. Based on the above observations, we propose a fuzzy-aware learning method to address the fuzzy problems, in which the core is a novel fuzzy-aware loss (FAL) function. FAL combines the standard cross entropy loss with a fuzzy-aware loss term. This new loss term adaptively calibrates the losses from other categories to give attention to the fuzzy problems, thereby achieving more robust domain adaptation. We also provide a theoretical analysis of FAL and demonstrate its robustness to noise in generated pseudo-labels. Furthermore, we establish the connection between FAL and two classical loss functions, i.e., focal loss  [14]  and reverse cross entropy loss  [15] , to enhance the understanding of FAL.\n\nTo validate the effectiveness of the proposed method, we conduct extensive experiments comparing eight SFDA methods and eight loss functions across multiple public datasets. These datasets include 2-class sentiment datasets, 8-class emotion datasets, and the Office-Home object recognition dataset, encompassing a total of 26 different domain adaptation sub-tasks. Our experimental results reveal that existing SFDA methods exhibit poor performance on the SFDA-VER task, while loss functions perform better. However, these loss functions struggle on general SFDA datasets. This indicates a disparity between SFDA-VER and traditional SFDA tasks, which leads to varying performance for SFDA methods and loss functions. In contrast, our method not only achieves the best performance on the SFDA-VER task but also demonstrates comparable results to state-of-the-art methods on the challenging Office-Home dataset.\n\nIn summary, our contributions are as follows:\n\n• We explore the SFDA-VER problem from a fuzzy perspective, which holds significant implications for data privacy protection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we will introduce the related literature in the field of visual emotion recognition and source-free domain adaptation. Given that the proposed method is closely related to loss functions designed for learning with noisy labels, we will also outline the related research in this area.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Visual Emotion Recognition",
      "text": "Existing VER work can be classified into two main categories based on the study subjects: human-centered  [16]  [17] and human-agnostic  [18]    [12] . Human-centered emotion recognition (HCER) focuses on identifying the emotional states of the target person, necessitating the presence of human subjects within the visual content. Traditional HCER methods predominantly utilize human-related attributes, such as facial expressions  [19] , body postures  [20] , and acoustic behaviors  [21] , to predict people's emotional state. Recently, advancements in psychological research have spurred interest in context-aware emotion recognition (CARE)  [22]    [23] . CARE aims to enhance emotional features by extracting rich contextual information from images. For example, Yang et al.  [17]  introduce a counterfactual emotion inference (CLEF) framework to mitigate context bias. CLEF employs a causal graph and a context branch to eliminate the direct effects of contextual bias, thereby enhancing the robustness of visual emotion recognition.\n\nHuman-agnostic emotion recognition (HAER) investigates the emotional responses elicited by images, focusing on the emotions conveyed by the visual content without imposing restrictions on the image's subject matter. HAER does not require the presence of humans in images, thus encompassing a broader range of content and serving as a generalized form of HCER. To determine an image's emotion, it is essential to establish the connection between visual cues and abstract emotions. Early deep learning methods  [24]  [10] leverage convolutional neural networks (CNNs) to extract image features, followed by emotion classification using fully connected classifiers. For instance, You et al.  [10]  introduce a progressive CNN (PCNN) approach that incrementally finetunes a CNN model with a dataset containing generated pseudo-labels. While these methods connect the holistic image features to emotions directly, they often overlook local image cues. To address this limitation, You et al.  [25]  and Yang et al.  [18]  propose identifying significant emotional regions and constructing classification models based on these regions. To further integrate global and local emotional information, Yang et al.  [26]  develop a method using a graph convolutional network (GCN) to model the relationships between scenes and objects, thereby inferring the image's emotion through these associations. Xu et al.  [27]  design a multi-level dependent attention network (MDAN), which connects image features with emotional semantics at multiple levels, enhancing emotion recognition outcomes. Recently, Zhao et al.  [28]  propose a novel emotion recognition evaluation method based on the Mikel emotion wheel from psychology, which considers the distances between different emotions on the wheel to capture emotional nuances more accurately.\n\nFurthermore, several approaches incorporate supplementary information to enhance emotion recognition in images. Borth et al.  [29]  develop a visual concept detector library named SentiBank, which significantly improves emotion recognition performance by utilizing adjective-noun pairs. Yang et al.  [12]  demonstrate on the large-scale sentiment recognition dataset EmoSet that models integrating emotion attribute prediction can achieve superior emotion recognition results. Recently, with the remarkable performance of large-scale pre-trained models such as CLIP  [30] , there has been increased interest in leveraging large language models to provide additional guidance for emotion recognition  [31]    [32] . For instance, Deng et al.  [31]  put forward a prompt-based fine-tuning method that combines visual and language features to capture richer emotional cues. However, there remains an absence of SFDA research within the emotion recognition domain. Given the importance of data security and privacy protection, we believe that the exploration of SFDA-VER will facilitate the application of VER technology in the future.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Source-Free Domain Adaptation",
      "text": "The difference between SFDA and standard unsupervised domain adaptation (UDA) lies in their data utilization strategies. UDA methods  [33]  [34] leverage both source and target data during model training, whereas SFDA operates without access to source data, relying exclusively on a pre-trained source model and target data for updates. This characteristic of SFDA underscores its importance in maintaining data privacy. Since the inception of SFDA by Liang et al.  [35] , the field has attracted considerable interest within the research community, leading to notable advancements.\n\nIn the pioneering SHOT  [35]  framework, the authors freeze the classifier module of the source model and then update the feature extractor through an information maximization loss and self-supervised pseudo-labeling. SHOT exhibits excellent performance in the SFDA task for object recognition and has been adopted and extended by lots of subsequent works  [36] -  [43] . Building upon SHOT, Liang et al.  [36]  propose the SHOT++ method, which further improve SHOT's accuracy through a label transfer strategy and semi-supervised learning. Yang et al.  [39]  conceptualize SFDA as an unsupervised clustering problem. They accomplish effective clustering of local neighborhood features within the feature space by optimizing an objective focused on prediction consistency. Zhang et al.  [41]  introduce a paradigm termed Divide and Contrast (DaC), which divides target data into source-like and target-specific samples. Within an adaptive contrastive learning framework, DaC fulfills distinct global and local learning objectives for each sample group. Xia et al.  [43]  propose a discriminative pattern calibration (DPC) method, which leverages the discriminative patterns of target images to enhance feature representation for SFDA.\n\nMost of the aforementioned SFDA works rely on generated pseudo-labels and new loss functions to guide the learning of specific features or local structures in the target data. The proposed FAL method also aligns with this learning paradigm. However, despite the extensive body of work on SFDA, its applicability to the field of VER remains unexplored. Our experimental results show that most of the existing SFDA methods do not perform well when transferred to the VER task. Therefore, we posit the critical need for methods specifically tailored to the distinctive attributes of VER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Loss Functions For Learning With Noisy Labels",
      "text": "During the process of model adaptation to the target domain, it is typically necessary to utilize pseudo-labels generated on target images. However, these labels inevitably contain many errors, making learning based on partially incorrect labels close to the problem of learning with noisy labels (LNL). Within the field of LNL, a critical research direction involves designing loss functions that are robust to noisy labels.\n\nPrevious research has theoretically proven that the widely used cross entropy (CE) loss function lacks robustness in the presence of noisy labels  [44] . Consequently, subsequent studies  [15] ,  [45] -  [49]  have focused on developing modified versions of the CE loss to facilitate more robust learning. These methods usually mitigate the impact of noisy labels on model training by incorporating additional objective functions or calibrating the loss values. For example, the generalized cross entropy (GCE)  [45]  loss integrates the mean absolute error (MAE) with CE, balancing the two terms through a hyperparameter. Wang et al.  [15]  propose a symmetric cross entropy (SCE) loss that combines CE with a robust reverse cross entropy (RCE) loss. Considering the challenge of identifying a convex loss function under symmetric conditions, Zhou et al.  [49]  present a new family of loss functions, namely asymmetric loss functions, which preserve some beneficial properties of convex loss functions to facilitate subsequent network optimization.\n\nExisting loss functions for LNL have improved the robustness of models in noisy environments. However, since the target data addressed by these methods differ significantly from VER data in terms of emotional semantics and noise distribution, they cannot be directly applied to solve the problem of SFDA-VER. This discrepancy has been verified in the experiments of Section IV. Hence, how to design a robust loss function suitable for SFDA-VER remains an urgent challenge for further investigation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "In this section, we begin by introducing some preliminary knowledge related to SFDA and conduct a thorough analysis of the drawbacks of CE in SFDA-VER. Then, we provide a detailed description of the proposed FAL method, accompanied by theoretical analysis and discussion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Preliminaries",
      "text": "In this paper, we focus on the classical closed-set setting of SFDA, where the source and target domains share the same K classes. In the SFDA task, we first define two datasets: the source domain dataset D s containing N s samples, and the target domain dataset D t containing N t samples. Specifically, the source domain dataset is represented as D s = {(x s i , y s i )} Ns i=1 , where x s i denotes the samples from the source domain and y s i indicates the corresponding class labels. The target domain dataset is denoted as D t = {x t i } Nt i=1 , containing only samples x t i from the target domain, without providing the true class labels. Given a model M s that has been pre-trained on the source domain dataset D s using supervised learning, the objective is to adapt this model to maximize its recognition performance on the target domain D t without utilizing source data. This adaptation results in an adapted model M t such that M t (x t i ) can accurately predict the class labels of the target domain samples.\n\nIn general, the model M t is composed of two components: a feature extractor F t and a classifier C t . The feature extractor F t processes an input sample x t from the target domain, yielding a feature vector z = F t (x t ) that resides within a feature space of dimension h. Following this, the classifier C t takes the feature vector z as input and produces an output p = δ(C t (z)) ∈ R K , where δ denotes the softmax function. The output p satisfies the condition K i=1 p(i|x t ) = 1 and each p(i|x t ) represents the predicted probability that the sample x t belongs to class i. Subsequently, the pseudo-label ŷ for x t is obtained by identifying the class with the highest probability:\n\n(1)\n\nRegarding the imprecise pseudo-labels generated on target data, our goal is to formulate a robust loss function for the task of SFDA-VER. This new loss aims to mitigate the negative impact of ambiguous labels on model updates, thereby improving the model's stability and performance in handling uncertain or ambiguous inputs.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Revisit Cross Entropy Loss",
      "text": "We denote the label distribution for sample x t by q(i|x t ) and K i=1 q{i|x t } = 1. In the commonly used one-hot encoding for labels, we have q(ŷ|x t ) = 1 and q(i|x t ) = 0 for all i ̸ = ŷ. For notational convenience, we use p i and q i in place of p(i|x t ) and q(i|x t ) in all the following equations. The cross entropy (CE) loss is formulated as:\n\nRecall that when i = ŷ, q i = 1; otherwise q i = 0. Therefore, Eq. (  2 ) can be rewritten as:\n\nwhere the second term equals to 0. From Eq. (  3 ), it is evident that the CE loss only focuses on the difference between the model's predictions and the pseudo-labels. The objective is to make the model's predictions gradually approach the pseudolabels. This method is effective on datasets with clean labels, as it helps guide the model in optimizing towards the true labels. However, in the SFDA-VER task, due to the fuzzy problems, where the generated pseudo-labels are not entirely accurate, the CE loss may lead the model to overfit on incorrect pseudo-labels. As a result, the CE loss is not an ideal option in this context.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Fuzzy-Aware Learning",
      "text": "In SFDA-VER, there are two types of fuzzy problems: fuzzy emotion labels and fuzzy pseudo-labels. Both issues can result in the pseudo-labels that the model uses during domain adaptation being potentially inaccurate, i.e., ŷ may not be equal to the true label y. As we have observed in the analysis of CE loss, CE is ineffective in dealing with the fuzzy problems because it solely concentrates on the loss associated with pseudo-labels, neglecting the importance of true labels that may exist in other classes. This limitation results in poor performance when handling noisy data, as CE cannot differentiate between true and pseudo-labels. Consequently, there is a need for a more robust loss function that accounts for the impact of both true labels and pseudo-labels, thereby improving the model's performance in the SFDA-VER task.\n\nTo incorporate the potential impact of true labels belonging to other classes into the loss function, an alternative solution is to introduce a non-zero coefficient λ i to replace q i in Eq. (3). Since λ i is not zero, the outputs of other classes will also contribute to the model's updating. This approach can increase the model's attention to samples that may be misclassified during adaptation, thereby improving its robustness. Then, we have a new variant of the CE loss:\n\nwhere λ i is still an undetermined coefficient. In the Shannon entropy of information theory, p i log(p i ) represents the information content of each possible value, reflecting the con-tribution of each event's probability to the overall uncertainty.\n\nInspired by this, we set λ i = p i , then we have:\n\nIn the above equation, introducing λ i = p i enables the new loss ℓ f al better to handle data uncertainty with pseudo-labels. This allows the model to take into account the fuzzy problems between different classes during decision-making. Therefore, we refer to ℓ f al as fuzzy-aware loss (FAL) and the domain adaptation learning based on ℓ f al as fuzzy-aware learning.\n\nAdditionally, we introduce a weighting coefficient to adjust the loss for each category during the adaptation process. As a commonly used method to alleviate class imbalance  [50] , weighted loss allows for fine-tuning the impact of each category on the overall loss, enhancing the model's ability to adapt to varying category distributions. Specifically, we maintain a memory bank B to store the predictions from the last testing round for all samples. Then, the weight for class i is defined as,\n\nin which\n\nwhere n i represents the number of samples within the memory bank B that belong to the i-th category. Thus, we obtain a weight-balanced variant of FAL:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Theoretical Analysis",
      "text": "As illustrated in the first equality of Eq. (  5 ), the proposed FAL consists of two parts: the traditional CE loss and a new fuzzy-aware loss term ℓ f t = -i̸ =ŷ p i log(p i ). As highlighted in references  [44]  [45], the CE loss is unbounded and lacks robustness against noisy data. Therefore, the robustness of FAL primarily stems from the fuzzy-aware term. Next, we will theoretically explore the boundedness and robustness of the fuzzy-aware term.\n\nBoundedness analysis. Before analyzing the boundedness of ℓ f t , we first consider the function g(p) = -p log(p) for p ∈ (0, 1). When p → 0 + , although log(p) → -∞, but with the product of p, g(p) → 0. When p → 1 -, log(p) → 0, hence g(p) → 0. Next, we compute the derivative of g(p):\n\nand set g ′ (p) = 0 to find critical points:\n\nThen, we evaluate g(p) at the critical point p = e -1 :\n\nThus, the maximum and minimum values of g(p) on (0, 1) are e -1 and 0, respectively. As ℓ f t equals to i̸ =ŷ g(p i ), we have:\n\nThis demonstrates the boundedness of ℓ f t , which makes it more robust compared to unbounded losses such as CE. Robustness analysis. We consider the pseudo-label ŷ of a sample x as a noisy label, with the true label represented by y. Given any classifier f and loss function ℓ f t , we define R(f ) = E x,y ℓ f t as the risk of the classifier f with clean labels and R η (f ) = E x,ŷ ℓ f t as the risk under noisy label rate η. Let f and f * be the global minimizers of R η (f ) and R(f ), respectively. Following  [15]  [44], a loss function is deemed noise-tolerant if it ensures that the probability of misclassification at the optimal decision boundary is identical for clean and noisy data. In the following, we will introduce an assumption for the SFDA-VER task and prove ℓ f t is robust under label noises.\n\nAssumption 1. Let η yi be the probability that a sample x with a true label y is predicted as class i by the source model. The target domain dataset D t = {x t i , ŷi } Nt i=1 in SFDA-VER is clean-labels-dominant, i.e., it satisfies that ∀i ̸ = y, η yi < 1 -η y with i̸ =y η yi = η y .\n\nIn SFDA-VER, the source model exhibits significant performance differences across different categories, resulting in uneven noise distribution among various categories in the target domain dataset D t = {x t i , ŷi } Nt i=1 . This can be approximately regarded as asymmetric or class-dependent noise. Here, 1 -η y represents the probability of the label being correct, while η yi < 1 -η y indicates that for category y, the probability of correctly predicting the label is greater than the probability of being incorrectly predicted as another class. As discussed in  [49] , a method capable of learning the correct classifier when clean labels are not dominant will fail in scenarios where clean labels are dominant. This occurs because the learned classifier will likely predict samples as belonging to the less dominant class instead of the true dominant class. Therefore, to train a reliable classifier, the training dataset needs to satisfy the assumption of clean-labels-domination.\n\nAn empirical study of the clean-labels-domination assumption is illustrated in Fig.  1 . We utilized the source model (ResNet-50) trained on EmoSet to directly test its performance on FI, thereby obtaining the confusion matrix for eight emotion categories. From this figure, it can be observed that the confusion matrix is diagonal-dominant, which means that it satisfies condition η yi < 1 -η y . It should be noted that Assumption 1 is relatively strict and may not hold for each category in every task due to variations of source models and target data domains. However, based on practical experience from SFDA-VER, this assumption is generally valid in most cases. Therefore, noise-tolerant loss functions remain effective. True labels 67.9\n\n1.9  Theorem 1. When\n\nProof. Following the proof of Theorem 1 in  [15]  and the proof of Theorem 2.3 in  [51] , for asymmetric or classdependent label noise, we have:\n\nRecall that 0 < ℓ f t ≤ (K -1)e -1 in Eq. (  12 ), we have:\n\nwhere C K = K(K -1)e -1 E x,y (1 -η y ) and φ i = (1 -η yη yi ). On the other hand, we have:\n\nHence,\n\nDue to the assumption that η yi < 1 -η y , it follows that φ i = (1 -η y -η yi ) > 0. We represent the lower bound of the loss ℓ f t with b -and the upper bound with b + . As f * is the global minimizer of R(f ), we assume R(f * ) = b -. In this instance, ℓ f t (f * (x), i) should be equal to the upper bound b + for all i ̸ = y. Thus, we have:\n\nAs\n\nFinally, from Eq. (  16 ), we have:\n\nThis completes the proof.\n\nAccording to Theorem 1, we can conclude that the risk difference between the global minimizers f and f * under noisy and clean labels consistently falls within a specific boundary. Therefore, the minimizer of the true risk can be approximately regarded as the minimizer of the risk under noisy data. This demonstrates that ℓ f t is noise-tolerant under asymmetric or class-dependent label noise.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Discussion",
      "text": "In this section, we will connect two well-known loss functions-focal loss and reverse cross entropy loss-to FAL. This discussion can enhance the understanding of our method. For the clarity of discussion, we reformulate Eq. (  5 ) into a twoterm expression:\n\nFocal loss (FL), introduced by Lin et al.  [14]  for improving the performance of dense object detection tasks, also serves as a powerful loss function for classification problems. FL is defined as:\n\nwhere (1 -p i ) γ is a modulating factor and γ is a tunable focusing parameter. When γ = 0, FL simplifies to CE, but as γ increases, the effect of the modulating factor is enhanced, putting more emphasis on misclassified examples. When γ = 1, FL becomes exactly the first term of FAL.\n\nReverse cross entropy (RCE) loss  [15]  is proposed to be combined with CE to create the symmetric cross entropy (SCE) loss, which has been demonstrated to be robust against label noise. The RCE loss is defined as:\n\nDue to the label q i being one-hot encoded, there might be an issue with values of zero inside the logarithm. To address this problem, the authors set log(0) to a negative value A, e.g., A = -4. For notational convenience, we define A * :\n\nand rewrite ℓ rce = -K i=1 p i A * . In the second term of Eq. (  19 ), log(p i ) is a small value that changes with p i . If we treat it as the constant A * , this term becomes equivalent to RCE.\n\nThus, we have elucidated the relationship between FAL and both FL and RCE, and found that FAL can be regarded as the combination of learning components associated with these two loss functions. FAL incorporates the advantages of FL and RCE and does not contain manually set hyperparameters, which can achieve better performance on the SFDA-VER task.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we experimentally validate the proposed method and compare it with several contemporary state-of-theart techniques. We begin by outlining the datasets, the methods used for comparison, and the implementation details. Next, we present the results of visual emotion recognition, as well as the experimental results for object recognition. Finally, we provide a comprehensive analysis and discussion of our method.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Datasets",
      "text": "To explore the effectiveness of SFDA-VER methods, we adopt six widely used visual emotion recognition datasets, as shown in Table  III . These datasets' emotion categories are based on two psychological models: the sentiment model and the Mikels model  [52] . The sentiment model classifies emotions into two categories: positive and negative. The Mikels model categorizes emotions into eight groups: amusement, awe, contentment, excitement, anger, disgust, fear, and sadness. In constructing the evaluation benchmarks, we removed a small number of problematic images to more accurately evaluate the performance of the methods, including duplicate and damaged images. Below is a summary of these datasets:\n\n• 2-class Sentiment datasets: (1) Twitter I  [10] , abbreviated as T1, consists of 1,251 images. (2) Twitter II  [53] , abbreviated as T2, consists of 545 images. (3) Instagram  [11] , abbreviated as In, consists of 42,848 images. (  4 ) Flickr  [11] , abbreviated as Fl, consists of 60,729 images. All images in these datasets are collected from social websites using keyword searches aligned with emotion categories. In these four datasets, any dataset can be chosen as the source domain and another as the target domain, resulting in a total of 12 domain adaptation tasks.\n\n• 8-class Emotion datasets: (1) FI  [13]  is constructed from Flickr and Instagram and contains 21,829 images. (  2 ) EmoSet  [12]  is the largest visual emotion recognition dataset available to date, comprising a total of 118,102 images. We alternate the two datasets as source and target domains, thus forming two domain adaptation tasks. Although the number of sub-tasks is less than the Sentiment datasets, this benchmark is more challenging due to the inclusion of more emotion categories. This can be seen from the difference in accuracy observed in the experimental results. To further verify the generality of the proposed method, we perform experiments on the Office-Home dataset  [54]  to assess our FAL. Office-Home is a widely used and highly challenging Twitter I (T1)  [10]  1,251 Sentiment (2) positive, negative Twitter II (T2)  [53]  545 Instagram (In)  [11]  42,848 Flickr (Fl)  [11]  60,729 FI  [13]  21,829\n\namusement, awe, contentment, excitement, anger, disgust, fear, sadness EmoSet  [12]  118,102 object recognition dataset within the SFDA field. It comprises four image domains: Artistic images (Ar), Clipart (Cl), Product images (Pr), and Real-world images (Rw), encompassing a total of 12 domain adaptation tasks. This dataset includes 65 categories of everyday objects, amounting to 15,500 images in total.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Comparison Methods",
      "text": "We select and implement several representative methods for comparison in our experiments. These methods consist of the most advanced SFDA techniques and a range of up-to-date loss functions. The following is a summary of these comparison methods and the academic conferences or journals where they were published:\n\n• SFDA methods: (1) SHOT (ICML 2020)  [35] , which learns domain-specific features with the information maximization loss. (  2 ) SHOT++ (TPAMI 2021)  [36] , which upgrades SHOT through a labeling transfer strategy. (3) G-SFDA (ICCV 2021)  [37] , which groups target features that are semantically similar through local structure clustering. (  4 ) NRC (NeurIPS 2021)  [38] , which encourages semantic consistency among samples with high neighborhood affinity through neighborhood reciprocity clustering. (  5 ) AaD (NeurIPS 2022)  [39] , which utilizes a two-term objective function that encourages local neighbor predictions to be consistent while pushing predictions for dissimilar features as far apart as possible. (  6 ) DaC (NeurIPS 2022)  [41] , which divides the target data into two subsets: source-like and targetspecific, and then achieves class-wise domain adaptation through adaptive contrastive learning.  (7)  CoWA-JMDS (ICML 2022)  [40] , which distinguishes the importance of samples in the target domain based on the joint modeldata structure (JMDS) score. (  8 ) C-SFDA (CVPR 2023)  [42] , which prevents noise propagation in pseudo-labels through curriculum learning.\n\n• Loss functions: (1) Focal loss (ICCV 2017)  [14] , which adds a modulating factor to the standard CE loss to address the class imbalance problem. (  2 ) GCE (NeurIPS 2018)  [45] , which uses the negative Box-Cox transformation as a noise-robust loss function and can be seen as a generalization of mean absolute error (MAE) and categorical cross entropy (CCE). (  3 ) NLNL (ICCV 2019)  [55] , which proposes negative learning to prevent deep models from overfitting to noisy data. (  4 ) SCE (ICCV 2019)  [15] , which combines RCE with CE to form a robust symmetric loss. (  5 ) APL (ICML 2020)  [47] , which normalizes existing loss functions to make them robust to noise and combines active and passive losses to form more powerful losses. (  6 ) PolyLoss (ICLR 2022)  [56] , which represents loss functions as a linear combination of polynomial functions and can adjust the importance of different polynomial bases. (  7 ) ANL (NeurIPS 2023)  [48] , which replaces the MAE in APL with normalized negative loss functions. (  8 ) AUL (TPAMI 2023)  [49] , which introduces the asymmetry in loss functions to make them more robust to noise.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Implementation Details",
      "text": "To ensure a fair comparison with related work, we utilize a pre-trained ResNet-50  [57]  model on ImageNet as the backbone network. Following the literature  [35] -  [39] , we replace the original fully connected (FC) layer with a bottleneck layer and a new FC classifier layer. The bottleneck layer comprises a fully connected layer followed by a batch normalization (BN) layer. During network training, we employ the stochastic gradient descent (SGD) optimizer with a momentum of 0.9. The initial learning rate is set to 1e-3 for the backbone network and 1e-2 for both the bottleneck and FC layers, using a batch size of 64. All images are resized to 256 × 256 and randomly cropped to 224×224 for network input. In the testing phase, the center crop is employed to obtain 224 × 224 inputs.\n\nWhen training the source model on the Sentiment, Emotion, and Office-Home datasets, the maximum number of training epochs is empirically set to 10, 10, and 50, respectively. In the optimization of the target model, the maximum epochs are set to 5, 5, and 15, respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Results Of Visual Emotion Recognition",
      "text": "Table  IV  shows the comparison results on the 2-class sentiment datasets. We report the classification accuracies for 12 sub-tasks and the average performance among them. From the table, we can see that: (1) Existing SFDA methods do not perform well on this dataset, with most methods even performing worse than using the source model alone. For instance, AaD  [39] 's performance drops by 7.4% compared to the source model. This is mainly due to the significant distribution differences between visual emotion data and traditional image classification data, which causes these advanced SFDA methods to fail in the SFDA-VER task. (2) Compared to SFDA methods, the loss functions show superior performance, improving the source model's accuracy to varying degrees. As most of these loss functions have been proven to be noise-tolerant, this indicates a certain similarity between SFDA-VER and the task of learning with noisy labels. (3) Our FAL achieves significant improvements in classification accuracy, exhibiting a 3.5% increase over the source model, and performs best or second-best in 7 out of 12 sub-tasks. This demonstrates the effectiveness of FAL in SFDA-VER.\n\nTable  V  presents the experimental results on the 8-class emotion datasets. We provide the accuracy for each of the 8 emotion categories. To save space in the paper, we combine the domain adaptation results for EmoSet→FI and FI→EmoSet, which are shown on the left and right sides of the \"|\" symbol, respectively. From the experimental results, it can be observed that: (1) The two datasets are more challenging than the 2-class sentiment datasets, as evidenced by the significant decrease in accuracy. This is mainly because EmoSet and FI have more categories and more severe fuzzy problems, which increases the difficulty of SFDA-VER.  (2)  The overall performance of SFDA methods is less stable compared to loss functions. Notably, SHOT outperforms other methods in this comparison. However, some of the most advanced methods, such as C-SFDA  [42]  and AUL  [49] , do not perform well. This suggests that more complex methods may not consistently deliver optimal results, advocating for the potential superiority of simpler, more intuitive approaches. (3) Our FAL obtains substantial performance enhancements of 3% and 6.8% across the two sub-tasks, surpassing all other methods. This indicates that even on more complex VER datasets, FAL can still",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Results Of Object Recognition",
      "text": "In addition to VER datasets, we also evaluate the performance of FAL and other loss functions on the highly challenging Office-Home dataset. Similar to the 2-class sentiment datasets, we report the results on 12 sub-tasks. Considering that many SFDA methods combine some auxiliary techniques to achieve better performance, we combine FAL with the labeling transfer strategy proposed in SHOT++  [36]  to obtain an enhanced version, i.e., FAL++. From Table  VI , we can see that: (1) The loss functions perform much worse than advanced SFDA methods on this dataset, which contrasts sharply with its performance on VER datasets. This further highlights the significant differences between SFDA-VER and the conventional SFDA task. (2) Our FAL significantly outperforms all other loss functions, achieving an accuracy improvement of 11.8% compared to the source model. Additionally, FAL++ achieves performance that is comparable to state-of-the-art SFDA methods. This indicates that, although FAL is designed for VER, it is also applicable to standard SFDA tasks, demonstrating its stronger generalization ability compared to existing SFDA methods and loss functions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "F. Analysis And Discussions",
      "text": "Ablation study. We investigate the advantages of each component of the proposed loss function in Table  VII . ℓ f al-1 and ℓ f al-2 denote the first and second terms of FAL in Eq.  (19) , and weights refer to the w in Eq.  (8) . The results clearly show that ℓ f al-1 has a significant effect, and the performance is further improved by adding ℓ f al-2 . After incorporating weights, there is a slight improvement in accuracy. It is worth noting that using ℓ f al-2 alone can lead to performance degradation as ℓ f al-2 primarily provides robustness to noise but lacks high-level semantic guidance for emotion categories. This is consistent with the observations in works such as SHOT  [35] , AaD  [39] , SCE  [15] , and AUL  [49] , which typically require combining two loss terms to achieve better learning results. Feature visualization. We further compare the visualization results of the feature representations learned by the source model and our FAL. We first extract the high-dimensional features output by the second-to-last layer, i.e., the bottleneck layer, and then utilize t-SNE  [63]  to project these features into 2D embeddings. Considering that the dataset size of EmoSet and FI is too large, displaying all of them would be too messy. To visualize more clearly, we uniformly sample 1/10 and 1/50 samples on the EmoSet and FI datasets, respectively. The visualization results of the feature representations are shown in Fig.  2 . We can see that the feature representations extracted by the source model on the target domain are not discriminative enough, with different categories mixed together. In contrast, the feature representations learned by our FAL have clearer boundaries and more concentrated clusters, indicating a significant improvement in feature quality compared to the source model.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this work, we delve into the SFDA-VER task from a fuzzy perspective and identify two prominent issues: fuzzy emotion labels and fuzzy pseudo-labels. To tackle these issues, we propose a novel objective function called fuzzy-aware loss (FAL). FAL integrates the standard cross entropy loss with a fuzzy-aware loss term, aiming to calibrate the loss of nonpredicted classes to accommodate fuzzy labels. Furthermore, we provide theoretical proof of FAL's robustness to noise and discuss the relationship between FAL and two typical loss functions. Extensive experimental results on three benchmark datasets show that FAL achieves state-of-the-art accuracy. We hope that this work can lay the foundation for the future development of SFDA-VER and promote the practical application of VER technology.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: We utilized the source model",
      "page": 5
    },
    {
      "caption": "Figure 1: The confusion matrix (%) for directly testing the source model",
      "page": 6
    },
    {
      "caption": "Figure 2: We can see that the feature representations extracted",
      "page": 10
    },
    {
      "caption": "Figure 2: Visualization of feature representations learned by the source model",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "recognition (SFDA-VER)\nis\na highly\nchallenging\ntask that\nre-",
          "TABLE I": "THE PERCENTAGE (%) OF CERTAIN AND UNCERTAIN SAMPLES IN"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "MANUALLY ANNOTATED VISUAL EMOTION DATASET. A CERTAIN SAMPLE"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "quires adapting VER models to the target domain without relying",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "MEANS THAT SEVERAL ANNOTATORS TAG THE SAME EMOTIONAL LABEL"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "on source data, which is of great\nsignificance\nfor data privacy",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "TO THIS IMAGE, WHILE AN UNCERTAIN SAMPLE INDICATES THE"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "protection. However, due to the unignorable disparities between",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "EXISTENCE OF DISAGREEMENT AMONG THE ANNOTATORS."
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "visual\nemotion data\nand traditional\nimage\nclassification data,",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "existing SFDA methods perform poorly on this task. In this paper,",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "Positive\nNegative"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "we\ninvestigate\nthe\nSFDA-VER task\nfrom a\nfuzzy\nperspective",
          "TABLE I": "Dataset"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "Certain\nUncertain\nCertain\nUncertain"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "and\nidentify\ntwo\nkey\nissues:\nfuzzy\nemotion\nlabels\nand\nfuzzy",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "pseudo-labels. These issues arise from the inherent uncertainty of",
          "TABLE I": "Twitter\nI\n(T1)\n[10]\n75.6\n24.4\n60.2\n39.8"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "Instagram (In)\n[11]\n49.5\n50.5\n43.1\n56.9"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "emotion annotations and the potential mispredictions in pseudo-",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "Flickr\n(Fl)\n[11]\n44.8\n55.2\n31.6\n68.4"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "labels.\nTo\naddress\nthese\nissues, we\npropose\na\nnovel\nfuzzy-",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "aware\nloss\n(FAL)\nto\nenable\nthe VER model\nto\nbetter\nlearn",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "and adapt\nto new domains under fuzzy labels. Specifically, FAL",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "modifies the standard cross entropy loss and focuses on adjusting",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "the\nlosses\nof non-predicted categories, which prevents\na\nlarge",
          "TABLE I": "is usually collected from various\nsources and environments,"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "number of uncertain or incorrect predictions from overwhelming",
          "TABLE I": "leading\nto\nsubstantial\ndifferences\nin\ndata\ndistribution.\nFor"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "the VER model during\nadaptation.\nIn addition, we provide\na",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "example,\nusers\nfrom different\nsocial\nplatforms may\ncome"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "theoretical analysis of FAL and prove its robustness in handling",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "from diverse cultural backgrounds and use different shooting"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "the noise in generated pseudo-labels. Extensive experiments on",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "conditions,\nresulting\nin\ndistinct\nemotional\nexpression\nstyles"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "26 domain adaptation sub-tasks across three benchmark datasets",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "demonstrate the effectiveness of our method.",
          "TABLE I": "and features. This distribution difference makes it difficult for"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "models trained on one dataset to perform well on new datasets."
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "Index Terms—Source-free domain adaptation, visual emotion",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "Furthermore, visual emotional data often raises privacy con-"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "recognition,\nfuzzy-aware learning,\nloss function.",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "cerns,\nand the difficulty and cost\nassociated with acquiring"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "large-scale\nannotated\ndata\nfurther\nlimit\nthe\ngeneralization"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "I.\nINTRODUCTION",
          "TABLE I": "ability of VER models [8]."
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "emerged as\na",
          "TABLE I": "To address\nthe aforementioned issues,\nsource-free domain"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "research direction in computer vision and affec-\nV ISUAL emotion recognition (VER) has",
          "TABLE I": "adaptation (SFDA) presents a promising solution. SFDA seeks"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "tive computing [1]–[3]. With the explosive growth of\nsocial",
          "TABLE I": "to facilitate\na\ntrained model\nto learn and adapt\nto the data"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "media\nand\ndigital\ncontent, VER has\nshown\nsignificant\npo-",
          "TABLE I": "distribution\nof\nan\nunlabeled\ntarget\ndomain without\nrelying"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "tential\nfor\napplications\nsuch as human-robot\ninteraction [4]",
          "TABLE I": "on source domain data [9]. This approach allows\nthe model"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "[5],\nsocial media analytics\n[6], and enhancing entertainment",
          "TABLE I": "to\nachieve\nstrong\nperformance\nin\nthe\ntarget\ndomain while"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "experiences [7]. By interpreting the emotional cues embedded",
          "TABLE I": "alleviating\nprivacy\nconcerns\nrelated\nto\nthe\nuse\nof\nsource"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "in visual data like images and videos, VER enables machines",
          "TABLE I": "domain\ndata. By\nleveraging SFDA, models\ncan\neffectively"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "to more effectively comprehend and react\nto human emotions,",
          "TABLE I": "perform emotion\nrecognition\nin\nthe\ntarget\ndomain,\nthereby"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "ultimately enhancing user experiences. However, despite sig-",
          "TABLE I": "enhancing\nthe\npracticality\nand\nreliability\nof VER systems."
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "nificant progress in recent years, VER still encounters several",
          "TABLE I": "However, existing SFDA methods are primarily designed for"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "issues\nin real-world scenarios. One of\nthe main issues\nis\nthe",
          "TABLE I": "general\nimage\nclassification tasks\nand do not perform well"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "diversity\nof\nvisual\ndata\ndistribution. Visual\nemotional\ndata",
          "TABLE I": "when\ndirectly\napplied\nto VER. This\nis mainly\ndue\nto\nthe"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "specific complexity of VER. Firstly,\nthe diversity and subtlety"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "The research work was partly conducted in the JC STEM Lab of Machine",
          "TABLE I": "of emotional expressions make the emotion recognition task"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "Learning\nand Computer Vision\nfunded\nby The Hong Kong\nJockey Club",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "more\ncomplex\nthan\ngeneral\nclassification\ntasks.\nSecondly,"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "Charities Trust. This work was\nsupported in part by the National Natural",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "Science Foundation of China (No. 62106236).",
          "TABLE I": "the subjectivity of emotional data increases\nthe difficulty of"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "Ying\nZheng, Yi Wang\nand\nLap-Pui\nChau\nare with\nthe Department",
          "TABLE I": "model\nadaptation. Existing SFDA methods\nusually\ndo\nnot"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "of\nElectrical\nand\nElectronic\nEngineering,\nThe Hong Kong\nPolytechnic",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "consider\nthese\nspecific\nemotional\ncharacteristics,\nleading\nto"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "University, Hong Kong, China.\nE-mail:\n{ying1.zheng,\nyi-eie.wang,\nlap-",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "pui.chau}@polyu.edu.hk.",
          "TABLE I": "poor performance in the SFDA-VER task."
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "Yiyi Zhang is with the Department of Computer Science\nand Engineer-",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "To\ntackle\nthe\nchallenges\nof\nSFDA-VER, we\nfirst\ndelve"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "ing, The Chinese University\nof Hong Kong, Hong Kong, China. E-mail:",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "",
          "TABLE I": "into this\ntask from a fuzzy perspective. Specifically,\nthrough"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "yyzhang24@cse.cuhk.edu.hk.",
          "TABLE I": ""
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "† denotes equal contribution; ∗ denotes corresponding author.",
          "TABLE I": "the analysis of visual emotion data, we identify two aspects"
        },
        {
          "Abstract—Source-free\ndomain\nadaptation\nin\nvisual\nemotion": "0000–0000/00$00.00 © 2025 IEEE. Personal use of",
          "TABLE I": "this material\nis permitted."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "TABLE II\ntask, while loss functions perform better. However,\nthese loss"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "THE SOURCE-PRETRAINED MODEL’S CLASSIFICATION ACCURACY (%)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "functions\nstruggle on general SFDA datasets. This\nindicates"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "FOR EACH CATEGORY IN THE TARGET DOMAIN DATASET. E AND F"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "a disparity between SFDA-VER and traditional SFDA tasks,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "REPRESENT EMOSET [12] AND FI [13], RESPECTIVELY, AND “→”"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "INDICATES THAT THE MODEL IS TRAINED ON THE SOURCE DOMAIN (LEFT)\nwhich leads\nto varying performance for SFDA methods and"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "AND TESTED ON THE TARGET DOMAIN (RIGHT). TO BETTER FORMAT THE"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "loss functions.\nIn contrast, our method not only achieves the"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "TABLE, WE SHORTEN THE NAMES OF EACH EMOTIONAL CATEGORY."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "best performance on the SFDA-VER task but\nalso demon-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "strates comparable results\nto state-of-the-art methods on the"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "Task\nAmu.\nAng.\nAwe\nCon.\nDis.\nExc.\nFea.\nSad."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "challenging Office-Home dataset."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "E→F\n67.9\n40.0\n66.3\n44.2\n50.8\n48.8\n61.7\n53.9"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "In summary, our contributions are as follows:"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n2": "F→E\n39.0\n27.2\n61.1\n69.6\n76.4\n65.8\n26.2\n36.1"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "challenging Office-Home dataset.",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "In summary, our contributions are as follows:",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "• We explore the SFDA-VER problem from a fuzzy per-",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "spective, which",
          "to state-of-the-art methods on the": "significant"
        },
        {
          "strates comparable results": "privacy protection.",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "• We propose",
          "to state-of-the-art methods on the": "loss\nfunction,"
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "lems in SFDA-VER. Additionally, we provide a theoret-",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "ical analysis of",
          "to state-of-the-art methods on the": "the robustness of FAL."
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "conduct",
          "to state-of-the-art methods on the": "experiments"
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "superior performance of FAL in the SFDA-VER task, and",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "we also validate the effectiveness of FAL on the general",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "SFDA dataset.",
          "to state-of-the-art methods on the": ""
        },
        {
          "strates comparable results": "",
          "to state-of-the-art methods on the": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "image features, followed by emotion classification using fully\nbeen adopted and extended by lots of subsequent works [36]–"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "et\nal.\nconnected classifiers. For instance, You et al. [10] introduce a\n[43]. Building\nupon SHOT, Liang\n[36]\npropose\nthe"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "progressive CNN (PCNN)\napproach that\nincrementally fine-\nSHOT++ method, which\nfurther\nimprove SHOT’s\naccuracy"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "tunes\na CNN model with\na\ndataset\ncontaining\ngenerated\nthrough a label\ntransfer strategy and semi-supervised learning."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "pseudo-labels. While these methods connect the holistic image\nYang et al. [39] conceptualize SFDA as an unsupervised clus-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "features to emotions directly,\nthey often overlook local\nimage\ntering problem. They accomplish effective clustering of\nlocal"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "et\nal.\ncues. To\naddress\nthis\nlimitation, You\n[25]\nand Yang\nneighborhood features within the feature space by optimizing"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "et al.\n[18] propose\nidentifying significant\nemotional\nregions\nan objective focused on prediction consistency. Zhang et al."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "and constructing classification models based on these regions.\n[41] introduce a paradigm termed Divide and Contrast (DaC),"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "To further\nintegrate global\nand local\nemotional\ninformation,\nwhich divides\ntarget data into source-like and target-specific"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "Yang et al. [26] develop a method using a graph convolutional\nsamples. Within an adaptive contrastive learning framework,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "network (GCN) to model the relationships between scenes and\nDaC fulfills distinct global and local\nlearning objectives\nfor"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "et\nal.\nobjects,\nthereby inferring the image’s emotion through these\neach\nsample\ngroup. Xia\n[43]\npropose\na\ndiscrimina-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "associations. Xu et al.\n[27] design a multi-level dependent\ntive pattern calibration (DPC) method, which leverages\nthe"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "attention network (MDAN), which connects\nimage\nfeatures\ndiscriminative\npatterns\nof\ntarget\nimages\nto\nenhance\nfeature"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "with emotional semantics at multiple levels, enhancing emo-\nrepresentation for SFDA."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "tion recognition outcomes. Recently, Zhao et al. [28] propose\nMost of the aforementioned SFDA works rely on generated"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "a novel emotion recognition evaluation method based on the\npseudo-labels\nand new loss\nfunctions\nto guide\nthe\nlearning"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "Mikel emotion wheel\nfrom psychology, which considers\nthe\nof specific features or\nlocal structures in the target data. The"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "distances between different emotions on the wheel\nto capture\nproposed FAL method also aligns with this learning paradigm."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "emotional nuances more accurately.\nHowever, despite\nthe\nextensive body of work on SFDA,\nits"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "applicability\nto\nthe field\nof VER remains\nunexplored. Our\nFurthermore, several approaches incorporate supplementary"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "experimental\nresults\nshow that most\nof\nthe\nexisting SFDA\ninformation to enhance emotion recognition in images. Borth"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "et al.\nmethods do not perform well when transferred to the VER\n[29] develop a visual\nconcept detector\nlibrary named"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "task. Therefore, we posit\nthe critical need for methods specif-\nSentiBank, which significantly improves emotion recognition"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "ically tailored to the distinctive attributes of VER.\nperformance by utilizing adjective-noun pairs. Yang et al. [12]"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "demonstrate on the large-scale sentiment\nrecognition dataset"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "EmoSet\nthat models\nintegrating emotion attribute prediction"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "C. Loss Functions for Learning with Noisy Labels"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "can\nachieve\nsuperior\nemotion\nrecognition\nresults. Recently,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "During the process of model adaptation to the target domain,\nwith\nthe\nremarkable\nperformance\nof\nlarge-scale\npre-trained"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "it\nis typically necessary to utilize pseudo-labels generated on\nmodels\nsuch as CLIP [30],\nthere has been increased interest"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "target\nimages. However,\nthese labels inevitably contain many\nin\nleveraging\nlarge\nlanguage models\nto\nprovide\nadditional"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "errors, making\nlearning\nbased\non\npartially\nincorrect\nlabels\nguidance\nfor\nemotion\nrecognition\n[31]\n[32].\nFor\ninstance,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "close\nto\nthe\nproblem of\nlearning with\nnoisy\nlabels\n(LNL).\net\nal.\nDeng\n[31]\nput\nforward\na\nprompt-based\nfine-tuning"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "Within the field of LNL, a critical\nresearch direction involves\nmethod that combines visual and language features to capture"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "designing loss functions that are robust\nto noisy labels.\nricher emotional cues. However,\nthere remains an absence of"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "Previous\nresearch has\ntheoretically proven that\nthe widely\nSFDA research within the emotion recognition domain. Given"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "used\ncross\nentropy\n(CE)\nloss\nfunction\nlacks\nrobustness\nin\nthe\nimportance of data\nsecurity and privacy protection, we"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "the presence of noisy labels\n[44]. Consequently,\nsubsequent\nbelieve that\nthe exploration of SFDA-VER will\nfacilitate the"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "studies\n[15],\n[45]–[49] have focused on developing modified\napplication of VER technology in the future."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "versions\nof\nthe CE loss\nto\nfacilitate more\nrobust\nlearning."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "These methods usually mitigate the impact of noisy labels on"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "B.\nSource-free Domain Adaptation"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "model\ntraining by incorporating additional objective functions"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "The difference between SFDA and standard unsupervised\nor\ncalibrating the\nloss values. For\nexample,\nthe generalized"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "domain adaptation (UDA)\nlies in their data utilization strate-\ncross\nentropy (GCE)\n[45]\nloss\nintegrates\nthe mean absolute"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "gies. UDA methods [33]\n[34]\nleverage both source and target\nerror\n(MAE) with CE,\nbalancing\nthe\ntwo\nterms\nthrough\na"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "data during model\ntraining, whereas SFDA operates without\nhyperparameter. Wang et al.\n[15] propose a symmetric cross"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "access\nto\nsource\ndata,\nrelying\nexclusively\non\na\npre-trained\nentropy (SCE)\nloss\nthat combines CE with a robust\nreverse"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "source model and target data for updates. This characteristic of\ncross entropy (RCE)\nloss. Considering the challenge of\niden-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "SFDA underscores its importance in maintaining data privacy.\ntifying a\nconvex loss\nfunction under\nsymmetric\nconditions,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "Since the inception of SFDA by Liang et al. [35],\nthe field has\nZhou et al. [49] present a new family of loss functions, namely"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "attracted considerable interest within the research community,\nasymmetric\nloss\nfunctions, which\npreserve\nsome\nbeneficial"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "leading to notable advancements.\nproperties\nof\nconvex\nloss\nfunctions\nto\nfacilitate\nsubsequent"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "network optimization.\nIn the pioneering SHOT [35] framework,\nthe authors freeze"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "the\nclassifier module of\nthe\nsource model\nand then update\nExisting loss functions for LNL have improved the robust-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "the feature extractor through an information maximization loss\nness\nof models\nin\nnoisy\nenvironments. However,\nsince\nthe"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "and self-supervised pseudo-labeling. SHOT exhibits excellent\ntarget\ndata\naddressed\nby\nthese methods\ndiffer\nsignificantly"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n3": "performance in the SFDA task for object\nrecognition and has\nfrom VER data\nin terms of\nemotional\nsemantics\nand noise"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "distribution,\nthey\ncannot\nbe\ndirectly\napplied\nto\nsolve\nthe\nin place of p(i|xt) and q(i|xt) in all\nthe following equations."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "problem of SFDA-VER. This discrepancy has been verified\nThe cross entropy (CE)\nloss is formulated as:"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "in the\nexperiments of Section IV. Hence, how to design a"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "robust loss function suitable for SFDA-VER remains an urgent"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "K(cid:88) i\n(2)\nchallenge for\nfurther\ninvestigation.\nℓce = −\nqi log(pi),"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "=1"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "III. METHOD"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "Recall\nthat when i = ˆy, qi = 1; otherwise qi = 0. Therefore,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "In this section, we begin by introducing some preliminary"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "Eq.\n(2) can be rewritten as:"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "knowledge related to SFDA and conduct a thorough analysis"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "of\nthe drawbacks of CE in SFDA-VER. Then, we provide a"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "(3)\nℓce = − log(pˆy) −\nqi log(pi),"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "(cid:88) i\ndetailed description of\nthe proposed FAL method, accompa-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "̸=ˆy"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "nied by theoretical analysis and discussion."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "where the second term equals to 0. From Eq. (3),\nit\nis evident"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "A. Preliminaries"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "that\nthe CE loss only focuses on the difference between the"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "In this paper, we focus on the classical closed-set setting of\nmodel’s predictions and the pseudo-labels. The objective is to"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "SFDA, where the source and target domains\nshare the same\nmake the model’s predictions gradually approach the pseudo-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "K classes. In the SFDA task, we first define two datasets:\nthe\nlabels. This method is effective on datasets with clean labels,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "source domain dataset Ds containing Ns samples, and the tar-\nas\nit helps guide\nthe model\nin optimizing towards\nthe\ntrue"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "get domain dataset Dt containing Nt samples. Specifically, the\nlabels. However,\nin the SFDA-VER task, due\nto the\nfuzzy"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "source domain dataset\nis represented as Ds = {(xs\nproblems, where the generated pseudo-labels are not entirely\ni , ys\ni )}Ns\ni=1,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "where xs\ndenotes\nthe\nsamples\nfrom the\nsource domain and\naccurate, the CE loss may lead the model to overfit on incorrect\ni"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "ys\nindicates the corresponding class labels. The target domain\npseudo-labels. As a result,\nthe CE loss is not an ideal option"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "i"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n4": "dataset\nis denoted as Dt = {xt\nin this context.\ni}Nt\ni=1, containing only samples"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "tribution of each event’s probability to the overall uncertainty.\nand set g′(p) = 0 to find critical points:"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "then we have:\nInspired by this, we set λi = pi,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "log(p) = −1\n⇒\np = e−1.\n(10)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "ℓf al = − log(pˆy) −\npi log(pi)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "(cid:88) i̸\nThen, we evaluate g(p) at\nthe critical point p = e−1:"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "=ˆy"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "g (cid:0)e−1(cid:1) = −e−1 log (cid:0)e−1(cid:1) = e−1.\n(11)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "K(cid:88) i\nK(cid:88) i\n= −\nqi log(pi) −\npi log(pi) + pˆy log(pˆy)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "Thus,\nthe maximum and minimum values of g(p) on (0, 1)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "=1\n=1"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "are e−1 and 0,\nrespectively."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "As ℓf t equals to (cid:80)\ni̸=ˆy g(pi), we have:"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "K(cid:88) i\nK(cid:88) i\nK(cid:88) i\n= −\nqi log(pi) −\npi log(pi) +\npiqi log(pi)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "=1\n=1\n=1\n(12)\n0 < ℓf t ≤ (K − 1)e−1."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "This demonstrates\nthe boundedness of\nit\nℓf t, which makes"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "K(cid:88) i\n= −\n(5)\n[(1 − pi)qi + pi] log(pi)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "more robust compared to unbounded losses such as CE."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "=1"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "ˆ\nRobustness\nanalysis. We\nconsider\nthe pseudo-label\nof"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "In the above equation,\nenables\nthe new\nintroducing λi = pi"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "a sample x as a noisy label, with the true label\nrepresented"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "loss ℓf al better to handle data uncertainty with pseudo-labels."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "by y. Given any classifier f and loss function ℓf t, we define"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "This allows the model\nto take into account\nthe fuzzy problems"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "as\nthe\nrisk of\nthe\nclassifier\nf with clean\nR(f ) = Ex,yℓf t"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "between different classes during decision-making. Therefore,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "labels\nas\nthe\nrisk under noisy label\nand Rη(f ) = Ex,ˆyℓf t"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "we refer\nas\nfuzzy-aware loss\n(FAL) and the domain\nto ℓf al"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "˜"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "f\nf ∗\nrate\nη. Let\nand\nbe\nthe\nglobal minimizers\nof Rη(f )"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "adaptation learning based on ℓf al as fuzzy-aware learning."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "and R(f ),\nrespectively. Following [15]\n[44], a loss\nfunction"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "Additionally, we introduce a weighting coefficient\nto adjust"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "is deemed noise-tolerant\nif\nit ensures\nthat\nthe probability of"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "the\nloss\nfor\neach\ncategory\nduring\nthe\nadaptation\nprocess."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "misclassification at\nthe optimal decision boundary is identical"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "As\na\ncommonly\nused method\nto\nalleviate\nclass\nimbalance"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "for clean and noisy data.\nIn the following, we will\nintroduce"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "[50], weighted loss allows for fine-tuning the impact of each"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "is robust\nan assumption for the SFDA-VER task and prove ℓf t"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "category on the overall\nloss,\nenhancing the model’s\nability"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "under\nlabel noises."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "to\nadapt\nto\nvarying\ncategory\ndistributions. Specifically, we"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "Assumption 1. Let ηyi be the probability that a sample x"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "maintain a memory bank B to store the predictions from the"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "with a true label y is predicted as class i by the source model."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "last\ntesting round for all samples. Then,\nthe weight\nfor class"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "The\nin SFDA-VER\ntarget domain dataset Dt = {xt\ni, ˆyi}Nt\ni=1"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "i\nis defined as,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "is clean-labels-dominant,\ni.e.,\nit\nsatisfies\nthat ∀i\n̸= y, ηyi <"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "M n\n,\n(6)\nwi =\n1 − ηy with (cid:80)\ni̸=y ηyi = ηy."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "i"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "In SFDA-VER, the source model exhibits significant perfor-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "in which"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "mance differences across different categories,\nresulting in un-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "even noise distribution among various categories in the target"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "1 K\nK(cid:88) i\nM =\n(7)\nni,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "domain dataset Dt = {xt\ni, ˆyi}Nt\ni=1. This can be approximately"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "=1"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "regarded as asymmetric or class-dependent noise. Here, 1 − ηy"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "where ni represents the number of samples within the memory"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "represents\nthe\nprobability\nof\nthe\nlabel\nbeing\ncorrect, while"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "bank B that belong to the i-th category. Thus, we obtain a"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "indicates\nthat\nfor\ncategory y,\nthe probability\nηyi < 1 − ηy"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "weight-balanced variant of FAL:"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "of correctly predicting the label\nis greater than the probability"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "of being incorrectly predicted as another class. As discussed in"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "K(cid:88) i\n(8)\n[49], a method capable of learning the correct classifier when\nwi[(1 − pi)qi + pi] log(pi)\nℓf al = −"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "=1\nclean labels are not dominant will fail in scenarios where clean"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "labels are dominant. This occurs because the learned classifier"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "will\nlikely predict samples as belonging to the less dominant\nD. Theoretical Analysis"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "class\ninstead of\nthe\ntrue dominant\nclass. Therefore,\nto train"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "As illustrated in the first equality of Eq.\n(5),\nthe proposed"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "a\nreliable\nclassifier,\nthe\ntraining dataset needs\nto satisfy the"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "FAL consists of\ntwo parts:\nthe traditional CE loss and a new"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "assumption of clean-labels-domination."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "fuzzy-aware\nloss\nhigh-\nterm ℓf t = − (cid:80)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "i̸=ˆy pi log(pi). As"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "An empirical study of the clean-labels-domination assump-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "lighted in references [44]\n[45],\nthe CE loss is unbounded and"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "tion\nis\nillustrated\nin Fig.\n1. We\nutilized\nthe\nsource model"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "lacks robustness against noisy data. Therefore,\nthe robustness"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "(ResNet-50) trained on EmoSet to directly test its performance"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "of FAL primarily stems from the fuzzy-aware term. Next, we"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "on FI,\nthereby obtaining the confusion matrix for eight emo-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "will\ntheoretically explore the boundedness and robustness of"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "tion categories. From this figure,\nit can be observed that\nthe"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "the fuzzy-aware term."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "confusion matrix is diagonal-dominant, which means\nthat\nit"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "Boundedness analysis. Before analyzing the boundedness"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "satisfies\ncondition\nIt\nshould\nbe\nnoted\nthat\nηyi < 1 − ηy."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "of\nconsider\nthe\nfunction g(p) = −p log(p)\nfor\nℓf t, we first"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "Assumption 1 is\nrelatively strict and may not hold for each"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "p ∈ (0, 1). When p → 0+, although log(p) → −∞, but with"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "category in every task due to variations of source models and"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "the product of p, g(p) → 0. When p → 1−, log(p) → 0, hence"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "target data domains. However, based on practical experience"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "g(p) → 0. Next, we compute the derivative of g(p):"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "from SFDA-VER,\nthis assumption is generally valid in most"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n5": "g′(p) = −(log(p) + 1)\n(9)\ncases. Therefore, noise-tolerant loss functions remain effective."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "0",
          "6": "the loss"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "1",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "2",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "3",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "True labels",
          "6": "(17)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "4",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "5",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "6",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": "(18)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "7",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": "risk"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "Fig.\n1.",
          "6": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "(ResNet-50) on the target data domain (EmoSet→FI).",
          "6": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "and rewrite ℓrce = − (cid:80)K",
          "7": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "In the second term of Eq.\ni=1 piA∗.",
          "7": "TABLE III"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "7": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "(19),\nlog(pi) is a small value that changes with pi. If we treat",
          "7": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "it as the constant A∗,\nthis term becomes equivalent\nto RCE.",
          "7": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "7": "Model"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "7": "Categories"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "Thus, we have elucidated the relationship between FAL and",
          "7": "(#category)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "both FL and RCE, and found that FAL can be regarded as",
          "7": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "7": "Sentiment"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "7": "positive, negative"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "the combination of learning components associated with these",
          "7": "(2)"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "7": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "two loss\nfunctions. FAL incorporates\nthe\nadvantages of FL",
          "7": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "7": "Mikels\namusement, awe, contentment, excitement,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "and RCE and does not contain manually set hyperparameters,",
          "7": "(8)\nanger, disgust,\nfear, sadness"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "which can achieve better performance on the SFDA-VER task.",
          "7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "FI\n[13]\n21,829\nMikels\namusement, awe, contentment, excitement,"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "EmoSet\n[12]\n118,102\n(8)\nanger, disgust,\nfear, sadness"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "object recognition dataset within the SFDA field. It comprises"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "four image domains: Artistic images (Ar), Clipart (Cl), Product"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "images\n(Pr),\nand Real-world images\n(Rw),\nencompassing a"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "total of 12 domain adaptation tasks. This dataset\nincludes 65"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "categories of everyday objects, amounting to 15,500 images"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "in total."
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "B. Comparison Methods"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "We select and implement several representative methods for"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "comparison in our experiments. These methods consist of\nthe"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "most advanced SFDA techniques and a range of up-to-date loss"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "functions. The following is a summary of\nthese comparison"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "methods and the academic conferences or journals where they"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "were published:"
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": ""
        },
        {
          "Flickr\n(Fl)\n[11]\n60,729": "•\n[35], which\nSFDA methods:\n(1)\nSHOT (ICML 2020)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "functions. The following is a summary of\nthese comparison"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "methods and the academic conferences or journals where they"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "were published:"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "•\n[35], which\nSFDA methods:\n(1)\nSHOT (ICML 2020)"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "learns domain-specific features with the information max-"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "imization loss.\n(2) SHOT++ (TPAMI 2021)\n[36], which"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "upgrades SHOT through a labeling transfer strategy.\n(3)"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "G-SFDA (ICCV 2021)\n[37], which\ngroups\ntarget\nfea-"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "tures\nthat\nare\nsemantically similar\nthrough local\nstruc-"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "ture\nclustering.\n(4) NRC (NeurIPS 2021)\n[38], which"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "encourages\nsemantic\nconsistency\namong\nsamples with"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "high neighborhood affinity through neighborhood reci-"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "procity clustering.\n(5) AaD (NeurIPS 2022)\n[39], which"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "utilizes\na\ntwo-term objective\nfunction\nthat\nencourages"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "local neighbor predictions\nto be consistent while push-"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "ing\npredictions\nfor\ndissimilar\nfeatures\nas\nfar\napart\nas"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "possible.\n(6) DaC (NeurIPS 2022)\n[41], which divides"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "the target data into two subsets:\nsource-like and target-"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "specific, and then achieves class-wise domain adaptation"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "through adaptive contrastive learning.\n(7) CoWA-JMDS"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "(ICML 2022) [40], which distinguishes the importance of"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "samples\nin the target domain based on the joint model-"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "data structure (JMDS) score.\n(8) C-SFDA (CVPR 2023)"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "[42], which prevents noise propagation in pseudo-labels"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "through curriculum learning."
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "•\n[14], which\nLoss functions:\n(1) Focal\nloss (ICCV 2017)"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "adds\na modulating\nfactor\nto\nthe\nstandard CE loss\nto"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "address the class imbalance problem.\n(2) GCE (NeurIPS"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "2018)\n[45], which uses\nthe negative Box-Cox transfor-"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "mation as a noise-robust\nloss\nfunction and can be seen"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "as\na generalization of mean absolute\nerror\n(MAE)\nand"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "categorical cross entropy (CCE). (3) NLNL (ICCV 2019)"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": ""
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "[55], which proposes negative learning to prevent deep"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "models\nfrom overfitting to noisy data.\n(4) SCE (ICCV"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "2019)\n[15], which\ncombines RCE with CE to\nform a"
        },
        {
          "most advanced SFDA techniques and a range of up-to-date loss": "robust symmetric loss. (5) APL (ICML 2020) [47], which"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "T2→In"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "53.0"
        },
        {
          "TABLE IV": "51.4"
        },
        {
          "TABLE IV": "44.8"
        },
        {
          "TABLE IV": "52.7"
        },
        {
          "TABLE IV": "59.5"
        },
        {
          "TABLE IV": "52.1"
        },
        {
          "TABLE IV": "61.5"
        },
        {
          "TABLE IV": "58.0"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "61.3"
        },
        {
          "TABLE IV": "66.1"
        },
        {
          "TABLE IV": "60.7"
        },
        {
          "TABLE IV": "58.8"
        },
        {
          "TABLE IV": "62.9"
        },
        {
          "TABLE IV": "61.0"
        },
        {
          "TABLE IV": "64.5"
        },
        {
          "TABLE IV": "55.7"
        },
        {
          "TABLE IV": "59.7"
        },
        {
          "TABLE IV": "64.1"
        },
        {
          "TABLE IV": "84.8"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "Target-supervised\n-\n77.1\n84.8\n86.2\n82.1",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "84.8\n86.2\n82.1\n77.1\n86.2\n82.1\n77.1\n84.8\n82.6"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "normalizes existing loss\nfunctions\nto make them robust",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "not\nperform well\non\nthis\ndataset, with most methods\neven"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "to noise and combines active and passive losses to form",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "performing worse\nthan\nusing\nthe\nsource model\nalone. For"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "more powerful\nlosses.\n(6) PolyLoss\n(ICLR 2022)\n[56],",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "instance, AaD [39]’s performance drops by 7.4% compared"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "which represents\nloss\nfunctions as a linear combination",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "to\nthe\nsource model. This\nis mainly\ndue\nto\nthe\nsignificant"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "of polynomial\nfunctions\nand can adjust\nthe\nimportance",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "distribution differences between visual emotion data and tradi-"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "of different polynomial bases.\n(7) ANL (NeurIPS 2023)",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "tional\nimage classification data, which causes these advanced"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "[48], which replaces\nthe MAE in APL with normalized",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "SFDA methods to fail\nin the SFDA-VER task.\n(2) Compared"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "negative\nloss\nfunctions.\n(8) AUL (TPAMI\n2023)\n[49],",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "to SFDA methods,\nthe\nloss\nfunctions\nshow superior perfor-"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "which introduces the asymmetry in loss functions to make",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "mance,\nimproving\nthe\nsource model’s\naccuracy\nto\nvarying"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "them more robust\nto noise.",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "degrees. As most of\nthese\nloss\nfunctions have been proven"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "to be noise-tolerant,\nthis indicates a certain similarity between"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "SFDA-VER and the\ntask of\nlearning with noisy labels.\n(3)"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "C.\nImplementation Details",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "Our FAL achieves\nsignificant\nimprovements\nin classification"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "To ensure a fair comparison with related work, we utilize a",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "accuracy, exhibiting a 3.5% increase over\nthe source model,"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "pre-trained ResNet-50 [57] model on ImageNet as\nthe back-",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "and performs best or\nsecond-best\nin 7 out of 12 sub-tasks."
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "bone network. Following the literature [35]–[39], we replace",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "This demonstrates the effectiveness of FAL in SFDA-VER."
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "the original fully connected (FC) layer with a bottleneck layer",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "Table V presents\nthe\nexperimental\nresults on the 8-class"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "and a new FC classifier layer. The bottleneck layer comprises",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "emotion datasets. We provide the accuracy for each of\nthe 8"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "a\nfully\nconnected\nlayer\nfollowed\nby\na\nbatch\nnormalization",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "emotion categories. To save space in the paper, we combine the"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "(BN) layer. During network training, we employ the stochastic",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "domain adaptation results\nfor EmoSet→FI and FI→EmoSet,"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "gradient descent\n(SGD) optimizer with a momentum of 0.9.",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "which are shown on the left and right sides of the “|” symbol,"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "The initial learning rate is set to 1e-3 for the backbone network",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "respectively. From the experimental results,\nit can be observed"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "and\n1e-2\nfor\nboth\nthe\nbottleneck\nand\nFC layers,\nusing\na",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "that:\n(1) The\ntwo\ndatasets\nare more\nchallenging\nthan\nthe"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "batch size of 64. All\nimages\nare\nresized to 256 × 256 and",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "2-class\nsentiment\ndatasets,\nas\nevidenced\nby\nthe\nsignificant"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "randomly cropped to 224×224 for network input. In the testing",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "decrease\nin\naccuracy. This\nis mainly\nbecause EmoSet\nand"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "phase,\nthe center crop is employed to obtain 224 × 224 inputs.",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "FI\nhave more\ncategories\nand more\nsevere\nfuzzy\nproblems,"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "When training the source model on the Sentiment, Emotion,",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "which increases the difficulty of SFDA-VER.\n(2) The overall"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "and Office-Home datasets,\nthe maximum number of\ntraining",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "performance of SFDA methods is less stable compared to loss"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "epochs is empirically set\nto 10, 10, and 50, respectively. In the",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "functions. Notably, SHOT outperforms other methods in this"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "optimization of the target model,\nthe maximum epochs are set",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "comparison. However,\nsome of\nthe most advanced methods,"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "to 5, 5, and 15,\nrespectively.",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "such as C-SFDA [42]\nand AUL [49], do not perform well."
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "This suggests that more complex methods may not consistently"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "D. Results of Visual Emotion Recognition",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": ""
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "deliver optimal results, advocating for the potential superiority"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "Table\nIV shows\nthe\ncomparison\nresults\non\nthe\n2-class",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "of\nsimpler, more\nintuitive\napproaches.\n(3) Our FAL obtains"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "sentiment datasets. We report\nthe classification accuracies for",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "substantial performance enhancements of 3% and 6.8% across"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "12 sub-tasks and the average performance among them. From",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "the two sub-tasks, surpassing all other methods. This indicates"
        },
        {
          "FAL (ours)\n78.2\n-\n68.8\n77.8\n69.7": "the\ntable, we\ncan see\nthat:\n(1) Existing SFDA methods do",
          "68.4\n84.5\n74.5\n64.1\n69.9\n80.3\n80.3\n69.2\n82.5": "that\neven\non more\ncomplex VER datasets,\nFAL can\nstill"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Contentment"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "45.0|54.5"
        },
        {
          "TABLE V": "44.7|49.1"
        },
        {
          "TABLE V": "38.2|58.9"
        },
        {
          "TABLE V": "36.6|34.0"
        },
        {
          "TABLE V": "33.7|13.8"
        },
        {
          "TABLE V": "45.3|54.3"
        },
        {
          "TABLE V": "42.9|43.8"
        },
        {
          "TABLE V": "44.2|72.2"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "45.8|72.2"
        },
        {
          "TABLE V": "49.1|77.7"
        },
        {
          "TABLE V": "49.8|78.1"
        },
        {
          "TABLE V": "47.5|77.7"
        },
        {
          "TABLE V": "47.2|78.6"
        },
        {
          "TABLE V": "46.0|73.8"
        },
        {
          "TABLE V": "46.7|76.5"
        },
        {
          "TABLE V": "42.1|78.1"
        },
        {
          "TABLE V": "44.2|69.6"
        },
        {
          "TABLE V": "49.3|57.3"
        },
        {
          "TABLE V": "84.5|76.9"
        },
        {
          "TABLE V": "TABLE VI"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "Cl→Pr"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "78.2"
        },
        {
          "TABLE VI": "77.2"
        },
        {
          "TABLE VI": "79.8"
        },
        {
          "TABLE VI": "75.5"
        },
        {
          "TABLE VI": "79.3"
        },
        {
          "TABLE VI": "79.6"
        },
        {
          "TABLE VI": "77.3"
        },
        {
          "TABLE VI": "80.0"
        },
        {
          "TABLE VI": "79.8"
        },
        {
          "TABLE VI": "78.9"
        },
        {
          "TABLE VI": "79.5"
        },
        {
          "TABLE VI": "80.1"
        },
        {
          "TABLE VI": "79.7"
        },
        {
          "TABLE VI": "79.5"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "64.7"
        },
        {
          "TABLE VI": "71.2"
        },
        {
          "TABLE VI": "64.5"
        },
        {
          "TABLE VI": "63.1"
        },
        {
          "TABLE VI": "70.2"
        },
        {
          "TABLE VI": "66.1"
        },
        {
          "TABLE VI": "67.3"
        },
        {
          "TABLE VI": "69.2"
        },
        {
          "TABLE VI": "62.7"
        },
        {
          "TABLE VI": "79.1"
        },
        {
          "TABLE VI": "81.3"
        },
        {
          "TABLE VI": "91.4"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "maintain its effectiveness.",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "SFDA methods on this dataset, which contrasts sharply with its"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "performance on VER datasets. This further highlights the sig-"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "nificant differences between SFDA-VER and the conventional"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "E. Results of Object Recognition",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": ""
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "SFDA task.\n(2) Our FAL significantly outperforms all other"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "In addition to VER datasets, we also evaluate the perfor-",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "loss functions, achieving an accuracy improvement of 11.8%"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "mance of FAL and other\nloss\nfunctions on the highly chal-",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "compared to the source model. Additionally, FAL++ achieves"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "lenging Office-Home dataset. Similar to the 2-class sentiment",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "performance that is comparable to state-of-the-art SFDA meth-"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "datasets, we report\nthe results on 12 sub-tasks. Considering",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "ods. This\nindicates\nthat, although FAL is designed for VER,"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "that many SFDA methods combine some auxiliary techniques",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "it\nis\nalso applicable\nto standard SFDA tasks, demonstrating"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "to\nachieve\nbetter\nperformance, we\ncombine FAL with\nthe",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "its stronger generalization ability compared to existing SFDA"
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "labeling transfer strategy proposed in SHOT++ [36]\nto obtain",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": "methods and loss functions."
        },
        {
          "Target-supervised\n-\n77.9\n91.4\n84.4\n74.5\n91.4": "an enhanced version,\ni.e., FAL++. From Table VI, we can see",
          "84.4\n74.5\n77.9\n84.4\n74.5\n77.9\n91.4\n82.0": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "F\n. Analysis and Discussions",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "40",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "Ablation\nstudy. We\ninvestigate\nthe\nadvantages\nof\neach",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "20\ncomponent of the proposed loss function in Table VII. ℓf al−1",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "and ℓf al−2 denote the first and second terms of FAL in Eq.",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "tsne-2d-two\n0",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "(19), and weights refer to the w in Eq. (8). The results clearly",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "20\nshow that ℓf al−1 has a significant effect, and the performance",
          "10": "0 1 2 3 4 5 6 7"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "is\nfurther\nimproved\nby\nadding\nincorporating\nℓf al−2. After\n40",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "40\n20\n0\nweights,\nthere\nis\na\nslight\nimprovement\nin\naccuracy.\nIt\nis\ntsne-2d-one",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "(a) Source model",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "worth noting that using ℓf al−2 alone can lead to performance",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "robustness\nto noise\ndegradation as ℓf al−2 primarily provides",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "40",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "but\nlacks high-level semantic guidance for emotion categories.",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "20",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "This\nis\nconsistent with\nthe\nobservations\nin works\nsuch\nas",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "0\nSHOT [35], AaD [39],\nSCE [15],\nand AUL [49], which",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "tsne-2d-two",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "typically require combining two loss\nterms\nto achieve better\n20",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "learning results.",
          "10": "0 1 2 3 4 5 6 7"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "40",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "40\n20\n0\nTABLE VII",
          "10": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "tsne-2d-one",
          "10": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VII": "",
          "40\n20\n0\n20\n40": "tsne-2d-one"
        },
        {
          "TABLE VII": "AVERAGE ACCURACIES (%) ON THE 8-CLASS EMOTION DATASETS.",
          "40\n20\n0\n20\n40": "(d) Our FAL (FI→EmoSet)"
        },
        {
          "TABLE VII": "",
          "40\n20\n0\n20\n40": "feature representations learned by the source model"
        },
        {
          "TABLE VII": "Methods\nEmoSet→FI\nFI→EmoSet",
          "40\n20\n0\n20\n40": "the 8-class emotion datasets."
        },
        {
          "TABLE VII": "Source model only\n54.2\n50.2",
          "40\n20\n0\n20\n40": ""
        },
        {
          "TABLE VII": "56.5\n55.5\nℓf al−1",
          "40\n20\n0\n20\n40": ""
        },
        {
          "TABLE VII": "51.0\n45.9\nℓf al−2",
          "40\n20\n0\n20\n40": ""
        },
        {
          "TABLE VII": "",
          "40\n20\n0\n20\n40": "REFERENCES"
        },
        {
          "TABLE VII": "57.1\n56.7\nℓf al−1 + ℓf al−2",
          "40\n20\n0\n20\n40": ""
        },
        {
          "TABLE VII": "57.2\n57.0\nℓf al−1 + ℓf al−2 + weights",
          "40\n20\n0\n20\n40": "S. Zhao, X. Yao, J. Yang, G. Jia, G. Ding, T.-S. Chua, B. W. Schuller, and"
        },
        {
          "TABLE VII": "",
          "40\n20\n0\n20\n40": "K. Keutzer, “Affective image content analysis: Two decades review and"
        },
        {
          "TABLE VII": "",
          "40\n20\n0\n20\n40": "new perspectives,” IEEE Transactions on Pattern Analysis and Machine"
        },
        {
          "TABLE VII": "Feature visualization. We further compare the visualization",
          "40\n20\n0\n20\n40": ""
        },
        {
          "TABLE VII": "",
          "40\n20\n0\n20\n40": ""
        },
        {
          "TABLE VII": "results of\nthe\nfeature\nrepresentations\nlearned by the\nsource",
          "40\n20\n0\n20\n40": "“A fuzzy\nneural\nnetwork\nenabled"
        },
        {
          "TABLE VII": "",
          "40\n20\n0\n20\n40": "facial expression"
        },
        {
          "TABLE VII": "model\nand\nour FAL. We first\nextract\nthe\nhigh-dimensional",
          "40\n20\n0\n20\n40": ""
        },
        {
          "TABLE VII": "",
          "40\n20\n0\n20\n40": ""
        },
        {
          "TABLE VII": "features output by the second-to-last\nlayer,\ni.e.,\nthe bottleneck",
          "40\n20\n0\n20\n40": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[15] Y. Wang, X. Ma, Z. Chen, Y. Luo,\nJ. Yi,\nand J. Bailey,\n“Symmetric\n[38]\nS. Yang,\nJ. Van de Weijer, L. Herranz, S.\nJui\net al.,\n“Exploiting the"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "IEEE/CVF\nintrinsic neighborhood structure for source-free domain adaptation,” in\ncross\nentropy\nfor\nrobust\nlearning with\nnoisy\nlabels,”\nin"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "International Conference on Computer Vision, 2019, pp. 322–330.\nAdvances in Neural Information Processing Systems, 2021, pp. 29 393–"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "29 405.\n[16] R. Kosti, J. M. Alvarez, A. Recasens, and A. Lapedriza, “Context based"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[39]\nS. Yang, S.\nJui,\nJ. van de Weijer et al., “Attracting and dispersing: A\nemotion recognition using emotic dataset,” IEEE Transactions on Pattern"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "simple approach for source-free domain adaptation,” Advances in Neural\nAnalysis and Machine Intelligence, vol. 42, no. 11, pp. 2755–2766, 2020."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Information Processing Systems, vol. 35, pp. 5802–5815, 2022.\n[17] D. Yang, K. Yang, M. Li, S. Wang, S. Wang, and L. Zhang, “Robust"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[40]\nJ. Lee, D.\nJung,\nJ. Yim, and S. Yoon, “Confidence score for\nsource-\nemotion recognition in context debiasing,” in IEEE/CVF Conference on"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "free unsupervised domain adaptation,” in International Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 12 447–12 457."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Machine Learning, 2022, pp. 12 365–12 377.\n[18]\nJ. Yang, D. She, M. Sun, M.-M. Cheng, P. L. Rosin,\nand L. Wang,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[41]\nZ. Zhang, W. Chen, H. Cheng, Z. Li, S. Li, L. Lin, and G. Li, “Divide\n“Visual sentiment prediction based on automatic discovery of affective"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "and\ncontrast: Source-free\ndomain\nadaptation\nvia\nadaptive\ncontrastive\nregions,” IEEE Transactions on Multimedia, vol. 20, no. 9, pp. 2513–"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "learning,” Advances in Neural Information Processing Systems, vol. 35,\n2525, 2018."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "pp. 5137–5149, 2022.\n[19]\nS. Li\nand W. Deng,\n“Deep facial\nexpression recognition: A survey,”"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[42] N. Karim, N. C. Mithun, A. Rajvanshi, H.-p. Chiu, S. Samarasekera,\nIEEE Transactions on Affective Computing, vol. 13, no. 3, pp. 1195–"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "and N. Rahnavard, “C-sfda: A curriculum learning aided self-training\n1215, 2020."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "framework for efficient\nsource free domain adaptation,” in IEEE/CVF"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[20] U. Bhattacharya, T. Mittal, R. Chandra, T. Randhavane, A. Bera, and"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Conference\non Computer Vision\nand Pattern Recognition,\n2023,\npp."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "D. Manocha,\n“Step:\nSpatial\ntemporal\ngraph\nconvolutional\nnetworks"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "24 120–24 131."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "for\nemotion perception from gaits,”\nin AAAI Conference on Artificial"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[43] H. Xia, S. Xia, and Z. Ding, “Discriminative pattern calibration mech-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Intelligence, vol. 34, no. 02, 2020, pp. 1342–1350."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "anism for source-free domain adaptation,” in IEEE/CVF Conference on"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[21]\nT. Mittal, U. Bhattacharya, R. Chandra, A. Bera,\nand D. Manocha,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Computer Vision and Pattern Recognition, 2024, pp. 23 648–23 658."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "“M3er: Multiplicative multimodal\nemotion\nrecognition\nusing\nfacial,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[44] A. Ghosh, H. Kumar,\nand P. S. Sastry,\n“Robust\nloss\nfunctions under"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "textual, and speech cues,” in AAAI Conference on Artificial Intelligence,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "label noise for deep neural networks,” in AAAI Conference on Artificial"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "vol. 34, no. 02, 2020, pp. 1359–1367."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Intelligence, 2017, pp. 1919–1925."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[22] R. Kosti,\nJ. M. Alvarez, A. Recasens,\nand A. Lapedriza,\n“Emotion"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[45]\nZ. Zhang\nand M. R. Sabuncu,\n“Generalized\ncross\nentropy\nloss\nfor"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "recognition in context,” in IEEE Conference on Computer Vision and"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "training deep neural networks with noisy labels,” in Advances in Neural"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Pattern Recognition, 2017, pp. 1667–1675."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Information Processing Systems, 2018, pp. 8792–8802."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[23] D. Yang, Z. Chen, Y. Wang, S. Wang, M. Li, S. Liu, X. Zhao, S. Huang,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[46] Y. Liu and H. Guo, “Peer\nloss\nfunctions: Learning from noisy labels"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Z. Dong, P. Zhai et al., “Context de-confounded emotion recognition,”"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "without knowing noise rates,” in International Conference on Machine"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "in IEEE/CVF Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Learning, 2020, pp. 6226–6236."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "2023, pp. 19 005–19 015."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[47] X. Ma, H. Huang, Y. Wang, S. Romano, S. Erfani,\nand\nJ. Bailey,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[24] C. Xu, S. Cetintas, K.-C. Lee, and L.-J. Li, “Visual sentiment prediction"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "“Normalized\nloss\nfunctions\nfor\ndeep\nlearning with\nnoisy\nlabels,”\nin"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "with deep convolutional neural networks,” arXiv:1411.5731, 2014."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "International Conference on Machine Learning, 2020, pp. 6543–6553."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[25] Q. You, H.\nJin,\nand J. Luo,\n“Visual\nsentiment\nanalysis by attending"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[48] X. Ye, X. Li, T. Liu, Y. Sun, W. Tong et al., “Active negative loss func-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "on local\nimage regions,” in AAAI Conference on Artificial\nIntelligence,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "in Neural\nInformation\ntions\nfor\nlearning with noisy labels,” Advances"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "2017, pp. 231–237."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Processing Systems, vol. 36, pp. 6917–6940, 2023."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[26]\nJ. Yang, X. Gao, L. Li, X. Wang, and J. Ding, “Solver: Scene-object"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[49] X. Zhou, X. Liu, D. Zhai, J. Jiang, and X. Ji, “Asymmetric loss functions"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "interrelated visual emotion reasoning network,” IEEE Transactions on"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "for noise-tolerant learning: Theory and applications,” IEEE Transactions"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Image Processing, vol. 30, pp. 8686–8701, 2021."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "on Pattern Analysis and Machine Intelligence, vol. 45, no. 7, pp. 8094–"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[27]\nL. Xu, Z. Wang, B. Wu,\nand S. Lui,\n“Mdan: Multi-level dependent"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "8109, 2023."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "attention network for visual emotion analysis,” in IEEE/CVF Conference"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[50] Y. Zheng, H. Yao,\nand X. Sun,\n“Deep semantic parsing of\nfreehand"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "on Computer Vision and Pattern Recognition, 2022, pp. 9479–9488."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "sketches with\nhomogeneous\ntransformation,\nsoft-weighted\nloss,\nand"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[28] C. Zhao,\nJ. Shi, L. Nie,\nJ. Yang et al., “To err\nlike human: Affective"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "staged learning,” IEEE Transactions on Multimedia, vol. 23, pp. 3590–"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "bias-inspired measures\nfor visual\nemotion recognition evaluation,”\nin"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "3602, 2020."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Advances in Neural\nInformation Processing Systems, 2024."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[51] H. Wei, H. Zhuang, R. Xie, L. Feng, G. Niu, B. An, and Y. Li, “Miti-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[29] D. Borth, T. Chen, R.\nJi,\nand S.-F. Chang,\n“Sentibank:\nlarge-scale"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "gating memorization of noisy labels by clipping the model prediction,”"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "ontology and classifiers for detecting sentiment and emotions in visual"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "in International Conference on Machine Learning, 2023, pp. 36 868–"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "content,”\nin ACM International Conference on Multimedia, 2013, pp."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "36 886."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "459–460."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[52]\nJ. A. Mikels, B. L. Fredrickson, G. R. Larkin, C. M. Lindberg, S.\nJ."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[30] A. Radford,\nJ. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Maglio, and P. A. Reuter-Lorenz, “Emotional category data on images"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "from the\ninternational\naffective\npicture\nsystem,” Behavior Research"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "International\nvisual models\nfrom natural\nlanguage\nsupervision,”\nin"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Methods, vol. 37, pp. 626–630, 2005."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Conference on Machine Learning, 2021, pp. 8748–8763."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[53] D. Borth, R. Ji, T. Chen, T. Breuel, and S.-F. Chang, “Large-scale visual"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[31]\nS. Deng, L. Wu, G. Shi, L. Xing, W. Hu, H. Zhang,\nand Y. Xiang,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "sentiment ontology and detectors using adjective noun pairs,” in ACM"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "“Simple but powerful, a language-supervised method for image emotion"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "International Conference on Multimedia, 2013, pp. 223–232."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "classification,” IEEE Transactions on Affective Computing, vol. 14, no. 4,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[54] H. Venkateswara,\nJ. Eusebio, S. Chakraborty,\nand S. Panchanathan,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "pp. 3317–3331, 2023."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "“Deep hashing network for unsupervised domain adaptation,” in IEEE"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[32]\nJ. Pan, J. Lu, and S. Wang, “A multi-stage visual perception approach"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "and Pattern Recognition,\n2017,\npp."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "for image emotion analysis,” IEEE Transactions on Affective Computing,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "5018–5027."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "2024."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[55] Y. Kim, J. Yim, J. Yun, and J. Kim, “Nlnl: Negative learning for noisy"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[33] Y.\nZhang, Y.\nZheng, X. Xu,\nand\nJ. Wang,\n“How well\ndo\nself-\nIEEE/CVF International Conference\nlabels,”\nin\non Computer Vision,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "supervised methods\nperform in\ncross-domain\nfew-shot\nlearning?”\n2019, pp. 101–110."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "arXiv:2202.09014, 2022.\n[56]\nZ. Leng, M. Tan, C. Liu, E. D. Cubuk, J. Shi, S. Cheng, and D. Anguelov,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[34] K. Shi, J. Lu, Z. Fang, and G. Zhang, “Unsupervised domain adaptation\n“Polyloss: A polynomial\nexpansion\nperspective\nof\nclassification\nloss"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "IEEE Transactions\non Fuzzy\nenhanced\nby\nfuzzy\nprompt\nlearning,”\nfunctions,”\nin International Conference on Learning Representations,"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Systems, 2024.\n2022."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[35]\nJ. Liang, D. Hu, and J. Feng, “Do we really need to access the source\n[57] K. He, X. Zhang, S. Ren,\nand\nJ. Sun,\n“Deep\nresidual\nlearning\nfor"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "data? source hypothesis transfer for unsupervised domain adaptation,” in\nimage recognition,” in IEEE Conference on Computer Vision and Pattern"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "International Conference on Machine Learning, 2020, pp. 6028–6039.\nRecognition, 2016, pp. 770–778."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[36]\nJ. Liang, D. Hu, Y. Wang, R. He,\nand J. Feng,\n“Source data-absent\n[58]\nZ. Qiu, Y. Zhang, H. Lin, S. Niu, Y. Liu, Q. Du, and M. Tan, “Source-"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "unsupervised domain adaptation through hypothesis transfer and labeling\nfree domain adaptation via avatar prototype generation and adaptation,”"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "transfer,” IEEE Transactions on Pattern Analysis and Machine Intelli-\nJoint Conference on Artificial\nin International\nIntelligence, 2021, pp."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "gence, vol. 44, no. 11, pp. 8602–8617, 2021.\n2921–2927."
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "[59] H. Xia, H. Zhao, and Z. Ding, “Adaptive adversarial network for source-\n[37]\nS. Yang, Y. Wang, J. Van De Weijer, L. Herranz, and S. Jui, “Generalized"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "IEEE/CVF International Conference\non\nsource-free domain adaptation,” in IEEE/CVF International Conference\nfree\ndomain\nadaptation,”\nin"
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION\n11": "Computer Vision, 2021, pp. 9010–9019.\non Computer Vision, 2021, pp. 8978–8987."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "[60]\nS. Roy, M. Trapp, A. Pilzer, J. Kannala, N. Sebe, E. Ricci, and A. Solin,",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "“Uncertainty-guided source-free domain adaptation,” in European Con-",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "ference on Computer Vision, 2022, pp. 537–555.",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "[61]\nL. Yi, G. Xu, P. Xu,\nJ. Li, R. Pu, C. Ling,\nI. McLeod, and B. Wang,",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "“When source-free domain adaptation meets learning with noisy labels,”",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "in International Conference on Learning Representations, 2023.",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "[62] Y. Mitsuzumi, A. Kimura,\nand H. Kashima,\n“Understanding and im-",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "proving source-free domain adaptation from a theoretical perspective,”",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "in IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "2024, pp. 28 515–28 524.",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "[63]\nL. van der Maaten and G. Hinton, “Visualizing data using t-sne,” Journal",
          "12": ""
        },
        {
          "FUZZY-AWARE LOSS FOR SOURCE-FREE DOMAIN ADAPTATION IN VISUAL EMOTION RECOGNITION": "of Machine Learning Research, vol. 9, pp. 2579–2605, 2008.",
          "12": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "3",
      "title": "Affective image content analysis: Two decades review and new perspectives",
      "authors": [
        "S Zhao",
        "X Yao",
        "J Yang",
        "G Jia",
        "G Ding",
        "T.-S Chua",
        "B Schuller",
        "K Keutzer"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "A fuzzy neural network enabled deep subspace domain adaptive fusion approaches for facial expression recognition",
      "authors": [
        "W Shu",
        "F Zhang",
        "R Wan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Fuzzy Systems"
    },
    {
      "citation_id": "5",
      "title": "Fmfn: A fuzzy multimodal fusion network for emotion recognition in ensemble conducting",
      "authors": [
        "X Han",
        "F Chen",
        "J Ban"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Fuzzy Systems"
    },
    {
      "citation_id": "6",
      "title": "A fuzzy deep neural network with sparse autoencoder for emotional intention understanding in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "M Wu",
        "W Pedrycz",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Fuzzy systems"
    },
    {
      "citation_id": "7",
      "title": "A survey of embodied learning for object-centric robotic manipulation",
      "authors": [
        "Y Zheng",
        "L Yao",
        "Y Su",
        "Y Zhang",
        "Y Wang",
        "S Zhao",
        "Y Zhang",
        "L.-P Chau"
      ],
      "year": "2025",
      "venue": "Machine Intelligence Research"
    },
    {
      "citation_id": "8",
      "title": "Affective image classification using features inspired by psychology and art theory",
      "authors": [
        "J Machajdik",
        "A Hanbury"
      ],
      "year": "2010",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Sketchspecific data augmentation for freehand sketch recognition",
      "authors": [
        "Y Zheng",
        "H Yao",
        "X Sun",
        "S Zhang",
        "S Zhao",
        "F Porikli"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "10",
      "title": "Multi-source domain adaptation for visual sentiment classification",
      "authors": [
        "C Lin",
        "S Zhao",
        "L Meng",
        "T.-S Chua"
      ],
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "A comprehensive survey on source-free domain adaptation",
      "authors": [
        "J Li",
        "Z Yu",
        "Z Du",
        "L Zhu",
        "H Shen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2015",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Image sentiment analysis using latent correlations among visual, textual, and sentiment views",
      "authors": [
        "M Katsurai",
        "S Satoh"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Emoset: A large-scale visual emotion dataset with rich attributes",
      "authors": [
        "J Yang",
        "Q Huang",
        "T Ding",
        "D Lischinski",
        "D Cohen-Or",
        "H Huang"
      ],
      "year": "2023",
      "venue": "IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2016",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Symmetric cross entropy for robust learning with noisy labels",
      "authors": [
        "Y Wang",
        "X Ma",
        "Z Chen",
        "Y Luo",
        "J Yi",
        "J Bailey"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Robust emotion recognition in context debiasing",
      "authors": [
        "D Yang",
        "K Yang",
        "M Li",
        "S Wang",
        "S Wang",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Visual sentiment prediction based on automatic discovery of affective regions",
      "authors": [
        "J Yang",
        "D She",
        "M Sun",
        "M.-M Cheng",
        "P Rosin",
        "L Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Context de-confounded emotion recognition",
      "authors": [
        "D Yang",
        "Z Chen",
        "Y Wang",
        "S Wang",
        "M Li",
        "S Liu",
        "X Zhao",
        "S Huang",
        "Z Dong",
        "P Zhai"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Visual sentiment prediction with deep convolutional neural networks",
      "authors": [
        "C Xu",
        "S Cetintas",
        "K.-C Lee",
        "L.-J Li"
      ],
      "year": "2014",
      "venue": "Visual sentiment prediction with deep convolutional neural networks",
      "arxiv": "arXiv:1411.5731"
    },
    {
      "citation_id": "27",
      "title": "Visual sentiment analysis by attending on local image regions",
      "authors": [
        "Q You",
        "H Jin",
        "J Luo"
      ],
      "year": "2017",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Solver: Scene-object interrelated visual emotion reasoning network",
      "authors": [
        "J Yang",
        "X Gao",
        "L Li",
        "X Wang",
        "J Ding"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "29",
      "title": "Mdan: Multi-level dependent attention network for visual emotion analysis",
      "authors": [
        "L Xu",
        "Z Wang",
        "B Wu",
        "S Lui"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "To err like human: Affective bias-inspired measures for visual emotion recognition evaluation",
      "authors": [
        "C Zhao",
        "J Shi",
        "L Nie",
        "J Yang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content",
      "authors": [
        "D Borth",
        "T Chen",
        "R Ji",
        "S.-F Chang"
      ],
      "year": "2013",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "33",
      "title": "Simple but powerful, a language-supervised method for image emotion classification",
      "authors": [
        "S Deng",
        "L Wu",
        "G Shi",
        "L Xing",
        "W Hu",
        "H Zhang",
        "Y Xiang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "A multi-stage visual perception approach for image emotion analysis",
      "authors": [
        "J Pan",
        "J Lu",
        "S Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "How well do selfsupervised methods perform in cross-domain few-shot learning",
      "authors": [
        "Y Zhang",
        "Y Zheng",
        "X Xu",
        "J Wang"
      ],
      "year": "2022",
      "venue": "How well do selfsupervised methods perform in cross-domain few-shot learning",
      "arxiv": "arXiv:2202.09014"
    },
    {
      "citation_id": "36",
      "title": "Unsupervised domain adaptation enhanced by fuzzy prompt learning",
      "authors": [
        "K Shi",
        "J Lu",
        "Z Fang",
        "G Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Fuzzy Systems"
    },
    {
      "citation_id": "37",
      "title": "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
      "authors": [
        "J Liang",
        "D Hu",
        "J Feng"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "38",
      "title": "Source data-absent unsupervised domain adaptation through hypothesis transfer and labeling transfer",
      "authors": [
        "J Liang",
        "D Hu",
        "Y Wang",
        "R He",
        "J Feng"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Generalized source-free domain adaptation",
      "authors": [
        "S Yang",
        "Y Wang",
        "J Van De Weijer",
        "L Herranz",
        "S Jui"
      ],
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "40",
      "title": "Exploiting the intrinsic neighborhood structure for source-free domain adaptation",
      "authors": [
        "S Yang",
        "J Van De Weijer",
        "L Herranz",
        "S Jui"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "41",
      "title": "Attracting and dispersing: A simple approach for source-free domain adaptation",
      "authors": [
        "S Yang",
        "S Jui",
        "J Van De Weijer"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "42",
      "title": "Confidence score for sourcefree unsupervised domain adaptation",
      "authors": [
        "J Lee",
        "D Jung",
        "J Yim",
        "S Yoon"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "43",
      "title": "Divide and contrast: Source-free domain adaptation via adaptive contrastive learning",
      "authors": [
        "Z Zhang",
        "W Chen",
        "H Cheng",
        "Z Li",
        "S Li",
        "L Lin",
        "G Li"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "C-sfda: A curriculum learning aided self-training framework for efficient source free domain adaptation",
      "authors": [
        "N Karim",
        "N Mithun",
        "A Rajvanshi",
        "H -P. Chiu",
        "S Samarasekera",
        "N Rahnavard"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Discriminative pattern calibration mechanism for source-free domain adaptation",
      "authors": [
        "H Xia",
        "S Xia",
        "Z Ding"
      ],
      "year": "2024",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Robust loss functions under label noise for deep neural networks",
      "authors": [
        "A Ghosh",
        "H Kumar",
        "P Sastry"
      ],
      "year": "2017",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
      "authors": [
        "Z Zhang",
        "M Sabuncu"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "48",
      "title": "Peer loss functions: Learning from noisy labels without knowing noise rates",
      "authors": [
        "Y Liu",
        "H Guo"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "49",
      "title": "Normalized loss functions for deep learning with noisy labels",
      "authors": [
        "X Ma",
        "H Huang",
        "Y Wang",
        "S Romano",
        "S Erfani",
        "J Bailey"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "50",
      "title": "Active negative loss functions for learning with noisy labels",
      "authors": [
        "X Ye",
        "X Li",
        "T Liu",
        "Y Sun",
        "W Tong"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "51",
      "title": "Asymmetric loss functions for noise-tolerant learning: Theory and applications",
      "authors": [
        "X Zhou",
        "X Liu",
        "D Zhai",
        "J Jiang",
        "X Ji"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "52",
      "title": "Deep semantic parsing of freehand sketches with homogeneous transformation, soft-weighted loss, and staged learning",
      "authors": [
        "Y Zheng",
        "H Yao",
        "X Sun"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Mitigating memorization of noisy labels by clipping the model prediction",
      "authors": [
        "H Wei",
        "H Zhuang",
        "R Xie",
        "L Feng",
        "G Niu",
        "B An",
        "Y Li"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "54",
      "title": "Emotional category data on images from the international affective picture system",
      "authors": [
        "J Mikels",
        "B Fredrickson",
        "G Larkin",
        "C Lindberg",
        "S Maglio",
        "P Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "55",
      "title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs",
      "authors": [
        "D Borth",
        "R Ji",
        "T Chen",
        "T Breuel",
        "S.-F Chang"
      ],
      "year": "2013",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "56",
      "title": "Deep hashing network for unsupervised domain adaptation",
      "authors": [
        "H Venkateswara",
        "J Eusebio",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "Nlnl: Negative learning for noisy labels",
      "authors": [
        "Y Kim",
        "J Yim",
        "J Yun",
        "J Kim"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "58",
      "title": "Polyloss: A polynomial expansion perspective of classification loss functions",
      "authors": [
        "Z Leng",
        "M Tan",
        "C Liu",
        "E Cubuk",
        "J Shi",
        "S Cheng",
        "D Anguelov"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "59",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Sourcefree domain adaptation via avatar prototype generation and adaptation",
      "authors": [
        "Z Qiu",
        "Y Zhang",
        "H Lin",
        "S Niu",
        "Y Liu",
        "Q Du",
        "M Tan"
      ],
      "year": "2021",
      "venue": "International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "61",
      "title": "Adaptive adversarial network for sourcefree domain adaptation",
      "authors": [
        "H Xia",
        "H Zhao",
        "Z Ding"
      ],
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "62",
      "title": "Uncertainty-guided source-free domain adaptation",
      "authors": [
        "S Roy",
        "M Trapp",
        "A Pilzer",
        "J Kannala",
        "N Sebe",
        "E Ricci",
        "A Solin"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "63",
      "title": "When source-free domain adaptation meets learning with noisy labels",
      "authors": [
        "L Yi",
        "G Xu",
        "P Xu",
        "J Li",
        "R Pu",
        "C Ling",
        "I Mcleod",
        "B Wang"
      ],
      "year": "2023",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "64",
      "title": "Understanding and improving source-free domain adaptation from a theoretical perspective",
      "authors": [
        "Y Mitsuzumi",
        "A Kimura",
        "H Kashima"
      ],
      "year": "2024",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "65",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "66",
      "title": "Before that, he received the Ph",
      "venue": "His research interests include machine learning"
    },
    {
      "citation_id": "67",
      "title": "She is currently working towards the Ph.D. degree with The Chinese University of Hong Kong. Her research interests include Computer Vision and Medical AI",
      "venue": "She is currently working towards the Ph.D. degree with The Chinese University of Hong Kong. Her research interests include Computer Vision and Medical AI"
    },
    {
      "citation_id": "68",
      "title": "IEEE) received BEng degree in electronic information engineering and MEng degree in information and signal processing from the School of Electronics and Information",
      "authors": [
        "Yi Wang",
        "( Member"
      ],
      "year": "2013",
      "venue": "His research interest includes Image/Video Processing, Computer Vision, Intelligent Transport Systems, and Digital Forensics"
    },
    {
      "citation_id": "69",
      "title": "The Hong Kong Polytechnic University. His current research interests include large language model, perception for autonomous driving, egocentric computer vision, and 3D computer vision. He is an IEEE Fellow. He was the chair of Technical Committee on Circuits & Systems for Communications of IEEE Circuits and Systems Society from 2010 to 2012. He was general chairs and program chairs for some international conferences",
      "authors": [
        "Lap-Pui Chau",
        "; Ieee Bts"
      ],
      "year": "1997",
      "venue": "The Hong Kong Polytechnic University. His current research interests include large language model, perception for autonomous driving, egocentric computer vision, and 3D computer vision. He is an IEEE Fellow. He was the chair of Technical Committee on Circuits & Systems for Communications of IEEE Circuits and Systems Society from 2010 to 2012. He was general chairs and program chairs for some international conferences"
    }
  ]
}