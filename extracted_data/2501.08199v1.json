{
  "paper_id": "2501.08199v1",
  "title": "Emonext: An Adapted Convnext For Facial Emotion Recognition",
  "published": "2025-01-14T15:23:36Z",
  "authors": [
    "Yassine El Boudouri",
    "Amine Bohi"
  ],
  "keywords": [
    "Facial expression recognition",
    "Deep-learning",
    "Convolutional neural network",
    "Emotion classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expressions play a crucial role in human communication serving as a powerful and impactful means to express a wide range of emotions. With advancements in artificial intelligence and computer vision, deep neural networks have emerged as effective tools for facial emotion recognition. In this paper, we propose EmoNeXt, a novel deep learning framework for facial expression recognition based on an adapted ConvNeXt architecture network. We integrate a Spatial Transformer Network (STN) to focus on feature-rich regions of the face and Squeeze-and-Excitation blocks to capture channel-wise dependencies. Moreover, we introduce a self-attention regularization term, encouraging the model to generate compact feature vectors. We demonstrate the superiority of our model over existing state-of-the-art deep learning models on the FER2013 dataset regarding emotion classification accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial expressions are a powerful means of non-verbal communication in face-to-face interactions, allowing humans to convey a wide range of information. According to Albert Mehrabian, facial expressions are more effective than words in face-to-face communication  [1] . He revealed that words contribute only 7% to effective communication, while voice tone accounts for 38% and body language for 55%. Therefore, facial expressions play a vital role in human communication.\n\nIn this context, it is legitimate to attempt to model the process of human perception of facial expressions. In the last two decades, computer vision and artificial intelligence research have shown great interest in the automatic recognition of facial expressions in videos and static images. Facial Expression Recognition (FER) has gained immense importance in various fields such as security, healthcare, driver fatigue surveillance, and human-machine interaction applications  [2] -  [4] .\n\nOver the past few years, numerous conventional FER approaches have emerged, employing classical descriptors to explicitly extract features from face data. These approaches can be categorized into two groups: geometric methods and appearance-based methods. While geometric features capture the shape, location and interconnections of facial components during expressions  [5] ,  [6] , appearance-based features focus on the variations in facial appearance, such as wrinkles and furrows, and can be extracted from the whole face or specific regions  [3] ,  [7] ,  [8] . To classify these features, encompassing both geometric and texture-based ones, various classifiers have been employed, including Support Vector Machines (SVM), K-Nearest Neighbor (KNN), as well as neural networks such as MultiLayer Perceptron (MLP).\n\nConventional FER approaches follow a two-step process: they initially analyze and define facial features, and subsequently utilize these features for inference. However, as these two steps are performed separately, sub-optimal performance is obtained, particularly when dealing with complex datasets containing numerous sources of variability. Consequently, It is more advantageous to perform these two steps together for better recognition performance.\n\nOver the past two decades, deep neural networks have demonstrated exceptional effectiveness in automatic recognition tasks, making them a natural fit for automatic FER. Deep learning is used to learn discriminative feature representations for automatic FER by designing a hierarchical architecture composed of multiple non-linear transformations based on different types of neural networks such as convolutional networks (CNNs) and recurrent networks (RNNs). These networks are coupled with a classification layer for the classification task. As a result, the learning parameters of the classifier are determined in conjunction with the learning of the feature representations. This automation of feature extraction and classification directly from raw data greatly reduces dependence on models based on face geometry and other preprocessing techniques.\n\nIn this paper, we introduce EmoNeXt, a novel deep learning framework for FER based on an adapted ConvNeXt network  [9] . We integrate a Spatial Transformer Network (STN)  [10]  to allow the network to learn and apply spatial transformations to input images and Squeeze-and-Excitation blocks  [11]  to enable adaptive recalibration of channel-wise feature. Furthermore, we combine a Self-Attention regularization term and the classical Cross-Entropy  [12]  Loss to encourage the model to generate compact features. The proposed architecture was able to produce significantly better results than the original ConvNeXt network and outperform other state-of-the-art deep learning models under the same experimental setup on the FER2013 dataset  [13] . arXiv:2501.08199v1 [cs.CV] 14 Jan 2025",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we will briefly review some recent research on facial emotion classification, with a specific focus on models that have been evaluated using the FER2013 dataset.\n\nGiven the remarkable achievements and the rise of deep learning in the realm of computer vision, particularly in image classification tasks, several studies have introduced a range of deep learning approaches to address automated FER on the FER2013 dataset with the sole objective of achieving the best possible classification accuracy.\n\nIn  [14] , Georgescu et al. presented a method where automatic features, learned by multiple CNN architectures, were combined with handcrafted features computed using the bagof-visual-words (BOVW) model. Once the fusion of the two types of features is accomplished, a local learning framework is utilized to make predictions of the class label for each individual test image. In  [15] , Pecoraro et al. introduced a novel channel self-attention module called Local multi-Head Channel (LHC), that can be seamlessly incorporated into any existing CNN architecture. This module offers a solution to the limitation of Global Attention mechanisms by effectively directing attention to crucial facial features that significantly influence facial expressions. In another paper  [16] , Fard et al. proposed an Adaptive Correlation (Ad-Corre) Loss consisting of three components: Feature Discriminator (FD), Mean Discriminator (MD) and Embedding Discriminator (ED). The proposed Ad-Corre Loss was combined with the classical Cross-Entropy Loss and used to train two backbone models: Xception  [17]  and Resnet50  [18] . The authors demonstrated that irrespective of the deep-learning model employed, the Ad-Corre Loss allows to increase the discriminative power of generated feature vectors, consequently leading to high accuracy in classification. Another novel deep learning model known as Segmentation VGG-19, was proposed in a recent study by  Vignesh et al. [19] . This model enhances FER by integrating U-Net  [20]  based segmentation blocks into the VGG-19 (Visual Geometry Group) architecture  [21] . By inserting these segmentation blocks between the layers of VGG-19, the model effectively emphasizes significant features from the feature map, leading to an enhanced feature extraction process. In another recent paper  [22]",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": ", Mukhopadhyay Et Al. Presented A Deep-Learning-Based Fer Method By Exploiting Textural Features Such As Local Binary Patterns (Lbp), Local Ternary Patterns (Ltp) And Completed Local Binary Patterns (Clbp)",
      "text": ". A CNN model is then trained over these textural image features to achieve improved accuracy in detecting facial emotions. In a recent publication by Shahzad et al.  [23] , a zoning-based FER (ZFER) was introduced. The objective of ZFER was to accurately identify additional facial landmarks such as eyes, eyebrows, nose, forehead, and mouth, enabling a more comprehensive understanding of deep facial emotions through zoning. After the initial steps of face detection and extraction of face landmarks, these zoning-based landmarks were employed to train a hybrid VGG-16 model. The resulting feature maps generated by the hybrid model were then utilized as input for a fully CNN to classify facial emotions.\n\nLater in the results section, Table  II  presents a compilation of references to other deep learning-based models utilized for the FER task. This table provides an overview of the classification scores achieved by these state-of-the-art FER models on the FER2013 dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methods",
      "text": "This section explores various techniques and components integrated into a comprehensive deep learning model designed specifically for facial emotion detection. The focus primarily revolves around three key aspects: the preprocessing of input images at the beginning of the network, the feature extraction and classification, and the loss function minimizations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Spatial Transformer Networks",
      "text": "Spatial Transformer Networks (STN)  [10]  extend the concept of differentiable attention to encompass various spatial transformations. By integrating a differentiable geometric transformation module into the network architecture, STN allow neural networks to learn and apply spatial transformations to input data. This capability proves invaluable in FER, where the performance is heavily influenced by scale, rotation, and translation variations.\n\nThe spatial transformer mechanism comprises three main components, illustrated in Figure  1 . The first component is the localization network, which employs convolutional layers to generate a vector representation of the input image. This vector is then utilized by the grid generator to create a sampling grid. The grid consists of points that determine where the input map should be sampled to generate the transformed output. Typically, fully connected layers are employed in the grid generator. Lastly, the feature map and the sampling grid are fed into the sampler, which samples the input image at the grid points to produce the final output image. The key advantage of STN is their ability to learn the spatial transformations automatically as part of the neural network training process.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Convnext",
      "text": "Introduced in 2022, ConvNeXt  [9]  is a pure convolutional model that draws inspiration from Vision Transformers  [24] . It was designed to compete with state-of-the-art Vision Transformers while retaining the simplicity and efficiency of CNNs.\n\nIt incorporates various enhancements to the architecture of a standard ResNet  [18] , with many of these modifications evident in the ConvNeXt block.\n\nThe ConvNeXt block, illustrated in Figure  2 , uses larger kernel-sized and depthwise convolutions, increases the network width to 96 channels, and utilizes an inverted bottleneck design, to lower the overall network floating-point operations (FLOPs) while enhancing performance.\n\nConvNeXt also replaces ReLU  [25]  with GELU  [26]  as the activation function and substituting BatchNorm (BN)  [27]  with Layer Normalization (LN)  [28]  as the normalization technique, allowing the model to acheive a slightly better performance. GELU and LN are commonly used in advanced Transformers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Squeeze-And-Excitation Blocks",
      "text": "Squeeze-and-Excitation (SE)  [11]  is a powerful technique used in deep learning models to enhance the representational power of CNN models. It introduces a mechanism that allows the network to adaptively recalibrate the channel-wise features, improving the model's discriminative capabilities. As illustrated in Figure  3 , the SE block consists of two fundamental operations: squeezing and exciting. In the squeezing phase, global average pooling is applied to each channel of the feature map (W, H, C), reducing its spatial dimensions to a single value per channel (1, 1, C). The exciting phase follows, where the squeezed values are transformed using a small set of fully connected layers. These layers learn channel-wise weights, capturing the interdependencies among feature channels. The resulting attention weights are then multiplied element-wise with the original feature map, emphasizing informative channels and suppressing less relevant ones.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Emonext Final Architecture",
      "text": "The EmoNeXt architecture begins with the inclusion of STN at the beginning of the network. STN enable the model to handle variations in scale, rotation, and translation by learning and applying spatial transformations to facial images.\n\nAfter passing through the STN, the inputs are then passed through ConvNeXt's patchify module. This module downsamples the input image using a non-overlapping convolution with a kernel size of 4. The downsampling helps to reduce the dimensionality of the input and capture relevant features efficiently.\n\nThe downscaled inputs then go through the ConvNeXt stages. Each stage is followed by a SE block to recalibrate the feature map before going into the next stage. This recalibration enhances the model's ability to extract discriminative facial features for accurate emotion recognition. The overall architecture is illustrated in Figure  4 .\n\nBy leveraging these techniques, our EmoNeXt model achieves robust and accurate facial emotion detection, effectively handling variations in facial expressions and improving overall performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Self-Attention Regularization Term",
      "text": "Self-attention is a powerful mechanism used in various domains, including natural language processing, image analysis, and feature extraction  [29] . It enables models to assign importance to different parts of the input sequence, allowing for effective capturing of dependencies and relationships.\n\nA commonly used approach is the dot product self-attention, known for its simplicity and effectiveness. It calculates the relevance and importance of the feature vector by computing the dot product similarity between pairs of its elements. The resulting attention scores are then used to weight the feature vector, generating an attended representation that highlights the most significant information.\n\nMathematically, the dot product self-attention weights can be computed as follows: Here, Q and K represent queries and keys of dimension d. We use a linear layer to compute the queries (Q) and keys (K) from a feature vector. The dot product between Q and K ⊤ computes pairwise similarities, while scaling the dot products by √ d prevents gradient issues. The softmax operation normalizes the attention scores.\n\nTo optimize the model's performance, an appropriate loss function is crucial. For multiclass classification tasks, the widely used Cross-Entropy Loss  [12]  serves as the standard choice. However, we introduce a novel component called the Self-Attention regularization term (SA).\n\nThe SA regularization term aims to minimize the attention weights to encourage balanced importance between generated features. It is defined as:\n\nwhere W represents the self-attention weights, W represents the mean attention weights and N is the length of the feature vector. By minimizing this term, we encourage the attention weights to be closer to the mean value, promoting balanced importance across the features.\n\nThe SA regularization term is added to the overall loss function of the model, in addition to the Cross-Entropy Loss. This helps to incorporate the regularization effect during training, encouraging the model to produce more balanced attention weights and improve generalization.\n\nBy combining the Cross-Entropy Loss and the SA regularization term, the final loss function becomes:\n\nwhere L CE is the Cross-Entropy Loss, L SA is the SA regularization term, and λ is a hyperparameter that controls the trade-off between the two terms.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments A. Dataset",
      "text": "The experiments were conducted on the FER2013 dataset, initially introduced in ICML 2013 Challenges in Representation Learning  [13] . The dataset consists of 35,887 grayscale images, each sized 48 2 pixels (Figure  5 ). It is divided into three subsets: 28,709 images for training, 3,589 images for validation, and 3,589 images for testing. The faces in the dataset are labeled with one of the seven classes as mentioned in Table  I . This dataset is widely used for FER tasks due to its significant number of samples. However, the distribution of classes in this dataset is highly unbalanced, which poses a challenge for any deep learning model. Figure  5",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Training",
      "text": "Recent studies have highlighted the effectiveness of modern training techniques in significantly improving the performance of deep learning models. In our model training, we employ various advanced strategies to improve the results. We utilize the AdamW optimizer  [30]  with a learning rate of 1e-4, combined with a cosine decay schedule to enhance convergence. To prevent overfitting, we implement regularization schemes like Stochastic Depth  [31]  and Label Smoothing  [32] , which contribute to more robust and generalized models.\n\nTo further enhance the performance and address memory constraints, we employ the Exponential Moving Average (EMA) technique  [33] . EMA has proven effective in alleviating overfitting, particularly in larger models. Moreover, we adopt Mixed Precision Training  [34] , a method that reduces memory consumption by almost 2x while accelerating the training process.\n\nIn addition, we enhance our model's capabilities by incorporating weights from pretrained ConvNeXt on the ImageNet-22k dataset  [35] . This dataset, known for its vast collection of diverse images, allows our model to leverage a wealth of learned knowledge. To ensure compatibility with the pretrained weights, we resize the images in our training pipeline to 224 2 , adhering to the established industry practice. This resizing technique enables us to effectively utilize the pretrained weights, resulting in improved performance and enhanced proficiency.\n\nFinally, we trained both the EmoNeXt and ConvNeXt models, encompassing all five sizes (T, S, B, L, and XL), utilizing an Nvidia T4 GPU with 16GB of VRAM. The implementation was done using PyTorch version 2.0.0, and the code is available at: https://github.com/yelboudouri/EmoNeXt",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Results",
      "text": "The results presented in Table II demonstrate the superior performance of our proposed model, EmoNeXt, compared to existing state-of-the-art single network architectures trained on the FER2013 dataset. Among the listed models, EmoNeXt-XLarge stands out with an accuracy of 76.12%. This achievement can be attributed to the unique design and architecture of EmoNeXt, which effectively captures and emphasizes relevant facial features for accurate emotion classification.\n\nWhen considering accuracy, EmoNeXt-Tiny achieves an accuracy of 73.34%, surpassing well-known models like ResNet50 (73.20%) and VGG (73.28%), as well as the three first versions of ConvNeXt: tiny, small, and base.\n\nWith notable progress, EmoNeXt-Small exhibits enhanced performance compared to EmoNeXt-Tiny, attaining an accu-",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model",
      "text": "Accuracy (%) GoogleNet  [36]  65.20 Deep Emotion  [37]  70.02 Inception  [38]  71.60 ConvNeXt-Tiny 71.99 Ad-Corre  [16]  72.03 ConvNeXt-Small 72.34 SE-Net50  [39]  72.50 ResNet50  [39]  73. EmoNeXt-Base maintains its performance by achieving an accuracy of 74.91%, surpassing models like LHC-NetC (74.28%). On the other hand, EmoNeXt-Large achieves an accuracy of 75.57%, outperforming the combination of CNN and BOVW models (75.42%).\n\nNotably, EmoNeXt-XLarge achieves an accuracy of 76.12%, surpassing the current best state-of-the-art accuracy attained by the Segmentation VGG-19 model (75.97%). This remarkable performance firmly establishes EmoNeXt-XLarge as a highly effective model for image classification tasks, specifically for facial emotion recognition (FER).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we presented EmoNeXt, a novel deep learning framework for facial expression recognition based on an adapted ConvNeXt architecture network. The EmoNeXt model integrates a Spatial Transformer Network (STN), Squeeze-and-Excitation blocks, and self-attention regularization to capture rich facial features and improve emotion classification accuracy. Experimental results on the FER2013 dataset demonstrate the superiority of EmoNeXt over existing state-of-the-art models. The integration of STN, SE blocks, and self-attention provides robust and accurate facial emotion detection, making EmoNeXt a promising approach for various applications requiring emotion recognition.\n\nA detailed study is underway and will be published in a forthcoming journal paper. In this upcoming publication, our objective is to conduct a thorough evaluation of our model and compare its performance with other models, using diverse FER databases.\n\nFurthermore, we have future plans to extend the application of our model to emotion recognition in elderly Alzheimer's patients, highlighting its potential for real-world impact and further research avenues.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The first component is the",
      "page": 2
    },
    {
      "caption": "Figure 1: The architecture of a spatial transformer module.",
      "page": 2
    },
    {
      "caption": "Figure 2: , uses larger",
      "page": 3
    },
    {
      "caption": "Figure 2: The ConvNeXt block.",
      "page": 3
    },
    {
      "caption": "Figure 3: , the SE block consists of two fundamental",
      "page": 3
    },
    {
      "caption": "Figure 3: The architecture of the Squeeze-and-Excitation block.",
      "page": 3
    },
    {
      "caption": "Figure 4: By leveraging these techniques, our EmoNeXt model",
      "page": 3
    },
    {
      "caption": "Figure 4: Architecture designes for ConvNeXt and EmoNeXt.",
      "page": 4
    },
    {
      "caption": "Figure 5: ). It is divided into",
      "page": 4
    },
    {
      "caption": "Figure 5: includes a",
      "page": 4
    },
    {
      "caption": "Figure 5: Sample images from the FER2013 dataset.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "yelboudouri@cesi.fr": "Abstract—Facial\nexpressions\nplay\na\ncrucial\nrole\nin\nhuman",
          "abohi@cesi.fr": "regions [3],\n[7],\n[8]. To classify these features, encompassing"
        },
        {
          "yelboudouri@cesi.fr": "communication serving as a powerful and impactful means\nto",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "both geometric and texture-based ones, various classifiers have"
        },
        {
          "yelboudouri@cesi.fr": "express a wide range of emotions. With advancements in artificial",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "been employed,\nincluding Support Vector Machines\n(SVM),"
        },
        {
          "yelboudouri@cesi.fr": "intelligence\nand\ncomputer\nvision,\ndeep\nneural\nnetworks\nhave",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "K-Nearest Neighbor\n(KNN), as well as neural networks such"
        },
        {
          "yelboudouri@cesi.fr": "emerged as effective tools for facial emotion recognition. In this",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "paper, we propose EmoNeXt, a novel deep learning framework",
          "abohi@cesi.fr": "as MultiLayer Perceptron (MLP)."
        },
        {
          "yelboudouri@cesi.fr": "for facial expression recognition based on an adapted ConvNeXt",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "Conventional FER approaches\nfollow a\ntwo-step process:"
        },
        {
          "yelboudouri@cesi.fr": "architecture network. We integrate a Spatial Transformer Net-",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "they initially analyze\nand define\nfacial\nfeatures,\nand subse-"
        },
        {
          "yelboudouri@cesi.fr": "work (STN)\nto\nfocus\non feature-rich regions\nof\nthe\nface\nand",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "Squeeze-and-Excitation blocks\nto\ncapture\nchannel-wise depen-",
          "abohi@cesi.fr": "quently utilize these features for inference. However, as these"
        },
        {
          "yelboudouri@cesi.fr": "dencies. Moreover, we\nintroduce a self-attention regularization",
          "abohi@cesi.fr": "two steps are performed separately, sub-optimal performance"
        },
        {
          "yelboudouri@cesi.fr": "term, encouraging the model to generate compact feature vectors.",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "is obtained, particularly when dealing with complex datasets"
        },
        {
          "yelboudouri@cesi.fr": "We\ndemonstrate\nthe\nsuperiority\nof\nour model\nover\nexisting",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "containing numerous\nsources of variability. Consequently,\nIt"
        },
        {
          "yelboudouri@cesi.fr": "state-of-the-art deep learning models\non the FER2013 dataset",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "is more advantageous to perform these two steps together\nfor"
        },
        {
          "yelboudouri@cesi.fr": "regarding emotion classification accuracy.",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "better\nrecognition performance."
        },
        {
          "yelboudouri@cesi.fr": "Index\nTerms—Facial\nexpression\nrecognition, Deep-learning,",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "Convolutional neural network, Emotion classification.",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "Over\nthe\npast\ntwo\ndecades,\ndeep\nneural\nnetworks\nhave"
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "demonstrated exceptional effectiveness\nin automatic recogni-"
        },
        {
          "yelboudouri@cesi.fr": "I.\nINTRODUCTION",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "tion tasks, making them a natural fit for automatic FER. Deep"
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "learning is used to learn discriminative feature representations"
        },
        {
          "yelboudouri@cesi.fr": "Facial\nexpressions\nare\na\npowerful means\nof\nnon-verbal",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "for\nautomatic FER by designing a hierarchical\narchitecture"
        },
        {
          "yelboudouri@cesi.fr": "communication in face-to-face interactions, allowing humans",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "composed of multiple non-linear transformations based on dif-"
        },
        {
          "yelboudouri@cesi.fr": "to convey a wide range of\ninformation. According to Albert",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "ferent types of neural networks such as convolutional networks"
        },
        {
          "yelboudouri@cesi.fr": "Mehrabian,\nfacial expressions are more effective than words",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "(CNNs) and recurrent networks (RNNs). These networks are"
        },
        {
          "yelboudouri@cesi.fr": "in\nface-to-face\ncommunication\n[1]. He\nrevealed\nthat words",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "coupled with a classification layer\nfor\nthe classification task."
        },
        {
          "yelboudouri@cesi.fr": "contribute only 7% to effective communication, while voice",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "As\na\nresult,\nthe\nlearning\nparameters\nof\nthe\nclassifier\nare"
        },
        {
          "yelboudouri@cesi.fr": "tone accounts for 38% and body language for 55%. Therefore,",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "determined\nin\nconjunction with\nthe\nlearning\nof\nthe\nfeature"
        },
        {
          "yelboudouri@cesi.fr": "facial expressions play a vital\nrole in human communication.",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "representations. This automation of feature extraction and clas-"
        },
        {
          "yelboudouri@cesi.fr": "In this context,\nit\nis legitimate to attempt\nto model\nthe pro-",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "sification directly from raw data greatly reduces dependence"
        },
        {
          "yelboudouri@cesi.fr": "cess of human perception of facial expressions. In the last\ntwo",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "on models based on face geometry and other preprocessing"
        },
        {
          "yelboudouri@cesi.fr": "decades,\ncomputer vision and artificial\nintelligence\nresearch",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "",
          "abohi@cesi.fr": "techniques."
        },
        {
          "yelboudouri@cesi.fr": "have shown great interest in the automatic recognition of facial",
          "abohi@cesi.fr": ""
        },
        {
          "yelboudouri@cesi.fr": "expressions\nin\nvideos\nand\nstatic\nimages. Facial Expression",
          "abohi@cesi.fr": "In this paper, we introduce EmoNeXt, a novel deep learning"
        },
        {
          "yelboudouri@cesi.fr": "Recognition (FER) has gained immense importance in various",
          "abohi@cesi.fr": "framework for FER based on an adapted ConvNeXt network"
        },
        {
          "yelboudouri@cesi.fr": "fields such as security, healthcare, driver\nfatigue surveillance,",
          "abohi@cesi.fr": "[9]. We integrate a Spatial Transformer Network (STN) [10] to"
        },
        {
          "yelboudouri@cesi.fr": "and human-machine interaction applications [2]–[4].",
          "abohi@cesi.fr": "allow the network to learn and apply spatial transformations to"
        },
        {
          "yelboudouri@cesi.fr": "Over\nthe past\nfew years, numerous conventional FER ap-",
          "abohi@cesi.fr": "input images and Squeeze-and-Excitation blocks [11] to enable"
        },
        {
          "yelboudouri@cesi.fr": "proaches\nhave\nemerged,\nemploying\nclassical\ndescriptors\nto",
          "abohi@cesi.fr": "adaptive\nrecalibration of\nchannel-wise\nfeature. Furthermore,"
        },
        {
          "yelboudouri@cesi.fr": "explicitly extract\nfeatures\nfrom face data. These\napproaches",
          "abohi@cesi.fr": "we\ncombine\na\nSelf-Attention\nregularization\nterm and\nthe"
        },
        {
          "yelboudouri@cesi.fr": "can be categorized into two groups: geometric methods and",
          "abohi@cesi.fr": "classical Cross-Entropy\n[12] Loss\nto\nencourage\nthe model"
        },
        {
          "yelboudouri@cesi.fr": "appearance-based methods. While geometric features capture",
          "abohi@cesi.fr": "to generate compact\nfeatures. The proposed architecture was"
        },
        {
          "yelboudouri@cesi.fr": "the shape,\nlocation and interconnections of facial components",
          "abohi@cesi.fr": "able\nto produce\nsignificantly better\nresults\nthan the original"
        },
        {
          "yelboudouri@cesi.fr": "during expressions\n[5],\n[6],\nappearance-based features\nfocus",
          "abohi@cesi.fr": "ConvNeXt network and outperform other state-of-the-art deep"
        },
        {
          "yelboudouri@cesi.fr": "on the variations\nin facial appearance,\nsuch as wrinkles and",
          "abohi@cesi.fr": "learning models under\nthe\nsame\nexperimental\nsetup on the"
        },
        {
          "yelboudouri@cesi.fr": "furrows, and can be extracted from the whole face or specific",
          "abohi@cesi.fr": "FER2013 dataset\n[13]."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "II. RELATED WORK": "",
          "as input\nfor a fully CNN to classify facial emotions.": "Later\nin the results section, Table II presents a compilation"
        },
        {
          "II. RELATED WORK": "In this section, we will briefly review some recent research",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "",
          "as input\nfor a fully CNN to classify facial emotions.": "of\nreferences\nto\nother\ndeep\nlearning-based models\nutilized"
        },
        {
          "II. RELATED WORK": "on\nfacial\nemotion\nclassification, with\na\nspecific\nfocus\non",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "",
          "as input\nfor a fully CNN to classify facial emotions.": "for\nthe FER task. This\ntable\nprovides\nan\noverview of\nthe"
        },
        {
          "II. RELATED WORK": "models that have been evaluated using the FER2013 dataset.",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "",
          "as input\nfor a fully CNN to classify facial emotions.": "classification\nscores\nachieved\nby\nthese\nstate-of-the-art FER"
        },
        {
          "II. RELATED WORK": "Given the\nremarkable\nachievements\nand the\nrise of deep",
          "as input\nfor a fully CNN to classify facial emotions.": "models on the FER2013 dataset."
        },
        {
          "II. RELATED WORK": "learning in the realm of computer vision, particularly in image",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "",
          "as input\nfor a fully CNN to classify facial emotions.": "III. METHODS"
        },
        {
          "II. RELATED WORK": "classification tasks, several studies have introduced a range of",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "deep learning approaches\nto address automated FER on the",
          "as input\nfor a fully CNN to classify facial emotions.": "This\nsection explores various\ntechniques\nand components"
        },
        {
          "II. RELATED WORK": "FER2013 dataset with the sole objective of achieving the best",
          "as input\nfor a fully CNN to classify facial emotions.": "integrated into a comprehensive deep learning model designed"
        },
        {
          "II. RELATED WORK": "possible classification accuracy.",
          "as input\nfor a fully CNN to classify facial emotions.": "specifically for\nfacial emotion detection. The focus primarily"
        },
        {
          "II. RELATED WORK": "In [14], Georgescu et al. presented a method where auto-",
          "as input\nfor a fully CNN to classify facial emotions.": "revolves around three key aspects:\nthe preprocessing of\ninput"
        },
        {
          "II. RELATED WORK": "matic features,\nlearned by multiple CNN architectures, were",
          "as input\nfor a fully CNN to classify facial emotions.": "images at\nthe beginning of the network,\nthe feature extraction"
        },
        {
          "II. RELATED WORK": "combined with handcrafted features computed using the bag-",
          "as input\nfor a fully CNN to classify facial emotions.": "and classification, and the loss function minimizations."
        },
        {
          "II. RELATED WORK": "of-visual-words (BOVW) model. Once the fusion of\nthe two",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "",
          "as input\nfor a fully CNN to classify facial emotions.": "A.\nSpatial Transformer Networks"
        },
        {
          "II. RELATED WORK": "types of features is accomplished, a local\nlearning framework",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "is\nutilized\nto make\npredictions\nof\nthe\nclass\nlabel\nfor\neach",
          "as input\nfor a fully CNN to classify facial emotions.": "Spatial Transformer Networks (STN)\n[10] extend the con-"
        },
        {
          "II. RELATED WORK": "individual\ntest\nimage.\nIn [15], Pecoraro et\nal.\nintroduced a",
          "as input\nfor a fully CNN to classify facial emotions.": "cept\nof\ndifferentiable\nattention\nto\nencompass\nvarious\nspa-"
        },
        {
          "II. RELATED WORK": "novel channel\nself-attention module called Local multi-Head",
          "as input\nfor a fully CNN to classify facial emotions.": "tial\ntransformations. By integrating a differentiable geometric"
        },
        {
          "II. RELATED WORK": "Channel\n(LHC),\nthat can be seamlessly incorporated into any",
          "as input\nfor a fully CNN to classify facial emotions.": "transformation module into the network architecture, STN al-"
        },
        {
          "II. RELATED WORK": "existing CNN architecture. This module offers a solution to",
          "as input\nfor a fully CNN to classify facial emotions.": "low neural networks to learn and apply spatial\ntransformations"
        },
        {
          "II. RELATED WORK": "the limitation of Global Attention mechanisms by effectively",
          "as input\nfor a fully CNN to classify facial emotions.": "to input data. This capability proves invaluable in FER, where"
        },
        {
          "II. RELATED WORK": "directing attention to crucial\nfacial\nfeatures\nthat\nsignificantly",
          "as input\nfor a fully CNN to classify facial emotions.": "the performance is heavily influenced by scale,\nrotation, and"
        },
        {
          "II. RELATED WORK": "influence facial expressions. In another paper [16], Fard et al.",
          "as input\nfor a fully CNN to classify facial emotions.": "translation variations."
        },
        {
          "II. RELATED WORK": "proposed an Adaptive Correlation (Ad-Corre) Loss\nconsist-",
          "as input\nfor a fully CNN to classify facial emotions.": "The\nspatial\ntransformer mechanism comprises\nthree main"
        },
        {
          "II. RELATED WORK": "ing of\nthree components: Feature Discriminator\n(FD), Mean",
          "as input\nfor a fully CNN to classify facial emotions.": "components,\nillustrated in Figure 1. The first component\nis the"
        },
        {
          "II. RELATED WORK": "Discriminator (MD) and Embedding Discriminator (ED). The",
          "as input\nfor a fully CNN to classify facial emotions.": "localization network, which employs convolutional\nlayers\nto"
        },
        {
          "II. RELATED WORK": "proposed Ad-Corre Loss was\ncombined with\nthe\nclassical",
          "as input\nfor a fully CNN to classify facial emotions.": "generate a vector representation of the input image. This vector"
        },
        {
          "II. RELATED WORK": "Cross-Entropy Loss and used to train two backbone models:",
          "as input\nfor a fully CNN to classify facial emotions.": "is then utilized by the grid generator to create a sampling grid."
        },
        {
          "II. RELATED WORK": "Xception [17] and Resnet50 [18]. The authors demonstrated",
          "as input\nfor a fully CNN to classify facial emotions.": "The grid consists of points\nthat determine where\nthe\ninput"
        },
        {
          "II. RELATED WORK": "that\nirrespective\nof\nthe\ndeep-learning model\nemployed,\nthe",
          "as input\nfor a fully CNN to classify facial emotions.": "map should be sampled to generate the transformed output."
        },
        {
          "II. RELATED WORK": "Ad-Corre Loss\nallows\nto increase\nthe discriminative power",
          "as input\nfor a fully CNN to classify facial emotions.": "Typically,\nfully\nconnected\nlayers\nare\nemployed\nin\nthe\ngrid"
        },
        {
          "II. RELATED WORK": "of\ngenerated\nfeature\nvectors,\nconsequently\nleading\nto\nhigh",
          "as input\nfor a fully CNN to classify facial emotions.": "generator. Lastly,\nthe feature map and the sampling grid are"
        },
        {
          "II. RELATED WORK": "accuracy in classification. Another novel deep learning model",
          "as input\nfor a fully CNN to classify facial emotions.": "fed into the sampler, which samples\nthe\ninput\nimage at\nthe"
        },
        {
          "II. RELATED WORK": "known as Segmentation VGG-19, was proposed in a\nrecent",
          "as input\nfor a fully CNN to classify facial emotions.": "grid points to produce the final output\nimage."
        },
        {
          "II. RELATED WORK": "study\nby Vignesh\net\nal.\n[19]. This model\nenhances\nFER",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "by\nintegrating U-Net\n[20]\nbased\nsegmentation\nblocks\ninto",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "the VGG-19 (Visual Geometry Group) architecture [21]. By",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "inserting\nthese\nsegmentation\nblocks\nbetween\nthe\nlayers\nof",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "VGG-19, the model effectively emphasizes significant features",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "from the feature map, leading to an enhanced feature extraction",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "process.\nIn another\nrecent paper\n[22], Mukhopadhyay et\nal.",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "presented a deep-learning-based FER method by exploiting",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "textural\nfeatures\nsuch as\nlocal binary patterns\n(LBP),\nlocal",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "ternary\npatterns\n(LTP)\nand\ncompleted\nlocal\nbinary\npatterns",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "(CLBP). A CNN model\nis\nthen trained over\nthese\ntextural",
          "as input\nfor a fully CNN to classify facial emotions.": "Fig. 1.\nThe architecture of a spatial\ntransformer module."
        },
        {
          "II. RELATED WORK": "image\nfeatures\nto\nachieve\nimproved\naccuracy\nin\ndetecting",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "facial emotions. In a recent publication by Shahzad et al. [23],",
          "as input\nfor a fully CNN to classify facial emotions.": "The key advantage of STN is their ability to learn the spatial"
        },
        {
          "II. RELATED WORK": "a zoning-based FER (ZFER) was introduced. The objective of",
          "as input\nfor a fully CNN to classify facial emotions.": "transformations\nautomatically as part of\nthe neural network"
        },
        {
          "II. RELATED WORK": "ZFER was\nto accurately identify additional\nfacial\nlandmarks",
          "as input\nfor a fully CNN to classify facial emotions.": "training process."
        },
        {
          "II. RELATED WORK": "such as eyes, eyebrows, nose,\nforehead, and mouth, enabling",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "",
          "as input\nfor a fully CNN to classify facial emotions.": "B. ConvNeXt"
        },
        {
          "II. RELATED WORK": "a more comprehensive understanding of deep facial emotions",
          "as input\nfor a fully CNN to classify facial emotions.": ""
        },
        {
          "II. RELATED WORK": "through zoning. After\nthe initial\nsteps of\nface detection and",
          "as input\nfor a fully CNN to classify facial emotions.": "Introduced in 2022, ConvNeXt\n[9]\nis a pure convolutional"
        },
        {
          "II. RELATED WORK": "extraction of\nface\nlandmarks,\nthese\nzoning-based landmarks",
          "as input\nfor a fully CNN to classify facial emotions.": "model\nthat draws\ninspiration from Vision Transformers\n[24]."
        },
        {
          "II. RELATED WORK": "were employed to train a hybrid VGG-16 model. The resulting",
          "as input\nfor a fully CNN to classify facial emotions.": "It was designed to compete with state-of-the-art Vision Trans-"
        },
        {
          "II. RELATED WORK": "feature maps generated by the hybrid model were then utilized",
          "as input\nfor a fully CNN to classify facial emotions.": "formers while retaining the simplicity and efficiency of CNNs."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "a\nstandard ResNet\n[18], with many\nof\nthese modifications"
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "evident\nin the ConvNeXt block."
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "The ConvNeXt block,\nillustrated in Figure 2, uses\nlarger"
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "kernel-sized\nand\ndepthwise\nconvolutions,\nincreases\nthe\nnet-"
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "work width to 96 channels, and utilizes an inverted bottleneck"
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "design,\nto lower\nthe overall network floating-point operations"
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "(FLOPs) while enhancing performance."
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "ConvNeXt also replaces ReLU [25] with GELU [26] as the"
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "activation function and substituting BatchNorm (BN) [27] with"
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "Layer Normalization (LN) [28] as the normalization technique,"
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "allowing the model\nto acheive a slightly better performance."
        },
        {
          "It\nincorporates\nvarious\nenhancements\nto\nthe\narchitecture\nof": "GELU and LN are commonly used in advanced Transformers."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "Fig. 2.\nThe ConvNeXt block."
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "The authors of ConvNeXt have developed multiple versions,"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "distinguished by variations in the number of channels (C) and"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "blocks (B) within each stage. Here are the configurations:"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "C = (96, 192, 384, 768)\nB = (3, 3,\n9, 3)\nTiny"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "C = (96, 192, 384, 768)\nB = (3, 3, 27, 3)\nSmall"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "C = (128, 256, 512, 1024)\nB = (3, 3, 27, 3)\nBase"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "C = (192, 384, 768, 1536)\nB = (3, 3, 27, 3)\nLarge"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "C = (256, 512, 1024, 2048)\nB = (3, 3, 27, 3)\nXLarge"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "C. Squeeze-and-Excitation Blocks"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "Squeeze-and-Excitation (SE)\n[11]\nis a powerful\ntechnique"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "used in deep learning models to enhance the representational"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "power of CNN models. It\nintroduces a mechanism that allows"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "the network to adaptively recalibrate the channel-wise features,"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "improving\nthe model’s\ndiscriminative\ncapabilities. As\nillus-"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "trated in Figure 3,\nthe SE block consists of\ntwo fundamental"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "operations:\nsqueezing and exciting.\nIn the\nsqueezing phase,"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "global average pooling is applied to each channel of the feature"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "map (W, H, C),\nreducing its\nspatial dimensions\nto a\nsingle"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "value per channel (1, 1, C). The exciting phase follows, where"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "the squeezed values are transformed using a small set of fully"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "connected\nlayers. These\nlayers\nlearn\nchannel-wise weights,"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "capturing the interdependencies among feature channels. The"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": "resulting attention weights\nare\nthen multiplied element-wise"
        },
        {
          "GELU and LN are commonly used in advanced Transformers.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "selection of example data taken from FER2013.": "TABLE I"
        },
        {
          "selection of example data taken from FER2013.": "FER2013 DATASET SUMMARY"
        },
        {
          "selection of example data taken from FER2013.": ""
        },
        {
          "selection of example data taken from FER2013.": "Class\nTraining\nValidation\nTesting\nClass total"
        },
        {
          "selection of example data taken from FER2013.": "angry\n3995\n467\n491\n4953"
        },
        {
          "selection of example data taken from FER2013.": "disgust\n436\n56\n55\n547"
        },
        {
          "selection of example data taken from FER2013.": "fear\n4097\n496\n528\n5121"
        },
        {
          "selection of example data taken from FER2013.": "happy\n7215\n895\n879\n8989"
        },
        {
          "selection of example data taken from FER2013.": ""
        },
        {
          "selection of example data taken from FER2013.": "sad\n4830\n653\n594\n6077"
        },
        {
          "selection of example data taken from FER2013.": ""
        },
        {
          "selection of example data taken from FER2013.": "surprise\n3171\n415\n416\n4002"
        },
        {
          "selection of example data taken from FER2013.": "neutral\n4965\n607\n626\n6198"
        },
        {
          "selection of example data taken from FER2013.": ""
        },
        {
          "selection of example data taken from FER2013.": "Total\n28709\n3589\n3589\n35887"
        },
        {
          "selection of example data taken from FER2013.": ""
        },
        {
          "selection of example data taken from FER2013.": ""
        },
        {
          "selection of example data taken from FER2013.": "B. Training"
        },
        {
          "selection of example data taken from FER2013.": "Recent studies have highlighted the effectiveness of modern"
        },
        {
          "selection of example data taken from FER2013.": "training techniques in significantly improving the performance"
        },
        {
          "selection of example data taken from FER2013.": "of deep learning models.\nIn our model\ntraining, we employ"
        },
        {
          "selection of example data taken from FER2013.": "various advanced strategies to improve the results. We utilize"
        },
        {
          "selection of example data taken from FER2013.": "the AdamW optimizer [30] with a learning rate of 1e-4, com-"
        },
        {
          "selection of example data taken from FER2013.": "bined with a cosine decay schedule to enhance convergence."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "Segmentation VGG-19 [19]\n75.97"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "EmoNeXt-XLarge\n76.12"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "racy of 74.33%. This achievement surpasses advanced archi-"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "tectures\nlike\nthe Residual Masking Network\n(74.14%)\nand"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "LHC-NetC (74.28%), as well as the last\ntwo sizes of the orig-"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "inal ConvNext\n(Large and XLarge). These results underscore"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "EmoNeXt-Small’s\nexceptional\nability\nto\neffectively\ncapture"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "and classify facial emotions."
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "EmoNeXt-Base maintains\nits\nperformance\nby\nachieving"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "an\naccuracy\nof\n74.91%,\nsurpassing models\nlike LHC-NetC"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "(74.28%). On the other hand, EmoNeXt-Large\nachieves\nan"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "accuracy of 75.57%, outperforming the combination of CNN"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "and BOVW models (75.42%)."
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "Notably,\nEmoNeXt-XLarge\nachieves\nan\naccuracy\nof"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "76.12%,\nsurpassing the current best\nstate-of-the-art accuracy"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "attained by the Segmentation VGG-19 model\n(75.97%). This"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "remarkable performance firmly establishes EmoNeXt-XLarge"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "as\na\nhighly\neffective model\nfor\nimage\nclassification\ntasks,"
        },
        {
          "EmoNeXt-Large\n75.57": ""
        },
        {
          "EmoNeXt-Large\n75.57": "specifically for\nfacial emotion recognition (FER)."
        },
        {
          "EmoNeXt-Large\n75.57": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": "PERFORMANCE COMPARISON ON FER2013 TEST SET"
        },
        {
          "TABLE II": "Model\nAccuracy (%)"
        },
        {
          "TABLE II": "GoogleNet\n[36]\n65.20"
        },
        {
          "TABLE II": "Deep Emotion [37]\n70.02"
        },
        {
          "TABLE II": "Inception [38]\n71.60"
        },
        {
          "TABLE II": "ConvNeXt-Tiny\n71.99"
        },
        {
          "TABLE II": "Ad-Corre [16]\n72.03"
        },
        {
          "TABLE II": "ConvNeXt-Small\n72.34"
        },
        {
          "TABLE II": "SE-Net50 [39]\n72.50"
        },
        {
          "TABLE II": "ResNet50 [39]\n73.20"
        },
        {
          "TABLE II": "ConvNeXt-Base\n73.22"
        },
        {
          "TABLE II": "VGG [38]\n73.28"
        },
        {
          "TABLE II": "EmoNeXt-Tiny\n73.34"
        },
        {
          "TABLE II": "ConvNeXt-Large\n73.46"
        },
        {
          "TABLE II": "Residual Masking Network [40]\n74.14"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "ConvNeXt-XLarge\n74.15"
        },
        {
          "TABLE II": "LHC-NetC [15]\n74.28"
        },
        {
          "TABLE II": "EmoNeXt-Small\n74.33"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "LHC-Net\n[15]\n74.42"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "EmoNeXt-Base\n74.91"
        },
        {
          "TABLE II": "CNNs + BOVW [14]\n75.42"
        },
        {
          "TABLE II": "EmoNeXt-Large\n75.57"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Segmentation VGG-19 [19]\n75.97"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "EmoNeXt-XLarge\n76.12"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "racy of 74.33%. This achievement surpasses advanced archi-"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "tectures\nlike\nthe Residual Masking Network\n(74.14%)\nand"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "LHC-NetC (74.28%), as well as the last\ntwo sizes of the orig-"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "inal ConvNext\n(Large and XLarge). These results underscore"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "EmoNeXt-Small’s\nexceptional\nability\nto\neffectively\ncapture"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "and classify facial emotions."
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "EmoNeXt-Base maintains\nits\nperformance\nby\nachieving"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "an\naccuracy\nof\n74.91%,\nsurpassing models\nlike LHC-NetC"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "(74.28%). On the other hand, EmoNeXt-Large\nachieves\nan"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "accuracy of 75.57%, outperforming the combination of CNN"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "and BOVW models (75.42%)."
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Notably,\nEmoNeXt-XLarge\nachieves\nan\naccuracy\nof"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "76.12%,\nsurpassing the current best\nstate-of-the-art accuracy"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "attained by the Segmentation VGG-19 model\n(75.97%). This"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "remarkable performance firmly establishes EmoNeXt-XLarge"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "as\na\nhighly\neffective model\nfor\nimage\nclassification\ntasks,"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "specifically for\nfacial emotion recognition (FER)."
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "V. CONCLUSION"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "In this paper, we presented EmoNeXt, a novel deep learning"
        },
        {
          "TABLE II": "framework\nfor\nfacial\nexpression\nrecognition\nbased\non\nan"
        },
        {
          "TABLE II": "adapted ConvNeXt architecture network. The EmoNeXt model"
        },
        {
          "TABLE II": "integrates a Spatial Transformer Network (STN), Squeeze-and-"
        },
        {
          "TABLE II": "Excitation blocks, and self-attention regularization to capture"
        },
        {
          "TABLE II": "rich facial\nfeatures and improve emotion classification accu-"
        },
        {
          "TABLE II": "racy. Experimental\nresults on the FER2013 dataset demon-"
        },
        {
          "TABLE II": "strate the superiority of EmoNeXt over existing state-of-the-art"
        },
        {
          "TABLE II": "models. The integration of STN, SE blocks, and self-attention"
        },
        {
          "TABLE II": "provides\nrobust and accurate facial emotion detection, mak-"
        },
        {
          "TABLE II": "ing EmoNeXt a promising approach for various applications"
        },
        {
          "TABLE II": "requiring emotion recognition."
        },
        {
          "TABLE II": "A detailed study is underway and will be published in a"
        },
        {
          "TABLE II": "forthcoming journal paper.\nIn this upcoming publication, our"
        },
        {
          "TABLE II": "objective\nis\nto conduct\na\nthorough evaluation of our model"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "the IEEE conference on computer vision and\ntions,” in Proceedings of"
        },
        {
          "and compare its performance with other models, using diverse": "FER databases.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "pattern recognition, 2017, pp. 1251–1258."
        },
        {
          "and compare its performance with other models, using diverse": "Furthermore, we have future plans to extend the application",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image"
        },
        {
          "and compare its performance with other models, using diverse": "of our model\nto emotion recognition in elderly Alzheimer’s",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "the IEEE conference on computer vision\nrecognition,” in Proceedings of"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "and pattern recognition, 2016, pp. 770–778."
        },
        {
          "and compare its performance with other models, using diverse": "patients, highlighting its potential\nfor\nreal-world impact and",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[19]\nS. Vignesh, M. Savithadevi, M. Sridevi, and R. Sridhar, “A novel facial"
        },
        {
          "and compare its performance with other models, using diverse": "further\nresearch avenues.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "emotion\nrecognition model\nusing\nsegmentation\nvgg-19\narchitecture,”"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "International Journal of\nInformation Technology, pp. 1–11, 2023."
        },
        {
          "and compare its performance with other models, using diverse": "ACKNOWLEDGMENT",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[20] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Image Computing\nfor\nbiomedical\nimage\nsegmentation,”\nin Medical"
        },
        {
          "and compare its performance with other models, using diverse": "This work was funded by the Dijon Metropole under con-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "and Computer-Assisted Intervention–MICCAI 2015: 18th International"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III"
        },
        {
          "and compare its performance with other models, using diverse": "tract DEVECO DM2023-029-20230301. The\nauthors would",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "18.\nSpringer, 2015, pp. 234–241."
        },
        {
          "and compare its performance with other models, using diverse": "like to thank Dr. Imad SFEIR from VYV3 Bourgogne for the",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[21] K. Simonyan and A. Zisserman, “Very deep convolutional networks for"
        },
        {
          "and compare its performance with other models, using diverse": "helpful discussions.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014."
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[22] M. Mukhopadhyay, A. Dey,\nand S. Kahali,\n“A deep-learning-based"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "facial\nexpression recognition method using textural\nfeatures,” Neural"
        },
        {
          "and compare its performance with other models, using diverse": "REFERENCES",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Computing and Applications, vol. 35, no. 9, pp. 6499–6514, 2023."
        },
        {
          "and compare its performance with other models, using diverse": "[1] A. Mehrabian, “Some referents and measures of nonverbal behavior,”",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[23]\nT. Shahzad, K.\nIqbal, M. A. Khan, N.\nIqbal\net al.,\n“Role of\nzoning"
        },
        {
          "and compare its performance with other models, using diverse": "Behavior Research Methods & Instrumentation, vol. 1, no. 6, pp. 203–",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "in facial\nexpression using deep learning,”\nIEEE Access, vol. 11, pp."
        },
        {
          "and compare its performance with other models, using diverse": "207, 1968.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "16 493–16 508, 2023."
        },
        {
          "and compare its performance with other models, using diverse": "[2] G. Cortellessa, R. De Benedictis, F. Fracasso, A. Orlandini, A. Umbrico,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,"
        },
        {
          "and compare its performance with other models, using diverse": "and A. Cesta, “Ai and robotics to help older adults: Revisiting projects",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,"
        },
        {
          "and compare its performance with other models, using diverse": "in search of\nlessons learned,” Paladyn, Journal of Behavioral Robotics,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "“An image is worth 16x16 words: Transformers for\nimage recognition"
        },
        {
          "and compare its performance with other models, using diverse": "vol. 12, no. 1, pp. 356–378, 2021.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "at scale,” arXiv preprint arXiv:2010.11929, 2020."
        },
        {
          "and compare its performance with other models, using diverse": "[3] M. Sajjad, M. Nasir, F. U. M. Ullah, K. Muhammad, A. K. Sangaiah,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[25] V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltz-"
        },
        {
          "and compare its performance with other models, using diverse": "and S. W. Baik,\n“Raspberry pi\nassisted facial\nexpression recognition",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "the 27th international conference on\nmann machines,” in Proceedings of"
        },
        {
          "and compare its performance with other models, using diverse": "framework for smart security in law-enforcement services,” Information",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "machine learning (ICML-10), 2010, pp. 807–814."
        },
        {
          "and compare its performance with other models, using diverse": "Sciences, vol. 479, pp. 416–431, 2019.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[26] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv"
        },
        {
          "and compare its performance with other models, using diverse": "[4] M. Jeong and B. C. Ko, “Driver’s facial expression recognition in real-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "preprint arXiv:1606.08415, 2016."
        },
        {
          "and compare its performance with other models, using diverse": "time for safe driving,” Sensors, vol. 18, no. 12, p. 4270, 2018.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[27]\nS.\nIoffe,\n“Batch renormalization: Towards\nreducing minibatch depen-"
        },
        {
          "and compare its performance with other models, using diverse": "[5] D. Ghimire,\nJ. Lee, Z.-N. Li,\nand S.\nJeong,\n“Recognition\nof\nfacial",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "in\nneural\ninformation\ndence\nin\nbatch-normalized models,” Advances"
        },
        {
          "and compare its performance with other models, using diverse": "expressions\nbased\non\nsalient\ngeometric\nfeatures\nand\nsupport\nvector",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "processing systems, vol. 30, 2017."
        },
        {
          "and compare its performance with other models, using diverse": "machines,” Multimedia Tools and Applications, vol. 76, pp. 7921–7946,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[28]\nJ. L. Ba,\nJ. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv"
        },
        {
          "and compare its performance with other models, using diverse": "2017.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "preprint arXiv:1607.06450, 2016."
        },
        {
          "and compare its performance with other models, using diverse": "[6]\nI. Kotsia and I. Pitas, “Facial expression recognition in image sequences",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,"
        },
        {
          "and compare its performance with other models, using diverse": "using\ngeometric\ndeformation\nfeatures\nand\nsupport\nvector machines,”",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "in\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances"
        },
        {
          "and compare its performance with other models, using diverse": "IEEE transactions on image processing, vol. 16, no. 1, pp. 172–187,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "neural\ninformation processing systems, vol. 30, 2017."
        },
        {
          "and compare its performance with other models, using diverse": "2006.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[30]\nI. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”"
        },
        {
          "and compare its performance with other models, using diverse": "[7] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recognition",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "arXiv preprint arXiv:1711.05101, 2017."
        },
        {
          "and compare its performance with other models, using diverse": "Image\nand\nbased\non\nlocal\nbinary\npatterns: A comprehensive\nstudy,”",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[31] G. Huang, Y. Sun, Z. Liu, D. Sedra,\nand K. Q. Weinberger,\n“Deep"
        },
        {
          "and compare its performance with other models, using diverse": "vision Computing, vol. 27, no. 6, pp. 803–816, 2009.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "networks with stochastic depth,” in Computer Vision–ECCV 2016: 14th"
        },
        {
          "and compare its performance with other models, using diverse": "[8]\nJ. Chen, Z. Chen, Z. Chi, H. Fu et al., “Facial expression recognition",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "European Conference, Amsterdam, The Netherlands, October\n11–14,"
        },
        {
          "and compare its performance with other models, using diverse": "based on facial components detection and hog features,” in International",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "2016, Proceedings, Part\nIV 14.\nSpringer, 2016, pp. 646–661."
        },
        {
          "and compare its performance with other models, using diverse": "workshops on electrical and computer engineering subfields, 2014, pp.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[32] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking"
        },
        {
          "and compare its performance with other models, using diverse": "884–888.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "the\nthe inception architecture for computer vision,” in Proceedings of"
        },
        {
          "and compare its performance with other models, using diverse": "[9]\nZ. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "IEEE conference on computer vision and pattern recognition, 2016, pp."
        },
        {
          "and compare its performance with other models, using diverse": "the IEEE/CVF Conference on\nconvnet for the 2020s,” in Proceedings of",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "2818–2826."
        },
        {
          "and compare its performance with other models, using diverse": "Computer Vision and Pattern Recognition, 2022, pp. 11 976–11 986.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[33] B. T. Polyak and A. B. Juditsky, “Acceleration of stochastic approxima-"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "tion by averaging,” SIAM journal on control and optimization, vol. 30,"
        },
        {
          "and compare its performance with other models, using diverse": "[10] M.\nJaderberg, K. Simonyan, A. Zisserman et al., “Spatial\ntransformer",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "no. 4, pp. 838–855, 1992."
        },
        {
          "and compare its performance with other models, using diverse": "networks,” Advances in neural\ninformation processing systems, vol. 28,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[34]\nP. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,"
        },
        {
          "and compare its performance with other models, using diverse": "2015.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al.,\n“Mixed"
        },
        {
          "and compare its performance with other models, using diverse": "[11]\nJ. Hu, L. Shen,\nand G. Sun,\n“Squeeze-and-excitation\nnetworks,”\nin",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "precision training,” arXiv preprint arXiv:1710.03740, 2017."
        },
        {
          "and compare its performance with other models, using diverse": "Proceedings of\nthe\nIEEE conference on computer\nvision and pattern",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[35] O. Russakovsky,\nJ. Deng, H.\nSu,\nJ. Krause,\nS.\nSatheesh,\nS. Ma,"
        },
        {
          "and compare its performance with other models, using diverse": "recognition, 2018, pp. 7132–7141.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet\nlarge"
        },
        {
          "and compare its performance with other models, using diverse": "[12]\nZ. Zhang and M. Sabuncu, “Generalized cross entropy loss for\ntraining",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "journal of computer\nscale visual\nrecognition challenge,” International"
        },
        {
          "and compare its performance with other models, using diverse": "deep neural networks with noisy labels,” Advances in neural information",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "vision, vol. 115, pp. 211–252, 2015."
        },
        {
          "and compare its performance with other models, using diverse": "processing systems, vol. 31, 2018.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[36]\nP. Giannopoulos,\nI. Perikos,\nand\nI. Hatzilygeroudis,\n“Deep\nlearning"
        },
        {
          "and compare its performance with other models, using diverse": "[13]\nI.\nJ. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "approaches\nfor\nfacial emotion recognition: A case study on fer-2013,”"
        },
        {
          "and compare its performance with other models, using diverse": "B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee et al., “Chal-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Advances in Hybridization of Intelligent Methods: Models, Systems and"
        },
        {
          "and compare its performance with other models, using diverse": "lenges\nin representation learning: A report on three machine learning",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Applications, pp. 1–16, 2018."
        },
        {
          "and compare its performance with other models, using diverse": "contests,” in Neural Information Processing: 20th International Confer-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[37]\nS. Minaee, M. Minaei, and A. Abdolrashidi, “Deep-emotion: Facial ex-"
        },
        {
          "and compare its performance with other models, using diverse": "ence,\nICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "pression recognition using attentional convolutional network,” Sensors,"
        },
        {
          "and compare its performance with other models, using diverse": "Part\nIII 20.\nSpringer, 2013, pp. 117–124.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "vol. 21, no. 9, p. 3046, 2021."
        },
        {
          "and compare its performance with other models, using diverse": "[14] M.-I. Georgescu, R. T.\nIonescu, and M. Popescu, “Local\nlearning with",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[38] Y. Khaireddin and Z. Chen, “Facial emotion recognition: State of\nthe"
        },
        {
          "and compare its performance with other models, using diverse": "deep and handcrafted features for\nfacial expression recognition,” IEEE",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "art performance on fer2013,” arXiv preprint arXiv:2105.03588, 2021."
        },
        {
          "and compare its performance with other models, using diverse": "Access, vol. 7, pp. 64 827–64 836, 2019.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[39] A. Khanzada, C. Bai, and F. T. Celepcikay, “Facial expression recogni-"
        },
        {
          "and compare its performance with other models, using diverse": "[15] R. Pecoraro, V. Basile, and V. Bono, “Local multi-head channel\nself-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "tion with deep learning,” arXiv preprint arXiv:2004.11823, 2020."
        },
        {
          "and compare its performance with other models, using diverse": "attention for\nfacial expression recognition,” Information, vol. 13, no. 9,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[40]\nL. Pham, T. H. Vu, and T. A. Tran, “Facial expression recognition using"
        },
        {
          "and compare its performance with other models, using diverse": "p. 419, 2022.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "residual masking network,” in 2020 25Th international conference on"
        },
        {
          "and compare its performance with other models, using diverse": "[16] A. P. Fard and M. H. Mahoor, “Ad-corre: Adaptive correlation-based loss",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "pattern recognition (ICPR).\nIEEE, 2021, pp. 4513–4519."
        },
        {
          "and compare its performance with other models, using diverse": "for facial expression recognition in the wild,” IEEE Access, vol. 10, pp.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "26 756–26 768, 2022.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "the IEEE conference on computer vision and\ntions,” in Proceedings of"
        },
        {
          "and compare its performance with other models, using diverse": "FER databases.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "pattern recognition, 2017, pp. 1251–1258."
        },
        {
          "and compare its performance with other models, using diverse": "Furthermore, we have future plans to extend the application",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image"
        },
        {
          "and compare its performance with other models, using diverse": "of our model\nto emotion recognition in elderly Alzheimer’s",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "the IEEE conference on computer vision\nrecognition,” in Proceedings of"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "and pattern recognition, 2016, pp. 770–778."
        },
        {
          "and compare its performance with other models, using diverse": "patients, highlighting its potential\nfor\nreal-world impact and",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[19]\nS. Vignesh, M. Savithadevi, M. Sridevi, and R. Sridhar, “A novel facial"
        },
        {
          "and compare its performance with other models, using diverse": "further\nresearch avenues.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "emotion\nrecognition model\nusing\nsegmentation\nvgg-19\narchitecture,”"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "International Journal of\nInformation Technology, pp. 1–11, 2023."
        },
        {
          "and compare its performance with other models, using diverse": "ACKNOWLEDGMENT",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[20] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Image Computing\nfor\nbiomedical\nimage\nsegmentation,”\nin Medical"
        },
        {
          "and compare its performance with other models, using diverse": "This work was funded by the Dijon Metropole under con-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "and Computer-Assisted Intervention–MICCAI 2015: 18th International"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III"
        },
        {
          "and compare its performance with other models, using diverse": "tract DEVECO DM2023-029-20230301. The\nauthors would",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "18.\nSpringer, 2015, pp. 234–241."
        },
        {
          "and compare its performance with other models, using diverse": "like to thank Dr. Imad SFEIR from VYV3 Bourgogne for the",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[21] K. Simonyan and A. Zisserman, “Very deep convolutional networks for"
        },
        {
          "and compare its performance with other models, using diverse": "helpful discussions.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014."
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[22] M. Mukhopadhyay, A. Dey,\nand S. Kahali,\n“A deep-learning-based"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "facial\nexpression recognition method using textural\nfeatures,” Neural"
        },
        {
          "and compare its performance with other models, using diverse": "REFERENCES",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Computing and Applications, vol. 35, no. 9, pp. 6499–6514, 2023."
        },
        {
          "and compare its performance with other models, using diverse": "[1] A. Mehrabian, “Some referents and measures of nonverbal behavior,”",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[23]\nT. Shahzad, K.\nIqbal, M. A. Khan, N.\nIqbal\net al.,\n“Role of\nzoning"
        },
        {
          "and compare its performance with other models, using diverse": "Behavior Research Methods & Instrumentation, vol. 1, no. 6, pp. 203–",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "in facial\nexpression using deep learning,”\nIEEE Access, vol. 11, pp."
        },
        {
          "and compare its performance with other models, using diverse": "207, 1968.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "16 493–16 508, 2023."
        },
        {
          "and compare its performance with other models, using diverse": "[2] G. Cortellessa, R. De Benedictis, F. Fracasso, A. Orlandini, A. Umbrico,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,"
        },
        {
          "and compare its performance with other models, using diverse": "and A. Cesta, “Ai and robotics to help older adults: Revisiting projects",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,"
        },
        {
          "and compare its performance with other models, using diverse": "in search of\nlessons learned,” Paladyn, Journal of Behavioral Robotics,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "“An image is worth 16x16 words: Transformers for\nimage recognition"
        },
        {
          "and compare its performance with other models, using diverse": "vol. 12, no. 1, pp. 356–378, 2021.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "at scale,” arXiv preprint arXiv:2010.11929, 2020."
        },
        {
          "and compare its performance with other models, using diverse": "[3] M. Sajjad, M. Nasir, F. U. M. Ullah, K. Muhammad, A. K. Sangaiah,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[25] V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltz-"
        },
        {
          "and compare its performance with other models, using diverse": "and S. W. Baik,\n“Raspberry pi\nassisted facial\nexpression recognition",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "the 27th international conference on\nmann machines,” in Proceedings of"
        },
        {
          "and compare its performance with other models, using diverse": "framework for smart security in law-enforcement services,” Information",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "machine learning (ICML-10), 2010, pp. 807–814."
        },
        {
          "and compare its performance with other models, using diverse": "Sciences, vol. 479, pp. 416–431, 2019.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[26] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv"
        },
        {
          "and compare its performance with other models, using diverse": "[4] M. Jeong and B. C. Ko, “Driver’s facial expression recognition in real-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "preprint arXiv:1606.08415, 2016."
        },
        {
          "and compare its performance with other models, using diverse": "time for safe driving,” Sensors, vol. 18, no. 12, p. 4270, 2018.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[27]\nS.\nIoffe,\n“Batch renormalization: Towards\nreducing minibatch depen-"
        },
        {
          "and compare its performance with other models, using diverse": "[5] D. Ghimire,\nJ. Lee, Z.-N. Li,\nand S.\nJeong,\n“Recognition\nof\nfacial",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "in\nneural\ninformation\ndence\nin\nbatch-normalized models,” Advances"
        },
        {
          "and compare its performance with other models, using diverse": "expressions\nbased\non\nsalient\ngeometric\nfeatures\nand\nsupport\nvector",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "processing systems, vol. 30, 2017."
        },
        {
          "and compare its performance with other models, using diverse": "machines,” Multimedia Tools and Applications, vol. 76, pp. 7921–7946,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[28]\nJ. L. Ba,\nJ. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv"
        },
        {
          "and compare its performance with other models, using diverse": "2017.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "preprint arXiv:1607.06450, 2016."
        },
        {
          "and compare its performance with other models, using diverse": "[6]\nI. Kotsia and I. Pitas, “Facial expression recognition in image sequences",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,"
        },
        {
          "and compare its performance with other models, using diverse": "using\ngeometric\ndeformation\nfeatures\nand\nsupport\nvector machines,”",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "in\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances"
        },
        {
          "and compare its performance with other models, using diverse": "IEEE transactions on image processing, vol. 16, no. 1, pp. 172–187,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "neural\ninformation processing systems, vol. 30, 2017."
        },
        {
          "and compare its performance with other models, using diverse": "2006.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[30]\nI. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”"
        },
        {
          "and compare its performance with other models, using diverse": "[7] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recognition",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "arXiv preprint arXiv:1711.05101, 2017."
        },
        {
          "and compare its performance with other models, using diverse": "Image\nand\nbased\non\nlocal\nbinary\npatterns: A comprehensive\nstudy,”",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[31] G. Huang, Y. Sun, Z. Liu, D. Sedra,\nand K. Q. Weinberger,\n“Deep"
        },
        {
          "and compare its performance with other models, using diverse": "vision Computing, vol. 27, no. 6, pp. 803–816, 2009.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "networks with stochastic depth,” in Computer Vision–ECCV 2016: 14th"
        },
        {
          "and compare its performance with other models, using diverse": "[8]\nJ. Chen, Z. Chen, Z. Chi, H. Fu et al., “Facial expression recognition",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "European Conference, Amsterdam, The Netherlands, October\n11–14,"
        },
        {
          "and compare its performance with other models, using diverse": "based on facial components detection and hog features,” in International",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "2016, Proceedings, Part\nIV 14.\nSpringer, 2016, pp. 646–661."
        },
        {
          "and compare its performance with other models, using diverse": "workshops on electrical and computer engineering subfields, 2014, pp.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[32] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking"
        },
        {
          "and compare its performance with other models, using diverse": "884–888.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "the\nthe inception architecture for computer vision,” in Proceedings of"
        },
        {
          "and compare its performance with other models, using diverse": "[9]\nZ. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "IEEE conference on computer vision and pattern recognition, 2016, pp."
        },
        {
          "and compare its performance with other models, using diverse": "the IEEE/CVF Conference on\nconvnet for the 2020s,” in Proceedings of",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "2818–2826."
        },
        {
          "and compare its performance with other models, using diverse": "Computer Vision and Pattern Recognition, 2022, pp. 11 976–11 986.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[33] B. T. Polyak and A. B. Juditsky, “Acceleration of stochastic approxima-"
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "tion by averaging,” SIAM journal on control and optimization, vol. 30,"
        },
        {
          "and compare its performance with other models, using diverse": "[10] M.\nJaderberg, K. Simonyan, A. Zisserman et al., “Spatial\ntransformer",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "no. 4, pp. 838–855, 1992."
        },
        {
          "and compare its performance with other models, using diverse": "networks,” Advances in neural\ninformation processing systems, vol. 28,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[34]\nP. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,"
        },
        {
          "and compare its performance with other models, using diverse": "2015.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al.,\n“Mixed"
        },
        {
          "and compare its performance with other models, using diverse": "[11]\nJ. Hu, L. Shen,\nand G. Sun,\n“Squeeze-and-excitation\nnetworks,”\nin",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "precision training,” arXiv preprint arXiv:1710.03740, 2017."
        },
        {
          "and compare its performance with other models, using diverse": "Proceedings of\nthe\nIEEE conference on computer\nvision and pattern",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[35] O. Russakovsky,\nJ. Deng, H.\nSu,\nJ. Krause,\nS.\nSatheesh,\nS. Ma,"
        },
        {
          "and compare its performance with other models, using diverse": "recognition, 2018, pp. 7132–7141.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet\nlarge"
        },
        {
          "and compare its performance with other models, using diverse": "[12]\nZ. Zhang and M. Sabuncu, “Generalized cross entropy loss for\ntraining",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "journal of computer\nscale visual\nrecognition challenge,” International"
        },
        {
          "and compare its performance with other models, using diverse": "deep neural networks with noisy labels,” Advances in neural information",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "vision, vol. 115, pp. 211–252, 2015."
        },
        {
          "and compare its performance with other models, using diverse": "processing systems, vol. 31, 2018.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[36]\nP. Giannopoulos,\nI. Perikos,\nand\nI. Hatzilygeroudis,\n“Deep\nlearning"
        },
        {
          "and compare its performance with other models, using diverse": "[13]\nI.\nJ. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "approaches\nfor\nfacial emotion recognition: A case study on fer-2013,”"
        },
        {
          "and compare its performance with other models, using diverse": "B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee et al., “Chal-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Advances in Hybridization of Intelligent Methods: Models, Systems and"
        },
        {
          "and compare its performance with other models, using diverse": "lenges\nin representation learning: A report on three machine learning",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "Applications, pp. 1–16, 2018."
        },
        {
          "and compare its performance with other models, using diverse": "contests,” in Neural Information Processing: 20th International Confer-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[37]\nS. Minaee, M. Minaei, and A. Abdolrashidi, “Deep-emotion: Facial ex-"
        },
        {
          "and compare its performance with other models, using diverse": "ence,\nICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "pression recognition using attentional convolutional network,” Sensors,"
        },
        {
          "and compare its performance with other models, using diverse": "Part\nIII 20.\nSpringer, 2013, pp. 117–124.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "vol. 21, no. 9, p. 3046, 2021."
        },
        {
          "and compare its performance with other models, using diverse": "[14] M.-I. Georgescu, R. T.\nIonescu, and M. Popescu, “Local\nlearning with",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[38] Y. Khaireddin and Z. Chen, “Facial emotion recognition: State of\nthe"
        },
        {
          "and compare its performance with other models, using diverse": "deep and handcrafted features for\nfacial expression recognition,” IEEE",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "art performance on fer2013,” arXiv preprint arXiv:2105.03588, 2021."
        },
        {
          "and compare its performance with other models, using diverse": "Access, vol. 7, pp. 64 827–64 836, 2019.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[39] A. Khanzada, C. Bai, and F. T. Celepcikay, “Facial expression recogni-"
        },
        {
          "and compare its performance with other models, using diverse": "[15] R. Pecoraro, V. Basile, and V. Bono, “Local multi-head channel\nself-",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "tion with deep learning,” arXiv preprint arXiv:2004.11823, 2020."
        },
        {
          "and compare its performance with other models, using diverse": "attention for\nfacial expression recognition,” Information, vol. 13, no. 9,",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "[40]\nL. Pham, T. H. Vu, and T. A. Tran, “Facial expression recognition using"
        },
        {
          "and compare its performance with other models, using diverse": "p. 419, 2022.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "residual masking network,” in 2020 25Th international conference on"
        },
        {
          "and compare its performance with other models, using diverse": "[16] A. P. Fard and M. H. Mahoor, “Ad-corre: Adaptive correlation-based loss",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": "pattern recognition (ICPR).\nIEEE, 2021, pp. 4513–4519."
        },
        {
          "and compare its performance with other models, using diverse": "for facial expression recognition in the wild,” IEEE Access, vol. 10, pp.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        },
        {
          "and compare its performance with other models, using diverse": "26 756–26 768, 2022.",
          "[17]\nF. Chollet, “Xception: Deep learning with depthwise separable convolu-": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Some referents and measures of nonverbal behavior",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1968",
      "venue": "Behavior Research Methods & Instrumentation"
    },
    {
      "citation_id": "2",
      "title": "Ai and robotics to help older adults: Revisiting projects in search of lessons learned",
      "authors": [
        "G Cortellessa",
        "R Benedictis",
        "F Fracasso",
        "A Orlandini",
        "A Umbrico",
        "A Cesta"
      ],
      "year": "2021",
      "venue": "Paladyn, Journal of Behavioral Robotics"
    },
    {
      "citation_id": "3",
      "title": "Raspberry pi assisted facial expression recognition framework for smart security in law-enforcement services",
      "authors": [
        "M Sajjad",
        "M Nasir",
        "F Ullah",
        "K Muhammad",
        "A Sangaiah",
        "S Baik"
      ],
      "year": "2019",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "4",
      "title": "Driver's facial expression recognition in realtime for safe driving",
      "authors": [
        "M Jeong",
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "5",
      "title": "Recognition of facial expressions based on salient geometric features and support vector machines",
      "authors": [
        "D Ghimire",
        "J Lee",
        "Z.-N Li",
        "S Jeong"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "6",
      "title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines",
      "authors": [
        "I Kotsia",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "IEEE transactions on image processing"
    },
    {
      "citation_id": "7",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and vision Computing"
    },
    {
      "citation_id": "8",
      "title": "Facial expression recognition based on facial components detection and hog features",
      "authors": [
        "J Chen",
        "Z Chen",
        "Z Chi",
        "H Fu"
      ],
      "year": "2014",
      "venue": "International workshops on electrical and computer engineering subfields"
    },
    {
      "citation_id": "9",
      "title": "A convnet for the 2020s",
      "authors": [
        "Z Liu",
        "H Mao",
        "C.-Y Wu",
        "C Feichtenhofer",
        "T Darrell",
        "S Xie"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Spatial transformer networks",
      "authors": [
        "M Jaderberg",
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "11",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
      "authors": [
        "Z Zhang",
        "M Sabuncu"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "14",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Local multi-head channel selfattention for facial expression recognition",
      "authors": [
        "R Pecoraro",
        "V Basile",
        "V Bono"
      ],
      "year": "2022",
      "venue": "Information"
    },
    {
      "citation_id": "16",
      "title": "Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild",
      "authors": [
        "A Fard",
        "M Mahoor"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "A novel facial emotion recognition model using segmentation vgg-19 architecture",
      "authors": [
        "S Vignesh",
        "M Savithadevi",
        "M Sridevi",
        "R Sridhar"
      ],
      "year": "2023",
      "venue": "International Journal of Information Technology"
    },
    {
      "citation_id": "20",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2015",
      "venue": "Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference"
    },
    {
      "citation_id": "21",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "22",
      "title": "A deep-learning-based facial expression recognition method using textural features",
      "authors": [
        "M Mukhopadhyay",
        "A Dey",
        "S Kahali"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "23",
      "title": "Role of zoning in facial expression using deep learning",
      "authors": [
        "T Shahzad",
        "K Iqbal",
        "M Khan"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "25",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "V Nair",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Proceedings of the 27th international conference on machine learning (ICML-10)"
    },
    {
      "citation_id": "26",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "27",
      "title": "Batch renormalization: Towards reducing minibatch dependence in batch-normalized models",
      "authors": [
        "S Ioffe"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "31",
      "title": "Deep networks with stochastic depth",
      "authors": [
        "G Huang",
        "Y Sun",
        "Z Liu",
        "D Sedra",
        "K Weinberger"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference"
    },
    {
      "citation_id": "32",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "33",
      "title": "Acceleration of stochastic approximation by averaging",
      "authors": [
        "B Polyak",
        "A Juditsky"
      ],
      "year": "1992",
      "venue": "SIAM journal on control and optimization"
    },
    {
      "citation_id": "34",
      "title": "Mixed precision training",
      "authors": [
        "P Micikevicius",
        "S Narang",
        "J Alben",
        "G Diamos",
        "E Elsen",
        "D Garcia",
        "B Ginsburg",
        "M Houston",
        "O Kuchaiev",
        "G Venkatesh"
      ],
      "year": "2017",
      "venue": "Mixed precision training",
      "arxiv": "arXiv:1710.03740"
    },
    {
      "citation_id": "35",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "36",
      "title": "Deep learning approaches for facial emotion recognition: A case study on fer-2013",
      "authors": [
        "P Giannopoulos",
        "I Perikos",
        "I Hatzilygeroudis"
      ],
      "year": "2018",
      "venue": "Advances in Hybridization of Intelligent Methods: Models, Systems and Applications"
    },
    {
      "citation_id": "37",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "M Minaei",
        "A Abdolrashidi"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "38",
      "title": "Facial emotion recognition: State of the art performance on fer2013",
      "authors": [
        "Y Khaireddin",
        "Z Chen"
      ],
      "year": "2021",
      "venue": "Facial emotion recognition: State of the art performance on fer2013",
      "arxiv": "arXiv:2105.03588"
    },
    {
      "citation_id": "39",
      "title": "Facial expression recognition with deep learning",
      "authors": [
        "A Khanzada",
        "C Bai",
        "F Celepcikay"
      ],
      "year": "2020",
      "venue": "Facial expression recognition with deep learning",
      "arxiv": "arXiv:2004.11823"
    },
    {
      "citation_id": "40",
      "title": "Facial expression recognition using residual masking network",
      "authors": [
        "L Pham",
        "T Vu",
        "T Tran"
      ],
      "year": "2021",
      "venue": "2020 25Th international conference on pattern recognition (ICPR)"
    }
  ]
}