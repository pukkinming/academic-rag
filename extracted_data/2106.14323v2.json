{
  "paper_id": "2106.14323v2",
  "title": "List Of Algorithms",
  "published": "2021-06-27T21:41:08Z",
  "authors": [
    "Nathalie Deziderio",
    "Hugo Tremonte de Carvalho"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Refletindo sobre todo o trabalho realizado e todas as pessoas que acompanharam minha trajetória até aqui, me vejo na privilegiada posição de ter tantas pessoas a agradecer pelo apoio que me deram ao longo do caminho. Pessoas essas que viram o bom, o mau e o feio na minha vida, sem nunca deixar que nossos laços se desfizessem. Todos os dias escolhemos fazer parte das histórias uns dos outros, nos acolhemos, nos valorizamos e aprendemos a abraçar completamente as peças que nos constituem. Serei eternamente grata pelos elos que me conectam a essas pessoas e pelo nosso esforço em preservá-los. Não poderia começar sem agradecer à minha mãe, Marise Toffani, que sempre esteve ao meu lado e fez de tudo para que eu tivesse a melhor educação possível. Muito além disso, me deu apoio emocional em todos os momentos difíceis e me ouviu falar incessantemente sobre essa pesquisa mesmo detestando matemática com todas as forças. Agradeço muito ao Aluisio, Cristina e Bárbara Mariante por serem pessoas tão amorosas e especiais na minha vida. Vocês são minha segunda família e têm meu eterno carinho. Desde o início da minha vida acadêmica também tive a sorte de poder contar com tantos professores que ajudaram a me tornar quem sou. Meus queridos professores J. J., Cataldo e Felipe Ferreira, cujo carinho e didática impecável cultivaram meu amor pela Matemática durante o Ensino Médio. Ao meu Professor de Língua Portuguesa, Bernardo Miller, gostaria de agradecer pelos nossos diálogos sobre poesia e literatura que ainda enriquecem minha vida. Com certeza a qualidade desse texto está refletida nas suas rigorosíssimas aulas de produção escrita. Finalmente, gostaria de agradecer ao meu professor de teatro Rodrigo Gaia, que ajudou a desenvolver minha voz e habilidade de me fazer ser ouvida. Ao longo da minha trajetória, também pude contar com amigos que espero levar para o resto da minha vida. Pessoas que me acompanharam desde os meus anos formativos, com quem posso dividir qualquer experiência e torná-la especial.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Resumo",
      "text": "Esse trabalho foi criado com o objetivo de aplicar técnicas de Estatística ao campo de Reconhecimento de Emoções em Música, uma área bem conhecida dentro do mundo de Processamento de sinais mas ainda pouco explorada do ponto de vista estatístico. Aqui, abrimos diversas possibilidades dentro do meio, aplicando técnicas modernas de Estatística Bayesiana de maneira a criar algoritmos eficientes, focando na aplicabilidade dos resultados.\n\nApesar da motivação deste projeto ser a criação de um sistema de recomendação de músicas baseado em emoção, a principal contribuição aqui desenvolvida é um modelo multivariado altamente adaptável e pode ser útil para interpretar qualquer conjunto de dados onde se deseje aplicar regularização de maneira eficiente. De maneira geral, vamos explorar o papel que uma profunda análise teórica estatística pode ter na modelagem de um algoritmo que explore bem uma base de dados já conhecida e o que pode ser ganho com esse tipo de abordagem.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Chapter 1 Introduction",
      "text": "The motivation for this work emerged from a deep love for music and the desire to better understand its relations to human psychology through Statistics. This led us into the world of Music Emotion Recognition (MER)  [Panda, 2019] , a research area within Music Information Retrieval, that has been steadily growing due to its challenging and thought-provoking nature, and which we briefly introduce below.\n\nMusic is an indispensable part of human culture and it is deeply intertwined with the emotional response it causes: ancient Greeks used it to tell epic stories about great wars and journeys that brought awe and heartbreak to their listeners; the Apache people sang a war chant before striding into battle to give courage and unite their warriors; and even though Wolfgang Amadeus Mozart's motivation while composing \"Le Nozze di Figaro\" was to criticize the royals in Versailles, we laugh and cry as the melody unfolds.\n\nThere are several studies that explore the relation of music to the human brain  [Minsky, 1982 , Jourdain, 2008] , and one can also argue that nowadays it plays an even more important role in our lives than ever, since CD's, portable MP3 players and streaming services have made music broadly and easily accessible. That overload of options, however, comes at a price: we are constantly drowning in new pieces from all over the world, with new artists and genres appearing every day, so it is not an easy task to keep track of what is relevant to each person. To tackle that issue, many streaming services have employed personalized recommendation systems  [Song et al., 2012] , that study what each user listens to and offer suggestions that are similar in style, time period, artist or genre, to name a few possibilities. Considering the aforementioned relationship between music and sentiment, it could be very interesting to create a system that takes this connection in consideration.\n\nIn order to do that, we will explore several statistical methods to estimate emotions in music but before diving into it, let us set some boundaries, defining how this idea unfolds.\n\nSince \"emotion\" is a rather vague and imprecise target, the first step we need to take is coming up with more precise way to approach the subject. Exploring previous works in MER we came upon the Arousal-Valence (AV) scale, a measuring system created by psychologists in the 80's  [Russell, 1980]  thar has been widely used by the MER community.\n\nIt measures respectively how agitated a song is (Arousal) and how positive or negative are the feelings it evokes (Valence), from -1 to 1, as illustrated in Figure  1 .1. Since it is continuous rather than discrete, it more successfully addresses the fluid nature of emotions. Note that we seek to measure the sentiment a piece evokes as opposed to the one a particular listener perceives. This important distinction makes it possible to have consistent, straightforward data, independent from personal judgements that would be very difficult to predict without a great level of uncertainty. It is easy to infer that Arousal and Valence -respectively A and V from now on -are positively correlated, since agitated songs tend to evoke more positive feelings and slower songs tend to evoke more negative feelings. Even though the exact correlation between A and V may vary with each dataset being considered, it is usually not negligible and, in the dataset here employed  [Aljanaki et al., 2017] , it reaches 0.59.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Arousal",
      "text": "Ignoring this fact can simplify the modeling process quite a lot, but taking it into consideration could be valuable. In particular, since Valence is significantly harder to estimate than Arousal  [Yang et al., 2008] , this correlation could help the model provide better estimates. Although that information was obtained empirically, it is not difficult to understand why that difference exists: intuitively, agitation is reasonably objective to perceive, as it is a movement. Positivity and negativity on the other hand are much more subtle, thus harder to put a pin on.\n\nTo predict the desired responses, we will build a model that explores the relation between a song's AV-values and several physical properties of the waveform. From a myriad of avaliable pieces of data, we choose acoustic features to work with. As we will detail in Chapter 2, they are a reasonable choice in this scenario, since they reduce dimensionality by extracting important information contained within the waveform, being also easy to compute and, therefore, potentially numerous. However, in spite of their advantages, these low-level features are very lightly correlated to the response variables  [Panda, 2019] , and usual techniques will struggle from having a number of parameters potentially approaching the number of observations. To solve this particular issue, one can look for ways to select more important features in an efficient manner or calculate high-level features more relevant to the problem  [Panda, 2019] .\n\nhere, we seek to introduce more refined statistical techniques to perform feature selection automatically while estimating Arousal and Valence, and explore Bayesian inference to develop models that extract more interpretable results from the data. Apart from the theoretical improvement, we seek to produce a fast algorithm that can be easily adapted and then employed in a business setting, handling larger databases with multiple responses.\n\nMoreover, we seek to introduce the MER problem to the statistical community, not only by use of the techniques employed here but also to encourage further investigation in the field. Still, we keep in mind applicability of the results and propose a preliminary prototype for an emotion-based recommendation system with statistical foundations, that helped motivate this endeavor.\n\nAfter this brief introduction to the general basis of this work, Chapter 2 will carry a more detailed discussion on the subject of MER and literature review. Once we have a proper grasp of the field and some of its characteristics, we move on to Chapter 3 where we offer some more detail into the database being employed. Statistical techniques will begin to be introduced in Chapter 4, more specifically by the discussion and application of Bayesian linear regression to the Arousal and Valence estimation problem, and Variational\n\nInference within Automatic Relevance Determination (ARD) will be discussed in Chapter 5.\n\nNote that even though none of this approaches have been applied to the area of MER before, they are widely known in the Statistics community  [Blei et al., 2017, Casella and Berger, 2001] . In Chapter 6 we propose the Multivariate Automatic Relevance Determination (MARD), a generalization of the ARD that is able to handle multiple correlated response variables at once. Finally, in Chapter 7 we make the appropriate remarks on the entirety of the work done, as well as proposing further steps.\n\nChapter 2\n\nA Brief Overview of Music Emotion",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Recognition",
      "text": "Now that we have some notion of the Music Emotion Recognition (MER) problem itself and briefly presented some of our goals, let us walk through what has already been developed in the area in the past few decades. The first two sections of this chapter will focus on a literature review of the MER field: Section 2.1 will describe discrete approaches while Section 2.2 will account for continuous ones, more related to this work. In Section 2.3 we will briefly outline a new possibility to tackle this problem.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Discrete Approaches",
      "text": "The first attempt at estimating emotion from music was made in 1988 with a multi-label classification model  [Katayose et al., 1988] , with targets such as gloomy, urbane, pathetic, serious, hopeful and others, but the results showed low classification accuracy. Even though different databases and label sets were contemplated by researchers throughout time, mostly categorical approaches were made until 2009. The challenge of using discrete values to address the fluid nature of the emotions led to scenarios where multiple overlapping categories (18 categories containing 135 tags) were used  [Hu et al., 2009] , or a few generic ones (happy, sad, angry and fearful )  [Feng et al., 2003 ] that rather simplified the goal of identifying emotion in music. However, the idea of classification is more adequate to other MIR tasks, such as genre, artist, or time period identification, since discrete response values properly describe the desired information. Not only emotions are more fluid but they are also hard to describe or objectively box, what could cause confusion in the human annotators needed for this task and, therefore, inconsistent data or very broad categories.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Continuous Approaches",
      "text": "Ultimately, discrete approaches were mostly dropped in favor of continuous ones that resonated more with the problem, and the aforementioned AV scale  [Russell, 1980]  became more widely used by the MER community, with its first use recorded in [Yang and  Lee, 2004] . In some cases  [Leman et al., 2005] , a 3D version of the scale is used, contemplating values for Valence, Arousal (in  [Leman et al., 2005]  named Activity) and Interest or Dominance (exciting-boring).\n\nAlthough the addition of the Dominance axis expands the emotional spectre in one dimension, it also makes it a lot more complicated to understand and that can be troublesome, considering we need several humans to be capable of annotating this emotions with some level of precision.\n\nEven though this version has show some good results in  [Verma and Tiwary, 2017] ,\n\nthe new axis has a subjectiveness akin to Valence, thus increasing the difficulty in the annotation process. Again, it is important to find balance between necessary complexity to make the results realistically useful and enough simplicity to keep the annotation less confusing as possible.\n\nAlthough many different datasets have been used in literature, the data used to relate the song to the response is frequently composed by the songs' acoustic features  [Panda, 2019] , e.g. spectral centroid ( where the center of mass of the spectrum is located), range and Zero-crossing rate (instantaneous point at which there is no voltage present) of time signal among others  [Li et al., 2011] . It is worth noting that although some of them, like loudness, fundamental frequency (the most prominent frequency present in a signal in a give time), skewness (3rd order moment), kurtosis (4th order moment) and others are known to be relevant in modeling Arousal, the correlation between acoustic features and\n\nValence is less significant  [Yang et al., 2008] . This means that predicting both responses with this data have different difficulty levels.\n\nThese features can be extracted using softwares such as PsySound  [Cabrera, 1999]  and  Marsyas [Tzanetakis and Cook, 2002]  which are open-source and capable of computing several of them rather quickly using Fast Fourier Transform  [Diniz et al., 2010]  Model-wise, several options were previously explored such as support vector machines  [Li and Ogihara, 2003] , K-nearest neighbors  [Wieczorkowska et al., 2005]  and even deep belief neural networks  [Schmidt and Kim, 2011]  more recently, but one was particularly attention grabbing: a linear regression model built in  [Yang et al., 2008] .",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Our Approach",
      "text": "Our main goal is to find a model that is flexible, with continuous responses and easy to interpret, and then explore it from a statistical viewpoint. A linear regression seems a good choice, since it brings all this characteristics while also adding the benefit of allowing automatic feature selection. Although these attributes are not unique to a linear regression, it represents a good balance between simplicity and flexibility, as we open prospects of further investigation with more sophisticated models in the future. Regarding the pieces of information previously discussed we can set the roles of the AV values and acoustic features as response and explanatory variables, respectively.\n\nParting from the idea on  [Yang et al., 2008] , we implemented the LASSO to perform feature selection. It is easily merged to the least squares solution in a classic linear regression and it can also be implemented in a Bayesian linear regression as we will develop on Chapter 4. Expanding the possibilities of Bayesian models, we employ Automatic Relevance Determination (ARD) on Chapter 5 (roughly explaining, the ARD is a penalization that excludes features based on how high their variance are and does not shrink all of them by the same amount as the LASSO). Up to this point, the estimation of Arousal and Valence was done separately, but finally, we developed a generalization of the ARD for multivariate responses in Chapter 6, the Multivariate Automatic Relevance Determination (MARD).\n\nWe also used a different, broader dataset  [Aljanaki et al., 2017] , further explored in Chapter 3: while  [Yang et al., 2008]  chose one containing only oriental pop pieces, we preferred other with many different genres and styles to increase the estimates robustness regarding the subject.\n\nThe R 2 metric is commonly used not only by  [Yang et al., 2008]  but by many others in literature, but here we disclaim some concerns about it as well as propose a new one on Chapter 4: as the R 2 measures how much of the data variation is explained by the model it shows little insight whether we are correctly predicting the AV values of a song or not, so we need to find something more related to the subtlety of our problem.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Chapter 3",
      "text": "The Database\n\nIn order to test the proposed models, it was necessary to find a broad and reliable database, pertinent to the problem of Music Emotion Recognition. We will go through this database in detail in Section 3.1. It is also necessary to explore the chosen database and tailor it to our needs, so in Section 3.2 we will look into the preprocessing we have done to the data.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "The Database For Emotional Analysis Of Music Database",
      "text": "The Database for Emotional Analysis of Music (DEAM) database  [Aljanaki et al., 2017]  was the one chosen by us, as it presents enough data to test our approaches and significant care in its construction, increasing its reliability. It it an aggregation of the datasets created for the \"Emotion in Music\" task at MediaEval benchmarking campaign 2013-2015  [Aljanaki et al., 2015] . It carries 260 acoustic, low-level features already calculated and 1802 excerpts from royalty-free songs, annotated for Arousal and Valence by proper personnel, that is, several people were trained, tested and remunerated for this task. Although the features could have been easily calculated by us, the AV annotations would be much harder, time consuming and more expensive.\n\nThere was also a concern about stability of the data, meaning every piece was annotated by a minimum 10 of people through 2013 and 2014, then annotated again in 2015 by 5 people, three of which were the most successful workers from the previous experiment.\n\nThe results were verified to be very similar, even between annotators, thus giving even more credibility to this database. Even though the music genres were somewhat broad, it only adopted western music and without preliminary tests we cannot assure that a model trained in this database would interpret well genres not contemplated by it, but it is an aspect worth investigating in future research. works in This direction was taken in  [Coutinho et al., 2014] , by transfer learning emotion across music and speech in the AV domain.\n\nEach piece in the 2013 and 2014 subsets was trimmed to 45 seconds, while the 2015 subset was annotated in full. They were all processed to have the same sampling frequency, i.e., 44,100 Hz and annotated throughout its length at a 2 Hz rate. This means that every feature and AV value was measured two times per second throughout each song. Even though this greatly summarizes the information given (that would be 44100 per second otherwise) but it would still be a lot to process and would require a time sensitive model, which is not of our interest.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Preprocessing",
      "text": "Some preprocessing was necessary in order to work with DEAM, as we are aiming for a global estimate for A and V, rather than one for every few seconds. A model that incorporated the variable time can be considered, but it will not be explored in this work.\n\nWe began by excluding the initial 10% samples of every piece, since slow intros or fading effects could give erroneous information about the bulk of the song. Since some of the songs were snippets and others were full length, there were long strings of 0's recorded at the end of shorter pieces, so we also trimmed the endings to exclude them.   To mitigate the problem of high correlation, Principal Component Analysis (PCA)\n\nwas applied to the features, implying that the linear regression models implemented here will have linear combinations of features as explanatory variables, rather than the features themselves.\n\nThe preprocessed data was then allocated in matrices used to train and test models.\n\nThe response variables were stored in vectors A, V ∈ R n , n = 1802. The explanatory variables (features after PCA) were placed in a matrix X ∈ R n×p , n = 1802, p = 260.\n\nThe database was randomly split in a training set containing 1262 songs and a test set with 540 songs, that is, 2 3 and 1 3 of the database, respectively.\n\nChapter 4\n\nMusic Emotion Recognition Via the",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Bayesian Lasso",
      "text": "In this Chapter we will explore how we can incorporate Bayesian statistics into our problem, what possibilities it adds to the modeling process and what can be done to push previous limitations. In Section 4.1 we will lay out some basic insights on Bayesian modeling when applied to MER and in Section 4.2 we will work on a way to perform automatic feature selection, looking in how it fits in our scenario. The implementation of the model will be discussed in Section 4.3, finally laying out the results we obtained in comparison with the ones achieved in the classical approach in Section 4.4 and wrapping up final thoughts in Section 4.5.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Why Bayesian Inference?",
      "text": "Keeping in mind the problem at hand, it is reasonable to consider changing from a classic approach to a Bayesian one. Emotions are naturally fluid and uncertain, so it makes sense to consider our response variables as random variables and instead of point estimates\n\nfor the AV values we would have posterior probability distributions from which we can calculate credibility intervals. These intervals can be used not only for model evaluation but also for building a recommendation system, as we will see later on. A Bayesian approach also presents an advantage when smaller databases are considered and, particularly, the penalized model implemented here also takes that into consideration. In the scenario of MER, the annotation process can be slow and expensive so being able to work with smaller databases is not a disposable quality.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Linear Regression With The Lasso",
      "text": "For each song i in the dataset, let A i and V i , denote the respective Arousal and Valence values, along with a vector x i ∈ R p containing the respective song's features 1  . Even though only numerical features are being used, this method can be adapted to include categorical ones as well  [Montgomery et al., 2013 ].\n\nWe will model Arousal and Valence by two independent linear regressions, as stated in Equation  4 .1:\n\nwhere\n\n∈ R p and scalars β a 0 , β v 0 are the regression coefficients, to be estimated from the observed data. The LASSO (Least Absolute Shrinkage and Selection Operator), also known as the L1 penalty, was originally proposed in the geophysics literature in the 1980's  [Santosa and Symes, 1986] , and further popularized in Statistics in  [Tibishirani, 1996] . Roughly explaining, it performs feature selection by shrinking the regression coefficients and discarding the sufficiently small ones, excluding therefore the most irrelevant features. In the classical scenario, it can be incorporated in the least-squares estimator as:\n\nwhere the parameters λ a and λ v can be tuned to increase or decrease the penalization, and can be chosen, for example, via cross-validation  [Tibshirani et al., 2013] . Note that, since the intercept is not related to any feature, it is not included in the penalization term.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Bayesian Lasso",
      "text": "The least-squares solution in Equation  4 .2 can easily be interpreted as the maximum a posteriori of a Bayesian model on which the prior over each component of β a and β v are independent Laplace distributions with fixed parameter λ a and λ v , respectively [Park and  Casella, 2008] . This derivation is presented in Appendix A.\n\nSince Arousal and Valence are modeled analogously there is no need to present the model twice, and in order to overcome ambiguities, denote by y i the observed value of Arousal or Valence for song i, and the vector containing all of these observations by y.\n\nWe will also omit the superscript \"a\" and \"v\" on parameters and hyperparameters.\n\nEquation 4.3 presents an extension of this model, where a prior distribution is also assigned for λ, here a Gamma distribution parametrized by shape and rate, for conjugation purposes. The conditional distribution of y reflects the linear regression model in Equation  4 .1. The Inverse-Gamma prior attributed to σ 2 , parametrized by shape and scale, is a common choice for the variance of a Normal distribution as it conjugates with the likelihood. The intercept β 0 does not carry relevant prior information to the model, so we assign a flat improper prior for it.\n\nWith this we can induce sparsity on the regression coefficients and we are able to choose parameters for the prior distributions of σ 2 and λ such that they are non informative. The model in Equation  4 .3 is the Bayesian LASSO as proposed by  [Park and Casella, 2008] , but it will not result in closed form conditional posteriors, complicating the implementation of a Gibbs sampler. To overcome that, we will write the Laplace distribution as a mixture of Normal and Exponential  [Babacan et al., 2010] , at the expense of increasing the number of parameters, being the full model rewritten as:\n\nSince the calculations for this Gibbs sampler are more commonplace, they can be found in Appendix B. The result is presented below, where z -k denotes the vector z without the k-th entry:\n\nwhere GIG denotes the Generalized Inverse Gaussian distribution  [Jørgensen, 1981]  and\n\nx ij is the j th component of x i . Even though is not as common as the other distributions,\n\nwe can sample from it by employing the algorithms proposed in  [Devroye, 2014 , Statovic, 2017] , translated to Python by us.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Implementation And Results",
      "text": "All algorithms were implemented in Python language running on Google Colab, a free environment that runs Jupyter notebooks in the cloud.\n\nFor comparison reasons, firstly we implemented a classical linear regression model with LASSO penalization as in Equation  4 .2 to evaluate the Bayesian model's contributions.\n\nWe were able to obtain similar results to  [Yang et al., 2008]  in the training scores, although the test ones were remarkably lesser.\n\nFor the Bayesian approach we implemented a basic Gibbs sampler from scratch. We ran 10,000 iterations of it, with the whole process lasting 45 minutes for each model, and considered the first 1,000 as the burn-in time of the chain, that is, the number of iterations it took to achieve convergence. In order to compare the results with the closest approach in the literature and our prior inspiration  [Yang et al., 2008]  Now, let us study the role the penalization parameters λ a and λ v took in the modeling.\n\nIn Figure  4 .1 we display the values encountered for λ a and λ v after discarding the burn-in.\n\nThere we can see how λ a and λ v converge to high values, indicating the important role regularization takes place in this problem. Also note that the mean of Arousal's λ seems higher, probably due to the fact that less information about Valence is gained from the features and the model tries to preserve as much information as possible. This hypothesis cannot be confirmed by visual inspection of  After discarding the appropriate amount of samples we are able to better estimate the posterior mean from both chains, returning 1753.3 for λ a and 1501.6 for λ v , confirming our previous hypothesis. In  Since we have auto-correlated samples for both regularization parameters, it is worth analysing the autocorrelation for a few β's, let us say the first five for Arousal and Valence, since the β's are directed linked with the respective regularization parameters. As we can see on Figure  4 .4, the correlation between λ's did not contaminate the β's.\n\nIt is also worth analysing the convergence of σ 2 a and σ 2 v in Figure  4 .5 as evidence of the convergence for the other variables' chains:\n\nWe can also look at the autocorrelation plot in",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "Final Thoughts On The Basic Bayesian Model",
      "text": "Although we have clearly obtained some improvement regarding the classical approach, there is still more we can work with in the Bayesian scenario. Since the LASSO shrinks every coefficient by the same amount, we can lose some important information by lowering coefficients that matter to our problem, so we need another approach to select the most relevant features. Potentially, other means to introduce sparsity can lead to models where closed form conditional posteriors simple to sample from are unobtainable. Alternatives that could introduce sparsity in the model would render a rather complicated posterior, that would take much too long to explore via Markov-Chain Monte Carlo. With that in mind, it seems reasonable to find an approximated posterior, rather than an exact one, and employing a model that performs a feature selection more tailored to the problem of MER.  In this chapter we will introduce the basics of Variational Inference, as means to perform approximated rather than exact inference. This allows us have more freedom when it comes to the modeling process and also potentially reduces by great lengths the time spent training the model in comparison to MCMC methods, making this approach more attractive when dealing with large databases. The downside of this method is having to work with an approximation of the posterior distribution, rather than the exact one.\n\nWe will also go into Automatic Relevance Determination as means of performing feature selection in a more efficient way than the LASSO to overcome the difficulties we have established in Chapter 4. Although this model does not return closed form conditional posterior distributions, we can perform the aforementioned method and work with an approximation, rather than implementing something like a Metropolis step within a Gibbs sampler that would be required to perform exact inference, increasing by some measure the time needed to run the model.\n\nIn Section 5.1 we will go into the basics of Variational Inference and how it works, to then introduce Automatic Relevance Determination as means to perform feature selection in Section 5.2. This two ideas will be joined and applied to our problem in Section 5.3, with its results discussed in Section 5.4, where we will be able to compare and discuss the models previously tested alongside this one.",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "Modeling With Variational Inference",
      "text": "Variational Inference (VI)  [Broderick and Steorts, 2014, Bishop, 2006]  uses optimization to approximate a posterior distribution without a closed form by a distribution easier to calculate, and this process is usually much faster than employing MCMC. But how can we obtain that approximating distribution? Let us assume that our real posterior distribution for a parameter θ given some observed data X is p(θ|X) and we would like to pick the most similar distribution q * (θ) from a family D of known and simpler distributions. There are several similarity measures that can be used  [Inman and Bradley, 2007] , but VI employs the Kullback-Leibler divergence, given by\n\ndθ, (5.1) a quantity inspired by Information Theory  [Cover and Thomas, 2006] . It is an asymmetric and positive measure of the dissimilarity between two distributions, with KL = 0 meaning they are the same. In Statistics it can be loosely interpreted as how much information will be lost when making the proposed approximation  [Kullback and Leibler, 1951 ].\n\nSo we must find q * (θ) so that the KL-divergence is minimum:\n\nIn order to calculate the exact value of the KL divergence we would need to know the exact distribution p(θ) which we do not. In Equation  5 .2 we can see how we can bypass this issue, by interpreting the KL definition as a relation of expected values and rearranging them:\n\nNote that the right side of the equation is constant over θ and the left is independent from θ, so minimizing KL increases a quantity called variational lower bound or evidence lower bound (ELBO)  [Minka, 2005] . That is, since the lowest possible value for KL is zero, the ELBO is the lowest value that can be achieved by log p(X), also known as the log marginal likelihood or log evidence of the model, the latter being more used in Bayesian inference, meaning the logarithm of the probability of the data given the model type  [Bos, 2002] .\n\nOne drawback of using the ELBO is that since we do not have access to the real posterior, we have no way of knowing whether we have reached the global minimum or a local one. One way to assess this is choosing random starting values and analysing results, but even if they remain identical there is no way of being certain, meaning the distribution q we are using to approximate p might not be the optimal one.\n\nHere we will implement mean field VI  [Blei et al., 2017] , meaning that the unknown parameters will be partitioned as θ = {θ 1 , . . . , θ m }, which means we are assuming each partition is independent from the others, that is, members of the variational family D are given by q(θ) = q 1 (θ 1 ) . . . q m (θ m ). In this scenario, we are able to employ the Coordinate Ascending Variational Inference (CAVI) algorithm  [Bishop, 2006 , Blei et al., 2017] . It can be proven that the CAVI algorithm converges to a local maximum of the ELBO  [Bishop, 2006] , and it can be then fully described as  [Blei et al., 2017] :\n\nAlgorithm 1: Coordinate Ascending Variational Inference. Input: A model p(θ, X) and a dataset X Output: A variational density q * (θ) = q * 1 (θ 1 ) . . . q * m (θ m ) Initialize the variational factors q j (θ j ) while ELBO has not converged do i) For j = 1, . . . , m, update q j (θ j ) ∝ exp{E -q j [log p(θ j , θ -j , X)]}\n\nii) Compute the ELBO end Return: q * (θ) CAVI's update rule can be derived from the ELBO by reorganizing its equation, as stated in Equation  5 .3. The first step is to rewrite the subtraction in function of q j (θ j ), letting the terms independent from it be absorbed into the constant\n\nand exploring the assumed independence between the variables in the second term:\n\nWhere E -q j is the expected value with respect to q, except on the variable j. If we make an educated guess, it is reasonable to assume q j (θ j ) = exp{E -q j [log p(θ j , θ -j , X)]}, which would render E q j = E -q j (θ -j ) [log(p(θ -j , X))], meaning q j (θ j ) = exp{E -q j [log p(θ j , θ -j , X)]} up to a normalization factor. Thus, by making q * j (θ j ) ∝ exp{E -q j [log p(θ j , θ -j , X)]} we maximize the ELBO. Now there is an alternative to MCMC and we are able to explore other ways to introduce sparsity in our model.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Feature Selection With Automatic Relevance Determination",
      "text": "Automatic Relevance Determination (ARD)  [Wipf and Nagarajan, 2007 , Drugowitsch, 2013 , Blei et al., 2017]  is another way of introducing sparsity in a model by using a parameterized prior distribution that effectively prunes away redundant or superfluous coefficients instead of shrinking all of them by the same amount as we were doing before.\n\nIt performs features selection in a similar manner of the LASSO, but it takes into consideration the observed error within each parameter to evaluate the importance of each feature separately, attributing individual weights to each one. The hyper parameters a 0 , b 0 , c 0 and d 0 can be chosen to achieve purposely uninformative distributions to allow the model to learn more from the data. From here on, normal distributions will be parametrized by precision: [varicance] -1 . To implement ARD we are going to keep the linear regression model Y = Xβ + ε but with the following modifications:\n\nNotice that τ diag(α) is a precision matrix rather than covariance as it is ordinary in Automatic Relevance Determination and we will continue to employ that for the remainder of this work.\n\nIn our scenario, inspired by the prior structure, we will assume that the distributions in the variational family factorize as q(β, τ, α) = q(β, τ ) p j=1 q(α j ).\n\n(5.5)",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Employing Vi And Ard",
      "text": "And now we need to apply VI to our model in Equation  5 .4 so our conditional posterior will be:\n\nwhere the quantities c j * , d j * , β * , V -1 * , a * and b * are the parameters of the variational distribution defined previously. Since these calculations are not authorial  [Blei et al., 2017] , they will be omitted for the time being and detailed in Appendix C with only the results presented below:\n\nThe updated parameters are given by:\n\na * b * , j = 1, . . . , p.\n\n(5.7)\n\nNow we are able to implement CAVI's algorithm as we did before, inspired by the one in  [Blei et al., 2017] . The implementation od the algorithm was written for MATLAB in  [Drugowitsch, 2013]  and transcribed by us for Pyhton:\n\nAlgorithm 2: Coordinate Ascending Variational Inference for Music Emotion Recognition. Input: A model p(β, τ, α, X) and a dataset X Output: A variational density q * (β, τ, α) = q * (β, τ ) p j=1 q * (α j ) Initialize the parameters of the variational distributions q(β, τ ) p j=1 q j (α\n\nend Return: q * (β, τ, α)\n\nWith these parameters we can use the approximated posterior distribution to implement the approximated predictive distribution in order to make predictions from a set of new data.\n\nIn this scenario, this approximated predictive density follows a Student's t distribution with location β T * x, scale (1 + x T V * x) -1 a * b * and 2a * degrees of freedom, as demonstrated in  [Drugowitsch, 2013]  and adapted for the parameters and data we have in this work.",
      "page_start": 38,
      "page_end": 40
    },
    {
      "section_name": "Results",
      "text": "The ARD model was implemented with an adapted code from  [Drugowitsch, 2013]  and translated from MATLAB/Octave language.\n\nThe credibility intervals calculated for the ARD model were obtained from the approximated predictive density mentioned in Section 5.3. We can see in Tables 5.1 and 5.2 that ARD implemented with the CAVI algorithm showed significant improvement upon the previous results, with 270 intervals containing the right values for Arousal and 231 for Valence, and only taking a few seconds to run and draw samples from the variational distribution. The subset of songs that have both A-V values within the intervals built for them is more modest, reaching 131. The training and test R 2 metric were also improved, reaching 0.75 and 0.66 for Arousal and 0.59 and 0.29 for Valence.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Arousal",
      "text": "Classic  From the volume distribution around the columns centered in zero, we can see that the ARD model turns fewer coefficients to zero and allows others to be slightly higher, thus preserving information that was otherwise discarded in the LASSO model.",
      "page_start": 41,
      "page_end": 42
    },
    {
      "section_name": "Chapter 6",
      "text": "Developing Multivariate Automatic",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Relevance Determination In Music Emotion Recognition",
      "text": "The model presented in Chapter 5 ignored the substantial correlation between Arousal and Valence by treating these two response variables as independent, but the Multivariate Automatic Relevance Determination (MARD) model we seek to develop here accounts for that relationship. More generally, this model is capable of being adapted to deal with a dataset carrying m dependent response variables, considering valuable information that the ARD model does not account for. In this work, however, we will only consider the scenario where m = 2. We extended the ARD by generalizing the Normal-Gamma prior distribution jointly attributed to β and τ with a Normal-Wishart distribution attributed to β and the observations' precision matrix, setting requirements to adjust the generalized ARD to any problem.\n\nWe will begin by defining the bivariate model we are going to explore in Section 6.1, then develop it in Section 6.2 where several calculations will be made to find its updating rules. The predictive distribution will be calculated and discussed in Section 6.3, before testing the model in Section 6.4 and analyzing results in 6.5.",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Introducing A Bivariate Model",
      "text": "The model will follow the same structure as described in Equation  4 .1, but here it will be defined as a multivariate linear regression  [Rossi et al., 2005 ] Y = XB + E, where each pair (a i , v i ) is written as y i and the remaining variables will be described below. Underneath each matrix is stated its size, as to provide more clarity in the calculations that follow.\n\n, where each pair (a i , v i ) correspond to the AV values of the observation i\n\n, where each pair (β a j , β v j ) contains the regression parameters for Arousal and Valence\n\n, where the arrays x T i are explanatory variables corresponding to each observation i, with the first entry of each x T i is 1 to account for the intercept.\n\n, where each pair ε a i , ε v i correspond to the observation errors i associated to the regression model employed and will be represented by",
      "page_start": 43,
      "page_end": 44
    },
    {
      "section_name": "Creating The Foundation For Mard",
      "text": "In order to implement ARD's intuition in a multivariate scenario several adaptations to that model had to be made, most remarkably by creating a precision matrix for the parameters that is related to τ diag(α) used by ARD in a way that allowed MARD to support any size of correlated responses. Note that a precision matrix will be attributed to the Normal distributions rather than a covariance matrix, just as defined in Chapter 5.\n\nBefore we can state this generalization, the Kronecker Product must be defined, as it is essential for its construction:\n\nDefinition 1. Kronecker Product: Consider the matrices A m×n and B p×q . The Kronecker Product between them is defined as:\n\npm×qn Thus, we can elaborate on ARD in order to create the generalized model MARD:\n\nWhere W denotes the Wishart distribution. Also, note that β is not B, but its vectorization:\n\nDefinition 2. Vectorization: Consider the matrix A m×n . The vectorization of A, denoted by vec(A), is the mn × 1 column vector obtained by stacking the columns of the matrix A on top of one another:\n\nTherefore, the prior precision matrix of β|K will be given by\n\nFurther along, we will denote K ⊗ ∆ by Q to facilitate understanding. In this notation, B|K follows the so-called matrix normal distribution  [Gupta and Nagar, 1999] , therefore, K -1 represents covariance on its columns while ∆ -1 accounts for the lines, so we are assuming uncorrelated features as priors but the columns can have some correlation influenced by the correlation between Arousal and Valence.\n\nRemark. Notice that even though the matrix K ⊗ ∆ is related to the precision matrix τ diag(α) in ARD, initially we hoped to use the precision matrix Λ as defined below:\n\nAs we can see, Λ is more similar to τ diag(α) and it allowed each feature to have its own regularization parameter for Arousal and Valence. This option does not need to be excluded as it is possible to perform MCMC, utilize some form of Variational Inference that is not CAVI or choose a variational family where this precision matrix works, but those were not adequate for this work and another approach became preferable. So even though we have lost the information different α's would have given to each parameter related to Arousal and Valence separately it is easier to render a model with joint alphas while keeping some of the information they carry.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Developing The Model",
      "text": "Here we will work through the necessary calculations to deduce CAVI's updating rules for MARD and the variational families within CAVI as done in Chapter 5.",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Likelihood",
      "text": "With the definitions set in Section 6.1.1 we can begin to calculate the model's likelihood in the following way, considering the errors to be independent and follow a Normal distribution  [Rossi et al., 2005] .\n\nWith tr being the trace of\n\nIt performs a change of variable from E to Y, and the jacobian is constant and unitary  [Rossi et al., 2005] . The variable S = Y -X B, B = (X T X) -1 X T Y being the least squares estimate of B, will also be introduced to facilitate calculations. Below we have manipulated the terms inside the exponential in order to obtain something quadratic in B which is close to the desired normal distribution.\n\n3)\n\nThe result we achieve on Equation  6 .3 very closely resembles a normal distribution and we will use Vectorization and the Kronkcker product to derive some important relations to finish that off.\n\nThe following relations occur from properties of vectorizarion as defined above, further explored in  [Macedo and Oliveira, 2013]  and proved in Chapter 2 of  [Magnus and Neudecker, 1988]  Theorem 1. Properties of Vectorization and the Kronecker Product: Consider the matrices A, B and C as matrices such that the matrix product ABC is defined. We can relate vectorization and the Kronecker product as follows:\n\nRecalling that vec(B) = β we can develop a inner section of Equation  6 .3 as:\n\nSo we can follow up Equation  6 .3 with\n\nTo simplify the notation, K ⊗ X T X will be written as J.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Computation Of Variational Distributions",
      "text": "Joining our likelihood and prior distributions previously set, we obtain the following posterior:\n\nThe mean-field variational model will be written as q * (β, K) ∝ exp E -β,K log p(Y|β, K, α, X) + log p(β, K|α) + log p(α)\n\nFirstly, let us make a brief pause here to explain some actions in the computations above. The value E -β,K [log(α a1 . . . α ap , α v1 . . . α vp )] is unrelated to our variables of interest β and K so it can be disregarded. The expected value of Q requires more care:\n\nTo make the notation more efficient the last matrix will be noted as\n\nas we further the calculations in Equation  6 .7. The exponentials will also be manipulated to fulfill our goal of identifying a Normal-Wishart distribution in this posterior. This distribution would parallel well with the idea of ARD and the posterior we have indicates that this this can be achieved. Continuing Equation  6 .7, we have:\n\nFrom Equation  6 .8 we will find a Normal-Wishart distribution and therefore, the updating rules for the respective variational parameters. In order to do that we need only to identify the quadratic part of the Normal distribution and discover its parameters, so let us consider\n\nwhere\n\nwith γ being a constant value independent from β.\n\nFirst we will aim to identify a Normal distribution exploring similarities in f (β) and g(β). Ideally, the differences between them (γ) will be reallocated in a Wishart distribution in q * (β, K)\n\nTo find the mean and precision matrix of this distribution we know that\n\nSo we will obtain these parameters for f (β) in the same way:\n\nSo the first part of our variational distribution follows a normal distribution with\n\nOpening up the terms if f (β) we have:\n\nOpening up the terms in g(β) we have the following:\n\nFrom the four terms in Equation  6 .11 we can already see the first in Equation  6 .12, respectively. To explore the last term in g(β), the Woodbury matrix identity will be applied.\n\nTheorem 3. Woodbury Identity: Let A, U, C and V be matrices of adequate sizes,\n\nConsidering A = J, C = Q * , U and V = I, the identity matrix, we can write\n\nWith this, the remaining term of g(β) will be opened as well:\n\nNow we have found f (β) and the last term in f (β) should be allocated in the Wishart part of the variational distribution. So we can rewrite Equation  6 .8 as:\n\nwhere\n\n) -1 β will be transformed into something that can be incorporated in the Wishart part of Equation  6 .14.\n\nWe will also employ the properties listed in Theorem 1 and the ones in Definition 2:\n\nthat can be inserted in Equation  6 .14 as\n\nwhere\n\n(6.17)\n\nNow we need to find the distributions for each q * (α j ). Since they will be identically calculated, suffices to calculate for one of theses α's\n\nAs in Equation  6 .8, the terms that did not relate to α j were disregarded, and to find E -α j [β T Qβ] the following calculations were made:\n\nBy discarding whatever does not relate to the variable of interest α j we can explore the conditional expected value and linearity properties to further the calculations.\n\nNote that the expected value with respect to the variational distribution considers the variational distribution of β. We know that\n\non, this equation will be split into three parts, in order to improve its understanding.\n\nNow let us develop some calculations to work the expected values on Equation  6 .20.",
      "page_start": 48,
      "page_end": 49
    },
    {
      "section_name": "Note That K",
      "text": ". Since it will be a recurring value throughout the remaining of this work, let us attribute M * = X T X + ∆ * Also note that calculating β * does not involve K:\n\nTherefore is constant with respect to E K and this allow us to continue this calculations with more simplicity.\n\nwhat makes Var -α j (β aj |K) equals the (j, j) entry of the first block in (K * ) -1 and Var -α j (β vj |K) equals the (j, j) entry of the fourth block. So we can carry on Equation 6.20 with\n\nwe will need the following Definition and Theorem  [Gupta and Nagar, 1999] :\n\nDefinition 3. Commutation matrix: A commutation matrix G pq is a pq × pq matrix that transforms vec(A) in vec(A) T , where A is a p × q matrix. G pq is given by\n\n, where the (i, j) entry of H ij is 1 and the others are 0.\n\nTheorem 4. Expectation value of Wishart distribution: Let S ∼ W(Π, u) and G qq be a q × q commutation matrix. Then:\n\n. Applying this theorem to our case we have:\n\nCollecting the values we need from the equations above we can fulfill the expected values below:\n\nWith the information obtained in Equation  6 .20 we can continue Equation  6 .18 as q * (α j ) ∝ exp log(α j )(c * -1) -α j d * , (6.24)\n\nwhere\n\nWith this method, we are able to maintain the variational family given and slightly modify the Algorithm 2 to implement CAVI in MARD: Algorithm 3: Adapted Coordinate Ascending Variational Inference. Input: A model p(β, K, α, X) and a dataset X, where the response is Y ∈ R 2\n\nOutput: A variational density q * (β, K, α) = q * (β, K) p j=1 q * (α j ) Initialize the variational factors q(β, K)q j (α j )\n\n, for all j Return: q * (β, K, α)\n\nNote that we will not calculate the ELBO as in Algorithm 2, as it is much more difficult, but we will rely on the calculations provided and the theoretical evidence that guarantees convergence to a local maximum of the ELBO. The values obtained from this implementation will be considered converged when their precision reach 10 -3 .",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Predictive Distribution",
      "text": "In this chapter we worked through the calculations necessary to implement variational inference through CAVI in a multivariate generalization of ARD, taking in consideration the correlation between the responses. Since our model has a Normal likelihood, Normal-Wishart joint prior distribution and compatible variational family, our model is simple enough that it is possible to calculate the joint predictive distribution, that can be useful to perform more accurate predictions. We can also get a credible region, rather than two separate credibility intervals as we did in Chapter 5, achieving a narrower, more precise set of predictions.\n\nIn order to do that, we need to approximate the predictive distribution p(y|X):\n\nHere it is worth stopping to comment on a few steps of Equation  6 .25. The mean-field hypothesis aligned with α's absence in the likelihood allows this variable to be integrated out. Also note that B T x = (x T B) T , so the notation in Equation  6 .25 does not clash with the distribution set in Section 6.1.1. Writing it in this manner provides a column vector that will enable us to carry on the calculations. We also need to replace B by some form of β, as exploring a Multivariate Normal distribution can be more easily explored than a Matrix Normal distribution. If we define the matrix Ξ = I 2 ⊗ x T (I 2 being the 2 × 2 identity matrix) we can write B T x as Ξβ and continue as:\n\nTo calculate the inner integral we resort to the following Theorem in  [Bishop, 2006] :\n\nTheorem 5. Convolution of Normal densities: Consider the following distributions:\n\nThe marginal distribution of y, given by p(y, x) dx = p(y|x)p(x) dx is written as p(y) = N(y|Aµ + b, (L -1 + Aψ -1 A T ) -1 ).\n\nwhere d = 2 in our case. The marginalization can be computed as:  (6.29)  Note that this last line on Equation  6 .29 is the kernel of a Wishart distribution with\n\n, being the integral equal to the inverse of the respective normalizing constant:\n\n(6.30)\n\nIn order to carry on, let us explore the following relation:  (6.31)  This expression is similar to the kernel of a multivariate t-distribution, so with some algebra we can arrive at:\n\n, (6.32) which can be recognized as a multivariate t-distribution with ν * -d + 1 degrees of freedom (ν * -2 + 1 in our case), centered at Ξβ * and scale matrix given by ϕ -1 ν * -d+1 V -1 * . Therefore, we can write Equation  6 .28 as:  .33)  Since this distribution has a closed form and is from a known family of distributions, it is easy to sample using Algorithm 4 from and allows us to calculate approximated credible regions for each prediction. Algorithm 4: Sampling from MARD's predictive distribution For i = 1, . . . , Niter:\n\nReturn: From the set (y (i) , β (i) , K (i) ) i=1,...,Niter of samples from the joint distribution of y, β and K, keep only (y (i) ) i=1,...,Niter .",
      "page_start": 58,
      "page_end": 59
    },
    {
      "section_name": "Testing Mard In A Controlled Setting",
      "text": "To access MARD's overall performance, it was firstly tested in a controlled scenario.\n\nThe dummy dataset built for this purpose will have 2 responses and p = 100 features, where only 20 differ from 0. These features will be stored in a matrix B p×2 . The input data x i ∈ R p will follow a multivariate normal distribution with mean [0 0] and identity covariance matrix, for i = 1, . . . , n, and is stored in the matrix X n×p . In this scenario, the generated errors ε i are normally distributed with mean zero and covariance matrix K -1 , for i = 1, . . . , n and stored in E n,2 . Two choices will be made for K -1 : first as a multiple of the identity matrix for uncorrelated responses, then 100 85 85 100 for a scenario closer to ours, with a 0.57 correlation for the responses. The observed data will be Y = XB + E and split between training sets of size n = 1000, 500, 100 and test sets of size n = 1000.\n\nUnfortunately, due to the nature of MARD's algorithm, we cannot work with datasets features n < p in this format, as CAVI still requires inverting the matrix (X T X), only possible if n ≥ p. This does not exclude the possibility of implementing MARD in a way To analyse the main metric used in the previous models, we have calculated credible regions by sampling from the predictive distribution and displayed them in Figure  6 .4.\n\nThe black X marks the values each model predicted, while the blue dot is the real value the models were aiming to predict. The credible region is created by the dispersion dots sampled from the predictive distribution, with red representing ARD and green MARD. Overall, the results are show in    As done before, we will carry on observing some performance measures. Even though R 2 values are very similar, there has been a huge improvement considering the credible regions, with both of them almost reaching the total number of test samples (540). Let us take this information this further.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Arousal",
      "text": "Classic The Figure  6 .7 contains a histogram of the intervals' amplitudes. In it we can see that the intervals created by MARD are much larger, what at least partially accounts for the results we obtained, meaning a recommendation system created using MARD would be more generalist. Let us consider, for instance, that a user has chosen a particular piece of music, portrayed in navy blue in Figure  6 .8 and wishes to listen to n other songs of similar emotion from the dataset, in light blue. A recommendation system employing ARD would recommend the ones contained in the red rectangle, while MARD would choose from the green one.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Conclusions",
      "text": "In this Chapter, we went through a lengthy process in order to transform ARD into a multivariate model with hopes that it would capture more information and return better results. Even though comparisons between the two approaches did not differ much in the controlled setting, we were able to see great disparities in the actual case of MER.\n\nWhile ARD return a narrower region, MARD creates very broad intervals, more capable to encapsulate the target while also risking comprehending data that does not relate to the objective. The possible ramifications of this will be discussed in the following and final Chapter.\n\nChapter 7",
      "page_start": 69,
      "page_end": 69
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we have explored a relationship between Music Information Retrieval and\n\nStatistical knowledge and how we can use new Bayesian inference methods to expand possibilities in an old problem.\n\nAlthough Bayesian techniques can be very computationally taxing, here we find a way to work around that issue by bringing in Variotinal Inference. Albeit it returns an approximated result, this approach manages to return good results in mere seconds in the database tested here.\n\nAs mentioned throughout this work, the metric proposed for the Bayesian models can be used to build an emotion-based recommendation system where all songs with AV values within a chosen song's interval can be recommended to a user. Let us see in Figure p(β 0 , β, γ, λ, σ 2 |X) ∝ p(y|β 0 , β, σ 2 , X)p(β 0 )p(σ 2 )p(λ)p(γ|λ)p(β|γ)\n\nObserving the full posterior in Equation B.2 we can calculate the conditional posterior for each variable by discarding the terms not related to the variable of interest and observing which distribution it can be recognized as. In some cases, such as β 0 and β j the quadratic form stands out quite easily, while σ 2 and λ were respectively identified as an Inverse Gamma and Gamma distributions, with γ j having presented characteristics of a more unusual distribution, the Generalized Inverse Gaussian.",
      "page_start": 70,
      "page_end": 77
    },
    {
      "section_name": "B.1 Conditional Distribution Of Β 0",
      "text": "In order to derive the parameters of the distribution for β 0 we will explore the first and second moments of the normal distribution:",
      "page_start": 77,
      "page_end": 77
    },
    {
      "section_name": "B.5 Conditional Distribution Of Β J",
      "text": "Here, we will replicate the process done for β 0 to discover the parameters of β j 's distribution:",
      "page_start": 78,
      "page_end": 78
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 1: The Arousal-Valence Scale with examples of emotion placement.",
      "page": 13
    },
    {
      "caption": "Figure 3: 1 shows an example of the evolution of one particular feature, the fundamental",
      "page": 21
    },
    {
      "caption": "Figure 3: 1: Example of fundamental frequency evolution throughout time for a single musical",
      "page": 21
    },
    {
      "caption": "Figure 3: 2 on the other hand, shows the annotated AV values throughout time for four",
      "page": 22
    },
    {
      "caption": "Figure 3: 2: Example of annotated AV values for four songs and their reduction. Blue dots",
      "page": 22
    },
    {
      "caption": "Figure 3: 3. Ideally, all but the main diagonal should be white, meaning zero correlation,",
      "page": 22
    },
    {
      "caption": "Figure 3: 3: Excerpt of the correlation matrix between the 260 features, after preprocessing. Red",
      "page": 22
    },
    {
      "caption": "Figure 4: 1 we display the values encountered for λa and λv after discarding the burn-in.",
      "page": 28
    },
    {
      "caption": "Figure 4: 1: Sampled values for λa and λv in the Bayesian LASSO.",
      "page": 29
    },
    {
      "caption": "Figure 4: 1 alone, so in order to investigate the issue further we would need to calculate each",
      "page": 29
    },
    {
      "caption": "Figure 4: 1 still spread out quite a bit, indicating the sampled values",
      "page": 29
    },
    {
      "caption": "Figure 4: 2. Before we make any assessments, only one in every 30 samples should be",
      "page": 29
    },
    {
      "caption": "Figure 4: 2: Autocorrelation function of the sampled values for λa and λv in the Bayesian",
      "page": 29
    },
    {
      "caption": "Figure 4: 3 we can see an estimation of the density of their",
      "page": 30
    },
    {
      "caption": "Figure 4: 3: Density estimation of λa and λv from samples for Arousal and Valence for the",
      "page": 30
    },
    {
      "caption": "Figure 4: 4, the correlation between λ’s did not contaminate the β’s.",
      "page": 30
    },
    {
      "caption": "Figure 4: 5 as evidence of the",
      "page": 30
    },
    {
      "caption": "Figure 4: 6, as an indicator that our",
      "page": 30
    },
    {
      "caption": "Figure 4: 7: Coeﬃcient values for βa and βv in the Gibbs model.",
      "page": 31
    },
    {
      "caption": "Figure 4: 4: Autocorrelation function of the sampled values for ﬁrst few βa’s and βv’s in the",
      "page": 32
    },
    {
      "caption": "Figure 4: 5: Convergence of σ2",
      "page": 33
    },
    {
      "caption": "Figure 4: 6: Autocorrelation function of the sampled values for σ2",
      "page": 33
    },
    {
      "caption": "Figure 5: 1: Histograms of linear regression coeﬃcients for Arousal with feature selection via",
      "page": 42
    },
    {
      "caption": "Figure 5: 2: Histograms of linear regression coeﬃcients for Valence with feature selection via",
      "page": 42
    },
    {
      "caption": "Figure 6: 1. On the left we can see",
      "page": 63
    },
    {
      "caption": "Figure 6: 1: Manufactured dataset display of parameters with one response variable in blue and",
      "page": 63
    },
    {
      "caption": "Figure 6: 2 we provide a visual aid for the information stade above. Each image",
      "page": 63
    },
    {
      "caption": "Figure 6: 2: ARD and MARD performances in predicting coeﬃcients for n = 1000, 500, 100,",
      "page": 64
    },
    {
      "caption": "Figure 6: 3 compares the spread of predictions made by ARD (red) and MARD (green)",
      "page": 64
    },
    {
      "caption": "Figure 6: 1 and seem to provide similar results.",
      "page": 65
    },
    {
      "caption": "Figure 6: 3: Comparison between predictions achieved by ARD and MARD for Y with n = 500.",
      "page": 65
    },
    {
      "caption": "Figure 6: 4: Display of preditive samples by ARD and MARD predictions for a single response",
      "page": 65
    },
    {
      "caption": "Figure 6: 5: Histograms of linear regression coeﬃcients for Arousal with feature selection via",
      "page": 66
    },
    {
      "caption": "Figure 6: 6: Histograms of linear regression coeﬃcients for Valence with feature selection via",
      "page": 67
    },
    {
      "caption": "Figure 6: 7 contains a histogram of the intervals’ amplitudes. In it we can see that",
      "page": 67
    },
    {
      "caption": "Figure 6: 7: Spread of credibility intervals created by ARD (red) and MARD (green) for Arousal",
      "page": 68
    },
    {
      "caption": "Figure 6: 8 and wishes to listen to n other songs of similar",
      "page": 68
    },
    {
      "caption": "Figure 6: 8: Example of how a few pieces would be recommended by ARD and MARD considering",
      "page": 69
    },
    {
      "caption": "Figure 7: 1 below how that would work for each model, Note that the images were enlarged and",
      "page": 70
    },
    {
      "caption": "Figure 7: 1: Recommendation system created by the Gibss, ARD and MARD model, respectively.",
      "page": 71
    },
    {
      "caption": "Figure 7: 1. We have deliberately",
      "page": 71
    },
    {
      "caption": "Figure 7: 2 we can see how the other models can end up stuck far from the target",
      "page": 72
    },
    {
      "caption": "Figure 7: 2: Recommendation system created by the Gibss, ARD and MARD model, respectively.",
      "page": 73
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nUnsettledness\nAnger\nConforto": "-1\nMelancholy\nSadness",
          "Arousal\nJoy\nComfort\nValence": "1\nEase\nPeace\n-1"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion in music task at mediaeval 2015",
      "authors": [
        "Aljanaki"
      ],
      "year": "2015",
      "venue": "MediaEval 2015"
    },
    {
      "citation_id": "2",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "Aljanaki"
      ],
      "year": "2017",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "3",
      "title": "Bayesian compressive sensing using laplace priors",
      "authors": [
        "Babacan"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "4",
      "title": "Pattern Recognition and Machine Learning",
      "authors": [
        "C Bishop ; Bishop"
      ],
      "year": "2006",
      "venue": "Pattern Recognition and Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Variational inference: A review for statisticians",
      "authors": [
        "Blei"
      ],
      "year": "2017",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "6",
      "title": "A comparison of marginal likelihood computation methods",
      "authors": [
        "C Bos ; Bos"
      ],
      "year": "2002",
      "venue": "A comparison of marginal likelihood computation methods"
    },
    {
      "citation_id": "7",
      "title": "Variational bayes for merging noisy databases",
      "authors": [
        "Steorts ; Broderick",
        "T Broderick",
        "R Steorts"
      ],
      "year": "2014",
      "venue": "Variational bayes for merging noisy databases",
      "arxiv": "arXiv:1410.4792"
    },
    {
      "citation_id": "8",
      "title": "Psysound: A computer program for psychoacoustical analysis",
      "authors": [
        "D Cabrera ; Cabrera"
      ],
      "year": "1999",
      "venue": "Proceedings of the Australian Acoustical Society Conference"
    },
    {
      "citation_id": "9",
      "title": "Transfer learning emotion manifestation across music and speech",
      "authors": [
        "Casella",
        "G Berger ; Casella",
        "R Berger",
        "Coutinho"
      ],
      "year": "2001",
      "venue": "2014 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "10",
      "title": "Random variate generation for the generalized inverse gaussian distribution",
      "authors": [
        "L Devroye ; Devroye"
      ],
      "year": "2014",
      "venue": "Statistics and Computing"
    },
    {
      "citation_id": "11",
      "title": "Digital Signal Processing: System Analysis and Design",
      "authors": [
        "Diniz"
      ],
      "year": "2010",
      "venue": "Digital Signal Processing: System Analysis and Design"
    },
    {
      "citation_id": "12",
      "title": "Variational bayesian inference for linear and logistic regression",
      "authors": [
        "J Drugowitsch ; Drugowitsch"
      ],
      "year": "2013",
      "venue": "Variational bayesian inference for linear and logistic regression",
      "arxiv": "arXiv:1310.5438"
    },
    {
      "citation_id": "13",
      "title": "Popular music retrieval by detecting mood",
      "authors": [
        "Feng"
      ],
      "year": "2003",
      "venue": "26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "14",
      "title": "Matrix Variate Distributions",
      "authors": [
        "Nagar ; Gupta",
        "A Gupta",
        "D Nagar",
        "R Horn",
        "C Johnson"
      ],
      "year": "1991",
      "venue": "Topics in Matrix Analysis"
    },
    {
      "citation_id": "15",
      "title": "Lyric text mining in music mood classification",
      "authors": [
        "Hu"
      ],
      "year": "2009",
      "venue": "10th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "16",
      "title": "The overlapping coefficient as a measure of agreement between probability distributions and point estimation of the overlap of two normal densities",
      "authors": [
        "Bradley Inman",
        "H Inman",
        "E Bradley"
      ],
      "year": "2007",
      "venue": "Communications in Statistics -Theory and Methods"
    },
    {
      "citation_id": "17",
      "title": "Music, the Brain, and Ecstasy: How Music Captures Our Imagination. Quill, 182 Whitley Mills Rd",
      "authors": [
        "R Jourdain ; Jourdain"
      ],
      "year": "2008",
      "venue": "Music, the Brain, and Ecstasy: How Music Captures Our Imagination. Quill, 182 Whitley Mills Rd"
    },
    {
      "citation_id": "18",
      "title": "Sentiment extraction in music",
      "authors": [
        "B Jørgensen ; Jørgensen",
        "Katayose"
      ],
      "year": "1981",
      "venue": "Statistical Properties of the Generalized Inverse Gaussian Distribution"
    },
    {
      "citation_id": "19",
      "title": "On information and sufficiency",
      "authors": [
        "Kullback",
        "S Leibler ; Kullback",
        "R Leibler"
      ],
      "year": "1951",
      "venue": "The Annals of Mathematical Statistics"
    },
    {
      "citation_id": "20",
      "title": "Prediction of musical affect using a combination of acoustic structural cues",
      "authors": [
        "Leman"
      ],
      "year": "2005",
      "venue": "Journal of New Music Research"
    },
    {
      "citation_id": "21",
      "title": "Detecting emotion in music",
      "authors": [
        "Ogihara ; Li",
        "T Li",
        "M Ogihara"
      ],
      "year": "2003",
      "venue": "4th International Symposium on Music Information Retrieval"
    },
    {
      "citation_id": "22",
      "title": "Music Data Mining",
      "authors": [
        "Li"
      ],
      "year": "2011",
      "venue": "Music Data Mining"
    },
    {
      "citation_id": "23",
      "title": "Typing linear algebra: A biproduct-oriented approach",
      "authors": [
        "Macedo",
        "H Oliveira ; Macedo",
        "J Oliveira"
      ],
      "year": "2013",
      "venue": "Science of Computer Programming"
    },
    {
      "citation_id": "24",
      "title": "Matrix Differential Calculus with Applications in Statistics and Econometrics",
      "authors": [
        "Magnus Neudecker ; Magnus",
        "I Neudecker"
      ],
      "year": "1988",
      "venue": "Matrix Differential Calculus with Applications in Statistics and Econometrics"
    },
    {
      "citation_id": "25",
      "title": "Divergence measures and message passing",
      "authors": [
        "T Minka ; Minka"
      ],
      "year": "2005",
      "venue": "Divergence measures and message passing"
    },
    {
      "citation_id": "26",
      "title": "Music, Mind, and Brain: The Neuropsychology of Music",
      "authors": [
        "M Minsky ; Minsky",
        "Montgomery"
      ],
      "year": "1982",
      "venue": "Music, Mind, and Brain: The Neuropsychology of Music"
    },
    {
      "citation_id": "27",
      "title": "Emotion-based Analysis and Classification of Audio Music",
      "authors": [
        "R Panda ; Panda",
        "T Park",
        "G Casella"
      ],
      "year": "2008",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "28",
      "title": "Bayesian Statistics and Marketing",
      "authors": [
        "Rossi"
      ],
      "year": "2005",
      "venue": "Bayesian Statistics and Marketing"
    },
    {
      "citation_id": "29",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell ; Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "30",
      "title": "Linear inversion of bandlimited reflection seismograms",
      "authors": [
        "Santosa",
        "F Symes ; Santosa",
        "W Symes"
      ],
      "year": "1986",
      "venue": "SIAM Journal on Scientific and Statistical Computing"
    },
    {
      "citation_id": "31",
      "title": "Implementation of the devroye (2014) algorithm for sampling from the generalized inverse gaussian (gig) distribution",
      "authors": [
        "Kim Schmidt",
        "E Schmidt",
        "Y Kim",
        "Song"
      ],
      "year": "2011",
      "venue": "The 9th International Symposium on Computer Music Modeling and Retrieval (CMMR)"
    },
    {
      "citation_id": "32",
      "title": "Regression shrinkage and selection via the lasso",
      "authors": [
        "R Tibishirani ; Tibishirani"
      ],
      "year": "1996",
      "venue": "Journal of the Royal Statistical Society"
    },
    {
      "citation_id": "33",
      "title": "Musical genre classification of audio signals",
      "authors": [
        "Tibshirani"
      ],
      "year": "2002",
      "venue": "Tzanetakis and Cook"
    },
    {
      "citation_id": "34",
      "title": "Affect representation and recognition in 3d continuous valence-arousal-dominance space",
      "authors": [
        "Tiwary ; Verma",
        "G Verma",
        "U Tiwary"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "35",
      "title": "Extracting emotions from music data",
      "authors": [
        "Wieczorkowska"
      ],
      "year": "2005",
      "venue": "International Symposium on Method-ologies for Intelligent Systems 2005: Foundations of Intelligent Systems"
    },
    {
      "citation_id": "36",
      "title": "A new view of automatic relevance determination. NIPS",
      "authors": [
        "D Wipf And Nagarajan ; Wipf",
        "S Nagarajan"
      ],
      "year": "2007",
      "venue": "A new view of automatic relevance determination. NIPS"
    },
    {
      "citation_id": "37",
      "title": "Disambiguating music emotion using software agents",
      "authors": [
        "Lee ; Yang",
        "D Lee"
      ],
      "year": "2004",
      "venue": "5th International Conference on Music Information Retrieval"
    },
    {
      "citation_id": "38",
      "title": "A regression approach to music emotion recognition",
      "year": "2008",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    }
  ]
}