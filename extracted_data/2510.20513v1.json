{
  "paper_id": "2510.20513v1",
  "title": "Decoding The Ear: A Framework For Objectifying Expressiveness From Human Preference Through Efficient Alignment",
  "published": "2025-10-23T12:57:46Z",
  "authors": [
    "Zhiyu Lin",
    "Jingwen Yang",
    "Jiale Zhao",
    "Meng Liu",
    "Sunzhu Li",
    "Benyou Wang"
  ],
  "keywords": [
    "Speech expressiveness",
    "objective metric",
    "human preference alignment",
    "speech-to-speech models",
    "data curation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent speech-to-speech (S2S) models generate intelligible speech but still lack natural expressiveness, largely due to the absence of a reliable evaluation metric. Existing approaches, such as subjective MOS ratings, low-level acoustic features, and emotion recognition are costly, limited, or incomplete. To address this, we present DeEAR (Decoding the Expressive Preference of eAR), a framework that converts human preference for speech expressiveness into an objective score. Grounded in phonetics and psychology, DeEAR evaluates speech across three dimensions: Emotion, Prosody, and Spontaneity, achieving strong alignment with human perception (Spearman's Rank Correlation Coefficient, SRCC = 0.86) using fewer than 500 annotated samples. Beyond reliable scoring, DeEAR enables fair benchmarking and targeted data curation. It not only distinguishes expressiveness gaps across S2S models but also selects 14K expressive utterances to form Expres-siveSpeech, which improves the expressive score (from 2.0 to 23.4 on a 100-point scale) of S2S models. Demos and codes are available at https://github.com/FreedomIntelligence/ ExpressiveSpeech",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent end-to-end speech models can generate clear speech in Textto-Speech (TTS) tasks. Yet in conversational settings, their output often sounds robotic and lacks the expressiveness vital for applications such as voice assistants, role-playing, and AI companions. The core of this problem is the absence of a reliable evaluation metric.\n\nWhile fields like speech recognition and audio enhancement benefit from WER  [1, 2]  and DNSMOS  [3] , expressiveness still relies on subjective MOS  [4] , which is costly and unscalable. Existing alternatives are limited: low-level acoustic features  [5]  (e.g., pitch, energy) miss perceptual subtleties, and emotion recognition  [6]  captures only one facet of expressiveness. In short, a comprehensive, human-aligned metric is urgently needed.\n\nTo address this gap, we introduce DeEAR, a novel framework that transforms human preference for speech expressiveness into a reliable, objective score. Building on established theories in phonetics (e.g., intonational phonology  [7] ) and psychology (e.g., the circumplex model of affect  [8] ), we define expressiveness along three core dimensions: Emotion, Prosody, and Spontaneity. We then train a unified model to capture these dimensions and align them with human preference, producing a single expressiveness score.\n\nEmail: 224040288@link.cuhk.edu.cn * Email: wangbenyou@cuhk.edu.cn Notably, our method achieves a Spearman correlation of 0.86 with human perception using fewer than 500 annotated samples, making it both data-efficient and practically scalable.\n\nTo demonstrate its utility, we apply DeEAR in two key tasks. First, DeEAR provides a reliable and convenient framework for quantifying speech expressiveness through objective scores. It demonstrates strong consistency, achieving a high correlation with human rankings of systems (SRCC = 0.96), and exhibits strong discriminative power. For example, when comparing state-of-the-art dialogue systems, the gap between the highest-scoring (DouBao) and lowest-scoring (Qwen2.5-Omni) models reaches 60.1 points.\n\nSecond, DeEAR can also be used to curate data, selecting highly expressive speech to support the training of more expressive TTS or S2S models. In practice, we applied DeEAR to several open-source datasets with potential expressiveness (e.g., Expresso  [9] , NCSSD  [10] ), using a threshold of 63.5 to extract approximately 14K utterances, named ExpressiveSpeech. We then fine-tuned an S2S model with this curated dataset, which led to a substantial improvement in expressiveness: the overall expressiveness score rose from 2.0 to 23.4. All three sub-dimensions improved, with particularly notable gains in emotion (from 5.7 to 15.9) and spontaneity (from 33.7 to 62.0).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Deear",
      "text": "This section introduces the methodology of DeEAR. Scoring an abstract concept like expressiveness with a single model is unreliable due to limited training data. To address this, we follow four principles: (1) decompose the expressiveness into concrete, solvable tasks;\n\n(2) design specialized models for each task to ensure accuracy; (3) align outputs with human preference using limited but interpretable data; and (4) enhance efficiency and scalability. These principles are ultimately instantiated in a four-stage pipeline (Figure  1 (A) ).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Decomposing Expressiveness For Alignment",
      "text": "Drawing from linguistics, psychology, and computational paralinguistics  [11, 12, 8] , we decompose expressiveness into three complementary dimensions. (i) Emotion intensity, a central element of expressiveness  [12] , is measured by arousal  [8]  and correlates with acoustic cues such as pitch range and intensity  [13] . (ii) Prosody, the melody and rhythm of speech, is fundamental to expressiveness as it conveys the speaker's attitudes and intentions beyond the literal words  [7, 14, 15] . (iii) Spontaneity reflects perceived authenticity, which listeners infer from acoustic cues such as disfluencies and variable prosody  [16, 17] . While theoretically distinct, the three components interact in human perception, leading us to design a modular system with a final fusion layer to combine them.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Prosody Scorer",
      "text": "Prompt-based Gemini 2.5 Pro: High human agreement (ρ = 0.73)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Spontaneity Scorer",
      "text": "Fine-tuned Wav2Vec with different levels of spontaneity data",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unifier Scorer",
      "text": "Fine-tuned Wav2Vec with Expressi data",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Expressive Dimensions",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "S Emotion S Prosody S Spontaneity",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Expressive Scorer",
      "text": "XGBoost (S emotion, S prosody, S spontaneity )\n\nThe unifier model replaces the three dimensions system.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proxy Modeling For Sub-Dimensions",
      "text": "We adopted a task-specific approach to measure each dimension, training specialized models where applicable. Specifically, we used supervised learning for the data-rich Emotion Intensity; leveraged a large language model to generate labels for the subjective and datascarce Prosodic Richness; and applied a hybrid rule-based heuristic for Spontaneity.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Intensity Scoring",
      "text": "Emotion intensity, defined here as arousal, is a well-established paralinguistic construct. The availability of large labeled datasets makes it well-suited to supervised learning. We therefore finetuned the state-of-the-art wav2vec2-large-robust-12-ftemotion-msp-dim model, which was already pre-trained for emotion recognition on English speech data. To improve bilingual performance, we further trained it on 12,000 Chinese samples from CNSCED  [18]  and 2,000 English samples from IEMOCAP  [19] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Prosodic Richness Scoring",
      "text": "Evaluating prosodic richness is challenging because it is subjective and lacks a large-scale labeled dataset. Traditional acoustic features often fail to distinguish engaging from unpleasant melodies.\n\nTo overcome these issues, we used LMMs as proxies for human perception, drawing on their ability to judge expressive qualities directly from audio. Using carefully engineered prompts, Gemini 2.5 Pro served as an automated annotator for prosodic quality. Its scores achieved a strong Spearman's rank correlation (SRCC=0.73) with human ratings, validating the approach and enabling scalable generation of consistent prosodic richness scores (Spros).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Spontaneity Scoring",
      "text": "Our scoring of spontaneity is based on the premise that perceived spontaneity (sounding unscripted) requires perceived naturalness (sounding human). This is similar to the speech uncanny valley effect  [20] , where technically perfect voices sound unnatural because they lack human-like imperfections  [21] . Recent studies confirm that this loss of naturalness also reduces perceived spontaneity  [22] . We call the cause of this problem perceptual incongruence: a mismatch between high acoustic quality and a non-human speech style.\n\nWe employ a two-stage, knowledge-guided supervised strategy.\n\nStage 1: Heuristic-Based Pseudo-Label Generation. We designed a heuristic function that combines a categorical base level of spontaneity, Lbase ∈ {1, 3, 5, 7, 9}, with an acoustic quality metric, Mavg. The base level is manually assigned at the dataset level. This metric is the mean of four DNSMOS outputs (OVRL, SIG, BAK, P.808 MOS)  [3] . The score is calculated conditionally: rewarding quality for congruent cases. In contrast, the map penalty (•) function performs a reverse linear scaling to a much narrower, predefined punitive range (e.g., [0.0, 0.5] for Lbase = 1). This aggressively penalizes perceptually incongruent samples that are too clean for their category.\n\nStage 2: Supervised Model Fine-tuning. We then used these pseudo-labels to fine-tune the same wav2vec2-large-robust model backbone used for emotion scoring. This process distills our explicit, knowledge-based heuristic into a robust deep learning model, creating the final spontaneity scorer.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Learning The Human Preference Fusion Function",
      "text": "The core of our alignment strategy lies in an explicit fusion function, engineered to model the complex, non-linear mapping from our subdimension scores to a holistic human judgment. This function is designed as a separate, lightweight module to ensure interpretability and fidelity to human preference data.\n\nTo model this, we collected a small dataset of 480 audio clips, for which three human annotators provided a single, overall expressiveness score. Using the three proxy scores (Semo, Spros, Sspon) as input features, we trained a XGBoost model  [23]  to predict the human-annotated overall score. The resulting model serves as our preference fusion function, capable of predicting a holistic expressiveness score by learning the complex interplay and non-linear trade-offs between the sub-dimensions directly from human preference data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Distillation And Decoupling For A Modular System",
      "text": "To convert the powerful but demanding teacher system for deployment, we employ a twofold strategy: knowledge distillation for efficiency and architecture decoupling for interpretability.\n\nIn the distillation step, the capabilities of the three proxy models are compressed into a single student model, DeEAR-Base. The teacher system is applied to 20,000 unlabeled utterances to produce pseudo-labels for Semo, Spros, and Sspon. DeEAR-Base adopts a wav2vec2-large-xlsr-53  [24]  backbone with three regression heads, jointly trained in a multi-task setup to predict the subdimensions, thus inheriting nuanced perceptual capabilities in a significantly more efficient form.\n\nIn the decoupling step, the final overall score Sexpr is not generated directly by DeEAR-Base; instead, its sub-scores are passed to an independently trained XGBoost fusion layer (Section 2.3). This modular design makes the preference logic explicit and detachable, allowing future updates without expensive retraining of the backbone. The combination of DeEAR-Base and the fusion layer constitutes the final DeEAR, which not only yields an objective expressiveness score but also supports practical uses such as filtering training data and guiding generative models (Figure  1 (B) ). For clarity, all scores from DeEAR-overall expressiveness (Sexpr) and the sub-dimensions of Emotion (Semo), Prosody (Spros), and Spontaneity (Sspon)-are presented on a 0-100 scale, where higher is better.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "High-Expressive Bilingual Dataset",
      "text": "Existing dialogue datasets often lack consistent vocal expressiveness. To address this gap, we developed ExpressiveSpeech, a real world dataset built specifically for high-quality, expressive speech.\n\nThe dataset contains approximately 14,000 utterances, totaling 51 hours, with a Chinese-English language ratio close to 1:1. It is composed of curated samples from five open-source emotional dialogue datasets: Expresso  [9] , NCSSD  [10] , M 3 ED  [25] , MultiDialog  [26] , and IEMOCAP  [19] . Our pipeline ensures that all selected data meets high standards for both acoustic quality and expressiveness. As shown in Table  1 , our dataset achieves a significantly higher average expressiveness score of 80.2 compared to its sources.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Curation Pipeline",
      "text": "Our curation pipeline consists of four main stages to ensure quality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Standardization And Enhancement:",
      "text": "We first standardized all audio to 16kHz mono and segmented multi-turn dialogues into single-speaker utterances. We used ClearerVoice  [27]  to remove background noise and separate overlapping speakers. This process significantly improved audio clarity.\n\nQuality and Expressiveness Scoring: We evaluated overall speech quality using DNSMOS P.835 OVRL score, achieving an average of 3.17. For expressiveness, we used DeEAR to assign scores to each utterance based on its Emotion, Prosody, and Spontaneity.\n\nHigh-Expressiveness Subset Selection: We set an expressiveness score threshold of 63.5 to select the final dataset. This value was determined empirically to align with human perception of high expressiveness. The threshold effectively selects samples that humans perceive as highly expressive and filters out utterances with low or unclear expressiveness.\n\nMetadata Organization: Finally, we generated text transcriptions for audio samples using Automatic Speech Recognition (ASR).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ethical Considerations And Licensing",
      "text": "The construction of ExpressiveSpeech adhered to strict ethical guidelines. It is derived from public, anonymized academic datasets containing no personally identifiable information (PII), and we followed all original data protocols. In line with the non-commercial restrictions of its sources, the dataset is released under the CC BY-NC-SA 4.0 license.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Validity: Alignment With Human Perception",
      "text": "DeEAR demonstrates a strong alignment with human perception of expressiveness. To validate this, we created four test sets, each containing 100 utterances. These sets were composed of diverse audio, including real-world conversations, professional recordings, and TTS-generated speech.\n\nWe then asked three graduate students in speech processing to independently rate each utterance on a 1-to-5 scale. The ratings followed a standardized protocol with clear definitions and anchor examples. The human judgments showed strong reliability, achieving a Krippendorff's alpha of α = 0.72. We averaged these ratings to create the final ground-truth score for our evaluation.\n\nAs shown in Table  2 , DeEAR's scores strongly correlate with the human ratings. For the overall expressiveness score (Sexpr), our metric achieved a Pearson Correlation Coefficient (PCC) of 0.91 and a Spearman's Rank Correlation Coefficient (SRCC) of 0.86. These high correlations provide compelling evidence that DeEAR accurately quantifies speech expressiveness. DeEAR enables reliable automated model benchmarking, achieving a near-perfect rank correlation (SRCC) of 0.96 with human evaluations. This capability addresses a critical need in the field, as benchmarking state-of-the-art (SOTA) models is vital for progress but is often limited by slow, expensive, and subjective listening tests.\n\nTo demonstrate this utility, we used DeEAR to rank seven leading S2S models, including both open-and closed-source systems. For a fair comparison, each model generated a response for the same 20 audio prompts, which covered a range of conversational emotions. We then compared the automated ranking with that from human listeners. This human ranking was created by four native speakers who rated each model's output on a 3-point MOS scale.\n\nThe results in Table  3  quantitatively substantiate our claim. Beyond the near-perfect rank correlation, the metric also demonstrates strong discriminative power, creating a wide overall score gap of nearly 60 points between the top and bottom-performing systems. This confirms that DeEAR can reliably replace manual evaluations for system-level model comparison, providing a scalable and objective solution to a key challenge in speech synthesis research. Table  3 . Automated benchmarking of SOTA models using DeEAR versus human evaluation. The rankings demonstrate a near-perfect align (SRCC = 0.96). The table presents scores for overall expressiveness (Sexpr) and its sub-dimensions, with final ranks in parentheses. Green and Red in the ranks indicate that the DeEAR rank is better or worse than the human rank, respectively. Having established DeEAR as a valid metric and benchmark, we demonstrate its utility in an evaluation-driven paradigm. We aim to prove that a reliable metric can guide data curation to systematically improve a model's expressive capability.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Design",
      "text": "To quantify the contribution of our method to high-quality data curation, we performed SFT on our S2S model Expressive-FT (Ours) with ExpressiveSpeech mentioned in Section 3. This model, analogous to architectures like MinMo  [28]     4 , our model significantly outperforms the baseline across all dimensions. The model's strong generalization, evidenced by the minimal performance drop on outof-domain data, stems from its gains being concentrated on highly transferable emotion and spontaneity cues. Our curation process prioritized these dimensions as they were the most significant deficiencies, leading to less focus on the comparatively higher-scoring baseline for prosody. T-tests confirmed that all reported gains are statistically significant (p < 0.001), underscoring the efficacy of our targeted data curation for both familiar and unseen data distributions.\n\nSubjective Results: Human evaluations corroborated these findings. In A/B preference tests, listeners favored our Expressive-FT model in 78.5% of cases, versus just 10% for the baseline, with 11.5% rated as ties. This strong preference is statistically significant (p < 0.001), providing ground-truth validation of our model's superior expressiveness.\n\nThe strong agreement between DeEAR's objective scores and human preference provides conclusive evidence for our central thesis: a powerful, human-aligned metric is the key to systematically and effectively developing more expressive conversational AI. Table  4 . Objective results for in-domain, out-of-domain, and overall test sets. The proposed Expressive-FT model consistently outperforms the baseline across expressiveness (Sexpr), emotion (Semo), prosody (Spros), and spontaneity (Sspon), with all gains statistically significant (p < 0.001).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Set Model Semo Spros Sspon Sexpr",
      "text": "In",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced DeEAR, a human-aligned and dataefficient metric for multi-dimensional speech expressiveness. By capturing Emotion, Prosody, and Spontaneity, DeEAR achieves strong correlation with human perception and scales beyond costly subjective evaluation. Leveraging this metric, we curated Expres-siveSpeech, a large-scale bilingual dataset of highly expressive speech, and fine-tuned a baseline S2S model to achieve substantial improvements in expressiveness. Our findings establish a paradigm of evaluation-driven data curation, underscoring that reliable metrics are crucial for advancing expressive speech synthesis. Future directions include extending DeEAR to reinforcement learning for end-to-end expressiveness optimization.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The DeEAR Framework: (A) The training follows a four-",
      "page": 2
    },
    {
      "caption": "Figure 1: (B)). For clar-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Li Auto Inc., China": "ABSTRACT"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "Recent\nspeech-to-speech\n(S2S) models\ngenerate\nintelligible"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "speech but still\nlack natural expressiveness,\nlargely due to the ab-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "sence of a reliable evaluation metric.\nExisting approaches,\nsuch"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "as\nsubjective MOS ratings,\nlow-level acoustic features, and emo-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "tion recognition are\ncostly,\nlimited,\nor\nincomplete.\nTo address"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "this, we present DeEAR (Decoding the Expressive Preference of"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "eAR), a framework that converts human preference for speech ex-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "pressiveness\ninto an objective score.\nGrounded in phonetics and"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "psychology, DeEAR evaluates\nspeech\nacross\nthree\ndimensions:"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "Emotion,\nProsody,\nand Spontaneity,\nachieving\nstrong\nalignment"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "with human perception (Spearman’s Rank Correlation Coefficient,"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "SRCC = 0.86) using fewer than 500 annotated samples. Beyond re-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "liable scoring, DeEAR enables fair benchmarking and targeted data"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "curation.\nIt not only distinguishes expressiveness gaps across S2S"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "models but also selects 14K expressive utterances to form Expres-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "siveSpeech, which improves the expressive score (from 2.0 to 23.4"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "on a 100-point scale) of S2S models. Demos and codes are avail-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "https://github.com/FreedomIntelligence/\nable\nat"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "ExpressiveSpeech"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "Index Terms— Speech expressiveness, objective metric, human"
        },
        {
          "2Li Auto Inc., China": "preference alignment, speech-to-speech models, data curation"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "1.\nINTRODUCTION"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "Recent end-to-end speech models can generate clear speech in Text-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "to-Speech (TTS) tasks. Yet\nin conversational settings,\ntheir output"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "often sounds robotic and lacks the expressiveness vital for applica-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "tions such as voice assistants, role-playing, and AI companions. The"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "core of this problem is the absence of a reliable evaluation metric."
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "While fields\nlike\nspeech recognition and audio enhancement"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "benefit from WER [1, 2] and DNSMOS [3], expressiveness still re-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "lies on subjective MOS [4], which is costly and unscalable. Existing"
        },
        {
          "2Li Auto Inc., China": "alternatives are limited:\nlow-level acoustic features [5] (e.g., pitch,"
        },
        {
          "2Li Auto Inc., China": "energy) miss perceptual subtleties, and emotion recognition [6] cap-"
        },
        {
          "2Li Auto Inc., China": "tures only one facet of expressiveness.\nIn short, a comprehensive,"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "human-aligned metric is urgently needed."
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "To address this gap, we introduce DeEAR, a novel framework"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "that\ntransforms human preference for speech expressiveness into a"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "reliable, objective score. Building on established theories in phonet-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "ics (e.g.,\nintonational phonology [7]) and psychology (e.g.,\nthe cir-"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "cumplex model of affect [8]), we define expressiveness along three"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "core dimensions: Emotion, Prosody, and Spontaneity. We then"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "train a unified model\nto capture these dimensions and align them"
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "with human preference, producing a single expressiveness\nscore."
        },
        {
          "2Li Auto Inc., China": ""
        },
        {
          "2Li Auto Inc., China": "Email: 224040288@link.cuhk.edu.cn"
        },
        {
          "2Li Auto Inc., China": "∗Email: wangbenyou@cuhk.edu.cn"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2.2. Prosodic Richness Scoring": "Evaluating prosodic richness is challenging because it\nis subjective"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "and lacks a large-scale labeled dataset. Traditional acoustic features"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "often fail to distinguish engaging from unpleasant melodies."
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "To overcome these issues, we used LMMs as proxies for human"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "perception, drawing on their ability to judge expressive qualities di-"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "rectly from audio. Using carefully engineered prompts, Gemini 2.5"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "Pro served as an automated annotator for prosodic quality. Its scores"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "achieved a strong Spearman’s rank correlation (SRCC=0.73) with"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "human ratings, validating the approach and enabling scalable gener-"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "ation of consistent prosodic richness scores (Spros)."
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "2.2.3.\nSpontaneity Scoring"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "Our scoring of spontaneity is based on the premise that perceived"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "spontaneity\n(sounding\nunscripted)\nrequires\nperceived\nnaturalness"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "(sounding human). This is similar to the speech uncanny valley ef-"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "fect [20], where technically perfect voices sound unnatural because"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "they lack human-like imperfections [21]. Recent studies confirm that"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "this loss of naturalness also reduces perceived spontaneity [22]. We"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "call the cause of this problem perceptual incongruence: a mismatch"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "between high acoustic quality and a non-human speech style."
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "We employ a two-stage, knowledge-guided supervised strategy."
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "Stage 1: Heuristic-Based Pseudo-Label Generation. We de-"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "signed a heuristic function that combines a categorical base level of"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "spontaneity, Lbase ∈ {1, 3, 5, 7, 9}, with an acoustic quality metric,"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "Mavg. The base level is manually assigned at the dataset level. This"
        },
        {
          "2.2.2. Prosodic Richness Scoring": ""
        },
        {
          "2.2.2. Prosodic Richness Scoring": "metric is the mean of\nfour DNSMOS outputs (OVRL, SIG, BAK,"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "P.808 MOS) [3]. The score is calculated conditionally:"
        },
        {
          "2.2.2. Prosodic Richness Scoring": "(cid:40)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(cid:40)": "if hyper-clean and Lbase < Lmax\nmappenalty(Mavg)"
        },
        {
          "(cid:40)": "(1)\nSspon ="
        },
        {
          "(cid:40)": "otherwise\nmapnormal(Mavg)"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "A sample is considered hyper-clean when all four underlying DNS-"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "MOS metrics exceed a threshold Tq = 3.5. The mapnormal(·) func-"
        },
        {
          "(cid:40)": "tion linearly scales Mavg to a target range (e.g., [Lbase −1, Lbase +1]),"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "rewarding quality for congruent cases.\nIn contrast,\nthe mappenalty(·)"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "function performs a reverse linear scaling to a much narrower, pre-"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "defined punitive range (e.g.,\n[0.0, 0.5] for Lbase = 1). This aggres-"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "sively penalizes perceptually incongruent samples that are too clean"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "for their category."
        },
        {
          "(cid:40)": "Stage 2: Supervised Model Fine-tuning. We then used these"
        },
        {
          "(cid:40)": "pseudo-labels to fine-tune the same wav2vec2-large-robust"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "model backbone used for emotion scoring.\nThis process distills"
        },
        {
          "(cid:40)": "our explicit, knowledge-based heuristic into a robust deep learning"
        },
        {
          "(cid:40)": "model, creating the final spontaneity scorer."
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "2.3. Learning the Human Preference Fusion Function"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "The core of our alignment strategy lies in an explicit fusion function,"
        },
        {
          "(cid:40)": "engineered to model the complex, non-linear mapping from our sub-"
        },
        {
          "(cid:40)": "dimension scores to a holistic human judgment.\nThis function is"
        },
        {
          "(cid:40)": ""
        },
        {
          "(cid:40)": "designed as a separate, lightweight module to ensure interpretability"
        },
        {
          "(cid:40)": "and fidelity to human preference data."
        },
        {
          "(cid:40)": "To model\nthis, we collected a small dataset of 480 audio clips,"
        },
        {
          "(cid:40)": "for which three human annotators provided a single, overall expres-"
        },
        {
          "(cid:40)": "siveness\nscore.\nUsing the three proxy scores\n(Semo, Spros, Sspon)"
        },
        {
          "(cid:40)": "as input\nfeatures, we trained a XGBoost model\n[23]\nto predict\nthe"
        },
        {
          "(cid:40)": "human-annotated overall score. The resulting model serves as our"
        },
        {
          "(cid:40)": "preference fusion function, capable of predicting a holistic expres-"
        },
        {
          "(cid:40)": "siveness\nscore by learning the\ncomplex interplay and non-linear"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: , DeEAR’s scores strongly correlate with",
      "data": [
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "ence data.",
          "Standardization and Enhancement: We first\nstandardized": "all audio to 16kHz mono and segmented multi-turn dialogues into"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "single-speaker utterances. We used ClearerVoice [27]\nto remove"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "background noise and separate overlapping speakers. This process"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "2.4. Distillation and Decoupling for a Modular System",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "significantly improved audio clarity."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "To convert\nthe powerful but demanding teacher system for deploy-",
          "Standardization and Enhancement: We first\nstandardized": "Quality and Expressiveness Scoring: We evaluated overall"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "ment, we employ a twofold strategy: knowledge distillation for effi-",
          "Standardization and Enhancement: We first\nstandardized": "speech quality using DNSMOS P.835 OVRL score, achieving an av-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "ciency and architecture decoupling for interpretability.",
          "Standardization and Enhancement: We first\nstandardized": "erage of 3.17. For expressiveness, we used DeEAR to assign scores"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "In the distillation step,\nthe capabilities of the three proxy mod-",
          "Standardization and Enhancement: We first\nstandardized": "to each utterance based on its Emotion, Prosody, and Spontaneity."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "els are compressed into a single student model, DeEAR-Base. The",
          "Standardization and Enhancement: We first\nstandardized": "High-Expressiveness Subset Selection: We set an expressive-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "teacher system is applied to 20,000 unlabeled utterances to produce",
          "Standardization and Enhancement: We first\nstandardized": "ness score threshold of 63.5 to select the final dataset. This value was"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "pseudo-labels\nDeEAR-Base adopts a\nfor Semo, Spros,\nand Sspon.",
          "Standardization and Enhancement: We first\nstandardized": "determined empirically to align with human perception of high ex-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "wav2vec2-large-xlsr-53 [24] backbone with three regres-",
          "Standardization and Enhancement: We first\nstandardized": "pressiveness. The threshold effectively selects samples that humans"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "sion heads,\njointly trained in a multi-task setup to predict\nthe sub-",
          "Standardization and Enhancement: We first\nstandardized": "perceive as highly expressive and filters out utterances with low or"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "dimensions, thus inheriting nuanced perceptual capabilities in a sig-",
          "Standardization and Enhancement: We first\nstandardized": "unclear expressiveness."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "nificantly more efficient form.",
          "Standardization and Enhancement: We first\nstandardized": "Metadata Organization: Finally, we generated text\ntranscrip-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "is not gener-\nIn the decoupling step, the final overall score Sexpr",
          "Standardization and Enhancement: We first\nstandardized": "tions for audio samples using Automatic Speech Recognition (ASR)."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "ated directly by DeEAR-Base;\ninstead,\nits sub-scores are passed to",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "an independently trained XGBoost fusion layer (Section 2.3). This",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "3.2. Ethical Considerations and Licensing"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "modular design makes the preference logic explicit and detachable,",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "allowing future updates without expensive retraining of\nthe back-",
          "Standardization and Enhancement: We first\nstandardized": "The\nconstruction\nof ExpressiveSpeech\nadhered\nto\nstrict\nethical"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "bone. The combination of DeEAR-Base and the fusion layer con-",
          "Standardization and Enhancement: We first\nstandardized": "guidelines. It is derived from public, anonymized academic datasets"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "stitutes\nthe final DeEAR, which not only yields an objective ex-",
          "Standardization and Enhancement: We first\nstandardized": "containing no personally identifiable information (PII), and we fol-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "pressiveness score but also supports practical uses such as filtering",
          "Standardization and Enhancement: We first\nstandardized": "lowed all original data protocols.\nIn line with the non-commercial"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "training data and guiding generative models (Figure 1 (B)). For clar-",
          "Standardization and Enhancement: We first\nstandardized": "restrictions of\nits\nsources,\nthe dataset\nis\nreleased under\nthe CC"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "ity, all scores from DeEAR—overall expressiveness (Sexpr) and the",
          "Standardization and Enhancement: We first\nstandardized": "BY-NC-SA 4.0 license."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "sub-dimensions of Emotion (Semo), Prosody (Spros), and Spontaneity",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "(Sspon)—are presented on a 0-100 scale, where higher is better.",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "4. EXPERIMENTS"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "3. HIGH-EXPRESSIVE BILINGUAL DATASET",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "4.1. Validity: Alignment with Human Perception"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "Existing dialogue datasets often lack consistent vocal expressive-",
          "Standardization and Enhancement: We first\nstandardized": "DeEAR demonstrates a strong alignment with human perception of"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "ness. To address this gap, we developed ExpressiveSpeech, a real",
          "Standardization and Enhancement: We first\nstandardized": "expressiveness. To validate this, we created four test sets, each con-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "world dataset built specifically for high-quality, expressive speech.",
          "Standardization and Enhancement: We first\nstandardized": "taining 100 utterances.\nThese sets were composed of diverse au-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "The dataset contains approximately 14,000 utterances,\ntotaling",
          "Standardization and Enhancement: We first\nstandardized": "dio, including real-world conversations, professional recordings, and"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "51 hours, with a Chinese-English language ratio close to 1:1.\nIt\nis",
          "Standardization and Enhancement: We first\nstandardized": "TTS-generated speech."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "composed of curated samples from five open-source emotional dia-",
          "Standardization and Enhancement: We first\nstandardized": "We then asked three graduate students in speech processing to"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "logue datasets: Expresso [9], NCSSD [10], M3ED [25], MultiDia-",
          "Standardization and Enhancement: We first\nstandardized": "independently rate each utterance on a 1-to-5 scale. The ratings fol-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "log [26], and IEMOCAP [19]. Our pipeline ensures that all selected",
          "Standardization and Enhancement: We first\nstandardized": "lowed a standardized protocol with clear definitions and anchor ex-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "data meets high standards for both acoustic quality and expressive-",
          "Standardization and Enhancement: We first\nstandardized": "amples. The human judgments showed strong reliability, achieving"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "ness. As shown in Table 1, our dataset achieves a significantly higher",
          "Standardization and Enhancement: We first\nstandardized": "a Krippendorff’s alpha of α = 0.72. We averaged these ratings to"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "average expressiveness score of 80.2 compared to its sources.",
          "Standardization and Enhancement: We first\nstandardized": "create the final ground-truth score for our evaluation."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "As shown in Table 2, DeEAR’s scores strongly correlate with"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "the human ratings. For the overall expressiveness score (Sexpr), our"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "Table 1. Comparison of ExpressiveSpeech with its source datasets.",
          "Standardization and Enhancement: We first\nstandardized": "metric achieved a Pearson Correlation Coefficient (PCC) of 0.91 and"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "is the\nLexpr marks datasets with explicit expressiveness labels. Sexpr",
          "Standardization and Enhancement: We first\nstandardized": "a Spearman’s Rank Correlation Coefficient (SRCC) of 0.86. These"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "average expressiveness scored from our DeEAR, with the highest",
          "Standardization and Enhancement: We first\nstandardized": "high correlations provide compelling evidence that DeEAR accu-"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "score highlighted in bold.",
          "Standardization and Enhancement: We first\nstandardized": "rately quantifies speech expressiveness."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "Dataset\nLanguage\nDuration(h)\nLexpr\nSexpr",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "✗\nMultidialog [26]\nEN\n340\n39.4",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "Table 2.\nCorrelation between DeEAR scores and human ratings."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "✗\nM3ED [25]\nZH\n14\n49.9",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "Pearson (PCC) and Spearman (SRCC) coefficients are reported for"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "✗\nNCSSD [10]\nEN, ZH\n236\n50.1",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "three dimensions (Emotion, Prosody, Spontaneity) and the overall"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "✗\nIEMOCAP [19]\nEN\n12\n50.9",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "expressiveness score."
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "✗\nExpresso [9]\nEN\n46\n62.9",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "Dimension\nPCC\nSRCC"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "✓\n80.2\nExpressiveSpeech\nEN, ZH\n51",
          "Standardization and Enhancement: We first\nstandardized": ""
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "0.72\n0.65\nEmotion (Semo)"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "0.70\n0.68\nProsody (Spros)"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "",
          "Standardization and Enhancement: We first\nstandardized": "0.84\n0.84\nSpontaneity (Sspon)"
        },
        {
          "trade-offs between the sub-dimensions directly from human prefer-": "3.1. Data Curation Pipeline",
          "Standardization and Enhancement: We first\nstandardized": "0.91\n0.86\nExpressiveness (Sexpr)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "jective scoring with DeEAR and a subjective A/B preference test"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "DeEAR enables reliable automated model benchmarking, achieving",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "with 10 native speakers, who chose the more expressive output or"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "a near-perfect rank correlation (SRCC) of 0.96 with human evalua-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "declared a tie."
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "tions. This capability addresses a critical need in the field, as bench-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "marking state-of-the-art (SOTA) models is vital for progress but\nis",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "often limited by slow, expensive, and subjective listening tests.",
          "like Emilia) data to test generalization. The evaluation involved ob-": "4.3.2. Results and Analysis"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "To demonstrate this utility, we used DeEAR to rank seven lead-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "DeEAR successfully guides data curation, yielding a model of supe-"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "ing S2S models,\nincluding both open- and closed-source systems.",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "rior expressiveness."
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "For a fair comparison, each model generated a response for the same",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Objective Results: As shown in Table 4, our model significantly"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "20 audio prompts, which covered a range of conversational emo-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "outperforms the baseline across all dimensions. The model’s strong"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "tions. We then compared the automated ranking with that from hu-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "generalization, evidenced by the minimal performance drop on out-"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "man listeners. This human ranking was created by four native speak-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "of-domain data, stems from its gains being concentrated on highly"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "ers who rated each model’s output on a 3-point MOS scale.",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "transferable emotion and spontaneity cues.\nOur curation process"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "The results in Table 3 quantitatively substantiate our claim. Be-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "prioritized these dimensions as they were the most significant de-"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "yond the near-perfect rank correlation, the metric also demonstrates",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "ficiencies, leading to less focus on the comparatively higher-scoring"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "strong discriminative power, creating a wide overall\nscore gap of",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "baseline for prosody.\nT-tests confirmed that all\nreported gains are"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "nearly 60 points between the top and bottom-performing systems.",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "statistically significant (p < 0.001), underscoring the efficacy of our"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "This confirms that DeEAR can reliably replace manual evaluations",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "targeted data curation for both familiar and unseen data distributions."
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "for system-level model comparison, providing a scalable and objec-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Subjective Results:\nHuman\nevaluations\ncorroborated\nthese"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "tive solution to a key challenge in speech synthesis research.",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "findings.\nIn A/B preference tests,\nlisteners favored our Expressive-"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "FT model in 78.5% of cases, versus just 10% for the baseline, with"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "11.5% rated as ties.\nThis strong preference is statistically signifi-"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "Table 3. Automated benchmarking of SOTA models using DeEAR",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "cant (p < 0.001), providing ground-truth validation of our model’s"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "versus human evaluation. The rankings demonstrate a near-perfect",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "superior expressiveness."
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "align (SRCC = 0.96). The table presents scores for overall expres-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "The strong agreement between DeEAR’s objective scores and"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "siveness (Sexpr) and its sub-dimensions, with final ranks in parenthe-",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "human preference provides conclusive evidence for our central\nthe-"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "ses. Green and Red in the ranks indicate that\nthe DeEAR rank is",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "sis:\na powerful, human-aligned metric is the key to systematically"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "better or worse than the human rank, respectively.",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "and effectively developing more expressive conversational AI."
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "DeEAR Scores",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "Model\nHuman",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "Semo\nSpros\nSspon\nSexpr",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Table 4. Objective results for in-domain, out-of-domain, and over-"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "67.7\n58.6\n92.5\n65.4(1)\n84.2(1)\nDoubao",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "all\ntest sets.\nThe proposed Expressive-FT model consistently out-"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "64.8\n51.7\n76.8\n45.2(2)\n80.8(2)\nGrok-4 Voice",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "performs the baseline across expressiveness (Sexpr), emotion (Semo),"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "56.2\n39.4\n67.4\n31.1(4)\n66.3(3)\nGPT-4o Audio",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "prosody (Spros), and spontaneity (Sspon), with all gains statistically"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "40.9\n33.2\n88.4\n44.9(3)\n56.1(4)\nSesame",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "significant (p < 0.001)."
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "44.2\n34.3\n69.4\n29.3(5)\n42.9(5)\nStep Audio 2",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Set\nModel\nSemo\nSpros\nSspon\nSexpr"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "44.4\n37.6\n31.9\n5.3(7)\n41.2(6)\nQwen2.5-Omni",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "39.5\n30.3\n40.1\n7.0(6)\n34.7(7)\nGemini-2.5 Pro",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Baseline\n5.9\n35.6\n34.1\n2.3"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "In-domain"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Ours\n15.8\n35.8\n62.9\n24.0"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Baseline\n5.4\n36.3\n33.2\n1.8"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Out-of-domain"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Ours\n15.9\n37.6\n61.1\n23.0"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "4.3. Application 2: Evaluation-driven Data Curation",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Baseline\n5.7\n35.7\n33.7\n2.0"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Overall"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "Having established DeEAR as a valid metric and benchmark, we",
          "like Emilia) data to test generalization. The evaluation involved ob-": "Ours\n15.9\n36.7\n62.0\n23.4"
        },
        {
          "4.2. Application 1: Automated Benchmarking of SOTA Models": "demonstrate its utility in an evaluation-driven paradigm. We aim to",
          "like Emilia) data to test generalization. The evaluation involved ob-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "vs. read speech in story-telling tasks–evidence from intonation"
        },
        {
          "6. REFERENCES": "[1] Ahmed Ali and Steve Renals,\n“Word error rate estimation for",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "analysis using gtobi,” Journal of Phonetics, vol. 48, pp. 29–44,"
        },
        {
          "6. REFERENCES": "the 56th An-\nspeech recognition:\ne-wer,”\nin Proceedings of",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "2015."
        },
        {
          "6. REFERENCES": "nual Meeting of the Association for Computational Linguistics",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[18] Xiaolong Wu, Chaobo Song, Shanshan Xiang, Ronghe Cao,"
        },
        {
          "6. REFERENCES": "(Volume 2: Short Papers). Association for Computational Lin-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Chang Feng, Hankiz Yilahun, Mingxing Xu, Askar Hamdulla,"
        },
        {
          "6. REFERENCES": "guistics (ACL), 2018, pp. 20–24.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "and Thomas Fang Zheng,\n“A chinese natural speech complex"
        },
        {
          "6. REFERENCES": "[2] Andrew Cameron Morris, Viktoria Maier, and Phil D Green,",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "emotion dataset based on emotion vector annotation method:"
        },
        {
          "6. REFERENCES": "“From wer and ril\nto mer and wil:\nimproved evaluation mea-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "X. wu et al.,” Language Resources and Evaluation, pp. 1–22,"
        },
        {
          "6. REFERENCES": "sures for connected speech recognition.,” in Interspeech, 2004,",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "2025."
        },
        {
          "6. REFERENCES": "pp. 2765–2768.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[19] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "6. REFERENCES": "[3] Chandan KA Reddy, Vishak Gopal, and Ross Cutler,\n“Dns-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "6. REFERENCES": "mos: A non-intrusive perceptual objective speech quality met-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:"
        },
        {
          "6. REFERENCES": "ric to evaluate noise suppressors,” in ICASSP 2021-2021 IEEE",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Lan-\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "6. REFERENCES": "International Conference on Acoustics, Speech and Signal Pro-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "guage resources and evaluation, vol. 42, no. 4, pp. 335–359,"
        },
        {
          "6. REFERENCES": "cessing (ICASSP). IEEE, 2021, pp. 6493–6497.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "2008."
        },
        {
          "6. REFERENCES": "[4]\nS´ebastien Le Maguer, Simon King, and Naomi Harte,\n“The",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[20] Harm Lameris, Shivam Mehta, Gustav Eje Henter,\nJoakim"
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "´\nGustafson,\nand\nEva Sz´ekely,\n“Prosody-controllable sponta-"
        },
        {
          "6. REFERENCES": "limits of\nthe mean opinion score for speech synthesis evalu-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "ation,”\nComputer Speech & Language, vol. 84, pp. 101577,",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "neous tts with neural hmms,”\nin ICASSP 2023-2023 IEEE In-"
        },
        {
          "6. REFERENCES": "2024.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "ternational Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "cessing (ICASSP). IEEE, 2023, pp. 1–5."
        },
        {
          "6. REFERENCES": "[5] Berrak Sisman, Junichi Yamagishi, Simon King, and Haizhou",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "Li, “An overview of voice conversion and its challenges: From",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[21] Yuta Matsunaga,\nTakaaki\nSaeki,\nShinnosuke\nTakamichi,"
        },
        {
          "6. REFERENCES": "IEEE/ACM Transac-\nstatistical modeling to deep learning,”",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "and Hiroshi Saruwatari,\n“Improving\nrobustness\nof\nspon-"
        },
        {
          "6. REFERENCES": "tions on Audio, Speech, and Language Processing, vol. 29, pp.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "taneous\nspeech synthesis with linguistic\nspeech regulariza-"
        },
        {
          "6. REFERENCES": "132–157, 2020.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv\npreprint\ntion\nand\npseudo-filled-pause\ninsertion,”"
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv:2210.09815, 2022."
        },
        {
          "6. REFERENCES": "[6] Elisa Straulino, Cristina Scarpazza, and Luisa Sartori,\n“What",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "is missing in the study of emotion expression?,” Frontiers in",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[22] Weiqin Li, Shun Lei, Qiaochu Huang, Yixuan Zhou, Zhiy-"
        },
        {
          "6. REFERENCES": "Psychology, vol. 14, pp. 1158136, 2023.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "ong Wu, Shiyin Kang,\nand Helen Meng,\n“Towards\nspon-"
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "taneous\nstyle modeling with\nsemi-supervised\npre-training"
        },
        {
          "6. REFERENCES": "[7] D Robert Ladd,\nIntonational phonology, Cambridge Univer-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv preprint\nfor conversational\ntext-to-speech synthesis,”"
        },
        {
          "6. REFERENCES": "sity Press, 2008.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv:2308.16593, 2023."
        },
        {
          "6. REFERENCES": "Journal\n[8]\nJames A Russell,\n“A circumplex model of affect.,”",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[23] Tianqi Chen and Carlos Guestrin,\n“Xgboost: A scalable tree"
        },
        {
          "6. REFERENCES": "of personality and social psychology, vol. 39, no. 6, pp. 1161,",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "boosting system,” in Proceedings of the 22nd acm sigkdd inter-"
        },
        {
          "6. REFERENCES": "1980.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "national conference on knowledge discovery and data mining,"
        },
        {
          "6. REFERENCES": "[9] Tu Anh Nguyen, Wei-Ning Hsu, Antony d’Avirro, Bowen",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "2016, pp. 785–794."
        },
        {
          "6. REFERENCES": "Shi,\nItai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet,",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[24] Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdel-"
        },
        {
          "6. REFERENCES": "Gabriel Synnaeve, Michael Hassid, et al., “Expresso: A bench-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "rahman Mohamed, and Michael Auli,\n“Unsupervised cross-"
        },
        {
          "6. REFERENCES": "mark and analysis of discrete expressive speech resynthesis,”",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv\nlingual representation learning for speech recognition,”"
        },
        {
          "6. REFERENCES": "arXiv preprint arXiv:2308.05725, 2023.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "preprint arXiv:2006.13979, 2020."
        },
        {
          "6. REFERENCES": "[10] Rui Liu, Yifan Hu, Yi Ren, Xiang Yin, and Haizhou Li, “Gen-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[25]\nJinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen Liu, Qin"
        },
        {
          "6. REFERENCES": "erative expressive conversational speech synthesis,”\nin Pro-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Jin, Xinchao Wang, and Haizhou Li,\n“M3ed: Multi-modal"
        },
        {
          "6. REFERENCES": "ceedings of the 32nd ACM International Conference on Multi-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv\nmulti-scene multi-label emotional dialogue database,”"
        },
        {
          "6. REFERENCES": "media, 2024, pp. 4187–4196.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "preprint arXiv:2205.10237, 2022."
        },
        {
          "6. REFERENCES": "Computational paralin-\n[11] Bj¨orn Schuller and Anton Batliner,",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[26]\nSe Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim,"
        },
        {
          "6. REFERENCES": "guistics:\nemotion, affect and personality in speech and lan-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Joanna Hong, Jeong Hun Yeo, and Yong Man Ro,\n“Let’s go"
        },
        {
          "6. REFERENCES": "guage processing, John Wiley & Sons, 2013.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "real\ntalk:\nSpoken dialogue model\nfor\nface-to-face conversa-"
        },
        {
          "6. REFERENCES": "[12] Klaus R Scherer, “Vocal communication of emotion: A review",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "tion,” arXiv preprint arXiv:2406.07867, 2024."
        },
        {
          "6. REFERENCES": "of\nresearch paradigms,”\nSpeech communication, vol. 40, no.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[27]\nShengkui Zhao, Zexu Pan, and Bin Ma, “Clearervoice-studio:"
        },
        {
          "6. REFERENCES": "1-2, pp. 227–256, 2003.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Bridging advanced speech processing research and practical"
        },
        {
          "6. REFERENCES": "[13] Rainer Banse and Klaus R Scherer, “Acoustic profiles in vocal",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "deployment,” arXiv preprint arXiv:2506.19398, 2025."
        },
        {
          "6. REFERENCES": "Journal of personality and social psy-\nemotion expression.,”",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[28] Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda"
        },
        {
          "6. REFERENCES": "chology, vol. 70, no. 3, pp. 614, 1996.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Chen,\nChong Deng,\nZhihao Du,\nRuize Gao,\nChangfeng"
        },
        {
          "6. REFERENCES": "[14] Carlos Gussenhoven, “The phonology of tone and intonation,”",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Gao, Zhifu Gao,\net al.,\n“Minmo: A multimodal\nlarge lan-"
        },
        {
          "6. REFERENCES": "2004.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv preprint\nguage model\nfor seamless voice interaction,”"
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv:2501.06282, 2025."
        },
        {
          "6. REFERENCES": "[15]\nPilar Prieto,\n“Intonational meaning,” Wiley Interdisciplinary",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "Reviews: Cognitive Science, vol. 6, no. 4, pp. 371–381, 2015.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "[29]\nJin Xu, Zhifang Guo,\nJinzheng He, Hangrui Hu, Ting He,"
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "Shuai Bai, Keqin Chen,\nJialin Wang, Yang Fan, Kai Dang,"
        },
        {
          "6. REFERENCES": "[16] Elizabeth Shriberg,\n“Spontaneous\nspeech:\nhow people re-",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv preprint\net\nal.,\n“Qwen2. 5-omni\ntechnical\nreport,”"
        },
        {
          "6. REFERENCES": "ally talk and why engineers should care.,” in INTERSPEECH,",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        },
        {
          "6. REFERENCES": "",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": "arXiv:2503.20215, 2025."
        },
        {
          "6. REFERENCES": "2005, pp. 1781–1784.",
          "[17] Laura E De Ruiter, “Information status marking in spontaneous": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Word error rate estimation for speech recognition: e-wer",
      "authors": [
        "Ahmed Ali",
        "Steve Renals"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "3",
      "title": "From wer and ril to mer and wil: improved evaluation measures for connected speech recognition",
      "authors": [
        "Andrew Cameron",
        "Viktoria Maier",
        "Phil Green"
      ],
      "year": "2004",
      "venue": "From wer and ril to mer and wil: improved evaluation measures for connected speech recognition"
    },
    {
      "citation_id": "4",
      "title": "Dnsmos: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors",
      "authors": [
        "K Chandan",
        "Vishak Reddy",
        "Ross Gopal",
        "Cutler"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "The limits of the mean opinion score for speech synthesis evaluation",
      "authors": [
        "Le Sébastien",
        "Simon Maguer",
        "Naomi King",
        "Harte"
      ],
      "year": "2024",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "6",
      "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
      "authors": [
        "Berrak Sisman",
        "Junichi Yamagishi",
        "Simon King",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "What is missing in the study of emotion expression?",
      "authors": [
        "Elisa Straulino",
        "Cristina Scarpazza",
        "Luisa Sartori"
      ],
      "year": "2023",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "8",
      "title": "Intonational phonology",
      "authors": [
        "Robert Ladd"
      ],
      "year": "2008",
      "venue": "Intonational phonology"
    },
    {
      "citation_id": "9",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "10",
      "title": "Expresso: A benchmark and analysis of discrete expressive speech resynthesis",
      "authors": [
        "Anh Tu",
        "Wei-Ning Nguyen",
        "Antony Hsu",
        "Bowen Avirro",
        "Itai Shi",
        "Maryam Gat",
        "Tal Fazel-Zarani",
        "Jade Remez",
        "Gabriel Copet",
        "Michael Synnaeve",
        "Hassid"
      ],
      "year": "2023",
      "venue": "Expresso: A benchmark and analysis of discrete expressive speech resynthesis",
      "arxiv": "arXiv:2308.05725"
    },
    {
      "citation_id": "11",
      "title": "Generative expressive conversational speech synthesis",
      "authors": [
        "Rui Liu",
        "Yifan Hu",
        "Yi Ren",
        "Xiang Yin",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "Björn Schuller",
        "Anton Batliner"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "13",
      "title": "Vocal communication of emotion: A review of research paradigms",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "14",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "Rainer Banse",
        "Klaus Scherer"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "15",
      "title": "The phonology of tone and intonation",
      "authors": [
        "Carlos Gussenhoven"
      ],
      "year": "2004",
      "venue": "The phonology of tone and intonation"
    },
    {
      "citation_id": "16",
      "title": "Intonational meaning",
      "authors": [
        "Pilar Prieto"
      ],
      "year": "2015",
      "venue": "Wiley Interdisciplinary Reviews: Cognitive Science"
    },
    {
      "citation_id": "17",
      "title": "Spontaneous speech: how people really talk and why engineers should care",
      "authors": [
        "Elizabeth Shriberg"
      ],
      "year": "2005",
      "venue": "Spontaneous speech: how people really talk and why engineers should care"
    },
    {
      "citation_id": "18",
      "title": "Information status marking in spontaneous vs. read speech in story-telling tasks-evidence from intonation analysis using gtobi",
      "authors": [
        "Laura E De Ruiter"
      ],
      "year": "2015",
      "venue": "Journal of Phonetics"
    },
    {
      "citation_id": "19",
      "title": "A chinese natural speech complex emotion dataset based on emotion vector annotation method",
      "authors": [
        "Xiaolong Wu",
        "Chaobo Song",
        "Shanshan Xiang",
        "Ronghe Cao",
        "Chang Feng",
        "Hankiz Yilahun",
        "Mingxing Xu",
        "Askar Hamdulla",
        "Thomas Fang Zheng",
        "; Wu"
      ],
      "year": "2025",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Prosody-controllable spontaneous tts with neural hmms",
      "authors": [
        "Harm Lameris",
        "Shivam Mehta",
        "Gustav Henter",
        "Joakim Gustafson",
        "Éva Székely"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "22",
      "title": "Improving robustness of spontaneous speech synthesis with linguistic speech regularization and pseudo-filled-pause insertion",
      "authors": [
        "Yuta Matsunaga",
        "Takaaki Saeki",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari"
      ],
      "year": "2022",
      "venue": "Improving robustness of spontaneous speech synthesis with linguistic speech regularization and pseudo-filled-pause insertion",
      "arxiv": "arXiv:2210.09815"
    },
    {
      "citation_id": "23",
      "title": "Towards spontaneous style modeling with semi-supervised pre-training for conversational text-to-speech synthesis",
      "authors": [
        "Weiqin Li",
        "Shun Lei",
        "Qiaochu Huang",
        "Yixuan Zhou",
        "Zhiyong Wu",
        "Shiyin Kang",
        "Helen Meng"
      ],
      "year": "2023",
      "venue": "Towards spontaneous style modeling with semi-supervised pre-training for conversational text-to-speech synthesis",
      "arxiv": "arXiv:2308.16593"
    },
    {
      "citation_id": "24",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "Tianqi Chen",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised crosslingual representation learning for speech recognition",
      "authors": [
        "Alexis Conneau",
        "Alexei Baevski",
        "Ronan Collobert",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised crosslingual representation learning for speech recognition",
      "arxiv": "arXiv:2006.13979"
    },
    {
      "citation_id": "26",
      "title": "M3ed: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "M3ed: Multi-modal multi-scene multi-label emotional dialogue database",
      "arxiv": "arXiv:2205.10237"
    },
    {
      "citation_id": "27",
      "title": "Let's go real talk: Spoken dialogue model for face-to-face conversation",
      "authors": [
        "Jin Se",
        "Chae Park",
        "Hyeongseop Kim",
        "Minsu Rha",
        "Joanna Kim",
        "Jeong Hong",
        "Yong Hun Yeo",
        "Ro"
      ],
      "year": "2024",
      "venue": "Let's go real talk: Spoken dialogue model for face-to-face conversation",
      "arxiv": "arXiv:2406.07867"
    },
    {
      "citation_id": "28",
      "title": "Clearervoice-studio: Bridging advanced speech processing research and practical deployment",
      "authors": [
        "Shengkui Zhao",
        "Zexu Pan",
        "Bin Ma"
      ],
      "year": "2025",
      "venue": "Clearervoice-studio: Bridging advanced speech processing research and practical deployment",
      "arxiv": "arXiv:2506.19398"
    },
    {
      "citation_id": "29",
      "title": "Minmo: A multimodal large language model for seamless voice interaction",
      "authors": [
        "Qian Chen",
        "Yafeng Chen",
        "Yanni Chen",
        "Mengzhe Chen",
        "Yingda Chen",
        "Chong Deng",
        "Zhihao Du",
        "Ruize Gao",
        "Changfeng Gao",
        "Zhifu Gao"
      ],
      "year": "2025",
      "venue": "Minmo: A multimodal large language model for seamless voice interaction",
      "arxiv": "arXiv:2501.06282"
    },
    {
      "citation_id": "30",
      "title": "Qwen2. 5-omni technical report",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-omni technical report",
      "arxiv": "arXiv:2503.20215"
    }
  ]
}