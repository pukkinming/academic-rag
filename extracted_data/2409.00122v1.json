{
  "paper_id": "2409.00122v1",
  "title": "Brant-X: A Unified Physiological Signal Alignment Framework",
  "published": "2024-08-28T13:26:42Z",
  "authors": [
    "Daoze Zhang",
    "Zhizhang Yuan",
    "Junru Chen",
    "Kerui Chen",
    "Yang Yang"
  ],
  "keywords": [
    "Physiological signal",
    "Multi-channel time series",
    "Contrastive learning",
    "Alignment",
    "Healthcare"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Physiological signals serve as indispensable clues for understanding various physiological states of human bodies. Most existing works have focused on a single type of physiological signals for a range of application scenarios. However, as the body is a holistic biological system, the inherent interconnection among various physiological data should not be neglected. In particular, given the brain's role as the control center for vital activities, electroencephalogram (EEG) exhibits significant correlations with other physiological signals. Therefore, the correlation between EEG and other physiological signals holds potential to improve performance in various scenarios. Nevertheless, achieving this goal is still constrained by several challenges: the scarcity of simultaneously collected physiological data, the differences in correlations between various signals, and the correlation differences between various tasks. To address these issues, we propose a unified physiological signal alignment framework, Brant-X , to model the correlation between EEG and other signals. Our approach (1) employs the EEG foundation model to data-efficiently transfer the rich knowledge in EEG to other physiological signals, and (2) introduces the two-level alignment to fully align the semantics of EEG and other signals from different semantic scales. In the experiments, Brant-X achieves state-of-the-art performance compared with task-agnostic and task-specific baselines on various downstream tasks in diverse scenarios, including sleep stage classification, emotion recognition, freezing of gaits detection, and eye movement communication. Moreover, the analysis on the arrhythmia detection task and the visualization in case study further illustrate the effectiveness of Brant-X in the knowledge transfer from EEG to other physiological signals. The model's homepage is at https://github.com/zjunet/Brant-X/.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Physiological signals, as indispensable biomarkers, characterize the underlying complexities of the human body and encapsulate a wide range of critical information about an individual's health, with great significance for health monitoring, disease diagnosis, and treatment  [22, 42] . Among these, several key signals including electroencephalogram (EEG), electrooculography (EOG), electrocardiogram (ECG) and electromyogram (EMG), are especially essential in capturing primary physiological manifestations  [49] . For instance, EEG signals, which record neural activity in the brain, have been utilized to study different stages of sleep and human emotions, aiding in diagnosing sleep-related disorders and emotional health issues  [47] . Also, EOG signals, owing to their ability to monitor potential changes during eyeball movements, have proved instrumental in enabling communication for individuals living with neurodegenerative disorders  [10, 59] . Moreover, ECG signals, which record the fluctuation of the heart's bio-electric activities, have been widely employed in investigations relating to cardiac health and diseases  [66] . Finally, EMG signals capture the electrical activity of human muscles, helping the diagnosis and rehabilitation training of neuromuscular diseases  [2] . The applications of these physiological signals allow clinicians to monitor individual health in real-time and make data-driven decisions, holding far-reaching implications for many research fields like healthcare.\n\nDespite each physiological signal records the physiological conditions of its corresponding body part, it is worth noting that the body functions as an integrated biological system rather than some independent components  [68] . Thus, there exists an inherent interconnection among different physiological signals. Among these, given the brain's role as the epicenter for controlling vital activities, EEG exhibits significant correlations with synchronous physiological signals from other body parts  [29] . Specifically, in some scenarios, since the information of single-type signal may be insufficient or noisy, ignoring this correlation can lead to great performance losses. Taking sleep staging as an example, as shown in Fig.  1(a) , although EEG records different brainwaves in different stages, the rapid oscillations of EOG are particularly essential criteria for the rapid eye movement (REM) stage. Moreover, Sharma et al.  [51]  has also shown that introducing EOG signals can bring a relative improvement of 14.98% in accuracy. Besides, the correlations between EEG and other signals also exists in other scenarios: (1) EEG&EOG: For individuals with neurodegenerative disorders who can only express their thoughts and achieve interaction through eye movements, EEG and EOG can contribute to the development of assistive communication systems  [59] . (2) EEG&ECG: During different emotional states in Fig.  1(b) , brain signals and heartbeats consistently present different patterns, such that EEG and ECG can be utilized for emotion recognition  [20] . (3) EEG&EMG: Since the abrupt muscle rigidity (named freezing of gaits, FoG) of Parkinson's disease is related to a complex interplay between motor, cognitive and affective factors, EEG and EMG can be employed in FoG detection to enhance patient safety and quality of life  [74] . Hence, the correlations between EEG and other physiological signals (refered to as \"EXG\" in this paper, including EOG, ECG, and EMG) hold potential to improve performance in a variety of scenarios. Therefore, our work focuses on establishing an EEG-centric unified framework for modeling the correlation between EEG and EXG, which exploits the combined information of EEG and EXG to contribute to various application scenarios. However, current researches leave much to be explored in this direction, primarily due to the following challenges.\n\nFrom the viewpoint of data, simultaneously collected EEG and EXG signals face a conspicuous lack of data. Due to the acquisition costs, ethical restrictions, and a lack of emphasis on the signal correlation in current machine learning research, the majority of physiological data records only a single type of signal, such as EEG datasets of several terabytes in size  [19] . In contrast, available multi-type physiological datasets, which contain various physiological data collected simultaneously, are much smaller in scale, most being less than a few gigabytes. Therefore, the scarcity of simultaneously collected EEG and EXG data poses challenges in training a unified framework for modeling the correlation between EEG and EXG.\n\nFrom a method perspective, there exist significant inherent differences in correlations between EEG and different EXG signals. Different types of physiological signals differ greatly in their inherent properties such as amplitude and bandwidth  [58] . To satisfy the sampling theorem  [50] , the huge gap in bandwidth further leads to differences in sampling rates. Specifically, due to the gap in bandwidth, the sampling rates for EOG, ECG, and EMG may vary respectively within the ranges of 50-100Hz, 250-500Hz, and 1000-2000Hz. These discrepancies are also evident in other features like typical waveforms and rhythmicity  [58] . The above factors result in vast inherent differences in correlations between EEG and different EXGs, posing a challenge to the unified modeling method of EEG-EXG correlation.\n\nFrom the viewpoint of task, in different scenarios, various downstream tasks depend on different correlations even between EEG and the same EXG. Given that different application scenarios involve different physiological activities of body organs, different downstream tasks need to capture different correlations between EEG and even the same EXG. Specifically, since the physiological changes during sleep is relatively slow, in sleep staging task, the EEG-EOG correlation is required to capture on a scale up to 30sec, which is defined as a sleep stage  [7] . In contrast, in eye movement communication task, eyeball movements may occur in less than 1sec, depending on different EEG-EOG correlation from sleep staging  [24] . Therefore, it is challenging to capture different EEG-EXG correlations for various downstream scenarios.\n\nTo tackle the above issues, we propose a contrastive-learningbased framework named Brant-X , to efficiently align EEG and EXG signals from different semantic scales for the modeling of correlation between EEG and EXG. To address the scarcity of simultaneously collected EEG and EXG data, our intuitive idea is to use models trained with a large amount of EEG data to empower the representation learning on EXG signals. Inspired by large language models that are widely applied in other research fields like computer vision  [39, 63] , we employ the EEG foundation model Brant-2  [70, 73] , which is pre-trained on 4TB brain signal data and contains 1B parameters. Based on this, we summarize existing public multi-type physiological datasets 1  , to perform data-efficient knowledge transfer from EEG to EXG. Observing that the gaps between tasks primarily stem from the differences in semantic scales of correlation, to address the gaps among various signals and tasks, we introduce the two-level alignment that aligns the semantics of EEG and EXG at both patch-and sequence-level. The patch-level alignment overcomes finer inherent differences and captures EEG-EXG correlation at a smaller semantic scale, while the sequence-level one aligns coarser differences and captures the correlation at a larger scale. Moreover, we adopt the sampling augmentation to enhance model robustness to different sampling rates.  To validate the effectiveness of Brant-X , extensive experiments show that Brant-X achieves SOTA performance on various downstream tasks across diverse scenarios involving EEG and EXG signals, including sleep stage classification, emotion recognition, freezing of gaits detection, and eye movement communication. The analysis on the arrhythmia detection task and the visualization in case study further demonstrate that Brant-X can effectively transfer the knowledge from EEG to EXG signals through alignment. Overall, our key contributions comprise:\n\nâ€¢ We are the first to design a unified EEG-centric alignment framework to model the correlations between EEG and other physiological signals, which can be applied to various scenarios. â€¢ Based on the EEG foundation model, we adopt the two-level alignment for data-efficient knowledge transfer from EEG to EXG signals, which combines the semantics of EEG and EXG to jointly improve the performance on downstream tasks. â€¢ We validate Brant-X through extensive experiments on multiple downstream tasks involving various physiological signals. Moreover, the analysis and visualization illustrate the effectiveness of Brant-X in knowledge transfer from EEG to EXG.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we introduce the technical details of the proposed framework Brant-X . Specifically, as shown in the upper left part of Fig.  3 , we first split the EEG and EXG sequences into continuous data patches. Then, considering the variance in sampling rates between physiological signals in different scenarios, we adopt the sampling augmentation (lower left corner of Fig.  3 ) to enhance the model's robustness to changes in sampling rates. As shown in middle part of Fig.  3 , the EEG patches and EXG patches, along with the augmented patches, are fed into the EEG and EXG encoder, respectively, to acquire the representation of each data patch. Here we employ the EEG foundation model Brant-2 as the EEG encoder of our framework (details in Sec. 2.2). During the unsupervised training process, we propose the two-level alignment (right part of Fig.  3 ), which aligns the simultaneously collected patches and sequences at both patch-and sequence-level. After the unsupervised alignment, the representations of EEG and EXG data output by the two encoders will be aggregated via the attention mechanism for various tasks in diverse scenarios.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Formulation",
      "text": "First, we formalize the definitions of the four downstream tasks where the experiments are conducted.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Foundation Models For Eeg",
      "text": "Due to the scarcity of simultaneously collected physiological data, it is challenging to build a unified framework for the modeling of correlations between EEG and EXG. To address this issue, we adopt the brain signal foundation model to perform data-efficient knowledge transfer from EEG to EXG. To the best of our knowledge, only the series of works named Brant currently serves as open-source foundation models on brain signals, including Brant and Brant-2. Specifically, Zhang et al.  [73]  provide the first off-the-shelf foundation model named Brant for intracranial EEG (iEEG) 2  signals, which contains 500M parameters pre-trained on 1.01TB iEEG data. Based on Brant, Yuan et al.  [70]  propose the foundation model for brain signals named Brant-2. It consists of over 1B parameters and is pre-trained on as much as nearly 4TB mixed data (with 2.3TB iEEG data from 26 subjects and 1.6TB EEG data from about 15,000 subjects). Our choice to use Brant-2 as the EEG encoder in our framework was two-fold. Firstly, it is pre-trained on a large corpus of brain signal data and can learn powerful representations from EEG signals. More importantly, it uses pre-training data with different sampling rates, resulting in a heightened level of robustness towards changes in sampling rates.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overall Architecture",
      "text": "Patching. Given that physiological data are bioelectric signals, the semantic information of the physiological states can only be collectively expressed with multiple sampling points, rather than  a single one. Therefore, we split a whole data sequence into several consecutive patches to aggregate semantic information within patches and reduce computation demand  [43] .\n\nFormally, as shown in the upper left part of Fig.  3 , given the ğ‘–-th multi-channel EEG data sequence ğ’™ ğ‘– âˆˆ R ğ¶ Ã—ğ¿ where ğ¶ denotes the number of EEG channels and ğ¿ denotes the number of timestamps (length of the sequence), we split ğ’™ ğ‘– with length ğ‘€ to generate a set of non-overlapping patches {ğ’™ ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 , where ğ’™ ğ‘–,ğ‘— âˆˆ R ğ¶ Ã—ğ‘€ and ğ‘ƒ = âŒŠğ¿/ğ‘€âŒ‹ is the number of patches in this sequence. For the EXG data, we apply the same patching process as above, and the symbols are also similar. Specifically, we use xğ‘– âˆˆ R C Ã— L to denote the ğ‘–-th EXG sequence, where C is the number of EXG channels and L is the sequence length. Also, { xğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 denotes the set of patches, where xğ‘–,ğ‘— âˆˆ R C Ã—ğ‘€ .\n\nSampling Augmentation. Considering that different physiological signals exhibit large differences in sampling rates, models that learn representations from physiological data must be sufficiently robust to changes in sampling rates. For EEG, as presented in Sec. 2.2, Brant-2 utilize pre-training data at various sampling rates, making it fairly robust to changes in sampling rates. Hence, serving as the EEG encoder of our framework, it is capable of handling differences in the sampling rate of EEG data.\n\nFor the EXG signals, to address the issue of various sampling rates, we adopt sampling augmentation to enhance the model's robustness to changes in sampling rate. Specifically, as shown in the lower left corner of Fig.  3 , we both upsample the original data to twice its original rate and downsample it to half, producing two sets of augmented data with different sampling rates. Formally, given the original EXG data patches { xğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 , we upsample the data to twice its sampling rate, generating the upsampled data patches { xâ€² ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 where xâ€² ğ‘–,ğ‘— âˆˆ R C Ã—2ğ‘€ . Similarly, the original patches are also downsampled to half the sampling rate, thus obtaining the downsampled data patches { xâ€²â€² ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 where xâ€²â€² ğ‘–,ğ‘— âˆˆ R C Ã—âŒŠğ‘€/2âŒ‹ . In subsequent representation learning and semantic alignment sections, the original data x, along with the upsampled data xâ€² and downsampled data xâ€²â€² , will be fed into the EXG encoder for model learning purposes.\n\nEmbedding to Latent Space. For EEG data, as shown in the middle part of Fig.  3 , we feed it directly into the pre-trained EEG encoder (details in Sec. 2.2) to obtain the EEG representation. Formally, ğ‘ƒ consecutive patches {ğ’™ ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 from the ğ‘–-th EEG data sequence ğ’™ ğ‘– will be input into the EEG encoder, yielding the representations {ğ’‘ ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 of these patches, where ğ’‘ ğ‘–,ğ‘— âˆˆ R ğ· ğ‘ denotes the representation of the ğ‘—-th patch from the ğ‘–-th sequence of EEG data, and ğ· ğ‘ denotes the dimension of patch representations.\n\nWhen it comes to EXG data, it will be fed into the EXG encoder to obtain its representation. Formally, all the patches { xğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 from the ğ‘–-th EXG data sequence xğ‘– are input into the EXG encoder, generating their representations { pğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 , where pğ‘–,ğ‘— âˆˆ R ğ· ğ‘ . Given that the focus of our work is the alignment framework, the specific architecture of the EXG encoder can be flexible. For the technical details of the EXG encoder used in this paper, please refer to App. A.\n\nSimilarly, the upsampled EXG patches { xâ€² ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 and the downsampled patches { xâ€²â€² ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 undergo the same process, obtaining the representations { pâ€² ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 and { pâ€²â€² ğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 of augmented EXG data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Two-Level Alignment",
      "text": "We adopt two-level alignment that fully aligns the semantics of EEG and EXG signals at patch-and sequence-level, to overcome inherent differences and capture the correlation between EEG and EXG at different semantic scales.\n\nPatch-level Alignment. At a finer grain, we align EEG and EXG data at patch-level by placing the simultaneous EEG and EXG patches close together in the latent space, while mapping unrelated patches further apart. As shown in the upper right part of Fig.  3 , since our EEG encoder is pre-trained on a large amount of data (Sec. 2.2), it is reasonable to believe it can output representative representations of EEG patches. Therefore, we set the EEG representation ğ’‘ ğ‘–,ğ‘— as the anchor. The anchor ğ’‘ ğ‘–,ğ‘— and the simultaneously collected EXG patch pğ‘–,ğ‘— are set as the positive sample pair. Negative samples are randomly selected from the representations { pğ‘š } ğ‘šâ‰ ğ‘– from other EXG data sequences. It is noteworthy that, contrary to the sequence-level alignment described later, we can't randomly select the representations { pğ‘–,ğ‘› } ğ‘›â‰ ğ‘— from the EXG sequence pğ‘– as negative samples. This is because these representations { pğ‘–,ğ‘› } ğ‘›â‰ ğ‘— and the anchor originate from the same physiological process and may have a temporal dependency between them. Formally, for the anchor ğ’‘ ğ‘–,ğ‘— , the negative sample set ğ‘ ğ‘ ğ‘–,ğ‘— is randomly sampled from all the negative samples { pğ‘š,ğ‘› |ğ‘š â‰  ğ‘–, ğ‘› = 0, ..., ğ‘ƒ -1}. The InfoNCE  [44]  loss is applied to retain the maximum mutual information between positive pairs:\n\nwhere ğ‘¡ ğ‘ denotes the temperature hyperparameter to adjust scale, and L ğ‘ denotes the InfoNCE loss between EEG and original EXG data in patch-level alignment.\n\nSimilarly, the same alignment process would also exist between the EEG data and the two sets of augmented EXG data. These two losses are denoted as L â€² ğ‘ and L â€²â€² ğ‘ , respectively. Overall, the optimization objective of patch-level alignment is given by:\n\nSequence-level Alignment. At a coarser granularity level, we employ sequence-level alignment to align the corresponding sequence in the latent space. To aggregate the representations of patches from a data sequence, we firstly perform a linear projection ğ‘Š ğ‘ğ‘Ÿğ‘œ ğ‘— âˆˆ R ğ· ğ‘  Ã—ğ‘ƒğ· ğ‘ on all patch representations ğ’‘ ğ‘–,: âˆˆ R ğ‘ƒ Ã—ğ· ğ‘ from sequence ğ’™ ğ‘– , thus obtaining the sequence representation ğ’” ğ‘– âˆˆ R ğ· ğ‘  , where ğ· ğ‘  denotes the dimension of sequence representations:\n\nThis linear projection is applied similarly for EXG data pğ‘–,: and the augmented data pâ€² ğ‘–,: , pâ€²â€² ğ‘–,: as well, yielding the sequence representations sğ‘– , sâ€² ğ‘– and sâ€²â€² ğ‘– respectively. After obtaining the sequence representations, we set the representations of simultaneously collected EEG and EXG sequences (ğ’” ğ‘– and sğ‘– ) as positive sample pairs, while all other sequence pairs are set as negative pairs. Formally, the negative sample set ğ‘ ğ‘  ğ‘– of sequence ğ’” ğ‘– is randomly sampled from all the negative samples { sğ‘š |ğ‘š â‰  ğ‘–}. The sequence-level InfoNCE loss L ğ‘  for the EEG and the original EXG data can be given as follows:\n\nwhere ğ‘¡ ğ‘  denotes the temperature hyperparameter. As shown in the bottom right part of Fig.  3 , following the common practice in CLIP  [48] , we adopt a similarity matrix to optimize this objective. Likewise, we carry out the same alignment process between EEG and augmented EXG data, resulting in two losses L â€² ğ‘  and L â€²â€² ğ‘  in the same form. The overall loss in sequence-level alignment is:\n\nFinally, the objective of joint optimization is obtained by adding the patch-level and sequence-level alignment losses L * ğ‘ and L * ğ‘  .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment 3.1 Experimental Setup",
      "text": "Alignment. To align the simultaneously recorded EEG and arbitrary EXG data, the training data used for unsupervised alignment is collectively assembled from three datasets: CAP  [57] , ISRUC  [32] ,\n\nand HMC  [4] , which include EEG, EOG, ECG, and EMG signals.\n\nOverall, the alignment training data includes 359 recordings from 267 subjects. The alignment is performed on a Linux system with 2 CPUs (AMD EPYC9654 96-Core Processor) and 2 GPUs (NVIDIA Tesla A100 80G). The learning rate of EEG encoder is set as 1 Ã— 10 -5 for finetuning, while the EXG encoder is trained with a higher learning rate of 3 Ã— 10 -4 .\n\nDownstream Tasks. Here we introduce the four downstream tasks used to validate the effectiveness of our Brant-X , along with the datasets, setups and and evaluation metrics.\n\nâ€¢ Sleep Stage Classification. In sleep health research, sleep staging refines human understanding of sleep states and patterns, which holds significance for the prevention and diagnosis of sleep-related diseases  [47] . According to the American Academy of Sleep Medicine (AASM) manual  [7] , sleep occurs in five stages: wake, N1, N2, N3, and REM. Among these, N1 to N3 are non-rapid eye movement sleep, with each stage leading to progressively deeper sleep. Hence, sleep stage classification is a five-class classification problem.\n\nAs for the dataset, the Sleep-EDF datasets  [31]  are very popular in sleep staging researches. The Sleep-EDF-78 dataset contains 153 whole-night polysomnographic sleep recordings from sleep cassette studies, containing 100Hz EEG and EOG data from 78 subjects aged 25-101 years (37 males and 41 females). Data are segmented into 30sec epochs and manually annotated by experts. The Sleep-EDF-20 dataset, which contains 39 recordings from 20 subjects, is also used in our study to facilitate the comparison with the existing methods.\n\nThe experiment is conducted on EEG and EOG signals in a subject-independent setting. We divide the subjects into training, validation, and test sets in a 3:1:1 ratio. The experiments are repeated on all subjects to obtain overall results. The evaluation metrics include accuracy, sensitivity, specificity, macro F1 score, and Cohen's kappa ğœ….\n\nâ€¢ Emotion Recognition. Automatic emotion recognition has made a remarkable entry in the domain of biomedical, brain-computer interface, smart environment, safe driving and so on  [28] . Emotions are categorized into two types: (1) discrete emotions like joy, fear and sadness; and (2) multi-dimensional emotions on three emotion dimensions: arousal, valence, and dominance dimensions. Existing works  [30, 35, 36, 54, 56]  mainly focus on the recognition of multi-dimensional emotions, so the task can be regarded as three independent binary classification problems: low/high valence, low/high arousal and low/high dominance.\n\nThe DREAMER dataset  [30]  is used to conduct experiments on emotion recognition task. It contains EEG (128Hz) and ECG (256Hz) data of 23 subjects (14 males and 9 females) when they are watching 18 film clips. Each film clip has an average length of 199s, which is thought to be sufficient for eliciting single emotion. After watching a film clip, emotion statuses are labeled as low or high on the three emotion dimensions, serving as the labels for emotion recognition.\n\nThe experiment in this task is conducted on EEG and ECG signals in a subject-independent setting. We split subjects into training, validation, and test sets in a 3:1:1 ratio and repeat the experiments on all subjects. The evaluation metrics are mainly accuracy  [35, 36, 56] , with some studies  [30, 54]  also including the F1 score and the AUC of precision-recall curve.\n\nâ€¢ Freezing of Gaits Detection. FoG, which refers to the interruption of the motion caused by the brain's incompetence to deal with concurrent cognitive and motor request, affects about 50%-80% of Parkinson's disease patients as one of the severest manifestations. Thus, accurate detection of FoG can significantly improve patients' life quality and promote personalized treatment  [74] . The FoG detection task is a binary classification problem, that is, determining whether FoG appears during a walking process.\n\nThe FoG dataset  [74]  is used in this work, which includes EEG and EMG signals (1000Hz) collected from 12 Parkinson's disease patients (6 males and 6 females) aged 57-81 years with disease durations between 1 and 20 years. The valid data lasts for 3h42min, including 2h14min of normal gait and 1h28min of freezing of gait, labeled by two qualified physicians.\n\nThe experiment in this task is conducted on EEG and EMG. The training, validation, and test data are randomly split in a 3:1:1 ratio. We also repeat the experiments to obtain the overall results. As a classification problem, the evaluation metrics used for this task are accuracy, precision, recall and F1 score.\n\nâ€¢ Eye Movement Communication. Due to paralysis caused by neurodegenerative disorders like amyotrophic lateral sclerosis (ALS), many patients lost almost all their communication abilities  [24] , and only have remnant oculomotor control to form words, phrases, and sentences using a speller system  [59] . The speller system works on a binary principle where the patient responds to auditory questions by moving their eyes to say \"yes\" and not moving the eyes for \"no\". Therefore, the eye movement communication task is also a binary classification problem (yes or no).\n\nThe dataset published by Jaramillo-Gonzalez et al.  [24]  is used for the eye movement communication experiment. The dataset contains EEG and EOG data (500Hz) recorded from four patients suffering from ALS. Data are recorded during 2-10 visits, each visit consisting of an average of 3.22 days with 5.57 sessions recorded per day. Due to the inconsistency in EOG channels across different files in the dataset, we exclude files lacking specific EOG channels to conduct the experiment.\n\nThe experiment in this task is conducted using EEG and EOG. Experiments are conducted on training, validation, and test data split 8:1:1 and are repeated on all data files. The evaluation metrics are accuracy, precision, recall and F1 score.\n\nAs for data pre-processing, for the three tasks except eye movement classification, we did not perform filtering or other processing, directly using the preprocessed data of the original datasets. For the eye movement dataset, as the publisher didn't filter, we applied 45Hz low-pass filtering and z-score normalization.\n\nBaselines. As a unified unsupervised alignment framework for physiological signal modeling, we compare Brant-X with the advanced self-supervised or unsupervised methods designed for general time series on all the downstream tasks, including TF-C  [75]  and SimMTM  [15] . Also, to compare Brant-X with the methods that performs time series classification based on pre-trained language models, we set OneFitsAll  [76]  and Time-LLM  [27]  as a baseline. As for the supervised methods, MiniRocket  [14]  is selected as our baseline due to its efficiency and versatility. Furthermore, we compare Brant-X with the SOTA methods that are specially designed for each task, to demonstrate the effectiveness of Brant-X in various scenarios. These task-specific or signal-specific supervised methods includes: (1) TinySleepNet  [55] , XSleepNet  [45] , L-SeqSleepNet  [46] , SleepHGNN  [25] , SleepKD  [34] , and SleepDG  [62]  for sleep stage classification; (2) MLF-CapsNet  [36] , EEG-Conformer  [52] , Lin et al.  [35]  and Wang et al.  [64]  for emotion recognition; (3) Aly and Youssef  [5] , Batool and Javeed  [6]  and Goel et al.  [17]  for freezing of gaits detection; and (4) eyeSay  [77] , Adama and Bogdan  [1]  and Hossieny et al.  [23]  for eye movement communication. More details about these baselines are given in App. B.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "Fig.  4  summarizes the overall accuracy of Brant-X and other baselines on various downstream tasks (including the arrhythmia detection in Sec. 3.3). Since the task-specific methods vary across different tasks, we use \"Task-specific Methods\" to collectively represent their best results on each task. As shown in Fig.  4 , compared with other baseline methods, Brant-X achieves SOTA performance on all of the five tasks, illustrating the effectiveness of our framework in various scenarios. Detailed comparisons on each task are discussed in following paragraphs, where in all the tables we mark values ranking the first (v), second (v) and third (v*) in each column. The performance comparison on sleep stage classification task is given in Tab. 1. It shows that Brant-X achieves top rankings in almost all performance metrics, demonstrating that Brant-X can effectively transfer the knowledge from EEG to EOG signals, combining the information of both EEG and EOG signals to learn the high-level semantic information therein. The baselines on general time series did not yield good results, mainly because these models struggle to overcome the huge gap in inherent features between various physiological signals, and do not model correlations from different semantic scales like Brant-X does. As shown in Tab. 2, on emotion recognition task, Brant-X achieves SOTA performance compared to all the baselines. Compared to the baselines designed solely for EEG, Wang et al.  [64]  claims the second spot, because it adopt the same strategy as Brant-X for combining the information of both EEG and ECG, thereby demonstrating stronger learning capabilities. However, Brant-X still surpasses Wang et al.  [64]  on all metrics, benefiting from alignment training based on contrastive learning. The overall results on the freezing of gaits detection and eye movement communication tasks are given in Tab. 3 and Tab. 4, respectively. Brant-X defeats all the baseline methods on these two tasks, showing its ability to learn representations from simultaneously collected EEG, EMG, and EOG data. Batool et al.  [6]  and Adama et al.  [1]  achieve the second-best performance on these two tasks, respectively, mainly because they explicitly extract the frequency domain information of physiological signals as inherent features for physiological data modeling.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "To evaluate the effectiveness of each component in Brant-X , we conduct ablation experiments on four model variants, including: (1) Brant-X w/o sampling-aug: Brant-X without the sampling augmentation during alignment; (2) Brant-X w/o patch-align: Brant-X without the patch-level alignment; (3) Brant-X w/o seq-align: Brant-X without the sequence-level alignment; (4) Brant-X w/o EEG-encoder: Brant-X without the EEG encoder during downstream evaluation after alignment; (5) Brant-X w/o EXG-encoder: Brant-X without the EXG encoder during downstream evaluation after alignment.\n\nThe comparison results of the ablation experiments on the four downstream tasks are presented in Fig.  5 . It demonstrates that Brant-X outperforms other variants on all metrics of all the tasks, evidencing the contribution of each component in our framework. Compared to the full Brant-X , the performance of Brant-X w/o sampling-aug decreases, showing the boost of model robustness against variable sampling rates provided by the sampling augmentation. Also, Brant-X w/o patch-align and Brant-X w/o seq-align show a decrease in performance, suggesting that the two-level alignment can align EEG and EXG signals from different semantic scales to learn informative representations from physiological data. For sleep staging and emotion recognition, Brant-X w/o EEG-encoder drops greatly in performance, as EEG signals play an important role in these scenarios. This corroborates the significance of the brain as a central control in vital activities, as we emphasized in Sec. 1.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Exg Encoder Analysis.",
      "text": "As a supplement to the Brant-X w/o EEGencoder in the ablation experiments, we extend our assessment to more tasks using the standalone EXG encoder, to validate whether the EXG encoder can learn useful representations from EXG data during the alignment training. Specifically, we conduct experiments with the aligned EXG encoder on ECG data (without incorporating the EEG encoder on EEG data) on the arrhythmia detection task.\n\nMore details about this task and the results are given in App. C. As shown in Tab. 5, Brant-X achieves SOTA performance on the arrhythmia detection task, showing that the alignment training indeed enables the EXG encoder to learn the representations from ECG signals, and then effectively classify cardiac rhythms.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Case Study",
      "text": "Fig.  6  displays four similarity matrices between patch representations of two multi-type physiological data sequences, {ğ’™ ğ‘– , xğ‘– } and {ğ’™ ğ‘— , x ğ‘— }. The vertical axis represents the patch representations of two EEG sequences, ğ’™ ğ‘– and ğ’™ ğ‘— , and the horizontal axis represents the patch representations of two EXG sequences, xğ‘– and x ğ‘— . Thus, four similarity matrices are given in Fig.  6 . The darker the colour of each small square, the higher the normalised similarity between the representations of two corresponding patches.\n\nAmong these, matrix (a) (or (d)) indicates the similarity of patch representations of simultaneously collected EEG sequence ğ’™ ğ‘– (or ğ’™ ğ‘— ) and EXG sequence xğ‘– (or x ğ‘— ). It presents an overall darker colour, demonstrating the correlations between patches from the simultaneously collected EEG and EXG sequence. Moreover, the diagonal of matrix (a) (or (d)) is particularly dark, indicating that the simultaneous EEG and EXG patches are well-aligned. However, as for matrices (b) and (c), they have an overall lighter colour, suggesting little to no correlation between patch representations of non-simultaneously collected EEG and EXG data. These four similarity matrices in this case illustrate well that the two-level alignment can bring the representations of simultaneous EEG and EXG data closer, while distancing irrelevant sequences, such that Brant-X can perform knowledge transfer from EEG to EXG.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Related Work",
      "text": "Physiological Signal Modeling. With the maturation of physiological recording technology and the advancement of machine learning methods, physiological signal modeling has captivated many researchers. Initially, researchers mainly focus on model learning on a single type of signal. A large body of works propose to use time series  [18, 36, 37, 45, 52, 55, 71]  or graph  [8, 12, 35]  data structures with supervised  [18, 35-37, 45, 52, 55]  or self-supervised  [8, 12, 71]  learning paradigms for various tasks on EEG signals. Recently, some large EEG models  [26, 70, 73]  also emerged, which break through the limitations of different tasks on EEG. Also, methods based on feature engineering or supervised learning are proposed to learn representations from EOG  [1, 23, 77] , ECG  [3, 40] , and EMG  [41, 72]   Multimodal Alignment. To capitalize on the information consistency in multimodal data, contrastive-learning-based alignment strategy has achieved impressive results in many fields like imagetext  [48, 67, 69] . For physiological research, methods  [9, 11]  conduct alignment to the features of images for medical images segmentation. Wang et al.  [61]  aligns the paired medical image and radiology reports (text) for image classification and object detection, etc. Fan et al.  [16]  propose a domain adaptation approach to bridge the gap between the EEG data distribution of source and target domains for sleep staging. Lv et al.  [38]  reinforce features by aligning the visual and acoustic video clips emotion However, these primarily explore the consistency among image, audio modalities, none of which explicitly align the simultaneously collected physiological signals.\n\nTime seires Modeling. Time series (TS) analysis has been utilized in many real-world applications, including finance, meteorology, healthcare, and so on, attracting more and more researchers. Wu et al.  [65]  propose TimesNet as a task-general backbone to discover the multi-periodicity adaptively for TS analysis. Dong et al.  [15]  propose to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold for TS modeling. Zhou et al.  [76]  propose a unified model that leverages language or vision models for TS analysis. Jin et al.  [27]  present a reprogramming framework named Time-LLM to repurpose large language models for general TS forecasting. However, most TS works cannot adapt well to high-frequency physiological signals and ignore the correlation between physiological signals.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we are the first to propose a unified physiological signal alignment framework, Brant-X . Based on the EEG foundation model, we summarize available multi-type physiological datasets, to transfer the rich knowledge from the EEG foundation model to EXG signals. We adopt the two-level alignment that aligns the semantics of EEG and EXG data at both patch-and sequence-level, to adapt to various downstream scenarios. In this way, EEG is viewed as a bridge between the EEG foundation model and EXG data, allowing the data and model resources in the EEG field to empower the research on other physiological signals, paving a new avenue to model the correlations between various physiological signals. In the future, motivated by the positive results of Brant-X , it would be intriguing to explore further studies along this research line on more physiological signals.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A Details Of The Exg Encoder",
      "text": "Because the focus of this paper is to introduce our proposed alignment framework, the specific encoder architecture can be flexible. The model architecture of the EXG encoder used in this paper are introduced here.\n\nSince physiological signals are bioelectric signals, the time domain provides information about the amplitude and duration, while the frequency domain can reveal the oscillation patterns and underlying biological rhythms  [33] . Therefore, to combine the information from both time and frequency domains, in our EXG encoder, we first calculate the power spectral density (PSD)  [53] , which describes the distribution of the signal's total average power over frequency, as the information in frequency domain. Then, a convolutional neural network (CNN) performs on the PSD to extract features in the frequency domain of the EXG signal. The extracted features in the frequency domain will be concatenated with the convolution-derived features in the time domain, serving as the features within a single patch. Due to the fact that physiological signals are time series, each patch has a temporal dependency with its contextual patches from the same sequence. With this in mind, the features of consecutive patches from a sequence will be fed into the Transformer  [60]  to obtain a more comprehensive representation that considers temporal dependencies.\n\nFormally, all the patches { xğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 from the ğ‘–-th EXG data sequence xğ‘– are input into the EXG encoder, generating their representations { pğ‘–,ğ‘— } ğ‘ƒ -1 ğ‘—=0 :\n\nwhere pğ‘–,ğ‘— âˆˆ R ğ· ğ‘ denotes the representation of the ğ‘—-th patch from the ğ‘–-th EXG data sequence xğ’Š , and ğ· ğ‘ denotes the dimension of patch representations.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "B Details Of Baselines",
      "text": "Firstly, we compare Brant-X with the existing self-supervised or unsupervised works on genral time series. The Details of these baseline models are given here:\n\nâ€¢ TF-C  [75] : A decomposable pre-training model for general time series modeling, where the self-supervised signal is provided by the distance between time and frequency components. â€¢ SimMTM  [15] : A pre-training framework on time series to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold.\n\nAlso, we compare Brant-X with the methods that performs time series classification based on pre-trained language models. Hence, we set OneFitsAll  [76]  as a baseline:\n\nâ€¢ OneFitsAll  [76] : A unified model that leverages language or vision models for time series analysis, leading to a comparable or SOTA performance in all main time series analysis tasks.\n\nFurthermore, to illustrate the effectiveness of Brant-X in various application scenarios, we compare our framework with the SOTA methods those are specially designed for each of the four downstream tasks. These supervised methods includes:\n\n(1) For the sleep stage classification task:\n\nâ€¢ TinySleepNet  [55] : An end-to-end model based on CNN and LSTM for automatic sleep stage scoring on raw single-channel EEG with a less number of trainable parameters. â€¢ XSleepNet  [45] : A sequence-to-sequence sleep staging model that is capable of learning a joint representation from both raw signals and time-frequency images. (2) For the emotion recognition task:\n\nâ€¢ MLF-CapsNet  [36] : A multi-level features guided capsule network for multi-channel EEG-based emotion recognition, which can simultaneously extract features from the raw EEG signals and determine the emotional states. â€¢ EEG-Conformer  [52] : A compact convolutional Transformer to encapsulate local and global features in a unified EEG classification framework for motor imagery and emotion recognition. â€¢ Lin et al.  [35] : A graph convolution model with dynamic channel selection for emotion classification, which combines the advantages of 1D convolution and graph convolution to capture the intra-and inter-channel EEG features. â€¢ Wang et al.  [64] : An emotion recognition method based on the feature fusion of single-lead EEG and ECG signals, using various time-domain, frequency-domain, and nonlinear features.\n\n(3) For the freezing of gaits detection task:\n\nâ€¢ Aly and Youssef  [5] : A model based on CNN and LSTM that integrates EEG with EMG signals to investigate the efficiency of deep learning in hybrid systems with signal fusion for motion classification. â€¢ Batool and Javeed  [6] : A feature engineering method that uses time-frequency feature extraction strategy and CNN-BiLSTM to detect walking disorder in Parkinson's disease patients. â€¢ Goel et al.  [17] : An ensemble techniques that combines the prediction of multiple methods to improve the model performance for freezing of gaits detection on EEG signals (4) For the eye movement communication task:\n\nâ€¢ eyeSay  [77] : A multi-stage convolutional neural network to decode eye dynamics using electrooculography, towards voice-free communication for patients with amyotrophic lateral sclerosis. â€¢ Adama and Bogdan  [1] : A feature engineering method that employs features like relative power, spectral edge frequencies and symbolic mutual information for eye movement classification. â€¢ Hossieny et al.  [23] : A model based on ResNet  [21]  using horizontal and vertical EOG signals to determine six eye movement directions.\n\nFor some baselines that are not open source, we re-implemented them for experiments. In order to make a fair comparison, for baselines designed for only one type of physiological signal (EEG or EXG), we take their best results on the following three settings as their final results: only on EEG, only on EXG, and aggregation the representation from EEG and EXG.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "C Analysis On The Arrhythmia Detection Task",
      "text": "Atrial fibrillation (AF) is the most common sustained cardiac arrhythmia, occurring in about 2% of the general population and is associated with significant mortality and morbidity through association of risk of death, stroke, heart failure and coronary artery disease  [13] . Therefore, accurate rhythm classification and arrhythmia detection are vital to the prevention and treatment of heart disease. Depending on the different classifications of cardiac states, the task can be viewed as a multi-classification problem. The AFDB dataset  [13]  comprises 12,186 single lead ECG recordings of 30 and 60sec long, gathered from subjects undergoing longhaul mobile ECG checking. Data are collected at 300Hz, and each sample may belong to one of four classes: (1) normal sinus rhythm, (2) AF, (3) other rhythm, or (4) too noisy to classify. In our experiment, we remove the noisy samples so that this task is a three-class classification problem.\n\nAs a supplement to Brant-X w/o EEG-encoder in the ablation study, we conduct this experiment with the aligned EXG encoder on ECG data (without incorporating the EEG encoder on EEG data). The experiment is conducted on training, validation and test data in a 3:1:1 ratio and repeated to obtain the overall results. For each of the above three classes, we use sensitivity, specificity and precision as metrics to evaluate the performance of our aligned EXG encoder and other baselines. We also report the overall accuracy as an overall assessment of model performance. In line with the main experiments on the four main tasks, besides TF-C  [75] , SimMTM  [15]  and OneFitsAll  [76] , we also compare our EXG encoder with the SOTA methods in the field of arrhythmia detection to demonstrate the effectiveness of our model. These methods includes DeepArr  [40]  and Alamatsaz et al.  [3] .\n\nAs shown in Tab. 5, our Brant-X beats all of the baselines on the arrhythmia detection task. This demonstrates that the phase of alignment training empowers the EXG encoder to effectively learn semantic representations from ECG and classify cardiac rhythms.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of inherent correlations between EEG",
      "page": 2
    },
    {
      "caption": "Figure 1: (a), although",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), brain signals and heartbeats consistently present differ-",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of the physiological signal alignment framework Brant-X. Firstly, based on the EEG foundation model, the EXG",
      "page": 3
    },
    {
      "caption": "Figure 3: , we first split the EEG and EXG sequences into continu-",
      "page": 3
    },
    {
      "caption": "Figure 3: ) to enhance",
      "page": 3
    },
    {
      "caption": "Figure 3: , the EEG patches and EXG patches, along",
      "page": 3
    },
    {
      "caption": "Figure 3: ), which aligns the simultaneously collected patches and se-",
      "page": 3
    },
    {
      "caption": "Figure 3: Architecture of Brant-X. In the data preparation stage, EXG data are upsampled and downsampled for data augmentation. Then,",
      "page": 4
    },
    {
      "caption": "Figure 3: , given the ğ‘–-th",
      "page": 4
    },
    {
      "caption": "Figure 3: , we both upsample the original data to",
      "page": 4
    },
    {
      "caption": "Figure 3: , we feed it directly into the pre-trained EEG en-",
      "page": 4
    },
    {
      "caption": "Figure 3: , following the common practice in",
      "page": 5
    },
    {
      "caption": "Figure 4: Overall performance comparison on various tasks.",
      "page": 6
    },
    {
      "caption": "Figure 4: summarizes the overall accuracy of Brant-X and other base-",
      "page": 6
    },
    {
      "caption": "Figure 4: , compared",
      "page": 6
    },
    {
      "caption": "Figure 5: Results of the ablation study on all the downstream tasks.",
      "page": 8
    },
    {
      "caption": "Figure 5: It demonstrates that",
      "page": 8
    },
    {
      "caption": "Figure 6: displays four similarity matrices between patch representa-",
      "page": 8
    },
    {
      "caption": "Figure 6: The darker the colour",
      "page": 8
    },
    {
      "caption": "Figure 6: Similarity matrices of patch representations. The",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "EEG\nEOG",
          "Wake stage": "",
          "N1 stage": "",
          "REM stage": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "EEG\nECG",
          "Excitement": "",
          "Sadness": "",
          "Relaxation": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EEG encoder\n(foundation model)\nEXG encoder": "",
          "EEG encoder\n(foundation model)": "EXG encoder",
          "Column_3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "Accura\nâ€™24,A\nSleep",
          "Column_3": "cy\nugus\nSta",
          "Column_4": "t25\nge",
          "Column_5": "",
          "Column_6": "Mac\nâ€“29\nClas",
          "Column_7": "ro F1\n,2024\nsficati",
          "Column_8": "score\n,Bar\non",
          "Column_9": ""
        },
        {
          "Column_1": "",
          "Column_2": "Accura\nheo\nveme\nectiv\ns,sh\nusly\nma e\ntasks\nuenc\nuresf\nA\nevalu\nducta\nnt-X\nondu",
          "Column_3": "cy\nvera\nntc\nely.\nowi\ncoll\nt al\n,re\nydo\nor\nbla\nate\nbla\nw/o\nring",
          "Column_4": "ll\no\nB\nn\nec\n.\ns\nm\nph\nt\nth\nti\nsa\na",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "cro F1\nults\nunic\n-X d\nabi\nEEG\nachie\nively\ninfo\nologi\nSt\nffecti\nxper\ning-\nmen",
          "Column_8": "scor\non\natio\nefea\nlity\n,E\nve\n,ma\nrma\ncal\nudy\nven\nime\naug:\nt;(2",
          "Column_9": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "A\nF",
          "Column_2": "ccurac\nreezin",
          "Column_3": "y\ng of",
          "Column_4": "Gai",
          "Column_5": "",
          "Column_6": "ts D",
          "Column_7": "F1 scor\nDa\netectio",
          "Column_8": "e\noze\nn",
          "Column_9": "",
          "Column_10": ""
        },
        {
          "Column_1": "",
          "Column_2": "Accur\nnstu",
          "Column_3": "acy\ndy\nM\nA\na",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "na\ned\nho\nyt",
          "Column_7": "F1 sc\nllth\netail\nwni\nhmia",
          "Column_8": "ore\ned\nsa\nnT\nde",
          "Column_9": "",
          "Column_10": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "enab\ngnals\nCas\nispla\nftwo\n}.Th\nGse\nchre\nmilar\nsma\nresen\nngt\nntati\nd EX\ndem\naneou\nalof\nultan\nmatr",
          "Column_8": "",
          "Column_9": "",
          "Column_10": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": ",Zh\nEy",
          "Column_2": "Acc\nizh\ne M",
          "Column_3": "uracy\nangY\noveme",
          "Column_4": "",
          "Column_5": "uan,Ju\nnt Co",
          "Column_6": "n\nm",
          "Column_7": "F1\nru\nmun",
          "Column_8": "score\nChen,\nication",
          "Column_9": ""
        },
        {
          "Column_1": "",
          "Column_2": "Acc\nam",
          "Column_3": "ura\nt",
          "Column_4": "cy\nas",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "F1",
          "Column_8": "sco",
          "Column_9": "re"
        },
        {
          "Column_1": "",
          "Column_2": "ta\nant\nask\nen\nffe\nila\nph\nxis\nan\nns\nar\neh\nwo\n(a)\nlta\nğ’™Ëœğ‘–\nhe\nd",
          "Column_3": "sk\n-X\n,s\nco\ncti\nrity\nys\nre\ndğ’™\nof\ne\nigh\nco\n(o\nne\n(\nco\nEE",
          "Column_4": "a\na\nh\nde\nve\nio\npr\nğ‘—,\ntw\ngiv\ne\nrr\nr\no\nor\nrr\nG",
          "Column_5": "",
          "Column_6": "e\nve\ng\nle\na\nc\na\nts\nt\nX\nn\nn\nn\nn\nc\nI\no\nE",
          "Column_7": "re\nsS\nth\nar\nssif\nes\nlda\nth\nhe\nGs\nFig\nor\ndin\ndic\nolle\nt p\nns\nXG",
          "Column_8": "su\nO\nat\nnt\ny\nbet\nta\nep\nho\neq\n.6\nma\ng\nate\nct\nre\nbe\ns",
          "Column_9": "lts\nTA\nth\nhe\ncar\nw\nse\nat\nriz\nue\n.T\nlis\npa\nst\ned\nsen\ntw\neq"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "A\ngur\nzin\nare\neb\nnre\nEO\nond\ncau\nphy\ndel",
          "Column_3": "ccurac\ne5:\ngof\ngiv\naselin\npres\nGd\n-bes\nseth\nsiolo\ning.",
          "Column_4": "y\nR\nga\nen\ne\nen\nata\nt p\ney\ngi",
          "Column_5": "F\nlts\ndet\nTa\ntho\nion\nato\norm\nplic\nsig",
          "Column_6": "1 scor\noft\nectio\nb.3\ndson\nsfro\nolet\nanc\nitly\nnals",
          "Column_7": "e\nhe\nn\nan\nth\nm\nal.\ne o\next\nas",
          "Column_8": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)": "(c)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(b)": "(d)"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Yes/No Classification of EEG data from CLIS patients",
      "authors": [
        "Sophie Adama",
        "Martin Bogdan"
      ],
      "year": "2021",
      "venue": "International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "2",
      "title": "Electromyography Monitoring Systems in Rehabilitation: A Review of Clinical Applications",
      "authors": [
        "Muhammad Al-Ayyad",
        "Abu Hamza",
        "Roberto Owida",
        "Bassam De Fazio",
        "Paolo Al-Naami",
        "Visconti"
      ],
      "year": "2023",
      "venue": "Wearable Devices and Signal Acquisition Methodologies. Electronics"
    },
    {
      "citation_id": "3",
      "title": "A lightweight hybrid CNN-LSTM explainable model for ECG-based arrhythmia detection",
      "authors": [
        "Negin Alamatsaz",
        "Leyla Tabatabaei",
        "Mohammadreza Yazdchi",
        "Hamidreza Payan",
        "Nima Alamatsaz",
        "Fahimeh Nasimi"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "4",
      "title": "Inter-database validation of a deep learning approach for automatic sleep scoring",
      "authors": [
        "Diego Alvarez-Estevez",
        "Roselyne Rijsman"
      ],
      "year": "2021",
      "venue": "PloS one"
    },
    {
      "citation_id": "5",
      "title": "Bio-signal based motion control system using deep learning models: A deep learning approach for motion classification using EEG and EMG signal fusion",
      "authors": [
        "Heba Aly",
        "M Sherin",
        "Youssef"
      ],
      "year": "2023",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "6",
      "title": "Movement Disorders Detection in Parkinson's Patients using Hybrid Classifier",
      "authors": [
        "Mouazma Batool",
        "Madiha Javeed"
      ],
      "year": "2022",
      "venue": "International Bhurban Conference on Applied Sciences and Technology (IBCAST)"
    },
    {
      "citation_id": "7",
      "title": "AASM scoring manual updates for",
      "authors": [
        "Rita Richard B Berry",
        "Charlene Brooks",
        "Susan Gamaldo",
        "Robin Harding",
        "Stuart Lloyd",
        "Matthew Quan",
        "Bradley Troester",
        "Vaughn"
      ],
      "year": "2017",
      "venue": "AASM scoring manual updates for"
    },
    {
      "citation_id": "8",
      "title": "MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals",
      "authors": [
        "Donghong Cai",
        "Junru Chen",
        "Yang Yang",
        "Teng Liu",
        "Yafeng Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "9",
      "title": "Contrastive learning of global and local features for medical image segmentation with limited annotations",
      "authors": [
        "Krishna Chaitanya",
        "Ertunc Erdil",
        "Neerav Karani",
        "Ender Konukoglu"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Systems"
    },
    {
      "citation_id": "10",
      "title": "Braininterfaces for communication and rehabilitation",
      "authors": [
        "Ujwal Chaudhary",
        "Niels Birbaumer",
        "Ander Ramos-Murguialday"
      ],
      "year": "2016",
      "venue": "Nature Reviews Neurology"
    },
    {
      "citation_id": "11",
      "title": "Jing Qin, and Pheng Ann Heng. 2020. Unsupervised bidirectional cross-modality adaptation via deeply synergistic image and feature alignment for medical image segmentation",
      "authors": [
        "Cheng Chen",
        "Qi Dou",
        "Chen Hao"
      ],
      "year": "2020",
      "venue": "IEEE transactions on medical imaging"
    },
    {
      "citation_id": "12",
      "title": "Brainnet: Epileptic wave detection from seeg with hierarchical graph diffusion learning",
      "authors": [
        "Junru Chen",
        "Yang Yang",
        "Tao Yu",
        "Yingying Fan",
        "Xiaolong Mo",
        "Carl Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "13",
      "title": "AF classification from a short single lead ECG recording: The PhysioNet/computing in cardiology challenge",
      "authors": [
        "Chengyu Gari D Clifford",
        "Benjamin Liu",
        "H Moody",
        "Ikaro Lehman Li-Wei",
        "Qiao Silva",
        "Li",
        "Roger Johnson",
        "Mark"
      ],
      "year": "2017",
      "venue": "Computing in Cardiology (CinC)"
    },
    {
      "citation_id": "14",
      "title": "Minirocket: A very fast (almost) deterministic transform for time series classification",
      "authors": [
        "Angus Dempster",
        "Daniel Schmidt",
        "Geoffrey Webb"
      ],
      "year": "2021",
      "venue": "Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "15",
      "title": "SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling",
      "authors": [
        "Jiaxiang Dong",
        "Haixu Wu",
        "Haoran Zhang",
        "Li Zhang",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "year": "2023",
      "venue": "SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised domain adaptation by statistics alignment for deep sleep staging networks",
      "authors": [
        "Jiahao Fan",
        "Hangyu Zhu",
        "Xinyu Jiang",
        "Long Meng",
        "Chen Chen",
        "Cong Fu",
        "Huan Yu",
        "Chenyun Dai",
        "Wei Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "17",
      "title": "Ensemble Technique based Parkison's Disease Detection from FOG and EEG Signals",
      "authors": [
        "Kshitij Goel",
        "Neetu Sood",
        "Indu Saini"
      ],
      "year": "2023",
      "venue": "World Conference on Communication & Computing (WCONF)"
    },
    {
      "citation_id": "18",
      "title": "ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Network for EEG-Based Emotion Recognition",
      "authors": [
        "Peiliang Gong",
        "Ziyu Jia",
        "Pengpai Wang",
        "Yueying Zhou",
        "Daoqiang Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "The TUH EEG CORPUS: A big data resource for automated EEG interpretation",
      "authors": [
        "Harati",
        "I Lopez",
        "J Obeid",
        "Picone",
        "Jacobson",
        "Tobochnik"
      ],
      "year": "2014",
      "venue": "IEEE signal processing in medicine and biology symposium (SPMB). IEEE"
    },
    {
      "citation_id": "20",
      "title": "Electrocardiogram-based emotion recognition systems and their applications in healthcare-A review",
      "authors": [
        "Muhammad Anas Hasnul",
        "Nor Azlina Ab",
        "Salem Aziz",
        "Mohamed Alelyani",
        "Azlan Mohana",
        "Aziz Abd"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "21",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "Flexible miniaturized sensor technologies for long-term physiological monitoring",
      "authors": [
        "Rongyan He",
        "Hao Liu",
        "Yan Niu",
        "Huiqing Zhang",
        "Guy Genin",
        "Feng Xu"
      ],
      "year": "2022",
      "venue": "npj Flexible Electronics"
    },
    {
      "citation_id": "23",
      "title": "Developing a Method for Classifying Electro-Oculography (EOG) Signals Using Deep Learning",
      "authors": [
        "Radwa Hossieny",
        "Manal Tantawi",
        "Mohamed Fahmy Tolba"
      ],
      "year": "2022",
      "venue": "International Journal of Intelligent Computing and Information Sciences"
    },
    {
      "citation_id": "24",
      "title": "A dataset of EEG and EOG from an auditory EOG-based communication system for patients in locked-in state",
      "authors": [
        "Andres Jaramillo-Gonzalez",
        "Shizhe Wu",
        "Alessandro Tonin",
        "Aygul Rana",
        "Khalili Majid",
        "Niels Ardali",
        "Ujwal Birbaumer",
        "Chaudhary"
      ],
      "year": "2021",
      "venue": "Scientific data"
    },
    {
      "citation_id": "25",
      "title": "Exploiting Interactivity and Heterogeneity for Sleep Stage Classification Via Heterogeneous Graph Neural Network",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Yuhan Zhou",
        "Xiyang Cai",
        "Peng Zheng",
        "Qiang Li",
        "Jing Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
      "authors": [
        "Wei-Bang Jiang",
        "Li-Ming Zhao",
        "Bao-Liang Lu"
      ],
      "year": "2024",
      "venue": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
      "arxiv": "arXiv:2405.18765"
    },
    {
      "citation_id": "27",
      "title": "Time-llm: Time series forecasting by reprogramming large language models",
      "authors": [
        "Ming Jin",
        "Shiyu Wang",
        "Lintao Ma",
        "Zhixuan Chu",
        "James Zhang",
        "Xiaoming Shi",
        "Pin-Yu Chen",
        "Yuxuan Liang",
        "Yuan-Fang Li",
        "Shirui Pan"
      ],
      "year": "2023",
      "venue": "Time-llm: Time series forecasting by reprogramming large language models",
      "arxiv": "arXiv:2310.01728"
    },
    {
      "citation_id": "28",
      "title": "A comprehensive survey on emotion recognition based on electroencephalograph (EEG) signals. Multimedia Tools and Applications",
      "authors": [
        "Kranti Kamble",
        "Joydeep Sengupta"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on emotion recognition based on electroencephalograph (EEG) signals. Multimedia Tools and Applications"
    },
    {
      "citation_id": "29",
      "title": "Internal state: dynamic, interconnected communication loops distributed across body, brain, and time",
      "authors": [
        "Emma Jessleen K Kanwal",
        "Rachel Coddington",
        "Daniela Frazer",
        "Grace Limbania",
        "Karla Turner",
        "Michael Davila",
        "Valarie Givens",
        "Sandeep Williams",
        "Sara Datta",
        "Wasserman"
      ],
      "year": "2021",
      "venue": "Integrative and Comparative Biology"
    },
    {
      "citation_id": "30",
      "title": "DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "31",
      "title": "Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the EEG",
      "authors": [
        "Bob Kemp",
        "Bert Aeilko H Zwinderman",
        "Hilbert Tuk",
        "Josefien Jl Ac Kamphuisen",
        "Oberye"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "32",
      "title": "ISRUC-Sleep: A comprehensive public dataset for sleep researchers",
      "authors": [
        "Sirvan Khalighi",
        "Teresa Sousa",
        "JosÃ© Moutinho Santos",
        "Urbano Nunes"
      ],
      "year": "2016",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "33",
      "title": "Time and frequency domain analysis of physiological features during autonomic dysreflexia after spinal cord injury",
      "authors": [
        "Ana Karina Kirby",
        "Sidharth Pancholi",
        "Zada Anderson",
        "Caroline Chesler",
        "Thomas Everett",
        "Bradley Duerstock"
      ],
      "year": "2023",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "34",
      "title": "Teacher assistant-based knowledge distillation extracting multi-level features on single channel sleep EEG",
      "authors": [
        "Heng Liang",
        "Yucheng Liu",
        "Haichao Wang",
        "Ziyu Jia",
        "Brainnetome Center"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI"
    },
    {
      "citation_id": "35",
      "title": "EEG emotion recognition using improved graph neural network with channel selection",
      "authors": [
        "Xuefen Lin",
        "Jielin Chen",
        "Weifeng Ma",
        "Wei Tang",
        "Yuchen Wang"
      ],
      "year": "2023",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "36",
      "title": "Multi-channel EEG-based emotion recognition via a multi-level features guided capsule network",
      "authors": [
        "Yu Liu",
        "Yufeng Ding",
        "Chang Li",
        "Juan Cheng",
        "Rencheng Song",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "37",
      "title": "Bstt: A bayesian spatial-temporal transformer for sleep staging",
      "authors": [
        "Yuchen Liu",
        "Ziyu Jia"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "38",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Fengmao Lv",
        "Xiang Chen",
        "Yanyong Huang",
        "Lixin Duan",
        "Guosheng Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "Visual classification via description from large language models",
      "authors": [
        "Sachit Menon",
        "Carl Vondrick"
      ],
      "year": "2022",
      "venue": "Visual classification via description from large language models",
      "arxiv": "arXiv:2210.07183"
    },
    {
      "citation_id": "40",
      "title": "DeepArr: An investigative tool for arrhythmia detection using a contextual deep neural network from electrocardiograms (ECG) signals",
      "authors": [
        "Wissal Midani",
        "Wael Ouarda",
        "Mounir Ben"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "41",
      "title": "ViT-HGR: Vision transformer-based hand gesture recognition from high density surface EMG signals",
      "authors": [
        "Mansooreh Montazerin",
        "Soheil Zabihi",
        "Elahe Rahimian",
        "Arash Mohammadi",
        "Farnoosh Naderkhani"
      ],
      "year": "2022",
      "venue": "International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "42",
      "title": "A comprehensive survey on multimodal medical signals fusion for smart healthcare systems",
      "authors": [
        "Ghulam Muhammad",
        "Fatima Alshehri",
        "Fakhri Karray",
        "Abdulmotaleb Saddik",
        "Mansour Alsulaiman",
        "Tiago H Falk"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "43",
      "title": "A time series is worth 64 words: Long-term forecasting with transformers",
      "authors": [
        "Yuqi Nie",
        "Nam Nguyen",
        "Phanwadee Sinthong",
        "Jayant Kalagnanam"
      ],
      "year": "2023",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "44",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "45",
      "title": "XSleepNet: Multi-view sequential model for automatic sleep staging",
      "authors": [
        "Huy Phan",
        "Minh Oliver Y ChÃ©n",
        "Philipp Tran",
        "Alfred Koch",
        "Maarten Mertins",
        "Vos"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "46",
      "title": "L-SeqSleepNet: Whole-cycle long sequence modelling for automatic sleep staging",
      "authors": [
        "Huy Phan",
        "Kristian Lorenzen",
        "Elisabeth Heremans",
        "Minh Oliver Y ChÃ©n",
        "Philipp Tran",
        "Alfred Koch",
        "Mathias Mertins",
        "Baumert",
        "Maarten Kaare B Mikkelsen",
        "Vos"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "47",
      "title": "Automatic sleep staging of EEG signals: recent development, challenges, and future directions",
      "authors": [
        "Huy Phan",
        "Kaare Mikkelsen"
      ],
      "year": "2022",
      "venue": "Physiological Measurement"
    },
    {
      "citation_id": "48",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "49",
      "title": "Deep learning in physiological signal data: A survey",
      "authors": [
        "Beanbonyka Rim",
        "Nak-Jun Sung",
        "Sedong Min",
        "Min Hong"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "50",
      "title": "Communication in the presence of noise",
      "authors": [
        "Claude Shannon"
      ],
      "year": "1949",
      "venue": "Proceedings of the IRE"
    },
    {
      "citation_id": "51",
      "title": "An automated wavelet-based sleep scoring model using eeg, emg, and eog signals with more than 8000 subjects",
      "authors": [
        "Manish Sharma",
        "Anuj Yadav",
        "Jainendra Tiwari",
        "Murat Karabatak",
        "Ozal Yildirim",
        "U Rajendra"
      ],
      "year": "2022",
      "venue": "International Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "52",
      "title": "EEG conformer: Convolutional transformer for EEG decoding and visualization",
      "authors": [
        "Yonghao Song",
        "Qingqing Zheng",
        "Bingchuan Liu",
        "Xiaorong Gao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "53",
      "title": "Spectral analysis of signals",
      "authors": [
        "Petre Stoica",
        "Randolph Moses"
      ],
      "year": "2005",
      "venue": "Spectral analysis of signals"
    },
    {
      "citation_id": "54",
      "title": "A Dual-Branch Dynamic Graph Convolution Based Adaptive TransFormer Feature Fusion Network for EEG Emotion Recognition",
      "authors": [
        "Mingyi Sun",
        "Weigang Cui",
        "Shuyue Yu",
        "Hongbin Han",
        "Bin Hu",
        "Yang Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "TinySleepNet: An efficient deep learning model for sleep stage scoring based on raw single-channel EEG",
      "authors": [
        "Akara Supratak",
        "Yike Guo"
      ],
      "year": "2020",
      "venue": "International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "56",
      "title": "EEG-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "Wei Tao",
        "Chang Li",
        "Rencheng Song",
        "Juan Cheng",
        "Yu Liu",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Atlas, rules, and recording techniques for the scoring of cyclic alternating pattern (CAP) in human sleep",
      "authors": [
        "Mario Giovanni Terzano",
        "Liborio Parrino",
        "Adriano Sherieri",
        "Ronald Chervin",
        "Sudhansu Chokroverty",
        "Christian Guilleminault",
        "Max Hirshkowitz",
        "Mark Mahowald",
        "Harvey Moldofsky",
        "Agostino Rosa"
      ],
      "year": "2001",
      "venue": "Sleep medicine"
    },
    {
      "citation_id": "58",
      "title": "Biopotentials and electrophysiology measurement. Measurement, Instrumentation, and Sensors Handbook",
      "authors": [
        "V Nitish",
        "Thakor"
      ],
      "year": "2017",
      "venue": "Biopotentials and electrophysiology measurement. Measurement, Instrumentation, and Sensors Handbook"
    },
    {
      "citation_id": "59",
      "title": "Auditory electrooculogram-based communication system for ALS patients in transition from locked-in to complete locked-in state",
      "authors": [
        "Alessandro Tonin",
        "Andres Jaramillo-Gonzalez",
        "Aygul Rana",
        "Majid Khalili-Ardali",
        "Niels Birbaumer",
        "Ujwal Chaudhary"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "60",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "61",
      "title": "Multi-granularity cross-modal alignment for generalized medical visual representation learning",
      "authors": [
        "Fuying Wang",
        "Yuyin Zhou",
        "Shujun Wang",
        "Varut Vardhanabhuti",
        "Lequan Yu"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "62",
      "title": "Generalizable sleep staging via multi-level domain alignment",
      "authors": [
        "Jiquan Wang",
        "Sha Zhao",
        "Haiteng Jiang",
        "Shijian Li",
        "Tao Li",
        "Gang Pan"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "63",
      "title": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
      "authors": [
        "Wenhai Wang",
        "Zhe Chen",
        "Xiaokang Chen",
        "Jiannan Wu",
        "Xizhou Zhu",
        "Gang Zeng",
        "Ping Luo",
        "Tong Lu",
        "Jie Zhou",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "64",
      "title": "A Novel Emotion Recognition Method Based on the Feature Fusion of Single-Lead EEG and ECG Signals",
      "authors": [
        "Xiaoman Wang",
        "Jianwen Zhang",
        "Chunhua He",
        "Heng Wu",
        "Lianglun Cheng"
      ],
      "year": "2023",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "65",
      "title": "Timesnet: Temporal 2d-variation modeling for general time series analysis",
      "authors": [
        "Haixu Wu",
        "Tengge Hu",
        "Yong Liu",
        "Hang Zhou",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "year": "2022",
      "venue": "The eleventh international conference on learning representations"
    },
    {
      "citation_id": "66",
      "title": "Deep Learning-Based ECG Arrhythmia Classification: A Systematic Review. Applied Sciences",
      "authors": [
        "Qiao Xiao",
        "Khuan Lee",
        "Aisah Siti",
        "Iskasymar Mokhtar",
        "Ahmad Ismail",
        "Luqman Bin Md Qiuxia",
        "Poh Zhang",
        "Lim"
      ],
      "year": "2023",
      "venue": "Deep Learning-Based ECG Arrhythmia Classification: A Systematic Review. Applied Sciences"
    },
    {
      "citation_id": "67",
      "title": "Unified contrastive learning in image-text-label space",
      "authors": [
        "Jianwei Yang",
        "Chunyuan Li",
        "Pengchuan Zhang",
        "Bin Xiao",
        "Ce Liu",
        "Lu Yuan",
        "Jianfeng Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "68",
      "title": "Integration of biological systems with electronic-mechanical assemblies",
      "authors": [
        "Ning Yi",
        "Haitao Cui",
        "Lijie Zhang",
        "Huanyu Cheng"
      ],
      "year": "2019",
      "venue": "Acta biomaterialia"
    },
    {
      "citation_id": "69",
      "title": "Coca: Contrastive captioners are image-text foundation models",
      "authors": [
        "Jiahui Yu",
        "Zirui Wang",
        "Vijay Vasudevan",
        "Legg Yeung",
        "Mojtaba Seyedhosseini",
        "Yonghui Wu"
      ],
      "year": "2022",
      "venue": "Coca: Contrastive captioners are image-text foundation models",
      "arxiv": "arXiv:2205.01917"
    },
    {
      "citation_id": "70",
      "title": "Brant-2: Foundation Model for Brain Signals",
      "authors": [
        "Zhizhang Yuan",
        "Daoze Zhang",
        "Junru Chen",
        "Gefei Gu",
        "Yang Yang"
      ],
      "year": "2024",
      "venue": "Brant-2: Foundation Model for Brain Signals"
    },
    {
      "citation_id": "71",
      "title": "PPi: Pretraining Brain Signal Model for Patient-independent Seizure Detection",
      "authors": [
        "Zhizhang Yuan",
        "Daoze Zhang",
        "Yang Yang",
        "Junru Chen",
        "Yafeng Li"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "72",
      "title": "Trahgr: Transformer for hand gesture recognition via electromyography",
      "authors": [
        "Soheil Zabihi",
        "Elahe Rahimian",
        "Amir Asif",
        "Arash Mohammadi"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "73",
      "title": "Brant: Foundation Model for Intracranial Neural Signal",
      "authors": [
        "Daoze Zhang",
        "Zhizhang Yuan",
        "Yang Yang",
        "Junru Chen",
        "Jingjing Wang",
        "Yafeng Li"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "74",
      "title": "Jing Pan, et al. 2022. Multimodal data for the detection of freezing of gait in",
      "authors": [
        "Wei Zhang",
        "Zhuokun Yang",
        "Hantao Li",
        "Debin Huang",
        "Lipeng Wang",
        "Yanzhao Wei",
        "Lei Zhang",
        "Lin Ma",
        "Huanhuan Feng"
      ],
      "year": "2022",
      "venue": "Parkinson's disease. Scientific data"
    },
    {
      "citation_id": "75",
      "title": "Self-supervised contrastive pre-training for time series via time-frequency consistency",
      "authors": [
        "Xiang Zhang",
        "Ziyuan Zhao",
        "Theodoros Tsiligkaridis",
        "Marinka Zitnik"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "76",
      "title": "One Fits All: Power General Time Series Analysis by Pretrained LM",
      "authors": [
        "Tian Zhou",
        "Peisong Niu",
        "Xue Wang",
        "Liang Sun",
        "Rong Jin"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "77",
      "title": "2021. eyeSay: Eye electrooculography decoding with deep learning",
      "authors": [
        "Jiadao Zou",
        "Qingxue Zhang"
      ],
      "venue": "IEEE International Conference on Consumer Electronics (ICCE)"
    }
  ]
}