{
  "paper_id": "2011.09767v1",
  "title": "Deep Residual Local Feature Learning For Speech Emotion Recognition",
  "published": "2020-11-19T11:04:31Z",
  "authors": [
    "Sattaya Singkul",
    "Thakorn Chatchaisathaporn",
    "Boontawee Suntisrivaraporn",
    "Kuntpong Woraratpanya"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Residual Feature Learning",
    "CNN Network",
    "Log-Mel Spectrogram",
    "Chromagram"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is becoming a key role in global business today to improve service efficiency, like call center services. Recent SERs were based on a deep learning approach. However, the efficiency of deep learning depends on the number of layers, i.e., the deeper layers, the higher efficiency. On the other hand, the deeper layers are causes of a vanishing gradient problem, a low learning rate, and high time-consuming. Therefore, this paper proposed a redesign of existing local feature learning block (LFLB). The new design is called a deep residual local feature learning block (DeepResLFLB). DeepResLFLB consists of three cascade blocks: LFLB, residual local feature learning block (ResLFLB), and multilayer perceptron (MLP). LFLB is built for learning local correlations along with extracting hierarchical correlations; DeepResLFLB can take advantage of repeatedly learning to explain more detail in deeper layers using residual learning for solving vanishing gradient and reducing overfitting; and MLP is adopted to find the relationship of learning and discover probability for predicted speech emotions and gender types. Based on two available published datasets: EMODB•and RAVDESS, the proposed DeepResLFLB can significantly improve performance when evaluated by standard metrics: accuracy, precision, recall, and F1-score.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional analysis has been an active research area for a few decades, especially in recognition domains of text and speech emotions. Even if text and speech emotions are closely relevant, both kinds of emotions have different challenges. One of the challenges in text emotion recognition is ambiguous words, resulting from omitted words  [1, 2] . On the other hand, one of the challenges in speech emotion recognition is creating an efficient model. However, this paper focuses on only the recognition of speech emotions. In this area, two types of information, linguistic and paralinguistic, were mainly considered in speech emotion recognition. The linguistic information refers to the meaning or context of speech. The paralinguistic information implies the implicit message meaning, like the emotion in speech  [3, 4, 5, 6] . Speech characteristics can interpret the meaning of speech; therefore, behavioral expression was investigated in most of the speech emotion recognition works  [7, 8, 9] .\n\nIn recent works, local feature learning block (LFLB)  [10] , one of the efficient methods, has been used in integrating local and global speech emotion features, which provide better results in recognition. Inside LFLB, convolution neural network (CNN) was used for extracting local features, and then long shortterm memory (LSTM) was applied for extracting contextual dependencies from those local features to learn in a time-related relationship. However, vanishing gradient problems may occur with CNN  [11] . Therefore, residual deep learning was applied to the CNN by using skip-connection to reduce unnecessary learning and add feature details that may be lost in between layers.\n\nFurthermore, the accuracy of speech recognition does not only rely on the efficiency of a model, but also of a speech feature selection  [12] . In terms of speech characteristics, there are many distinctive acoustic features that usually used in recognizing the speech emotion, such as continuous features, qualitative features, and spectral features  [13, 12, 14, 15, 16] . Many of them have been investigated to recognize speech emotions. Some researchers compared the pros and cons of each feature, but no one can identify which feature was the best one until now  [3, 4, 17, 18] .\n\nAs previously mentioned, we proposed a method to improve the efficiency of LFLB  [11]  for deeper learning. The proposed method, deep residual local feature learning block (DeepResLFLB), was inspired by the concept of human brain learning; that is, 'repeated reading makes learning more effective,' as the same way that Sari  [19]  and Shanahan  [20]  were used. Responding to our inspired concept, we implemented a learning method for speech emotion recognition with three parts: Part 1 is for general learning, like human reading for the first time, Part 2 is for further learning, like additional readings, and the last part is for associating parts learned to decide types of emotions. Besides, the feature selection is compared with two types of distinctive features to find the most effective feature in our work: the normal and specific distinctive features are log-mel spectrogram (LMS), which is fully filtered sound elements, and MFCC deltas, delta-deltas, and chromagram (LMSDDC) are more clearly identify speech characteristics extracted based on the human mood.\n\nOur main contributions of this paper are as follows: (i) Deep residual local feature learning block (DeepResLFLB) was proposed. DeepResLFLB was arranged its internal network as LFLB, batch normalization (BN), activation function, normalization-activation-CNN (NAC), and deep layers. (ii) Learning sequences of DeepResLFLB were imitated from human re-reads. (iii) Speech emotion features, based on human mood determination factors such as LMS and LMSDDC, were applied and compared their performances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Reviews",
      "text": "Model efficiency is one of the important factors in SER. Many papers focused on learning methods of machine learning or deep learning. Demircan  [17]  introduced fuzzy c-mean as a preprocessing step to group and add characteristics before using machine learning. Venkataramanan  [21]  studied of using deep learning in SER. The findings of the study revealed that CNN outperformed the traditional machine learning. Also, Huang  [15]  showed that semi-CNN in SER can increase accuracy. Zhao  [10]  presented the use of CNN in conjunction with LSTM to extract and learn features. Zhao's method used a sequence of CNN in a block style, consisting of CNN, BN, activation function, and pooling, for local feature learning, and then used LSTM for extracting contextual dependencies in a timerelated relationship. In this way, both local and global features are extracted and learned.\n\nIt is undeniable that the effectiveness of deep learning mainly depends on the data size for training  [22] . Recently, Google brain research  [23]  proposed data augmentation, one of the efficient techniques that can increase the amount of data, by adding spectrogram characteristics, also known as \"Spectrogram Augmentation.\" This augmentation consists of time warping to see more time shift patterns, time masking to reduce the overfitting rate of the model and improve the sound tolerance that may have characteristics of silence, frequency masking to reduce the overfitting rate and increase sound resistance from concealing characteristics of a specific wavelength. The spectrogram is a basic feature of sound that can lead to various specific features. Therefore, by using above methods, the model can learn more perspectives of the data.\n\nAlso, different features lead to different performances in speech emotion recognition. Among the features of speech, mel-frequency cepstral coefficient (MFCC)  [21] , which can be characterized by the frequency filter in the range of 20 Hz to 20 kHz, similar to human hearing, is widely used to obtain coefficients from the filtered sound. Recent research papers  [21, 17]  used the difference of MFCC to get more specific details, but, in the aspect of MFCC, it has no time relationship. Therefore, many papers  [21, 10, 15]  used mel spectrogram (MS) instead. MS can respond to the time relationship, thus providing better results than just using MFCC. Besides, music can be looked different from speech; therefore, chromagram is widely used instead of MFCC, since it can provide better features than normal MFCC and MS.\n\nOur work is different from the previously mentioned works in that the deep residual local feature learning block (DeepResLFLB) was redesigned from LFLB. This method helps reduce the chance of feature and updated losses caused by CNN model in the LFLB, especially in deeper layers. DeepResLFLB uses a repeated learning style that local features extracted from a bias frame with silent voice (see subsection 3.3) can be learned through a residual deep learning approach. Moreover, we extracted distinctive features based on a concept of determining human emotions, consisting of prosodic  [24] , filter bank  [24, 21] , and glottal flow  [25] . These three features in conjunction with our ResLFLB can improve learning efficiency.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Proposed Model",
      "text": "To enable SER as efficiently as possible, the following factors: raw datasets, environments, and features are included in our system design. Based on such factors, a new designed framework, called DeepResLFLB, was proposed as shown in Fig.  1 . This framework consists of five parts: (i) raw data preparation, (ii) voice activity detection, (iii) bias frame cleaning, (iv) feature extraction, and (v) deep learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Raw Data Preparation",
      "text": "Due to the complex nature of datasets and the difference of languages like EMODB in Berlin German and RAVDESS in English, the representation of the original datasets may not be enough for training a model based on deep learning. Therefore, increasing a variety of data to see more new dimensions or characteristics is essential. Responding to this, various data augmentation techniques, including noise adding, pitch tuning, and spectrogram, were used in this work to make the model more robust to noise and unseen voice patterns.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Voice Activity Detection",
      "text": "Although both datasets, EMODB and RAVDESS, were produced in a closed environment, quite a bit of noise, they were found that noise remains at the starting and stopping points of sound records. Indeed, noise is not related to speakers' voice, so it could be removed. Here, voice activity detection  [26]  was used to detect only voice locations, i.e., excluding noise locations. As a result of the voice activity detection, selected frames can be efficiently analyzed and classified male and female voices by energy-base features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Bias Frame Cleaning",
      "text": "Bias frame cleaning is used as a postprocessing of voice activity detection; that is, each frame segmented by the voice activity detection is identified its loudness through Fourier transform (FT). If FT coefficients of a segmented frame are zero, that frame is identified as no significant information for emotional analysis, so it is rejected.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction",
      "text": "Model performance of deep learning mainly depends on features. The good features usually gain more model performance. Thus, this paper focuses on efficient extraction of human emotion features. Naturally, speech signals always contain human emotions. In other words, we can extract human emotions from speech signals. Here, we briefly describe three important components of speech signals: glottal flow, prosody, and human hearing. Glottal flow can be viewed as a source of speech signals  [25] . It mainly produces fundamental frequencies  [27]  or latent sounds within the speech. Prosody is vocal frequencies, which are produced from the air pushed by the lung  [25] . It contains important characteristics, such as intonation, tone, stress, and rhythm. On the other hand, for human hearing, MFCC is one of the analytical tools that can mimic the behavior of human ears by applying cepstral analysis  [28] . Based on our assumption of extracting better emotion features, two important factors are included for feature extraction design: (i) the wide band frequencies of speech signals are regarded as much as possible to cover important features of speech emotions, and (ii) time-frequency processing is used for extracting speech emotions. Here, log-mel spectrogram (LMS) was used as time-frequency representation for emotion features. Two additional features extracted based on MFCC were delta and delta-delta. Furthermore, chromagram feature  [29, 30]  was extracted as one of the emotion features. Fig.  2  shows our emotion feature extraction. As a result, four features, LMS, delta, delta-delta, and chromagram were used as emotional representation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Deep Learning",
      "text": "Inspired by learning characteristics of human brain activity, i.e., the more repeated reading, the more comprehension. It is similar with Shanahan's definition, called \"repeatedly reads\" or \"re-reads\"  [20] . Responding to the use of re-reading theory for improving the accuracy of SER, we designed a feature learning method as shown in Fig.  3 , consisting of three sections: (i) main feature learning (MFL), (ii) sub-feature learning (SFL), and (iii) extracted relation of feature distribution (ERFD). Main Feature Learning (MFL) Section was designed based on behavior of human brain, like first reading that a human brain starts to learn things. We designed MFLS similar with the LFLB procedure to learn locally basic information as the following steps: (i) 2D-CNN was used to extract necessary features; (ii) BN was applied to enhance learning efficiency of a model; (iii) activation functions converted data to suit for the learning model; and (iv) pooling was for reducing feature size and increased learning speed.\n\nSub-Feature Learning (SFL) Section was a further learning process that plays a role in assembling repeated reading for deeper learning. In general, LFLB may be at risk of a vanishing gradient problem that affects learning efficiency. Therefore, we have improved the LFLB's efficient by means of residual deep learning, or also known as skipping connections, to skip deeper learning layers that are unnecessary and add more feature details after passing each learning layer; this can avoid the vanishing gradient problem. For sub-feature learning, there are two important sections: (i) A normal LFLB was used as a preprocessing phase of block to transform input data dimensions into a suitable form for the next section. (ii) Depth layers learned features in more depth by using the network sequence as normalization-activation-CNN (NAC)  [31] , which is a sequence of sub-layers in residual deep learning. With this structure, it can reduce test error. In addition, more deeper learning layers may be at risk of a vanishing gradient problem; therefore, the skipping connection was added at an input of this section to compensate features lost in deeper layers. This arrangement of two sections is called residual local feature block (ResLFLB) as shown in Fig.  4 . The LFLB also known as a dimensional converter and the NAC final layer had the same output filters to determine the number of output filters of ResLFLB. The kernel size in the first and last layer in deep layers were one. Furthermore, in between the first and last learning layers, the bottleneck design, the compression/decompression strategy, was applied to achieve higher learning performance.\n\nExtracted Relation of Feature Distribution (ERFD) Section was implemented with multilayer perceptron (MLP) to extract the relationship of learning results. A sequence of processes are as follows: (i) Activation ReLU transformed the data to the suitable relationship; (ii) Max pooling reduced data size and extracted important data based on maximum values; (iii) Dropout reduced the overfitting of the model; (iv) Dense layers obtained relationships in which neurons equals the number of classes needed to predict; and (v) Activation softmax determined the probability of predicting the emotions.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Discussion",
      "text": "The proposed DeepResLFLB and LMSDDC were evaluated with two main objectives: (i) classification performance and (ii) model performance. In classification performance, four metrics, accuracy, precision, recall, and F1-score as defined by (  1 ), (  2 ), (3), and (4), respectively, were used for evaluation. In model performance, validation loss was used as monitoring vanishing gradient problems and the number of CNN layer parameters was used as indicating resource-consuming. The experiments were conducted in comparison with three models: the normal ML with fuzzy c-mean  [17] , traditional LFLB  [10] , and DeepResLFLB, and two different features: LMS and LMSDDC. Note that, in Tables  2  and 4 , Dermircan's method was excluded from those experiments due to a mismatch of feature dimensions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Accuracy =",
      "text": "true positive + true negative number of data\n\nP recision = true positive total predicted positive\n\nRecall = true positive total actual positive\n\nDataset preparation Two available published datasets: Berlin emotional database (EMODB)  [32]  and Ryerson audio-visual database (RAVDESS)  [33]  were used to evaluate speech emotion performance of our and baseline methods. Two key factors of both selected datasets are the difference in data size and language vocalization that can prove the performance of test methods. EMODB is a German speech in Berlin with 535 utterances and RAVDESS is English speech with 1440 utterances. EMODB dataset was recorded by male and female voices and contained seven different emotions: happiness, sadness, angry, neutral, fear, boredom, and disgust while RAVDESS dataset has one more emotion than EMODB, that is the calm emotion. Here, each dataset was divided into three subsets: 80% for training set, 10% for validation set, and 10% for test set.\n\nParameter settings for learning model All learning models were set up with the following parameter settings. Learning rate (LR) is very important in deep learning, when compared to step rate to find the minimum gradient. Generally, a high LR may make it over the minimum point. On the other hand, a low LR may take a long time to reach the goal. Here, we choose Adam optimizer for our experiments. Its learning rate and maximum epoch were set to 0.001 and 150, respectively. In addition, plateau strategy was used for reducing the LR and for avoiding overstepping the minimum point. In this case, we set the minimum LR to 0.00001. Batch size of models was set to 10. Finally, if an error value tends to increase, the early stopping criteria is active and then take the model weight with the previous minimum error. Result discussion Based on dataset preparation and parameter setup for learning models, all experiments were used 5-fold validation. Tables  1  and 2  shows performance comparison between LMS and LMSDDC features, respectively, tested on EMODB dataset. It can be seen that LMSDDC feature (Table  2 ) provided the improvement of accuracy, precision, recall, and F1-score, when compared with LMS feature (Table  1 ). In the same way, when the same learning models with different features, LMS and LMSDDC, were tested on RAVDESS dataset, as shown in Tables  3  and 4 , the evaluation results were comparable, not much of an improvement. One of the main reasons is that RAVDESS has less speech variation than EMODB, as reported by Breitenstein research  [34] . The less variation of speech leads to the lower quality of features. This made no difference in quality of LMS and LMSDDC features. These results have proved that the LMSDDC feature extracted with three components of human emotions: glottal flow, prosody, and human hearing, usually provided wider speech band frequencies, can improve the speech emotion recognition, especially with high speech variation datasets. When considering the efficiency of the learning model, Tables  1, 2 , 3, and 4 show that DeepResLFLB outperforms the baselines with the highest accuracy, precision, recall, and F1-score. This achievement proved that a learning sequence of DeepResLFLB, imitated from \"repeatedly reading\" concept of human, is efficient. In addition, DeepResLFLB can avoid a vanishing gradient problem and reduce resource-consuming. Fig.  5  shows that DeepResLFLB had better validation loss and generalization; it can be seen from the graph that has less fluctuation than 2D-LFLB, and Table  5  reports that DeepResLFLB still used fewer parameters than the baseline model around 40%. These results have proved that DeepResLFLB used residual deep learning by arranging its internal network as LFLB, BN, activation function, NAC, and deep layers can solve vanishing gradient and resource-consuming. Besides, when regarding resource-consuming between LMS and LMSDDC, LMSDDC parameters were slightly more than a baseline.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This paper has described a DeepResLFLB model and LMSDDC feature for speech emotion recognition. The DeepResLFLB was redesigned from LFLB based on the 'repeatedly reads' concept while the LMSDDC was emotional feature extracted from speech signals based on human glottal flow and human hearing. Performance of our model and emotional feature was tested on two well-known databases. The results show that the DeepResLFLB can perform better than baselines and use fewer resources in learning layers. In addition, the proposed LMSDDC can outperform conventional LMS.\n\nAlthough DeepResLFLB presented in this paper have provided better performance in speech emotion recognition, many aspects still can be improved, especially activation function. In future work, we will apply different kinds of activation function in each section of neural network; this will improve the performance of DeepResLFLB.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This framework consists of ﬁve parts: (i) raw data preparation, (ii) voice",
      "page": 4
    },
    {
      "caption": "Figure 1: A deep residual local feature learning framework.",
      "page": 4
    },
    {
      "caption": "Figure 2: shows our emotion feature extraction. As a result, four features, LMS,",
      "page": 5
    },
    {
      "caption": "Figure 2: Feature extraction of LMS and LMSDDC",
      "page": 5
    },
    {
      "caption": "Figure 3: , consisting of three sections: (i) main feature learning (MFL),",
      "page": 5
    },
    {
      "caption": "Figure 3: A feature learning structure based on the re-reading theory.",
      "page": 6
    },
    {
      "caption": "Figure 4: A residual local feature learning block structure.",
      "page": 6
    },
    {
      "caption": "Figure 4: The LFLB also known as a dimensional",
      "page": 7
    },
    {
      "caption": "Figure 5: shows that DeepResLFLB had better valida-",
      "page": 9
    },
    {
      "caption": "Figure 5: Validation loss of learning models: conventional LFLB tested on EMODB (top-",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 2: A performance comparison of DeepResLFLB and baseline methods with",
      "data": [
        {
          "Method": "Demircan [17]\n1D-LFLB [10]\n2D-LFLB [10]\nDeepResLFLB",
          "Accuracy": "0.6755±0.0351\n0.7577±0.0241\n0.8269±0.0214",
          "Precision": "0.7549±0.0375\n0.7609±0.0224\n0.831±0.0228\n0.8404±0.0225 0.8481±0.0225 0.8298±0.0236 0.8328±0.0244",
          "Recall": "0.6295±0.0346\n0.7574±0.0318\n0.824±0.0215",
          "F1-score": "0.6295±0.0346\n0.7514±0.0275\n0.8233±0.0.0233"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: A performance comparison of DeepResLFLB and baseline methods with",
      "data": [
        {
          "Method": "Demircan [17]\n1D-LFLB [10]\n2D-LFLB [10]\nDeepResLFLB",
          "Accuracy": "-\n0.8355±0.0186\n0.8754±0.0232",
          "Precision": "-\n0.8385±0.0170\n0.8802±0.0237\n0.8922±0.0251 0.8961±0.0212 0.8856±0.0322 0.8875±0.0293",
          "Recall": "-\n0.8313±0.0205\n0.8733±0.0237",
          "F1-score": "-\n0.8322±0.0198\n0.8745±0.0226"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: A performance comparison of DeepResLFLB and baseline methods with",
      "data": [
        {
          "Method": "Demircan [17]\n1D-LFLB [10]\n2D-LFLB [10]\nDeepResLFLB",
          "Accuracy": "0.7528±0.0126\n0.9487±0.0138\n0.9456±0.0128",
          "Precision": "0.7809±0.0109\n0.9491±0.0134\n0.9438±0.0136\n0.9602±0.0075 0.9593±0.0072 0.9583±0.0066 0.9584±0.0071",
          "Recall": "0.7422±0.0076\n0.948±0.0123\n0.946±0.0129",
          "F1-score": "0.7479±0.0114\n0.9478±0.0133\n0.9442±0.0135"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: A performance comparison of DeepResLFLB and baseline methods with",
      "data": [
        {
          "Method": "Demircan [17]\n1D-LFLB [10]\n2D-LFLB [10]\nDeepResLFLB",
          "Accuracy": "-\n0.9367±0.0225\n0.9466±0.0159\n0.949±0.0142",
          "Precision": "-\n0.9363±0.0196\n0.9475±0.0171\n0.9492±0.0143",
          "Recall": "-\n0.9352±0.0218\n0.9441±0.0162\n0.9486±0.016",
          "F1-score": "-\n0.9347±0.0217\n0.9449±0.0168\n0.9484±0.0154"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Parsing thai social data: A new challenge for thai nlp",
      "authors": [
        "S Singkul",
        "B Khampingyot",
        "N Maharattamalai",
        "S Taerungruang",
        "T Chalothorn"
      ],
      "year": "2019",
      "venue": "14th International Joint Symposium on Artificial Intelligence and Natural Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Thai dependency parsing with character embedding",
      "authors": [
        "S Singkul",
        "K Woraratpanya"
      ],
      "year": "2019",
      "venue": "11th International Conference on Information Technology and Electrical Engineering (ICITEE)"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "5",
      "title": "Cooperative learning and its application to emotion recognition from speech",
      "authors": [
        "Z Zhang",
        "E Coutinho",
        "J Deng",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Automatic analysis of speech f0 contour for the characterization of mood changes in bipolar patients",
      "authors": [
        "A Guidi",
        "N Vanello",
        "G Bertschy",
        "C Gentili",
        "L Landini",
        "E Scilingo"
      ],
      "year": "2015",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "7",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2007",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "8",
      "title": "Implementation of wavelet packet transform and non linear analysis for emotion classification in stroke patient using brain signals",
      "authors": [
        "S Bong",
        "K Wan",
        "M Murugappan",
        "N Ibrahim",
        "Y Rajamanickam",
        "K Mohamad"
      ],
      "year": "2017",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "9",
      "title": "Detection of emotions in parkinson's disease using higher order spectral features from brain's electrical activity",
      "authors": [
        "R Yuvaraj",
        "M Murugappan",
        "N Ibrahim",
        "K Sundaraj",
        "M Omar",
        "K Mohamad",
        "R Palaniappan"
      ],
      "year": "2014",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "13",
      "title": "Study of empirical mode decomposition and spectral analysis for stress and emotion classification in natural speech",
      "authors": [
        "L He",
        "M Lech",
        "N Maddage",
        "N Allen"
      ],
      "year": "2011",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "14",
      "title": "Acoustic feature selection and classification of emotions in speech using a 3d continuous emotion model",
      "authors": [
        "H Pérez-Espinosa",
        "C Reyes-Garcia",
        "L Villaseñor-Pineda"
      ],
      "year": "2012",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Extraction of adaptive wavelet packet filterbank-based acoustic feature for speech emotion recognition",
      "authors": [
        "Y Huang",
        "A Wu",
        "G Zhang",
        "Y Li"
      ],
      "year": "2015",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Application of fuzzy c-means clustering algorithm to spectral features for emotion classification from speech",
      "authors": [
        "S Demircan",
        "H Kahramanli"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "18",
      "title": "Weighted spectral features based on local hu moments for speech emotion recognition",
      "authors": [
        "Y Sun",
        "G Wen",
        "J Wang"
      ],
      "year": "2015",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "19",
      "title": "The influence of using repeated reading strategy towards student's reading comprehension. Proceeding 1st Annual International Confrence on Islamic Education and Language: The Education and 4.0 Industrial Era in Islamic Perspective p",
      "authors": [
        "S Sari"
      ],
      "year": "2019",
      "venue": "The influence of using repeated reading strategy towards student's reading comprehension. Proceeding 1st Annual International Confrence on Islamic Education and Language: The Education and 4.0 Industrial Era in Islamic Perspective p"
    },
    {
      "citation_id": "20",
      "title": "Everything you wanted to know about repeated reading. Reading Rockets",
      "authors": [
        "T Shanahan"
      ],
      "year": "2017",
      "venue": "Everything you wanted to know about repeated reading. Reading Rockets"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition from speech",
      "authors": [
        "K Venkataramanan",
        "H Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech"
    },
    {
      "citation_id": "22",
      "title": "On the impact of data set size in transfer learning using deep neural networks",
      "authors": [
        "D Soekhoe",
        "P Putten",
        "A Plaat"
      ],
      "year": "2016",
      "venue": "On the impact of data set size in transfer learning using deep neural networks"
    },
    {
      "citation_id": "23",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "arxiv": "arXiv:1904.08779"
    },
    {
      "citation_id": "24",
      "title": "Exploring emotion specific features for emotion recognition system using pca approach",
      "authors": [
        "N Jagini",
        "R Rao"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Intelligent Computing and Control Systems (ICICCS)"
    },
    {
      "citation_id": "25",
      "title": "Glottal source and vocal-tract separation",
      "authors": [
        "G Degottex"
      ],
      "year": "2010",
      "venue": "Glottal source and vocal-tract separation"
    },
    {
      "citation_id": "26",
      "title": "An open-source speaker gender detection framework for monitoring gender equality",
      "authors": [
        "D Doukhan",
        "J Carrive",
        "F Vallet",
        "A Larcher",
        "S Meignier"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "The spectrum of glottal flow models",
      "authors": [
        "B Doval",
        "C Alessandro",
        "N Henrich"
      ],
      "year": "2006",
      "venue": "Acta acustica united with acustica"
    },
    {
      "citation_id": "28",
      "title": "Recognizing human emotional state from audiovisual signals",
      "authors": [
        "Y Wang",
        "L Guan"
      ],
      "year": "2008",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "29",
      "title": "The stimulus duration required to identify vowels, their octave, and their pitch chroma",
      "authors": [
        "K Robinson",
        "R Patterson"
      ],
      "year": "1995",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "30",
      "title": "Chromagram visualization of the singing voice",
      "authors": [
        "G Wakefield"
      ],
      "year": "1999",
      "venue": "International Workshop on Models and Analysis of Vocal Emissions for Biomedical Applications"
    },
    {
      "citation_id": "31",
      "title": "Identity mappings in deep residual networks",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Identity mappings in deep residual networks"
    },
    {
      "citation_id": "32",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "33",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "34",
      "title": "The contribution of speech rate and pitch variation to the perception of vocal emotions in a german and an american sample",
      "authors": [
        "C Breitenstein",
        "D Lancker",
        "I Daum"
      ],
      "year": "2001",
      "venue": "Cognition & Emotion"
    }
  ]
}