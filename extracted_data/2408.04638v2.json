{
  "paper_id": "2408.04638v2",
  "title": "Affective Computing In The Era Of Large Language Models: A Survey From The Nlp Perspective",
  "published": "2024-07-30T08:12:04Z",
  "authors": [
    "Yiqun Zhang",
    "Xiaocui Yang",
    "Xingle Xu",
    "Zeran Gao",
    "Yijie Huang",
    "Shiyi Mu",
    "Shi Feng",
    "Daling Wang",
    "Yifei Zhang",
    "Kaisong Song",
    "Ge Yu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLPoriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference-or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing (AC), encompassing the study and development of systems capable of recognizing, interpreting, processing, and simulating human affects through natural language and other modalities, seeks to bridge the gap between human emotions and machine understanding  Picard (2000) ;  Tao & Tan (2005) ;  Banafa (2024) . The AC task can be broadly categorized into two mainstream tasks: the Affective Understanding (AU) task  Zhao et al. (2023a)  and the Affective Generation (AG) task  Nie & Zhan (2022) ;  Pei et al. (2024) . Affective Understanding focuses on recognizing and interpreting human emotions, covering Sentiment Analysis  Medhat et al. (2014) ;  Taboada (2016) , Sarcasm Detection  Joshi et al. (2017) ;  Verma et al. (2021) , and so on. Affective Generation involves creating responses or content that can elicit specific emotional responses in humans to generate expressions with emotionally nuanced, including Emotional-aware Dialogue Generation  Rashkin et al. (2019) ;  Liu et al. (2021) , Creative Content Generation, etc. With the emergence of Large Language Models (LLMs) OpenAI (2023);  Touvron et al. (2023) ;  Zeng et al. (2023) , the Affective Computing task is experiencing a paradigm shift to explore and model human emotions in unprecedented ways.\n\nPrior to the advent of LLMs, the dominant paradigm of Affective Computing involves fine-tuning Pre-trained Language Models (PLMs)  Amin et al. (2023a) , such as  BERT Devlin et al. (2019)  and RoBERTa  Liu et al. (2019) , on labelled datasets for specific tasks to improve performance. While the above training paradigm has achieved numerous successes in the tasks related to Affective Understanding, it still is inherently limited by the quality and scale of manually annotated data and struggles to generalize to new domains  Mao et al. (2023) . The rise of LLMs research, as demonstrated in Figure  1 , profoundly transforms the research paradigm of AC, expanding task variety and surpassing the constraints of previous studies, which often focus on particular datasets and single-task the performance improvements  Tan et al. (2023a) ;  Krugmann & Hartmann (2024) . This evolution is accompanied by developing novel approaches that exploit the vast knowledge of LLMs  Wei et al. (2022a)  to simultaneously boost the performance of various tasks related to Affective Computing.\n\nThe auto-regressive nature and massive pre-training of LLMs give them an advantage in processing dialogue inputs and generating coherent responses  Radford et al. (2018) ;  Brown et al. (2020) . Various techniques are proposed to fully harness and enhance the capabilities of LLMs, including Instruction Tuning, Prompt Engineering and Reinforcement Learning. Specifically, Instruction Tuning Chung   2009 ) (e.g.,  MLP Rosenblatt (1958) ,  LSTM Hochreiter & Schmidhuber (1997) ), typically requiring a separate model per task. In the LLMs era, a single (mostly frozen) LLM handles multiple tasks via (i) instruction tuning with adapters, (ii) prompt engineering, and (iii) reinforcement learning that samples rollouts and uses reward/preference signals to optimize a loss and update the adapters.\n\net al. (  2024 );  Zhang et al. (2024d)  involves fine-tuning a pre-trained language model on the dataset where tasks are explicitly defined through instructions. For example, Flan-T5  Chung et al. (2024)  designs specific instructions for different tasks, e.g., Sentiment Analysis, Commonsense Reasoning, Question Answering, etc. Instruction Tuning is a form of targeted training that aims to make the model more adept at following complex and varied instructions across different domains. Prompt Engineering  Zhou et al. (2023b) ;  White et al. (2023)  is more about crafting the input given to the model to elicit the desired output without modifying the model itself. In-context Learning can further mine a model to learn and apply knowledge based on the instances provided directly within the prompt, without any prior explicit training on these specific tasks  Xie et al. (2022) . Reinforcement Learning optimizes the model against preference-based or verifiable rewards to improve helpfulness, safety, and style. A common pipeline, Reinforcement Learning from Human Feedback (RLHF), first trains a reward model from human comparisons and then updates the policy with online RL-typically Proximal Policy Optimization (PPO)  Lambert (2025) . Complementary directions include Reinforcement Learning with Verifiable Rewards (RLVR)  Lambert et al. (2024) , which uses rule-checkers or pass/fail signals as rewards, and methods that reduce or avoid online RL, such as Direct Preference Optimization (DPO)  Rafailov et al. (2023a) . Reinforcement Learning from AI Feedback (RLAIF) distills feedback from capable AI evaluators or rule sets into the preference signal (e.g.,  Wen et al. (2025) ). For AC tasks, these approaches allow alignment directly toward affectaware objectives, covering politeness, empathy, and non-toxic style.\n\nAlthough LLMs offer new opportunities and show initial success in various tasks, a comprehensive review that consolidates and summarizes the recent advancements in AC with LLMs from an NLP perspective still needs to be present. Existing surveys either focus narrowly on specific tasks like sentiment analysis or a limited set of classic affective computing tasks  Tan et al. (2023a) ;  Wake et al. (2023) ;  Cui et al. (2023) ;  Zhang et al. (2023e) ;  Zhu et al. (2024) , or they concentrate on traditional, non-LLM-based methods  Amin et al. (2023a) ;  Afzal et al. (2023) . To mitigate the gap, it is essential to provide a comprehensive overview (Figure  2 ) of how LLMs are being applied across the diverse spectrum of tasks related to AC, along with the unique opportunities and challenges they bring. Accordingly, we investigate the capabilities and limitations of LLMs in AC tasks, covering both Affective Understanding and Generation. While our focus is primarily on NLP, we also include textcentric multimodal affective computing tasks, recognizing the growing importance of integrating multiple modalities in AC. We begin by summarizing early studies that laid the groundwork for understanding the capabilities and limitations of LLMs in AC. Building on this, we explore three critical technologies for adapting LLMs to AC tasks: instruction tuning ( ¬ß4), prompt engineering ( ¬ß5) and reinforcement learning ( ¬ß6). While tuning grounds the model in task formats and prompting controls behavior at inference, RL supplies preference-or programmatically verifiable objectives that close the optimization loop. To deliver a comprehensive assessment, we further summarize benchmarks and evaluation methods ( ¬ß7) for systematically evaluating LLMs' performance in AC, including the innovative use of LLMs as evaluation tools. Finally, we enumerate open challenges and Table  1  The traditional tasks of Affective Computing from an NLP perspective, with representative methods in the LLM era.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Tasks",
      "text": "Affective Computing (AC) comprises two mainstream task families: Affective Understanding (AU)  Zhao et al. (2023a)  and Affective Generation (AG)  Nie & Zhan (2022) ;  Pei et al. (2024) , as summarized in Table  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Affective Understanding",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sentiment Analysis",
      "text": "AU focuses on recognizing and interpreting human affect, with Sentiment Analysis (SA) as the core task. The basic form is Polarity Classification (PC)  Pang et al. (2002) , which labels a text as negative, positive, or neutral. Emotional Classification (EC)  Li et al. (2017) ;  Demszky et al. (2020)  extends this to finer-grained emotion categories. Aspect-Based Sentiment Analysis (ABSA)  Schouten & Frasincar (2015) ;  Zhang et al. (2023g)  targets aspect-level opinions. Typical subtasks include Aspect Term Extraction (ATE)  Toh & Wang (2014) ;  Li et al. (2018)  and Aspect-oriented Sentiment Classification (ASC)  Lau et al. (2014 Lau et al. ( , 2018)) ; Joint Aspect-Sentiment Analysis (JASA)  Zhuang et al. (2020)  models both simultaneously, and ACOS  Cai et al. (2021)  further incorporates category and opinion dimensions. Beyond label granularity and aspects, Emotional Intensity Detection (EID)  Bonnet et al. (2015) ;  Alonso et al. (2015)  estimates strength, Implicit Sentiment Analysis (ISA)  Tubishat et al. (2018) ;  Wei et al. (2020a)  captures sentiment not explicitly expressed, and Multimodal Sentiment Analysis (MSA)  Das & Singh (2023) ;  Jung et al. (2022)  fuses textual, visual, and acoustic cues.\n\nConversation-level understanding has become central with advances in human-computer interaction  Chen et al. (2017) . Emotion Recognition in Conversation (ERC)  Li et al. (2017) ;  Poria et al. (2019b Poria et al. ( , 2018) )  predicts utterance-level emotions while tracking dynamics in context. Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU) jointly infers current-utterance emotion and intent  Liu et al. (2024b) . Emotion Cause Pair Extraction (ECPE)  Xia & Ding (2019a)  extracts emotions together with their causes.\n\nSee Table  1  for the complete taxonomy and representative methods.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Subjective Text Analysis",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Affective Generation",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotional Dialogue",
      "text": "Affective Generation (AG) aims to produce emotionally appropriate, empathetic, or supportive content. In Emotional Dialogue, Empathetic Response Generation (ERG)  Rashkin et al. (2019)  generates context-aware empathetic replies, while Emotional Support Conversation (ESC)  Liu et al. (2021)  targets goal-directed support. Multimodal variants (e.g., MERG) leverage audio/vision with text to enhance empathy and naturalness  Jung et al. (2022) ;  Zhang et al. (2024f) .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Review Summarization",
      "text": "Review Summarization (RS) is a broader task where Opinion Summarization (OS)  Kim et al. (2011) ;  Moussa et al. (2018)  condenses many user opinions on a topic-often from social media or UGC-into concise summaries that surface overall sentiment and key points. Challenges arise from noisy, unstructured text and figurative or diverse language use (e.g., sarcasm).\n\nIt is worth noting that research on AG lagged behind AU in the pre-LLM era due to limitations in model capabilities and technology. However, with their exceptional sequence generation abilities, the advent of LLMs has ushered in new possibilities for affect generation, making it a focal point of research in the LLM era  Park et al. (2023) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preliminary Study",
      "text": "Large Language Models (LLMs), with their powerful natural language processing, are rapidly transforming the field of Affective Computing. This section explores the emerging applications of LLMs in crucial areas of Affective Computing: Affective Understanding and Affective Generation. We review the current state of research, highlighting the strengths and limitations of LLMs in these tasks and suggest promising directions for future exploration.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Affective Understanding",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sentiment Analysis",
      "text": "Sentiment Analysis (SA), a classic task in the Affective Computing domain, focus on the automatic identification and categorization of emotions and sentiments expressed through contents from users  Tan et al. (2023b) ;  Rodr√≠guez-Ib√°nez et al. (2023) ;  Das & Singh (2023) .\n\nTo evaluate the performance of LLMs in SA tasks, researchers conduct assessments on multiple datasets and discover that the performance of LLMs is closely related to the complexity of the SA tasks they face. For more straightforward sentiment analysis tasks, such as Polarity Classification (PC), closed-source large language models like GPT-3.5-turbo demonstrate robust zero-shot learning capabilities and can deliver satisfactory results  Krugmann & Hartmann (2024) ;  Zhang et al. (2023f); Belal et al. (2023) ;  Lossio-Ventura et al. (2024) ;  Alexandersen & Rutlin (2024) . However, when confronted with complex SA tasks requiring deep semantic understanding or structured emotional information, such as Emotional Classification (EC), Aspected-Based Sentiment Analysis (ABSA), Emotional Recognition in Conversation (ERC), etc., which involves identifying multiple sentiments corresponding to different aspects of a post, or Emotion Cause Analysis, the performance of LLMs still lags behind that of pre-trained language models (PLMs) fine-tuned on specific datasets  Zhang et al. (2023f) ;  Amin et al. (2024) ;  Wang et al. (2024h) ;  Wu et al. (2024a) . This discrepancy may stem from LLMs lacking training data tailored to specific tasks and having insufficient capabilities to process complex semantic information.\n\nAlthough the zero-shot learning capabilities of opensource LLMs are inferior to those of closed-source models, their superior few-shot learning abilities give them an advantage on datasets with limited training data or imbalanced distributions, outperforming PLMs in such scenarios. However, when data is abundant and label distribution is balanced, PLMs can still perform better due to their finetuning advantage on specific datasets  Zhang et al. (2023d) . Notably, LLMs show superior generalization capabilities in SA tasks compared to PLMs. By simply altering the prompts, LLMs can adapt to different datasets and tasks without requiring model retraining  Wang et al. (2024h) . However, the performance of LLMs is also affected by prompts and decoding parameters. For example, minor text attacks like synonym substitution can lead to fluctuations in their performance. Additionally, because of the auto-regressive decoding mechanism, parameters such as temperature and Top-P can significantly impact the effectiveness of LLMs in sentiment analysis tasks. Typically, a lower temperature value (below 0.3) and a lower Top-P value are necessary for achieving better results  Amin et al. (2023a) ;  Amin & Schuller (2024) . This suggests that although research shows carefully crafted prompts can greatly improve LLMs'performance, developing optimal prompts and setting effective hyper-parameters for specific tasks and contexts remains an unresolved research challenge.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Subjective Text Analysis",
      "text": "LLMs demonstrate considerable performance variations across various Subjective Text Analysis (STA) related tasks. For instance, they excel in tasks involving negative emotion recognition, such as well-being assessment and toxicity detection. The superior performance could be due to the focus on safety and value alignment during the Reinforcement Learning from Human Feedback (RLHF) stage of LLM training  Zheng et al. (2023a) ;  Amin et al. (2023a Amin et al. ( , 2024)) . However, in tasks requiring the understanding of implicit emotional cues, such as engagement measurement, personality assessment, and sarcasm detection, LLMs exhibit a performance gap compared to PLMs fine-tuned on task-specific datasets. This discrepancy may stem from a need for more specialized training data during the supervised fine-tuning (SFT) phase of LLM development. Optimizing prompts for LLMs can enhance their performance on these specific tasks  Amin et al. (2023a Amin et al. ( , 2024)) . Furthermore, leveraging their robust capacity for capturing contextual information, LLMs consistently demonstrate superior performance in processing longer texts compared to shorter texts across various subjective text analysis tasks  Amin et al. (2024) ;  Krugmann & Hartmann (2024) . LLMs perform slightly worse than SFT models tailored to specific tasks in complex sentiment computing areas like sarcasm detection and subtle sentiment analysis. However, the substantial computational and time resources needed to fine-tune these large language models present a significant hurdle. Consequently, developing LLMs with outperformance and minimal training for specific tasks is an important research priority.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Affective Generation",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotional Dialogue",
      "text": "Empathetic Response Generation (ERG) and Emotional Support Conversation (ESC) are two prominent tasks in AG that attract significant attention, utilizing the EMPA-THETICDIALOGUES  Rashkin et al. (2019)  and  ESConv Liu et al. (2021) , respectively. Early research suggests that LLMs possess immense potential in emotionally-aware generation tasks. For example, GPT-3.5-turbo has shown competitive performance across multiple datasets  Zhao et al. (2023a) , while GPT-4 generates empathetic responses in various emotional scenarios. Human evaluations indicate that GPT-4's responses exceed human performance by an average of 10% in empathy levels  Welivita & Pu (2024) . Unlike earlier models, LLMs produce longer and more diverse text  Welivita & Pu (2024) , introducing new evaluation challenges. Traditional automatic evaluation methods based on overlap metrics need to adequately measure the performance of LLMs in emotional generation tasks, while manual evaluation is costly and burdensome to scale. Developing novel evaluation methods and establishing standardized frameworks to accurately measure LLM-generated affective responses' quality, empathy, and emotional intelligence is crucial for advancing research in this area.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotional Intelligence",
      "text": "Emotional Intelligence (EI)  Dulewicz & Higgs (2000) ;  Mayer et al. (2003) ;  Neubauer & Freudenthaler (2005)  is the ability to recognize, understand, manage, and use emotions effectively in various contexts. EI is crucial for enhancing the naturalness and effectiveness of human-computer interaction and is a vital component in achieving Artificial General Intelligence (AGI). To explore whether LLMs possess EI, researchers undertake investigations from various perspectives. Current studies  Elyoseph et al. (2024) ;  Ratican & Hutson (2023)  draw upon assessment methods from the field of human psychology, employing tools, like the Levels of Emotional Awareness Scale (LEAS)  Lane et al. (1990) , to evaluate the ability of LLMs to perceive emotions. LLMs can generate relatively appropriate emotional responses, which has been evidenced.\n\nHowever, using human-centric evaluation systems to measure the EI of LLMs has limitations, including the fact that the scales are designed with humans in mind, not explicitly specifically for LLMs and that the evaluations rely on human experts to conduct the assessments. Researchers have developed evaluation tools designed explicitly for LLMs to address these issue, such as the Situational Evaluation of Complex Emotional Understanding (SECEU)  Wang et al. (2023e) . Moreover, to improve the objectivity and efficiency of evaluations, EQ-bench has been proposed  Paech (2024) . It removes the need for expert assessments and instead directly evaluates the ability of LLMs to understand complex emotions and social interactions by predicting the intensity of emotional states in dialogue characters. While LLMs have demonstrated remarkable performance on SECEU and EQBench, which focus more on the cognition and understanding of human emotional scenarios, there is still insufficient evidence to suggest that they possess emotional intelligence comparable to humans. To address this gap, researchers propose a new task called Sentiment and Opinion Understanding of Language (SOUL), which aims to comprehensively evaluate the model's ability to understand complex emotions through two sub-tasks, including Review Comprehension and Justification Generation. However, experimental results reveal a substantial performance gap of up to 27% between LLMs and human performance on the SOUL  Deng et al. (2023d) . It highlights the considerable room for improvement in the capacity of LLMs to understand the nuances and complexities of human emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Instruction Tuning",
      "text": "LLMs show excellent performance on different tasks, such as Question Answering, Sequence Generation, and so on  Zhao et al. (2023b) . Instruction Tuning  Zhang et al. (2023c)  is introduced to adapt LLMs to specific tasks, thereby maintaining the benefits of the original model while improving the performance of LLMs on specialized downstream tasks. Due to the substantial size of LLMs, which often exceed ten billion parameters, Full Parameter Fine-Tuning (FPFT) of all parameters of the complete model could be more practical.\n\nTo reduce the cost of model training, a series of Parameter-Efficient Fine-Tuning (PEFT) approaches are proposed, such as LoRA  Devalal & Karthikeyan (2018) , Prefix Tuning  Li & Liang (2021) , and P-Tuning  Liu et al. (2022) , as shown in Figure  3 . LoRA reduces the number of parameters that need to be trained during the fine-tuning process by freezing all the original parameters and injecting a pair of low-rank decomposition matrices next to the original weights. Prefix Tuning adds trainable \"prefix\" tokens related to specific tasks as an implicit learnable prompt to the input sequence of the LLM, inserting trainable free vectors into each layer of the Transformer. Applying PEFT in instruction tuning can effectively reduce the training costs in AC tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Affective Understanding",
      "text": "In addressing affective computing tasks, PLMs typically necessitate separate approaches for affective understanding and generation tasks. In contrast, LLMs can unify all affective computing tasks into a sequence generation format. However, directly employing LLMs for AU tasks does not offer significant advantages over fine-tuned PLMs. Consequently, numerous recent studies focus on constructing task-specific instruction tuning templates tailored to the unique characteristics of various tasks. These studies apply instruction tuning to the models, aiming to enhance the performance of LLMs on specific tasks or within particular domains.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Polarity And Emotional Classification",
      "text": "Polarity Classification (PC) and Emotional Classification (EC) tasks are frequently employed to assess a model's comprehension and perceptual abilities. Several studies pioneer the exploration of the effects of instruction tuning of LLMs on the SA task  Raffel et al. (2020b) ;  Peng et al. (2024) . The  T5 Raffel et al. (2020a)  series model, in particular, demonstrates an improved understanding of the sentiment content of text after instruction tuning. Recent research  Peng et al. (2024)  focuses on exploring the different PEFT methods, adopting the LoRA and P-tuning methods to finetuning ChatGLM2  GLM et al. (2024) . Experimental results demonstrate that after instruction tuning, LLMs are greatly improved in eight datasets compared with the model without instruction tuning. However, the experimental results must conclusively determine which instruction tuning method is more effective in enhancing performance on PC.\n\nWhile these studies demonstrate LLMs' effectiveness in PC and EC tasks, it is equally important to understand their underlying sentiment processing mechanisms. Research shown that words with sentiment information can influence the overall sentiment of the text  Gan et al. (2023) . Building on this understanding, LLMs leverage techniques of instruction   2024 ) allows the addition of two trainable vectors, d and b, augmenting the LoRA layer. This approach also shares LoRA_A and LoRA_B's parameters to every LoRA layer. Moreover, when the rank of LoRA in each layer varies, this method can transition into adaLoRA  Zhang et al. (2023b) . Subfigure 3d illustrates the soft prompt tuning, where a trainable prefix is appended to the model's embedding layer, known as p-tuning. Furthermore, scientific prefixes can be optionally added to each layer of the model, with only the added prefixes being trained, a technique referred to as prefix-tuning.\n\ntuning to grasp word polarity, enabling them to effectively perform methods PC and EC tasks. These studies primarily focus on text-based PC and EC tasks, it is essential to recognize that real-world user expressions are not limited to text alone. Usergenerated content often encompasses multiple modalities, including images and audio, which can provide additional context and nuance to sentiment analysis.   2025 ) has a similar model structure to Emotion-LLaMA and explores various fusion schemes of facial features with other visual features. Experiments found that the method of separately extracting the visual and facial features of the entire video and then concatenating them has the least impact on MLLMs.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Aspect-Based Sentiment Analysis",
      "text": "Aspect-Based Sentiment Analysis (ABSA) encompasses multiple subtasks, such as Aspect Term Extraction (ATE), Aspect-oriented Sentiment Classification (ASC), Joint Aspect-Sentiment Analysis (JASA), and Aspect-Category-Opinion-Sentiment (ACOS), and others. Traditional PLMs often design a separate module for each ABSA subtask, which reduces the correlation between ABSA subtasks. LLMs can leverage their strong generalization abilities to construct a unified framework for all ABSA subtasks.\n\nRecent research explore this potential. InstructABSA introduces Instruction tuning based on the Tùëò-Instruct model  Wang et al. (2022)  for ABSA. This study finds that LLMs are sensitive to different instructions, with experimental results fluctuating by up to 10%  Scaria et al. (2023) .  ITSCL Zhang et al. (2024b)  is based on the T5 model and performs unified encoding of emotions and opinions. It introduces contrastive learning loss at each layer of the model to differentiate the feature vectors of different emotions and entity labels, thereby enhancing the model's performance. In addition to strengthening the correlations between ABSA tasks, large language models can leverage their extensive world knowledge to enhance information exchange across different domains. Some reseraches  Ding et al. (2024)  propose an ABSA cross-domain learning fine-tuning framework. In the first stage, domain knowledge is used to train the domain variation adapter, while replay data is used to train the domaininvariant adapter, employing orthogonal constraints to force the model to learn the differences between domain-invariant and domain-variant knowledge. In the second stage, all replay data will be used to train the domain-invariant adapter while freezing the corresponding domain variation adapter. This two-stage fine-tuning effectively utilizes knowledge from different domains and avoids the issue of forgetting previous domain knowledge. The model achieved the best performance across 19 datasets.\n\nThe ABSA task shares similar objectives with sentiment analysis and sentiment polarity analysis. Unisa aims to build a unified multimodal emotion analysis model that integrates various sentiment analysis tasks  Li et al. (2023b) . This model has already established a new dataset and benchmark, SAEval, to facilitate research in this field. SAEval encompasses various ABSA sub-tasks, providing a comprehensive evaluation platform for future research.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Emotion Recognition in Conversation (ERC) tasks involve recognizing the sentiment label of each utterance in the conversation, which is highly dependent on the conversation history and the speaker. InstructERC addresses these challenges by developing a template that includes a system prompt, dialogue history, and the label map. It also introduces Instruction Tuning using LoRA for ERC tasks  Lei et al. (2024) . Additionally, InstructERC creates two auxiliary tasks, speaker identification and emotion impact prediction, to enhance the model's understanding of speaker information. This model performs very well on the  MELD Poria et al. (2019a)  and EmoryNLP  Zahiri & Choi (2018)  datasets.\n\nResearchers extend ERC tasks to incorporate multimodal inputs, considering the frequently multimodal aspects of human communication, leading to the development of Multimodal Emotion Recognition in Conversation (MERC). Dia-logueLLM  Zhang et al. (2023h)  generates text descriptions of visual information through GPT-4, combines these descriptions with the conversation history as an instruction dataset, and uses LoRA to train LLaMA2. Through experiments, it has been found that adding information from other modalities can improve the model's ability to understand dialogue information.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Cause Extraction",
      "text": "Emotion Cause Extraction (ECE) has garnered widespread attention in recent years. This task aims to extract potential causes behind certain emotions in text, but its application scenarios are significantly limited due to the need for annotating emotional information. To address this issue, the Emotion-Cause Pair Extraction (ECPE) task propose. The ECPE task simultaneously extracts potential Emotion-Cause pairs and their corresponding causes within a document, thereby reducing the dependence on emotion labels  Xia & Ding (2019b) . Most current research on ECPE tasks involves fine-tuning traditional PLMs  Ding et al. (2020) ;  Wei et al. (2020b)  or leveraging the understanding capabilities of LLMs using well-designed prompts  Wu et al. (2024b) . These methods primarily focus on the Textual Emotion-Cause Pair Extraction in Conversations task (TECPE). However, emotion causes may also be embedded in other modalities, leading to the development of Multimodal Emotion-Cause Pair Extraction in Conversations task (MECPE)  Wang et al. (2024c) . LLMs shows potential in better integrating information from multiple modalities for such tasks.\n\nRecent work on multimodal emotion-cause understanding has progressed from clause extraction to explicit cause generation. NUS-Emo  Luo et al. (2024)",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Subjective Text Analysis",
      "text": "LLMs are employed to explore the Stance Classification task. In this task, the model determines whether a user agrees, disagrees, or is neutral on a given point of view  Cruickshank & Ng (2024) . The study investigates ten open-source models and seven prompting schemes, revealing exciting conclusions. These include the fact that LLMs are competitive with indomain supervised models and that the fine-tuning process only sometimes leads to better performance than zero-shot models. This is likely because the fine-tuning makes the model too specialized, thus unable to generalize to out-ofdomain data points and varying stance definitions. In the opinion expression task,  STOEI Jia et al. (2024)  concatenates text embeddings and speech embeddings, inputting them together into the LLM for instruction fine-tuning, enabling the model to learn to extract opinion words from the input and analyze the sentiments associated with those opinion words. This approach surpasses existing techniques by over 9.20% and achieves state-of-the-art (SOTA) results.\n\nAlthough LLMs possess inherent advantages in these subjective sentiment analysis tasks, the datasets for these tasks are mostly scarce, making instruction fine-tuning of the model challenging.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Affective Generation",
      "text": "In the current landscape of artificial intelligence, content generation that expresses sentiment is applied to various practical fields, including emotional dialogue, chatbots, emotional support conversation, and more  Nie & Zhan (2022) . It enhances humanized services by analyzing and generating emotional information from users.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotional Dialogue",
      "text": "Emotional Dialogue (ED) refers to the ability to understand and share another person's feelings and then respond in a way that conveys this understanding and compassion. LLMs must to possess high emotional intelligence to resonate with users in their replies. However, due to the limitations of the dataset and its relative homogeneity, the generated responses often tend to be formulaic and mechanical, resulting in lower interaction quality. Some researchers aim to construct a more diverse empathetic dialogue dataset. Considering the high labor costs associated with manually constructing datasets, some current studies utilize LLM-generated synthetic data to build high-quality empathetic dialogue datasets.  Zheng et al. (2023b) ;  Wang et al. (2023b) ;  Zhang et al. (2024f) .\n\nTo enlarge training resources for empathetic dialogue, several works exploit LLM-driven self-augmentation. SoulChat  Chen et al. (2023b)  iteratively prompts GPT-3.5-turbo to grow multi-turn exchanges, yielding richer emotional trajectories.  EKTC Cao et al. (2025)  constructs the ED-TooL dataset by asking LLaMA3 to label the exact moments at which external tools should be invoked; the resulting high-quality tool-call exemplars are then used for instruction-fine-tuning, enabling models to decide autonomously-and more cleanly-when tool usage will enhance their responses. In the task of empathetic responses, the information dimension carried by a single text modality is relatively limited. Stickers and voice can convey non-verbal cues in interpersonal communication, and multimodal interaction mechanisms hold significant value in enhancing empathetic expression. StickerConv generates a multi-scenario, multimodal empathetic dialogue dataset constructed from user information through a multi-agent system, enabling more natural and fluid interactions with users by generating stickers within the dialogue  Zhang et al. (2024f) .\n\nSpeech-centric approaches further exploit prosody to boost empathy. BLSP-Emo  Wang et al. (2024a)  follows a two-step alignment strategy: (i) convert the speaker's audio into an explicit textual emotion hint and let an LLM draft a reply; (ii) apply QLoRA to inject the raw audio encodings themselves, forcing the model to ground its responses directly in vocal affect. PerceptiveAgent  Yan et al. (2024)  transcribes prosodic features with an audio-encoder-text-decoder pair, feeds the resulting description together with dialogue history to an LLM, and finally vocalises the generated reply via a synthesiser-thus closing the speech-to-speech loop. EMOVA  Chen et al. (2024a)  generalises this pipeline to multimodal inputs and, crucially, lets the model emit style tokens that steer the synthesiser, yielding richer and more engaging emotional speech.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Chatbot",
      "text": "Current datasets still limit most of the ERG and ESC tasks described above and need to capture the diversity of users and scenarios in real life fully. Chatbots capable of understanding user input and emotions are designed to address the practical application of emotional dialogue systems. These chatbots can offer encouragement during adversity and share in users' happiness. PICA  Zhang et al. (2023i)  and EMOLLM EmoLLM (2024) are two notable examples of such chatbots. While LLMs can function well as \"assistants,\" their emotional intelligence is often lacking, and long replies frequently fail to convey empathy. To address this issue, PICA is proposed. It uses P-tuning to fine-tune the ChatGLM2-6B model on two synthetic Chinese empathetic dialogue datasets, effectively enhancing the emotional capabilities of LLMs. EmoLLM is a series of large mental health models that can support the mental health counseling link of understanding users, supporting users, and helping users. It uses the QLoRADettmers et al. (  2024 ) method to fine-tune multiple LLMs, such as Deepseek-R1_14b_int4, LLaMA3-8B-Instruct, and others, across different languages. By supporting various roles, EMOLLM enables the chatbot to generate more contextually appropriate user responses.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multi-Task Of Affective Computing",
      "text": "Many experiments show that LLMs possess strong comprehension and generalization capabilities. Furthermore, their sequence generation ability enables them to construct a unified paradigm for AU and AG tasks. Consequently, researchers began exploring the development of a general AC model.\n\nDue to the high similarity of mission objectives and approach in affective understanding tasks, researchers try to build a unified model that can accomplish tasks simultaneously. Emollms  Liu et al. (2024c)  develops a multi-task instruction dataset, AAID, covering SA and ERC. Experimental results show improved performance on each task after multi-task training. GSA-7B  Hou et al. (2024)  undergoes progressive LoRA training on three tasks, SA, ERC, and SD that are both related and increasingly challenging. It has been found that this hierarchical training approach can effectively improve the model's performance across multiple tasks. In addition to classification tasks,  M2SE Li et al. (2024a)  also introduces tasks such as sentiment cause extraction, achieving a unified fine-tuning framework for multimodal and multitask scenarios. By sharing model parameters and facilitating knowledge transfer between tasks, it enhances the model's ability to generalize in complex emotional contexts. This model is applicable to various emotion computation tasks.\n\nHowever, current fine-tuning methods present limitations when dataset sources differ across tasks. Fine-tuning a model under these conditions can create conflicts and impact the original world knowledge embedded in the model  Li et al. (2024b) . These challenges highlight the need for more research on ensuring high-quality performance across all sentiment analysis tasks. To address these issues, researchers propose a novel Modular Emotional Intelligence enhancement method (MoEI) featuring two collaborative techniques-Modular Parameter Expansion (MPE) and Intra-Inter Modulation (I 2 M)-is proposed to fit most emotion analysis tasks  Zhao et al. (2024b) . In conjunction with this method, the study develops EIBENCH, a unified affective computing dataset containing 88 datasets across 15 EI-related tasks, is introduced. MoEI adopts the Mixture-of-LoRA (MoLoRA) method for fine-tuning to improve EI while maintaining the generative ability of the model.\n\nIn multimodality, MEILLMDong et al. (  2024 ) integrates deep feature vectors extracted from visual, textual, and audio models into baichuan13B-chat through the MLP layer. After fine-tuning the instructions, he can complete a variety of emotional interaction tasks, including psychological assessment, psychological portrait, psychological counseling, etc. MODA  Zhang et al. (2025c)  develop a multimodal artificial intelligence that is both emotionally and intellectually capable. It addresses the issue of attention imbalance among different modalities in large models through a duplex attention alignment mechanism and a modular attention mask, allowing the model to focus more on fine-grained information from auxiliary modalities. In terms of cognitive and emotional understanding, MODA can accurately identify user intentions and emotional inclinations, demonstrating immense potential in the field of human-computer dialogue.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Challenge",
      "text": "While Instruction Tuning has enabled LLMs to make significant strides in AC tasks, several challenges remain to be addressed.\n\n‚Ä¢ FPFT LLMs for specific domains are time-consuming, labor-intensive, and requires significant computational resources. While PEFT can reduce resource consumption in fine-tuning LLMs, they are more suitable for specific datasets and often sacrifice the generalization ability of LLMs. There is a need for scholarly research about the comparative efficacy of various PEFT methods on AC tasks. Consequently, developing a simple yet effective fine-tuning approach that balances efficiency, task-specific performance, and generalization capability remains a significant challenge in LLMs.\n\n‚Ä¢ While LLMs with instruction tuning demonstrate excellent performance in AC tasks, they are still slightly inferior to the previous fully fine-tuned PLMs method in some tasks.\n\nAchieving state-of-the-art performance with minimal training remains a major challenge.\n\n‚Ä¢ Despite their known universality, LLMs often struggle to simultaneously excel in both AU and AG within the domain of AC. It remains a significant challenge to develop a unified model capable of effectively addressing AU and AG.\n\n‚Ä¢ Research in niche areas of AC tasks, such as financial sentiment analysis, sarcasm detection, and metaphor analysis, still needs to be improved. Furthermore, the full potential of LLMs in these specialized AC tasks has yet to be thoroughly explored and evaluated.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Prompt Engineering",
      "text": "Prompt engineering  Brown et al. (2020)  guides LLMs to produce desired outputs by designing appropriate prompts, effectively improving the accuracy and reliability of AC tasks. This section reviews recent advances in prompt engineering for AC, focusing on four techniques: zero-shot prompting, few-shot prompting, Chain-of-Thought (CoT) prompting, and agent.\n\nZero-shot Prompting  Brown et al. (2020)  generates text for a specific topic or domain without any demonstrations. It is simple to use, highly adaptable, and especially suitable for scenarios where data are scarce or hard to obtain.\n\nFew-shot Prompting  Brown et al. (2020)  supplies the model with a small set of task-relevant examples or demonstrations. These examples, together with the task description, are used as prompts to help the model better understand the task, reason more effectively, and produce high-quality outputs.\n\nChain-of-Thought  Wei et al. (2022b)  adopts a multi-step reasoning paradigm that guides the LLMs to reason logically and explicitly reveal the intermediate steps of its reasoning chain. Chain-of-Thought enables LLMs to better leverage context, produce more accurate and interpretable answers, and perform better on complex reasoning tasks.\n\nAgent  Park et al. (2023)  assigns clear roles, objectives, and collaboration protocols to multiple LLMs, driving them to collaborate via message passing and feedback loops to complete tasks. Agent effectively handles task decomposition and planning, tool invocation, and memory management, and excels in open-domain, multi-stage, highly interactive scenarios.\n\nWhile these prompting paradigms share commonalities, they differ in emphasis and application contexts. Zero-shot prompting directs the model with task instructions alone, without examples. Few-shot prompting appends a small number of exemplars to the instruction to align format and decision boundaries. Chain-of-thought prompting explicitly exposes intermediate reasoning steps or uses trigger phrases to elicit step-by-step thinking. Agent methods orchestrate a workflow of one or more agents and tools to support task decomposition, planning, memory, and self-verification, and to aggregate multiple outputs. Figure  4  summarizes the distinctions and composability of the four approaches.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Affective Understanding",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Zero-Shot For Affective Understanding",
      "text": "In affective understanding (AU), zero-shot prompting has been applied. Researchers preset a task description as the prompt to the target text to guide LLMs to produce sentiment labels Amin et al.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Few-Shot For Affective Understanding",
      "text": "The pivotal role of demonstrations in prompt drisves exploration of the few-shot prompting. Early methods required manually curating and inserting demonstrations Zhou et al.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Chain-Of-Thought For Affective Understanding",
      "text": "Zero-shot or few-shot prompting carries limited information in a single-turn interaction, making it insufficient for complex AU tasks. To address this, researchers introduce CoT  Lee et al. (2024) , which explicitly adds intermediate reasoning steps to better handle complex tasks. Given the complexity of AU,  THOR Fei et al. (2023)  employs a threehop CoT framework for Implicit Sentiment Analysis (ISA).\n\nIt sequentially extracts fine-grained aspects, analyzes latent opinions, and infers implicit sentiment. In addition, injecting causal relations as causal cues into the model  Lyu et al. (2024)  can enhance LLMs' sentiment analysis performance. Unlike traditional methods that extract regularities directly from text, Chain of Empathy (CoE)  Lee et al. (2023b)  combines multiple psychotherapeutic approaches  Beck (1979) ;  Linehan (1987) ;  Cooper & McLeod (2011) ;  Wubbolding et al. (2017) , designs four types of chains of thought, and interprets users' psychological states within a professional framework, thereby enabling a more precise understanding and handling of emotions. Meanwhile, other work  Manzoor et al. (2024)  adopts a psychological information-mining perspective and explores LLMs' potential for empathetic understanding. ECR-Chain  Huang et al. (2024)  draws on appraisal theory's \"stimulus-appraisal-emotion\" process and employs multistep reasoning to infer the deeper causes of emotions.\n\nMethods such as CoE and ECR-Chain generate taskspecific reasoning and achieve good results, but they still face challenges on complex extraction tasks such as Joint Aspect Sentiment Analysis (JASA) and Emotion-Cause Pair Extraction (ECPE). Accordingly, RP-CoT  Wang & Luo (2023)  adopts multi-step reasoning to decompose the task: it first identifies aspect-level sentiment polarities and then determines the overall polarity. Likewise, DECC  Wu et al. (2024b)  first detects emotions and locates the corresponding emotion clauses, and then, for each emotion, selects the most probable cause clause and outputs emotion-cause pairs. Beyond generating intermediate reasoning for emotion understanding and task decomposition, CoT also supports prompt construction. The Evolutionary Multi-Objective (EMO) process  Baumann & Kramer (2024)  starts from human-written seed prompts, feeds them to an LLM and evaluates fitness via the generated stories. It then performs crossover recombination and applies mutation operators, iteratively producing new prompts.\n\nThe CoT can be combined with few-shot prompting. For Market Sentiment Analysis  Chen et al. (2018) , the complexity of financial and social-media terminology and data scarcity constrain traditional supervised methods. Researchers construct contextual exemplars and use few-shot prompts to elicit CoT summaries  Deng et al. (2023a) , prompting the model to retrieve and integrate domain knowledge before concluding and thereby injecting relevant financial knowledge into the Market Sentiment Analysis.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Agent For Affective Understanding",
      "text": "Prior studies mostly adopt a single LLM decision paradigm  Fei et al. (2023) ;  Yang et al. (2024a) . However, a single model's output does not reliably approach the optimum. To address this, Agent techniques orchestrate collaboration and evaluation among multiple LLMs, enabling more robust ensemble decisions.\n\nTo achieve multi-LLM negotiation in sentiment analysis, the Generator-Discriminator Role-switching Decision-Making framework  Sun et al. (2023)  is proposed. The generator injects reasoning and provides decisions with rationales, the discriminator explains and assesses credibility, and the two iterate until they reach consensus, leveraging model complementarity for more accurate emotion interpretation and correction. Building on this idea, agent design expands with an automated 5W1H question-generation module to predict emotions from social information  Wei et al. (2024) . Multi-agent interaction also builds an emotion contagion model that characterizes how emotions propagate within groups and their effects  Van Haeringen et al. (2023) . The PANAS framework  Regan et al. (2024)  employs three specialized agents for memory, context understanding, and evaluation. Meanwhile, AnnaAgent  Wang et al. (2025a)  uses multi-session memory to capture users' long-term emotional dynamics, offering a practical route to more realistic and deeper digital companions.\n\nIn Financial Sentiment Analysis (FSA), HAD Xing (2024) summarizes five common error types and, accordingly, instantiates five specialized agents-sentiment, rhetoric, dependency, aspect, and reference. Each agent targets the errorprone dimension for LLMs, and their outputs are aggregated and deliberated to produce the final FSA decision.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Affective Generation",
      "text": "Prompt engineering is also used to improve the performance of AG. The following introduces the application of four prompt engineering technologies in AG.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Zero-Shot For Affective Generation",
      "text": "To align prompts with the characteristics of AG, researchers propose Perspective-Taking  Lee et al. (2023a) , which enhances LLMs' empathetic dialogue generation by shifting viewpoints. In HEF  Yang et al. (2024c) , a small empathy model first extracts emotion causes and primary emotion categories. These are then provided to the LLM under zero-shot prompting to produce more precise empathetic responses. The TCG pipeline  Bhaskar et al. (2022) , built on GPT-3.5-turbo, performs sentence-level opinion extraction, grouping, and clustering to generate long-form summaries of affective opinions.\n\nIn the emotion-conditioned text generation task, researchers optimizes prompts by iteratively adding, replacing, and deleting the original prompts  Resendiz & Klinger (2023) . The optimized prompts better achieve the expected emotions, resulting in improved emotional text generation. Since the output of LLMs largely depends on prompts, users needing more background knowledge might experience a diminished interaction with chatbots due to the absence of contextual understanding. To address this issue, PromptMind  Su et al. (2023)  generates multiple contextually relevant prompts during the conversation for users to choose from, thereby prompting LLMs to produce answers that better meet user expectations and enhance human-chatbot interaction.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Few-Shot For Affective Generation",
      "text": "In the Sibyl framework  Wang et al. (2023c) , researchers randomly select a single example as a demonstration to prompt the LLM to generate four types of prospective commonsense related to the dialogue, which are then used as training data for subsequent dialogue-generation models. For mixed-initiative dialogue generation tasks such as PersuasionForGood and Emotional Support Conversations, responses are generated conditioned on different exemplar strategies  Chen et al. (2023a) .\n\nResearchers indicate that generated responses utilizing prompt engineering demonstrate high quality and adherence to semantic controls  Chen et al. (2023a) . However, in emotional support tasks, the inherent preference of LLMs for specific strategies can hinder effective support despite improving the robustness of predicting appropriate strategies  Kang et al. (2024) . To address this issue, researchers conduct extensive experiments involving Self-contact approaches, external strategy planning, and example expansion, demonstrating that External-contact approaches help reduce LLM preference bias and enhance the performance of emotional support.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Cot For Affective Generation",
      "text": "In AG, information and knowledge are crucial for producing high-quality responses. To obtain more valuable information, researchers introduce CoT to support empathetic generation.  CFEG Chen et al. (2024b)  decomposes emotion and reasoning into multiple steps, strengthening the listener's role awareness and significantly improving the empathy of LLMs. User utterances serve as latent linguistic cues that reveal hidden needs and guide the model to generate more personalized responses. Cue-CoT  Wang et al. (2023a)  breaks down reasoning into sequential steps to infer user states (e.g., personality, psychological status, emotions) and finally generates responses based on the reasoning outputs.  EDIT Wu et al. (2023)  enhances response generation by detecting implicit user intentions. It first asks open-ended questions to capture potential intentions, then retrieves knowledge base answers, and finally integrates the additional knowledge into generation. Aptness  Hu et al. (2024)  adopts a similar strategy by constructing an empathetic response database via multistep CoT and using retrieval during generation to enhance empathetic capability.\n\nSince AG requires LLMs to address emotion-related issues more proactively, planning information plays an important role in the process of obtaining and using information with LLMs. ProCoT  Deng et al. (2023b)  guides LLMs to produce intermediate steps of reasoning and planning and analyzes through dynamic reasoning how to achieve dialogue goals, which enhances proactive dialogue ability.  ECoT Li et al. (2024g)  proposes five consecutive steps to simulate human emotional intelligence and generate responses with richer emotional expression.  ESCoT Zhang et al. (2024e)  promotes response generation by producing emotional stimuli and designing regulation strategies. Further research transforms step-by-step cognitive commentary prompts into a hierarchical concept  Li et al. (2024d)  and proposes a threelevel emotional integration framework.\n\nAdditionally, in metaphor generation, GROUNDS naturally serve as the reasoning chain that connects TENOR and VEHICLE. Researchers guide the model to generate logically consistent  GROUNDS Shao et al. (2024a) , which in turn produce coherent and rich VEHICLE expressions, thereby improving the quality of metaphor generation.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Agent For Affective Generation",
      "text": "As a new technology based on LLMs, agent is gradually applied to AG. The Agent4SC  Zhang et al. (2024f)  generates user information, enhances memory, and formulates response strategies through collaboration among LLMs, realistically simulating human sticker usage to improve multimodal empathetic dialogue. The COOPER dialogue framework  Cheng et al. (2024a)  coordinates multiple specialized agents focusing on specific dialogue goals to generate emotionally supportive and persuasive responses. Researchers propose a multi-turn dialogue framework based on psychologist agents  Wu et al. (2024c) , in which debaters from different psychological schools generate candidate replies, and an unbiased decision-maker selects the final response, thus addressing the challenge of integrating multiple psychological schools in LLMs. The PerceptiveAgent  Yan et al. (2024)  simulates different speaking styles according to prompts to build multimodal dialogue systems. The conversational agent with acoustic emotion perception (CA)  Hu et al. (2022b)  performs speech emotion recognition and incorporates empathetic feedback and interjections to give responses an emotional style. In addition, introducing self-emotion prompts is also shown to help Agents demonstrate more human-like dialogue strategies  Zhang et al. (2024c) .\n\nAdditionally, in the multimodal domain, researchers study multimodal Conversational Health Agents (CHAs)  Abbasian et al. (2024)  and design five types of agents as components to interact with users to generate audio responses based on user emotions.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Challenge",
      "text": "Despite the significant achievements of Prompt Engineering in the field of AC, the following challenges persist:\n\n‚Ä¢ As an efficient method with low resource requirements, it greatly reduces the cost of models. However, its performance indicators across various tasks still need to catch up to those of fine-tuned models, unable to meet practical application needs.\n\n‚Ä¢ Currently, the most widely used and advanced CoT method has significantly improved the effectiveness of AC tasks. However, its underlying logic aims to fit human reasoning processes, which conflicts with the unique mechanisms of human emotion generation. This often results in noise during the intermediate reasoning process, affecting the final inference outcomes.\n\n‚Ä¢ Moreover, since emotions are influenced by various factors such as psychology, knowledge, and context, the ways these factors impact and are obtained vary greatly. The current division of agents is not detailed enough, each agent's design is overly simplistic, and the information aggregation process is quite arbitrary, failing to fully exploit the potential of LLMs.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Reinforcement Learning",
      "text": "Reinforcement learning (RL) is increasingly used to align large language models (LLMs) with affect-related objectives beyond supervised fine-tuning. We organize RL for AC into three families that differ by the source of rewards yet share the goal of optimizing behavior at token-, utterance-, and dialogue-levels across AU/AG (see Figure  5    et al. (2024b) ; and (iii) Reinforcement Learning from AI Feedback (RLAIF), which leverages capable AI evaluators to provide multi-aspect feedback that can be plugged into RL or preference-optimization pipelines. The following subsections detail representative methods and applications of these three families in affective computing.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Reinforcement Learning From Human Feedback",
      "text": "A key application of RLHF is enhancing empathy in dialogue systems.  Zhu et al. Zhu et al. (2023a)  propose a threestage framework for Empathetic Emotion Elicitation, where reinforcement learning leverages user reactions as rewards to improve emotional engagement across conversations. However, collecting human feedback is costly and difficult to scale. To mitigate this, Xu et al.  Xu et al. (2025)  introduce Reinforcement Learning from Targeted Human Feedback (RLTHF), which first uses a general-purpose LLM to label data, then directs limited human effort toward \"hard\" or mislabeled cases. This approach achieves full human-level alignment at only 6-7% of the annotation cost.\n\nThe quality of preference data remains critical. Liang et al.  Liang et al. (2024)  present AlignCap, a preference optimization method that aligns speech emotion captions with human preferences, reducing hallucinations and producing more faithful emotional descriptions. Complementarily,  Chen Chen et al. (2025)  release EmpathyAgent, the first benchmark for training and evaluating empathetic embodied agents, offering 10,000 multimodal samples with preferencelabeled empathetic actions for RLHF training.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Reinforcement Learning With Verifiable Rewards",
      "text": "To bypass the human annotation bottleneck, RLVR leverages programmatic or model-based reward functions, enabling scalable and automated optimization. A representative work is RL-EMO by  Zhang et al. Zhang et al. (2024a) , the first to apply RL to multimodal emotion recognition in conversation (ERC). It frames ERC as a sequential decision-making problem, where correct or incorrect emotion predictions yield positive or negative rewards, respectively. Using a Deep Q-Network to model emotion flow, RL-EMO outperforms prior methods and demonstrates the effectiveness of policy optimization for capturing temporal affective dynamics.  Zhang et al. Zhang & Jaitly (2025)  further propose SAGE (State Augmented GEneration), a framework that explicitly models user and assistant emotional states as latent variables for dialogue generation. Reinforcement learning is applied at the state level, enabling strategic emotional control and producing more empathetic and context-aware responses. Extending into the multimodal domain,  Zhao et al. Zhao et al. (2025a) present R1-Omni, which applies RLVR to omni-multimodal (video, audio, text)  emotion recognition. With a simple verifiable reward-whether predictions match ground-truth labels-optimized via Group Relative Policy Optimization (GRPO), the framework improves accuracy, robustness, and interpretability in emotion reasoning.\n\nPushing the boundaries of affective understanding beyond static emotion recognition, the SAGE (Sentient Agent as a Judge) framework  Zhang et al. (2025a)  reconceptualizes comprehension as a dynamic, interactive process. Its core innovation is a \"Sentient Agent\" that simulates human-like emotional changes and inner thoughts during multi-turn conversations. By embedding evaluation within the agent's evolving persona, goals, and emotional state, comprehension is reframed from classification to a stateful simulation of a user \"feeling understood.\" The resulting emotion trajectories and interpretable inner thoughts provide a proxy for modeling and measuring the essence of being heard.\n\nMost RL applications in affective computing focus on generation, steering LLMs toward empathetic, polite, or strategically effective outputs. RLHF naturally fits this domain by embedding human preferences into the training loop. For example, RLVER by  Wang et al. Wang et al. (2025b)  introduces the first end-to-end RL framework leveraging verifiable emotion rewards from simulated users. Built upon the SAGE simulator, it produces deterministic emotion scores as direct PPO rewards, elevating a 7B open-source model to near-proprietary performance on Sentient-Benchmark. A key innovation in RLVR is finer-grained reward design.  Li et al. Li et al. (2024e)  propose TOLE, which formulates token-level rewards via classifier probability shifts, enabling efficient controllable text generation.  Cai Cai et al. (2024) 's EmpCRL combines empathy and diversity signals, while  Li et al. Li et al. (2024c)  integrate the \"Cognitive Relevance Principle\" into ORL, balancing helpfulness against user effort. Together these works show how automated, dense rewards lead to nuanced affective generation.  Ma et al. Ma et al. (2025)  propose EmpRL, aligning response empathy levels through a programmatic reward from a dedicated \"empathy identifier,\" based on reaction, interpretation, and exploration.  Chang et al. Chang et al. (2024)  further demonstrate multi-objective reward design in emotional support systems, combining goallevel, flow, and valence rewards for stage-aware dialogue policies. RLVR also supports domain-specific agents.  Mishra et al. Mishra et al. (2024)  introduce ABLE, a disability support agent guided by a six-component reward model covering persona consistency, politeness, empathy, naturalness, and coherence.  Priya et al. Priya et al. (2025)  develop GENTEEL-NEGOTIATOR, which uses MoE-based RL with detailed reward functions for polite yet strategic negotiation. For online safety,  Wang et al. Wang et al. (2024d)  present F 2 RL, leveraging triplet-based factuality and multi-aspect faithfulness rewards to generate effective counterspeech.\n\nBeyond dialogue, RLVR principles extend into strategic decision-making. Unnikrishnan et al.  Unnikrishnan (2024)  apply sentiment-aware RL to portfolio management, where sentiment scores from financial news served as programmatic rewards. The resulting trading agent achieved superior profitability, showing how effective signals can optimize complex, non-dialogue tasks.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Reinforcement Learning From Ai Feedback",
      "text": "Within the RLVR paradigm, Reinforcement Learning from AI Feedback (RLAIF) replaces human annotators with capable AI models to provide reward signals.  Yoshida et al. Yoshida et al. (2025)  exemplify this approach by training an LLM-based reward model on 12 conversational metrics (e.g., empathy, consistency, enjoyability). This AI-generated, multi-faceted feedback is then used with DPO to optimize dialogue agents, showing that AI feedback can scalably capture holistic qualities beyond turn-level scoring.\n\nRLVR also supports new agent architectures.  Yuan Yuan et al. (2024)  propose ReflectDiffu, integrating RL into diffusion models where a policy network selects empathetic intents conditioned on emotional contagion and mimicry. High-level planning approaches include  Deng Deng et al. (2023c) 's Plug-and-Play Policy Planner and  Rakib Rakib et al. (2025) 's DialogXpert, which employ LLM critics in self-play to train efficient policy planners for proactive tasks such as emotional support and negotiation. The paradigm further extends to autonomous self-improvement. Ye et al.  Ye et al. (2025)  propose a self-evolution framework where models refine their own outputs to generate synthetic preference pairs for DPO.  Zhao et al. Zhao et al. (2025b)  advance this idea with Chain-of-Strategy Optimization (CSO), which uses Monte Carlo Tree Search to explore conversational strategies. Candidate responses are scored by an auxiliary reward model, yielding high-quality preference datasets for DPO fine-tuning, significantly improving strategic empathy.  Zhang et al. Zhang et al. (2025b)  introduce Decoupled ESC, featuring Inferential Preference Mining (IPM) that identifies psychological errors (e.g., lack of empathy, premature shifts) in model outputs. Erroneous responses are paired with ground-truth ones to build synthetic preference data. The resulting decoupled system, with separate strategy planning and response generation, yields more robust and effective emotional support agents. Collectively, these works illustrate the trajectory of RLAIF toward fully autonomous systems capable of continuously enhancing emotional intelligence.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Challenge",
      "text": "Despite the growing success of reinforcement learning in aligning LLMs with affective objectives, several key challenges remain: ‚Ä¢ While RLHF has significantly improved model alignment with human preferences, it remains highly resourceintensive, requiring large-scale, high-quality human preference data. This poses a major scalability bottleneck, especially for fine-grained affective tasks that require nuanced and subjective judgments.\n\n‚Ä¢ Many current reward functions used in RLHF and RLVR are designed based on task-level or surface-level attributes (e.g., helpfulness, politeness), but they often lack grounding in psychological or cognitive theories of emotion. This can lead to misalignment between the reward signal and the underlying affective dynamics, limiting the depth of emotional understanding or expression the model can achieve.\n\n‚Ä¢ Despite advances in RL for affective understanding (e.g., RL-EMO, SAGE), temporal affect modeling remains underexplored. Emotions evolve over time, yet most RLbased affective systems still optimize at the turn-level or token-level, failing to capture long-term emotional trajectories in conversation.\n\n‚Ä¢ The lack of standardized benchmarks and evaluation metrics for affective RL makes it difficult to systematically compare approaches or track progress. Current evaluations often rely on subjective human scoring or limited automatic proxies, which may not fully reflect emotional richness, coherence, or alignment with human expectations.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Benchmark & Evaluation",
      "text": "This section compiles various benchmarks, evaluation methods, and metrics for Affective Computing, encompassing both Affective Understanding and Affective Generation. Table  2  comprehensively summarizes these benchmarks, detailing the evaluated tasks, datasets, metrics, and models.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Affective Understanding",
      "text": "This section introduces the benchmarks and representative datasets in the field of Affective Understanding for different sentiment analysis-related tasks.\n\nSOUL  Deng et al. (2023d)     2025a ) is an evaluation framework for higher-order social cognition in LLMs. It uses a simulated Sentient Agent to track emotional changes during dialogues, evaluating a model's ability to foster positive emotional shifts via its primary metric, the Sentient emotion score.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Affective Generation",
      "text": "This section introduces the benchmarks in the field of Affective Generation (AG), covering Emotional Support Conversation and Empathetic Response Generation.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Emotional Support Conversation",
      "text": "Emotional support conversation (ESC) aims to alleviate individuals' emotional intensity and provide guidance for navigating personal challenges through engaging dialogue. Cue-CoT  Wang et al. (2023a)  is a benchmark consisting of six in-depth dialogue datasets in both Chinese and English, considering three aspects of user statuses: personality, emotions, and psychology. It forms a comprehensive evaluation benchmark for dialogue response generation, employing both Automatic Evaluation and Human Evaluation.\n\nESC-Eval  Zhao et al. (2024a)  is a framework for evaluating Emotion Support Conversation (ESC) in LLMs. It uses a specialized role-playing agent (ESC-Role) and a benchmark of 2,801 role cards to simulate multi-turn dialogues. Evaluation is performed via manual annotation across seven dimensions, such as fluency and empathy, enabling a dynamic assessment that moves beyond static, ground-truth-reliant metrics.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Empathetic Response Generation",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Emotional Intelligence",
      "text": "In the context of large language models (LLMs), emotional intelligence (EI) has emerged as a critical area of focus. EI encompasses the capacity to identify, comprehend, regulate, and apply emotions across diverse situations  Mayer et al. (2016) .\n\nSECEU (Situational Evaluation of Complex Emotional Understanding)  Wang et al. (2023d)  is a standardized EI test suitable for both humans and LLMs. It evaluates the ability to comprehend and apply emotions in diverse situations.\n\nEQ-Bench Paech (  2024 ) is a benchmark designed to evaluate aspects of EI in LLMs. The test dataset is constructed using GPT-4 to generate dialogues that serve as the context for the test questions.\n\nEmoBench  Sabour et al. (   2024 ) is a theory-based comprehensive EI benchmark for LLM evaluation, consisting of 400 hand-crafted questions available in English and Chinese. The framework defines machine EI in terms of emotional understanding and emotional application.\n\nEIBENCH  Zhao et al. (2024b)  is a large collection of EI-related tasks in a text-to-text format, spanning 15 tasks across 88 datasets. It evaluates three key EI aspects (Emotion Perception, Cognition, and Expression) and general intelligence.\n\nEmoVIT  Xie et al. (2024)  is the first instruction-tuned benchmark for visual emotion understanding, connecting visual cues with emotional cognition. It features a large-scale dataset of image-instruction pairs for tasks including classification, interpretation, and emotion-grounded conversation.\n\nEMER  Lian et al. (2023a)  is a benchmark for Explainable Multimodal Emotion Recognition. It requires models to provide explanations for predicted emotions, evaluating their ability to integrate information from audio, video, and text modalities.\n\nMER-UniBench  Lian et al. (2025)  is the largest benchmark for emotion recognition, with 115K human-verified samples across over 2K fine-grained categories. It supports tasks like fine-grained/basic emotion recognition and sentiment analysis.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Others",
      "text": "This section introduces other benchmarks that do not easily fit into AU or AG tasks, including affective computing and multimodal reasoning benchmarks.\n\nChatGPT2AC  Amin et al. (2024)  evaluates InstructGPT and GPT-4 across a wide range of affective computing tasks, including sentiment analysis, toxicity detection, and personality assessment. The benchmark uses Unweighted Average Recall (UAR) and Accuracy as its automatic evaluation metrics.\n\nMM-  INSTRUCTEVAL Yang et al. (2024b)  establishes a benchmark for 31 models, including 23 MLLMs, using ten instructions across 16 diverse multimodal reasoning datasets with vision-text contexts. Evaluation metrics include Best Performance, Mean Relative Gain, Stability, and Adaptability.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Discussion And Future Works",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Ethics",
      "text": "Data is essential and invaluable in artificial intelligence. Sentiment data from the real world, such as data from social media, psychological counseling, is highly personal, sensitive, regional, and cultural. Therefore, ethics must be considered when constructing a dataset, including individual privacy protection, prejudice elimination, and value alignment. Acquiring, preprocessing, and annotating data are all very time-consuming and labor-intensive processes. Thus, leveraging the world knowledge and generative capabilities of LLMs to synthesize datasets for affective computing tasks is an important research direction. Furthermore, improving the quality of original datasets and balancing the new and old datasets is a problem worth studying.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Multimodal Affective Computing",
      "text": "Large Language Models (LLMs), initially designed to work exclusively with text, have now evolved into Multimodal Large Language Models (MLLMs), capable of processing both text and image modalities. In the domain of AC, data from diverse modalities-such as text, visuals, audio, electroencephalogram (EEG), and electrooculogram (EOG)-can convey individual emotions and sentiments. However, most current general-purpose MLLMs face limitations in their ability to integrate and interpret information from multiple modalities in a seamless manner. This often results in misinterpretations, particularly when critical emotional cues are absent or overlooked. To advance AC, it is essential to effectively collect and utilize multimodal data to train unified MLLMs. Key challenges include recognizing micro-expressions, capturing sentiment from audio and video, and enhancing brain-computer interface technologies and emotional intelligence systems. Addressing these challenges represents crucial research directions for the future.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Multilingual And Multicultural Affective Computing",
      "text": "Multilingual and Multicultural sentiment analysis spans different languages and cultures, focusing on sentiment recognition and analysis. Users from different linguistic or cultural backgrounds express emotions differently; for example, the same body movements can have different meanings in various cultural contexts. Current sentiment analysis is typically conducted on a single language or culture. Advances in multicultural sentiment analysis will significantly enhance the understanding of global users' emotional reactions and behavior patterns, thereby improving the quality of cross-cultural communication and business decision-making. When training the current LLMs, the quality and quantity of corpora in different languages vary greatly, resulting in uneven AC performance in different languages. The core challenge in this field is effectively processing and integrating information from multiple languages and cultures to achieve accurate sentiment analysis. Therefore, it is necessary to continue to improve the multilingual and multicultural capabilities of the LLMs from multiple perspectives.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Real-Time Affective Computing",
      "text": "The first step in sentiment analysis is building a dataset and performing sentiment analysis. Real-time sentiment analysis in a dynamic environment, by integrating data in real-time and identifying and analyzing sentiment, is particularly critical for various practical applications such as online customer service, real-time monitoring, and interactive entertainment. Real-time sentiment analysis requires collecting and processing data in real-time, which presents several challenges-ensuring the real-time performance of data processing and sentiment analysis, especially maintaining time synchronization and consistency across different modalities. Real-time systems often encounter various environmental noises. Another major challenge is ensuring that the system can adapt to environmental changes and noise interference while maintaining the accuracy and stability of sentiment recognition.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Reasoning In Affective Computing",
      "text": "Current affective computing tasks primarily focus on understanding emotions and generating emotion-related content. However, they often overlook the deeper, underlying causes that trigger emotional states-specifically, the reasons behind the emergence, persistence, or fluctuation of certain emotions in response to various stimuli or contexts. Gaining insight into these triggers is essential for advancing systems beyond surface-level emotion recognition toward a more nuanced and comprehensive understanding of human affect. A major challenge lies in the extraction and integration of meaningful emotional cues to enable accurate sentiment reasoning. Successfully addressing this challenge could significantly enhance the interpretability and reliability of affective computing systems. However, evaluating and validating the sentiment reasoning remains a significant obstacle, requiring innovative approaches to balance technical accuracy with user comprehension.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Effective Evaluation",
      "text": "Although LLMs have achieved excellent performance in various tasks, it remains essential to explore how to train an LLM that can effectively perceive sentiments and evaluate performance. When training LLMs dedicated to AC tasks, researchers face the challenge of not only retaining the original abilities of models, such as instruction understanding and reasoning, but also strengthening new abilities, specifically sentiment perception and cognition. Additionally, it is crucial to ensure the integrity of values, facts, and logic within the LLMs. In terms of evaluation, previous studies conducted before the emergence of LLMs proposed various metrics to measure model performance, such as Accuracy, BLEU, and ROUGE. However, these metrics are insufficient to fully assess the ability of LLMs, e.g., generalization, generation, perception, and cognitive. Establishing a comprehensive and effective evaluation standard remains an urgent problem to be solved.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Conclusion",
      "text": "Affective Computing (AC) has emerged as a crucial research direction in artificial intelligence. With the rise of Large Language Models (LLMs), significant progress has been made in understanding and generating emotions. This paper provides a comprehensive review of LLM applications in AC, exploring the roles of techniques such as instruction tuning, prompt engineering, and reinforcement learning in affective understanding and generation tasks. We further conclude benchmarks for multiple large language models in AC to offer in-depth assistance to researchers and practitioners in related fields. Research indicates that LLMs, leveraging their powerful contextual learning and sequence generation capabilities, have demonstrated excellent affective understanding and generation performance. However, LLMs still face numerous challenges in multilingual and multicultural sentiment analysis, real-time affective computing, and emotion-related tasks in specific vertical domains. We discuss challenges and future research directions related to AC, and aim to provide insights into this rapidly advancing field.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , profoundly transforms the research",
      "page": 1
    },
    {
      "caption": "Figure 1: Comparison of approaches to AC tasks across the PLMs and LLMs eras. In the PLMs era, task-specific datasets fine-tune",
      "page": 2
    },
    {
      "caption": "Figure 2: ) of how LLMs are being applied across the diverse",
      "page": 2
    },
    {
      "caption": "Figure 2: The overview of the paper.",
      "page": 4
    },
    {
      "caption": "Figure 3: LoRA reduces the number of parameters that",
      "page": 6
    },
    {
      "caption": "Figure 3: Illustrations of the instruction tuning methods. Pink blocks denote trainable modules, blue blocks signify frozen modules,",
      "page": 7
    },
    {
      "caption": "Figure 3: a depicts the FPFT method, the transformer blocks of two",
      "page": 7
    },
    {
      "caption": "Figure 3: b introduces the multi-LoRA method. In this approach, only the added",
      "page": 7
    },
    {
      "caption": "Figure 3: c demonstrates the LoRA method, The traditional LoRA method involves the addition of only two",
      "page": 7
    },
    {
      "caption": "Figure 3: d illustrates",
      "page": 7
    },
    {
      "caption": "Figure 4: summarizes the",
      "page": 10
    },
    {
      "caption": "Figure 4: The overview of prompt engineering.",
      "page": 11
    },
    {
      "caption": "Figure 5: The overview of reinforcement learning.",
      "page": 14
    },
    {
      "caption": "Figure 5: ): (i) Reinforce-",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The traditional tasks of Affective Computing from an NLP perspective, with representative methods in the LLM era.",
      "data": [
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Emotional Classification (EC)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "Zhang et.al Zhang et al. (2023e), Stigall et.al Stigall et al. (2024)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Aspect-Based Sentiment Analysis (ABSA)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "InstructABSA Scaria et al. (2023), SCRAP Kim et al. (2024)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Emotional\nIntensity Detection (EID)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "Amin et.al Amin et al. (2023b)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Implicit Sentiment Analysis (ISA)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "THOR Fei et al. (2023)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Emotion Recognition in Conversation (ERC)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "UniMSE Hu et al. (2022a), DialogueLLM Zhang et al. (2023h), CKERC Fu (2024),\nInstructERC Lei et al. (2024)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Emotion Cause Paired Extraction (ECPE)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "DECC Wu et al. (2024b), ECR-Chain Huang et al. (2024)"
        },
        {
          "Sentiment Analysis (SA)": "Subjective Text Analysis (STA)",
          "Polarity Classification (PC)": "Suicide Tendency Detection (STD)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "Zhang et.al Zhang et al. (2023e), Amin et.al Amin et al. (2023b)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Personality Assessment (PA)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "Zhang et.al Zhang et al. (2023e), Amin et.al Amin et al. (2023b)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Toxicity Detection (TD)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "Zhang et.al Zhang et al. (2023e), Amin et.al Amin et al. (2023b)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Sarcasm Detection (SD)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "Hoffmann et.al Hoffmann et al. (2022), Zhang et.al Zhang et al. (2023e), Amin et.al Amin et al. (2023b)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Well-Being Assessment (WBA)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "Zhang et.al Zhang et al. (2023e), Amin et.al Amin et al. (2023b)"
        },
        {
          "Sentiment Analysis (SA)": "",
          "Polarity Classification (PC)": "Engagement Measurement (EM)",
          "USA Gan et al. (2023), WisdoM Wang et al. (2024f), RP-CoT Wang & Luo (2023), Sun et.al Sun et al. (2023)": "Zhang et.al Zhang et al. (2023e), Amin et.al Amin et al. (2023b)"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Empathy through multimodality in conversational interfaces",
      "authors": [
        "M Abbasian",
        "I Azimi",
        "M Feli",
        "A Rahmani",
        "R Jain"
      ],
      "year": "2024",
      "venue": "Empathy through multimodality in conversational interfaces",
      "arxiv": "arXiv:2405.04777"
    },
    {
      "citation_id": "2",
      "title": "A comprehensive survey on affective computing; challenges, trends, applications, and future directions",
      "authors": [
        "S Afzal",
        "H Khan",
        "I Khan",
        "M Piran",
        "J Lee"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on affective computing; challenges, trends, applications, and future directions",
      "arxiv": "arXiv:2305.07665"
    },
    {
      "citation_id": "3",
      "title": "Sentiment and interest detection in social media using gptbased large language models",
      "authors": [
        "M Al Asad",
        "H Imran",
        "M Alamin",
        "T Abdullah",
        "S Chowdhury"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 6th International Conference on Machine Learning and Natural Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Man vs. machine: An applied study comparing a man-made lexicon, a machine learned lexicon, and openai's gpt for sentiment analysis",
      "authors": [
        "M Alexandersen",
        "J Rutlin"
      ],
      "year": "2024",
      "venue": "Man vs. machine: An applied study comparing a man-made lexicon, a machine learned lexicon, and openai's gpt for sentiment analysis"
    },
    {
      "citation_id": "5",
      "title": "New approach in quantification of emotional intensity from the speech signal: emotional temperature",
      "authors": [
        "J Alonso",
        "J Cabrera",
        "M Medina",
        "C Travieso"
      ],
      "year": "2015",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "6",
      "title": "Will affective computing emerge from foundation models and general artificial intelligence? a first evaluation of chatgpt",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2023.3254179"
    },
    {
      "citation_id": "7",
      "title": "A wide evaluation of chatgpt on affective computing tasks",
      "authors": [
        "M Amin",
        "R Mao",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "A wide evaluation of chatgpt on affective computing tasks",
      "arxiv": "arXiv:2308.13911"
    },
    {
      "citation_id": "8",
      "title": "A wide evaluation of chatgpt on affective computing tasks",
      "authors": [
        "M Amin",
        "R Mao",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "A wide evaluation of chatgpt on affective computing tasks",
      "doi": "10.48550/arXiv.2308.13911",
      "arxiv": "arXiv:2308.13911"
    },
    {
      "citation_id": "9",
      "title": "On prompt sensitivity of chatgpt in affective computing",
      "authors": [
        "M Amin",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "On prompt sensitivity of chatgpt in affective computing",
      "arxiv": "arXiv:2403.14006"
    },
    {
      "citation_id": "10",
      "title": "Angry men, sad women: Large language models reflect gendered stereotypes in emotion attribution",
      "authors": [
        "F Plaza-Del Arco",
        "A Curry",
        "A Curry",
        "G Abercrombie",
        "D Hovy"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "11",
      "title": "Is compound aspect-based sentiment analysis addressed by llms?",
      "authors": [
        "Y Bai",
        "Z Han",
        "Y Zhao",
        "H Gao",
        "Z Zhang",
        "X Wang",
        "M Hu"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024"
    },
    {
      "citation_id": "12",
      "title": "What is affective computing? URL",
      "authors": [
        "A Banafa"
      ],
      "year": "2024",
      "venue": "What is affective computing? URL"
    },
    {
      "citation_id": "13",
      "title": "Evolutionary multi-objective optimization of large language model prompts for balancing sentiments",
      "authors": [
        "J Baumann",
        "O Kramer"
      ],
      "year": "2024",
      "venue": "International Conference on the Applications of Evolutionary Computation (Part of EvoStar)"
    },
    {
      "citation_id": "14",
      "title": "Leveraging chatgpt as text annotation tool for sentiment analysis",
      "authors": [
        "A Beck"
      ],
      "year": "1979",
      "venue": "Leveraging chatgpt as text annotation tool for sentiment analysis",
      "arxiv": "arXiv:2306.17177"
    },
    {
      "citation_id": "15",
      "title": "Learning deep architectures for ai. Foundations and trends¬Æ in Machine Learning",
      "authors": [
        "Y Bengio"
      ],
      "year": "2009",
      "venue": "Learning deep architectures for ai. Foundations and trends¬Æ in Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Prompted opinion summarization with gpt-3.5",
      "authors": [
        "A Bhaskar",
        "A Fabbri",
        "G Durrett"
      ],
      "year": "2022",
      "venue": "Prompted opinion summarization with gpt-3.5",
      "arxiv": "arXiv:2211.15914"
    },
    {
      "citation_id": "17",
      "title": "The role of the amygdala in the perception of positive emotions: an \"intensity detector",
      "authors": [
        "L Bonnet",
        "A Comte",
        "L Tatu",
        "J.-L Millot",
        "T Moulin",
        "E Medeiros De Bustos"
      ],
      "year": "2015",
      "venue": "Frontiers in behavioral neuroscience"
    },
    {
      "citation_id": "18",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "It is simple sometimes: A study on improving aspect-based sentiment analysis performance",
      "authors": [
        "L Cabello",
        "U Akujuobi"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics ACL 2024"
    },
    {
      "citation_id": "20",
      "title": "Aspect-category-opinion-sentiment quadruple extraction with implicit aspects and opinions",
      "authors": [
        "H Cai",
        "R Xia",
        "J Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Empcrl: Controllable empathetic response generation via in-context commonsense reasoning and reinforcement learning",
      "authors": [
        "M Cai",
        "D Wang",
        "S Feng",
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024"
    },
    {
      "citation_id": "22",
      "title": "TOOL-ED: Enhancing empathetic response generation with the tool calling capability of LLM",
      "authors": [
        "H Cao",
        "Y Zhang",
        "S Feng",
        "X Yang",
        "D Wang",
        "Y Zhang"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Applying reinforcement learning and multi-generators for stage transition in an emotional support dialogue system",
      "authors": [
        "J Chang",
        "K.-Y Chen",
        "C.-H Wu"
      ],
      "year": "2024",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "24",
      "title": "Ntusd-fin: a market sentiment dictionary for financial social media data applications",
      "authors": [
        "C.-C Chen",
        "H.-H Huang",
        "H.-H Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the 1st financial narrative processing workshop"
    },
    {
      "citation_id": "25",
      "title": "A survey on dialogue systems: Recent advances and new frontiers",
      "authors": [
        "H Chen",
        "X Liu",
        "D Yin",
        "J Tang"
      ],
      "year": "2017",
      "venue": "Acm Sigkdd Explorations Newsletter"
    },
    {
      "citation_id": "26",
      "title": "Emova: Empowering language models to see, hear and speak with vivid emotions",
      "authors": [
        "K Chen",
        "Y Gou",
        "R Huang",
        "Z Liu",
        "D Tan",
        "J Xu",
        "C Wang",
        "Y Zhu",
        "Y Zeng",
        "K Yang"
      ],
      "year": "2024",
      "venue": "Emova: Empowering language models to see, hear and speak with vivid emotions",
      "arxiv": "arXiv:2409.18042"
    },
    {
      "citation_id": "27",
      "title": "Controllable mixed-initiative dialogue generation through prompting",
      "authors": [
        "M Chen",
        "X Yu",
        "W Shi",
        "U Awasthi",
        "Z Yu"
      ],
      "year": "2023",
      "venue": "Controllable mixed-initiative dialogue generation through prompting",
      "arxiv": "arXiv:2305.04147"
    },
    {
      "citation_id": "28",
      "title": "Empathyagent: Can embodied agents conduct empathetic actions? arXiv preprint",
      "authors": [
        "X Chen",
        "J Ge",
        "H Dai",
        "Q Zhou",
        "Q Feng",
        "J Hu",
        "Y Wang",
        "J Liu",
        "S Zhang"
      ],
      "year": "2025",
      "venue": "Empathyagent: Can embodied agents conduct empathetic actions? arXiv preprint",
      "arxiv": "arXiv:2503.16545"
    },
    {
      "citation_id": "29",
      "title": "Cause-aware empathetic response generation via chain-of-thought fine-tuning",
      "authors": [
        "X Chen",
        "C Yang",
        "M Lan",
        "L Cai",
        "Y Chen",
        "T Hu",
        "X Zhuang",
        "A Zhou"
      ],
      "year": "2024",
      "venue": "Cause-aware empathetic response generation via chain-of-thought fine-tuning",
      "arxiv": "arXiv:2408.11599"
    },
    {
      "citation_id": "30",
      "title": "Soulchat: Improving llms' empathy, listening, and comfort abilities through fine-tuning with multi-turn empathy conversations",
      "authors": [
        "Y Chen",
        "X Xing",
        "J Lin",
        "H Zheng",
        "Z Wang",
        "Q Liu",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "31",
      "title": "Depression detection in clinical interviews with llm-empowered structural element graph",
      "authors": [
        "Z Chen",
        "J Deng",
        "J Zhou",
        "J Wu",
        "T Qian",
        "M Huang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference of the North American Chapter"
    },
    {
      "citation_id": "32",
      "title": "Cooper: Coordinating specialized agents towards a complex dialogue goal",
      "authors": [
        "Y Cheng",
        "W Liu",
        "J Wang",
        "C Leong",
        "Y Ouyang",
        "W Li",
        "X Wu",
        "Y Zheng"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Z Cheng",
        "Z.-Q Cheng",
        "J.-Y He",
        "K Wang",
        "Y Lin",
        "Z Lian",
        "X Peng",
        "A Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "H Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "Y Li",
        "X Wang",
        "M Dehghani",
        "S Brahma",
        "A Webson",
        "S Gu",
        "Z Dai",
        "M Suzgun",
        "X Chen",
        "A Chowdhery",
        "A Castro-Ros",
        "M Pellat",
        "K Robinson",
        "D Valter",
        "S Narang",
        "G Mishra",
        "A Yu",
        "V Zhao",
        "Y Huang",
        "A Dai",
        "H Yu",
        "S Petrov",
        "E Chi",
        "J Dean",
        "J Devlin",
        "A Roberts",
        "D Zhou",
        "Q Le",
        "J Wei"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "35",
      "title": "Person-centered therapy: A pluralistic perspective",
      "authors": [
        "M Cooper",
        "J Mcleod"
      ],
      "year": "2011",
      "venue": "Person-Centered & Experiential Psychotherapies"
    },
    {
      "citation_id": "36",
      "title": "Prompting and fine-tuning open-sourced large language models for stance classification",
      "authors": [
        "I Cruickshank",
        "L Ng"
      ],
      "year": "2024",
      "venue": "Prompting and fine-tuning open-sourced large language models for stance classification",
      "arxiv": "arXiv:2309.13734"
    },
    {
      "citation_id": "37",
      "title": "Survey on sentiment analysis: Evolution of research methods and topics",
      "authors": [
        "J Cui",
        "Z Wang",
        "S.-B Ho",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence Review",
      "doi": "10.1007/s10462-022-10386-z"
    },
    {
      "citation_id": "38",
      "title": "Multimodal sentiment analysis: a survey of methods, trends, and challenges",
      "authors": [
        "R Das",
        "T Singh"
      ],
      "year": "2023",
      "venue": "Multimodal sentiment analysis: a survey of methods, trends, and challenges"
    },
    {
      "citation_id": "39",
      "title": "Goemotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "Goemotions: A dataset of fine-grained emotions",
      "arxiv": "arXiv:2005.00547"
    },
    {
      "citation_id": "40",
      "title": "Llms to the moon? reddit market sentiment analysis with large language models",
      "authors": [
        "X Deng",
        "V Bashlovkina",
        "F Han",
        "S Baumgartner",
        "M Bendersky"
      ],
      "year": "2023",
      "venue": "Companion Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "41",
      "title": "Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and non-collaboration",
      "authors": [
        "Y Deng",
        "L Liao",
        "L Chen",
        "H Wang",
        "W Lei",
        "T.-S Chua"
      ],
      "year": "2023",
      "venue": "Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and non-collaboration",
      "arxiv": "arXiv:2305.13626"
    },
    {
      "citation_id": "42",
      "title": "Plug-andplay policy planner for large language model powered dialogue agents",
      "authors": [
        "Y Deng",
        "W Zhang",
        "W Lam",
        "S.-K Ng",
        "T.-S Chua"
      ],
      "venue": "Plug-andplay policy planner for large language model powered dialogue agents",
      "arxiv": "arXiv:2311.00262"
    },
    {
      "citation_id": "43",
      "title": "Soul: Towards sentiment and opinion understanding of language",
      "authors": [
        "Y Deng",
        "W Zhang",
        "S Pan",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Soul: Towards sentiment and opinion understanding of language",
      "doi": "10.48550/arXiv.2310.17924",
      "arxiv": "arXiv:2310.17924"
    },
    {
      "citation_id": "44",
      "title": "Suicide and depression detection in social media forums",
      "authors": [
        "V Desu",
        "N Komati",
        "S Lingamaneni",
        "F Shaik"
      ],
      "year": "2022",
      "venue": "Smart Intelligent Computing and Applications"
    },
    {
      "citation_id": "45",
      "title": "Qlora: Efficient finetuning of quantized llms",
      "authors": [
        "T Dettmers",
        "A Pagnoni",
        "A Holtzman",
        "L Zettlemoyer"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "Lora technology -an overview",
      "authors": [
        "S Devalal",
        "A Karthikeyan"
      ],
      "year": "2018",
      "venue": "2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)",
      "doi": "10.1109/ICECA.2018.8474715"
    },
    {
      "citation_id": "47",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "doi": "10.48550/arXiv.1810.04805",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "48",
      "title": "Boosting large language models with continual learning for aspect-based sentiment analysis",
      "authors": [
        "X Ding",
        "J Zhou",
        "L Dou",
        "Q Chen",
        "Y Wu",
        "A Chen",
        "L He"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "doi": "10.18653/v1/2024.findings-emnlp.252"
    },
    {
      "citation_id": "49",
      "title": "End-to-end emotion-cause pair extraction based on sliding window multi-label learning",
      "authors": [
        "Z Ding",
        "R Xia",
        "J Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "50",
      "title": "Emoada: A multimodal emotion interaction and psychological adaptation system",
      "authors": [
        "T Dong",
        "F Liu",
        "X Wang",
        "Y Jiang",
        "X Zhang",
        "X Sun"
      ],
      "year": "2024",
      "venue": "International Conference on Multimedia Modeling"
    },
    {
      "citation_id": "51",
      "title": "Emotional intelligence-a review and evaluation study",
      "authors": [
        "V Dulewicz",
        "M Higgs"
      ],
      "year": "2000",
      "venue": "Journal of managerial Psychology"
    },
    {
      "citation_id": "52",
      "title": "Chatgpt outperforms humans in emotional awareness evaluations",
      "authors": [
        "Z Elyoseph",
        "D Hadar-Shoval",
        "K Asraf",
        "M Lvovsky"
      ],
      "year": "2024",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2023.1199058"
    },
    {
      "citation_id": "53",
      "title": "Emollm",
      "authors": [
        "Emollm"
      ],
      "year": "2024",
      "venue": "Emollm"
    },
    {
      "citation_id": "54",
      "title": "Reasoning implicit sentiment with chain-of-thought prompting",
      "authors": [
        "H Fei",
        "B Li",
        "Q Liu",
        "L Bing",
        "F Li",
        "T.-S Chua"
      ],
      "year": "2023",
      "venue": "Reasoning implicit sentiment with chain-of-thought prompting"
    },
    {
      "citation_id": "55",
      "title": "Ckerc : Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "authors": [
        "Y Fu"
      ],
      "year": "2024",
      "venue": "Ckerc : Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "arxiv": "arXiv:2403.07260"
    },
    {
      "citation_id": "56",
      "title": "Usa: Universal sentiment analysis model & construction of japanese sentiment text classification and part of speech dataset",
      "authors": [
        "C Gan",
        "Q Zhang",
        "T Mori"
      ],
      "year": "2023",
      "venue": "Usa: Universal sentiment analysis model & construction of japanese sentiment text classification and part of speech dataset",
      "arxiv": "arXiv:2309.03787"
    },
    {
      "citation_id": "57",
      "title": "",
      "authors": [
        "T Glm",
        "A Zeng",
        "B Xu",
        "B Wang",
        "C Zhang",
        "D Yin",
        "D Rojas",
        "G Feng",
        "H Zhao",
        "H Lai",
        "H Yu",
        "H Wang",
        "J Sun",
        "J Zhang",
        "J Cheng",
        "J Gui",
        "J Tang",
        "J Zhang",
        "J Li",
        "L Zhao",
        "L Wu",
        "L Zhong",
        "M Liu",
        "M Huang",
        "P Zhang",
        "Q Zheng",
        "R Lu",
        "S Duan",
        "S Zhang",
        "S Cao",
        "S Yang",
        "W Tam",
        "W Zhao",
        "X Liu",
        "X Xia",
        "X Zhang",
        "X Gu",
        "X Lv",
        "X Liu",
        "X Liu",
        "X Yang",
        "X Song",
        "X Zhang",
        "Y An",
        "Y Xu",
        "Y Niu",
        "Y Yang",
        "Y Li",
        "Y Bai",
        "Y Dong",
        "Z Qi",
        "Z Wang",
        "Z Yang",
        "Z Du",
        "Z Hou",
        "Z Wang"
      ],
      "year": "2024",
      "venue": "",
      "arxiv": "arXiv:2406.12793"
    },
    {
      "citation_id": "58",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "59",
      "title": "Training compute-optimal large language models",
      "authors": [
        "J Hoffmann",
        "S Borgeaud",
        "A Mensch",
        "E Buchatskaya",
        "T Cai",
        "E Rutherford",
        "D Casas",
        "L Hendricks",
        "J Welbl",
        "A Clark"
      ],
      "year": "2022",
      "venue": "Training compute-optimal large language models",
      "arxiv": "arXiv:2203.15556"
    },
    {
      "citation_id": "60",
      "title": "Aer-llm: Ambiguityaware emotion recognition leveraging large language models",
      "authors": [
        "X Hong",
        "Y Gong",
        "V Sethu",
        "T Dang"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "Progressive tuning: Towards generic sentiment abilities for large language models",
      "authors": [
        "G Hou",
        "Y Shen",
        "W Lu"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2024",
      "doi": "10.18653/v1/2024.findings-acl.855"
    },
    {
      "citation_id": "62",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu",
        "T.-E Lin",
        "Y Zhao",
        "G Lu",
        "Y Wu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "63",
      "title": "The acoustically emotion-aware conversational agent with speech emotion recognition and empathetic responses",
      "authors": [
        "J Hu",
        "Y Huang",
        "X Hu",
        "Y Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "64",
      "title": "Aptness: Incorporating appraisal theory and emotion support strategies for empathetic response generation",
      "authors": [
        "Y Hu",
        "M Tan",
        "C Zhang",
        "Z Li",
        "X Liang",
        "M Yang",
        "C Li",
        "X Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 33rd ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "65",
      "title": "Emotionally numb or empathetic? evaluating how llms feel using emotionbench",
      "authors": [
        "J Tse Huang",
        "M Lam",
        "E Li",
        "S Ren",
        "W Wang",
        "W Jiao",
        "Z Tu",
        "M Lyu"
      ],
      "year": "2023",
      "venue": "Emotionally numb or empathetic? evaluating how llms feel using emotionbench"
    },
    {
      "citation_id": "66",
      "title": "Ecr-chain: Advancing generative language models to better emotion-cause reasoners through reasoning chains",
      "authors": [
        "Z Huang",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2024",
      "venue": "Ecr-chain: Advancing generative language models to better emotion-cause reasoners through reasoning chains",
      "arxiv": "arXiv:2405.10860"
    },
    {
      "citation_id": "67",
      "title": "Llmdriven multimodal opinion expression identification",
      "authors": [
        "B Jia",
        "H Chen",
        "Y Sun",
        "M Zhang",
        "M Zhang"
      ],
      "year": "2024",
      "venue": "Llmdriven multimodal opinion expression identification",
      "arxiv": "arXiv:2406.18088"
    },
    {
      "citation_id": "68",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "69",
      "title": "Automatic sarcasm detection: A survey",
      "authors": [
        "A Joshi",
        "P Bhattacharyya",
        "M Carman"
      ],
      "year": "2017",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "70",
      "title": "An emotion-based korean multimodal empathetic dialogue system",
      "authors": [
        "M Jung",
        "Y Lim",
        "S Kim",
        "J Jang",
        "S Shin",
        "K.-H Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the Second Workshop on When Creative AI Meets Conversational AI"
    },
    {
      "citation_id": "71",
      "title": "Can large language models be good emotional supporter? mitigating preference bias on emotional support conversation",
      "authors": [
        "D Kang",
        "S Kim",
        "T Kwon",
        "S Moon",
        "H Cho",
        "Y Yu",
        "D Lee",
        "J Yeo"
      ],
      "year": "2024",
      "venue": "Can large language models be good emotional supporter? mitigating preference bias on emotional support conversation",
      "arxiv": "arXiv:2402.13211"
    },
    {
      "citation_id": "72",
      "title": "Comprehensive review of opinion summarization",
      "authors": [
        "H Kim",
        "K Ganesan",
        "P Sondhi",
        "C Zhai"
      ],
      "year": "2011",
      "venue": "Comprehensive review of opinion summarization"
    },
    {
      "citation_id": "73",
      "title": "Selfconsistent reasoning-based aspect-sentiment quad prediction with extractthen-assign strategy",
      "authors": [
        "J Kim",
        "R Heo",
        "Y Seo",
        "S Kang",
        "J Yeo",
        "D Lee"
      ],
      "year": "2024",
      "venue": "Selfconsistent reasoning-based aspect-sentiment quad prediction with extractthen-assign strategy",
      "arxiv": "arXiv:2403.00354"
    },
    {
      "citation_id": "74",
      "title": "Vera: Vector-based random matrix adaptation",
      "authors": [
        "D Kopiczko",
        "T Blankevoort",
        "Y Asano"
      ],
      "year": "2024",
      "venue": "Vera: Vector-based random matrix adaptation",
      "arxiv": "arXiv:2310.11454"
    },
    {
      "citation_id": "75",
      "title": "Sentiment analysis in the age of generative ai",
      "authors": [
        "J Krugmann",
        "J Hartmann"
      ],
      "year": "2024",
      "venue": "Customer Needs and Solutions",
      "doi": "10.1007/s40547-024-00143-4"
    },
    {
      "citation_id": "76",
      "title": "Reinforcement learning from human feedback",
      "authors": [
        "N Lambert"
      ],
      "year": "2025",
      "venue": "Reinforcement learning from human feedback",
      "arxiv": "arXiv:2504.12501"
    },
    {
      "citation_id": "77",
      "title": "Tulu 3: Pushing frontiers in open language model post-training",
      "authors": [
        "N Lambert",
        "J Morrison",
        "V Pyatkin",
        "S Huang",
        "H Ivison",
        "F Brahman",
        "L Miranda",
        "A Liu",
        "N Dziri",
        "S Lyu"
      ],
      "year": "2024",
      "venue": "Tulu 3: Pushing frontiers in open language model post-training",
      "arxiv": "arXiv:2411.15124"
    },
    {
      "citation_id": "78",
      "title": "The levels of emotional awareness scale: A cognitivedevelopmental measure of emotion",
      "authors": [
        "R Lane",
        "D Quinlan",
        "G Schwartz",
        "P Walker",
        "S Zeitlin"
      ],
      "year": "1990",
      "venue": "Journal of personality assessment"
    },
    {
      "citation_id": "79",
      "title": "Social analytics: Learning fuzzy product ontologies for aspect-oriented sentiment analysis",
      "authors": [
        "R Lau",
        "C Li",
        "S Liao"
      ],
      "year": "2014",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "80",
      "title": "Parallel aspect-oriented sentiment analysis for sales forecasting with big data",
      "authors": [
        "R Lau",
        "W Zhang",
        "W Xu"
      ],
      "year": "2018",
      "venue": "Parallel aspect-oriented sentiment analysis for sales forecasting with big data"
    },
    {
      "citation_id": "81",
      "title": "Analyzing key factors influencing emotion prediction performance of vllms in conversational contexts",
      "authors": [
        "J Lee",
        "Y Jang",
        "H Kim",
        "W Lee",
        "H Kim"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "82",
      "title": "Investigating the effects of zero-shot chain-of-thought on empathetic dialogue generation",
      "authors": [
        "Y.-J Lee",
        "D Lee",
        "J Im",
        "J Sung",
        "H.-J Choi"
      ],
      "year": "2023",
      "venue": "Investigating the effects of zero-shot chain-of-thought on empathetic dialogue generation"
    },
    {
      "citation_id": "83",
      "title": "Chain of empathy: Enhancing empathetic response of large language models based on psychotherapy models",
      "authors": [
        "Y Lee",
        "I Lee",
        "M Shin",
        "S Bae",
        "S Hahn"
      ],
      "year": "2023",
      "venue": "Chain of empathy: Enhancing empathetic response of large language models based on psychotherapy models",
      "arxiv": "arXiv:2311.04915"
    },
    {
      "citation_id": "84",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "S Lei",
        "G Dong",
        "X Wang",
        "K Wang",
        "S Wang"
      ],
      "year": "2024",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "85",
      "title": "M2se: A multistage multitask instruction tuning strategy for unified sentiment and emotion analysis",
      "authors": [
        "A Li",
        "L Xu",
        "C Ling",
        "J Zhang",
        "P Wang"
      ],
      "year": "2024",
      "venue": "M2se: A multistage multitask instruction tuning strategy for unified sentiment and emotion analysis",
      "arxiv": "arXiv:2412.08049"
    },
    {
      "citation_id": "86",
      "title": "Large language models understand and can be enhanced by emotional stimuli",
      "authors": [
        "C Li",
        "J Wang",
        "Y Zhang",
        "K Zhu",
        "W Hou",
        "J Lian",
        "F Luo",
        "Q Yang",
        "X Xie"
      ],
      "year": "2023",
      "venue": "Large language models understand and can be enhanced by emotional stimuli",
      "arxiv": "arXiv:2307.11760"
    },
    {
      "citation_id": "87",
      "title": "Mixlora: Enhancing large language models fine-tuning with lora-based mixture of experts",
      "authors": [
        "D Li",
        "Y Ma",
        "N Wang",
        "Z Ye",
        "Z Cheng",
        "Y Tang",
        "Y Zhang",
        "L Duan",
        "J Zuo",
        "C Yang",
        "M Tang"
      ],
      "year": "2024",
      "venue": "Mixlora: Enhancing large language models fine-tuning with lora-based mixture of experts",
      "arxiv": "arXiv:2404.15159"
    },
    {
      "citation_id": "88",
      "title": "Be helpful but don't talk too much-enhancing helpfulness in conversations through relevance in multi-turn emotional support",
      "authors": [
        "J Li",
        "B Peng",
        "Y.-Y Hsu",
        "C.-R Huang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "89",
      "title": "A sentiment consolidation framework for meta-review generation",
      "authors": [
        "M Li",
        "J Lau",
        "E Hovy"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "90",
      "title": "Reinforcement learning with token-level feedback for controllable text generation",
      "authors": [
        "W Li",
        "W Wei",
        "K Xu",
        "W Xie",
        "D Chen",
        "Y Cheng"
      ],
      "year": "2024",
      "venue": "Reinforcement learning with token-level feedback for controllable text generation",
      "arxiv": "arXiv:2403.11558"
    },
    {
      "citation_id": "91",
      "title": "Aspect term extraction with history attention and selective transformation",
      "authors": [
        "X Li",
        "L Bing",
        "P Li",
        "W Lam",
        "Z Yang"
      ],
      "year": "2018",
      "venue": "Aspect term extraction with history attention and selective transformation",
      "arxiv": "arXiv:1805.00760"
    },
    {
      "citation_id": "92",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "X Li",
        "P Liang"
      ],
      "year": "2021",
      "venue": "Prefix-tuning: Optimizing continuous prompts for generation",
      "arxiv": "arXiv:2101.00190"
    },
    {
      "citation_id": "93",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "94",
      "title": "Enhancing emotional generation capability of large language models via emotional chain-ofthought",
      "authors": [
        "Z Li",
        "G Chen",
        "R Shao",
        "D Jiang",
        "L Nie"
      ],
      "year": "2024",
      "venue": "Enhancing emotional generation capability of large language models via emotional chain-ofthought"
    },
    {
      "citation_id": "95",
      "title": "Enhancing the emotional generation capability of large language models via emotional chain-of-thought",
      "authors": [
        "Z Li",
        "G Chen",
        "R Shao",
        "D Jiang",
        "L Nie"
      ],
      "year": "2024",
      "venue": "Enhancing the emotional generation capability of large language models via emotional chain-of-thought",
      "arxiv": "arXiv:2401.06836"
    },
    {
      "citation_id": "96",
      "title": "Unisa: Unified generative framework for sentiment analysis",
      "authors": [
        "Z Li",
        "T.-E Lin",
        "Y Wu",
        "M Liu",
        "F Tang",
        "M Zhao",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "97",
      "title": "Affectgpt: A new dataset, model, and benchmark for emotion understanding with multimodal large language models. ICML (Spotlight)",
      "authors": [
        "Z Lian",
        "H Chen",
        "L Chen",
        "H Sun",
        "L Sun",
        "Y Ren",
        "Z Cheng",
        "B Liu",
        "R Liu",
        "X Peng"
      ],
      "year": "2025",
      "venue": "Affectgpt: A new dataset, model, and benchmark for emotion understanding with multimodal large language models. ICML (Spotlight)"
    },
    {
      "citation_id": "98",
      "title": "Explainable multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "H Gu",
        "Z Wen",
        "S Zhang",
        "S Chen",
        "M Xu",
        "K Xu",
        "K Chen"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion recognition",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "99",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "Y Ren",
        "H Gu",
        "H Sun",
        "L Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition"
    },
    {
      "citation_id": "100",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "H Sun",
        "K Chen",
        "Z Wen",
        "H Gu",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "101",
      "title": "Aligncap: Aligning speech emotion captioning to human preferences",
      "authors": [
        "Z Liang",
        "H Shi",
        "H Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "102",
      "title": "Integrating Plutchik's theory with mixture of experts for enhancing emotion classification",
      "authors": [
        "D Lim",
        "Y.-G Cheong"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "103",
      "title": "",
      "authors": [
        "Florida Miami",
        "Usa"
      ],
      "venue": "",
      "doi": "10.18653/v1/2024.emnlp-main.50"
    },
    {
      "citation_id": "104",
      "title": "Dialectical behavioral therapy: A cognitive behavioral approach to parasuicide",
      "authors": [
        "M Linehan"
      ],
      "year": "1987",
      "venue": "Journal of Personality disorders"
    },
    {
      "citation_id": "105",
      "title": "a). Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "106",
      "title": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "X Xing",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset"
    },
    {
      "citation_id": "107",
      "title": "Towards emotional support dialog systems",
      "authors": [
        "S Liu",
        "C Zheng",
        "O Demasi",
        "S Sabour",
        "Y Li",
        "Z Yu",
        "Y Jiang",
        "M Huang"
      ],
      "year": "2021",
      "venue": "Towards emotional support dialog systems",
      "arxiv": "arXiv:2106.01144"
    },
    {
      "citation_id": "108",
      "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
      "authors": [
        "X Liu",
        "K Ji",
        "Y Fu",
        "W Tam",
        "Z Du",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-short.8"
    },
    {
      "citation_id": "109",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "110",
      "title": "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "authors": [
        "Z Liu",
        "K Yang",
        "T Zhang",
        "Q Xie",
        "Z Yu",
        "S Ananiadou"
      ],
      "year": "2024",
      "venue": "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "arxiv": "arXiv:2401.08508"
    },
    {
      "citation_id": "111",
      "title": "A comparison of chatgpt and fine-tuned open pre-trained transformers (opt) against widely used sentiment analysis tools: Sentiment analysis of covid-19 survey data",
      "authors": [
        "J Lossio-Ventura",
        "R Weger",
        "A Lee",
        "E Guinee",
        "J Chung",
        "L Atlas",
        "E Linos",
        "F Pereira"
      ],
      "year": "2024",
      "venue": "JMIR Mental Health",
      "doi": "10.2196/50150"
    },
    {
      "citation_id": "112",
      "title": "Nus-emo at semeval-2024 task 3: Instruction-tuning llm for multimodal emotioncause analysis in conversations",
      "authors": [
        "M Luo",
        "H Zhang",
        "S Wu",
        "B Li",
        "H Han",
        "H Fei"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "113",
      "title": "Do llms think fast and slow? a causal study on sentiment analysis",
      "authors": [
        "Z Lyu",
        "Z Jin",
        "F Adauto",
        "R Mihalcea",
        "B Sch√∂lkopf",
        "M Sachan"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024"
    },
    {
      "citation_id": "114",
      "title": "Empathy level alignment via reinforcement learning for empathetic response generation",
      "authors": [
        "H Ma",
        "B Zhang",
        "B Xu",
        "J Wang",
        "H Lin",
        "X Sun"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "115",
      "title": "Learning word vectors for sentiment analysis",
      "authors": [
        "A Maas",
        "R Daly",
        "P Pham",
        "D Huang",
        "A Ng",
        "C Potts"
      ],
      "year": "2011",
      "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "116",
      "title": "Can machines resonate with humans? evaluating the emotional and empathic comprehension of lms",
      "authors": [
        "M Manzoor",
        "Y Wang",
        "M Wang",
        "P Nakov"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024"
    },
    {
      "citation_id": "117",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3204972"
    },
    {
      "citation_id": "118",
      "title": "The ability model of emotional intelligence: Principles and updates",
      "authors": [
        "J Mayer",
        "D Caruso",
        "P Salovey"
      ],
      "year": "2016",
      "venue": "Emotion review"
    },
    {
      "citation_id": "119",
      "title": "Measuring emotional intelligence with the msceit v2. 0. Emotion",
      "authors": [
        "J Mayer",
        "P Salovey",
        "D Caruso",
        "G Sitarenios"
      ],
      "year": "2003",
      "venue": "Measuring emotional intelligence with the msceit v2. 0. Emotion"
    },
    {
      "citation_id": "120",
      "title": "Sentiment analysis algorithms and applications: A survey",
      "authors": [
        "W Medhat",
        "A Hassan",
        "H Korashy"
      ],
      "year": "2014",
      "venue": "Ain Shams engineering journal"
    },
    {
      "citation_id": "121",
      "title": "Able: Personalized disability support with politeness and empathy integration",
      "authors": [
        "K Mishra",
        "M Burja",
        "A Ekbal"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "122",
      "title": "Sarcasm detection using news headlines dataset",
      "authors": [
        "R Misra",
        "P Arora"
      ],
      "year": "2023",
      "venue": "AI Open"
    },
    {
      "citation_id": "123",
      "title": "Introducing the lcc metaphor datasets",
      "authors": [
        "M Mohler",
        "M Brunson",
        "B Rink",
        "M Tomlinson"
      ],
      "year": "2016",
      "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16"
    },
    {
      "citation_id": "124",
      "title": "A survey on opinion summarization techniques for social media",
      "authors": [
        "M Moussa",
        "E Mohamed",
        "M Haggag"
      ],
      "year": "2018",
      "venue": "Future Computing and Informatics Journal"
    },
    {
      "citation_id": "125",
      "title": "Models of emotional intelligence. Emotional intelligence: An international handbook",
      "authors": [
        "A Neubauer",
        "H Freudenthaler"
      ],
      "year": "2005",
      "venue": "Models of emotional intelligence. Emotional intelligence: An international handbook"
    },
    {
      "citation_id": "126",
      "title": "A review of affective generation models",
      "authors": [
        "G Nie",
        "Y Zhan"
      ],
      "year": "2022",
      "venue": "A review of affective generation models",
      "arxiv": "arXiv:2202.10763"
    },
    {
      "citation_id": "127",
      "title": "From text to emotion: Unveiling the emotion annotation capabilities of llms",
      "authors": [
        "M Niu",
        "M Jaiswal",
        "E Provost"
      ],
      "year": "2024",
      "venue": "From text to emotion: Unveiling the emotion annotation capabilities of llms",
      "arxiv": "arXiv:2408.17026"
    },
    {
      "citation_id": "128",
      "title": "Gpt-4 technical report. arXiv, (p",
      "authors": [
        "R Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report. arXiv, (p",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "129",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "130",
      "title": "Stability analysis of chatgpt-based sentiment analysis in ai quality assurance",
      "authors": [
        "T Ouyang",
        "A Maungmaung",
        "K Konishi",
        "Y Seo",
        "I Echizen"
      ],
      "year": "2024",
      "venue": "Stability analysis of chatgpt-based sentiment analysis in ai quality assurance",
      "arxiv": "arXiv:2401.07441"
    },
    {
      "citation_id": "131",
      "title": "Eq-bench: An emotional intelligence benchmark for large language models",
      "authors": [
        "S Paech"
      ],
      "year": "2024",
      "venue": "Eq-bench: An emotional intelligence benchmark for large language models",
      "doi": "10.48550/arXiv.2312.06281",
      "arxiv": "arXiv:2312.06281"
    },
    {
      "citation_id": "132",
      "title": "Thumbs up? sentiment classification using machine learning techniques",
      "authors": [
        "B Pang",
        "L Lee",
        "S Vaithyanathan"
      ],
      "year": "2002",
      "venue": "Thumbs up? sentiment classification using machine learning techniques"
    },
    {
      "citation_id": "133",
      "title": "Generative agents: Interactive simulacra of human behavior",
      "authors": [
        "J Park",
        "J O'brien",
        "C Cai",
        "M Morris",
        "P Liang",
        "M Bernstein"
      ],
      "year": "2023",
      "venue": "Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology"
    },
    {
      "citation_id": "134",
      "title": "Affective computing: Recent advances, challenges, and future trends. Intelligent Computing, 3, 0076",
      "authors": [
        "G Pei",
        "H Li",
        "Y Lu",
        "Y Wang",
        "S Hua",
        "T Li"
      ],
      "year": "2024",
      "venue": "Affective computing: Recent advances, challenges, and future trends. Intelligent Computing, 3, 0076",
      "doi": "10.34133/icomputing.0076"
    },
    {
      "citation_id": "135",
      "title": "Customising general large language models for specialised emotion recognition tasks",
      "authors": [
        "L Peng",
        "Z Zhang",
        "T Pang",
        "J Han",
        "H Zhao",
        "H Chen",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "136",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "137",
      "title": "Chalearn lap 2016: First round challenge on first impressions-dataset and results",
      "authors": [
        "V Ponce-L√≥pez",
        "B Chen",
        "M Oliu",
        "C Corneanu",
        "A Clap√©s",
        "I Guyon",
        "X Bar√≥",
        "H Escalante",
        "S Escalera"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016 Workshops: Amsterdam"
    },
    {
      "citation_id": "138",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "139",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "140",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "141",
      "title": "Genteelnegotiator: Llm-enhanced mixture-of-expert-based reinforcement learning approach for polite negotiation dialogue",
      "authors": [
        "P Priya",
        "R Chigrupaatii",
        "M Firdaus",
        "A Ekbal"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "142",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "143",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "R Rafailov",
        "A Sharma",
        "E Mitchell",
        "C Manning",
        "C Finn"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "144",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "R Rafailov",
        "A Sharma",
        "E Mitchell",
        "C Manning",
        "S Ermon",
        "C Finn"
      ],
      "year": "2023",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "145",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "146",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "147",
      "title": "Dialogxpert: Driving intelligent and emotion-aware conversations through online value-based reinforcement learning with llm priors",
      "authors": [
        "T Rakib",
        "A Mehrish",
        "L.-K Soon",
        "W Lim",
        "S Poria"
      ],
      "year": "2025",
      "venue": "Dialogxpert: Driving intelligent and emotion-aware conversations through online value-based reinforcement learning with llm priors",
      "arxiv": "arXiv:2505.17795"
    },
    {
      "citation_id": "148",
      "title": "Towards empathetic open-domain conversation models: a new benchmark and dataset",
      "authors": [
        "H Rashkin",
        "E Smith",
        "M Li",
        "Y.-L Boureau"
      ],
      "year": "2019",
      "venue": "Towards empathetic open-domain conversation models: a new benchmark and dataset",
      "arxiv": "arXiv:1811.00207"
    },
    {
      "citation_id": "149",
      "title": "Stress detection from social media articles: New dataset benchmark and analytical study",
      "authors": [
        "A Rastogi",
        "Q Liu",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "150",
      "title": "The six emotional dimension (6de) model: A multidimensional approach to analyzing human emotions and unlocking the potential of emotionally intelligent artificial intelligence (ai) via large language models (llm)",
      "authors": [
        "J Ratican",
        "J Hutson"
      ],
      "year": "2023",
      "venue": "Journal of Artificial Intelligence and Robotics"
    },
    {
      "citation_id": "151",
      "title": "Can generative agents predict emotion? arXiv preprint",
      "authors": [
        "C Regan",
        "N Iwahashi",
        "S Tanaka",
        "M Oka"
      ],
      "year": "2024",
      "venue": "Can generative agents predict emotion? arXiv preprint",
      "arxiv": "arXiv:2402.04232"
    },
    {
      "citation_id": "152",
      "title": "Emotion-conditioned text generation through automatic prompt optimization",
      "authors": [
        "Y Resendiz",
        "R Klinger"
      ],
      "year": "2023",
      "venue": "Emotion-conditioned text generation through automatic prompt optimization",
      "arxiv": "arXiv:2308.04857"
    },
    {
      "citation_id": "153",
      "title": "A review on sentiment analysis from social media platforms",
      "authors": [
        "M Rodr√≠guez-Ib√°nez",
        "A Cas√°nez-Ventura",
        "F Castej√≥n-Mateos",
        "P.-M Cuenca-Jim√©nez"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "154",
      "title": "The perceptron: a probabilistic model for information storage and organization in the brain",
      "authors": [
        "F Rosenblatt"
      ],
      "year": "1958",
      "venue": "Psychological review"
    },
    {
      "citation_id": "155",
      "title": "Emobench: Evaluating the emotional intelligence of large language models",
      "authors": [
        "S Sabour",
        "S Liu",
        "Z Zhang",
        "J Liu",
        "J Zhou",
        "A Sunaryo",
        "J Li",
        "T Lee",
        "R Mihalcea",
        "M Huang"
      ],
      "year": "2024",
      "venue": "Emobench: Evaluating the emotional intelligence of large language models"
    },
    {
      "citation_id": "156",
      "title": "Large language modelbased emotional speech annotation using context and acoustic feature for speech emotion recognition",
      "authors": [
        "J Santoso",
        "K Ishizuka",
        "T Hashimoto"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "157",
      "title": "Instruction learning for aspect based sentiment analysis",
      "authors": [
        "K Scaria",
        "H Gupta",
        "S Goyal",
        "S Sawant",
        "S Mishra",
        "C Baral"
      ],
      "year": "2023",
      "venue": "Instruction learning for aspect based sentiment analysis",
      "arxiv": "arXiv:2302.08624"
    },
    {
      "citation_id": "158",
      "title": "Survey on aspect-level sentiment analysis",
      "authors": [
        "K Schouten",
        "F Frasincar"
      ],
      "year": "2015",
      "venue": "IEEE transactions on knowledge and data engineering"
    },
    {
      "citation_id": "159",
      "title": "Proximal policy optimization algorithms",
      "authors": [
        "J Schulman",
        "F Wolski",
        "P Dhariwal",
        "A Radford",
        "O Klimov"
      ],
      "year": "2017",
      "venue": "Proximal policy optimization algorithms",
      "arxiv": "arXiv:1707.06347"
    },
    {
      "citation_id": "160",
      "title": "Cmdag: A chinese metaphor dataset with annotated grounds as cot for boosting metaphor generation",
      "authors": [
        "Y Shao",
        "X Yao",
        "X Qu",
        "C Lin",
        "S Wang",
        "W Huang",
        "G Zhang",
        "J Fu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024"
    },
    {
      "citation_id": "161",
      "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
      "authors": [
        "Z Shao",
        "P Wang",
        "Q Zhu",
        "R Xu",
        "J Song",
        "X Bi",
        "H Zhang",
        "M Zhang",
        "Y Li",
        "Y Wu",
        "D Guo"
      ],
      "year": "2024",
      "venue": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
      "arxiv": "arXiv:2402.03300"
    },
    {
      "citation_id": "162",
      "title": "Heart-felt narratives: Tracing empathy and narrative style in personal stories with llms",
      "authors": [
        "J Shen",
        "J Mire",
        "H Park",
        "C Breazeal",
        "M Sap"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "163",
      "title": "Large language models performance comparison of emotion and sentiment classification",
      "authors": [
        "W Stigall",
        "M Al Hafiz Khan",
        "D Attota",
        "F Nweke",
        "Y Pei"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 ACM Southeast Conference ACM SE '24",
      "doi": "10.1145/3603287.3651183"
    },
    {
      "citation_id": "164",
      "title": "Prompt your mind: Refine personalized text prompts within your mind",
      "authors": [
        "G Su",
        "Y Yang",
        "J Guo"
      ],
      "year": "2023",
      "venue": "Prompt your mind: Refine personalized text prompts within your mind",
      "arxiv": "arXiv:2311.05114"
    },
    {
      "citation_id": "165",
      "title": "Sentiment analysis through llm negotiations",
      "authors": [
        "X Sun",
        "X Li",
        "S Zhang",
        "S Wang",
        "F Wu",
        "J Li",
        "T Zhang",
        "G Wang"
      ],
      "year": "2023",
      "venue": "Sentiment analysis through llm negotiations",
      "arxiv": "arXiv:2311.01876"
    },
    {
      "citation_id": "166",
      "title": "Sentiment analysis: An overview from linguistics",
      "authors": [
        "M Taboada"
      ],
      "year": "2016",
      "venue": "Annual Review of Linguistics"
    },
    {
      "citation_id": "167",
      "title": "A survey of sentiment analysis: Approaches, datasets, and future research",
      "authors": [
        "K Tan",
        "C Lee",
        "K Lim"
      ],
      "year": "2023",
      "venue": "Applied Sciences",
      "doi": "10.3390/app13074550"
    },
    {
      "citation_id": "168",
      "title": "A survey of sentiment analysis: Approaches, datasets, and future research",
      "authors": [
        "K Tan",
        "C Lee",
        "K Lim"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "169",
      "title": "Affective computing: A review",
      "authors": [
        "J Tao",
        "T Tan"
      ],
      "year": "2005",
      "venue": "International Conference on Affective computing and intelligent interaction"
    },
    {
      "citation_id": "170",
      "title": "Dlirec: Aspect term extraction and term polarity classification system",
      "authors": [
        "Z Toh",
        "W Wang"
      ],
      "year": "2014",
      "venue": "Proceedings of the 8th international workshop on semantic evaluation"
    },
    {
      "citation_id": "171",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozi√®re",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "172",
      "title": "Implicit aspect extraction in sentiment analysis: Review, taxonomy, oppportunities, and open challenges",
      "authors": [
        "M Tubishat",
        "N Idris",
        "M Abushariah"
      ],
      "year": "2018",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "173",
      "title": "Financial news-driven llm reinforcement learning for portfolio management",
      "authors": [
        "A Unnikrishnan"
      ],
      "year": "2024",
      "venue": "Financial news-driven llm reinforcement learning for portfolio management",
      "arxiv": "arXiv:2411.11059"
    },
    {
      "citation_id": "174",
      "title": "Empirical validation of an agent-based model of emotion contagion",
      "authors": [
        "E Van Haeringen",
        "E Veltmeijer",
        "C Gerritsen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "175",
      "title": "Instruction tuning for few-shot aspect-based sentiment analysis",
      "authors": [
        "S Varia",
        "S Wang",
        "K Halder",
        "R Vacareanu",
        "M Ballesteros",
        "Y Benajiba",
        "N John",
        "R Anubhai",
        "S Muresan",
        "D Roth"
      ],
      "year": "2023",
      "venue": "Instruction tuning for few-shot aspect-based sentiment analysis",
      "arxiv": "arXiv:2210.06629"
    },
    {
      "citation_id": "176",
      "title": "Techniques of sarcasm detection: A review",
      "authors": [
        "P Verma",
        "N Shukla",
        "A Shukla"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)"
    },
    {
      "citation_id": "177",
      "title": "Bias in emotion recognition with chatgpt",
      "authors": [
        "N Wake",
        "A Kanehira",
        "K Sasabuchi",
        "J Takamatsu",
        "K Ikeuchi"
      ],
      "year": "2023",
      "venue": "Bias in emotion recognition with chatgpt",
      "doi": "10.48550/arXiv.2310.11753",
      "arxiv": "arXiv:2310.11753"
    },
    {
      "citation_id": "178",
      "title": "Blsp-emo: Towards empathetic large speech-language models",
      "authors": [
        "C Wang",
        "M Liao",
        "Z Huang",
        "J Wu",
        "C Zong",
        "J Zhang"
      ],
      "year": "2024",
      "venue": "Blsp-emo: Towards empathetic large speech-language models",
      "arxiv": "arXiv:2406.03872"
    },
    {
      "citation_id": "179",
      "title": "Observe before generate: Emotion-cause aware video caption for multimodal emotion cause generation in conversations",
      "authors": [
        "F Wang",
        "H Ma",
        "X Shen",
        "J Yu",
        "R Xia"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "180",
      "title": "SemEval-2024 task 3: Multimodal emotion cause analysis in conversations",
      "authors": [
        "F Wang",
        "H Ma",
        "R Xia",
        "J Yu",
        "E Cambria"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "181",
      "title": "F2rl: Factuality and faithfulness reinforcement learning framework for claimguided evidence-supported counterspeech generation",
      "authors": [
        "H Wang",
        "Y Pan",
        "X Song",
        "X Zhao",
        "M Hu",
        "B Zhou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "182",
      "title": "Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms",
      "authors": [
        "H Wang",
        "R Wang",
        "F Mi",
        "Y Deng",
        "Z Wang",
        "B Liang",
        "R Xu",
        "K.-F Wong"
      ],
      "year": "2023",
      "venue": "Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms",
      "arxiv": "arXiv:2305.11792"
    },
    {
      "citation_id": "183",
      "title": "Enhancing empathetic and emotion support dialogue generation with prophetic commonsense inference",
      "authors": [
        "L Wang",
        "J Li",
        "C Yang",
        "Z Lin",
        "W Wang"
      ],
      "year": "2023",
      "venue": "Enhancing empathetic and emotion support dialogue generation with prophetic commonsense inference",
      "arxiv": "arXiv:2311.15316"
    },
    {
      "citation_id": "184",
      "title": "Sibyl: Sensible empathetic dialogue generation with visionary commonsense knowledge",
      "authors": [
        "L Wang",
        "J Li",
        "C Yang",
        "Z Lin",
        "W Wang"
      ],
      "year": "2023",
      "venue": "Sibyl: Sensible empathetic dialogue generation with visionary commonsense knowledge"
    },
    {
      "citation_id": "185",
      "title": "Generative emotion cause explanation in multimodal conversations",
      "authors": [
        "L Wang",
        "X Yang",
        "S Feng",
        "D Wang",
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "Generative emotion cause explanation in multimodal conversations",
      "arxiv": "arXiv:2411.02430"
    },
    {
      "citation_id": "186",
      "title": "AnnaAgent: Dynamic evolution agent system with multi-session memory for realistic seeker simulation",
      "authors": [
        "M Wang",
        "P Wang",
        "L Wu",
        "X Yang",
        "D Wang",
        "S Feng",
        "Y Chen",
        "B Wang",
        "Y Zhang"
      ],
      "year": "2025",
      "venue": "AnnaAgent: Dynamic evolution agent system with multi-session memory for realistic seeker simulation"
    },
    {
      "citation_id": "187",
      "title": "Findings of the Association for Computational Linguistics: ACL 2025",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2025",
      "doi": "10.18653/v1/2025.findings-acl.1192"
    },
    {
      "citation_id": "188",
      "title": "Rlver: Reinforcement learning with verifiable emotion rewards for empathetic agents",
      "authors": [
        "P Wang",
        "R Ma",
        "B Zhang",
        "X Chen",
        "Z He",
        "K Luo",
        "Q Lv",
        "Q Jiang",
        "Z Xie",
        "S Wang",
        "Y Li",
        "F Ye",
        "J Li",
        "Y Yang",
        "Z Tu",
        "X Li"
      ],
      "year": "2025",
      "venue": "Rlver: Reinforcement learning with verifiable emotion rewards for empathetic agents",
      "arxiv": "arXiv:2507.03112"
    },
    {
      "citation_id": "189",
      "title": "Wisdom: Improving multimodal sentiment analysis by fusing contextual world knowledge",
      "authors": [
        "W Wang",
        "L Ding",
        "L Shen",
        "Y Luo",
        "H Hu",
        "D Tao"
      ],
      "year": "2024",
      "venue": "Wisdom: Improving multimodal sentiment analysis by fusing contextual world knowledge",
      "arxiv": "arXiv:2401.06659"
    },
    {
      "citation_id": "190",
      "title": "Emotional intelligence of large language models",
      "authors": [
        "X Wang",
        "X Li",
        "Z Yin",
        "Y Wu"
      ],
      "year": "2023",
      "venue": "Journal of Pacific Rim Psychology"
    },
    {
      "citation_id": "191",
      "title": "Emotional intelligence of large language models",
      "authors": [
        "X Wang",
        "X Li",
        "Z Yin",
        "Y Wu",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Journal of Pacific Rim Psychology",
      "doi": "10.1177/18344909231213958"
    },
    {
      "citation_id": "192",
      "title": "Reasoning in conversation: Solving subjective tasks through dialogue simulation for large language models",
      "authors": [
        "X Wang",
        "Y Wang",
        "Y Zhang",
        "F Luo",
        "P Li",
        "M Sun",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Reasoning in conversation: Solving subjective tasks through dialogue simulation for large language models",
      "arxiv": "arXiv:2402.17226"
    },
    {
      "citation_id": "193",
      "title": "Enhance multi-domain sentiment analysis of review texts through prompting strategies",
      "authors": [
        "Y Wang",
        "Z Luo"
      ],
      "year": "2023",
      "venue": "2023 International Conference on High Performance Big Data and Intelligent Systems (HDIS)"
    },
    {
      "citation_id": "194",
      "title": "Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks",
      "authors": [
        "Y Wang",
        "S Mishra",
        "P Alipoormolabashi",
        "Y Kordi",
        "A Mirzaei",
        "A Arunkumar",
        "A Ashok",
        "A Dhanasekaran",
        "A Naik",
        "D Stap"
      ],
      "year": "2022",
      "venue": "Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks"
    },
    {
      "citation_id": "195",
      "title": "Is chatgpt a good sentiment analyzer? a preliminary study",
      "authors": [
        "Z Wang",
        "Q Xie",
        "Y Feng",
        "Z Ding",
        "Z Yang",
        "R Xia"
      ],
      "year": "2024",
      "venue": "Is chatgpt a good sentiment analyzer? a preliminary study",
      "arxiv": "arXiv:2304.04339"
    },
    {
      "citation_id": "196",
      "title": "Bilstm with multi-polarity orthogonal attention for implicit sentiment analysis",
      "authors": [
        "J Wei",
        "J Liao",
        "Z Yang",
        "S Wang",
        "Q Zhao"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "197",
      "title": "Emergent abilities of large language models",
      "authors": [
        "J Wei",
        "Y Tay",
        "R Bommasani",
        "C Raffel",
        "B Zoph",
        "S Borgeaud",
        "D Yogatama",
        "M Bosma",
        "D Zhou",
        "D Metzler",
        "E Chi",
        "T Hashimoto",
        "O Vinyals",
        "P Liang",
        "J Dean",
        "W Fedus"
      ],
      "year": "2022",
      "venue": "Emergent abilities of large language models",
      "arxiv": "arXiv:2206.07682"
    },
    {
      "citation_id": "198",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "199",
      "title": "Effective inter-clause modeling for end-to-end emotion-cause pair extraction",
      "authors": [
        "P Wei",
        "J Zhao",
        "W Mao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "200",
      "title": "Mimicking the mavens: agent-based opinion synthesis and emotion prediction for social media influencers",
      "authors": [
        "Q Wei",
        "R Xue",
        "Y Wang",
        "H Xiao",
        "Y Wang",
        "X Duan"
      ],
      "year": "2024",
      "venue": "Mimicking the mavens: agent-based opinion synthesis and emotion prediction for social media influencers",
      "arxiv": "arXiv:2407.20668"
    },
    {
      "citation_id": "201",
      "title": "Is chatgpt more empathetic than humans? URL",
      "authors": [
        "A Welivita",
        "P Pu"
      ],
      "year": "2024",
      "venue": "Is chatgpt more empathetic than humans? URL",
      "arxiv": "arXiv:2403.05572"
    },
    {
      "citation_id": "202",
      "title": "Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms",
      "authors": [
        "X Wen",
        "Z Liu",
        "S Zheng",
        "Z Xu",
        "S Ye",
        "Z Wu",
        "X Liang",
        "Y Wang",
        "J Li",
        "Z Miao"
      ],
      "year": "2025",
      "venue": "Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms",
      "arxiv": "arXiv:2506.14245"
    },
    {
      "citation_id": "203",
      "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "authors": [
        "J White",
        "Q Fu",
        "S Hays",
        "M Sandborn",
        "C Olea",
        "H Gilbert",
        "A Elnashar",
        "J Spencer-Smith",
        "D Schmidt"
      ],
      "year": "2023",
      "venue": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "arxiv": "arXiv:2302.11382"
    },
    {
      "citation_id": "204",
      "title": "Enhancing large language model with decomposed reasoning for emotion cause pair extraction",
      "authors": [
        "J Wu",
        "Y Shen",
        "Z Zhang",
        "L Cai"
      ],
      "year": "2024",
      "venue": "Enhancing large language model with decomposed reasoning for emotion cause pair extraction",
      "arxiv": "arXiv:2401.17716"
    },
    {
      "citation_id": "205",
      "title": "Enhancing large language model with decomposed reasoning for emotion cause pair extraction",
      "authors": [
        "J Wu",
        "Y Shen",
        "Z Zhang",
        "L Cai"
      ],
      "year": "2024",
      "venue": "Enhancing large language model with decomposed reasoning for emotion cause pair extraction",
      "arxiv": "arXiv:2401.17716"
    },
    {
      "citation_id": "206",
      "title": "A new dialogue response generation agent for large language models by asking questions to detect user's intentions",
      "authors": [
        "S Wu",
        "X Shen",
        "R Xia"
      ],
      "year": "2023",
      "venue": "A new dialogue response generation agent for large language models by asking questions to detect user's intentions",
      "arxiv": "arXiv:2310.03293"
    },
    {
      "citation_id": "207",
      "title": "Llm-based empathetic response through psychologist-agent debate",
      "authors": [
        "Y Wu",
        "S Feng",
        "M Wang",
        "D Wang",
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data"
    },
    {
      "citation_id": "208",
      "title": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "authors": [
        "Z Wu",
        "Z Gong",
        "L Ai",
        "P Shi",
        "K Donbekci",
        "J Hirschberg"
      ],
      "year": "2024",
      "venue": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "arxiv": "arXiv:2407.21315"
    },
    {
      "citation_id": "209",
      "title": "Using the wdep system of reality therapy to support person-centered treatment planning",
      "authors": [
        "R Wubbolding",
        "W Casstevens",
        "M Fulkerson"
      ],
      "year": "2017",
      "venue": "Journal of Counseling & Development"
    },
    {
      "citation_id": "210",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "R Xia",
        "Z Ding"
      ],
      "year": "2019",
      "venue": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "arxiv": "arXiv:1906.01267"
    },
    {
      "citation_id": "211",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "R Xia",
        "Z Ding"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1096"
    },
    {
      "citation_id": "212",
      "title": "Emovit: Revolutionizing emotion insights with visual instruction tuning",
      "authors": [
        "H Xie",
        "C.-J Peng",
        "Y.-W Tseng",
        "H.-J Chen",
        "C.-F Hsu",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2024",
      "venue": "Emovit: Revolutionizing emotion insights with visual instruction tuning"
    },
    {
      "citation_id": "213",
      "title": "An explanation of in-context learning as implicit bayesian inference",
      "authors": [
        "S Xie",
        "A Raghunathan",
        "P Liang",
        "T Ma"
      ],
      "year": "2022",
      "venue": "An explanation of in-context learning as implicit bayesian inference",
      "arxiv": "arXiv:2111.02080"
    },
    {
      "citation_id": "214",
      "title": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "authors": [
        "B Xing",
        "Z Yu",
        "X Liu",
        "K Yuan",
        "Q Ye",
        "W Xie",
        "H Yue",
        "J Yang",
        "H K√§lvi√§inen"
      ],
      "year": "2024",
      "venue": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "arxiv": "arXiv:2408.11424"
    },
    {
      "citation_id": "215",
      "title": "Designing heterogeneous llm agents for financial sentiment analysis",
      "authors": [
        "F Xing"
      ],
      "year": "2024",
      "venue": "Designing heterogeneous llm agents for financial sentiment analysis",
      "arxiv": "arXiv:2401.05799"
    },
    {
      "citation_id": "216",
      "title": "Rlthf: Targeted human feedback for llm alignment",
      "authors": [
        "Y Xu",
        "T Chakraborty",
        "E Kƒ±cƒ±man",
        "B Aryal",
        "E Rodrigues",
        "S Sharma",
        "R Estevao",
        "M Balaguer",
        "J Wolk",
        "R Padilha"
      ],
      "year": "2025",
      "venue": "Rlthf: Targeted human feedback for llm alignment",
      "arxiv": "arXiv:2502.13417"
    },
    {
      "citation_id": "217",
      "title": "Talk with human-like agents: Empathetic dialogue through perceptible acoustic reception and reaction",
      "authors": [
        "H Yan",
        "Y Zhu",
        "K Zheng",
        "B Liu",
        "H Cao",
        "D Jiang",
        "L Xu"
      ],
      "year": "2024",
      "venue": "Talk with human-like agents: Empathetic dialogue through perceptible acoustic reception and reaction",
      "arxiv": "arXiv:2406.12707"
    },
    {
      "citation_id": "218",
      "title": "An empirical study of multimodal entity-based sentiment analysis with chatgpt: Improving in-context learning via entity-aware contrastive learning",
      "authors": [
        "L Yang",
        "Z Wang",
        "Z Li",
        "J.-C Na",
        "J Yu"
      ],
      "year": "2024",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "219",
      "title": "Omni-emotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis",
      "authors": [
        "Q Yang",
        "D Bai",
        "Y.-X Peng",
        "X Wei"
      ],
      "year": "2025",
      "venue": "Omni-emotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis",
      "arxiv": "arXiv:2501.09502"
    },
    {
      "citation_id": "220",
      "title": "Mm-instructeval: Zero-shot evaluation of (multimodal) large language models on multimodal reasoning tasks",
      "authors": [
        "X Yang",
        "W Wu",
        "S Feng",
        "M Wang",
        "D Wang",
        "Y Li",
        "Q Sun",
        "Y Zhang",
        "X Fu",
        "S Poria"
      ],
      "year": "2024",
      "venue": "Mm-instructeval: Zero-shot evaluation of (multimodal) large language models on multimodal reasoning tasks"
    },
    {
      "citation_id": "221",
      "title": "Enhancing empathetic response generation by augmenting llms with small-scale empathetic models",
      "authors": [
        "Z Yang",
        "Z Ren",
        "W Yufeng",
        "S Peng",
        "H Sun",
        "X Zhu",
        "X Liao"
      ],
      "year": "2024",
      "venue": "Enhancing empathetic response generation by augmenting llms with small-scale empathetic models",
      "arxiv": "arXiv:2402.11801"
    },
    {
      "citation_id": "222",
      "title": "From generic empathy to personalized emotional support: A self-evolution framework for user preference alignment",
      "authors": [
        "J Ye",
        "L Xiang",
        "Y Zhang",
        "C Zong"
      ],
      "year": "2025",
      "venue": "From generic empathy to personalized emotional support: A self-evolution framework for user preference alignment",
      "arxiv": "arXiv:2505.16610"
    },
    {
      "citation_id": "223",
      "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
      "authors": [
        "Q Ye",
        "H Xu",
        "J Ye",
        "M Yan",
        "A Hu",
        "H Liu",
        "Q Qian",
        "J Zhang",
        "F Huang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "224",
      "title": "Training dialogue systems by ai feedback for improving overall dialogue impression",
      "authors": [
        "K Yoshida",
        "M Mizukami",
        "S Kawano",
        "C Kruengkrai",
        "H Sugiyama",
        "K Yoshino"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "225",
      "title": "Reflectdiffu: Reflect between emotion-intent contagion and mimicry for empathetic response generation via a rl-diffusion framework",
      "authors": [
        "J Yuan",
        "Z Di",
        "Z Cui",
        "G Yang",
        "U Naseem"
      ],
      "year": "2024",
      "venue": "Reflectdiffu: Reflect between emotion-intent contagion and mimicry for empathetic response generation via a rl-diffusion framework",
      "arxiv": "arXiv:2409.10289"
    },
    {
      "citation_id": "226",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "227",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "A Zeng",
        "X Liu",
        "Z Du",
        "Z Wang",
        "H Lai",
        "M Ding",
        "Z Yang",
        "Y Xu",
        "W Zheng",
        "X Xia",
        "W Tam",
        "Z Ma",
        "Y Xue",
        "J Zhai",
        "W Chen",
        "P Zhang",
        "Y Dong",
        "J Tang"
      ],
      "year": "2023",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "arxiv": "arXiv:2210.02414"
    },
    {
      "citation_id": "228",
      "title": "Sentient agent as a judge: Evaluating higher-order social cognition in large language models",
      "authors": [
        "B Zhang",
        "R Ma",
        "Q Jiang",
        "P Wang",
        "J Chen",
        "Z Xie",
        "X Chen",
        "Y Wang",
        "F Ye",
        "J Li"
      ],
      "year": "2025",
      "venue": "Sentient agent as a judge: Evaluating higher-order social cognition in large language models",
      "arxiv": "arXiv:2505.02847"
    },
    {
      "citation_id": "229",
      "title": "Enhancing financial sentiment analysis via retrieval augmented large language models",
      "authors": [
        "B Zhang",
        "H Yang",
        "T Zhou",
        "A Babar",
        "X.-Y Liu"
      ],
      "year": "2023",
      "venue": "Enhancing financial sentiment analysis via retrieval augmented large language models",
      "arxiv": "arXiv:2310.04027"
    },
    {
      "citation_id": "230",
      "title": "Decoupledesc: Enhancing emotional support generation via strategy-response decoupled preference optimization",
      "authors": [
        "C Zhang",
        "X Shi",
        "X Zhang",
        "Y Zhu",
        "Y Yang",
        "Y Luo"
      ],
      "year": "2025",
      "venue": "Decoupledesc: Enhancing emotional support generation via strategy-response decoupled preference optimization",
      "arxiv": "arXiv:2505.16995"
    },
    {
      "citation_id": "231",
      "title": "Rl-emo: A reinforcement learning framework for multimodal emotion recognition",
      "authors": [
        "C Zhang",
        "Y Zhang",
        "B Cheng"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "232",
      "title": "An instruction tuningbased contrastive learning framework for aspect sentiment quad prediction with implicit aspects and opinions",
      "authors": [
        "H Zhang",
        "Y.-N Cheah",
        "C He",
        "F Yi"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "doi": "10.18653/v1/2024.findings-emnlp.453"
    },
    {
      "citation_id": "233",
      "title": "Adalora: Adaptive budget allocation for parameter-efficient fine-tuning",
      "authors": [
        "Q Zhang",
        "M Chen",
        "A Bukharin",
        "N Karampatziakis",
        "P He",
        "Y Cheng",
        "W Chen",
        "T Zhao"
      ],
      "year": "2023",
      "venue": "Adalora: Adaptive budget allocation for parameter-efficient fine-tuning",
      "arxiv": "arXiv:2303.10512"
    },
    {
      "citation_id": "234",
      "title": "Self-emotion blended dialogue generation in social simulation agents",
      "authors": [
        "Q Zhang",
        "J Naradowsky",
        "Y Miyao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "235",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "S Zhang",
        "L Dong",
        "X Li",
        "S Zhang",
        "X Sun",
        "S Wang",
        "J Li",
        "R Hu",
        "T Zhang",
        "F Wu",
        "G Wang"
      ],
      "year": "2024",
      "venue": "Instruction tuning for large language models: A survey",
      "arxiv": "arXiv:2308.10792"
    },
    {
      "citation_id": "236",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "S Zhang",
        "L Dong",
        "X Li",
        "S Zhang",
        "X Sun",
        "S Wang",
        "J Li",
        "R Hu",
        "T Zhang",
        "F Wu"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: A survey",
      "arxiv": "arXiv:2308.10792"
    },
    {
      "citation_id": "237",
      "title": "Revisiting sentiment analysis for software engineering in the era of large language models",
      "authors": [
        "T Zhang",
        "I Irsan",
        "F Thung",
        "D Lo"
      ],
      "year": "2023",
      "venue": "Revisiting sentiment analysis for software engineering in the era of large language models",
      "arxiv": "arXiv:2310.11113"
    },
    {
      "citation_id": "238",
      "title": "Escot: Towards interpretable emotional support dialogue systems",
      "authors": [
        "T Zhang",
        "X Zhang",
        "J Zhao",
        "L Zhou",
        "Q Jin"
      ],
      "year": "2024",
      "venue": "Escot: Towards interpretable emotional support dialogue systems",
      "arxiv": "arXiv:2406.10960"
    },
    {
      "citation_id": "239",
      "title": "Aspect sentiment quad prediction as paraphrase generation",
      "authors": [
        "W Zhang",
        "Y Deng",
        "X Li",
        "Y Yuan",
        "L Bing",
        "W Lam"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "240",
      "title": "Sentiment analysis in the era of large language models: A reality check",
      "authors": [
        "W Zhang",
        "Y Deng",
        "B Liu",
        "S Pan",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Sentiment analysis in the era of large language models: A reality check",
      "arxiv": "arXiv:2305.15005"
    },
    {
      "citation_id": "241",
      "title": "Sentiment analysis in the era of large language models: A reality check",
      "authors": [
        "W Zhang",
        "Y Deng",
        "B Liu",
        "S Pan",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Sentiment analysis in the era of large language models: A reality check",
      "arxiv": "arXiv:2305.15005"
    },
    {
      "citation_id": "242",
      "title": "A survey on aspect-based sentiment analysis: Tasks, methods, and challenges",
      "authors": [
        "W Zhang",
        "X Li",
        "Y Deng",
        "L Bing",
        "W Lam"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "doi": "10.1109/TKDE.2022.3230975"
    },
    {
      "citation_id": "243",
      "title": "Character-level convolutional networks for text classification",
      "authors": [
        "X Zhang",
        "J Zhao",
        "Y Lecun"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "244",
      "title": "Sage: Steering and refining dialog generation with state-action augmentation",
      "authors": [
        "Y Zhang",
        "N Jaitly"
      ],
      "year": "2025",
      "venue": "Sage: Steering and refining dialog generation with state-action augmentation",
      "arxiv": "arXiv:2503.03040"
    },
    {
      "citation_id": "245",
      "title": "Stickerconv: Generating multimodal empathetic responses from scratch",
      "authors": [
        "Y Zhang",
        "F Kong",
        "P Wang",
        "S Sun",
        "L Wang",
        "S Feng",
        "D Wang",
        "Y Zhang",
        "K Song"
      ],
      "year": "2024",
      "venue": "Stickerconv: Generating multimodal empathetic responses from scratch",
      "arxiv": "arXiv:2402.01679"
    },
    {
      "citation_id": "246",
      "title": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "authors": [
        "Y Zhang",
        "M Wang",
        "P Tiwari",
        "Q Li",
        "B Wang",
        "J Qin"
      ],
      "year": "2023",
      "venue": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "arxiv": "arXiv:2310.11374"
    },
    {
      "citation_id": "247",
      "title": "Pica: Unleashing the emotional power of large language model",
      "authors": [
        "Y Zhang",
        "J Zhang",
        "Y Liu",
        "C Gao",
        "D Wang",
        "S Feng",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Pica: Unleashing the emotional power of large language model"
    },
    {
      "citation_id": "248",
      "title": "Moda: Modular duplex attention for multimodal perception, cognition, and emotion understanding",
      "authors": [
        "Z Zhang",
        "W Xia",
        "C Zhao",
        "Z Yan",
        "X Liu",
        "Y Zhu",
        "W Qin",
        "P Wan",
        "D Zhang",
        "J Yang"
      ],
      "year": "2025",
      "venue": "Moda: Modular duplex attention for multimodal perception, cognition, and emotion understanding",
      "arxiv": "arXiv:2507.04635"
    },
    {
      "citation_id": "249",
      "title": "Esc-eval: Evaluating emotion support conversations in large language models",
      "authors": [
        "H Zhao",
        "L Li",
        "S Chen",
        "S Kong",
        "J Wang",
        "K Huang",
        "T Gu",
        "Y Wang",
        "W Jian",
        "D Liang"
      ],
      "year": "2024",
      "venue": "Esc-eval: Evaluating emotion support conversations in large language models",
      "arxiv": "arXiv:2406.14952"
    },
    {
      "citation_id": "250",
      "title": "a). R1-omni: Explainable omnimultimodal emotion recognition with reinforcement learning",
      "authors": [
        "J Zhao",
        "X Wei",
        "L Bo"
      ],
      "year": "2025",
      "venue": "a). R1-omni: Explainable omnimultimodal emotion recognition with reinforcement learning",
      "arxiv": "arXiv:2503.05379"
    },
    {
      "citation_id": "251",
      "title": "Both matter: Enhancing the emotional intelligence of large language models without compromising the general intelligence",
      "authors": [
        "W Zhao",
        "Z Li",
        "S Wang",
        "Y Wang",
        "Y Hu",
        "Y Zhao",
        "C Wei",
        "B Qin"
      ],
      "year": "2024",
      "venue": "Both matter: Enhancing the emotional intelligence of large language models without compromising the general intelligence",
      "arxiv": "arXiv:2402.10073"
    },
    {
      "citation_id": "252",
      "title": "Chain of strategy optimization makes large language models better emotional supporter",
      "authors": [
        "W Zhao",
        "X Sui",
        "X Han",
        "Y Deng",
        "Y Hu",
        "J Guo",
        "L Qin",
        "Q Du",
        "S Wang",
        "Y Zhao"
      ],
      "year": "2025",
      "venue": "Chain of strategy optimization makes large language models better emotional supporter",
      "arxiv": "arXiv:2503.05362"
    },
    {
      "citation_id": "253",
      "title": "Is chatgpt equipped with emotional dialogue capabilities?",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu",
        "S Wang",
        "Y Tong",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Is chatgpt equipped with emotional dialogue capabilities?",
      "doi": "10.48550/arXiv.2304.09582",
      "arxiv": "arXiv:2304.09582"
    },
    {
      "citation_id": "254",
      "title": "A survey of large language models",
      "authors": [
        "W Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong",
        "Y Du",
        "C Yang",
        "Y Chen",
        "Z Chen",
        "J Jiang",
        "R Ren",
        "Y Li",
        "X Tang",
        "Z Liu",
        "P Liu",
        "J.-Y Nie",
        "J.-R Wen"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "255",
      "title": "Secrets of rlhf in large language models part i: Ppo",
      "authors": [
        "R Zheng",
        "S Dou",
        "S Gao",
        "Y Hua",
        "W Shen",
        "B Wang",
        "Y Liu",
        "S Jin",
        "Q Liu",
        "Y Zhou",
        "L Xiong",
        "L Chen",
        "Z Xi",
        "N Xu",
        "W Lai",
        "M Zhu",
        "C Chang",
        "Z Yin",
        "R Weng",
        "W Cheng",
        "H Huang",
        "T Sun",
        "H Yan",
        "T Gui",
        "Q Zhang",
        "X Qiu",
        "X Huang"
      ],
      "year": "2023",
      "venue": "Secrets of rlhf in large language models part i: Ppo",
      "doi": "10.48550/arXiv.2307.04964",
      "arxiv": "arXiv:2307.04964"
    },
    {
      "citation_id": "256",
      "title": "Building emotional support chatbots in the era of llms",
      "authors": [
        "Z Zheng",
        "L Liao",
        "Y Deng",
        "L Nie"
      ],
      "year": "2023",
      "venue": "Building emotional support chatbots in the era of llms",
      "arxiv": "arXiv:2308.11584"
    },
    {
      "citation_id": "257",
      "title": "A comprehensive evaluation of large language models on aspect-based sentiment analysis",
      "authors": [
        "C Zhou",
        "D Song",
        "Y Tian",
        "Z Wu",
        "H Wang",
        "X Zhang",
        "J Yang",
        "Z Yang",
        "S Zhang"
      ],
      "year": "2024",
      "venue": "A comprehensive evaluation of large language models on aspect-based sentiment analysis",
      "arxiv": "arXiv:2412.02279"
    },
    {
      "citation_id": "258",
      "title": "Prompt consistency for multi-label textual emotion detection",
      "authors": [
        "Y Zhou",
        "X Kang",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "259",
      "title": "Large language models are human-level prompt engineers",
      "authors": [
        "Y Zhou",
        "A Muresanu",
        "Z Han",
        "K Paster",
        "S Pitis",
        "H Chan",
        "J Ba"
      ],
      "year": "2023",
      "venue": "Large language models are human-level prompt engineers",
      "arxiv": "arXiv:2211.01910"
    },
    {
      "citation_id": "260",
      "title": "The model arena for cross-lingual sentiment analysis: A comparative study in the era of large language models",
      "authors": [
        "X Zhu",
        "S Gardiner",
        "T Rold√°n",
        "D Rossouw"
      ],
      "year": "2024",
      "venue": "The model arena for cross-lingual sentiment analysis: A comparative study in the era of large language models",
      "arxiv": "arXiv:2406.19358"
    },
    {
      "citation_id": "261",
      "title": "Grafting fine-tuning and reinforcement learning for empathetic emotion elicitation in dialog generation",
      "authors": [
        "Y Zhu",
        "B Wang",
        "D Zhao",
        "K Huang",
        "Z Jiang",
        "R He",
        "Y Hou"
      ],
      "year": "2023",
      "venue": "26th European Conference on Artificial Intelligence (ECAI 2023)"
    },
    {
      "citation_id": "262",
      "title": "Medic: A multimodal empathy dataset in counseling",
      "authors": [
        "Z Zhu",
        "X Li",
        "J Pan",
        "Y Xiao",
        "Y Chang",
        "F Zheng",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "263",
      "title": "Joint aspectsentiment analysis with minimal user guidance",
      "authors": [
        "H Zhuang",
        "F Guo",
        "C Zhang",
        "L Liu",
        "J Han"
      ],
      "year": "2020",
      "venue": "Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval"
    }
  ]
}