{
  "paper_id": "2006.03001v1",
  "title": "A Siamese Neural Network With Modified Distance Loss For Transfer Learning In Speech Emotion Recognition",
  "published": "2020-06-04T16:44:33Z",
  "authors": [
    "Kexin Feng",
    "Theodora Chaspari"
  ],
  "keywords": [
    "Emotion recognition",
    "Speech",
    "Transfer learning",
    "Finetuning",
    "Siamese neural network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition plays a significant role in the process of human computer interaction and the design of Internet of Things (IOT) technologies. Yet, a common problem in emotion recognition systems lies in the scarcity of reliable labels. By modelling pairwise differences between samples of interest, a Siamese network can help to mitigate this challenge since it requires fewer samples than traditional deep learning methods. In this paper, we propose a distance loss, which can be applied on the Siamese network fine-tuning, by optimizing the model based on the relevant distance between same and different class pairs. Our system uses samples from the source data to pre-train the weights of proposed Siamese neural network, which are fine-tuned based on the target data. We present an emotion recognition task that uses speech, since it is one of the most ubiquitous and frequently used biobehavioral signals. Our target data comes from the RAVDESS dataset, while the CREMA-D and eNTERFACE05 are used as source data, respectively. Our results indicate that the proposed distance loss is able to greatly benefit the fine-tuning process of Siamese network. Also, the selection of source data has more effect on the Siamese network performance compared to the number of frozen layers. These suggest the great potential of applying the Siamese network and modelling pairwise differences in the field of transfer learning for automatic emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic emotion recognition refers to identifying emotions using various humanrelated signals such as facial expression, physiological signals, and speech  [11] . It can potentially benefit with many applications related to human computer interaction, health informatics, or even the design of smart cities and communities. Among these signals, speech data is largely explored due to its relatively higher availability and ease of collection. Acquiring reliable annotation for such large amounts of audio clips can be extremely hard to obtain, providing a significant impediment for the reliable training of emotion recognition systems.\n\nA great number of machine learning approaches have been proposed to address this challenge, and transfer learning was shown to be one of the most promising directions. Transfer learning methods such as fine-tuning  [1]  make use of a well-trained model on another emotion dataset. Also, progressive neural networks (PNN)  [5] , which are less forgetful when applied to target, have been proposed and obtained good performance in leveraging knowledge between various conditions. However, these methods might be less effective when there are very limited number of data in target domain, preventing adequate training of the corresponding machine learning models.\n\nModelling the pairwise differences between samples of interest (e.g., through Siamese networks  [7] ) could be a potential solution to address small amounts of labelled target data in various applications. In this paper, we propose the use of Siamese network structure for the task of transfer learning in speech emotion recognition, trained using the fine-tuning method, and further optimized using a distance loss that incorporates relative distance among pairs. A publicly available dataset, RAVDESS  [9] , was used as target data due to its relatively small number of speakers and sample size. Two other emotional datasets, CREMA-D  [3]  and eNTERFACE05  [10] , are used as source data to compare the influence of different source domains. Our results indicate that the selection of source data can have a significant impact on the Siamese network fine-tuning, and our distance loss can significantly benefit the Siamese network fine-tuning process, which yields an improvement of up to 7% compared to fine-tuning the Siamese network without the proposed distance loss.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Previous Work",
      "text": "The Siamese network is a type of neural network which takes a pair of data samples as an input, and decides whether the corresponding samples belong to the same or different classes. This network structure was first introduced by Bromley et al. for the task of signature verification by comparing whether the two signatures are from the same person  [2] . In Siamese network, each data sample in a pair is the input to the network, then the network outputs an extracted feature vector. The l2-norm between the two extracted feature vectors is further calculated based on the two extracted features. If the l2-norm distance is less than a certain threshold point, the model will decide the samples in this pair are from the same class, otherwise the samples belong to different classes.\n\nResearchers further optimized the initially proposed structure of the Siamese network by considering the data distribution within the classes of interest  [7] . Instead of calculating the l2-norm, the l1-norm vector was calculated, and was followed by fully connected layers with sigmoid activation to allow the model learn the relations between distance and classes.\n\nThe Siamese network has been applied to emotion-related tasks with a promising performance  [6, 8, 12] . However, to the best of our knowledge, this is the first time the Siamese network was applied and further optimized for emotion-related transfer learning task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Description",
      "text": "The target data comes from the RAVDESS dataset  [9] , which includes 47 minutes of audio from 24 actors, and each audio sample is further labeled by 247 annotators. The source data comes from two different publicly available datasets: the eNTERFACE05  [10]  and CREMA-D  [3] . The eNTERFACE05 contains 45 minutes of data from 42 participants, who were asked to express their emotions in scripted sentences after listening to specific stories. The CREMA-D dataset contains 165 minutes of audio data from 91 actors, who were to perform until they get approved by a director and each audio clip is further labeled by human annotators.\n\nSpeech samples depicting four common emotions (anger, happiness, sadness, and fear) across all datasets were selected and processed using openSMILE toolkit  [4] . A 64-dimensional feature set, part of the INTERSPEECH09 emotion challenge feature set  [13] , is extracted from each audio segment which includes the mean and standard deviations of speech intensity, zero-crossing rate, voice probability, fundamental frequency, and the first 12 Mel-frequency cepstral coefficient (MFCC). The first order derivative of each of these features is also computed.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we plan to discuss our three baseline methods: in-domain training, out of domain training, and Siamese network fine-tuning. Then we will introduce the proposed distance loss, and how this loss is applied for the Siamese network. All the methods are based on an optimized Siamese network structure, which contains 64, 32, and 16 nodes with ReLU activation for the first three feature extractor layers, followed by a 16-node decision-based layer. During the training process, the cross entropy loss is calculated and used to update the weights of the model. After the Siamese network is trained, the test data will be compared with all the available labeled target data, and a final classification decision will be generated based on the similarity of log-sum with each class. The unweighted average recall (UAR) will be used to evaluate the performance for each method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline",
      "text": "In order to assess whether the proposed loss will benefit the knowledge transfer efficiency when limited data sample is available, we proposed three baseline methods. The first baseline performs an out-of-domain training (OODT). A Siamese network is trained on all the source data, and tested on the target dataset without any adaptation. For this baseline, the data samples from the source are used to determine the final classes, since no labeled target data is available.\n\nThe second baseline is an in-domain training (IDT), which is trained using sufficient amount of target data. A leave-one-subject-out (LOSO) crossvalidation is performed. More specifically, samples from a given speaker are used for testing, and all other data are used for training of the Siamese network. This process is repeated until all the speakers have been used for testing. This baseline serves as an upper limit of potential knowledge transfer.\n\nThe third baseline is the traditional fine-tuning method in the field of transfer learning, to further assess the benefit provided by our proposed distance loss. The models trained in OODT are used for fine-tuning. One random data sample for each emotion from random 2 speakers are selected as labeled data. As a result, a total number of 8 samples (2 speakers × 4 emotions × 1 sample/emotion/speaker) are used for fine-tuning process and the remaining data are used as testing set. This process is repeated 10 times to increase the robustness of the results. A different number of frozen layers (none, first layer, or first two layers), as well as the different number of speakers in target data (2, 4, . . . , 18, 20) are tested to evaluate in detail the knowledge transfer efficiency.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Siamese Nn Fine-Tuning With Modified Loss",
      "text": "Inspired by the fact that the traditional fine-tuning method fails to efficiently leverage the knowledge when limited target data is available, we designed a distance loss to maximize the distance difference between pairs from the same class and pairs from the different class. Let X s be the set of s pairs with the same class in a batch, and X d is the set of d pairs with the different class. Let g W (x) be the function parameterized by the weights W of the Siamese network that performs the transformation between the original input x and the extracted feature vector. The parameter W is learned by minimizing the average relative distance between pairs X s of the same class and maximizing the distance between pairs X d of different classes:\n\nXs 0\n\nx,x ∈Xs g(x), g(x ) 2 (1) As indicated in the previous description, this loss is only applied to the feature extraction layers at the end of a batch in the Siamese network finetuning process to minimize the possible influence to the decision making process (Figure  1 ). Besides this loss, the other part for this method remains the same compared to the fine-tuning baseline method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "In an effort to discuss the domain difference between the different speech emotion datasets, we first examine the unweighted average recall (UAR) for four emotion classification task on OODT baseline. This yields a 32.8% UAR on the RAVDESS dataset when using eNTERFACE05 as source and 29.3% when using CREMA-D as source. We then examines the upper limit of classification performance using the proposed feature and model structure, and obtained a UAR at 50.0% with in-domain training.\n\nThe fine-tuning of the Siamese neural network without the proposed loss is used to illustrated the knowledge transfer efficiency. This approach resulted in a minor improvement when using eNTERFACE05 as source data at 32.9%, and a relatively large improvement when using CREMA-D as source data at 37.8%. We finally added our proposed distance loss in the fine-tuning process, and obtained a significant improvement of 39.9% using eNTERFACE05 and 43.3% using CREMA-D. A comparison of the performance of different methods can be found at Table  1 . Our results also indicate that compared with different number of frozen layers, different source data may has a more important role for Siamese network fine-tuning (Figure  2 ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "Our results indicate that including 3-5 speakers in the Siamese NN fine-tuning with modified loss may result in the best trade-off between performance and the    number of data samples used for training. This can be potentially explained by the fact that this number of speakers might not be enough to capture the distribution of the target dataset. Performance is degraded when including a smaller number of speakers in the target data. Also, if the data distribution includes a lot of variability, Siamese NN with modified loss will be greatly restricted, since pairwise differences are less likely to express the class information.\n\nThere has not been a lot of research relevant to few-shot emotion recognition with speech signals, where data from only a few number of speakers is included in the target data. The most comparable results are from Gideon et al., in which the proposed approach with progressive neural networks achieved a limited improvement (around 3%) compared to traditional fine-tuning (  [5] ). Our proposed distance loss is able to bring a relative larger improvement at about 7% compared to fine-tuning, without significant increase in the computational cost. Even though Siamese NN fine-tuning with modified loss still fails to outperform the in-domain training, it shows great potential. We will attempt to modify the proposed methodology and explore whether it can reach in-domain performance as part of our future work.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "We propose a distance loss which is based on the relative distance between same and different class pairs. Such loss can increase the upper limit and increase knowledge transfer efficiency when very limited target data is available. Our results also indicate that the selection of source data plays a more important role than the number of frozen layers. Findings of this work can be applied on other tasks using Siamese network, fine-tuning, or few-shot learning. The application of distance between different emotion classes can be a fundamental step in understanding the difference between emotion categories.\n\nAs part of our future work, we plan to explore the usage of pairs in addressing the domain mismatch, and mitigate the influence of different source data in the process of knowledge transfer. We will further perform the multi-source transfer learning by using pairwise information to select proper source data from each dataset. Finally, we plan to understand the distance between different emotions with the help of the pairing information.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic representation of the proposed Siamese network ﬁne-tuning with",
      "page": 4
    },
    {
      "caption": "Figure 1: ). Besides this loss, the other part for this method remains the same",
      "page": 5
    },
    {
      "caption": "Figure 2: Unweighted average recall (UAR) for freeze diﬀerent number of layers and",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Texas A&M University": "{kexin0814,\nchaspari}@tamu.edu"
        },
        {
          "Texas A&M University": "Abstract. Automatic emotion recognition plays a signiﬁcant role in the"
        },
        {
          "Texas A&M University": "process of human computer\ninteraction and the design of\nInternet of"
        },
        {
          "Texas A&M University": "Things (IOT) technologies. Yet, a common problem in emotion recogni-"
        },
        {
          "Texas A&M University": "tion systems lies in the scarcity of reliable labels. By modelling pairwise"
        },
        {
          "Texas A&M University": "diﬀerences between samples of\ninterest, a Siamese network can help to"
        },
        {
          "Texas A&M University": "mitigate this challenge since it\nrequires\nfewer\nsamples\nthan traditional"
        },
        {
          "Texas A&M University": "deep learning methods. In this paper, we propose a distance loss, which"
        },
        {
          "Texas A&M University": "can be applied on the Siamese network ﬁne-tuning, by optimizing the"
        },
        {
          "Texas A&M University": "model based on the relevant distance between same and diﬀerent class"
        },
        {
          "Texas A&M University": "pairs. Our\nsystem uses\nsamples\nfrom the source data to pre-train the"
        },
        {
          "Texas A&M University": "weights of proposed Siamese neural network, which are ﬁne-tuned based"
        },
        {
          "Texas A&M University": "on the target data. We present an emotion recognition task that uses"
        },
        {
          "Texas A&M University": "speech,\nsince it\nis one of\nthe most ubiquitous and frequently used bio-"
        },
        {
          "Texas A&M University": "behavioral signals. Our target data comes from the RAVDESS dataset,"
        },
        {
          "Texas A&M University": "while the CREMA-D and eNTERFACE05 are used as source data, re-"
        },
        {
          "Texas A&M University": "spectively. Our\nresults\nindicate that\nthe proposed distance loss\nis able"
        },
        {
          "Texas A&M University": "to greatly beneﬁt the ﬁne-tuning process of Siamese network. Also, the"
        },
        {
          "Texas A&M University": "selection of source data has more eﬀect on the Siamese network perfor-"
        },
        {
          "Texas A&M University": "mance compared to the number of frozen layers. These suggest the great"
        },
        {
          "Texas A&M University": "potential of applying the Siamese network and modelling pairwise diﬀer-"
        },
        {
          "Texas A&M University": "ences in the ﬁeld of transfer learning for automatic emotion recognition."
        },
        {
          "Texas A&M University": "Keywords: Emotion recognition · Speech · Transfer\nlearning · Fine-"
        },
        {
          "Texas A&M University": "tuning · Siamese neural network."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "Automatic emotion recognition refers to identifying emotions using various human-",
          "Introduction": ""
        },
        {
          "1": "related signals such as facial expression, physiological signals, and speech [11]. It",
          "Introduction": ""
        },
        {
          "1": "can potentially beneﬁt with many applications related to human computer inter-",
          "Introduction": ""
        },
        {
          "1": "action, health informatics, or even the design of smart cities and communities.",
          "Introduction": ""
        },
        {
          "1": "Among these signals, speech data is largely explored due to its relatively higher",
          "Introduction": ""
        },
        {
          "1": "availability and ease of collection. Acquiring reliable annotation for such large",
          "Introduction": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nFeng et al.": "amounts of audio clips can be extremely hard to obtain, providing a signiﬁcant"
        },
        {
          "2\nFeng et al.": "impediment for the reliable training of emotion recognition systems."
        },
        {
          "2\nFeng et al.": "A great number of machine learning approaches have been proposed to ad-"
        },
        {
          "2\nFeng et al.": "dress\nthis\nchallenge, and transfer\nlearning was\nshown to be one of\nthe most"
        },
        {
          "2\nFeng et al.": "promising directions. Transfer learning methods such as ﬁne-tuning [1] make use"
        },
        {
          "2\nFeng et al.": "of a well-trained model on another\nemotion dataset. Also, progressive neural"
        },
        {
          "2\nFeng et al.": "networks (PNN) [5], which are less forgetful when applied to target, have been"
        },
        {
          "2\nFeng et al.": "proposed and obtained good performance in leveraging knowledge between var-"
        },
        {
          "2\nFeng et al.": "ious conditions. However, these methods might be less eﬀective when there are"
        },
        {
          "2\nFeng et al.": "very limited number of data in target domain, preventing adequate training of"
        },
        {
          "2\nFeng et al.": "the corresponding machine learning models."
        },
        {
          "2\nFeng et al.": "Modelling the pairwise diﬀerences between samples of\ninterest (e.g., through"
        },
        {
          "2\nFeng et al.": "Siamese networks [7]) could be a potential solution to address small amounts of"
        },
        {
          "2\nFeng et al.": "labelled target data in various applications.\nIn this paper, we propose the use"
        },
        {
          "2\nFeng et al.": "of Siamese network structure for the task of transfer learning in speech emotion"
        },
        {
          "2\nFeng et al.": "recognition, trained using the ﬁne-tuning method, and further optimized using a"
        },
        {
          "2\nFeng et al.": "distance loss that incorporates relative distance among pairs. A publicly available"
        },
        {
          "2\nFeng et al.": "dataset, RAVDESS [9], was used as target data due to its relatively small number"
        },
        {
          "2\nFeng et al.": "of speakers and sample size. Two other emotional datasets, CREMA-D [3] and"
        },
        {
          "2\nFeng et al.": "eNTERFACE05 [10], are used as source data to compare the inﬂuence of diﬀerent"
        },
        {
          "2\nFeng et al.": "source domains. Our results indicate that the selection of source data can have a"
        },
        {
          "2\nFeng et al.": "signiﬁcant impact on the Siamese network ﬁne-tuning, and our distance loss can"
        },
        {
          "2\nFeng et al.": "signiﬁcantly beneﬁt\nthe Siamese network ﬁne-tuning process, which yields an"
        },
        {
          "2\nFeng et al.": "improvement of up to 7% compared to ﬁne-tuning the Siamese network without"
        },
        {
          "2\nFeng et al.": "the proposed distance loss."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "time the Siamese network was applied and further optimized for emotion-related"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "transfer learning task."
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "3\nData Description"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "The target data comes from the RAVDESS dataset [9], which includes 47 min-"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "utes of audio from 24 actors, and each audio sample is\nfurther\nlabeled by 247"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "annotators. The source data comes from two diﬀerent publicly available datasets:"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "the eNTERFACE05 [10] and CREMA-D [3]. The eNTERFACE05 contains 45"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "minutes of data from 42 participants, who were asked to express their emotions"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "in scripted sentences after listening to speciﬁc stories. The CREMA-D dataset"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "contains 165 minutes of audio data from 91 actors, who were asked to perform"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "until they get approved by a director and each audio clip is further labeled by"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "human annotators."
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "Speech samples depicting four common emotions (anger, happiness, sadness,"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "and fear)\nacross\nall datasets were\nselected and processed using\nopenSMILE"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "toolkit\n[4]. A 64-dimensional\nfeature set, part of\nthe INTERSPEECH09 emo-"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "tion challenge feature set [13],\nis extracted from each audio segment which in-"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "cludes the mean and standard deviations of speech intensity, zero-crossing rate,"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "voice probability, fundamental frequency, and the ﬁrst 12 Mel-frequency cepstral"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "coeﬃcient\n(MFCC). The ﬁrst order derivative of each of\nthese features\nis also"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "computed."
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "4\nMethodology"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "In this section, we plan to discuss our three baseline methods: in-domain training,"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "out of domain training, and Siamese network ﬁne-tuning. Then we will introduce"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "the proposed distance loss, and how this loss is applied for the Siamese network."
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "All\nthe methods are based on an optimized Siamese network structure, which"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "contains 64, 32, and 16 nodes with ReLU activation for\nthe ﬁrst\nthree feature"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "extractor layers, followed by a 16-node decision-based layer. During the training"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "process, the cross entropy loss is calculated and used to update the weights of"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "the model. After the Siamese network is trained, the test data will be compared"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "with all the available labeled target data, and a ﬁnal classiﬁcation decision will"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "be generated based on the similarity of log-sum with each class. The unweighted"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "average recall (UAR) will be used to evaluate the performance for each method."
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "4.1\nBaseline"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "In order to assess whether the proposed loss will beneﬁt the knowledge transfer"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "eﬃciency when limited data sample\nis available, we proposed three baseline"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "methods. The ﬁrst baseline performs\nan out-of-domain training\n(OODT). A"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "Siamese network is\ntrained on all\nthe\nsource data, and tested on the\ntarget"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n3": "dataset without any adaptation. For\nthis baseline,\nthe data samples\nfrom the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "L1-norm": "different\tpair\nmaking\tlayer"
        },
        {
          "L1-norm": "Feature\t\nFeature"
        },
        {
          "L1-norm": "Input\tlayer\nHidden\tlayer\nData\tsample\t2"
        },
        {
          "L1-norm": "extractor\tout\nvector\t2"
        },
        {
          "L1-norm": "Fig. 1. Schematic\nrepresentation of\nthe proposed Siamese network ﬁne-tuning with"
        },
        {
          "L1-norm": "modiﬁed distance loss."
        },
        {
          "L1-norm": "source are used to determine the ﬁnal classes,\nsince no labeled target data is"
        },
        {
          "L1-norm": "available."
        },
        {
          "L1-norm": "The\nsecond baseline\nis an in-domain training (IDT), which is\ntrained us-"
        },
        {
          "L1-norm": "ing\nsuﬃcient\namount\nof\ntarget data. A leave-one-subject-out\n(LOSO)\ncross-"
        },
        {
          "L1-norm": "validation is performed. More speciﬁcally, samples from a given speaker are used"
        },
        {
          "L1-norm": "for testing, and all other data are used for training of the Siamese network. This"
        },
        {
          "L1-norm": "process is repeated until all the speakers have been used for testing. This baseline"
        },
        {
          "L1-norm": "serves as an upper limit of potential knowledge transfer."
        },
        {
          "L1-norm": "The third baseline is the traditional ﬁne-tuning method in the ﬁeld of trans-"
        },
        {
          "L1-norm": "fer\nlearning,\nto further assess\nthe beneﬁt provided by our proposed distance"
        },
        {
          "L1-norm": "loss. The models trained in OODT are used for ﬁne-tuning. One random data"
        },
        {
          "L1-norm": "sample for each emotion from random 2 speakers are selected as labeled data."
        },
        {
          "L1-norm": "As a result, a total number of 8 samples\n(2 speakers × 4 emotions × 1 sam-"
        },
        {
          "L1-norm": "ple/emotion/speaker) are used for ﬁne-tuning process and the remaining data"
        },
        {
          "L1-norm": "are used as testing set. This process is repeated 10 times to increase the robust-"
        },
        {
          "L1-norm": "ness of the results. A diﬀerent number of\nfrozen layers (none, ﬁrst layer, or ﬁrst"
        },
        {
          "L1-norm": "two layers), as well as the diﬀerent number of speakers in target data (2, 4,\n. . ."
        },
        {
          "L1-norm": ", 18, 20) are tested to evaluate in detail the knowledge transfer eﬃciency."
        },
        {
          "L1-norm": "4.2\nSiamese NN ﬁne-tuning with modiﬁed loss"
        },
        {
          "L1-norm": "Inspired by the fact\nthat\nthe traditional ﬁne-tuning method fails\nto eﬃciently"
        },
        {
          "L1-norm": "leverage\nthe knowledge when limited target data is available, we designed a"
        },
        {
          "L1-norm": "distance loss to maximize the distance diﬀerence between pairs from the same"
        },
        {
          "L1-norm": "class and pairs\nfrom the diﬀerent class. Let Xs be the set of s pairs with the"
        },
        {
          "L1-norm": "is the set of d pairs with the diﬀerent class. Let\nsame class in a batch, and Xd"
        },
        {
          "L1-norm": "gW(x) be the function parameterized by the weights W of the Siamese network"
        },
        {
          "L1-norm": "that performs the transformation between the original input x and the extracted"
        },
        {
          "L1-norm": "feature vector. The parameter W is learned by minimizing the average relative"
        },
        {
          "L1-norm": "distance between pairs Xs of the same class and maximizing the distance between"
        },
        {
          "L1-norm": "pairs Xd of diﬀerent classes:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Unweighted average recall (UAR%) of the out-of-domain training (OODT),",
      "data": [
        {
          "Table 1. Unweighted average recall (UAR%) of the out-of-domain training (OODT),": "in-domain training (IDT), and the best results obtained among the diﬀerent number"
        },
        {
          "Table 1. Unweighted average recall (UAR%) of the out-of-domain training (OODT),": "of"
        },
        {
          "Table 1. Unweighted average recall (UAR%) of the out-of-domain training (OODT),": "modiﬁed loss."
        },
        {
          "Table 1. Unweighted average recall (UAR%) of the out-of-domain training (OODT),": ""
        },
        {
          "Table 1. Unweighted average recall (UAR%) of the out-of-domain training (OODT),": ""
        },
        {
          "Table 1. Unweighted average recall (UAR%) of the out-of-domain training (OODT),": ""
        },
        {
          "Table 1. Unweighted average recall (UAR%) of the out-of-domain training (OODT),": ""
        },
        {
          "Table 1. Unweighted average recall (UAR%) of the out-of-domain training (OODT),": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "0.44",
          "Feng et al.": ""
        },
        {
          "6": "",
          "Feng et al.": ""
        },
        {
          "6": "0.42",
          "Feng et al.": ""
        },
        {
          "6": "",
          "Feng et al.": ""
        },
        {
          "6": "0.40",
          "Feng et al.": ""
        },
        {
          "6": "",
          "Feng et al.": ""
        },
        {
          "6": "0.38",
          "Feng et al.": ""
        },
        {
          "6": "",
          "Feng et al.": ""
        },
        {
          "6": "UAR\n0.36",
          "Feng et al.": ""
        },
        {
          "6": "",
          "Feng et al.": ""
        },
        {
          "6": "0.34",
          "Feng et al.": ""
        },
        {
          "6": "0.32",
          "Feng et al.": ""
        },
        {
          "6": "0.30",
          "Feng et al.": ""
        },
        {
          "6": "",
          "Feng et al.": ""
        },
        {
          "6": "0.28",
          "Feng et al.": ""
        },
        {
          "6": "",
          "Feng et al.": ""
        },
        {
          "6": "",
          "Feng et al.": "8\n10\n12\n14"
        },
        {
          "6": "",
          "Feng et al.": "Number of Speakers Adopted"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "(a) No frozen layers.\n(b) First layer is frozen.\n(c) First two layers are frozen."
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "Fig. 2. Unweighted average\nrecall\n(UAR)\nfor\nfreeze diﬀerent number of\nlayers and"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "diﬀerent number of\nspeakers adopted in ﬁne-tuning process with / without distance"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "loss."
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "number of data samples used for training. This can be potentially explained by"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "the fact that this number of speakers might not be enough to capture the distri-"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "bution of the target dataset. Performance is degraded when including a smaller"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "number of speakers in the target data. Also,\nif the data distribution includes a"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "lot of variability, Siamese NN with modiﬁed loss will be greatly restricted, since"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "pairwise diﬀerences are less likely to express the class information."
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "There has not been a lot of\nresearch relevant\nto few-shot\nemotion recog-"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "nition with speech signals, where data from only a few number of\nspeakers\nis"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "included in the target data. The most comparable results are from Gideon et"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "al.,\nin which the proposed approach with progressive neural networks achieved a"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "limited improvement (around 3%) compared to traditional ﬁne-tuning ( [5]). Our"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "proposed distance loss is able to bring a relative larger improvement at about 7%"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "compared to ﬁne-tuning, without signiﬁcant increase in the computational cost."
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "Even though Siamese NN ﬁne-tuning with modiﬁed loss still\nfails to outperform"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "the in-domain training,\nit shows great potential. We will attempt to modify the"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "proposed methodology and explore whether it can reach in-domain performance"
        },
        {
          "Number of Speakers Adopted\nNumber of Speakers Adopted\nNumber of Speakers Adopted": "as part of our future work."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n7": "process of knowledge transfer. We will further perform the multi-source transfer"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n7": "learning by using pairwise information to select proper\nsource data from each"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n7": "dataset. Finally, we plan to understand the distance between diﬀerent emotions"
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n7": "with the help of the pairing information."
        },
        {
          "Siamese Network with Modiﬁed Distance Loss in Transfer Learning\n7": "References"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "1. Badshah, A.M., Ahmad, J., Rahim, N., Baik, S.W.: Speech emotion recognition"
        },
        {
          "References": "from spectrograms with deep convolutional neural network. In: 2017 international"
        },
        {
          "References": "conference on platform technology and service (PlatCon). pp. 1–5. IEEE (2017)"
        },
        {
          "References": "2. Bromley, J., Guyon, I., LeCun, Y., S¨ackinger, E., Shah, R.: Signature veriﬁcation"
        },
        {
          "References": "using a” siamese” time delay neural network.\nIn: Advances in neural\ninformation"
        },
        {
          "References": "processing systems. pp. 737–744 (1994)"
        },
        {
          "References": "3. Cao, H., Cooper, D.G., Keutmann, M.K., Gur, R.C., Nenkova, A., Verma, R.:"
        },
        {
          "References": "Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions"
        },
        {
          "References": "on aﬀective computing 5(4), 377–390 (2014)"
        },
        {
          "References": "4. Eyben, F., W¨ollmer, M., Schuller, B.: Opensmile:\nthe munich versatile and fast"
        },
        {
          "References": "open-source audio feature extractor. In: Proceedings of the 18th ACM international"
        },
        {
          "References": "conference on Multimedia. pp. 1459–1462. ACM (2010)"
        },
        {
          "References": "5. Gideon, J., Khorram, S., Aldeneh, Z., Dimitriadis, D., Provost, E.M.: Progres-"
        },
        {
          "References": "sive neural networks\nfor\ntransfer\nlearning in emotion recognition. arXiv preprint"
        },
        {
          "References": "arXiv:1706.03256 (2017)"
        },
        {
          "References": "6. Huang, J., Li, Y., Tao, J., Lian, Z., et al.: Speech emotion recognition from variable-"
        },
        {
          "References": "length inputs with triplet loss function. In: Interspeech. pp. 3673–3677 (2018)"
        },
        {
          "References": "7. Koch, G., Zemel, R., Salakhutdinov, R.: Siamese neural networks\nfor one-shot"
        },
        {
          "References": "image recognition. In: ICML deep learning workshop. vol. 2 (2015)"
        },
        {
          "References": "8. Lian, Z., Li, Y., Tao, J., Huang, J.: Speech emotion recognition via contrastive loss"
        },
        {
          "References": "under siamese networks. In: Proceedings of the Joint Workshop of the 4th Work-"
        },
        {
          "References": "shop on Aﬀective Social Multimedia Computing and ﬁrst Multi-Modal Aﬀective"
        },
        {
          "References": "Computing of Large-Scale Multimedia Data. pp. 21–26. ACM (2018)"
        },
        {
          "References": "9. Livingstone,\nS.R., Peck, K., Russo, F.A.: Ravdess: The\nryerson\naudio-visual"
        },
        {
          "References": "database of emotional\nspeech and song.\nIn: 22nd Annual Meeting of\nthe Cana-"
        },
        {
          "References": "dian Society for Brain, Behaviour and Cognitive Science (CSBBCS). pp. 1459–1462"
        },
        {
          "References": "(2012)"
        },
        {
          "References": "10. Martin, O., Kotsia,\nI., Macq, B., Pitas,\nI.: The\nenterface’05 audio-visual\nemo-"
        },
        {
          "References": "tion database. In: 22nd International Conference on Data Engineering Workshops"
        },
        {
          "References": "(ICDEW’06). pp. 8–8. IEEE (2006)"
        },
        {
          "References": "11. Picard, R.W.: Aﬀective computing. MIT press (2000)"
        },
        {
          "References": "12. Sabri, M., Kurita, T.: Facial\nexpression intensity estimation using siamese and"
        },
        {
          "References": "triplet networks. Neurocomputing 313, 143–154 (2018)"
        },
        {
          "References": "13. Schuller, B., Steidl, S., Batliner, A.: The interspeech 2009 emotion challenge.\nIn:"
        },
        {
          "References": "Tenth Annual Conference of the International Speech Communication Association"
        },
        {
          "References": "(2009)"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition from spectrograms with convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 international conference on platform technology and service (PlatCon)"
    },
    {
      "citation_id": "2",
      "title": "Signature verification using a\" siamese\" time delay neural network",
      "authors": [
        "J Bromley",
        "I Guyon",
        "Y Lecun",
        "E Säckinger",
        "R Shah"
      ],
      "year": "1994",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition",
      "arxiv": "arXiv:1706.03256"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition from variablelength inputs with triplet loss function",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian"
      ],
      "year": "2018",
      "venue": "Speech emotion recognition from variablelength inputs with triplet loss function"
    },
    {
      "citation_id": "7",
      "title": "Siamese neural networks for one-shot image recognition",
      "authors": [
        "G Koch",
        "R Zemel",
        "R Salakhutdinov"
      ],
      "year": "2015",
      "venue": "ICML deep learning workshop"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition via contrastive loss under siamese networks",
      "authors": [
        "Z Lian",
        "Y Li",
        "J Tao",
        "J Huang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "9",
      "title": "Ravdess: The ryerson audio-visual database of emotional speech and song",
      "authors": [
        "S Livingstone",
        "K Peck",
        "F Russo"
      ],
      "year": "2012",
      "venue": "22nd Annual Meeting of the Canadian Society for Brain, Behaviour and Cognitive Science (CSBBCS)"
    },
    {
      "citation_id": "10",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "11",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "12",
      "title": "Facial expression intensity estimation using siamese and triplet networks",
      "authors": [
        "M Sabri",
        "T Kurita"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    }
  ]
}