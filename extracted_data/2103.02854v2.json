{
  "paper_id": "2103.02854v2",
  "title": "Morphset: Augmenting Categorical Emotion Datasets With Dimensional Affect Labels Using Face Morphing",
  "published": "2021-03-04T06:33:06Z",
  "authors": [
    "Vassilios Vonikakis",
    "Dexter Neo",
    "Stefan Winkler"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition and understanding is a vital component in human-machine interaction. Dimensional models of affect such as those using valence and arousal have advantages over traditional categorical ones due to the complexity of emotional states in humans. However, dimensional emotion annotations are difficult and expensive to collect, therefore they are not as prevalent in the affective computing community. To address these issues, we propose a method to generate synthetic images from existing categorical emotion datasets using face morphing as well as dimensional labels in the circumplex space with full control over the resulting sample distribution, while achieving augmentation factors of at least 20x or more.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Classification of basic prototypical high-intensity facial expressions is an extensively researched topic. Inspired initially by the seminal work of Ekman  [1] , it has made significant strides in recent years  [2, 3] . However, such approaches have limited applicability in real life, where people rarely exhibit high-intensity prototypical expressions; low-key, nonprototypical expressions are much more common in everyday situations. Consequently, researchers have started to explore alternative approaches, such as intensity of facial action units  [4, 5] , compound expressions  [6] , or dimensional models of facial affect  [7] [8] [9] . Yet these alternatives have not received much attention in the computer vision community compared to categorical models.\n\nOne major problem that impedes the widespread use of dimensional models is the limited availability of datasets. This stems from the difficulty of collecting large sets of images across many subjects and expressions. It is even more difficult to acquire reliable emotion annotations for supervised learning. Continuous dimensional emotion labels such as Valence and Arousal are difficult for laymen users to assess and assign, Part of this work was conducted while V. Vonikakis and S. Winkler were with the Advanced Digital Sciences Center (ADSC), University of Illinois at Urbana-Champaign, Singapore. The contribution from V. Vonikakis was made by him prior to joining AWS. Contact: winkler@comp.nus.edu.sg and hiring experienced annotators to label a large corpus of images is prohibitively expensive and time consuming. Since even experienced annotators may disagree on these labels, multiple annotations per image are required, which further increases the cost and complexity of the task. Yet there are no guarantees that the full range of possible expressions and intensities will be covered, resulting in imbalanced datasets. Consequently, large, balanced emotion datasets, with highquality annotations, covering a wide range of expression variations and expression intensities of many different subjects, are in short supply.\n\nOur approach attempts to address this need. We propose a fast and cost-effective augmentation framework to create balanced, annotated image datasets, appropriate for training Facial Expression Analysis (FEA) systems for dimensional affect. The framework uses high-quality facial morphings to transform typical categorical datasets (usually 7 expressions per subject) into dimensional ones, with an augmentation factor of at least 20x or more. More importantly, it produces multiple annotated expressions per subject, balanced across the Valence-Arousal (VA) space. The resulting synthesized facial images look realistic and visually convincing. We also demonstrate that they can be used very effectively for training and testing real-world FEA systems.\n\nAlthough morphing, as a means of deriving an extended set of facial expressions, is a widely used tool in psychology  [10, 11] , it has found limited adoption in the computer vision community. Traditional work on expression synthesis usually incorporates manipulation of the facial geometry and texture mapping in images or videos  [12, 13] . Other approaches include the use of 3D meshes adopted from RGB-D space  [14]  or the adjustment of Action Units from the Facial Action Coding System (FACS)  [15] . More recently, researchers have employed Generative Adversarial Networks (GANs) for this purpose  [16] . Various conditional GAN variations have been used to generate novel expressions while preserving identity and other facial details  [17] [18] [19] . While most of them also take the categorical approach, a few models have been proposed based on action units  [20]  and continuous emotion dimensions  [21] . These GANs generally require a large dataset to start with, with no guarantees that generated faces will not exhibit unnatural artifacts, and the difficulty of creating proper annotations remains. In our approach on the other hand, we have full control over the pipeline, resulting in deterministic outputs both in terms of synthetic images and dimensional emotion labels.\n\nOur main contributions can be summarized as follows:\n\n• A new dataset augmentation framework that can transform a typical categorical facial expression dataset into a balanced augmented one.\n\n• The framework can generate hundreds of different expressions per subject with full user control over their distribution.\n\n• The augmented dataset comes with automatically generated, highly consistent Valence/Arousal annotations of continuous dimensional affect.\n\nThe code for the proposed augmentation framework is available at https://github.com/dexterdley/MorphSet.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset Generation",
      "text": "We assume a 2-dimensional polar affective space, similar to the Valence-Arousal (VA) space of the circumplex model  [7] , with Neutral located at the center. The typical 7 facial expressions, which are usually included in categorical emotion datasets, Neutral (Ne), Happy (Ha), Surprised (Su), Afraid (Af), Angry (An), Disgusted (Di) and Sad (Sa), can be mapped to points with specific coordinates in the polar AV space. Apart from these 7 points however, there is a lot of empty space on the remaining AV plane. These missing facial expressions comprise: (a) different expression variations e.g. Delighted, Excited, Upset etc., located at different angles in the AV polar space, and (b) different intensity variations of the expressions, e.g. slightly happy, moderately happy, extremely happy etc., spanning the area from the center (Neutral) outwards toward the periphery of the AV space. The basic premise of MorphSet is that many of these expression variations can be synthesized by high-quality morphings between images of categorical expressions.\n\nLet F E i denote the face image of subject i with facial expression E. For categorical datasets, usually E ∈ {Ne,Ha,Su,Af,An,Di,Sa}. Let θ E denote the specific angle of each expression in the polar AV space, estimated from emotion studies  [7, 23] . Let I E i ∈ [0, 1] denote the intensity of expression E of subject i. Zero expression intensity I E = 0 coincides with Neutral (by definition I Ne = 0), while\n\n, r be a morphing function, based on p facial landmarks, that returns a new face image, which is the result of morphing F source i towards F target i with a ratio r ∈ [0, 1]; when r = 0 the morphed image is identical to F source i , and when r = 1 it is identical to F target i . Any contemporary morphing approach can be used for this, such as Delaunay triangulation followed by local warping of 68 facial landmarks from Dlib  [24]  face recognition system.\n\nOur augmentation framework is based on 2 types of morphings. In order to synthesize new expression variations, Apex to Apex morphing (1) is used, between the given apex expressions of the categorical dataset:\n\nwhere A, A 1 and A 2 are apex expressions from the parent dataset. In order to synthesize new intensity variations, Neutral to Apex morphing (  2 ) is used, between the NE image and a given (or interpolated) apex image:\n\nFig.  1  shows an example of these 2 types of morphing. Once new interpolated apex expressions are generated by equation (  1 ), 'neutral to interpolated apex' morphings can further be generated by applying equation (2) on them.\n\nFor every given or generated face image F E i , with I E i and θ E , the Valence and Arousal annotations can be computed as\n\nand A E i = I E i sin(θ E ). Fig.  2  illustrates the proposed augmentation framework for a typical categorical dataset with the 7 prototypical expressions. We start from 10 • , the approximate location of Happy in VA space, and proceed in increments of 15 • steps in order to span the whole range up to 205 • , where Sad is approximately located. The proposed template is bounded within [10 • , 205 • ] only because negative Arousal expressions (Sleepy, Tired, Bored, Calm, etc.) are absent from the typical categorical emotion datasets. We use an angle increment of 15 • and an intensity increment of 0.1, because they strike a good balance between expression granularity, augmentation factor and symmetry between the positions of the given prototypical expressions in the AV space.\n\nBased on the above selected granularity, for a typical categorical dataset of 7 facial images per subject, the proposed augmentation framework can generate 134 new images, reaching a total of 134+7=141 facial images per subject. This translates to an augmentation factor of 20x, or 40x with simple image mirroring. Doubling the angular and radial granularity to 7.5 • and 0.05, respectively, results in an augmentation factor of 80x, or 160x with image mirroring.\n\nWe build an example augmented dimensional dataset from a combination of 3 categorical datasets, which have been extensively used in psychology: the Radboud  [22] , Karolinska  [25] , and Warsaw  [26]    because they have superior image quality, their facial expressions were guided by FACS experts, and they are accompanied by validation studies that provide the perceived intensity of expression I E i . Using the proposed MorphSet augmentation framework, all faces are aligned with respect to the subjects' eyes, and synthetic images are generated for each subject. The resulting augmented dataset comprises more than 55,000 images, which -with finer granularity and mirroring -can reach over 450,000. More importantly though, each image comes with continuous Valence, Arousal, and Intensity annotations, which can be used to train dimensional FEA systems.\n\nAs the proposed augmented dataset contains only frontal images, an FEA system trained on it would not be invariant to different head poses. There are various solutions to this, which are discussed in detail in  [27] . In fact, we have successfully used such an approach to build a robust in-the-wild facial expression analysis system  [28] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Comparison & Discussion",
      "text": "In this section, we evaluate the benefits of MorphSet as a dataset for facial emotion recognition. We also describe the AffectNet  [8]  and Aff-Wild  [9]  databases commonly used for emotion recognition.\n\nAffectNet  [8]  is a dataset comprising of roughly 450,000 images of in-the-wild-facial expressions collected from the Internet using affect keywords. These images are manually annotated with both categorical and dimensional labels. Despite its size, the training samples of AffectNet are noisy and highly imbalanced, causing many learning algorithms to perform poorly on the minority classes. Furthermore, human annotator agreement is just over 60%, suggesting that the dataset suffers from noisy/incorrect annotations.\n\nAff-Wild  [9]  is an in-the-wild video dataset, consisting of 298 Youtube videos displaying reactions of 200 subjects. The annotations for valence and arousal were collected continuously via joystick. For direct comparison with AffectNet and MorphSet, we extract individual frames according to the affect annotations from the time-stamped video sequences.\n\nTable  1  compares these two databases with MorphSet, which is unique in that it provides a balanced distribution of facial expressions for each subject.\n\nTo compare baseline FEA results, we train a ResNet-18 model  [29]  on each dataset. We add two neurons after the  Although the results are not directly comparable because of the difference in test sets, they provide useful insights. The performance for MorphSet is significantly better than Affect-Net and Aff-Wild, likely due to the frontal and highly controlled conditions of the images as well as higher consistency of the VA annotations.\n\nThe fact that wild datasets are often noisier and less controlled in terms of facial expressions than MorphSet is further illustrated by Fig.  3 , where images with specific VA labels are randomly sampled from each of the three datasets. AffectNet and Aff-Wild samples show significant fluctuations and outliers in facial expressions (indicated with circles and squares in Fig.  3 ), whereas the facial expressions from MorphSet are much more consistent across different subjects.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusions",
      "text": "We presented MorphSet, a dataset augmentation framework that can transform a typical categorical dataset of facial expressions into a balanced augmented one using image morphing. We use the framework to generate hundreds of different expressions per subject with full user control over their distribution. The augmented dataset comes with automatically generated, highly consistent dimensional annotations suitable for supervised learning of continuous affect.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows an example of these 2 types of morphing.",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates the proposed augmentation framework",
      "page": 2
    },
    {
      "caption": "Figure 1: Examples of the 2 types of face morphings utilized in the proposed augmentation framework, using images from the",
      "page": 3
    },
    {
      "caption": "Figure 2: Dataset augmentation framework based on face mor-",
      "page": 3
    },
    {
      "caption": "Figure 3: Randomly sampled training images around the same valence and arousal annotations from AffectNet, Aff-Wild, and",
      "page": 4
    },
    {
      "caption": "Figure 3: , where images with speciﬁc VA labels are",
      "page": 4
    },
    {
      "caption": "Figure 3: ), whereas the facial expressions from MorphSet are",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: compares these two databases with MorphSet,",
      "page": 3
    },
    {
      "caption": "Table 1: Comparison to other dimensional datasets.",
      "page": 4
    },
    {
      "caption": "Table 2: shows the Root Mean Square error (RMSE) and",
      "page": 4
    },
    {
      "caption": "Table 2: ResNet-18 baseline results for each dataset.",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "3",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Context-sensitive dynamic ordinal regression for intensity estimation of facial action units",
      "authors": [
        "O Rudovic",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Discriminant functional learning of color features for the recognition of facial action units and their intensities",
      "authors": [
        "F Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proc. National Academy of Sciences"
    },
    {
      "citation_id": "8",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "9",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Aff-Wild: Valence and arousal 'in-thewild' challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "11",
      "title": "Assessment of perception of morphed facial expressions using the emotion recognition task: Normative data from healthy participants aged 8-75",
      "authors": [
        "R Kessels",
        "B Montagne",
        "A Hendriks",
        "D Perrett",
        "E Haan"
      ],
      "year": "2014",
      "venue": "Journal of Neuropsychology"
    },
    {
      "citation_id": "12",
      "title": "Identification of emotional facial expressions: Effects of expression, intensity, and sex on eye gaze",
      "authors": [
        "L Wells",
        "S Gillespie",
        "P Rotshtein"
      ],
      "year": "2016",
      "venue": "PLOS One"
    },
    {
      "citation_id": "13",
      "title": "Mapping and manipulating facial expression",
      "authors": [
        "B.-J Theobald",
        "I Matthews",
        "M Mangini",
        "J Spies",
        "T Brick",
        "J Cohn",
        "S Boker"
      ],
      "year": "2009",
      "venue": "Language and Speech"
    },
    {
      "citation_id": "14",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Real-time expression transfer for facial reenactment",
      "authors": [
        "J Thies",
        "M Zollhöfer",
        "M Nießner",
        "L Valgaerts",
        "M Stamminger",
        "C Theobalt"
      ],
      "year": "2015",
      "venue": "ACM Trans. Graphics"
    },
    {
      "citation_id": "16",
      "title": "Realistic facial animation generation based on facial expression mapping",
      "authors": [
        "H Yu",
        "O Garrod",
        "R Jack",
        "P Schyns"
      ],
      "year": "2014",
      "venue": "Proc. SPIE 5th International Conference on Graphic and Image Processing (ICGIP)"
    },
    {
      "citation_id": "17",
      "title": "Adversarial training in affective computing and sentiment analysis: Recent advances and perspectives",
      "authors": [
        "J Han",
        "Z Zhang",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "18",
      "title": "ExprGAN: Facial expression editing with controllable expression intensity",
      "authors": [
        "H Ding",
        "K Sricharan",
        "R Chellappa"
      ],
      "year": "2018",
      "venue": "Proc. 32nd AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Facial expression synthesis by U-Net conditional generative adversarial networks",
      "authors": [
        "X Wang",
        "W Li",
        "G Mu",
        "D Huang",
        "Y Wang"
      ],
      "year": "2018",
      "venue": "Proc. ACM International Conference on Multimedia Retrieval (ICMR)"
    },
    {
      "citation_id": "20",
      "title": "SliderGAN: Synthesizing expressive face images by sliding 3D blendshape parameters",
      "authors": [
        "E Ververas",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "GANimation: Anatomically-aware facial animation from a single image",
      "authors": [
        "A Pumarola",
        "A Agudo",
        "A Martinez",
        "A Sanfeliu",
        "F Moreno-Noguer"
      ],
      "year": "2018",
      "venue": "Proc. European Conf. Computer Vision (ECCV)"
    },
    {
      "citation_id": "22",
      "title": "The many moods of emotion",
      "authors": [
        "V Vielzeuf",
        "C Kervadec",
        "S Pateux",
        "F Jurie"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "23",
      "title": "Presentation and validation of the Radboud faces database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bijlstra",
        "D Wigboldus",
        "S Hawk",
        "A Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "24",
      "title": "Seeing stars of valence and arousal in blog posts",
      "authors": [
        "G Paltoglou",
        "M Thelwall"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "26",
      "title": "The Karolinska directed emotional faces: A validation study",
      "authors": [
        "E Goeleven",
        "R Raedt",
        "L Leyman",
        "B Verschuere"
      ],
      "year": "2008",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "27",
      "title": "Warsaw set of emotional facial expression pictures: A validation study of facial display photographs",
      "authors": [
        "M Olszanowski",
        "G Pochwatko",
        "K Kuklinski",
        "M Scibor-Rylski",
        "P Lewinski",
        "R Ohme"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "28",
      "title": "Identity-invariant facial landmark frontalization for facial expression analysis",
      "authors": [
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Int'l Conf. Image Processing (ICIP)"
    },
    {
      "citation_id": "29",
      "title": "Efficient facial expression analysis for dimensional affect recognition using geometric features",
      "authors": [
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2021",
      "venue": "Efficient facial expression analysis for dimensional affect recognition using geometric features"
    },
    {
      "citation_id": "30",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}