{
  "paper_id": "2411.17674v2",
  "title": "Push The Limit Of Multi-Modal Emotion Recognition By Prompting Llms With Receptive-Field-Aware Attention Weighting",
  "published": "2024-11-26T18:35:24Z",
  "authors": [
    "Han Zhang",
    "Yu Lu",
    "Liyun Zhang",
    "Dian Ding",
    "Dinghua Zhao",
    "Yi-Chao Chen",
    "Ye Wu",
    "Guangtao Xue"
  ],
  "keywords": [
    "Emotion Recognition in Conversation",
    "Large Language Models",
    "Multimodal Fusion",
    "Prompting"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding the emotions in a dialogue usually requires external knowledge to accurately understand the contents. As the LLMs become more and more powerful, we do not want to settle on the limited ability of the pre-trained language model. However, the LLMs either can only process text modality or are too expensive to process the multimedia information. We aim to utilize both the power of LLMs and the supplementary features from the multimedia modalities. In this paper, we present a framework, Lantern, that can improve the performance of a certain vanilla model by prompting large language models with receptive-field-aware attention weighting. This framework trained a multi-task vanilla model to produce probabilities of emotion classes and dimension scores. These predictions are fed into the LLMs as references to adjust the predicted probabilities of each emotion class with its external knowledge and contextual understanding. We slice the dialogue into different receptive fields, and each sample is included in exactly t receptive fields. Finally, the predictions of LLMs are merged with a receptive-field-aware attention-driven weighting module. In the experiments, vanilla models CORECT and SDT are deployed in Lantern with GPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way settings demonstrated that the Lantern can significantly improve the performance of current vanilla models by up to 1.23% and 1.80%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding human emotions is always considered one of the necessary qualities for a powerful AI. Nevertheless, emotion recognition is a tricky task. It requires the models to understand the contents of dialogues, analyze the tones in the acoustic signals, and capture the expressions in the videos. Recent studies  [14, 22]  have made remarkable progress. These studies focus on how we extract information from each modality and fuse them to better understand the emotion efficiently. However, some dialogues need certain knowledge about modern ethics to infer the correct emotions. One example of this demand is sarcasm, which often has an emotion against the literal meanings. Most current studies only train models from scratch with specific emotion recognition corpora. Although these corpora have thousands of samples, the models are still hard to learn and understand the underlying ethical knowledge. The second approach leverages an LLM to process dialogue transcriptions. Our framework uses a vanilla model to process multimedia modalities and generate intermediate supportive information for LLM refinement.\n\nIntegrate a pre-trained natural language model  [17, 33]  to pre-process the text modality with its knowledge has been widely used to increase the model's awareness of external knowledge. As shown in Figure  1a , these frozen models are pre-trained on large corpora that are not specific for emotion recognition. They could have learned more comprehensive knowledge to understand the dialogues accurately. Previous studies built a vanilla model leveraging the BERT  [7]  to provide external knowledge for the backbone model for understanding the context. However, the shortage is that given the limited processability and context window of the BERT, only very limited sentences can be sent to BERT simultaneously. Lacking the information of the dialogues could cause the BERT to extract misdirected features.\n\nLarge language models have become a new trend in the study of natural language processing. These models with extremely numerous parameters are trained on a great deal of text corpora. They mastered more profound knowledge about the knowledge of our world and society and provided a larger context window compared to the BERT. Regarding the text inputs, hardly any vanilla models can outperform the LLMs. Nonetheless, most of the LLMs, like GPT-3.5, can only process text modality. Even if the latest GPT-4o could process multimodal inputs, asking LLMs to process these multimedia modalities is costly. For example, a typical video in IEMOCAP  [5]  is 720×480, which would cost 425 tokens for one frame when using GPT-4o. A 30-second video with 24 frames per second is over 300k tokens, while the context window size of GPT-4o is only 128k. It is still unacceptable even if we compress it to 8 frames per second with a total of 100k tokens. In addition, from the ablation studies from  [22]  and  [14] , we find that the improvement brought by the video modality is not significant enough for us to bear the cost. Text is the most important modality in emotion recognition.\n\nIn summary, while BERT is efficient, it is limited in context window and knowledge. Contrarily, LLMs are powerful but prohibitively expensive for processing multimedia information. We have been exploring a solution that can efficiently incorporate external knowledge as powerful as LLMs while still supporting multimedia modalities efficiently.   2 : Our framework Lantern has three stages: first, a vanilla model preprocesses all modalities to produce a preliminary prediction of the probabilities and the dimension scores for each sample. Second, prompts with the preliminary predictions and the transcriptions are fed into a frozen LLM for further adjustment. Each sample will be included in t prompts. Finally, a receptive-field-aware attention algorithm is implemented to assign weights for the t + 1 predictions to form the final prediction.\n\nIn this paper, we present a new framework, Lantern, prompting large language models with receptive-field-aware attention weighting. As shown in Figure  1c , Lantern uses a vanilla model to pre-process multimedia modalities and generate initial predictions. These predictions are then adjusted using a frozen LLM with its external knowledge and contextual understanding.\n\nLantern poses several challenges. First, we need to deliver information from multimedia modalities to LLMs efficiently. Besides the classification metric used by most of the studies, dimension scores  [24]  are another option to evaluate emotions. According to the results from  [31]  and  [2] , acoustic signals play a more important role in predicting the dimension scores than linguistic information. Therefore, we deliver emotion classification metrics and dimension scores as supplementary information correlated with multimedia modalities for the LLMs' reference. Furthermore, multitasking vanilla models are applied for both the categorical metrics and dimensional scores to enable a lightweight pipeline.\n\nAs powerful as LLMs are, their context window is still limited. In practice, when the length of a dialogue is out of its capability, we must split the contents. We take the sliding window strategy as the splitting policy. Each sample will be included in exactly t windows to reduce the randomness of the LLM responses. The samples are contained in different windows to provide the LLM with different receptive fields to mine the local and overall features of the dialogue. This feature enables us to design an attention-based algorithm to assign weights for each prediction considering the length and reliability of receptive fields.\n\nLantern deploys a multi-task vanilla model based on CORECT  [22]  and SDT  [19] , while using GPT-4  [1]  and Llama-3.1-405B  [8]  as the LLM agent. A single NVIDIA RTX 3090 GPU is enough to train a vanilla multi-modal model and a lightweight merge model. We conducted experiments on the most commonly used dataset IEMOCAP  [5] . The results confirm that Lantern significantly improves the vanilla model recognition accuracy by 1.23% to 1.80% in 6-way and 4-way settings when using CORECT and 0.79% when using SDT in the 6-way setting.\n\nOverall, our contributions are as follows:\n\n-We present an LLM-assisted emotion recognition framework that utilizes a multitask vanilla model to extract emotion classification metrics and dimension scores from multi-modal samples, while efficiently incorporating external knowledge through the use of an LLM. -We utilize a sliding window strategy to segment long dialogues, provide multiple receptive fields for the samples, and address the limitation of the LLM context window. -We employ a receptive-field-aware attention-driven weighting mechanism that assigns appropriate weights to each prediction. Compared with the prediction results of vanilla, the accuracy of sentiment recognition by fusing vanilla and LLM is significantly improved.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition. The emotion recognition task requires algorithms to analyze the speakers' emotions from the transcriptions, acoustic signals, and videos. There are two ways to measure the emotion of a speaker: discrete emotions, such as happy, sad, and angry, and emotion dimensions, which describe emotions through arousal, valence, and dominance. IEMOCAP  [5]  provided the transcriptions, acoustic signals, and videos as the input, and both the discrete emotion and dimension labels. Vanilla multi-modal models. Different modalities can supplement each other by providing more information or confirming shared information to help models better understand the world  [4, 20] . Convolutional Neural Networks (CNNs)  [18]  has been extensively applied in emotion recognition tasks. The transformer structure  [29]  has significantly improved the performance of fusing multiple modalities. It is often used to handle the visual and language features  [34] .  [26]  and  [6]  have applied the transformer to this task and achieved great success. Many researchers choose to use a Graph Neural Network (  [10] ,  [5] ) to capture the relationship in a conversation. The GNN has different options for its base structure. The most commonly used structures are convolution  [16] , recurrence  [23] , and attention  [30] .  [22] ,  [14] , and  [13]  deployed GNN to model the conversation to achieve significant results.  [13] ,  [9] ,  [21] , and  [11]  use the Gated Recurrent Unit (GRU) as an alternative method.\n\nThough the studies above focused on the classification task,  [31] ,  [27] , and  [2]  explored the methods to predict the dimension scores of emotions. Considering the importance of acoustic signals in the task of dimension scores, they applied the wav2vec 2.0  [3]  and HuBERT  [12]  to process the speech. Prompt LLMs for downstream tasks. The foundation of LLMs is the scaling law  [15] , which discovered that the models can achieve remarkable performance improvement by training with an enormously large dataset. For example, Llama 3  [28]  is trained on 15T tokens. With a large amount of training data, the LLMs are welllearned to understand the context of conversations with their internal knowledge. Therefore, it is intuitive that many works requiring external knowledge have included LLMs as part of their pipelines. In Visual Question Answering (VQA) tasks,  [25]  used vanilla local models to generate heuristics-enhanced prompts to provide information on images for LLMs.  [35]  uses GPT to generate video description and context knowledge for a fine-tuned Llama model for further prediction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "Our framework consists of three stages depicted in Figure  2 . In the first stage, we use a vanilla model to process the transcriptions and multi-modal inputs to produce preliminary predictions as the inputs of the LLMs. In the second stage, we generate the prompts for the LLMs to ask them to make adjustments according to our predictions in the first stage and their knowledge. In the end, we use a receptive-field-aware attention weighting algorithm to calculate the weighted sum of each prediction to form the final results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Adaption For Vanilla Models",
      "text": "The vanilla models should produce the classification predictions and dimension scores as sources of the prompts. However, current studies usually only focus on one of the tasks. These models shared the idea of using a backbone network to Backbone Prediction Prediction Head",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Head Head",
      "text": "Valence/ Arousal/ Dominance (a) Single-task (b) Multi-task Fig.  3 : Methods to predict metrics: Figure  3a  described a single-task model, where a dedicated backbone is used to extract specific to a task. Figure  3b  demonstrated a multi-task pattern, where backbone extract features suitable for both metrics and the predictions are based on the same feature.\n\nextract the features as a high-dimension vector as seen in the Figure  3a . The result type depends on the prediction head and the labels. The classification and dimension scores are both descriptions of the emotion and they are correlative variables (demonstrated in next section). It is trivial to extract the features twice with two different models. Therefore, as shown in Figure  3b , for a given vanilla model, an additional linear layer is added after the backbone to allow it to predict both tasks from the same emotion feature. This multi-task training pattern can reduce the burden at this step and provide a more comprehensive view during the training of the backbone network. The backbone of the vanilla models can be any emotion recognition model. In this work, we adopted the CORECT  [22]  as our backbone.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Prompting Llms",
      "text": "A prompt for the LLMs has two parts, the header and the samples. Each sample includes the transcription, the predicted dimension scores, and the preliminary prediction of the classification probabilities. The header describes the task, inputs, and output format. Each sample is repeated t times to reduce the randomness in the output of LLMs.\n\nThe form of the task can have two options, asking the LLMs to directly produce the most likely class for each sample or the probabilities for each class. The procedure first option implied in the first option is asking the LLMs to produce the probabilities and output the class of highest probabilities. It includes one more step in the reasoning chain of the LLMs, which is unnecessary and both depth reasoning and mathematical thoughts can lead to worse performance. In addition, we can only use the voting algorithm to merge the t predictions, which could cause coarser granularity and higher chances of tying. Due to these reasons, we ask the LLMs to adjust the preliminary probabilities generated by vanilla models and add the probabilities of each emotion when combining the t predictions.\n\nTo help the LLMs have a more accurate understanding of the relationship between the dimension scores and the emotion classes, we summarized the statistics of the dimension scores of each emotion (detailed in next section), which is appended in the header for the LLMs' reference. Inspired by  [32] , we selected a series of examples from the training set and manually constructed answers for them. These examples are attached for the LLMs to better understand the description. These answers guide the LLMs to not only examine the meaning of the transcriptions, the values of class probabilities, and the dimension scores but also compare the changes between samples.\n\nAnother step we adopted to reduce the randomness of LLMs' response is filtering the low-quality ones with an integrity check including: 1). whether all samples provided to the LLMs are predicted, 2). whether each sample's response contains all emotions and 3). whether the sums of the adjusted probabilities are 1. Any response that fails to pass the integrity check will be discarded, and the LLMs will be required to respond to these samples again.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Split Strategy",
      "text": "In practice, a dialogue could be too long to fit in the context window of the LLMs. Therefore, we had better split the dialogue into smaller windows. Another reason to split the contents is that the chances of passing the integrity check are negatively related to the number of samples passed to the LLMs. In addition, LLMs have output limits. If we feed too many samples in one prompt, they cannot assign enough thoughts to each sample and reach the limit before finishing adjusting all samples in the prompts. A simple strategy is to split the dialogue naively and repeat each window t times as in Figure  4a . However, a window may require some information from adjacent windows. We can pad dummy samples, like in Figure  4b , so that each window could overlap with each other. Due to the randomness of the prediction results of LLM, we need to fuse the multiple predictions. To build a dimensionally uniform fusion algorithm, each sample will be covered in exactly t windows and the paddings will be discarded. Nonetheless, for a sample, its predictions are all based on the same receptive field of a certain length. We hope that the predictions can include as many samples into consideration as possible. Therefore, we adopted a sliding window method to iterate the dialogue. During sliding, each prediction for the same sample is based on a different receptive field of the dialogue, which achieves feature extraction for the content of dialogues of different lengths. The Figure  4c  demonstrates an example when t = 3. We can see that the window uses Fig.  4 : Strategies to split a dialogue: Figure  4a  is the naive splitting, where each receptive field is not overlapped with each other. Figure  4b  pads some samples at the beginning and the end of each receptive field. The ×t and t× mean that repeat this receptive field for t times. Figure  4c  demonstrated the sliding window strategy when t = 3, which provides different receptive fields for each sample, mining dialogue features of different perspective views.\n\none step, moves forward one step each time, and ends until the entire window is moved out of the dialogue range. The first and last windows have the least amount of data, we performed padding using dummy samples to prevent their context is not sufficient for LLMs to understand.\n\nNo matter how we split the dialogues, we want the LLMs to have a comprehensive view of the entire dialogue. To achieve this goal, we ask the LLMs to summarize the dialogue and list some necessary ethical knowledge or common sense that might help identify the emotions before any adjustment request. This summary will be attached at every window for the LLMs to keep a global sight.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Receptive-Field-Aware Weighting",
      "text": "After passing the prompts to the LLMs, we can acquire a new prediction matrix x ∈ R n×(t+1) for each sample, where n is the number of emotion classes and the t + 1 represents t predictions from the LLMs and one from the vanilla models. When combining these prediction results to make the final decision, the most naive strategy is simply adding up the t + 1 predicted probabilities (or t when ignoring the one produced by the vanilla models). However, this would ignore the internal patterns between each receptive field. We would like to learn the weights for them instead of assigning some arbitrary constants. Another proposal is to learn an n × (t + 1) weight matrix and perform an element-wise product before summing up for each class. It ignored the length of the receptive fields for different predictions could be different. The lengths of receptive fields are different for the samples at the beginning or the end of the dialogue. In contrast, the samples in the middle would have the same length for their receptive fields. Here, we propose a receptive-field-aware attention-driven merge algorithm. l ∈ R t is the proportion of the lengths of t receptive fields with respect to the dialogue. First, the coarse importance of each prediction is calculated from the lengths of their receptive fields, denoted as l ′ ,\n\nInstead of training the entire projection matrices as parameters, we generate them from the lengths of the receptive fields. Specifically,\n\nThe rows of x are the predictions for each emotion. The Q i,j is the linear combination of the i-th row of x, which is the importance of the j-th prediction in emotion i considering all models' predictions in this emotion. The i-th row of Q describes the importance of the predictions of emotion i, which can represent the importance of emotion i.\n\nIn the calculation of K, the x T is used as the input instead of x as in Q. The x focuses on the predictions for each emotion, while the x T emphasizes the predictions of each receptive field. The i-th row of K is x T i,: W K + b Ki . We can see it as the prediction pattern of the i-th receptive field summarized from predictions for all emotions of it.\n\nV can be viewed as the first adjustment of each prediction considering the predictions of other receptive fields.\n\nThe W i,j is the relative confidence between the i-th emotion and receptive field, which considered all predictions for i-th Q i,: emotion and all predictions of j-th receptive field K j,: .\n\nThe predictions are weighted by performing element-wise product of the weights and V .\n\nx\n\n4 Experiments\n\nWe evaluated the performance of Lantern on the IEMOCAP  [5]  dataset. Since our method requires the classification label, dimension scores, and transcription, to the best of our knowledge, IEMOCAP is the only dataset that meets these requirements. We first discuss the effectiveness of using dimension scores to supplement information for classification. Then, introduce how we train our model. Finally, the detailed results of micro benchmarks and ablation studies are presented.  recall of 54%. Figure  5  displays the LDA coefficients. They suggest that valence is a strong predictor, especially for happy and excited with high positive weights and sad and angry with strong negative weights. Arousal and dominance also play significant roles, with arousal being crucial for distinguishing angry and excited, and dominance for angry and frustrated.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Dimension Scores And Emotional Classes",
      "text": "The confusion matrix is shown in Figure  6 . Sad has the highest precision while excited and neutral have the highest recalls. Happy and excited are the most confusing classes. The high recall of excited is the result of the tendency to classify both happy and excited as excited. Angry and frustrated are another pair of confusing emotions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Training Settings",
      "text": "In our experiments, we adopted the CORECT  [22]  and SDT  [19]  as the vanilla models. To make these models support multi-tasking, we added 8 lines of code to the network of CORECT, 21 lines to the network of SDT, and around 50 lines to their training procedure to save results for the LLM adjustment. In each modification, we preserved the training method of the classification task. Specifically, in CORECT, a simple linear layer is appended to predict the dimension scores accepting the same feature as the input of the classifier, while, in SDT, selfdistillation is also applied to the dimension scores by replacing the KL-diverse with MSE. The gpt-4-0125-preview and Llama-3.1-405B were the LLM deployed in our framework. Our settings completely followed previous studies. The fifth session in IEMOCAP is used as the test set and the validation and training sets are split from the remaining sessions. We train the CORECT model on the training set and select the best model based on the validation set performance. The LLM then makes predictions on both the validation and test sets. The attention-based merge algorithm is trained on the validation set predictions, and the highest-scoring model is used for final test set predictions.\n\nOur framework only introduced a prediction head and merge algorithm of the total parameter number of (t + 3) × (t + 1) + 3n + 3×feature_size, where feature size is the output size of the backbone. The backbone of CORECT has around 4.78M parameters, including 300 for the new prediction head for dimension scores. The CORECT and merge algorithm can be trained on a single NVIDIA RTX 3090 GPU, while the GPT is run on the cloud server of OpenAI.\n\nDuring the training, the loss function of the classification is cross-entropy loss, while the dimension scores use the concordance correlation coefficient (CCC) score. The final loss is a weighted sum of these two losses, where weights are arbitrary constants for different dataset settings.    1  exhibits the results on the 6-way IEMOCAP setting with multimedia modalities. Simple multi-tasking training can improve the performance of CORECT with 0.25% in accuracy and 0.26% in the F1-score. The accuracy and F1-sccore of the SDT are almost the same. This suggests that by applying multi-tasking loss signals, the model might be able to learn to understand the inputs more accurately and extract more useful features. It is no surprise that multi-tasking won't decrease performance since the dimension scores describe emotions from another perspective. However, we believe that it can increase the performance due to some emotions having remarkable patterns in the view of dimension scores. A straightforward proof is a significant improvement of excited (6.42% for CORECT and 8.77% for SDT) and sad (5.58% for CORECT and 7.10% for SDT), which is consistent with the results of the LDA analysis above. However, the side effect is also obvious. The improvements in excitement are also accompanied by a decline in the performance of happy, which also demonstrates the tendency of classifying happy as excited as the LDA analysis. Finally, both angry and frustrated are worse for CORECT (decreased by 5.17% and 4.07%), while the SDT became worse for angry (decreased by 6.43%) and performed better for frustrated (improved by 3.93%). These emotions are too similar in their dimension scores, making it difficult for the model to learn the distinguishing features. The SDT would tend to classify these samples as frustrated.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Micro Benchmarks",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Models",
      "text": "After we applied the entire Lantern framework on CORECT with GPT-4, the accuracy further increased 0.98% from the multi-task model and 1.23% from the CORECT, while the F1-scores increased 0.62% and 0.88% respectively. When we combined the SDT with GPT-4, the accuracy and F1-scores were improved by 0.48% and 0.55% from the SDT, while integrating it with Llama can improve by 0.78% and 0.71% respectively. The reason our framework's effect on the SDT is not as significant as the COREST is that the CORECT can estimate the dimension scores more accurately. When performing self-distillation to the dimension score, we simply copied the hyper-parameters of the classification due to a lack of resources to select a set of more reasonable values. Notably, with the help of our framework, the models became better at recognizing happiness. The price is the decrease in the performance of the excited However, the accuracy of angry and frustrated is still not as good as the vanilla models.   Table  4 : Results when using different merge algorithms using the same prompts as Lantern: The first row adds all t predictions. The second row trains an n × (t + 1) weight matrix and product with the predictions in element-wise. The last two rows are traditional attention  [29]  using x as Q and x T as K, V .\n\nment in the 4-way setting than the 6-way setting. The reason is that the excited is similar to the happy and the frustrated is similar to the angry from the perspective view of dimension scores. When eliminating the excited and frustrated from the dataset, the dimension score can provide a more distinguishable guide for classifications. The overall pipeline improved by 1.06% in accuracy and 1.09% in F1-score from the multi-task vanilla model, while 1.80% and 1.89% from the single-task vanilla model respectively. The improvements from the multi-task vanilla model are similar to the 6-way setting.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Ablation Studies",
      "text": "In ablation studies, we measured the performance of removing different parts of our prompt and the alternative designs mentioned previously using CORECT as the vanilla model. Table  3  presents the ablation results for different parts of the prompt. Omitting any component of the prompt leads to a decrease in performance. Particularly, removing the chain-of-thought (COT) from the example answers shows the most significant negative impact. Interestingly, completely removing the examples performs better than removing only the COT. This discrepancy may stem from the selected examples not closely aligning with the samples, which confuses the LLMs when learning surface patterns from them. In contrast, the COT helps LLMs understand the reasoning process of a sample, thereby yielding more positive effects than unrelated examples. Another notable observation is that when the COT is removed, the pass rate of the integrity check drastically increases. Even when all samples are consolidated into a single prompt, generating compliant responses becomes easier. We hypothesize this occurs because LLMs prioritize producing responses that fit the required format over reasoning for correct answers. A related observation is that attempts to use GPT-3.5 with COT resulted in few responses passing the integrity check. Its ability seems constrained such that when focused on reasoning steps, significant errors occur in both results and format.\n\nTable  4  shows the results using different merge algorithms. The first three settings that are unaware of the receptive fields show a notable decline in perfor-mance. Some are even worse than the multi-task vanilla model. This underscores the importance of receptive-field awareness in the merge algorithm.\n\nAnother phenomenon worth noting is that, when using SDT as the vanilla model, the accuracies of naively adding up the adjustment of the LLM are 73.51% for GPT-4 and 72.95% for Llama, which is 0.92% and 1.78% worse than using the receptive-field aware attention weighting while 0.43% and 0.99% worse than the multi-task model with LLM. The severely damaged performance is another evidence of the LLM being affected by the mistakes of the predicted dimension scores and the importance of our weighting method.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Discussion",
      "text": "As demonstrated in the experiment section, our framework Lantern can effectively improve the performance of the vanilla multi-modal networks. As we demonstrated, any vanilla model, like CORECT  [22]  or SDT  [19] , adding an extra prediction head to perform dimension scores and re-trained on the datasets that provided these labels can be deployed in our framework. Similarly, any power LLM can be leveraged as part of Lantern, including both more advanced frozen general LLMs, like GPT and Llama in our experiment, or the LLMs that are specifically fine-tuned for emotion recognition. We believe that the potential of Lantern goes far beyond what has been demonstrated in the experiments. We will keep exploring its potential in the future.\n\nThe limitation of our framework is that it requires the vanilla model to be trained on both the classification and dimension scores. Collecting and labeling a high-quality dataset will consume a significantly large amount of resources and time. We have realized the importance of building a comprehensive and sufficiently large corpus, and we will continue to make contributions in this aspect.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Conclusion",
      "text": "We present Lantern, a framework to improve the performance of vanilla models by prompting large language models with receptive-field-aware attention weighting. Lantern can be implemented with limited resources. Only a consumer-grade graphics card and a certain number of API invocations are needed. In different settings of IEMOCAP, we have proved that Lantern can significantly improve the performance of a vanilla model (CORECT in our work). We further analyzed the effect of multiple parts of our prompt and compared it with alternative algorithms to merge the predictions.\n\nResearch Development Funding (No.RDF-21-02-072) and Shanghai Key Laboratory of Trusted Data Circulation and Governance, and Web3.",
      "page_start": 15,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Integrating external knowledge: The first approach uses a pre-trained",
      "page": 2
    },
    {
      "caption": "Figure 1: a, these frozen models are",
      "page": 2
    },
    {
      "caption": "Figure 2: Our framework Lantern has three stages: first, a vanilla model pre-",
      "page": 3
    },
    {
      "caption": "Figure 2: In the first stage,",
      "page": 5
    },
    {
      "caption": "Figure 3: Methods to predict metrics: Figure 3a described a single-task model,",
      "page": 6
    },
    {
      "caption": "Figure 3: b, for a given vanilla model, an additional linear layer is added after",
      "page": 6
    },
    {
      "caption": "Figure 4: a. However, a win-",
      "page": 7
    },
    {
      "caption": "Figure 4: b, so that each window could overlap with each other.",
      "page": 7
    },
    {
      "caption": "Figure 4: c demonstrates",
      "page": 7
    },
    {
      "caption": "Figure 4: Strategies to split a dialogue: Figure 4a is the naive splitting, where each",
      "page": 8
    },
    {
      "caption": "Figure 4: b pads some samples",
      "page": 8
    },
    {
      "caption": "Figure 4: c demonstrated the sliding window",
      "page": 8
    },
    {
      "caption": "Figure 5: LDA coefficients",
      "page": 10
    },
    {
      "caption": "Figure 6: Confusion matrices of LDA results: panel 6a is normalized by the sum",
      "page": 11
    },
    {
      "caption": "Figure 5: displays the LDA coefficients. They suggest that valence",
      "page": 11
    },
    {
      "caption": "Figure 6: Sad has the highest precision",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The results on IEMOCAP (6-way): all models are trained with multi-",
      "data": [
        {
          "Models Acc. (%) w-F1 (%) Happy": "65.31\n65.34\n67.60\n68.20\n70.62\n69.93\n69.93\n70.02\n73.95\n74.08",
          "Sad Neutral Angry Excited Frustrated": "51.49\n74.54\n62.38\n67.25\n73.96\n59.97\n55.76\n80.17\n63.21\n61.69\n74.91\n63.90\n-\n-\n-\n-\n-\n-\n59.30\n80.53\n66.94\n69.59\n72.69\n68.50\n79.51\n76.33\n76.79\n67.14\n72.71\n71.88"
        },
        {
          "Models Acc. (%) w-F1 (%) Happy": "70.18\n70.28\n71.16\n70.90",
          "Sad Neutral Angry Excited Frustrated": "57.64\n86.11\n68.00\n64.42\n79.11\n64.43\n70.10\n86.22\n64.76\n65.02\n76.28\n68.12"
        },
        {
          "Models Acc. (%) w-F1 (%) Happy": "73.94\n74.14\n74.43\n74.63\n74.73\n74.79",
          "Sad Neutral Angry Excited Frustrated": "60.56\n71.65\n65.45\n86.61\n85.56\n71.07\n61.11\n86.55\n76.49\n65.43\n85.02\n68.37\n63.41\n81.20\n64.92\n82.39\n70.51\n78.78"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: Results when using different merge algorithms using the same prompts",
      "data": [
        {
          "Merge algo.": "",
          "6-way": "",
          "4-way": "Acc. w-F1 Acc. w-F1"
        },
        {
          "Merge algo.": "Naive add-up\nNaive weights\nAttention w/o RFA 69.75\nAttention w/ RFA 71.16 70.84",
          "6-way": "70.30\n70.46\n69.62\n69.02\n69.94",
          "4-way": "85.26\n85.25\n86.00\n85.99\n85.05\n85.03\n86.32\n86.32"
        },
        {
          "Merge algo.": "Lantern",
          "6-way": "71.16 70.90 86.53 86.53",
          "4-way": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Dimensional speech emotion recognition from speech features and word embeddings by using multitask learning",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "A transformer-based jointencoding for emotion recognition and sentiment analysis",
      "authors": [
        "J Delbrouck",
        "N Tits",
        "M Brousmiche",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "7",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin"
      ],
      "year": "2019",
      "venue": "NAACL HLT 2019-2019 Conf North Am Chapter Assoc Comput Linguist Hum Lang Technol-Proc Conf"
    },
    {
      "citation_id": "8",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Dubey",
        "A Jauhri",
        "A Pandey",
        "A Kadian",
        "A Al-Dahle",
        "A Letman",
        "A Mathur",
        "A Schelten",
        "A Yang",
        "A Fan"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "9",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "10",
      "title": "A new model for learning in graph domains",
      "authors": [
        "M Gori",
        "G Monfardini",
        "F Scarselli"
      ],
      "year": "2005",
      "venue": "Proceedings. 2005 IEEE international joint conference on neural networks"
    },
    {
      "citation_id": "11",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "12",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "14",
      "title": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "15",
      "title": "Scaling laws for neural language models",
      "authors": [
        "J Kaplan",
        "S Mccandlish",
        "T Henighan",
        "T Brown",
        "B Chess",
        "R Child",
        "S Gray",
        "A Radford",
        "J Wu",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "16",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "17",
      "title": "Pre-trained language modelenhanced conditional generative adversarial networks for intrusion detection",
      "authors": [
        "F Li",
        "H Shen",
        "J Mai",
        "T Wang",
        "Y Dai",
        "X Miao"
      ],
      "year": "2024",
      "venue": "Peerto-Peer Networking and Applications"
    },
    {
      "citation_id": "18",
      "title": "A system for real-time intervention in negative emotional contagion in a smart classroom deployed under edge computing service infrastructure. Peer-to-Peer",
      "authors": [
        "J Li",
        "D Shi",
        "P Tumnark",
        "H Xu"
      ],
      "year": "2020",
      "venue": "Networking and Applications"
    },
    {
      "citation_id": "19",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledgebased systems",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledgebased systems"
    },
    {
      "citation_id": "21",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "22",
      "title": "Conversation understanding using relational temporal graph neural networks with auxiliary cross-modality interaction",
      "authors": [
        "C Nguyen",
        "T Mai",
        "D Kieu",
        "D Le"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Recurrent space-time graph neural networks",
      "authors": [
        "A Nicolicioiu",
        "I Duta",
        "M Leordeanu"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "25",
      "title": "Prompting large language models with answer heuristics for knowledge-based visual question answering",
      "authors": [
        "Z Shao",
        "Z Yu",
        "M Wang",
        "J Yu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Multiemo: An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "T Shi",
        "S Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Representation learning through crossmodal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need. Advances in neural information processing systems",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need. Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Graph Attention Networks. International Conference on Learning Representations",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Liò",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Graph Attention Networks. International Conference on Learning Representations"
    },
    {
      "citation_id": "31",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Multimodal learning with transformers: A survey",
      "authors": [
        "P Xu",
        "X Zhu",
        "D Clifton"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "authors": [
        "Y Zhang",
        "M Wang",
        "P Tiwari",
        "Q Li",
        "B Wang",
        "J Qin"
      ],
      "year": "2023",
      "venue": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "arxiv": "arXiv:2310.11374"
    }
  ]
}