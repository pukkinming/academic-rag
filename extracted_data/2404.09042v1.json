{
  "paper_id": "2404.09042v1",
  "title": "Improving Personalisation In Valence And Arousal Prediction Using Data Augmentation",
  "published": "2024-04-13T16:57:37Z",
  "authors": [
    "Munachiso Nwadike",
    "Jialin Li",
    "Hanan Salam"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the field of emotion recognition and Human-Machine Interaction (HMI), personalised approaches have exhibited their efficacy in capturing individual-specific characteristics and enhancing affective prediction accuracy. However, personalisation techniques often face the challenge of limited data for target individuals. This paper presents our work on an enhanced personalisation strategy, that leverages data augmentation to develop tailored models for continuous valence and arousal prediction. Our proposed approach, Distance Weighting Augmentation (DWA), employs a weighting-based augmentation method that expands a target individual's dataset, leveraging distance metrics to identify similar samples at the segmentlevel. Experimental results on the MuSe-Personalisation 2023 Challenge dataset demonstrate that our method significantly improves the performance of features sets which have low baseline performance, on the test set. This improvement in poorperforming features comes without sacrificing performance on high-performing features. In particular, our method achieves a maximum combined testing CCC of 0.78, compared to the reported baseline score of 0.76 (reproduced at 0.72). It also achieved a peak arousal and valence scores of 0.81 and 0.76, compared to reproduced baseline scores of 0.76 and 0.67 respectively. Through this work, we make significant contributions to the advancement of personalised affective computing models, enhancing the practicality and adaptability of data-level personalisation in real world contexts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Automatic emotion recognition (ER) has been an active area of research in the field of Human-Machine Interaction (HMI) during the past decade. ER utility spans a wide range of applications in areas like healthcare  [22] ,  [7] ,  [45] , and robotics  [51] ,  [54] . Research approaches in automatic ER has focused on either the categorical model  [59] ,  [23]  or the dimensional model  [66] ,  [67] . The categorical model focuses on the recognition of discrete emotion classes (anger, disgust, fear, happiness, sadness, surprise, contempt). This model of ER has been widely explored in affective computing for its simplicity, due to the ease with which humans can readily interpret distinct emotional classes. The dimensional model, on the other hand, focuses on the prediction of continuous emotional attributes representing them in a continuous dimensional space such as arousal-valence space. The dimensional model is more suitable for modeling the complex emotions, which cannot easily be fit into distinct categories, but rather vary along a spectrum. Both models of emotions have been investigated and contrasted in the affective computing literature  [44] ,  [10] ,  [69] .\n\nThis work was supported by the Center for Artificial Intelligence and Robotics at NYUAD ER presents various complex challenges due to the diverse nature of emotional expressions across individuals and cultures. Each individual expresses emotion slightly differently  [26] ,  [63] , in a range of modalities such as voice audio, video facial expression, and physiological stress signals such as heart rate (BPM)  [70] ,  [71] . The question naturally arises, as to whether personalisation strategies can be used with multimodal supervised learning techniques to improve state-of-the-art performance on dimensional emotion recognition problems. Efforts such as the 4th Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation (MuSe 2023)  [14]  have sought to address this problem. Specifically, the MuSepersonalisation sub-challenge focuses on investigating customised approaches to enhance the accuracy of machine learning for customised predictions of emotional valence and arousal. A multimodal dataset, Ulm-Trier Social Stress Test (ULM-TSST)  [70] , has been provided, together with seven pre-extracted feature sets.\n\nEarly research in affective machine learning has yielded significant insights into the potential of personalisation for predicting emotional states  [29] ,  [75] . Techniques such as transfer learning  [28] ,  [47] ,  [78] ,  [9] ,  [34] ,  [46] ,  [49] , userspecific training  [80] ,  [60] ,  [62] ,  [31] ,  [1] ,  [61] , groupspecific training  [75] ,  [29] ,  [63] ,  [27]  and multitask learning  [37] ,  [25] ,  [72] ,  [50] ,  [13]  have demonstrated notable improvements in estimation performance  [33] . The MuSe-Personalisation challenge baseline paper  [14] , for instance, adopted the transfer learning approach. This approach involves training a generic model on global data and finetuning it using individualised data specific to each user. However, achieving effective personalisation in real-world settings remains a considerable challenge due to data constraints  [33] . The limited data availability and quality for target users poses significant challenges to the performance of the personalised models. To address these constraints, previous research has explored various techniques, including transfer learning  [48] , weighting-based approaches  [12] ,  [15] , feature augmentation  [53] ,  [81] ,  [52] , and generative-based models  [39] ,  [5] ,  [35] ,  [8] ,  [79] ,  [77] . Among them, weighting-based methods offer a unique advantage by allowing the fine-tuning of weights as hyperparameters during augmentation, providing added flexibility for model training. However, traditional approaches in weighting-based methods have not fully tapped into their potential. Most methods overlook the opportunity to utilise a two-stage procedure involving training a generic model on global non-personalised data. Consequently, they do not fully exploit the abundant information contained within this global data.\n\nOur work aims to contribute to and advance the field of multimodal personalised affective computing, by leveraging the insights of weighting-based methods. Building upon the foundations laid by previous studies, particularly the curriculum learning and grouping methods introduced in  [57] , we propose a distance weighting augmentation (DWA) approach. DWA may be interpreted as existing at the intersection of weighting-based methods and transfer learning. By experimenting on the ULM-TSST dataset using this approach, we seek to shed light on its potential and explore its effectiveness in enhancing affective understanding. The main contributions of this paper can be summarised as follows:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This work addresses multi-modal emotion recognition and personalisation, and it is based on sample re-weighting and individual model fine-tuning approaches. In this section, we give a brief review of these techniques.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Generic Multimodal Emotion Recognition",
      "text": "Emotion recognition refers to the field developing technical skills such as facial recognition, speech recognition, voice recognition, deep learning, and pattern recognition for recognizing human emotions. Among all the emotion recognition tasks, arousal-valence estimation has gained a lot of attention due to its ability to capture a wide range of emotional states in a nuanced manner. As essential dimensions in affect modeling, they represent emotional polarity and intensity, respectively. The estimation of these natural emotions can be derived from biosignals collected via wearable sensors  [64] , text  [40] , speech  [16] , video  [56]  and so on.\n\nEmotion recognition from multi-modal signals has gained considerable scholarly attention in recent years  [2] . Generic multi-modal approaches combine more than two different modalities like speech  [42] , visual  [41] , audio  [17] , text  [4] , and physiological signals  [68]  to recognize emotions. They extract informative features from each modality and fuse them, either through feature-level fusion or decisionlevel fusion after separate modeling per modality. The recent advancements in deep learning and multi-modal emotion recognition have significantly improved the accuracy of finegrained valence-arousal estimation  [30] . These include but are not limited to the studies on audio-visual scenarios  [43] ,  [36] , and audio-textual scenarios  [17] . For example, the multi-modal fusion framework proposed by  [43]  has enhanced valence-arousal estimation by leveraging complementary audio-visual modalities. The framework employs a joint cross-attentional architecture that calculates attention weights based on the correlation between combined audiovisual features and individual audio and video features, thus effectively extracts salient features across modalities while preserving intra-modal relationships. Another audio-textual framework explored multiple temporal models to incorporate sequential information in the visual signals with various ensemble strategies at decision level  [36] . On an audio-textual level, the method proposed in  [17]  developed a distilled encoder using audio-text features, significantly improving valence prediction while addressing the size constraints of large self-supervised learning (SSL) models. However, these generic models make general assumptions and cannot capture individual differences in emotional expression well. Consequently, personalised models that leverage on individual characteristics seem promising.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Personalised Multi-Modal Emotion Recognition",
      "text": "Early works in personalising affective computing models present from two angles, data-level techniques and modellevel techniques  [33] . Model-level techniques focus on directly incorporating personalisation in model architecture or training  [9] ,  [47] ,  [25] , while data-level techniques operate on data before feeding it into the model  [57] ,  [46] ,  [80] ,  [75] . Prior works have notably advances by accounting individual differences in social behavior and emotional states through tailoring the generic affective models to individualspecific and group-specific levels. In  [82] , for example, a multi-task learning approach was proposed for personalised emotion recognition from physiological signals. By modeling relationships between signals and personality in a hypergraph framework, emotion recognition was formulated as related tasks for multiple features, improving personalised emotion prediction accuracy.\n\n1) Individual Model Transfer Learning: Individual transfer learning is a two-step process that is widely adapted in personalised deep learning algorithms. It begins with the training of a deep learning model on a generic dataset, followed by fine-tuning the model on a dataset that is specific to an individual. This approach strikes a balance between generalisability and specialisation, ensuring that the datasets are utilised to their fullest extent. The research by  [57]  introduces the concept of \"late shaping,\" which aims to enhance the model's exposure to individual-specific data during the training phase using transfer learning. It fine-tunes a pretrained model using the data of the individual of interest. Similarly,  [28]  proposes to adapt the pre-trained general model for individual-level humor recognition by manually fine-tuning the generic model on an individual humor dataset. In the context of facial expression recognition,  [9]  also applies individual model transfer learning to facilitate the recognition of non-universal expressions. It builds on the feature extraction capabilities of a pre-trained convolutional encoder, and augments the existing encoder with an additional convolutional layer. This layer is specifically designed to learn and capture the unique representations inherent to the target dataset, thereby enhancing the model's performance in specialised facial expression recognition tasks.\n\n2) Sample Re-weighting: Re-weighting methods primarily attach weights to training samples, or to personalised model predictions that come from similar user datasets, based on their similarity to the target user's data. By effectively utilising the available dataset, the model becomes better at capturing the unique characteristics of the target user, leading to more accurate predictions. In  [57] , \"data grouping\" was proposed to augment the individual's small MNIST dataset with similar data points from the global MNIST dataset, with its similarity identified using an autoencoder.\n\nMotivated by such existing investigations into personalisation strategies, we propose to investigate the potential of the personalised multi-modal emotion recognition using a deep network trained under a combined re-weighting and transfer learning scheme. for each sample S ∈ AP do",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Algorithm 1 Distance Weighting Augmentation",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "12:",
      "text": "if distance metric == \"cosine\" then 13:\n\ndistance := cosine distance(S, W T )\n\n14:\n\n15:\n\nelse if distance metric == \"centroid DP\" then 16:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "18:",
      "text": "else if distance metric == \"centroid L2\" then 19:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "21:",
      "text": "end if",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "22:",
      "text": "end for",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "23:",
      "text": "Daug := n samples S ∈ AP",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "24:",
      "text": "with min distance 25:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "26:",
      "text": "end for 27: end for",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iii. Proposed Methodology",
      "text": "In this section, we present a data-level augmentation approach, aimed at enhancing personalised models' accuracy in predicting continuous valence and arousal under data label constraints. We provide a detailed explanation of our approach, organised into two sections. First, in section III-A, we introduce the distance weighting framework. Then, in section III-B, we discuss the criteria employed for the distance metrics used to calculate the weights during augmentation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Distance Weighting Augmentation",
      "text": "Our objective is to enhance emotion recognition performance, on each unique individual, with respect to our evaluation metric. Overall, we adopt a transfer learning technique as illustrated in",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Distance Metrics",
      "text": "We employed three distance metrics during augmentation, to compute the similarity between two time segments of equal length: (1) centroid L2 distance, (2) centroid dot product, and (3) cosine distance. These were chosen as well studied distance measures suitable to our problem statement. For a time segment W T , we have a set of features\n\n, where the number of features depends on the segment length. For instance, winlen = 10 implies that 10 timestamps are grouped together within each segment, with 1 feature vector for each timestamp. By W T , we denote the average of all winlen feature vectors belonging to segment W T . This is equivalent to computing the centroid feature vector, and it is utilised in Algorithm 1. For two time segments W 1 and W 2 , our three distance metrics can be defined as follows:\n\n1) Centroid L2 Distance: Centroid L2 Distance calculates segment similarity by measuring the L2 distance between the mean vectors of two given segments. The mean vector within a given segment represents the centroid, or average feature over that time frame. L2 distance, also denoted as euclidean distance, has been proven effective in the literature for measuring similarity between feature vectors in various domains, such as face recognition  [38] , speaker voice similarity  [65] ,  [55] , and text similarity  [74] ,  [24]  systems. A smaller Euclidean distance between two vectors suggests a higher degree of similarity, with a value of zero indicating that the vectors are identical.\n\n2) Centroid Dot Product (centroid DP) Distance: Similar to the Centroid L2 calculation, we utilise Centroid Dot Product distance to compute the distance between the centroids of two respective segments. However, as dot product is inherently a measure of similarity, we take the negative of the dot product as a measure of distance. The effectiveness of dot product as a measure of similarity is time-tested, not the least by virtue of its widespread usage in transformer models  [73] .\n\n3) Cosine Distance: Cosine distance is a robust and efficient metric that excels in high-dimensional applications such as text mining, information retrieval, and computer vision  [74] . Originating from the widely-used cosine distance measure in vector space  [32] ,  [3] , it quantifies the distance between pair-wise vectors or segments. Cosine distance is scale-invariant, focusing on the orientation of vectors rather than their magnitude, ensuring its effectiveness for computing distance for sparse and high-dimensional feature vectors in ULM-TSST dataset. Cosine distance has a bounded range from 0 to 1, with 0 indicating identical vectors.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "In this section, we describe the experimental setup including the baseline we compare our approach to, dataset, loss function, evaluation metric, and experimental details.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Baseline Models",
      "text": "The baseline work from  [14] , much like our work, uses a two-stage transfer learning approach. In the first stage, it trains a generic model M G , using the generic dataset D G . It then copies the generic model multiple times for each individual in the testing set, and trains separate personalised models using each D Ii . Similarly to our approach, the training and development parts, Train I and Devel I, of the Test individuals, are used to personalise the respective generic models. Our approach seeks to build upon this baseline using DWA, whereby each Train I i is augmented with similar samples from D G .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Dataset",
      "text": "We utilise the ULM-TSST dataset from the MuSe 2023 Personalisation sub-challenge  [14] . This dataset comprises video, audio, written transcripts, and physiological signals obtained from approximately 5 minutes of solo conversations with 69 different individuals. In each solo conversation, a single individual stands in front of a camera and speaks continuously in the German language. The dataset labels consist of continuous valence and arousal labels for small incremental timestamps through the length of each conversation. It is vital to note that the dataset is divided into training, development, and testing sets based on the individuals themselves, rather than time segments. That is, 41 individuals are assigned to the training set, 14 individuals to the development set, and 14 individuals to the testing set. In this paper we refer to the 55 training and development individuals as D G , while the 14 testing individuals are represented as D I = {D Ii } 14 i=1 . Our methodological objective is to accurately predict the arousal and valence values on the 14 individuals for the testing set. Notably, while labels are available for the entire Features. For the different modalities, pre-extracted feature sets were made available by  [14] . These are summarised in table I. Among the video features, FAU most directly encodes facial action units  [83] , while FACENET512 features are associated with deep facial features  [58] . While VIT  [20]  features encode information about visual semantics, these semantics may not be facial information. This is because the VIT model is pre-trained on Imagenet  [18]  using DINO  [11]  self-supervision. Among the audio features, EGEMAPS  [21]  are based on a set of interpretable speech parameters, which can be computed in closed form from the data. Each EGEMAPS dimension corresponds to a parameter such as pitch, shimmer, loudness, or alpha-ratio. DEEPSPECTRUM and WAV2VEC features are extracted via neural network. However, DEEPSPECTRUM  [6]  features are extracted from spectrogram images using a convolutional neural network (CNN), while WAV2VEC  [76]  features are extracted from audio waveforms using a combination of CNNs and transformers. BERT-4  [19]  features are extracted using a BERT transformer, which has been pretrained on the German language. The BIOSIGNALS features consist of machinerecorded readings of electrocardiograms (ECG), Respiration (RESP) and heart rate beats per minute (BPM).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Loss Function And Evaluation Metric",
      "text": "In line with  [14] , we adopt the Concordance Correlation Coefficient (CCC) as the loss function to train our model. It\n\nis typically used to assess how well two sets of observations agree with each other in terms of both their mean values and their dispersion, i.e. accuracy and precision. As a product of the Pearson Correlation Coefficient (PCC) and the Bias Correction Factor (BCF), the resulting value of CCC ranges between -1 and 1, where a value of 1 indicates perfect agreement, 0 indicates no agreement, and -1 indicates perfect disagreement. The CCC loss is formulated as follows:\n\nwhere µ ŷ and µ y represent the mean of the prediction ŷ and the label y, respectively. Similarly, σ ŷ and σ y correspond to their standard deviations. ρ denotes the PCC between ŷ and y. It measures the strength and direction of the linear relationship between X and Y . A value of ρ close to 1 indicates a strong positive linear relationship, while a value close to -1 indicates a strong negative linear relationship. A value of 0 indicates no linear relationship. In addition,",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Experimental Details",
      "text": "Baseline Models. The baseline work of  [14] , while insightful, only reports results for 6 audio and video features, namely, Fau, FaceNet512, ViT, eGeMAPS, DeepSpectrum, and Wav2Vec. We conduct all our experiments on these features, but also include results for the BERT-4, and BIOSIG-NALS features. We also employ the same generic model from the baseline methodology for consistent comparison.\n\nHyperparameters. DWA was used to augment each individual dataset and obtain D Iaug . For fair comparison with the baseline, we set consistent ranges for any tuned hyerparameters such as learning rates, number of RNN layers, and model dimensions, during hyperparameter search. All other hyperparameters, including early stopping patience, segment length, and hop length, are kept unchanged from the baseline method in  [14] . However, we perform the hyperparameter search for all 8 features in the dataset, expanding upon the 6 reported features of the  [14] .\n\nOur experiments sought to understand the impact of different choices of distance metrics and numbers of augmentation samples per segment. We trained models using data augmented with one of the three distance metrics: centroid DP, cosine, and centroid L2. We also explored varying values for number of augmentation samples per segment n, specifically n = 1 up to 3.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "We organise our discussion into 2 sections. Firstly, we seek to highlight the experimental effects of personalisation. To do this, we reproduce the generic model and personalised model results of the baseline paper on our dataset, and compare their respective performances on the testing set (section V-A). This is particularly important in the context of personalisation, as the generic model results on the testing set were not provided in the baseline work  [14] . Secondly, we perform a series of experiments to evaluate the performance of the DWA personalisation approach (section V-B). Our objective is to show that augmentation with individualspecific samples from our augmentation pool can enhance the performance of personalised model training on valence and arousal prediction for each testing individual.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. The Effect Of Personalisation",
      "text": "Generic Model Performance Evaluation. The findings presented in table III serve to demonstrate the need for personalisation, by demonstrating scores of the generic model M G , obtained without the use of personalisation, in order to have a reference for any effects that may result from a personalised training step. We observe that nearly all models have combined CCC scores below 0.53. The highest performing features are FACENET512 and EGEMAPS, with combined testing scores of 0.52. In particular, EGEMAPS scored highest on arousal with a testing score of 0.56. While FACENET512 had highest such score of 0.56 on valence. This suggests that without personalisation, a model trained on a general dataset will be less effective on a given individual. The lowest testing CCC scores are obtained by FAU, and BIOSIGNALS features, which score 0.23 and 0.03 respectively. The BIOSIGNALS feature scores lowest in both cases of arousal and valence, obtaining -0.14, and 0.19 respectively. For each timestamp, the BIOSIGNALS feature vector comprises 3 measurements: 1 for ECG, 1 for resps, and 1 for BPM. We hypothesise that the low dimensionality of the BIOSIGNALS feature may be one additional reason for its extraordinarily low performance. However, all in all, the generic model results serve as a backdrop to compare personalised model scores with and without DWA.\n\nBaseline Personalisation Performance Evaluation. The development and testing set CCC scores obtained from personalised training using the baseline approach  [14]  across all features, are presented in table IV. The FAU feature, which previously scored 0.23 (combined CCC) in our reproduced results, without personalisation, now obtains the highest CCC across all features, of 0.79, owed to respective arousal and valence scores of 0.79 and 0.78 respectively. The BIOSIGNALS feature still has the lowest score, at 0.48, but dramatically outperforms its corresponding generic model score of 0.03. BIOSIGNALS has the 5th highest score in arousal of 0.61, but scores only 0.35 in valence performance. In our experiments, while FACENET512 and EGEMAPS improved from a testing CCC of 0.52 to 0.69, and 0.56 respectively, they are by no means our best performing respective audio and video features. These were instead FAU and WAV2VEC, which improved from 0.23 to 0.79, and from 0.48 to 0.66 respectively, thanks to personalisation. In the case of WAV2VEC, which was the best performing audio feature, it scores 0.80 in valence, making it the highest performing feature in valence. However, it scored 0.53 on arousal.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Performance Evaluation Of Dwa",
      "text": "The 8 different feature types utilised in our experiments, each capture different characteristics of a target individual.  Video & Physiological Features. Among the video features, only FACENET512 and VIT features seem to benefit from DWA, with the latter seeing the most improvement compared to the baseline model. For FACENET512, the combined testing CCC peaks at 0.74 with DWA, compared to 0.69 without DWA (baseline model). Much of this improvement is due to the valence prediction, which increases from a testing CCC of 0.69 to 0.83. Arousal prediction does not see similar improvement for FACENET512. With VIT features, the final combined testing CCC increases 0.60 to 0.67. This is due to significant improvements in both arousal prediction from 0.64 to 0.71, and in valence prediction from 0.55 to 0.65 with respect to the baseline. The FAU features, which had the highest performance without DWA, saw a drop in performance with DWA. This suggests that DWA can only be used as a tool to improve the performance of features which are lagging behind on specific tasks. Indeed, this hypothesis is further supported by the improvement in BIOSIGNALS features testing CCC. With the use of DWA a compelling performance increase can be seen from 0.35 to 0.69 for the arousal prediction task.\n\nAudio Features. Results with audio features are also presented in table V. The performance increase due to DWA is most noticeable using the EGEMAPS feature. For the EGEMAPS, DWA brings about an increase in performance from 0.56 to 0.70. This is due to corresponding improvements in testing arousal and valence CCC, which increased from 0.50 and 0.59 respectively, to 0.71 and 0.68 respectively. This combined increase in EGEMAPS performance with to DWA is not only the highest among the audio features, but among all features which were tested. DWA also results in an improved performance in the DEEPSPECTRUM feature from 0.56 to 0.63. However, the improvement is largely limited to the valence prediction task. As a result, since the arousal prediction of DEEPSPECTRUM does not appear to benefit from DWA, the combined CCC score only improves from 0.55 to 0.60.\n\nNumber of Augmentations per segment. We ran experiments on the value of n to determine the effect of the number of augmentation samples on model performance. What we find, as per table V, is that the ideal number of augmentation samples varies depending on the feature and distance metric. For example, consider the case of valence prediction using DEEPSPECTRUM features and cosine distance. The performance on Devel I increases as n is increasing from 1 to 2, and decreases when n is further increased to 3. This suggests a diminishing return pattern Such is also the case for BIOSIGNALS feature. On the other hand, performance on FACENET512, with cosine feature, seems to be steadily increasing with n. This suggests that the value of n must be tuned uniquely as a hyperparamter for each feature-distancemetric combination.\n\nLate Fusion. Finally, we adopt the same late fusion strategy deployed in our baseline, for comparison purposes. For late fusion, we utilise proportional weighting, whereby two sets of predictions are weighted based on their performance on the development set. For fair comparison, we include the reported results for late fusion from the baseline, of FACENET512 and EGEMAPS features. However, in reproducing these results, we utilise consistent hyperparameter search between all methods, causing the reproduced late fusion score to be different from the reported baseline. The results are presented in table VI.\n\nIn the baseline work, the best performing feature fusion is of FACENET512 and EGEMAPS, reaching a testing CCC of 0.76. Our reproduced score is slightly lower, at 0.72. However, our DWA method results in a score of 0.78 when fusing VIT and EGEMAPS features. This improvement is significant, as without DWA, the late fusion of VIT and EGEMAPS would only result in a CCC testing score of 0.70. As expected, the fusion of EGEMAPS with VIT scores higher than with FACENET512, as the latter did not see signficant benefit in arousal prediction from DWA, as earlier discussed. However, this but reinforces our hypothesis, that DWA can be quite useful as a boost to personalisation performance for features which are not performing well.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusions And Future Works",
      "text": "In this paper, we introduced the Distance Weighting Augmentation (DWA) method for enhancing predictions of valence and arousal, at the data level. In simple terms, DWA expands a personalised dataset with samples from a global dataset that are the most similar. We implement DWA on all 8 features provided in our target dataset, with the best performance observed from a fusion of VIT and EGEMAPS predictions. Among the 3 distance metrics employed for similarity measurement, all proved to be effective for either valence or arousal, or both, in 4 out of 8 features. Our findings indicate that DWA is most useful in improving personalisation performance when utilising features which have poor performance for a particular task. Late fusion of the DWA-trained VIT and EGEMAPS models, yields a combined CCC score of 0.78, indicating the effectiveness of DWA, and opening the door for further research into datalevel personalisation.\n\nFuture work could expand upon DWA in 3 key ways. Firstly, we can explore further distance metrics to see if they could improve over the 3 used thus far. Secondly, it may be beneficial to explore fractional weighting of similar samples. This could be implemented via sample weighting, whereby we would alternate between normal samples and augmentation samples within a training loop. In effect, this could allow us to weight samples in the interval [0,1] rather than relying solely on binary weights. Thirdly, it is important to investigate the reasons for which DWA brings limited performance to features which already performed well. In effect, modifications to the algorithm itself, may help in expanding upon its applicability to different feature sets. This last point will be easier to explore as we seek to apply DWA to additional datasets for dimensional emotion prediction.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Recognizing the importance of",
      "page": 3
    },
    {
      "caption": "Figure 2: , DWA begins by constructing an",
      "page": 3
    },
    {
      "caption": "Figure 1: Our personalisation framework leverages transfer",
      "page": 3
    },
    {
      "caption": "Figure 2: DWA generates an dataset DIiaug from the original",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Munachiso Nwadike1": "1 Department of Computer Science, New York University Abu Dhabi, UAE",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "Abstract— In the field of\nemotion recognition and Human-",
          "Jialin Li1, Hanan Salam1": "ER presents various\ncomplex challenges due\nto the di-"
        },
        {
          "Munachiso Nwadike1": "Machine Interaction (HMI), personalised approaches have ex-",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "verse\nnature\nof\nemotional\nexpressions\nacross\nindividuals"
        },
        {
          "Munachiso Nwadike1": "hibited their\nefficacy\nin capturing\nindividual-specific\ncharac-",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "and\ncultures. Each\nindividual\nexpresses\nemotion\nslightly"
        },
        {
          "Munachiso Nwadike1": "teristics and enhancing affective prediction accuracy. However,",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "differently\n[26],\n[63],\nin\na\nrange\nof modalities\nsuch\nas"
        },
        {
          "Munachiso Nwadike1": "personalisation techniques often face\nthe\nchallenge of\nlimited",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "data for target\nindividuals. This paper presents our work on",
          "Jialin Li1, Hanan Salam1": "voice audio, video facial expression, and physiological stress"
        },
        {
          "Munachiso Nwadike1": "an enhanced personalisation strategy,\nthat\nleverages data aug-",
          "Jialin Li1, Hanan Salam1": "signals\nsuch as heart\nrate\n(BPM)\n[70],\n[71]. The question"
        },
        {
          "Munachiso Nwadike1": "mentation to develop tailored models for continuous valence and",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "naturally arises, as to whether personalisation strategies can"
        },
        {
          "Munachiso Nwadike1": "arousal prediction. Our proposed approach, Distance Weighting",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "be used with multimodal\nsupervised learning techniques\nto"
        },
        {
          "Munachiso Nwadike1": "Augmentation (DWA), employs a weighting-based augmentation",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "improve\nstate-of-the-art performance on dimensional\nemo-"
        },
        {
          "Munachiso Nwadike1": "method that expands a target\nindividual’s dataset,\nleveraging",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "distance metrics\nto\nidentify\nsimilar\nsamples\nat\nthe\nsegment-",
          "Jialin Li1, Hanan Salam1": "tion recognition problems. Efforts such as the 4th Multimodal"
        },
        {
          "Munachiso Nwadike1": "level. Experimental\nresults on the MuSe-Personalisation 2023",
          "Jialin Li1, Hanan Salam1": "Sentiment Analysis Challenge\nand Workshop: Mimicked"
        },
        {
          "Munachiso Nwadike1": "Challenge dataset demonstrate\nthat\nour method significantly",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "Emotions, Humour\nand Personalisation (MuSe 2023)\n[14]"
        },
        {
          "Munachiso Nwadike1": "improves\nthe\nperformance\nof\nfeatures\nsets which\nhave\nlow",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "have sought\nto address this problem. Specifically,\nthe MuSe-"
        },
        {
          "Munachiso Nwadike1": "baseline performance, on the test set. This improvement in poor-",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "personalisation sub-challenge focuses on investigating cus-"
        },
        {
          "Munachiso Nwadike1": "performing features comes without sacrificing performance on",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "high-performing features.\nIn particular, our method achieves",
          "Jialin Li1, Hanan Salam1": "tomised\napproaches\nto\nenhance\nthe\naccuracy\nof machine"
        },
        {
          "Munachiso Nwadike1": "a maximum combined testing CCC of 0.78,\ncompared to the",
          "Jialin Li1, Hanan Salam1": "learning for customised predictions of emotional valence and"
        },
        {
          "Munachiso Nwadike1": "reported baseline\nscore\nof\n0.76\n(reproduced at\n0.72).\nIt\nalso",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "arousal. A multimodal dataset, Ulm-Trier Social Stress Test"
        },
        {
          "Munachiso Nwadike1": "achieved a peak arousal and valence scores of 0.81 and 0.76,",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "(ULM-TSST)\n[70], has been provided,\ntogether with seven"
        },
        {
          "Munachiso Nwadike1": "compared\nto\nreproduced\nbaseline\nscores\nof\n0.76\nand\n0.67",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "pre-extracted feature sets."
        },
        {
          "Munachiso Nwadike1": "respectively. Through this work, we make significant contribu-",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "tions\nto the advancement of personalised affective\ncomputing",
          "Jialin Li1, Hanan Salam1": "Early research in affective machine learning has yielded"
        },
        {
          "Munachiso Nwadike1": "models, enhancing the practicality and adaptability of data-level",
          "Jialin Li1, Hanan Salam1": "significant\ninsights\ninto the potential of personalisation for"
        },
        {
          "Munachiso Nwadike1": "personalisation in real world contexts.",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "predicting emotional\nstates\n[29],\n[75]. Techniques\nsuch as"
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "transfer\nlearning [28],\n[47],\n[78],\n[9],\n[34],\n[46],\n[49], user-"
        },
        {
          "Munachiso Nwadike1": "I.\nINTRODUCTION",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "specific\ntraining\n[80],\n[60],\n[62],\n[31],\n[1],\n[61],\ngroup-"
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "specific training [75],\n[29],\n[63],\n[27] and multitask learn-"
        },
        {
          "Munachiso Nwadike1": "Automatic emotion recognition (ER) has been an active",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "ing [37],\n[25],\n[72],\n[50],\n[13] have demonstrated notable"
        },
        {
          "Munachiso Nwadike1": "area of\nresearch in the field of Human-Machine Interaction",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "improvements\nin estimation performance\n[33]. The MuSe-"
        },
        {
          "Munachiso Nwadike1": "(HMI) during the past decade. ER utility spans a wide range",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "Personalisation challenge baseline paper\n[14],\nfor\ninstance,"
        },
        {
          "Munachiso Nwadike1": "of applications\nin areas\nlike healthcare [22],\n[7],\n[45], and",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "adopted the\ntransfer\nlearning approach. This\napproach in-"
        },
        {
          "Munachiso Nwadike1": "robotics\n[51],\n[54]. Research approaches\nin automatic ER",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "volves\ntraining\na\ngeneric model\non\nglobal\ndata\nand fine-"
        },
        {
          "Munachiso Nwadike1": "has focused on either the categorical model [59], [23] or the",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "tuning it using individualised data specific to each user. How-"
        },
        {
          "Munachiso Nwadike1": "dimensional model [66], [67]. The categorical model focuses",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "ever,\nachieving effective personalisation in real-world set-"
        },
        {
          "Munachiso Nwadike1": "on the recognition of discrete emotion classes (anger, disgust,",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "tings remains a considerable challenge due to data constraints"
        },
        {
          "Munachiso Nwadike1": "fear,\nhappiness,\nsadness,\nsurprise,\ncontempt). This model",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "[33]. The\nlimited\ndata\navailability\nand\nquality\nfor\ntarget"
        },
        {
          "Munachiso Nwadike1": "of ER has\nbeen widely\nexplored\nin\naffective\ncomputing",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "users poses significant challenges to the performance of\nthe"
        },
        {
          "Munachiso Nwadike1": "for\nits\nsimplicity, due to the ease with which humans can",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "personalised models. To address\nthese constraints, previous"
        },
        {
          "Munachiso Nwadike1": "readily interpret distinct emotional classes. The dimensional",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "research has explored various techniques,\nincluding transfer"
        },
        {
          "Munachiso Nwadike1": "model,\non\nthe\nother\nhand,\nfocuses\non\nthe\nprediction\nof",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "learning [48], weighting-based approaches [12], [15], feature"
        },
        {
          "Munachiso Nwadike1": "continuous emotional attributes representing them in a con-",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "augmentation [53],\n[81],\n[52], and generative-based models"
        },
        {
          "Munachiso Nwadike1": "tinuous\ndimensional\nspace\nsuch\nas\narousal-valence\nspace.",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "[39], [5], [35], [8], [79], [77]. Among them, weighting-based"
        },
        {
          "Munachiso Nwadike1": "The dimensional model\nis more\nsuitable\nfor modeling the",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "methods offer a unique advantage by allowing the fine-tuning"
        },
        {
          "Munachiso Nwadike1": "complex emotions, which cannot easily be fit\ninto distinct",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "of weights as hyperparameters during augmentation, provid-"
        },
        {
          "Munachiso Nwadike1": "categories, but\nrather vary along a spectrum. Both models",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "ing added flexibility for model\ntraining. However,\ntraditional"
        },
        {
          "Munachiso Nwadike1": "of\nemotions\nhave\nbeen\ninvestigated\nand\ncontrasted\nin\nthe",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "approaches in weighting-based methods have not fully tapped"
        },
        {
          "Munachiso Nwadike1": "affective computing literature [44],\n[10],\n[69].",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "into their potential. Most methods overlook the opportunity"
        },
        {
          "Munachiso Nwadike1": "",
          "Jialin Li1, Hanan Salam1": "to utilise a two-stage procedure involving training a generic"
        },
        {
          "Munachiso Nwadike1": "This work was\nsupported by the Center\nfor Artificial\nIntelligence\nand",
          "Jialin Li1, Hanan Salam1": ""
        },
        {
          "Munachiso Nwadike1": "Robotics at NYUAD",
          "Jialin Li1, Hanan Salam1": "model on global non-personalised data. Consequently,\nthey"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "within this global data.",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "the multi-modal\nfusion\nframework\nproposed\nby\n[43]\nhas"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "Our work aims\nto contribute to and advance the field of",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "enhanced valence-arousal estimation by leveraging comple-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "multimodal personalised affective computing, by leveraging",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "mentary audio-visual modalities. The framework employs a"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "the insights of weighting-based methods. Building upon the",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "joint\ncross-attentional\narchitecture\nthat\ncalculates\nattention"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "foundations laid by previous studies, particularly the curricu-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "weights based on the correlation between combined audio-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "lum learning and grouping methods\nintroduced in [57], we",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "visual features and individual audio and video features,\nthus"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "propose a distance weighting augmentation (DWA) approach.",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "effectively extracts\nsalient\nfeatures across modalities while"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "DWA may be interpreted as existing at\nthe intersection of",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "preserving intra-modal\nrelationships. Another\naudio-textual"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "weighting-based methods and transfer\nlearning. By experi-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "framework explored multiple temporal models to incorporate"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "menting on the ULM-TSST dataset using this approach, we",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "sequential\ninformation in the visual signals with various en-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "seek to shed light on its potential and explore its effectiveness",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "semble strategies at decision level\n[36]. On an audio-textual"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "in enhancing affective understanding. The main contributions",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "level,\nthe method\nproposed\nin\n[17]\ndeveloped\na\ndistilled"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "of\nthis paper can be summarised as follows:",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "encoder\nusing\naudio-text\nfeatures,\nsignificantly\nimproving"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "valence prediction while addressing the size constraints of"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "• We highlight\nthe importance of personalisation across 8",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "large self-supervised learning (SSL) models. However,\nthese"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "different\nfeatures in 4 different modalities, by showing",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "generic models make general assumptions and cannot capture"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "the difference in model performance with and without",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "individual\ndifferences\nin\nemotional\nexpression well. Con-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "personalisation.",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "sequently, personalised models\nthat\nleverage on individual"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "• We\nintroduce\nthe DWA method,\nas\na data\naugmenta-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "characteristics seem promising."
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "tion strategy to enhance model performance in valence",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "and/or arousal prediction, showing how DWA aids per-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "B. Personalised Multi-modal Emotion Recognition"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "formance of\nfeatures which are poorly performing.",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "Early works in personalising affective computing models"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "• Through experimentation on competing distance met-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "present\nfrom two angles, data-level\ntechniques and model-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "rics, we illustrate how different\nfeature modalities\nre-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "level\ntechniques\n[33]. Model-level\ntechniques\nfocus on di-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "spond differently to DWA, and how valence and arousal",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "rectly incorporating personalisation in model architecture or"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "signals may be improved independently of one another.",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "training [9],\n[47],\n[25], while data-level\ntechniques operate"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "II. RELATED WORK",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "on data before\nfeeding it\ninto the model\n[57],\n[46],\n[80],"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "This work addresses multi-modal emotion recognition and",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "[75].\nPrior works\nhave\nnotably\nadvances\nby\naccounting"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "personalisation, and it\nis based on sample re-weighting and",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "individual differences in social behavior and emotional states"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "individual model fine-tuning approaches.\nIn this section, we",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "through tailoring the generic affective models to individual-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "give a brief\nreview of\nthese techniques.",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "specific\nand group-specific\nlevels.\nIn [82],\nfor\nexample,\na"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "multi-task learning approach was proposed for personalised"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "A. Generic Multimodal Emotion Recognition",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": ""
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "emotion recognition from physiological signals. By modeling"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "Emotion recognition refers to the field developing techni-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "relationships between signals and personality in a hypergraph"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "cal skills such as facial recognition, speech recognition, voice",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "framework, emotion recognition was\nformulated as\nrelated"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "recognition, deep learning, and pattern recognition for recog-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "tasks for multiple features,\nimproving personalised emotion"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "nizing human emotions. Among all\nthe emotion recognition",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "prediction accuracy."
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "tasks, arousal-valence estimation has gained a lot of attention",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "1)\nIndividual Model Transfer Learning:\nIndividual\ntrans-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "due\nto\nits\nability\nto\ncapture\na wide\nrange\nof\nemotional",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "fer\nlearning is a two-step process that\nis widely adapted in"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "states in a nuanced manner. As essential dimensions in affect",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "personalised\ndeep\nlearning\nalgorithms.\nIt\nbegins with\nthe"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "modeling,\nthey represent\nemotional polarity and intensity,",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "training\nof\na\ndeep\nlearning model\non\na\ngeneric\ndataset,"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "respectively. The\nestimation of\nthese natural\nemotions\ncan",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "followed by fine-tuning the model on a dataset that is specific"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "be derived from biosignals\ncollected via wearable\nsensors",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "to an individual. This\napproach strikes\na balance between"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "[64],\ntext\n[40], speech [16], video [56] and so on.",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "generalisability and specialisation, ensuring that\nthe datasets"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "Emotion recognition from multi-modal signals has gained",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "are utilised to their fullest extent. The research by [57] intro-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "considerable scholarly attention in recent years [2]. Generic",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "duces the concept of “late shaping,” which aims to enhance"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "multi-modal\napproaches\ncombine more\nthan\ntwo different",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "the model’s exposure to individual-specific data during the"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "modalities\nlike\nspeech\n[42],\nvisual\n[41],\naudio\n[17],\ntext",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "training phase using transfer\nlearning.\nIt fine-tunes\na pre-"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "[4],\nand physiological\nsignals\n[68]\nto recognize\nemotions.",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "trained model using the data of\nthe\nindividual of\ninterest."
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "They extract\ninformative\nfeatures\nfrom each modality and",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "Similarly,\n[28]\nproposes\nto\nadapt\nthe\npre-trained\ngeneral"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "fuse\nthem,\neither\nthrough feature-level\nfusion or decision-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "model\nfor\nindividual-level humor\nrecognition by manually"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "level fusion after separate modeling per modality. The recent",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "fine-tuning the generic model on an individual humor dataset."
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "advancements\nin\ndeep\nlearning\nand multi-modal\nemotion",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "In\nthe\ncontext\nof\nfacial\nexpression\nrecognition,\n[9]\nalso"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "recognition have significantly improved the accuracy of fine-",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "applies\nindividual model\ntransfer\nlearning to facilitate\nthe"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "grained valence-arousal estimation [30]. These include but",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "recognition\nof\nnon-universal\nexpressions.\nIt\nbuilds\non\nthe"
        },
        {
          "do\nnot\nfully\nexploit\nthe\nabundant\ninformation\ncontained": "are\nnot\nlimited\nto\nthe\nstudies\non\naudio-visual\nscenarios",
          "[43],\n[36],\nand audio-textual\nscenarios\n[17]. For\nexample,": "feature extraction capabilities of a pre-trained convolutional"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 1 Distance Weighting Augmentation": ""
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "Input: Global Data DG, Data of\nIndividual DI"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "Output: Augmented data of\nindividual I"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "Ensure: All segment\nlengths == winlen"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": ""
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "Generate AugPool AP"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "1: AP := {}"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "2:\nfor each individual DGi ∈ DG do"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": ""
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "3:"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "for each sample S ∈ DGi do"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "AP := AP ∪ S\n4:"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "end for\n5:"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": ""
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "end for\n6:"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": ""
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "Perform Augmentation"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "7:"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "for each individual DIi ∈ DI do"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "8:"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "DIiaug := DIi"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "9:"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "for each sample DIiW T ∈ DIi do"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": ""
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": "10:\nW T := DIiW T"
        },
        {
          "Algorithm 1 Distance Weighting Augmentation": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "tional convolutional\nlayer. This layer is specifically designed",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "distance metrics used to calculate the weights during aug-"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "to learn and capture the unique representations inherent to the",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "mentation."
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "target dataset,\nthereby enhancing the model’s performance in",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "specialised facial expression recognition tasks.",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "A. Distance Weighting Augmentation"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "2)\nSample Re-weighting: Re-weighting methods primar-",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "ily\nattach weights\nto\ntraining\nsamples,\nor\nto\npersonalised",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "Our objective is\nto enhance emotion recognition perfor-"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "model predictions that come from similar user datasets, based",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "mance, on each unique individual, with respect\nto our evalu-"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "on their\nsimilarity to the\ntarget user’s data. By effectively",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "ation metric. Overall, we adopt a transfer learning technique"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "utilising the available dataset,\nthe model becomes better at",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "as\nillustrated in Figure 1. Recognizing the\nimportance of"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "capturing the unique characteristics of the target user, leading",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "the size and quality of\nthe individual dataset DIi, we pro-"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "to more accurate predictions.\nIn [57], “data grouping” was",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "pose the Distance Weighting Augmentation (DWA) method."
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "proposed to augment\nthe individual’s small MNIST dataset",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "DWA seeks\nto augment\nthe\nlimited DIi , by incorporating"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "with similar data points from the global MNIST dataset, with",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "similar samples from the global dataset DG. Psuedo-code is"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "its similarity identified using an autoencoder.",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "provided in Algorithm 1 for a comprehensive breakdown of"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "Motivated by such existing investigations into personalisa-",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "DWA."
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "tion strategies, we propose to investigate the potential of the",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "As illustrated in Figure 2, DWA begins by constructing an"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "personalised multi-modal emotion recognition using a deep",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "augmentation pool AP from DG. AP comprises all samples"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "network trained under a combined re-weighting and transfer",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "(i.e.,\nsegments) S,\nfrom all\nindividuals\nin the training and"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "learning scheme.",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "development sets. “Training and development sets” here refer"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "to the training and development sets Train I\nto DG, and not"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "Algorithm 1 Distance Weighting Augmentation",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "I\nand Devel\n(see table II). DWA then iterates through\nin DIi"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "Input: Global Data DG, Data of\nIndividual DI",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "Output: Augmented data of\nindividual I",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "all segments in the target DIi. For each target segment W T ,"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "Ensure: All segment\nlengths == winlen",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "DWA computes\nthe distance\nto W T\nfrom all\nsegments\nin"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "AP . The nearest n segments, measured via a defined distance"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "Generate AugPool AP\n▷ Build Augmentation Pool\nfrom DG",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "1: AP := {}",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "metric (cf. section ), are selected, i.e given a weight of 1, and"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "2:\nfor each individual DGi ∈ DG do",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "segments\nin the\naugmentation\nappended to DIi . All other"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "▷ each sample is a segment\n3:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "for each sample S ∈ DGi do",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "AP := AP ∪ S\n4:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "pool\nare momentarily\ngiven\na weight\nof\n0,\ntill\nthe\nnext"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "end for\n5:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "iteration. The\nresult\nan augmented dataset,\nfor\nis DIiaug,"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "end for\n6:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "each individual\nin the\ntest\nset. DWA then returns DIiaug,"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "Perform Augmentation",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "7:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "which consists of the union of DIi with these n samples for"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "for each individual DIi ∈ DI do",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "8:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "all\nsegments\nis\nthen utilised for fine-tuning"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "DIiaug := DIi",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "in DIi . DIiaug"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "9:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "for each sample DIiW T ∈ DIi do",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "a generic model MG,\nto obtain a personalised model MIi"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "10:\nW T := DIiW T",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "for\nthe target\nindividual\nin question. All\nindividuals\nin the"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "for each sample S ∈ AP do\n11:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "if distance metric == “cosine” then\n12:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "testing set make use of\nthe same augmentation pool,\nfrom"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "distance := cosine distance(S, W T )\n13:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "−\n−\n→\n−",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "which we draw samples with replacement."
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "→S",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "winlen\nW T\nj ·\nj",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "=\n[1 −\n]\n14:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "The choice of the distance metric influences the calculated"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "(cid:88) j\n−\n−\n→\n−",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "→S\n=1\n∥\nW T\n∥2\nj ∥2∥",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "j",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "distance values and rankings of the n nearest segments. The"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "else if distance metric == “centroid DP” then\n15:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "−\n−",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "→W\n→S\n:= W T ;\n:= S\n▷ centroid of both\n16:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "subsequent\nsection provides a detailed description of\nthese"
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "▷\nsegments",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "−\n−",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": "metrics."
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "→W\n→S\ndistance := −\n·\n17:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "else if distance metric == “centroid L2” then\n18:",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        },
        {
          "encoder,\nand augments\nthe\nexisting encoder with an addi-": "−\n−",
          "in section III-B, we discuss\nthe\ncriteria\nemployed for\nthe": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "two indices in our": "augmentation pool in the"
        },
        {
          "two indices in our": "captions"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "DG1W1\n𝛂1DG1W1"
        },
        {
          "two indices in our": "DG1"
        },
        {
          "two indices in our": "—\n—"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "DG2\nDG1WL\n𝛂iDG1WL"
        },
        {
          "two indices in our": "Augmentation"
        },
        {
          "two indices in our": "V\n.S.\n…\n…\n≜"
        },
        {
          "two indices in our": "Pool"
        },
        {
          "two indices in our": "DIiWT\nAugment DIi \n?"
        },
        {
          "two indices in our": "DG3"
        },
        {
          "two indices in our": "repeating for all L target \nDistance Weighting"
        },
        {
          "two indices in our": "windows WT  in DIi  \nDGNW1\n𝛂jDGNW1"
        },
        {
          "two indices in our": "…"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "—\n—"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "DGNWL\n𝛂ADGNWL\nDGN"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "from the original\nFig. 2: DWA generates an dataset DIiaug"
        },
        {
          "two indices in our": "dataset DIi, by generating an augmentation pool, and select-"
        },
        {
          "two indices in our": "ing most similar samples from that pool,\nfor\ntarget segment"
        },
        {
          "two indices in our": "WT ."
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "IV. EXPERIMENTAL SETUP"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "In this section, we describe the experimental setup includ-"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "ing the baseline we compare our approach to, dataset,\nloss"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "function, evaluation metric, and experimental details."
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "A. Baseline Models"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "The baseline work from [14], much like our work, uses"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "a two-stage transfer\nlearning approach.\nIn the first\nstage,\nit"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "trains a generic model MG, using the generic dataset DG."
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "It\nthen\ncopies\nthe\ngeneric model multiple\ntimes\nfor\neach"
        },
        {
          "two indices in our": "individual\nin the testing set, and trains separate personalised"
        },
        {
          "two indices in our": "the train-\nmodels using each DIi . Similarly to our approach,"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "I, of\nthe Test\ning and development parts, Train I and Devel"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "individuals,\nare used to personalise\nthe\nrespective generic"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "models. Our approach seeks to build upon this baseline using"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "DWA, whereby\neach\nis\naugmented with\nsimilar\nTrain Ii"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "samples from DG."
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "B. Dataset"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "We utilise the ULM-TSST dataset\nfrom the MuSe 2023"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "Personalisation sub-challenge\n[14]. This dataset\ncomprises"
        },
        {
          "two indices in our": "video,\naudio, written transcripts,\nand physiological\nsignals"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "obtained from approximately 5 minutes of solo conversations"
        },
        {
          "two indices in our": "with 69 different individuals. In each solo conversation, a sin-"
        },
        {
          "two indices in our": "gle individual stands in front of a camera and speaks contin-"
        },
        {
          "two indices in our": "uously in the German language. The dataset\nlabels consist of"
        },
        {
          "two indices in our": "continuous valence and arousal\nlabels for small\nincremental"
        },
        {
          "two indices in our": "timestamps through the length of each conversation. It is vital"
        },
        {
          "two indices in our": "to note that\nthe dataset\nis divided into training, development,"
        },
        {
          "two indices in our": "and testing sets based on the individuals themselves,\nrather"
        },
        {
          "two indices in our": "than time segments. That\nis, 41 individuals are assigned to"
        },
        {
          "two indices in our": "the training set, 14 individuals\nto the development\nset, and"
        },
        {
          "two indices in our": "14 individuals to the testing set. In this paper we refer to the"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "55 training and development\nindividuals as DG, while the"
        },
        {
          "two indices in our": "14 testing individuals are represented as DI = {DIi}14\ni=1."
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "Our methodological objective is to accurately predict\nthe"
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": ""
        },
        {
          "two indices in our": "arousal\nand\nvalence\nvalues\non\nthe\n14\nindividuals\nfor\nthe"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "(CNN), while WAV2VEC [76]",
          "convolutional neural network": "features"
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "has",
          "convolutional neural network": "pretrained"
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": "features"
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "recorded readings of electrocardiograms (ECG), Respiration",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "rate beats per minute (BPM).",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "C. Loss Function and Evaluation Metric",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "In line with [14], we adopt",
          "convolutional neural network": "the Concordance Correlation"
        },
        {
          "spectrogram images using a": "Coefficient (CCC) as the loss function to train our model. It",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "",
          "convolutional neural network": ""
        },
        {
          "spectrogram images using a": "1https://py-feat.org/pages/au reference.html",
          "convolutional neural network": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "dataset\nlabels provided. The number\nindicate how"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Modality\nFeature\nDim.\nInformation",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "many individuals belong to the respective tasks."
        },
        {
          "TABLE I: Understanding the features in our dataset.": "FACENET512 [58]\n512\nFacial structure",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "FAU1\n[83]\n20\nFacial actions",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Video",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "DG\n{DIi }"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Visual semantics",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "VIT [11]\n384",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "(not necessarily facial)",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "Test\n(14)"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "Train G (41)\nDevel G (14)"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Speech features",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "Train I\nDevel\nI\nTest"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "EGEMAPS[21]\n88",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "(frequency, amplitude, etc.)",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Audio",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "✓\n✓\n✓\n✓\n✗"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "DEEPSPECTRUM [6]\n1024\nSpectrogram image features",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "WAV2VEC [76]\n1024\nAudio wave features",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Text\nBERT-42\n[19]\n768\nTranscription semantics",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Heart\nrate/electrical",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Physiological\nBIOSIGNALS\n3",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "activity, breathing",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "is typically used to assess how well\ntwo sets of observations"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "agree with each other in terms of both their mean values and"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "their dispersion,\ni.e.\naccuracy and precision. As\na product"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "of\nthe Pearson Correlation Coefficient\n(PCC) and the Bias"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "5 minutes of conversation for both the training and develop-",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "Correction Factor (BCF),\nthe resulting value of CCC ranges"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "ment\nindividuals, we are only provided labels for 2 minutes",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "1\nbetween −1\nand\n1, where\na\nvalue\nof\nindicates\nperfect"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "of conversation of each testing individual. That\nis,\nfor each",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "0\nagreement,\nindicates\nno\nagreement,\nand −1\nindicates"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "testing individual,\nthe first\nlabeled minute is for personalised",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "perfect disagreement. The CCC loss is formulated as follows:"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "training and the\nsecond minute\nis\nfor personalised devel-",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "opment/validation. This\nallows us\nto conduct personalised",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "after\ntraining a\nfine-tuning for each testing individual DIi",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "Loss = 1 − CCC\n(4)"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "generic model on DG. A summary of the available labels for",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "the train-test split\nis provided in table II. Notably,\nthe testing",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "2ρσxσy"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "CCC =\n(5)"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "portion of\nthe testing set\nlacks\nlabels, and our objective in",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "x + σ2\ny + (µx − µy)2"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "the MuSe-Personalisation challenge is\nto accurately predict",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "the valence and arousal\nlabels for\nthis unlabeled part of\nthe",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "represent\nthe mean of\nthe prediction ˆy\nwhere µˆy\nand µy"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "testing set.",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "and the label y, respectively. Similarly, σˆy and σy correspond"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Features.\nFor\nthe different modalities, pre-extracted fea-",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "to their\nstandard deviations. ρ denotes\nthe PCC between ˆy"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "ture sets were made available by [14]. These are summarised",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "and y. It measures the strength and direction of the linear re-"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "in\ntable\nI. Among\nthe\nvideo\nfeatures, FAU most\ndirectly",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "lationship between X and Y . A value of ρ close to 1 indicates"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "encodes facial action units [83], while FACENET512 features",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "a strong positive linear relationship, while a value close to −1"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "are associated with deep facial features [58]. While VIT [20]",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "indicates a strong negative linear\nrelationship. A value of 0"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "2σxσy"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "features\nencode\ninformation about visual\nsemantics,\nthese",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "indicates no linear relationship. In addition,"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "σ2\nx+σ2\ny+(µx−µy)2"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "semantics may not be\nfacial\ninformation. This\nis because",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "denotes the BCF.\nIt corrects for any deviation of\nthe means"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "the VIT model\nis pre-trained on Imagenet\n[18] using DINO",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "of X and Y from a perfect\nlinear relationship. It ensures that"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "[11] self-supervision. Among the audio features, EGEMAPS",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "the CCC accounts for the accuracy component as well. BCF"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "[21] are based on a set of\ninterpretable speech parameters,",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "ranges between 0 and 1, where a value of 1 indicates perfect"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "which can be computed in closed form from the data. Each",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "accuracy."
        },
        {
          "TABLE I: Understanding the features in our dataset.": "EGEMAPS dimension corresponds\nto a parameter\nsuch as",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "pitch,\nshimmer,\nloudness, or\nalpha-ratio. DEEPSPECTRUM",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "as\na\nTABLE III: Performance of Generic models on DI ,"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "and WAV2VEC features\nare\nextracted via neural network.",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "benchmark prior\nto personalisation. Yellow: maximum and"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "However, DEEPSPECTRUM [6]\nfeatures are extracted from",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "minimum scores. Bold: Best performing feature in the spe-"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "spectrogram images using a\nconvolutional neural network",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "cific modality. Bold: Best performance overall\nthe features."
        },
        {
          "TABLE I: Understanding the features in our dataset.": "(CNN), while WAV2VEC [76]\nfeatures\nare\nextracted from",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "audio waveforms using a combination of CNNs and trans-",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "Arousal\nValence\nCombined"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "Feature"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "formers. BERT-4 [19]\nfeatures are extracted using a BERT",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "Devel\nI\nDevel\nI\nTest\nTest\nTest"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "transformer, which\nhas\nbeen\npretrained\non\nthe German",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "0.56\nFACENET512\n0.43\n0.49\n0.76\n0.52"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "language. The BIOSIGNALS\nfeatures\nconsist\nof machine-",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "FAU\n0.21\n-0.00\n0.52\n0.47\n0.23"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "recorded readings of electrocardiograms (ECG), Respiration",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "0.51\nVIT\n0.61\n0.40\n0.40\n0.45"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "(RESP) and heart\nrate beats per minute (BPM).",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "0.56\n0.49\nEGEMAPS\n0.58\n0.58\n0.52"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "DEEPSPECTRUM\n0.54\n0.42\n0.57\n0.45\n0.44"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "C. Loss Function and Evaluation Metric",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "WAV2VEC\n0.52\n0.54\n0.46\n0.42\n0.48"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "In line with [14], we adopt\nthe Concordance Correlation",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "BERT-4\n0.51\n0.49\n0.34\n0.25\n0.37"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "Coefficient (CCC) as the loss function to train our model. It",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        },
        {
          "TABLE I: Understanding the features in our dataset.": "",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": "BIOSIGNALS\n-0.14\n-0.14\n0.23\n0.19\n0.03"
        },
        {
          "TABLE I: Understanding the features in our dataset.": "1https://py-feat.org/pages/au reference.html",
          "TABLE\nII: Understanding\nthe\nstructure\nof\nthe": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "Best performance overall"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "Feature"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "FACENET512"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "FAU"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "VIT"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "EGEMAPS"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "DEEPSPECTRUM"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "WAV2VEC"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "BERT-4"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": "BIOSIGNALS"
        },
        {
          "Bold: Best performing feature in the specific modality. Bold:": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "to highlight\nthe\neffect of personalisation. Yellow:\non DI ,"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "greatest increases due to personalisation w.r.t generic models."
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "Bold: Best performing feature in the specific modality. Bold:"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "Best performance overall\nthe features."
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "Arousal\nValence\nCombined"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "Feature"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "Devel\nI\nDevel\nI\nTest\nTest\nTest"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "FACENET512\n0.88\n0.68\n0.89\n0.69\n0.69"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "0.79\n0.78\nFAU\n0.88\n0.90\n0.79"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "VIT\n0.85\n0.64\n0.67\n0.55\n0.60"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "EGEMAPS\n0.70\n0.53\n0.68\n0.59\n0.56"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "DEEPSPECTRUM\n0.90\n0.55\n0.72\n0.56\n0.55"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "0.80\nWAV2VEC\n0.89\n0.53\n0.90\n0.66"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "0.76\nBERT-4\n0.90\n0.87\n0.60\n0.68"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "BIOSIGNALS\n0.78\n0.61\n0.45\n0.35\n0.48"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "D. Experimental Details"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "Baseline Models. The baseline work of\n[14], while\nin-"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "sightful, only reports results for 6 audio and video features,"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "namely, Fau, FaceNet512, ViT, eGeMAPS, DeepSpectrum,"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "and Wav2Vec. We conduct all our experiments on these fea-"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "tures, but also include results for\nthe BERT-4, and BIOSIG-"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "NALS features. We also employ the same generic model from"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "the baseline methodology for consistent comparison."
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "Hyperparameters. DWA was used to augment each in-"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "fair comparison with\ndividual dataset and obtain DIaug. For"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "the baseline, we set consistent\nranges for any tuned hyerpa-"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "rameters such as learning rates, number of RNN layers, and"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "model dimensions, during hyperparameter search. All other"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "hyperparameters,\nincluding early stopping patience, segment"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "length, and hop length, are kept unchanged from the baseline"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "method in [14]. However, we perform the hyperparameter"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "search for all 8 features in the dataset, expanding upon the"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "6 reported features of\nthe [14]."
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "Our experiments sought to understand the impact of differ-"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "ent choices of distance metrics and numbers of augmentation"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "samples per\nsegment. We\ntrained models using data\naug-"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "mented with one of\nthe three distance metrics: centroid DP,"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "cosine, and centroid L2. We also explored varying values for"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "number of augmentation samples per segment n, specifically"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "n = 1 up to 3."
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "V. RESULTS AND DISCUSSION"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "We organise our discussion into 2 sections. Firstly, we seek"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "to highlight\nthe experimental effects of personalisation. To"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "do this, we\nreproduce\nthe generic model\nand personalised"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "model\nresults\nof\nthe\nbaseline\npaper\non\nour\ndataset,\nand"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "compare\ntheir\nrespective\nperformances\non\nthe\ntesting\nset"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "(section V-A). This is particularly important in the context of"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "personalisation, as the generic model results on the testing set"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": ""
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "were not provided in the baseline work [14]. Secondly, we"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "perform a series of experiments to evaluate the performance"
        },
        {
          "TABLE IV: Performance of\nbaseline\npersonalised models": "of\nthe DWA personalisation\napproach\n(section V-B). Our"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "Feature"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "FACENET512"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "FAU"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "VIT"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "EGEMAPS"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "DEEPSPECTRUM"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "WAV2VEC"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "BERT-4"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": "BIOSIGNALS"
        },
        {
          "TABLE V: Performance of DWA with exploration of n and distance metrics. Green: DWA outperforms": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "from late fusion,\nfor": "",
          "features where DWA showed improvements": "",
          "in ablation. Yellow:": "Combined"
        },
        {
          "from late fusion,\nfor": "Feature",
          "features where DWA showed improvements": "",
          "in ablation. Yellow:": ""
        },
        {
          "from late fusion,\nfor": "",
          "features where DWA showed improvements": "Devel",
          "in ablation. Yellow:": "Test"
        },
        {
          "from late fusion,\nfor": "",
          "features where DWA showed improvements": "Baseline(s)",
          "in ablation. Yellow:": ""
        },
        {
          "from late fusion,\nfor": "FACENET512 + VIT",
          "features where DWA showed improvements": "0.90",
          "in ablation. Yellow:": "0.70"
        },
        {
          "from late fusion,\nfor": "Reproduced (FACENET512 + EGEMAPS)",
          "features where DWA showed improvements": "0.85",
          "in ablation. Yellow:": "0.72"
        },
        {
          "from late fusion,\nfor": "Reported (FACENET512 + EGEMAPS)",
          "features where DWA showed improvements": "0.92",
          "in ablation. Yellow:": "0.76"
        },
        {
          "from late fusion,\nfor": "",
          "features where DWA showed improvements": "Late Fusion, Proportional Weighting",
          "in ablation. Yellow:": ""
        },
        {
          "from late fusion,\nfor": "VIT+ EGEMAPS + DWA",
          "features where DWA showed improvements": "0.92",
          "in ablation. Yellow:": "0.61"
        },
        {
          "from late fusion,\nfor": "FACENET512 + EGEMAPS+DWA",
          "features where DWA showed improvements": "0.89",
          "in ablation. Yellow:": "0.71"
        },
        {
          "from late fusion,\nfor": "VIT+BERT-4+DWA",
          "features where DWA showed improvements": "0.93",
          "in ablation. Yellow:": "0.72"
        },
        {
          "from late fusion,\nfor": "VIT+DEEPSPECTRUM+DWA",
          "features where DWA showed improvements": "0.94",
          "in ablation. Yellow:": "0.73"
        },
        {
          "from late fusion,\nfor": "Facenet+VIT+DWA",
          "features where DWA showed improvements": "0.92",
          "in ablation. Yellow:": "0.78"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "valence\ntasks\nindependently. Our\nexperiments\nsuggest\nthat",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "1 to 2, and decreases when n is further\nincreased to 3. This"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "an improvement\nin valence prediction due to DWA, may not",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "suggests a diminishing return pattern Such is also the case"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "guarantee\nan improvement\nin arousal prediction,\nand vice",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "for BIOSIGNALS\nfeature. On the other hand, performance"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "versa. However,\nfor\nfeature-task combinations where DWA",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "on FACENET512, with cosine feature, seems to be steadily"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "does result in improvement, such improvements are generally",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "increasing with n. This suggests that\nthe value of n must be"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "seen among all distance metrics, and for most values of n,",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "tuned uniquely as a hyperparamter for each feature-distance-"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "as can be observed by the neat patterns of green highlighting",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "metric combination."
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "seen in the table. For\ninstance, while the DEEPSPECTRUM",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "Late Fusion. Finally, we adopt\nthe same late fusion strat-"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "feature improves on valence to a score of 0.72,\nit\nfails\nto",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "egy deployed in our baseline,\nfor comparison purposes. For"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "improve on arousal, peaking at 0.59. On the other hand,\nthe",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "late fusion, we utilise proportional weighting, whereby two"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "EGEMMAPS feature is able to improve on both arousal and",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "sets of predictions are weighted based on their performance"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "valence,\nreaching scores of 0.71 and 0.68 respectively.",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "on\nthe\ndevelopment\nset. For\nfair\ncomparison, we\ninclude"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "Video & Physiological Features. Among the video fea-",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "the\nreported\nresults\nfor\nlate\nfusion\nfrom the\nbaseline,\nof"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "tures, only FACENET512 and VIT features seem to benefit",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "FACENET512 and EGEMAPS features. However,\nin repro-"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "from DWA, with\nthe\nlatter\nseeing\nthe most\nimprovement",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "ducing\nthese\nresults, we\nutilise\nconsistent\nhyperparameter"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "compared\nto\nthe\nbaseline model.\nFor\nFACENET512,\nthe",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "search\nbetween\nall methods,\ncausing\nthe\nreproduced\nlate"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "combined testing CCC peaks at 0.74 with DWA, compared to",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "fusion score to be different\nfrom the reported baseline. The"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "0.69 without DWA (baseline model). Much of this improve-",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "results are presented in table VI."
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "ment is due to the valence prediction, which increases from a",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "In the baseline work,\nthe best performing feature fusion is"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "testing CCC of 0.69 to 0.83. Arousal prediction does not see",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "of FACENET512 and EGEMAPS,\nreaching a testing CCC"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "similar\nimprovement\nfor FACENET512. With VIT features,",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "of\n0.76. Our\nreproduced\nscore\nis\nslightly\nlower,\nat\n0.72."
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "the final combined testing CCC increases from 0.60 to 0.67.",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "However, our DWA method results in a score of 0.78 when"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "This\nis\ndue\nto\nsignificant\nimprovements\nin\nboth\narousal",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "fusing VIT and EGEMAPS features. This\nimprovement\nis"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "prediction from 0.64 to 0.71, and in valence prediction from",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "significant,\nas without DWA,\nthe\nlate\nfusion\nof VIT and"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "0.55 to 0.65 with respect\nto the baseline. The FAU features,",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "EGEMAPS would only result\nin a CCC testing score of"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "which had the highest performance without DWA,\nsaw a",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "0.70. As expected, the fusion of EGEMAPS with VIT scores"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "drop in performance with DWA. This\nsuggests\nthat DWA",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "higher\nthan with FACENET512,\nas\nthe\nlatter\ndid\nnot\nsee"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "can only be used as a tool\nto improve the performance of",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "signficant benefit\nin arousal prediction from DWA, as earlier"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "features which are lagging behind on specific tasks.\nIndeed,",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "discussed. However,\nthis but\nreinforces our hypothesis,\nthat"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "this hypothesis\nis\nfurther\nsupported by the improvement\nin",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "DWA can\nbe\nquite\nuseful\nas\na\nboost\nto\npersonalisation"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "BIOSIGNALS features testing CCC. With the use of DWA a",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "performance for\nfeatures which are not performing well."
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "compelling performance increase can be seen from 0.35 to",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": ""
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "0.69 for\nthe arousal prediction task.",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": ""
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "VI. CONCLUSIONS AND FUTURE WORKS"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "Audio\nFeatures. Results with\naudio\nfeatures\nare\nalso",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": ""
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "presented in table V. The performance increase due to DWA",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": ""
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "is most\nnoticeable\nusing\nthe\nEGEMAPS feature. For\nthe",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "In\nthis\npaper, we\nintroduced\nthe Distance Weighting"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "EGEMAPS, DWA brings about an increase in performance",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "Augmentation (DWA) method for enhancing predictions of"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "from 0.56 to 0.70. This\nis due\nto corresponding improve-",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "valence and arousal, at\nthe data level. In simple terms, DWA"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "ments in testing arousal and valence CCC, which increased",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "expands a personalised dataset with samples\nfrom a global"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "from 0.50 and 0.59 respectively,\nto 0.71 and 0.68 respec-",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "dataset\nthat\nare\nthe most\nsimilar. We\nimplement DWA on"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "tively. This combined increase in EGEMAPS performance",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "all 8 features provided in our\ntarget dataset, with the best"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "with\nto DWA is\nnot\nonly\nthe\nhighest\namong\nthe\naudio",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "performance observed from a fusion of VIT and EGEMAPS"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "features, but among all features which were tested. DWA also",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "predictions. Among\nthe\n3\ndistance metrics\nemployed\nfor"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "results in an improved performance in the DEEPSPECTRUM",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "similarity measurement, all proved to be effective for either"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "feature\nfrom 0.56\nto\n0.63. However,\nthe\nimprovement\nis",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "valence\nor\narousal,\nor\nboth,\nin\n4\nout\nof\n8\nfeatures. Our"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "largely limited to the valence prediction task. As\na\nresult,",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "findings\nindicate\nthat DWA is most\nuseful\nin\nimproving"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "since\nthe\narousal prediction of DEEPSPECTRUM does not",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "personalisation performance when utilising features which"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "appear to benefit from DWA,\nthe combined CCC score only",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "have\npoor\nperformance\nfor\na\nparticular\ntask. Late\nfusion"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "improves from 0.55 to 0.60.",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "of\nthe DWA-trained VIT and EGEMAPS models, yields a"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "Number of Augmentations per segment. We ran experi-",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "combined CCC score of 0.78,\nindicating the effectiveness of"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "ments on the value of n to determine the effect of the number",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "DWA, and opening the door\nfor\nfurther\nresearch into data-"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "of augmentation samples on model performance. What we",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "level personalisation."
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "find, as per\ntable V,\nis\nthat\nthe ideal number of augmenta-",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "Future work\ncould\nexpand\nupon DWA in\n3\nkey ways."
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "tion samples varies depending on the feature\nand distance",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "Firstly, we\ncan\nexplore\nfurther\ndistance metrics\nto\nsee\nif"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "metric. For example, consider the case of valence prediction",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "they could improve over\nthe 3 used thus\nfar. Secondly,\nit"
        },
        {
          "is\nthat DWA may\nresult\nin\nimprovement\nin\narousal\nor": "using DEEPSPECTRUM features\nand\ncosine\ndistance. The",
          "I\nincreases\nas n is\nincreasing from\nperformance on Devel": "may be beneficial\nto explore fractional weighting of similar"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "information\nfor\nefficient\nself-supervised\nemotion\nrecognition with"
        },
        {
          "samples. This could be implemented via sample weighting,": "whereby we would alternate between normal\nsamples\nand",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "audio-textual distilled models. arXiv preprint arXiv:2305.19184, 2023."
        },
        {
          "samples. This could be implemented via sample weighting,": "augmentation samples within a training loop.\nIn effect,\nthis",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[18]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet:"
        },
        {
          "samples. This could be implemented via sample weighting,": "could allow us to weight samples in the interval [0,1] rather",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "A large-scale hierarchical image database. In 2009 IEEE conference on"
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "computer vision and pattern recognition, pages 248–255.\nIeee, 2009."
        },
        {
          "samples. This could be implemented via sample weighting,": "than relying solely on binary weights. Thirdly, it is important",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[19]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training"
        },
        {
          "samples. This could be implemented via sample weighting,": "to\ninvestigate\nthe\nreasons\nfor which DWA brings\nlimited",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "arXiv\nof deep bidirectional\ntransformers for\nlanguage understanding."
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "preprint arXiv:1810.04805, 2018."
        },
        {
          "samples. This could be implemented via sample weighting,": "performance\nto features which already performed well.\nIn",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,"
        },
        {
          "samples. This could be implemented via sample weighting,": "effect, modifications\nto\nthe\nalgorithm itself, may\nhelp\nin",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al."
        },
        {
          "samples. This could be implemented via sample weighting,": "expanding upon its applicability to different feature sets. This",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "An image is worth 16x16 words: Transformers for\nimage recognition"
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "at scale. arXiv preprint arXiv:2010.11929, 2020."
        },
        {
          "samples. This could be implemented via sample weighting,": "last point will be easier to explore as we seek to apply DWA",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[21]\nF. Eyben, K. R. Scherer, B. W. Schuller,\nJ. Sundberg, E. Andr´e,"
        },
        {
          "samples. This could be implemented via sample weighting,": "to additional datasets for dimensional emotion prediction.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "C. Busso, L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan, et al."
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "The geneva minimalistic\nacoustic parameter\nset\n(gemaps)\nfor voice"
        },
        {
          "samples. This could be implemented via sample weighting,": "REFERENCES",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "IEEE transactions\non\naffective\nresearch\nand\naffective\ncomputing."
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "computing, 7(2):190–202, 2015."
        },
        {
          "samples. This could be implemented via sample weighting,": "[1] A. R. Abbasi, N. V. Afzulpurkar, and T. Uno. Towards emotionally-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[22] M. A. Hasnul, N. A. A. Aziz, S. Alelyani, M. Mohana, and A. A."
        },
        {
          "samples. This could be implemented via sample weighting,": "personalized computing: Dynamic prediction of student mental states",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Aziz. Electrocardiogram-based emotion recognition systems and their"
        },
        {
          "samples. This could be implemented via sample weighting,": "2009\nInternational\nfrom self-manipulatory\nbody movements.\nIn",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "applications in healthcare—a review. Sensors, 21(15):5015, 2021."
        },
        {
          "samples. This could be implemented via sample weighting,": "Conference on Emerging Technologies, pages 235–240.\nIEEE, 2009.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[23] A. R. Hazourli, A. Djeghri, H. Salam, and A. Othmani. Multi-facial"
        },
        {
          "samples. This could be implemented via sample weighting,": "[2] N. Ahmed, Z. Al Aghbari,\nand S. Girija.\nA systematic\nsurvey on",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "patches\naggregation\nnetwork\nfor\nfacial\nexpression\nrecognition\nand"
        },
        {
          "samples. This could be implemented via sample weighting,": "Intelligent\nmultimodal emotion recognition using learning algorithms.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "facial regions contributions to emotion display. Multimedia Tools and"
        },
        {
          "samples. This could be implemented via sample weighting,": "Systems with Applications, 17:200171, 2023.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Applications, 80:13639–13662, 2021."
        },
        {
          "samples. This could be implemented via sample weighting,": "[3]\nF. S. Al-Anzi\nand D. AbuZeina.\nToward an enhanced arabic\ntext",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[24] A. Huang et\nal.\nSimilarity measures\nfor\ntext document\nclustering."
        },
        {
          "samples. This could be implemented via sample weighting,": "classification\nusing\ncosine\nsimilarity\nand\nlatent\nsemantic\nindexing.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "the sixth new zealand computer\nscience research\nIn Proceedings of"
        },
        {
          "samples. This could be implemented via sample weighting,": "Journal of King Saud University-Computer and Information Sciences,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "student conference (NZCSRSC2008), Christchurch, New Zealand, vol-"
        },
        {
          "samples. This could be implemented via sample weighting,": "29(2):189–195, 2017.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "ume 4, pages 9–56, 2008."
        },
        {
          "samples. This could be implemented via sample weighting,": "[4] N. Alswaidan and M. E. B. Menai. A survey of\nstate-of-the-art ap-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[25] N.\nJaques, S. Taylor, E. Nosakhare, A. Sano, and R. Picard. Multi-"
        },
        {
          "samples. This could be implemented via sample weighting,": "proaches for emotion recognition in text. Knowledge and Information",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "task learning for predicting health,\nstress,\nand happiness.\nIn NIPS"
        },
        {
          "samples. This could be implemented via sample weighting,": "Systems, 62:2937–2987, 2020.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Workshop on Machine Learning for Healthcare, 2016."
        },
        {
          "samples. This could be implemented via sample weighting,": "[5] N. Alyuz, E. Okur, E. Oktay, U. Genc, S. Aslan, S. E. Mete, B. Arn-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[26] M. Jensen.\nPersonality traits and nonverbal communication patterns."
        },
        {
          "samples. This could be implemented via sample weighting,": "rich,\nand A. A. Esme.\nSemi-supervised model personalization for",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Int’l J. Soc. Sci. Stud., 4:57, 2016."
        },
        {
          "samples. This could be implemented via sample weighting,": "improved detection of learner’s emotional engagement. In Proceedings",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[27] A. Kasparova, O. Celiktutan,\nand M. Cukurova.\nInferring student"
        },
        {
          "samples. This could be implemented via sample weighting,": "of\nthe 18th ACM International Conference on Multimodal Interaction,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "engagement\nin collaborative problem solving from visual\ncues.\nIn"
        },
        {
          "samples. This could be implemented via sample weighting,": "pages 100–107, 2016.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Companion Publication\nof\nthe\n2020\nInternational Conference\non"
        },
        {
          "samples. This could be implemented via sample weighting,": "[6]\nS. Amiriparian, M. Gerczuk,\nS. Ottl, N. Cummins, M.\nFreitag,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Multimodal\nInteraction, pages 177–181, 2020."
        },
        {
          "samples. This could be implemented via sample weighting,": "S. Pugachevskiy, A. Baird, and B. Schuller. Snore sound classification",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[28] A. Kathan, S. Amiriparian, L. Christ, A. Triantafyllopoulos, N. M¨uller,"
        },
        {
          "samples. This could be implemented via sample weighting,": "using image-based deep spectrum features. 2017.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "A. K¨onig, and B. W. Schuller. A personalised approach to audiovisual"
        },
        {
          "samples. This could be implemented via sample weighting,": "[7] D. Ayata, Y. Yaslan,\nand M. E. Kamasak.\nEmotion\nrecognition",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "humour\nrecognition and its\nindividual-level\nfairness.\nIn Proceedings"
        },
        {
          "samples. This could be implemented via sample weighting,": "from multimodal physiological\nsignals\nfor emotion aware healthcare",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "of\nthe 3rd International on Multimodal Sentiment Analysis Workshop"
        },
        {
          "samples. This could be implemented via sample weighting,": "systems. Journal of Medical and Biological Engineering, 40:149–157,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "and Challenge, pages 29–36, 2022."
        },
        {
          "samples. This could be implemented via sample weighting,": "2020.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[29]\nJ. Kim, E. Andr´e, and T. Vogt. Towards user-independent classification"
        },
        {
          "samples. This could be implemented via sample weighting,": "[8]\nP. Barros, G. Parisi, and S. Wermter. A personalized affective memory",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "of multimodal emotional signals. In 2009 3rd International Conference"
        },
        {
          "samples. This could be implemented via sample weighting,": "model for improving emotion recognition. In International Conference",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "on Affective Computing and Intelligent\nInteraction and Workshops,"
        },
        {
          "samples. This could be implemented via sample weighting,": "on Machine Learning, pages 485–494. PMLR, 2019.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "pages 1–7.\nIEEE, 2009."
        },
        {
          "samples. This could be implemented via sample weighting,": "[9]\nP. Barros and A. Sciutti. Ciao! a contrastive adaptation mechanism for",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[30] D. Kollias,\nP.\nTzirakis, A. Baird, A. Cowen,\nand\nS.\nZafeiriou."
        },
        {
          "samples. This could be implemented via sample weighting,": "non-universal facial expression recognition. In 2022 10th International",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Abaw: Valence-arousal estimation, expression recognition, action unit"
        },
        {
          "samples. This could be implemented via sample weighting,": "Conference on Affective Computing and Intelligent Interaction (ACII),",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "detection & emotional\nreaction intensity estimation challenges.\nIn"
        },
        {
          "samples. This could be implemented via sample weighting,": "pages 1–8.\nIEEE, 2022.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Proceedings of\nthe\nIEEE/CVF Conference on Computer Vision and"
        },
        {
          "samples. This could be implemented via sample weighting,": "[10] O. Bruna, H. Avetisyan, and J. Holub.\nEmotion models\nfor\ntextual",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Pattern Recognition, pages 5888–5897, 2023."
        },
        {
          "samples. This could be implemented via sample weighting,": "Journal\nof\nphysics:\nconference\nemotion\nclassification.\nIn\nseries,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[31] D. Leyzberg, S. Spaulding,\nand B. Scassellati.\nPersonalizing robot"
        },
        {
          "samples. This could be implemented via sample weighting,": "volume 772, page 012063.\nIOP Publishing, 2016.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "of\nthe\ntutors\nto\nindividuals’\nlearning\ndifferences.\nIn Proceedings"
        },
        {
          "samples. This could be implemented via sample weighting,": "[11] M. Caron, H. Touvron,\nI. Misra, H. J´egou, J. Mairal, P. Bojanowski,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "2014 ACM/IEEE international\nconference on Human-robot\ninterac-"
        },
        {
          "samples. This could be implemented via sample weighting,": "and A.\nJoulin.\nEmerging properties\nin self-supervised vision trans-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "tion, pages 423–430, 2014."
        },
        {
          "samples. This could be implemented via sample weighting,": "the IEEE/CVF international conference\nformers.\nIn Proceedings of",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[32] B. Li and L. Han. Distance weighted cosine similarity measure for"
        },
        {
          "samples. This could be implemented via sample weighting,": "on computer vision, pages 9650–9660, 2021.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "text\nclassification.\nIn Intelligent Data Engineering and Automated"
        },
        {
          "samples. This could be implemented via sample weighting,": "[12] R. Chattopadhyay, Q. Sun, W. Fan,\nI. Davidson, S. Panchanathan,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Learning–IDEAL 2013: 14th International Conference,\nIDEAL 2013,"
        },
        {
          "samples. This could be implemented via sample weighting,": "and J. Ye. Multisource domain adaptation and its application to early",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Hefei, China, October 20-23, 2013. Proceedings 14, pages 611–618."
        },
        {
          "samples. This could be implemented via sample weighting,": "detection of fatigue. ACM Transactions on Knowledge Discovery from",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Springer, 2013."
        },
        {
          "samples. This could be implemented via sample weighting,": "Data (TKDD), 6(4):1–26, 2012.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[33]\nJ. Li, A. Waleed,\nand H.\nSalam.\nA survey\non\npersonalized\naf-"
        },
        {
          "samples. This could be implemented via sample weighting,": "[13] V. V. Chithrra Raghuram, H. Salam,\nJ. Nasir, B. Bruno, and O. Ce-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "arXiv\npreprint\nfective\ncomputing\nin\nhuman-machine\ninteraction."
        },
        {
          "samples. This could be implemented via sample weighting,": "liktutan.\nPersonalized productive\nengagement\nrecognition in robot-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "arXiv:2304.00377, 2023."
        },
        {
          "samples. This could be implemented via sample weighting,": "the 2022 Interna-\nmediated collaborative learning.\nIn Proceedings of",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[34]\nL. Li, H. Zhu, S. Zhao, G. Ding, and W. Lin.\nPersonality-assisted"
        },
        {
          "samples. This could be implemented via sample weighting,": "tional Conference on Multimodal\nInteraction, pages 632–641, 2022.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "multi-task\nlearning\nfor\ngeneric\nand\npersonalized\nimage\naesthetics"
        },
        {
          "samples. This could be implemented via sample weighting,": "[14]\nL. Christ, S. Amiriparian, A. Baird, A. Kathan, N. M¨uller, S. Klug,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "assessment.\nIEEE Transactions on Image Processing, 29:3898–3910,"
        },
        {
          "samples. This could be implemented via sample weighting,": "C. Gagne,\nP.\nTzirakis,\nE.-M. Meßner, A. K¨onig,\net\nal.\nThe",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "2020."
        },
        {
          "samples. This could be implemented via sample weighting,": "muse 2023 multimodal sentiment analysis challenge: Mimicked emo-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[35] G. Liang, S. Wang,\nand C. Wang.\nPose-aware\nadversarial domain"
        },
        {
          "samples. This could be implemented via sample weighting,": "arXiv\npreprint\ntions,\ncross-cultural\nhumour,\nand\npersonalisation.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "arXiv\nadaptation\nfor\npersonalized\nfacial\nexpression\nrecognition."
        },
        {
          "samples. This could be implemented via sample weighting,": "arXiv:2305.03369, 2023.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "preprint arXiv:2007.05932, 2020."
        },
        {
          "samples. This could be implemented via sample weighting,": "[15] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer machine",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[36] X. Liu, L. Sun, W.\nJiang, F. Zhang, Y. Deng, Z. Huang, L. Meng,"
        },
        {
          "samples. This could be implemented via sample weighting,": "IEEE transactions\non\nfor\npersonalized\nfacial\nexpression\nanalysis.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Y\n. Liu,\nand C. Liu.\nEvaef: Ensemble\nvalence-arousal\nestimation"
        },
        {
          "samples. This could be implemented via sample weighting,": "pattern analysis and machine intelligence, 39(3):529–545, 2016.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "the IEEE/CVF Conference\nframework in the wild.\nIn Proceedings of"
        },
        {
          "samples. This could be implemented via sample weighting,": "[16]\nS. Cunningham, H. Ridley, J. Weinel, and R. Picking. Audio emotion",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "on Computer Vision and Pattern Recognition, pages 5862–5870, 2023."
        },
        {
          "samples. This could be implemented via sample weighting,": "recognition\nusing machine\nlearning\nto\nsupport\nsound\ndesign.\nIn",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[37] D. Lopez-Martinez, K. Peng, S. C. Steele, A. J. Lee, D. Borsook, and"
        },
        {
          "samples. This could be implemented via sample weighting,": "Proceedings of\nthe 14th International Audio Mostly Conference: A",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "R. Picard. Multi-task multiple kernel machines for personalized pain"
        },
        {
          "samples. This could be implemented via sample weighting,": "Journey in Sound, pages 116–123, 2019.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "information\nfor\nefficient\nself-supervised\nemotion\nrecognition with"
        },
        {
          "samples. This could be implemented via sample weighting,": "whereby we would alternate between normal\nsamples\nand",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "audio-textual distilled models. arXiv preprint arXiv:2305.19184, 2023."
        },
        {
          "samples. This could be implemented via sample weighting,": "augmentation samples within a training loop.\nIn effect,\nthis",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[18]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet:"
        },
        {
          "samples. This could be implemented via sample weighting,": "could allow us to weight samples in the interval [0,1] rather",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "A large-scale hierarchical image database. In 2009 IEEE conference on"
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "computer vision and pattern recognition, pages 248–255.\nIeee, 2009."
        },
        {
          "samples. This could be implemented via sample weighting,": "than relying solely on binary weights. Thirdly, it is important",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[19]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training"
        },
        {
          "samples. This could be implemented via sample weighting,": "to\ninvestigate\nthe\nreasons\nfor which DWA brings\nlimited",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "arXiv\nof deep bidirectional\ntransformers for\nlanguage understanding."
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "preprint arXiv:1810.04805, 2018."
        },
        {
          "samples. This could be implemented via sample weighting,": "performance\nto features which already performed well.\nIn",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,"
        },
        {
          "samples. This could be implemented via sample weighting,": "effect, modifications\nto\nthe\nalgorithm itself, may\nhelp\nin",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al."
        },
        {
          "samples. This could be implemented via sample weighting,": "expanding upon its applicability to different feature sets. This",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "An image is worth 16x16 words: Transformers for\nimage recognition"
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "at scale. arXiv preprint arXiv:2010.11929, 2020."
        },
        {
          "samples. This could be implemented via sample weighting,": "last point will be easier to explore as we seek to apply DWA",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[21]\nF. Eyben, K. R. Scherer, B. W. Schuller,\nJ. Sundberg, E. Andr´e,"
        },
        {
          "samples. This could be implemented via sample weighting,": "to additional datasets for dimensional emotion prediction.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "C. Busso, L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan, et al."
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "The geneva minimalistic\nacoustic parameter\nset\n(gemaps)\nfor voice"
        },
        {
          "samples. This could be implemented via sample weighting,": "REFERENCES",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "IEEE transactions\non\naffective\nresearch\nand\naffective\ncomputing."
        },
        {
          "samples. This could be implemented via sample weighting,": "",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "computing, 7(2):190–202, 2015."
        },
        {
          "samples. This could be implemented via sample weighting,": "[1] A. R. Abbasi, N. V. Afzulpurkar, and T. Uno. Towards emotionally-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[22] M. A. Hasnul, N. A. A. Aziz, S. Alelyani, M. Mohana, and A. A."
        },
        {
          "samples. This could be implemented via sample weighting,": "personalized computing: Dynamic prediction of student mental states",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Aziz. Electrocardiogram-based emotion recognition systems and their"
        },
        {
          "samples. This could be implemented via sample weighting,": "2009\nInternational\nfrom self-manipulatory\nbody movements.\nIn",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "applications in healthcare—a review. Sensors, 21(15):5015, 2021."
        },
        {
          "samples. This could be implemented via sample weighting,": "Conference on Emerging Technologies, pages 235–240.\nIEEE, 2009.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[23] A. R. Hazourli, A. Djeghri, H. Salam, and A. Othmani. Multi-facial"
        },
        {
          "samples. This could be implemented via sample weighting,": "[2] N. Ahmed, Z. Al Aghbari,\nand S. Girija.\nA systematic\nsurvey on",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "patches\naggregation\nnetwork\nfor\nfacial\nexpression\nrecognition\nand"
        },
        {
          "samples. This could be implemented via sample weighting,": "Intelligent\nmultimodal emotion recognition using learning algorithms.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "facial regions contributions to emotion display. Multimedia Tools and"
        },
        {
          "samples. This could be implemented via sample weighting,": "Systems with Applications, 17:200171, 2023.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Applications, 80:13639–13662, 2021."
        },
        {
          "samples. This could be implemented via sample weighting,": "[3]\nF. S. Al-Anzi\nand D. AbuZeina.\nToward an enhanced arabic\ntext",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[24] A. Huang et\nal.\nSimilarity measures\nfor\ntext document\nclustering."
        },
        {
          "samples. This could be implemented via sample weighting,": "classification\nusing\ncosine\nsimilarity\nand\nlatent\nsemantic\nindexing.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "the sixth new zealand computer\nscience research\nIn Proceedings of"
        },
        {
          "samples. This could be implemented via sample weighting,": "Journal of King Saud University-Computer and Information Sciences,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "student conference (NZCSRSC2008), Christchurch, New Zealand, vol-"
        },
        {
          "samples. This could be implemented via sample weighting,": "29(2):189–195, 2017.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "ume 4, pages 9–56, 2008."
        },
        {
          "samples. This could be implemented via sample weighting,": "[4] N. Alswaidan and M. E. B. Menai. A survey of\nstate-of-the-art ap-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[25] N.\nJaques, S. Taylor, E. Nosakhare, A. Sano, and R. Picard. Multi-"
        },
        {
          "samples. This could be implemented via sample weighting,": "proaches for emotion recognition in text. Knowledge and Information",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "task learning for predicting health,\nstress,\nand happiness.\nIn NIPS"
        },
        {
          "samples. This could be implemented via sample weighting,": "Systems, 62:2937–2987, 2020.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Workshop on Machine Learning for Healthcare, 2016."
        },
        {
          "samples. This could be implemented via sample weighting,": "[5] N. Alyuz, E. Okur, E. Oktay, U. Genc, S. Aslan, S. E. Mete, B. Arn-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[26] M. Jensen.\nPersonality traits and nonverbal communication patterns."
        },
        {
          "samples. This could be implemented via sample weighting,": "rich,\nand A. A. Esme.\nSemi-supervised model personalization for",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Int’l J. Soc. Sci. Stud., 4:57, 2016."
        },
        {
          "samples. This could be implemented via sample weighting,": "improved detection of learner’s emotional engagement. In Proceedings",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[27] A. Kasparova, O. Celiktutan,\nand M. Cukurova.\nInferring student"
        },
        {
          "samples. This could be implemented via sample weighting,": "of\nthe 18th ACM International Conference on Multimodal Interaction,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "engagement\nin collaborative problem solving from visual\ncues.\nIn"
        },
        {
          "samples. This could be implemented via sample weighting,": "pages 100–107, 2016.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Companion Publication\nof\nthe\n2020\nInternational Conference\non"
        },
        {
          "samples. This could be implemented via sample weighting,": "[6]\nS. Amiriparian, M. Gerczuk,\nS. Ottl, N. Cummins, M.\nFreitag,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Multimodal\nInteraction, pages 177–181, 2020."
        },
        {
          "samples. This could be implemented via sample weighting,": "S. Pugachevskiy, A. Baird, and B. Schuller. Snore sound classification",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[28] A. Kathan, S. Amiriparian, L. Christ, A. Triantafyllopoulos, N. M¨uller,"
        },
        {
          "samples. This could be implemented via sample weighting,": "using image-based deep spectrum features. 2017.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "A. K¨onig, and B. W. Schuller. A personalised approach to audiovisual"
        },
        {
          "samples. This could be implemented via sample weighting,": "[7] D. Ayata, Y. Yaslan,\nand M. E. Kamasak.\nEmotion\nrecognition",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "humour\nrecognition and its\nindividual-level\nfairness.\nIn Proceedings"
        },
        {
          "samples. This could be implemented via sample weighting,": "from multimodal physiological\nsignals\nfor emotion aware healthcare",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "of\nthe 3rd International on Multimodal Sentiment Analysis Workshop"
        },
        {
          "samples. This could be implemented via sample weighting,": "systems. Journal of Medical and Biological Engineering, 40:149–157,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "and Challenge, pages 29–36, 2022."
        },
        {
          "samples. This could be implemented via sample weighting,": "2020.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[29]\nJ. Kim, E. Andr´e, and T. Vogt. Towards user-independent classification"
        },
        {
          "samples. This could be implemented via sample weighting,": "[8]\nP. Barros, G. Parisi, and S. Wermter. A personalized affective memory",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "of multimodal emotional signals. In 2009 3rd International Conference"
        },
        {
          "samples. This could be implemented via sample weighting,": "model for improving emotion recognition. In International Conference",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "on Affective Computing and Intelligent\nInteraction and Workshops,"
        },
        {
          "samples. This could be implemented via sample weighting,": "on Machine Learning, pages 485–494. PMLR, 2019.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "pages 1–7.\nIEEE, 2009."
        },
        {
          "samples. This could be implemented via sample weighting,": "[9]\nP. Barros and A. Sciutti. Ciao! a contrastive adaptation mechanism for",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[30] D. Kollias,\nP.\nTzirakis, A. Baird, A. Cowen,\nand\nS.\nZafeiriou."
        },
        {
          "samples. This could be implemented via sample weighting,": "non-universal facial expression recognition. In 2022 10th International",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Abaw: Valence-arousal estimation, expression recognition, action unit"
        },
        {
          "samples. This could be implemented via sample weighting,": "Conference on Affective Computing and Intelligent Interaction (ACII),",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "detection & emotional\nreaction intensity estimation challenges.\nIn"
        },
        {
          "samples. This could be implemented via sample weighting,": "pages 1–8.\nIEEE, 2022.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Proceedings of\nthe\nIEEE/CVF Conference on Computer Vision and"
        },
        {
          "samples. This could be implemented via sample weighting,": "[10] O. Bruna, H. Avetisyan, and J. Holub.\nEmotion models\nfor\ntextual",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Pattern Recognition, pages 5888–5897, 2023."
        },
        {
          "samples. This could be implemented via sample weighting,": "Journal\nof\nphysics:\nconference\nemotion\nclassification.\nIn\nseries,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[31] D. Leyzberg, S. Spaulding,\nand B. Scassellati.\nPersonalizing robot"
        },
        {
          "samples. This could be implemented via sample weighting,": "volume 772, page 012063.\nIOP Publishing, 2016.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "of\nthe\ntutors\nto\nindividuals’\nlearning\ndifferences.\nIn Proceedings"
        },
        {
          "samples. This could be implemented via sample weighting,": "[11] M. Caron, H. Touvron,\nI. Misra, H. J´egou, J. Mairal, P. Bojanowski,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "2014 ACM/IEEE international\nconference on Human-robot\ninterac-"
        },
        {
          "samples. This could be implemented via sample weighting,": "and A.\nJoulin.\nEmerging properties\nin self-supervised vision trans-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "tion, pages 423–430, 2014."
        },
        {
          "samples. This could be implemented via sample weighting,": "the IEEE/CVF international conference\nformers.\nIn Proceedings of",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[32] B. Li and L. Han. Distance weighted cosine similarity measure for"
        },
        {
          "samples. This could be implemented via sample weighting,": "on computer vision, pages 9650–9660, 2021.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "text\nclassification.\nIn Intelligent Data Engineering and Automated"
        },
        {
          "samples. This could be implemented via sample weighting,": "[12] R. Chattopadhyay, Q. Sun, W. Fan,\nI. Davidson, S. Panchanathan,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Learning–IDEAL 2013: 14th International Conference,\nIDEAL 2013,"
        },
        {
          "samples. This could be implemented via sample weighting,": "and J. Ye. Multisource domain adaptation and its application to early",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Hefei, China, October 20-23, 2013. Proceedings 14, pages 611–618."
        },
        {
          "samples. This could be implemented via sample weighting,": "detection of fatigue. ACM Transactions on Knowledge Discovery from",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Springer, 2013."
        },
        {
          "samples. This could be implemented via sample weighting,": "Data (TKDD), 6(4):1–26, 2012.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[33]\nJ. Li, A. Waleed,\nand H.\nSalam.\nA survey\non\npersonalized\naf-"
        },
        {
          "samples. This could be implemented via sample weighting,": "[13] V. V. Chithrra Raghuram, H. Salam,\nJ. Nasir, B. Bruno, and O. Ce-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "arXiv\npreprint\nfective\ncomputing\nin\nhuman-machine\ninteraction."
        },
        {
          "samples. This could be implemented via sample weighting,": "liktutan.\nPersonalized productive\nengagement\nrecognition in robot-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "arXiv:2304.00377, 2023."
        },
        {
          "samples. This could be implemented via sample weighting,": "the 2022 Interna-\nmediated collaborative learning.\nIn Proceedings of",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[34]\nL. Li, H. Zhu, S. Zhao, G. Ding, and W. Lin.\nPersonality-assisted"
        },
        {
          "samples. This could be implemented via sample weighting,": "tional Conference on Multimodal\nInteraction, pages 632–641, 2022.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "multi-task\nlearning\nfor\ngeneric\nand\npersonalized\nimage\naesthetics"
        },
        {
          "samples. This could be implemented via sample weighting,": "[14]\nL. Christ, S. Amiriparian, A. Baird, A. Kathan, N. M¨uller, S. Klug,",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "assessment.\nIEEE Transactions on Image Processing, 29:3898–3910,"
        },
        {
          "samples. This could be implemented via sample weighting,": "C. Gagne,\nP.\nTzirakis,\nE.-M. Meßner, A. K¨onig,\net\nal.\nThe",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "2020."
        },
        {
          "samples. This could be implemented via sample weighting,": "muse 2023 multimodal sentiment analysis challenge: Mimicked emo-",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[35] G. Liang, S. Wang,\nand C. Wang.\nPose-aware\nadversarial domain"
        },
        {
          "samples. This could be implemented via sample weighting,": "arXiv\npreprint\ntions,\ncross-cultural\nhumour,\nand\npersonalisation.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "arXiv\nadaptation\nfor\npersonalized\nfacial\nexpression\nrecognition."
        },
        {
          "samples. This could be implemented via sample weighting,": "arXiv:2305.03369, 2023.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "preprint arXiv:2007.05932, 2020."
        },
        {
          "samples. This could be implemented via sample weighting,": "[15] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer machine",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[36] X. Liu, L. Sun, W.\nJiang, F. Zhang, Y. Deng, Z. Huang, L. Meng,"
        },
        {
          "samples. This could be implemented via sample weighting,": "IEEE transactions\non\nfor\npersonalized\nfacial\nexpression\nanalysis.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "Y\n. Liu,\nand C. Liu.\nEvaef: Ensemble\nvalence-arousal\nestimation"
        },
        {
          "samples. This could be implemented via sample weighting,": "pattern analysis and machine intelligence, 39(3):529–545, 2016.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "the IEEE/CVF Conference\nframework in the wild.\nIn Proceedings of"
        },
        {
          "samples. This could be implemented via sample weighting,": "[16]\nS. Cunningham, H. Ridley, J. Weinel, and R. Picking. Audio emotion",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "on Computer Vision and Pattern Recognition, pages 5862–5870, 2023."
        },
        {
          "samples. This could be implemented via sample weighting,": "recognition\nusing machine\nlearning\nto\nsupport\nsound\ndesign.\nIn",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "[37] D. Lopez-Martinez, K. Peng, S. C. Steele, A. J. Lee, D. Borsook, and"
        },
        {
          "samples. This could be implemented via sample weighting,": "Proceedings of\nthe 14th International Audio Mostly Conference: A",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": "R. Picard. Multi-task multiple kernel machines for personalized pain"
        },
        {
          "samples. This could be implemented via sample weighting,": "Journey in Sound, pages 116–123, 2019.",
          "[17] D. de Oliveira, N. R. Prabhu, and T. Gerkmann. Leveraging semantic": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "In 2018 24th International Conference on Pattern Recognition (ICPR),",
          "IEEE conference on computer vision and pattern recognition, pages": "815–823, 2015."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "pages 2320–2325.\nIEEE, 2018.",
          "IEEE conference on computer vision and pattern recognition, pages": "[59]\nT. Senechal, V. Rapp, H. Salam, R. Seguier, K. Bailly, and L. Prevost."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[38] M. Malkauthekar.\nAnalysis\nof\neuclidean\ndistance\nand manhattan",
          "IEEE conference on computer vision and pattern recognition, pages": "Facial action recognition combining heterogeneous features via multi-"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "distance measure in face recognition.\nIn Third International Confer-",
          "IEEE conference on computer vision and pattern recognition, pages": "kernel learning. IEEE Transactions on Systems, Man, and Cybernetics,"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "ence on Computational Intelligence and Information Technology (CIIT",
          "IEEE conference on computer vision and pattern recognition, pages": "Part B (Cybernetics), 42(4):993–1005, 2012."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "2013), pages 503–507.\nIET, 2013.",
          "IEEE conference on computer vision and pattern recognition, pages": "[60] R. V. Shah, G. Grennan, M. Zafar-Khan, F. Alim, S. Dey, D. Ra-"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "¨\n[39] K. Niinuma,\nI.\nOnal Ertu˘grul,\nJ. F. Cohn, L. A.\nJeni, et al.\nFacial",
          "IEEE conference on computer vision and pattern recognition, pages": "manathan, and J. Mishra. Personalized machine learning of depressed"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "expression manipulation\nfor\npersonalized\nfacial\naction\nestimation.",
          "IEEE conference on computer vision and pattern recognition, pages": "mood using wearables. Translational psychiatry, 11(1):1–18, 2021."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Frontiers in Signal Processing, 2:1–16, 2022.",
          "IEEE conference on computer vision and pattern recognition, pages": "[61] M.\nShahabinejad, Y. Wang, Y. Yu,\nJ. Tang,\nand\nJ. Li.\nToward"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[40] Y. Odaka and K. Kaneiwa.\nBlock-segmentation vectors\nfor arousal",
          "IEEE conference on computer vision and pattern recognition, pages": "personalized emotion recognition: A face recognition based attention"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "prediction using semi-supervised learning.\nApplied Soft Computing,",
          "IEEE conference on computer vision and pattern recognition, pages": "of\nIEEE\nmethod\nfor\nfacial\nemotion\nrecognition.\nIn Proceedings"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "142:110327, 2023.",
          "IEEE conference on computer vision and pattern recognition, pages": "International Conference on Face & Gesture, 2021."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[41]\nS. Poria,\nI. Chaturvedi, E. Cambria, and A. Hussain. Convolutional",
          "IEEE conference on computer vision and pattern recognition, pages": "[62]\nZ. Shao, S. Song, S.\nJaiswal, L. Shen, M. Valstar,\nand H. Gunes."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "mkl based multimodal\nemotion recognition and sentiment\nanalysis.",
          "IEEE conference on computer vision and pattern recognition, pages": "Personality recognition by modelling person-specific\ncognitive pro-"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "In 2016 IEEE 16th international conference on data mining (ICDM),",
          "IEEE conference on computer vision and pattern recognition, pages": "the 29th ACM\ncesses using graph representation.\nIn proceedings of"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "pages 439–448.\nIEEE, 2016.",
          "IEEE conference on computer vision and pattern recognition, pages": "international conference on multimedia, pages 357–366, 2021."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[42]\nF. Povolny, P. Matejka, M. Hradis, A. Popkov´a, L. Otrusina, P. Smrz,",
          "IEEE conference on computer vision and pattern recognition, pages": "[63]\nF. A. Shaqra, R. Duwairi, and M. Al-Ayyoub. Recognizing emotion"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "I. Wood, C. Robin, and L. Lamel. Multimodal emotion recognition for",
          "IEEE conference on computer vision and pattern recognition, pages": "from speech\nbased\non\nage\nand\ngender\nusing\nhierarchical models."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "avec 2016 challenge. In Proceedings of the 6th International Workshop",
          "IEEE conference on computer vision and pattern recognition, pages": "Procedia Computer Science, 151:37–44, 2019."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "on Audio/Visual Emotion Challenge, pages 75–82, 2016.",
          "IEEE conference on computer vision and pattern recognition, pages": "[64]\nP. Siirtola, S. Tamminen, G. Chandra, A. Ihalapathirana, and J. R¨oning."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[43] R. G. Praveen, P. Cardinal, and E. Granger. Audio-visual\nfusion for",
          "IEEE conference on computer vision and pattern recognition, pages": "Predicting emotion with biosignals: A comparison of\nclassification"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "emotion recognition in the valence-arousal\nspace using joint\ncross-",
          "IEEE conference on computer vision and pattern recognition, pages": "and regression models for estimating valence and arousal\nlevel using"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "IEEE Transactions on Biometrics, Behavior, and Identity\nattention.",
          "IEEE conference on computer vision and pattern recognition, pages": "wearable sensors. Sensors, 23(3):1598, 2023."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Science, 2023.",
          "IEEE conference on computer vision and pattern recognition, pages": "[65] M. K. Singh, N. Singh, and A. Singh. Speaker’s voice characteristics"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "International\n[44]\nS. PS and G. Mahalakshmi. Emotion models: a review.",
          "IEEE conference on computer vision and pattern recognition, pages": "2019\nand\nsimilarity measurement\nusing\neuclidean\ndistances.\nIn"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Journal of Control Theory and Applications, 10(8):651–657, 2017.",
          "IEEE conference on computer vision and pattern recognition, pages": "International Conference on Signal Processing and Communication"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[45]\nF. A. Pujol, H. Mora, and A. Mart´ınez. Emotion recognition to improve",
          "IEEE conference on computer vision and pattern recognition, pages": "(ICSC), pages 317–322.\nIEEE, 2019."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "e-healthcare systems in smart cities.\nIn Research & Innovation Forum",
          "IEEE conference on computer vision and pattern recognition, pages": "[66] C. Soladi´e, H. Salam, C. Pelachaud, N. Stoiber, and R. S´eguier. A mul-"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "2019: Technology,\nInnovation, Education, and their Social\nImpact 1,",
          "IEEE conference on computer vision and pattern recognition, pages": "timodal\nfuzzy inference system using a continuous\nfacial expression"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "pages 245–254. Springer, 2019.",
          "IEEE conference on computer vision and pattern recognition, pages": "the 14th ACM\nrepresentation for emotion detection.\nIn Proceedings of"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[46] B. Ren, E. G. Balkind, B. Pastro, E. S.\nIsrael, D. A. Pizzagalli,",
          "IEEE conference on computer vision and pattern recognition, pages": "international conference on Multimodal\ninteraction, pages 493–500,"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "H. Rahimi-Eichi,\nJ. T. Baker,\nand C. A. Webb.\nPredicting\nstates",
          "IEEE conference on computer vision and pattern recognition, pages": "2012."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "of elevated negative affect\nin adolescents\nfrom smartphone sensors:",
          "IEEE conference on computer vision and pattern recognition, pages": "[67] C.\nSoladi´e, H.\nSalam, N.\nStoiber,\nand R.\nS´eguier.\nContinuous"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Psychological\nA novel\npersonalized machine\nlearning\napproach.",
          "IEEE conference on computer vision and pattern recognition, pages": "facial\nexpression\nrepresentation\nfor multimodal\nemotion\ndetection."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Medicine, pages 1–9, 2022.",
          "IEEE conference on computer vision and pattern recognition, pages": "International Journal of Advanced Computer Science (IJACSci), 3(5),"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[47] M. Rescigno, M. Spezialetti, and S. Rossi.\nPersonalized models\nfor",
          "IEEE conference on computer vision and pattern recognition, pages": "2013."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "facial emotion recognition through transfer learning. Multimedia Tools",
          "IEEE conference on computer vision and pattern recognition, pages": "[68] B. C. Song and D. H. Kim. Hidden emotion detection using multi-"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "and Applications, 79(47-48):35811 – 35828, 2020. Cited by: 12; All",
          "IEEE conference on computer vision and pattern recognition, pages": "the 2021 CHI Conference on\nmodal signals.\nIn Extended Abstracts of"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Open Access, Hybrid Gold Open Access.",
          "IEEE conference on computer vision and pattern recognition, pages": "Human Factors in Computing Systems, pages 1–7, 2021."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[48] M. Rescigno, M. Spezialetti, and S. Rossi.\nPersonalized models\nfor",
          "IEEE conference on computer vision and pattern recognition, pages": "[69] Y. Song, S. Dixon, M. T. Pearce,\nand A. R. Halpern.\nPerceived"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "facial emotion recognition through transfer learning. Multimedia Tools",
          "IEEE conference on computer vision and pattern recognition, pages": "and\ninduced\nemotion\nresponses\nto\npopular music: Categorical\nand"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "and Applications, 79:35811–35828, 2020.",
          "IEEE conference on computer vision and pattern recognition, pages": "dimensional models. Music Perception: An Interdisciplinary Journal,"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[49] O. Rudovic, J. Lee, M. Dai, B. Schuller, and R. W. Picard. Personal-",
          "IEEE conference on computer vision and pattern recognition, pages": "33(4):472–492, 2016."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "ized machine learning for\nrobot perception of affect and engagement",
          "IEEE conference on computer vision and pattern recognition, pages": "[70]\nL. Stappen, A. Baird, L. Christ, L. Schumann, B. Sertolli, E.-M."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "in autism therapy. Science Robotics, 3(19), 2018.",
          "IEEE conference on computer vision and pattern recognition, pages": "Messner, E. Cambria, G. Zhao,\nand B. W.\nSchuller.\nThe muse"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[50] A. Saeed and S. Trajanovski. Personalized driver stress detection with",
          "IEEE conference on computer vision and pattern recognition, pages": "2021 multimodal\nsentiment\nanalysis\nchallenge:\nsentiment,\nemotion,"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "multi-task neural networks using physiological signals. arXiv preprint",
          "IEEE conference on computer vision and pattern recognition, pages": "of\nthe\n2nd\non\nphysiological-emotion,\nand\nstress.\nIn Proceedings"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "arXiv:1711.06116, 2017.",
          "IEEE conference on computer vision and pattern recognition, pages": "Multimodal Sentiment Analysis Challenge, pages 5–14. 2021."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[51] H. Salam, O. Celiktutan, H. Gunes,\nand M. Chetouani.\nAutomatic",
          "IEEE conference on computer vision and pattern recognition, pages": "[71]\nL. Stappen, E.-M. Meßner, E. Cambria, G. Zhao, and B. W. Schuller."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "IEEE\ncontext-aware\ninference\nof\nengagement\nin\nhmi: A survey.",
          "IEEE conference on computer vision and pattern recognition, pages": "Muse 2021 challenge: Multimodal emotion, sentiment, physiological-"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Transactions on Affective Computing, 2023.",
          "IEEE conference on computer vision and pattern recognition, pages": "of\nthe\n29th ACM\nemotion,\nand\nstress\ndetection.\nIn Proceedings"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[52] H. Salam, O. Celiktutan, I. Hupont, H. Gunes, and M. Chetouani. Fully",
          "IEEE conference on computer vision and pattern recognition, pages": "International Conference on Multimedia, pages 5706–5707, 2021."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "automatic analysis of engagement and its\nrelationship to personality",
          "IEEE conference on computer vision and pattern recognition, pages": "[72]\nS. Taylor, N. Jaques, E. Nosakhare, A. Sano, and R. Picard. Person-"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "in human-robot\ninteractions.\nIEEE Access, 5:705–721, 2016.",
          "IEEE conference on computer vision and pattern recognition, pages": "alized multitask learning for predicting tomorrow’s mood, stress, and"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[53] H. Salam, O. Celiktutan, V. Manoranjan, I. Ismail, and H. Mukherjee.",
          "IEEE conference on computer vision and pattern recognition, pages": "health.\nIEEE Transactions on Affective Computing, 11(2):200–213,"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Learning personalised models for automatic self-reported personality",
          "IEEE conference on computer vision and pattern recognition, pages": "2017."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "recognition.\nIn ICCV 2021 Understanding Social Behavior in Dyadic",
          "IEEE conference on computer vision and pattern recognition, pages": "[73] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "and Small Group Interactions Challenge Fact\nsheet: Automatic self-",
          "IEEE conference on computer vision and pattern recognition, pages": "Gomez, Ł. Kaiser,\nand\nI. Polosukhin.\nAttention\nis\nall\nyou\nneed."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "reported personality recognition Track, 2021.",
          "IEEE conference on computer vision and pattern recognition, pages": "Advances in neural\ninformation processing systems, 30, 2017."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[54] H. Salam and M. Chetouani. Engagement detection based on mutli-",
          "IEEE conference on computer vision and pattern recognition, pages": "[74] M. Vijaymeena and K. Kavitha. A survey on similarity measures in"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "party cues for human robot\ninteraction.\nIn 2015 International Confer-",
          "IEEE conference on computer vision and pattern recognition, pages": "text mining. Machine Learning and Applications: An International"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "ence on Affective Computing and Intelligent Interaction (ACII), pages",
          "IEEE conference on computer vision and pattern recognition, pages": "Journal, 3(2):19–28, 2016."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "341–347.\nIEEE, 2015.",
          "IEEE conference on computer vision and pattern recognition, pages": "[75]\nT. Vogt\nand E. Andr´e.\nImproving\nautomatic\nemotion\nrecognition"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[55]\nE. San Segundo, A. Tsanas, and P. G´omez-Vilda. Euclidean distances",
          "IEEE conference on computer vision and pattern recognition, pages": "the Fifth\nfrom speech via gender differentiaion.\nIn Proceedings of"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "as measures of\nspeaker\nsimilarity including identical\ntwin\npairs:\na",
          "IEEE conference on computer vision and pattern recognition, pages": "International Conference\non\nLanguage Resources\nand Evaluation"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "forensic\ninvestigation\nusing\nsource\nand filter\nvoice\ncharacteristics.",
          "IEEE conference on computer vision and pattern recognition, pages": "(LREC’06), Genoa,\nItaly, May 2006. European Language Resources"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Forensic Science International, 270:25–38, 2017.",
          "IEEE conference on computer vision and pattern recognition, pages": "Association (ELRA)."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[56] A. V. Savchenko.\nEmotieffnets\nfor\nfacial processing in video-based",
          "IEEE conference on computer vision and pattern recognition, pages": "[76]\nJ. Wagner,\nA.\nTriantafyllopoulos,\nH. Wierstorf,\nM.\nSchmitt,"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "valence-arousal prediction,\nexpression classification and action unit",
          "IEEE conference on computer vision and pattern recognition, pages": "F. Burkhardt, F. Eyben, and B. W. Schuller. Dawn of\nthe transformer"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "the IEEE/CVF Conference on Computer\ndetection.\nIn Proceedings of",
          "IEEE conference on computer vision and pattern recognition, pages": "IEEE\nera\nin speech emotion recognition:\nclosing the valence gap."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Vision and Pattern Recognition, pages 5715–5723, 2023.",
          "IEEE conference on computer vision and pattern recognition, pages": "Transactions on Pattern Analysis and Machine Intelligence, 2023."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[57]\nJ. Schneider and M. Vlachos. Personalization of deep learning. In Data",
          "IEEE conference on computer vision and pattern recognition, pages": "[77] C. Wang\nand\nS. Wang.\nPersonalized multiple\nfacial\naction\nunit"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "Science–Analytics and Applications: Proceedings of\nthe 3rd Interna-",
          "IEEE conference on computer vision and pattern recognition, pages": "recognition through generative\nadversarial\nrecognition network.\nIn"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "tional Data Science Conference–iDSC2020, pages 89–96. Springer,",
          "IEEE conference on computer vision and pattern recognition, pages": "Proceedings of the 26th ACM international conference on Multimedia,"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "2021.",
          "IEEE conference on computer vision and pattern recognition, pages": "pages 302–310, 2018."
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "[58]\nF.\nSchroff, D. Kalenichenko,\nand\nJ.\nPhilbin.\nFacenet: A unified",
          "IEEE conference on computer vision and pattern recognition, pages": "[78] K. Woodward, E. Kanjo, D.\nJ. Brown, and T. McGinnity.\nTowards"
        },
        {
          "recognition from functional near-infrared spectroscopy brain signals.": "the\nembedding for\nface recognition and clustering.\nIn Proceedings of",
          "IEEE conference on computer vision and pattern recognition, pages": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "2021\nIEEE International\nSmart Cities\nlearning\n“in\nthe wild”.\nIn"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "Conference (ISC2), pages 1–7.\nIEEE, 2021."
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "[79] H. Yang, Z. Zhang, and L. Yin.\nIdentity-adaptive facial expression"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "recognition through expression regeneration using conditional genera-"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "tive adversarial networks. In 2018 13th IEEE International Conference"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "on Automatic Face & Gesture Recognition (FG 2018), pages 294–301."
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "IEEE, 2018."
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "[80] R. Yusuf, D. G. Sharma,\nI. Tanev, and K. Shimohara.\nIndividuality"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "and user-specific approach in adaptive emotion recognition model.\nIn"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "2017 International Conference on Biometrics and Kansei Engineering"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "(ICBAKE), pages 1–6.\nIEEE, 2017."
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "[81] H. Zhao, N. Ye, and R. Wang. Transferring age and gender attributes"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "for dimensional\nemotion prediction from big speech data using hi-"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "erarchical deep learning.\nIn 2018 IEEE 4th International Conference"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "on Big Data Security on Cloud (BigDataSecurity), IEEE International"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "Conference on High Performance and Smart Computing,(HPSC) and"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "IEEE International Conference on Intelligent Data and Security (IDS),"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "pages 20–24.\nIEEE, 2018."
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "[82]\nS. Zhao, A. Gholaminejad, G. Ding, Y. Gao, J. Han, and K. Keutzer."
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "Personalized\nemotion\nrecognition\nby\npersonality-aware\nhigh-order"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "ACM Transactions on Multimedia\nlearning of physiological\nsignals."
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "Computing, Communications, and Applications (TOMM), 15(1s):1–18,"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "2019."
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "[83] R. Zhi, M. Liu, and D. Zhang. A comprehensive survey on automatic"
        },
        {
          "personalised mental wellbeing\nrecognition\non-device\nusing\ntransfer": "facial action unit analysis. The Visual Computer, 36:1067–1093, 2020."
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Towards emotionallypersonalized computing: Dynamic prediction of student mental states from self-manipulatory body movements",
      "authors": [
        "A Abbasi",
        "N Afzulpurkar",
        "T Uno"
      ],
      "year": "2009",
      "venue": "2009 International Conference on Emerging Technologies"
    },
    {
      "citation_id": "2",
      "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
      "authors": [
        "N Ahmed",
        "Z Aghbari",
        "S Girija"
      ],
      "year": "2023",
      "venue": "Intelligent Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "Toward an enhanced arabic text classification using cosine similarity and latent semantic indexing",
      "authors": [
        "F Al-Anzi",
        "D Abuzeina"
      ],
      "year": "2017",
      "venue": "Journal of King Saud University-Computer and Information Sciences"
    },
    {
      "citation_id": "4",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "5",
      "title": "Semi-supervised model personalization for improved detection of learner's emotional engagement",
      "authors": [
        "N Alyuz",
        "E Okur",
        "E Oktay",
        "U Genc",
        "S Aslan",
        "S Mete",
        "B Arnrich",
        "A Esme"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "6",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "A Baird",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Snore sound classification using image-based deep spectrum features"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition from multimodal physiological signals for emotion aware healthcare systems",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "M Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering"
    },
    {
      "citation_id": "8",
      "title": "A personalized affective memory model for improving emotion recognition",
      "authors": [
        "P Barros",
        "G Parisi",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "9",
      "title": "Ciao! a contrastive adaptation mechanism for non-universal facial expression recognition",
      "authors": [
        "P Barros",
        "A Sciutti"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "10",
      "title": "Emotion models for textual emotion classification",
      "authors": [
        "O Bruna",
        "H Avetisyan",
        "J Holub"
      ],
      "year": "2016",
      "venue": "Journal of physics: conference series"
    },
    {
      "citation_id": "11",
      "title": "Emerging properties in self-supervised vision transformers",
      "authors": [
        "M Caron",
        "H Touvron",
        "I Misra",
        "H Jégou",
        "J Mairal",
        "P Bojanowski",
        "A Joulin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "12",
      "title": "Multisource domain adaptation and its application to early detection of fatigue",
      "authors": [
        "R Chattopadhyay",
        "Q Sun",
        "W Fan",
        "I Davidson",
        "S Panchanathan",
        "J Ye"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)"
    },
    {
      "citation_id": "13",
      "title": "Personalized productive engagement recognition in robotmediated collaborative learning",
      "authors": [
        "V Chithrra Raghuram",
        "H Salam",
        "J Nasir",
        "B Bruno",
        "O Celiktutan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "14",
      "title": "The muse 2023 multimodal sentiment analysis challenge: Mimicked emotions, cross-cultural humour, and personalisation",
      "authors": [
        "L Christ",
        "S Amiriparian",
        "A Baird",
        "A Kathan",
        "N Müller",
        "S Klug",
        "C Gagne",
        "P Tzirakis",
        "E.-M Meßner",
        "A König"
      ],
      "year": "2023",
      "venue": "The muse 2023 multimodal sentiment analysis challenge: Mimicked emotions, cross-cultural humour, and personalisation",
      "arxiv": "arXiv:2305.03369"
    },
    {
      "citation_id": "15",
      "title": "Selective transfer machine for personalized facial expression analysis",
      "authors": [
        "W.-S Chu",
        "F De La Torre",
        "J Cohn"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "16",
      "title": "Audio emotion recognition using machine learning to support sound design",
      "authors": [
        "S Cunningham",
        "H Ridley",
        "J Weinel",
        "R Picking"
      ],
      "year": "2019",
      "venue": "Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound"
    },
    {
      "citation_id": "17",
      "title": "Leveraging semantic information for efficient self-supervised emotion recognition with audio-textual distilled models",
      "authors": [
        "D Oliveira",
        "N Prabhu",
        "T Gerkmann"
      ],
      "year": "2023",
      "venue": "Leveraging semantic information for efficient self-supervised emotion recognition with audio-textual distilled models",
      "arxiv": "arXiv:2305.19184"
    },
    {
      "citation_id": "18",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "20",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "21",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "22",
      "title": "Electrocardiogram-based emotion recognition systems and their applications in healthcare-a review",
      "authors": [
        "M Hasnul",
        "N Aziz",
        "S Alelyani",
        "M Mohana",
        "A Aziz"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "Multi-facial patches aggregation network for facial expression recognition and facial regions contributions to emotion display",
      "authors": [
        "A Hazourli",
        "A Djeghri",
        "H Salam",
        "A Othmani"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "24",
      "title": "Similarity measures for text document clustering",
      "authors": [
        "A Huang"
      ],
      "year": "2008",
      "venue": "Proceedings of the sixth new zealand computer science research student conference (NZCSRSC2008)"
    },
    {
      "citation_id": "25",
      "title": "Multitask learning for predicting health, stress, and happiness",
      "authors": [
        "N Jaques",
        "S Taylor",
        "E Nosakhare",
        "A Sano",
        "R Picard"
      ],
      "year": "2016",
      "venue": "NIPS Workshop on Machine Learning for Healthcare"
    },
    {
      "citation_id": "26",
      "title": "Personality traits and nonverbal communication patterns",
      "authors": [
        "M Jensen"
      ],
      "year": "2016",
      "venue": "Int'l J. Soc. Sci. Stud"
    },
    {
      "citation_id": "27",
      "title": "Inferring student engagement in collaborative problem solving from visual cues",
      "authors": [
        "A Kasparova",
        "O Celiktutan",
        "M Cukurova"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "28",
      "title": "A personalised approach to audiovisual humour recognition and its individual-level fairness",
      "authors": [
        "A Kathan",
        "S Amiriparian",
        "L Christ",
        "A Triantafyllopoulos",
        "N Müller",
        "A König",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "29",
      "title": "Towards user-independent classification of multimodal emotional signals",
      "authors": [
        "J Kim",
        "E André",
        "T Vogt"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "30",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Personalizing robot tutors to individuals' learning differences",
      "authors": [
        "D Leyzberg",
        "S Spaulding",
        "B Scassellati"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction"
    },
    {
      "citation_id": "32",
      "title": "Distance weighted cosine similarity measure for text classification",
      "authors": [
        "B Li",
        "L Han"
      ],
      "year": "2013",
      "venue": "Intelligent Data Engineering and Automated Learning-IDEAL 2013: 14th International Conference, IDEAL 2013"
    },
    {
      "citation_id": "33",
      "title": "A survey on personalized affective computing in human-machine interaction",
      "authors": [
        "J Li",
        "A Waleed",
        "H Salam"
      ],
      "year": "2023",
      "venue": "A survey on personalized affective computing in human-machine interaction",
      "arxiv": "arXiv:2304.00377"
    },
    {
      "citation_id": "34",
      "title": "Personality-assisted multi-task learning for generic and personalized image aesthetics assessment",
      "authors": [
        "L Li",
        "H Zhu",
        "S Zhao",
        "G Ding",
        "W Lin"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "35",
      "title": "Pose-aware adversarial domain adaptation for personalized facial expression recognition",
      "authors": [
        "G Liang",
        "S Wang",
        "C Wang"
      ],
      "year": "2020",
      "venue": "Pose-aware adversarial domain adaptation for personalized facial expression recognition",
      "arxiv": "arXiv:2007.05932"
    },
    {
      "citation_id": "36",
      "title": "Evaef: Ensemble valence-arousal estimation framework in the wild",
      "authors": [
        "X Liu",
        "L Sun",
        "W Jiang",
        "F Zhang",
        "Y Deng",
        "Z Huang",
        "L Meng",
        "Y Liu",
        "C Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Multi-task multiple kernel machines for personalized pain recognition from functional near-infrared spectroscopy brain signals",
      "authors": [
        "D Lopez-Martinez",
        "K Peng",
        "S Steele",
        "A Lee",
        "D Borsook",
        "R Picard"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "38",
      "title": "Analysis of euclidean distance and manhattan distance measure in face recognition",
      "authors": [
        "M Malkauthekar"
      ],
      "year": "2013",
      "venue": "Third International Conference on Computational Intelligence and Information Technology (CIIT 2013)"
    },
    {
      "citation_id": "39",
      "title": "Facial expression manipulation for personalized facial action estimation",
      "authors": [
        "K Niinuma",
        "I Ertugrul",
        "J Cohn",
        "L Jeni"
      ],
      "year": "2022",
      "venue": "Frontiers in Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Block-segmentation vectors for arousal prediction using semi-supervised learning",
      "authors": [
        "Y Odaka",
        "K Kaneiwa"
      ],
      "year": "2023",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "41",
      "title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "2016 IEEE 16th international conference on data mining (ICDM)"
    },
    {
      "citation_id": "42",
      "title": "Multimodal emotion recognition for avec 2016 challenge",
      "authors": [
        "F Povolny",
        "P Matejka",
        "M Hradis",
        "A Popková",
        "L Otrusina",
        "P Smrz",
        "I Wood",
        "C Robin",
        "L Lamel"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "43",
      "title": "Audio-visual fusion for emotion recognition in the valence-arousal space using joint crossattention",
      "authors": [
        "R Praveen",
        "P Cardinal",
        "E Granger"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "44",
      "title": "Emotion models: a review",
      "authors": [
        "S Ps",
        "G Mahalakshmi"
      ],
      "year": "2017",
      "venue": "International Journal of Control Theory and Applications"
    },
    {
      "citation_id": "45",
      "title": "Emotion recognition to improve e-healthcare systems in smart cities",
      "authors": [
        "F Pujol",
        "H Mora",
        "A Martínez"
      ],
      "year": "2019",
      "venue": "Technology, Innovation, Education, and their Social Impact"
    },
    {
      "citation_id": "46",
      "title": "Predicting states of elevated negative affect in adolescents from smartphone sensors: A novel personalized machine learning approach",
      "authors": [
        "B Ren",
        "E Balkind",
        "B Pastro",
        "E Israel",
        "D Pizzagalli",
        "H Rahimi-Eichi",
        "J Baker",
        "C Webb"
      ],
      "year": "2022",
      "venue": "Psychological Medicine"
    },
    {
      "citation_id": "47",
      "title": "Personalized models for facial emotion recognition through transfer learning",
      "authors": [
        "M Rescigno",
        "M Spezialetti",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "48",
      "title": "Personalized models for facial emotion recognition through transfer learning",
      "authors": [
        "M Rescigno",
        "M Spezialetti",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "49",
      "title": "Personalized machine learning for robot perception of affect and engagement in autism therapy",
      "authors": [
        "O Rudovic",
        "J Lee",
        "M Dai",
        "B Schuller",
        "R Picard"
      ],
      "year": "2018",
      "venue": "Science Robotics"
    },
    {
      "citation_id": "50",
      "title": "Personalized driver stress detection with multi-task neural networks using physiological signals",
      "authors": [
        "A Saeed",
        "S Trajanovski"
      ],
      "year": "2017",
      "venue": "Personalized driver stress detection with multi-task neural networks using physiological signals",
      "arxiv": "arXiv:1711.06116"
    },
    {
      "citation_id": "51",
      "title": "Automatic context-aware inference of engagement in hmi: A survey",
      "authors": [
        "H Salam",
        "O Celiktutan",
        "H Gunes",
        "M Chetouani"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Fully automatic analysis of engagement and its relationship to personality in human-robot interactions",
      "authors": [
        "H Salam",
        "O Celiktutan",
        "I Hupont",
        "H Gunes",
        "M Chetouani"
      ],
      "year": "2016",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "53",
      "title": "Learning personalised models for automatic self-reported personality recognition",
      "authors": [
        "H Salam",
        "O Celiktutan",
        "V Manoranjan",
        "I Ismail",
        "H Mukherjee"
      ],
      "year": "2021",
      "venue": "ICCV 2021 Understanding Social Behavior in Dyadic and Small Group Interactions Challenge Fact sheet: Automatic selfreported personality recognition Track"
    },
    {
      "citation_id": "54",
      "title": "Engagement detection based on mutliparty cues for human robot interaction",
      "authors": [
        "H Salam",
        "M Chetouani"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "55",
      "title": "Euclidean distances as measures of speaker similarity including identical twin pairs: a forensic investigation using source and filter voice characteristics",
      "authors": [
        "E San Segundo",
        "A Tsanas",
        "P Gómez-Vilda"
      ],
      "year": "2017",
      "venue": "Forensic Science International"
    },
    {
      "citation_id": "56",
      "title": "Emotieffnets for facial processing in video-based valence-arousal prediction, expression classification and action unit detection",
      "authors": [
        "A Savchenko"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "Personalization of deep learning",
      "authors": [
        "J Schneider",
        "M Vlachos"
      ],
      "year": "2021",
      "venue": "Data Science-Analytics and Applications: Proceedings of the 3rd International Data Science Conference-iDSC2020"
    },
    {
      "citation_id": "58",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "59",
      "title": "Facial action recognition combining heterogeneous features via multikernel learning",
      "authors": [
        "T Senechal",
        "V Rapp",
        "H Salam",
        "R Seguier",
        "K Bailly",
        "L Prevost"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "60",
      "title": "Personalized machine learning of depressed mood using wearables",
      "authors": [
        "R Shah",
        "G Grennan",
        "M Zafar-Khan",
        "F Alim",
        "S Dey",
        "D Ramanathan",
        "J Mishra"
      ],
      "year": "2021",
      "venue": "Translational psychiatry"
    },
    {
      "citation_id": "61",
      "title": "Toward personalized emotion recognition: A face recognition based attention method for facial emotion recognition",
      "authors": [
        "M Shahabinejad",
        "Y Wang",
        "Y Yu",
        "J Tang",
        "J Li"
      ],
      "year": "2021",
      "venue": "Proceedings of IEEE International Conference on Face & Gesture"
    },
    {
      "citation_id": "62",
      "title": "Personality recognition by modelling person-specific cognitive processes using graph representation",
      "authors": [
        "Z Shao",
        "S Song",
        "S Jaiswal",
        "L Shen",
        "M Valstar",
        "H Gunes"
      ],
      "year": "2021",
      "venue": "proceedings of the 29th ACM international conference on multimedia"
    },
    {
      "citation_id": "63",
      "title": "Recognizing emotion from speech based on age and gender using hierarchical models",
      "authors": [
        "F Shaqra",
        "R Duwairi",
        "M Al-Ayyoub"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "64",
      "title": "Predicting emotion with biosignals: A comparison of classification and regression models for estimating valence and arousal level using wearable sensors",
      "authors": [
        "P Siirtola",
        "S Tamminen",
        "G Chandra",
        "A Ihalapathirana",
        "J Röning"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "65",
      "title": "Speaker's voice characteristics and similarity measurement using euclidean distances",
      "authors": [
        "M Singh",
        "N Singh",
        "A Singh"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Signal Processing and Communication (ICSC)"
    },
    {
      "citation_id": "66",
      "title": "A multimodal fuzzy inference system using a continuous facial expression representation for emotion detection",
      "authors": [
        "C Soladié",
        "H Salam",
        "C Pelachaud",
        "N Stoiber",
        "R Séguier"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "67",
      "title": "Continuous facial expression representation for multimodal emotion detection",
      "authors": [
        "C Soladié",
        "H Salam",
        "N Stoiber",
        "R Séguier"
      ],
      "year": "2013",
      "venue": "International Journal of Advanced Computer Science (IJACSci)"
    },
    {
      "citation_id": "68",
      "title": "Hidden emotion detection using multimodal signals",
      "authors": [
        "B Song",
        "D Kim"
      ],
      "year": "2021",
      "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "69",
      "title": "Perceived and induced emotion responses to popular music: Categorical and dimensional models",
      "authors": [
        "Y Song",
        "S Dixon",
        "M Pearce",
        "A Halpern"
      ],
      "year": "2016",
      "venue": "Music Perception: An Interdisciplinary Journal"
    },
    {
      "citation_id": "70",
      "title": "The muse 2021 multimodal sentiment analysis challenge: sentiment, emotion, physiological-emotion, and stress",
      "authors": [
        "L Stappen",
        "A Baird",
        "L Christ",
        "L Schumann",
        "B Sertolli",
        "E.-M Messner",
        "E Cambria",
        "G Zhao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "71",
      "title": "Muse 2021 challenge: Multimodal emotion, sentiment, physiologicalemotion, and stress detection",
      "authors": [
        "L Stappen",
        "E.-M Meßner",
        "E Cambria",
        "G Zhao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "72",
      "title": "Personalized multitask learning for predicting tomorrow's mood, stress, and health",
      "authors": [
        "S Taylor",
        "N Jaques",
        "E Nosakhare",
        "A Sano",
        "R Picard"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "73",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "74",
      "title": "A survey on similarity measures in text mining",
      "authors": [
        "M Vijaymeena",
        "K Kavitha"
      ],
      "year": "2016",
      "venue": "Machine Learning and Applications: An International Journal"
    },
    {
      "citation_id": "75",
      "title": "Improving automatic emotion recognition from speech via gender differentiaion",
      "authors": [
        "T Vogt",
        "E André"
      ],
      "year": "2006",
      "venue": "Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06)"
    },
    {
      "citation_id": "76",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "77",
      "title": "Personalized multiple facial action unit recognition through generative adversarial recognition network",
      "authors": [
        "C Wang",
        "S Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "78",
      "title": "Towards personalised mental wellbeing recognition on-device using transfer learning \"in the wild",
      "authors": [
        "K Woodward",
        "E Kanjo",
        "D Brown",
        "T Mcginnity"
      ],
      "venue": "2021 IEEE International Smart Cities Conference (ISC2)"
    },
    {
      "citation_id": "79",
      "title": "Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks",
      "authors": [
        "H Yang",
        "Z Zhang",
        "L Yin"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "80",
      "title": "Individuality and user-specific approach in adaptive emotion recognition model",
      "authors": [
        "R Yusuf",
        "D Sharma",
        "I Tanev",
        "K Shimohara"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Biometrics and Kansei Engineering (ICBAKE)"
    },
    {
      "citation_id": "81",
      "title": "Transferring age and gender attributes for dimensional emotion prediction from big speech data using hierarchical deep learning",
      "authors": [
        "H Zhao",
        "N Ye",
        "R Wang"
      ],
      "year": "2018",
      "venue": "2018 IEEE 4th International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing,(HPSC) and IEEE International Conference on Intelligent Data and Security (IDS)"
    },
    {
      "citation_id": "82",
      "title": "Personalized emotion recognition by personality-aware high-order learning of physiological signals",
      "authors": [
        "S Zhao",
        "A Gholaminejad",
        "G Ding",
        "Y Gao",
        "J Han",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "83",
      "title": "A comprehensive survey on automatic facial action unit analysis. The Visual Computer",
      "authors": [
        "R Zhi",
        "M Liu",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "A comprehensive survey on automatic facial action unit analysis. The Visual Computer"
    }
  ]
}