{
  "paper_id": "2409.14312v1",
  "title": "Avengers Assemble: Amalgamation Of Non-Semantic Features For Depression Detection",
  "published": "2024-09-22T04:40:04Z",
  "authors": [
    "Orchid Chetia Phukan",
    "Swarup Ranjan Behera",
    "Shubham Singh",
    "Muskaan Singh",
    "Vandana Rajan",
    "Arun Balaji Buduru",
    "Rajesh Sharma",
    "S. R. Mahadeva Prasanna"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this study, we address the challenge of depression detection from speech, focusing on the potential of non-semantic features (NSFs) to capture subtle markers of depression. While prior research has leveraged various features for this task, NSFsextracted from pre-trained models (PTMs) designed for nonsemantic tasks such as paralinguistic speech processing (TRILLsson), speaker recognition (x-vector), and emotion recognition (emoHuBERT)-have shown significant promise. However, the potential of combining these diverse features has not been fully explored. In this work, we demonstrate that the amalgamation of NSFs results in complementary behavior, leading to enhanced depression detection performance. Furthermore, to our end, we introduce a simple novel framework, FuSeR, designed to effectively combine these features. Our results show that FuSeR outperforms models utilizing individual NSFs as well as baseline fusion techniques and obtains state-of-the-art (SOTA) performance in E-DAIC benchmark with RMSE of 5.51 and MAE of 4.48, establishing it as a robust approach for depression detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methodology",
      "text": "In this section, we provide an overview of the NSFs utilized in our experiments, baseline fusion techniques, and the proposed framework, FuSeR.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Feature Representations",
      "text": "x-vector  [16] : It is a time-delay neural network trained for speaker recognition. It shows SOTA performance compared to its predecessor, i-vector. We include x-vector in our experiments as it has shown effectiveness in depression detection  [14]  and as well as related tasks like SER  [17] . For this study, we use the speechbrain x-vector model 2  . We extract features of 512-dimension by averaging across time.\n\nTRILLsson  [18] : It is distilled from the SOTA universal paralinguistic conformer (CAP12) model that has shown SOTA in various non-semantic or paralinguistic tasks such as SER, speaker recognition, deepfake detection, and so on. TRILLsson is open-sourced, but CAP12 is not. TRILLsson also achieves near SOTA performance on the Non-Semantic Speech (NOSS) benchmark. For this study, we utilize the 1024-dimensional feature vectors generated from TRILLsson 3  . emoHuBERT: For extracting emotion-specific features, we fine-tune HuBERT 4    [19] , originally pre-trained in a selfsupervised manner for comprehensive speech tasks and name it as, emoHuBERT. We train it for 50 epochs on CREMA-D  [20] , a benchmark SER dataset, by attaching a probing head on top of the HuBERT model architecture. We unfreeze all the HuBERT layers for fine-tuning and extract features of 768 from the last hidden state by mean pooling for depression detection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Fusion Techniques",
      "text": "Here, we discuss various fusion techniques employed in our study for the fusion of NSFs and the downstream modeling employed with the fusion techniques. For a detailed visualization, refer to Figure  1 . Early Fusion: It is a basic type of fusion, where we concat the NSFs directly after being extracted from the PTMs. We then use a convolutional block with a 1D-CNN layer of 32 filters with a 3 × 3 filter size, followed by a max-pooling layer. We then append a FCN module on top of the flattened features after maxpooling. For the FCN module, we attach a dense layer of 120 neurons followed by a output neuron that predicts a continous variable. Average Fusion: We add convolutional blocks with the same architectural details as used in Early Fusion CNN model on the top of each individual NSFs. We flatten the features after maxpooling followed by linear projection to the same dimension; we average the features from different NSFs and attach an FCN module on top of the averaged features (See Figure  1 ). For FCN module, we use the same modeling details as Early Fusion FCN module. Concatenation Fusion: Here, also we add convolutional block with the same architectural details as used in Early Fusion CNN model on the top of each individual NSFs. After flattening, we concat the features from different NSFs and attach a FCN module on top of the concatenated features (See Figure  1 ). For the FCN module, we apply the same modeling details as in the Early Fusion FCN module.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Proposed Feature Fusion Framework -Fuser",
      "text": "We introduce FuSeR, a simple yet effective feature fusion framework that makes use of bilinear pooling that as shown SOTA performance in multimodal tasks  [21] . The proposed framework is shown in Figure  1 . First, we add convolutional blocks with the same architectural details as used in Early Fusion CNN model on the top of each individual NSFs. After flattening the features, we linear project it to 120 dimension. After this, we apply bilinear pooling that performs outer product on top of the projected features. Mathematically, given two features a and b, with dimensions C A × 1 and C B × 1 respectively, their outer product yields a matrix M of dimensions C A × C B , represented as:\n\nHere, ⊗ denotes the outer product operation, and b T is the transpose of vector b. The matrix M encapsulates the pairwise interactions between features from two different NSFs, resulting in a richer representation. The resulting matrix M is flattened and passed to an FCN module with similar modeling details to the Early Fusion FCN module, followed by an output neuron.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiments A. Benchmark Dataset",
      "text": "We employ the Extended DAIC (E-DAIC) dataset  [22] ,  [23] , an enhanced version of the DAIC-WOZ dataset from the AVEC 2019 challenge, which includes semi-structured clinical information consistent with DAIC-WOZ. The E-DAIC dataset comprises data from 275 participants: 163 for training, 56 for development, and 56 for testing, specifically curated to aid in depression diagnosis. This extensive dataset supports robust model evaluation facilitating the development and validation of more effective depression detection methodologies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Data Pre-Processing",
      "text": "We remove the silence and trim the audio to 5 seconds. Further, we resample all the audios to 16kHz before passing it to the PTMs for feature extraction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Training Details",
      "text": "We use Mean Squared Error (MSE) as the loss function and Adam as the optimizer. We employ a learning rate of 1e-3 and train all the models for 30 epochs, with a batch size of 64. We use the official split given in the dataset. We trained the models on training set, validated on validation set and tested on testing set. To evaluate model performance comprehensively, we utilize mean absolute error (MAE) and root mean squared error (RMSE) as the evaluation metrics keeping in line with previous research  [23] ,  [24] . The training parameters for downstream models with different fusion techniques range between 0.2M to 1.1M.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Experimental Results",
      "text": "Firstly, as baseline experiments, we train downstream models on top of the three NSFs considered in our study. We train different downstream models, such as Support Vector Regression (SVR), Random Forest (RF), Fully Connected Network (FCN), and lastly CNN. We use the default parameters as given in Scikit-learn library for SVR and RF. For FCN, we have two dense layers of 120 and 90 neurons, followed by the output that  predicts a continuous variable. We follow the same modeling for the CNN model as followed in Section II-B Early Fusion technique. We follow the same training and evaluation regime as the fusion techniques.\n\nThe scores for all the tables are scores on the test set. Table  I  details the performance of different downstream models trained on various NSFs on E-DAIC. Among the NSFs evaluated, TRILLsson with CNN, achieves the lowest RMSE (6.56) and MAE (5.03) scores, indicating superior performance in capturing subtle markers of depression. Conversely, the xvector features exhibit comparatively higher error rates than the other NSFs.\n\nTable  II  presents the evaluation results of downstream models trained on different combinations of NSFs, employing various fusion techniques. The results demonstrate that FuSeR, the proposed fusion framework, consistently outperforms the baseline fusion techniques such as Early, Average, and Concatenation methods as well as the individual NSFs. Specifically, when combining x-vector and emoHuBERT, FuSeR achieves an RMSE of 6.43 and an MAE of 4.98, outperforming the concatenated fusion approach with an RMSE of 6.55 and MAE of 4.99. Notably, the combination of all three NSFsx-vector, emoHuBERT, and TRILLsson -with FuSeR yields the topmost performance delivering an RMSE of 5.51 and an MAE of 4.48. This highlights the effectiveness of FuSeR in integrating diverse feature sets to enhance model performance.\n\nTable  III  provides a comparison of FuSeR approach against SOTA on the E-DAIC dataset. The results reveal that FuSeR, using the combination of x-vector, emoHuBERT, and TRILLsson, achieves the lowest RMSE of 5.51 and MAE of 4.48, outperforming previous SOTA work. For instance, the GRU model with DS-VGG and BoAW-e features achieves higher RMSE values of 9.33 and 8.19, respectively, demonstrating that FuSeR not only improves upon current methods but sets a new benchmark in depression detection performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Additional Experiments",
      "text": "In our additional experiments, we evaluated the generalizability of the proposed FuSeR framework on the ANDROID dataset  [25] , a recent Italian dataset designed for depression detection. This dataset includes tasks such as Interview (I) and Reading (R). Notably, the proposed framework, FuSeR also demonstrates strong performance in a binary depression detection (Yes/No) scenario, highlighting its versatility and robustness across different types of tasks. For all the experiments on ANDROID, we follow the same pre-processing steps, modeling, and training details as followed in E-DAIC.\n\nTable  IV  presents the results for downstream models trained on individual NSFs for the ANDROID dataset. Among the NSFs evaluated, TRILLsson consistently demonstrates superior performance across all models. Specifically, CNN with TRILLsson achieves the highest Accuracy and F1-score for both the Interview (80.52% and 78.68%) and Reading (75.13% and 73.94%) tasks, indicating that TRILLsson features provide the most robust representation for depression detection. In contrast, x-vector features show relatively lower performance, with CNN achieving 66.32% Accuracy and 64.71% F1-score for the Interview task, and 74.11% Accuracy and 70.12% F1score for the Reading task.\n\nTable  V  showcases the performance of various feature fusion techniques on the ANDROID dataset. The proposed FuSeR framework significantly outperforms other fusion methods such as Early, Average, and Concatenation across all feature combinations. For example, with the combination of x-vector and emoHuBERT features, FuSeR achieves an Accuracy of 68.91% and an F1-score of 67.73% for the Interview task, and 75.38% Accuracy and 75.31% F1-score for the Reading task, surpassing the concatenated approach (65.39% and 64.29% for the Interview task, and 75.13% and 72.19% for the Reading task).\n\nWhen combining x-vector, emoHuBERT, and TRILLsson through FuSeR, we achieve the topmost results, with an Accuracy of 87.93% and an F1-score of 87.84% for the Interview task, and 84.72% Accuracy and 83.35% F1-score for the Reading task. These results underscore FuSeR's capacity to leverage complementary information from different NSFs effectively and setting a new SOTA on ANDROID dataset. Overall, these additional experiments validate the robustness and adaptability of the FuSeR for improved depression detection.\n\nIV. CONCLUSION In this work, we explore and show the potential of exploiting the complementary strength of NSFs to capture subtle  markers of depression for better depression detection. We explore combination of different NSFs such as from PTMs trained for paralinguistic speech processing (TRILLs-son), speaker recognition (x-vector), and emotion recognition (emo-HuBERT) using different fusion strategies. We also propose a simple novel framework, FuSeR for effective fusion of the NSFs. FuSeR outperforms individual NSFs and baseline fusion methods such as early fusion, average fusion, and concatenation fusion, achieving SOTA results on the E-DAIC benchmark with an RMSE of 5.51 and an MAE of 4.48, demonstrating its robustness for depression detection. Our findings demonstrate that the fusion of these diverse NSFs results in complementary behavior, significantly enhancing depression detection performance. Our study will inspire future research in coming up with more optimal fusion strategies for combining NSFs for depression detection and serve as a benchmark for the same.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Early Fusion: It is a basic type of fusion, where we concat",
      "page": 2
    },
    {
      "caption": "Figure 1: Modeling architecture for average/concatenation-based",
      "page": 2
    },
    {
      "caption": "Figure 1: ). For the FCN module, we apply the same modeling",
      "page": 2
    },
    {
      "caption": "Figure 1: First, we add convolutional",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "orchidp@iiitd.ac.in"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "Abstract—In this study, we address the challenge of depression\nRegression\n(LR)\n[8]. The\nadvent\nof\ndeep\nlearning\nbrought"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "detection from speech,\nfocusing on the potential of non-semantic"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "a paradigm shift,\nenabling end-to-end neural networks\nsuch"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "features\n(NSFs)\nto capture subtle markers of depression. While"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "as CNNs\nand LSTMs\nto\ncapture\ncomplex\nspeech\npatterns"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "prior research has leveraged various features for this task, NSFs-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "more effectively [9], [10]. Recent advancements have seen the"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "extracted from pre-trained models\n(PTMs) designed for non-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "introduction of attention-based mechanisms and multi-modal\nsemantic tasks such as paralinguistic speech processing (TRILLs-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "son),\nspeaker\nrecognition\n(x-vector),\nand\nemotion\nrecognition\napproaches,\nleveraging audio, video, and text data to enhance"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "(emoHuBERT)-have\nshown\nsignificant\npromise. However,\nthe"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "model\nrobustness and accuracy [11],\n[12]. Furthermore, with"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "potential of combining these diverse features has not been fully"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "the utilization of pre-trained models (PTMs) designed for non-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "explored.\nIn this work, we demonstrate that\nthe amalgamation"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "semantic tasks,\nincluding TRILLsson for paralinguistic speech"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "of NSFs results in complementary behavior,\nleading to enhanced"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "processing\n[13],\nx-vector\nfor\nspeaker\nrecognition\n[14],\nand\ndepression\ndetection\nperformance.\nFurthermore,\nto\nour\nend,"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "we\nintroduce\na\nsimple\nnovel\nframework, FuSeR,\ndesigned\nto\nmodels\ntrained for\nspeech emotion recognition (SER)\n[15],"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "effectively combine these features. Our results show that FuSeR"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "further\nenriched\nthe\nfeature\nsets\nemployed\nin\ndepression"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "outperforms models utilizing individual NSFs as well as baseline"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "detection, yielding significant performance\nimprovements\nin"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "fusion\ntechniques\nand\nobtains\nstate-of-the-art\n(SOTA)\nperfor-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "speech-based depression detection. These non-semantic\nfea-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "mance in E-DAIC benchmark with RMSE of 5.51 and MAE of"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "tures\n(NSFs) excel at capturing subtle vocal patterns\nindica-\n4.48, establishing it as a robust approach for depression detection."
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "tive of depressive\nstates, outperforming traditional\nsemantic"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "Index Terms: Depression Detection, Non-semantic Features,\nfeatures."
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "TRILLsson"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "Despite these advancements,\nthe majority of\nresearch has"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "I.\nINTRODUCTION\nconcentrated on individual NSFs\nsets,\nleaving untapped po-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "tential\nin\ntheir\ncombined\nuse. These\nfragmented\nfocus\non\nDepression\nis\nthe\nsilent\nepidemic\nof\nthe\n21st\ncentury,"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "individual NSFs overlooks\nthe opportunity to leverage\ntheir\naffecting millions but spoken of by few. With over 300 million"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "complementary strengths collectively. This raises a critical re-\npeople worldwide\nsuffering\nfrom depression,\nthe\ncondition"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "search question: How can we effectively amalgamate multiple\nnot\nonly\ndiminishes\nquality\nof\nlife\nbut\nalso\ncontributes\nto"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "NSFs to enhance depression detection performance, and what\nsevere\neconomic\nand\nsocial\nburdens1. The\ninsidious\nnature"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "are the optimal methods for doing so?\nof depression often leads to delayed diagnosis and treatment,"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "exacerbating its effects. Early detection is critical, yet\ntradi-\nTo address this question, our research presents the following"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "tional methods rely heavily on self-reporting, which may not\ncontributions:"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "always be accurate or timely. As a result, there is an increasing"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "• A comprehensive investigation into the fusion of various"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "need for innovative, objective methods to identify depression,"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "NSFs,\nincluding\nx-vector, TRILLsson,\nand\nfrom PTM"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "particularly\nin\nits\nearly\nstages.\nIn\nthis\ncontext,\nleveraging"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "trained for\nemotion recognition (emoHuBERT)\nand an"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "speech as a medium for detecting depressive symptoms offers"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "evaluation of different fusion techniques. This work tack-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "a non-invasive\nand potentially more\naccessible\napproach to"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "les the key question of “What to Fuse and How to Fuse?”"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "diagnosis."
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "to maximize depression detection accuracy."
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "Initial\nefforts\nin\nspeech-based\ndepression\ndetection\ncen-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "• A novel\nand simple\nfeature\nfusion framework, FuSeR,"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "tered on acoustic\nfeatures\nsuch as\nlow pitch,\nreduced pitch"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "which incorporates a bilinear pooling-based feature inter-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "variability,\nslower\nspeaking rate,\nincreased pause\nfrequency,"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "action for effective fusion. FuSeR with x-vector, TRILLs-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "and alterations in voice quality-markers indicative of depres-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "son,\nand emoHuBERT features demonstrates\nenhanced"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "sive\nsymptoms\n[1],\n[2],\n[3]. These\nparalinguistic\nfeatures,"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "performance\ncompared to models using individual\nfea-"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "encompassing prosody, voice quality,\nand articulation, were"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "tures and those employing baseline fusion methods. With"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "initially analyzed using classical ML algorithms like Gaussian"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "FuSeR, we\nalso\nreport\nSOTA results\nin\nthe E-DAIC"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "Mixture Models (GMMs) [4], [5], SVMs [6], [7], and Logistic"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "benchmark with RMSE of\n5.51\nand MAE of\n4.48\nin"
        },
        {
          "India\n¶Independent Researcher, UK, ∥University of Tartu, Estonia, ∗∗IIT-Dharwad,\nIndia, ††IIIT-Dharwad,": "1https://www.who.int/news-room/fact-sheets/detail/depression\ncomparison to the previous SOTA."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "the reviewing process."
        },
        {
          "We will release the models and codes made for this study after": "II. METHODOLOGY"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "In\nthis\nsection, we\nprovide\nan\noverview of\nthe NSFs"
        },
        {
          "We will release the models and codes made for this study after": "utilized in our\nexperiments, baseline\nfusion techniques,\nand"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "the proposed framework, FuSeR."
        },
        {
          "We will release the models and codes made for this study after": "A. Feature Representations"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "x-vector\n[16]:\nIt\nis a time-delay neural network trained for"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "speaker\nrecognition.\nIt\nshows SOTA performance\ncompared"
        },
        {
          "We will release the models and codes made for this study after": "to its predecessor,\ni-vector. We\ninclude x-vector\nin our\nex-"
        },
        {
          "We will release the models and codes made for this study after": "periments as\nit has\nshown effectiveness\nin depression detec-"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "tion [14] and as well as related tasks like SER [17]. For\nthis"
        },
        {
          "We will release the models and codes made for this study after": "study, we use\nthe\nspeechbrain x-vector model2. We\nextract"
        },
        {
          "We will release the models and codes made for this study after": "features of 512-dimension by averaging across time."
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "TRILLsson [18]: It\nis distilled from the SOTA universal par-"
        },
        {
          "We will release the models and codes made for this study after": "alinguistic conformer\n(CAP12) model\nthat has\nshown SOTA"
        },
        {
          "We will release the models and codes made for this study after": "in various non-semantic or paralinguistic tasks such as SER,"
        },
        {
          "We will release the models and codes made for this study after": "speaker recognition, deepfake detection, and so on. TRILLsson"
        },
        {
          "We will release the models and codes made for this study after": "is open-sourced, but CAP12 is not. TRILLsson also achieves"
        },
        {
          "We will release the models and codes made for this study after": "near SOTA performance on the Non-Semantic Speech (NOSS)"
        },
        {
          "We will release the models and codes made for this study after": "benchmark. For\nthis\nstudy, we utilize\nthe 1024-dimensional"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "feature vectors generated from TRILLsson3."
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "emoHuBERT: For\nextracting\nemotion-specific\nfeatures, we"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "fine-tune HuBERT4\n[19],\noriginally\npre-trained\nin\na\nself-"
        },
        {
          "We will release the models and codes made for this study after": "supervised manner\nfor comprehensive speech tasks and name"
        },
        {
          "We will release the models and codes made for this study after": "it as, emoHuBERT. We train it\nfor 50 epochs on CREMA-"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "D [20],\na\nbenchmark SER dataset,\nby\nattaching\na\nprobing"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "head on top of\nthe HuBERT model architecture. We unfreeze"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "all\nthe HuBERT layers for fine-tuning and extract\nfeatures of"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "768 from the last hidden state by mean pooling for depression"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "detection."
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "B. Fusion Techniques"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "Here, we discuss various fusion techniques employed in our"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "study for\nthe fusion of NSFs and the downstream modeling"
        },
        {
          "We will release the models and codes made for this study after": "employed with the fusion techniques. For a detailed visualiza-"
        },
        {
          "We will release the models and codes made for this study after": "tion,\nrefer\nto Figure 1."
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "Early Fusion:\nIt\nis a basic type of\nfusion, where we concat"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "the NSFs directly after being extracted from the PTMs. We"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "then use a convolutional block with a 1D-CNN layer of 32"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "filters with a 3 × 3 filter\nsize,\nfollowed by a max-pooling"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "layer. We then append a FCN module on top of\nthe flattened"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "features after maxpooling. For\nthe FCN module, we attach a"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "dense layer of 120 neurons followed by a output neuron that"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "predicts a continous variable."
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "Average Fusion: We add convolutional blocks with the same"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "architectural details as used in Early Fusion CNN model on the"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "top of each individual NSFs. We flatten the features after max-"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "pooling followed by linear projection to the same dimension;"
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "we\naverage\nthe\nfeatures\nfrom different NSFs\nand attach an"
        },
        {
          "We will release the models and codes made for this study after": "FCN module on top of\nthe averaged features (See Figure 1)."
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": ""
        },
        {
          "We will release the models and codes made for this study after": "2https://huggingface.co/speechbrain/spkrec-xvect-voxceleb"
        },
        {
          "We will release the models and codes made for this study after": "3https://tfhub.dev/google/nonsemantic-speech-benchmark/trillsson4/1"
        },
        {
          "We will release the models and codes made for this study after": "4https://huggingface.co/facebook/hubert-base-ls960"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Evaluation results with downstream models trained": "on different NSFs",
          "TABLE II: Evaluation results with downstream models trained": "the"
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "RMSE and MAE,",
          "TABLE II: Evaluation results with downstream models trained": "For RMSE and MAE,"
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": "performance."
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "Feature",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "x-vector",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "emoHuBERT",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "TRILLsson",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "flattened and passed to an FCN module with similar modeling",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "details to the Early Fusion FCN module, followed by an output",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "neuron.",
          "TABLE II: Evaluation results with downstream models trained": ""
        },
        {
          "TABLE I: Evaluation results with downstream models trained": "",
          "TABLE II: Evaluation results with downstream models trained": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "iments": "steps, modeling, and training details as followed in E-DAIC.",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "Acc(R), and F1(R)"
        },
        {
          "iments": "",
          "on ANDROID, we": "Table IV presents the results for downstream models trained",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "view (I) and Reading (R) tasks, respectively. Scores are in %."
        },
        {
          "iments": "on individual NSFs",
          "on ANDROID, we": "",
          "follow the": "for",
          "same": "the ANDROID dataset. Among the",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "Feature"
        },
        {
          "iments": "NSFs evaluated, TRILLsson consistently demonstrates",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "supe-",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "rior performance",
          "on ANDROID, we": "",
          "follow the": "across",
          "same": "all models. Specifically, CNN with",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "x-vector"
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "+"
        },
        {
          "iments": "TRILLsson achieves",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "the highest Accuracy and F1-score",
          "pre-processing": "for",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "emoHuBERT"
        },
        {
          "iments": "both the Interview (80.52% and 78.68%) and Reading (75.13%",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "and 73.94%) tasks, indicating that TRILLsson features provide",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "x-vector"
        },
        {
          "iments": "the most",
          "on ANDROID, we": "robust",
          "follow the": "representation\nfor",
          "same": "depression",
          "pre-processing": "detection.\nIn",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "+"
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "TRILLsson"
        },
        {
          "iments": "contrast, x-vector features show relatively lower performance,",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "with CNN achieving 66.32% Accuracy and 64.71% F1-score",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "for the Interview task, and 74.11% Accuracy and 70.12% F1-",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "emoHuBERT"
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "+"
        },
        {
          "iments": "score for",
          "on ANDROID, we": "the Reading task.",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "TRILLsson"
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "Table V showcases",
          "follow the": "the",
          "same": "performance\nof\nvarious",
          "pre-processing": "feature",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "fusion techniques on the ANDROID dataset. The proposed",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "x-vector"
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "+"
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "FuSeR framework",
          "on ANDROID, we": "",
          "follow the": "significantly",
          "same": "outperforms\nother",
          "pre-processing": "fusion",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "emoHuBERT"
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "+"
        },
        {
          "iments": "methods",
          "on ANDROID, we": "such",
          "follow the": "as Early, Average,",
          "same": "and Concatenation",
          "pre-processing": "across",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        },
        {
          "iments": "",
          "on ANDROID, we": "",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": "TRILLsson"
        },
        {
          "iments": "all",
          "on ANDROID, we": "feature combinations. For example, with the combination",
          "follow the": "",
          "same": "",
          "pre-processing": "",
          "combination of different NSFs for ANDROID. Acc(I), F1(I),": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "for ANDROID. Acc(I), F1(I), Acc(R), and F1(R)"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "resent accuracy and macro-average F1-score for the Interview"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "respectively. Scores are in %."
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "Acc(I)\nF1(I)"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "62.49\n60.49"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "63.83\n61.20"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "64.97\n61.70"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "66.32\n64.71"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "64.25\n61.16"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "73.89\n71.86"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "75.65\n72.39"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "75.96\n73.68"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "69.02\n64.91"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "79.38\n77.42"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "79.38\n77.57"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "80.52\n78.68"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "results with models"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "combination of different NSFs for ANDROID. Acc(I), F1(I),"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "represent accuracy and F1-score for"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "view (I) and Reading (R) tasks, respectively. Scores are in %."
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "Acc(I)\nF1(I)"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "63.52\n62.16"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "66.32\n64.78"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "65.39\n64.29"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "68.91\n67.73"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "69.12\n68.89"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "73.47\n70.78"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "74.82\n73.69"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "80.41\n78.78"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "74.30\n72.88"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "74.72\n73.03"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "76.06\n74.51"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "83.32\n81.73"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "78.45\n76.60"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "79.38\n77.63"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "78.76\n77.41"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "87.93\n87.84"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "better\ndepression"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "combination of different NSFs"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "speech\nprocessing"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "speaker recognition (x-vector), and emotion recognition (emo-"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "framework, FuSeR for"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "early\nfusion,"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "concatenation fusion, achieving SOTA results on the E-DAIC"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "an RMSE of\n5.51\nand"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "for\ndepression"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "the\nfusion of"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "results in complementary behavior, significantly enhancing de-"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "pression detection performance. Our study will"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "research in coming up with more optimal"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": "for depression detection and serve as a"
        },
        {
          "TABLE IV: Evaluation results with models trained on different": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "guistic Speech Representations,” in Proc.\nInterspeech 2022, 2022, pp."
        },
        {
          "REFERENCES": "[1]\nS. Alghowinem, “From joyous to clinically depressed: Mood detection",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "356–360."
        },
        {
          "REFERENCES": "using multimodal analysis of a person’s appearance and speech,” in 2013",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "[19] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and"
        },
        {
          "REFERENCES": "Humaine Association Conference on Affective Computing and Intelligent",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "A. Mohamed, “Hubert: Self-supervised speech representation learning"
        },
        {
          "REFERENCES": "Interaction.\nIEEE, 2013, pp. 648–654.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "IEEE/ACM Transactions\non\nby masked\nprediction\nof\nhidden\nunits,”"
        },
        {
          "REFERENCES": "[2] Y. Yang, C. Fairbairn, and J. F. Cohn, “Detecting depression severity",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "REFERENCES": "from vocal prosody,” IEEE transactions on affective computing, vol. 4,",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "[20] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,"
        },
        {
          "REFERENCES": "no. 2, pp. 142–150, 2012.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "and R. Verma, “Crema-d: Crowd-sourced emotional multimodal actors"
        },
        {
          "REFERENCES": "[3]\nJ. C. Mundt, A. P. Vogel, D. E. Feltner, and W. R. Lenderking, “Vocal",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "dataset,” IEEE transactions on affective computing, vol. 5, no. 4, pp."
        },
        {
          "REFERENCES": "acoustic\nbiomarkers\nof\ndepression\nseverity\nand\ntreatment\nresponse,”",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "377–390, 2014."
        },
        {
          "REFERENCES": "Biological psychiatry, vol. 72, no. 7, pp. 580–587, 2012.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "[21] G. K. Kumar and K. Nandakumar, “Hate-clipper: Multimodal hateful"
        },
        {
          "REFERENCES": "[4] B. S. Helfer, T. F. Quatieri, J. R. Williamson, D. D. Mehta, R. Horwitz,",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "meme classification based on cross-modal\ninteraction of clip features,”"
        },
        {
          "REFERENCES": "and B. Yu,\n“Classification\nof\ndepression\nstate\nbased\non\narticulatory",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "arXiv preprint arXiv:2210.05916, 2022."
        },
        {
          "REFERENCES": "precision.” in Interspeech, 2013, pp. 2172–2176.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "[22]\nJ. Gratch, R. Artstein, G. M. Lucas, G. Stratou, S. Scherer, A. Nazarian,"
        },
        {
          "REFERENCES": "[5]\nJ. R. Williamson, T. F. Quatieri, B. S. Helfer, R. Horwitz, B. Yu,",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "et\nR. Wood,\nJ. Boberg, D. DeVault, S. Marsella\nal.,\n“The\ndistress"
        },
        {
          "REFERENCES": "and D. D. Mehta,\n“Vocal\nbiomarkers\nof\ndepression\nbased\non motor",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "analysis interview corpus of human and computer interviews.” in LREC."
        },
        {
          "REFERENCES": "the 3rd ACM international workshop\nincoordination,” in Proceedings of",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "Reykjavik, 2014, pp. 3123–3128."
        },
        {
          "REFERENCES": "on Audio/visual emotion challenge, 2013, pp. 41–48.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "[23]\nF. Ringeval, B. Schuller, M. Valstar, N. Cummins, R. Cowie, L. Tavabi,"
        },
        {
          "REFERENCES": "[6] N. Cummins, J. Epps, and E. Ambikairajah, “Spectro-temporal analysis",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "M. Schmitt, S. Alisamir, S. Amiriparian, E.-M. Messner et al., “Avec"
        },
        {
          "REFERENCES": "of\nspeech\naffected\nby\ndepression\nand\npsychomotor\nretardation,”\nin",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "2019 workshop and challenge: state-of-mind, detecting depression with"
        },
        {
          "REFERENCES": "2013 IEEE international\nconference on acoustics,\nspeech and signal",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "of\nthe\n9th\nai,\nand\ncross-cultural\naffect\nrecognition,”\nin Proceedings"
        },
        {
          "REFERENCES": "processing.\nIEEE, 2013, pp. 7542–7546.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "International on Audio/visual Emotion Challenge and Workshop, 2019,"
        },
        {
          "REFERENCES": "[7] M. Nasir, A.\nJati, P. G. Shivakumar, S. Nallan Chakravarthula,\nand",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "pp. 3–12."
        },
        {
          "REFERENCES": "P. Georgiou,\n“Multimodal\nand multiresolution\ndepression\ndetection",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "[24] Q. Li, D. Wang, Y. Ren, Y. Gao,\nand Y. Li,\n“Fta-net: A frequency"
        },
        {
          "REFERENCES": "the 6th\nfrom speech and facial\nlandmark features,” in Proceedings of",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "and time attention network for speech depression detection,” in INTER-"
        },
        {
          "REFERENCES": "international workshop on audio/visual\nemotion challenge, 2016, pp.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "SPEECH 2023, 2023, pp. 1723–1727."
        },
        {
          "REFERENCES": "43–50.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "[25]\nF. Tao, A. Esposito, and A. Vinciarelli, “The Androids Corpus: A New"
        },
        {
          "REFERENCES": "[8] A. Jan, H. Meng, Y. F. B. A. Gaus, and F. Zhang, “Artificial\nintelligent",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "Publicly Available Benchmark for Speech Based Depression Detection,”"
        },
        {
          "REFERENCES": "system for\nautomatic\ndepression\nlevel\nanalysis\nthrough\nvisual\nand",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": "in Proc.\nINTERSPEECH 2023, 2023, pp. 4149–4153."
        },
        {
          "REFERENCES": "vocal expressions,” IEEE Transactions on Cognitive and Developmental",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "Systems, vol. 10, no. 3, pp. 668–680, 2017.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[9] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou,",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "B.\nSchuller,\nand\nS.\nZafeiriou,\n“Adieu\nfeatures?\nend-to-end\nspeech",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "emotion recognition using a deep convolutional\nrecurrent network,” in",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "2016 IEEE international\nconference on acoustics,\nspeech and signal",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "processing (ICASSP).\nIEEE, 2016, pp. 5200–5204.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[10] A. Othmani, D. Kadoch, K. Bentounes, E. Rejaibi, R. Alfred, and A. Ha-",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "did,\n“Towards\nrobust deep neural networks\nfor\naffect\nand depression",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "ICPR International\nrecognition from speech,” in Pattern Recognition.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "Workshops and Challenges: Virtual Event, January 10–15, 2021, Pro-",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "ceedings, Part\nII.\nSpringer, 2021, pp. 5–19.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[11]\nZ. Zhao, Z. Bao, Z. Zhang, N. Cummins, H. Wang, and B. Schuller,",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "“Hierarchical\nattention\ntransfer\nnetworks\nfor\ndepression\nassessment",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "from speech,” in ICASSP 2020-2020 IEEE international conference on",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "acoustics,\nspeech and signal processing (ICASSP).\nIEEE, 2020, pp.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "7159–7163.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[12]\nL. Yang, D.\nJiang, X. Xia, E. Pei, M. C. Oveneke,\nand H. Sahli,",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "“Multimodal measurement of depression using deep learning models,”",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "the 7th Annual Workshop on Audio/Visual Emotion\nin Proceedings of",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "Challenge, 2017, pp. 53–59.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[13]\nE. L. Campbell, J. Dineley, P. Conde, F. Matcham, K. M. White, C. Oet-",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "zmann, S. Simblett, S. Bruce, A. A. Folarin, T. Wykes, S. Vairavan,",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "R. J. B. Dobson, L. Docio-Fernandez, C. Garcia-Mateo, V. A. Narayan,",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "M. Hotopf, and N. Cummins, “Classifying depression symptom severity:",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "Assessment of\nspeech representations\nin personalized and generalized",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "machine learning models.” in INTERSPEECH 2023, 2023, pp. 1738–",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "1742.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[14]\nJ. V. Egas-L´opez, G. Kiss, D. Sztah´o,\nand G. Gosztolya,\n“Automatic",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "assessment\nof\nthe\ndegree\nof\nclinical\ndepression\nfrom speech\nusing",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "ICASSP 2022-2022\nIEEE International Conference\non\nx-vectors,”\nin",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2022, pp.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "8502–8506.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[15] W. Wu, M. Wu, and K. Yu, “Climate and weather:\nInspecting depres-",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "sion detection via\nemotion recognition,”\nin ICASSP 2022-2022 IEEE",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "International Conference on Acoustics, Speech and Signal Processing",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "(ICASSP).\nIEEE, 2022, pp. 6262–6266.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[16] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "vectors: Robust dnn embeddings for speaker recognition,” in 2018 IEEE",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "International Conference on Acoustics, Speech and Signal Processing",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "(ICASSP), 2018, pp. 5329–5333.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "[17] O. C.\nPhukan, A. B. Buduru,\nand R.\nSharma,\n“Transforming\nthe",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "embeddings: A lightweight\ntechnique\nfor\nspeech emotion recognition",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        },
        {
          "REFERENCES": "tasks,” in Interspeech, 2023.",
          "[18]\nJ. Shor and S. Venugopalan, “TRILLsson: Distilled Universal Paralin-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "From joyous to clinically depressed: Mood detection using multimodal analysis of a person's appearance and speech",
      "authors": [
        "S Alghowinem"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "2",
      "title": "Detecting depression severity from vocal prosody",
      "authors": [
        "Y Yang",
        "C Fairbairn",
        "J Cohn"
      ],
      "year": "2012",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "3",
      "title": "Vocal acoustic biomarkers of depression severity and treatment response",
      "authors": [
        "J Mundt",
        "A Vogel",
        "D Feltner",
        "W Lenderking"
      ],
      "year": "2012",
      "venue": "Biological psychiatry"
    },
    {
      "citation_id": "4",
      "title": "Classification of depression state based on articulatory precision",
      "authors": [
        "B Helfer",
        "T Quatieri",
        "J Williamson",
        "D Mehta",
        "R Horwitz",
        "B Yu"
      ],
      "year": "2013",
      "venue": "Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Vocal biomarkers of depression based on motor incoordination",
      "authors": [
        "J Williamson",
        "T Quatieri",
        "B Helfer",
        "R Horwitz",
        "B Yu",
        "D Mehta"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge"
    },
    {
      "citation_id": "6",
      "title": "Spectro-temporal analysis of speech affected by depression and psychomotor retardation",
      "authors": [
        "N Cummins",
        "J Epps",
        "E Ambikairajah"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "7",
      "title": "Multimodal and multiresolution depression detection from speech and facial landmark features",
      "authors": [
        "M Nasir",
        "A Jati",
        "P Shivakumar",
        "S Nallan",
        "P Chakravarthula",
        "Georgiou"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international workshop on audio/visual emotion challenge"
    },
    {
      "citation_id": "8",
      "title": "Artificial intelligent system for automatic depression level analysis through visual and vocal expressions",
      "authors": [
        "A Jan",
        "H Meng",
        "Y Gaus",
        "F Zhang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "9",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "10",
      "title": "Towards robust deep neural networks for affect and depression recognition from speech",
      "authors": [
        "A Othmani",
        "D Kadoch",
        "K Bentounes",
        "E Rejaibi",
        "R Alfred",
        "A Hadid"
      ],
      "year": "2021",
      "venue": "Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event"
    },
    {
      "citation_id": "11",
      "title": "Hierarchical attention transfer networks for depression assessment from speech",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Multimodal measurement of depression using deep learning models",
      "authors": [
        "L Yang",
        "D Jiang",
        "X Xia",
        "E Pei",
        "M Oveneke",
        "H Sahli"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "13",
      "title": "Classifying depression symptom severity: Assessment of speech representations in personalized and generalized machine learning models",
      "authors": [
        "E Campbell",
        "J Dineley",
        "P Conde",
        "F Matcham",
        "K White",
        "C Oetzmann",
        "S Simblett",
        "S Bruce",
        "A Folarin",
        "T Wykes",
        "S Vairavan",
        "R Dobson",
        "L Docio-Fernandez",
        "C Garcia-Mateo",
        "V Narayan",
        "M Hotopf",
        "N Cummins"
      ],
      "venue": "Classifying depression symptom severity: Assessment of speech representations in personalized and generalized machine learning models"
    },
    {
      "citation_id": "14",
      "title": "Automatic assessment of the degree of clinical depression from speech using x-vectors",
      "authors": [
        "J Egas-López",
        "G Kiss",
        "D Sztahó",
        "G Gosztolya"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Climate and weather: Inspecting depression detection via emotion recognition",
      "authors": [
        "W Wu",
        "M Wu",
        "K Yu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Xvectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks",
      "authors": [
        "O Phukan",
        "A Buduru",
        "R Sharma"
      ],
      "year": "2023",
      "venue": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks"
    },
    {
      "citation_id": "18",
      "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
      "authors": [
        "J Shor",
        "S Venugopalan"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "21",
      "title": "Hate-clipper: Multimodal hateful meme classification based on cross-modal interaction of clip features",
      "authors": [
        "G Kumar",
        "K Nandakumar"
      ],
      "year": "2022",
      "venue": "Hate-clipper: Multimodal hateful meme classification based on cross-modal interaction of clip features",
      "arxiv": "arXiv:2210.05916"
    },
    {
      "citation_id": "22",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "J Gratch",
        "R Artstein",
        "G Lucas",
        "G Stratou",
        "S Scherer",
        "A Nazarian",
        "R Wood",
        "J Boberg",
        "D Devault",
        "S Marsella"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "23",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "24",
      "title": "Fta-net: A frequency and time attention network for speech depression detection",
      "authors": [
        "Q Li",
        "D Wang",
        "Y Ren",
        "Y Gao",
        "Y Li"
      ],
      "year": "2023",
      "venue": "INTER-SPEECH 2023"
    },
    {
      "citation_id": "25",
      "title": "The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection",
      "authors": [
        "F Tao",
        "A Esposito",
        "A Vinciarelli"
      ],
      "year": "2023",
      "venue": "The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection"
    }
  ]
}