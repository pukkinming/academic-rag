{
  "paper_id": "2508.19533v1",
  "title": "Emotion Transfer With Enhanced Prototype For Unseen Emotion Recognition In Conversation",
  "published": "2025-08-27T03:16:16Z",
  "authors": [
    "Kun Peng",
    "Cong Cao",
    "Hao Peng",
    "Guanlin Wu",
    "Zhifeng Hao",
    "Lei Jiang",
    "Yanbing Liu",
    "Philip S. Yu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Current Emotion Recognition in Conversation (ERC) research follows a closed-domain assumption. However, there is no clear consensus on emotion classification in psychology, which presents a challenge for models when it comes to recognizing previously unseen emotions in real-world applications. To bridge this gap, we introduce the Unseen Emotion Recognition in Conversation (UERC) task for the first time and propose ProEmoTrans, a solid prototype-based emotion transfer framework. This prototype-based approach shows promise but still faces key challenges: First, implicit expressions complicate emotion definition, which we address by proposing an LLM-enhanced description approach. Second, utterance encoding in long conversations is difficult, which we tackle with a proposed parameter-free mechanism for efficient encoding and overfitting prevention. Finally, the Markovian flow nature of emotions is hard to transfer, which we address with an improved Attention Viterbi Decoding (AVD) method to transfer seen emotion transitions to unseen emotions. Extensive experiments on three datasets show that our method serves as a strong baseline for preliminary exploration in this new area.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC) aims to predict the emotional state of each utterance in multi-turn conversations, holding significant research value in areas such as Conversational Sentiment Analysis  (Li et al., 2023)  and Empathetic Responses  (Peng et al., 2022) . However, in the field of psychology, existing research works  (Ekman, 1999; Plutchik and Kellerman, 2013; Cowen and Keltner, 2017)  feature a variety of emotion classification theories, yet they have not reached a clear consensus 1 . Due to the complex defini-1 For instance,  Plutchik and Kellerman (2013)  categorizes emotions into 32 types, while  Cowen and Keltner (2017)    tions and the various classification theories, in realworld applications, such as open-domain dialogue systems, it is likely to occur new emotions that are unseen in the training stage. As shown in  Figure 1 (a) , the emotion labels across three widely used datasets  (Busso et al., 2008; Zahiri and Choi, 2018; Poria et al., 2019)  exhibit significant nonoverlapping portions. This makes it challenging to directly apply models trained on a single dataset to other datasets. For instance, a model trained on the MELD dataset may struggle to recognize the emotion powerful in the EmoryNLP dataset.\n\nTo bridge this gap, we introduce the Unseen Emotion Recognition in Conversation (UERC) task for the first time, which aims to predict unseen emotions by leveraging prior knowledge from seen emotions in training data. To address this task, we attempt the prototype-based approaches  (Chen and Li, 2021; Zhao et al., 2023; Li et al., 2024)  to learn a prototype vector for each emotion, helping the model capture the distinct meaning of emotions. However, three key challenges hinder progress. Challenge 1: Implicit emotion expression. Existing methods primarily rely on the provided label descriptions to enhance prototype semantics. Howgorizes emotions into 27 types, including unusual emotions like nostalgia and sexual desire. ever, the UERC task lacks emotion descriptions, and, even more critically, many complex emotions are hard to define clearly, and relying solely on descriptive information is insufficient to obtain robust and faithful prototypes. Challenge 2: Hard utterance encoding. Due to the extensive length of conversation texts, existing ERC methods  (Majumder et al., 2019; Hu et al., 2021; Zhang et al., 2023; Yang et al., 2024)  typically follow two steps: encoding utterance representations first, then modeling inter-utterance features with additional relationlearning modules. However, our preliminary experiments indicate that these additional modules can lead to overfitting the training data, compromising the model's ability to generalize to unseen emotions. Conversely, removing these modules results in losing valuable inter-utterance relations, creating a dilemma. Challenge 3: Unadapted emotion transition. It's found that emotions exhibit a Markov property  (Song et al., 2022b) , whereby the current utterance's emotion is influenced by preceding ones. As illustrated in Figure  1  (b), when the current emotion is Disgust, the transfer score for Anger in the subsequent utterance is notably highest, aligning with intuitive expectations. While the Markov property can effectively aid emotion prediction, the transfer score matrix for unseen emotions cannot be pre-learned.\n\nTo address these challenges, we propose a solid prototype-based emotion transfer framework called ProEmoTrans. Specifically, to address the implicit emotion expression challenge, we first employ a dictionary to obtain all the emotion descriptions. We then leverage the in-context learning capabilities of large language models (LLMs) to generate utterances that implicitly express these emotions, thereby enhancing the model's comprehension of complex emotions. To address the hard utterance encoding challenge, we refrain from using additional relation-learning modules to prevent the model from overfitting to seen emotions. Instead, we propose a Gaussian Self-Attention mechanism to capture inter-utterance relations. This parameter-free mechanism obtains utterance embeddings by using linear combinations of contextual representations, effectively leveraging relation information among utterances at varying distances. To leverage the emotion transition, we propose an improved Attention Viterbi Decoding (AVD) algorithm within the Conditional Random Field (CRF) framework, enabling the capture of transition probabilities for seen emotions between all adjacent utterances. Subsequently, we extend the transition probabilities of seen emotions to unseen emotions by utilizing prototype similarity. Our contributions can be summarized as follows:\n\n1) We propose the UERC task for the first time and introduce a novel model called ProEmoTrans 2  . Extensive experiments on three datasets demonstrate that this method serves as a solid baseline.\n\n2) We leverage the prior knowledge of LLMs to generate implicit contexts that enhance complex emotion prototypes.\n\n3) We introduce a Gaussian self-attention mechanism that effectively utilizes inter-utterance relations while avoiding overfitting to seen emotions.\n\n4) We improve the Viterbi decoding algorithm to extend the transition probabilities of seen emotions to unseen emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "ERC in a text-modality setting is an active research topic. Early RNN-based  (Jiao et al., 2019; Majumder et al., 2019; Hu et al., 2021)  and GCNbased  (Ghosal et al., 2019; Shen et al., 2021; Zhang et al., 2023 ) methods tried to model the temporal features or conversational structures. Some other studies  (Ghosal et al., 2020; Ong et al., 2022)  have also attempted to integrate more common-sense knowledge. The latest contrastive-based methods  (Hu et al., 2023; Yang et al., 2023; Yu et al., 2024)  focus on using contrastive learning to distinguish semantically similar emotions. While these additional modules can effectively help the model fit the distributions of seen emotions, in the UERC setting, they can impair the model's ability to generalize to unseen emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Zero-Shot Learning In Erc",
      "text": "Zero-shot Learning (ZSL) aims to train a model on one label set and then apply it to another set of previously unseen labels. Currently, research on ZSL in the ERC field is quite limited. A work that is closely related to ours is CTPT  (Xu et al., 2023) , which focuses on cross-task few-shot settings, while we are the first to explore the model's ability in zero-shot predicting for unseen emotions. In the zero-shot setting, CTPT primarily improves the recognition of similar emotions across tasks but performs poorly in recognizing unseen emotions.  Prototype alignment  (Chen and Li, 2021; Zhao et al., 2023; Li et al., 2024 ) is a powerful method in ZSL. It first encodes sentence and label information into a hidden vector space, then aligns sentence embeddings with label prototype embeddings using semantic matching. Through this process, the model acquires the ability to generalize label knowledge. During the inference phase, the model encodes the unseen label information and makes predictions through nearest neighbor search. In other prototype-based zero-shot NLP research fields, such as zero-shot relation extraction,  Zhao et al. (2023)  proposes a fine-grained semantic matching method to reduce the negative impact of irrelevant features.  Li et al. (2024)  enhances label prototypes by introducing more side descriptions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Definition",
      "text": "Given a conversation U = {(u 1 , t 1 ), (u 2 , t 2 ), ..., (u N , t N )}, where each utterance u i has only one speaker t i , N is the total number of utterances. Different utterances may belong to the same speaker, so it is possible to have t i = t j (i ̸ = j). The training dataset D s has a set of seen emotions E s , and the test dataset D u has a set of unseen emotions E u . There is no overlap between E s and E u . The objective of the UERC task is to learn from D s and transfer the model to predict the unseen emotion label e uns i ∈ E u of each utterance u i .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Framework Of Proemotrans",
      "text": "The overall architecture of our proposed ProEmo-Trans is illustrated in Figure  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Prototype Encoding",
      "text": "Given a seen emotion word e see i ∈ E s , we can find its corresponding description 3 X desc i in the Wiktionary 4 . However, unlike direct descriptions, emotions in conversation are often expressed implicitly. This gap makes the prototype learned from emotional descriptions lack sufficient generalization, especially for more complex emotions (e.g., powerful).\n\nTo improve the quality of emotion prototypes, we propose the LLM-enhanced Emotion Description (LED) method. We first design a prompt template Write two sentences expressing [MASK]'s emotions. Afterward, by filling in the [MASK] position with the emotion word e see i and leveraging the LLM's prompt generation capabilities, we generate sentences X llm i that implicitly express that emotion. The enhanced description X see i is defined 3 All the descriptions are listed in Appendix C. 4 https://en.m.wiktionary.org as the concatenation of e see i , X desc i and X llm i :\n\nWe feed it into the prototype encoder to obtain the final emotion prototype h see i :\n\nwhere h see i ∈ R d is the first token (i.e., [CLS]) of the last hidden layer. Through the above process, we can encode the prototypes of each emotion word in the seen emotion set E s and obtain\n\nSimilarly, for the unseen emotion set E u , we have\n\n, where n and m are the numbers of emotions in E s and E u , respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Utterance Encoding",
      "text": "Following previous works  (Hu et al., 2021; Shen et al., 2021; Zhang et al., 2023) , due to the conversation text being too lengthy, we use an utterance encoder to obtain the utterance representation h i :\n\nThe representation of all utterances is denoted as H ∈ R N ×d , where N is the number of utterances in U. After that, we propose a non-parametric Gaussian Self-Attention (GSA) mechanism that effectively learns the inter-utterance relationships and alleviates overfitting to seen emotions. Given the token h i ∈ H, the Gaussian attention score A i ∈ R N that attends to H is defined as:\n\nwhere N i ∈ R N are discrete values that follow the Gaussian distribution N (i, σ 2 ), and the variance σ is a hyperparameter. Using the Gaussian attention score, we aggregate highly relevant information from the entire conversation while reducing the impact of distant tokens. This inter-utterance relationship aggregation follows a non-parametric linear operation:\n\nwhere h utte i ∈ R d is the updated utterance representation. The final representation of all utterances is denoted as\n\nThe GSA mechanism has two key properties: First, parameter-free. Previous supervised methods used parameterized modules (such as LSTM and GCN) to learn inter-utterance relationships. However, in unsupervised scenarios, parameterized modules led to overfitting on the training set, hindering generalization on unseen datasets (Appendix B.1). Second, distance-aware learning of inter-utterance relationships. Directly sampling discrete values from a one-dimensional Gaussian distribution based on the distance between utterances, with closer utterances receiving more attention.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Contrastive Similarity And Training",
      "text": "In the above sections, we obtained emotion prototypes and utterance representations. In this section, through nearest neighbor search, we can align utterances with their corresponding emotion labels. Inspired by infoNCE  (Oord et al., 2018) , we define a contrastive similarity to pull the utterance embeddings closer to their corresponding prototype embeddings while pushing apart the inconsistent ones. This similarity S see ∈ R N ×n is defined as:\n\nwhere Eq. (  7 ) is the details of Eq. (  6 ). cos(•) is a cosine similarity function and τ is a temperature hyperparameter. s see ij represents the probability of the i-th utterance expressing the j-th seen emotion.\n\nDue to the transition dependencies between emotions, independent predictions are insufficient. Therefore, we subsequently feed S see into a Conditional Random Field (CRF)  (Lafferty et al., 2001) . For a sequence of predictions: y = (y 1 , y 2 , ..., y N ), its CRF score can be defined as:\n\nwhere M ∈ R (n+2)×n is the transition matrix 5  of the CRF layer. y 0 and y N +1 are the additional start and end tags. The probability of the sequence y is a softmax over the scores of all possible sequences:\n\nwhere Y U represents all possible predicted sequences. Our training goal is to minimize the loss: L = -log(p( y)), where y represents the true sequences.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Inference",
      "text": "The original Viterbi decoding is limited to the seen emotions, and the valuable emotion transition dependencies learned by the CRF layer cannot be adapted to unseen emotions. To address this gap, we propose the Attention Viterbi Decoding (AVD) algorithm. We define the score of the i-th utterance expressing the j-th seen emotion as:\n\nwhere c 0j = M 0,j . Y U [1:i] represents all possible tag sequences from u 1 to u i . The score c ij represents the maximum CRF score of all possible sequences ending with ỹi = e see j . Based on Eq. (  8 ), we can derive that:\n\nThe time complexity of calculating a single c ij is O(n). The overall time complexity for traversing all c ij is O(N n 2 ). During the traversal, we also record the path y * = (y * 1 , ..., y * N ) with the maximum CRF score, such that c N y * N > c N j , ∀j ̸ = y * N . The final output of the AVD algorithm is a probability matrix P ∈ R N ×n , where each p ij ∈ P is defined as follows:\n\nwhere p ij denotes the probability of the k-th utterance expressing the j-th seen emotion. Then, we can enhance the original utterance representation using the seen emotion prototypes:\n\nwhere h ′ i incorporates the seen emotion prototypes after considering similarity (from S see ) and emotional dependencies (from M ).\n\nFor a given u i , the predicted unseen emotion label is obtained through nearest neighbor search:\n\n4 Experiments Settings",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate our ProEmoTrans on three widely used datasets: IEMOCAP  (Busso et al., 2008)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "We utilize Bert-base-uncased  (Vaswani et al., 2017)  as both the utterance and prototype encoder. We use ChatGPT-3.5 to generate enhanced emotion descriptions. In each training batch, we input the emotion descriptions and the utterances into the encoders simultaneously. We use the AdamW optimizer (Kingma and Ba, 2015) with a batch size of 4 and a learning rate of 2e -5. The model is trained for 10 epochs with 100 warm-up steps. All experiments are conducted with an NVIDIA RTX 8000. The variance σ of the Gaussian distribution is set to 0.5, and the temperature τ in Eq. (  7 ) is set to 0.02. We use the weighted-averaged F1 score as the evaluation metric, considering only unseen emotions. In each epoch, we evaluate the training model on the validation set and save the best one to test. All results are averaged across five runs with different random seeds.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baselines",
      "text": "Due to limited research, we choose the following four types of baselines and make necessary modifications to their original architectures to achieve zero-shot prediction capability: Feature-based models: DialogueGCN  (Ghosal et al., 2019) , DialogueCRN  (Hu et al., 2021) , and DualGAT  (Zhang et al., 2023)  design special GNN/RNN-based modules to extract better utter-ance features and use a label-wise classification head to predict the label of each utterance. They use cross-entropy loss computed from the prediction logits and the labels. To enable zero-shot prediction capability, we replace the classification head with a prototype encoder, which enables the model to learn prototype vectors. Then we substitute the original cross-entropy loss with a contrastive loss based on prototype similarity (similar to Eq. 7).\n\nContrastive-based models: SACL-LSTM  (Hu et al., 2023) , SCCL  (Yang et al., 2023) , and EACL  (Yu et al., 2024)  focus on distinguishing semantically similar emotions using contrastive learning. Since these models natively use representation similarity for prediction, no modifications are needed.\n\nFew-shot model: CPTC  (Xu et al., 2023)  leverages sharable cross-task knowledge from the source task to improve few-shot performance. By removing task-specific prompts, it can also perform zeroshot prediction. Unlike in their original work, we evaluate the model only on unseen emotions. To ensure fairness, all of these comparison models use BERT-base-uncased as their backbone.\n\nLLMs: Llama-3.1-8b  (Grattafiori et al., 2024) , Qwen-2.5-7b  (Yang et al., 2025) , GPT-4o  (Bubeck et al., 2023) , and DeepSeek-V3  (Liu et al., 2024)  are used for zero-shot prediction. We design a unified prompt template: Given a conversation: <INPUT>. Please analyze the emotion of each utterance in the conversation. The emotions are included in <LABEL SET>.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Main Results",
      "text": "The overall performance on the three datasets is reported in Table  2 . We have the following observations: Our ProEmoTrans outperforms all other models by a significant margin. Compared to the best baseline DeepSeek-V3, ProEmoTrans achieved improvements in the weighted-averaged F1 score of 11.58%, 6.1%, 4.24%, 2.05%, 3.44%, and 5.88% across six different dataset settings. This demonstrates that our ProEmoTrans exhibits strong performance. The feature-based methods Di-alogueGCN, DialogueCRN, and DualGAT perform poorly due to their excessive parameter modules, which make them prone to overfitting on seen emotions. Few-shot model CPTC also shows inefficient recognition of unseen emotions. The contrastivebased methods SACL-LSTM, SCCL, and EACL focus on improving the distinguishability of different  emotions. Learning differentiated emotional prototypes helps them perform better on the UERC task than other supervised methods. LLMs outperform other baselines with their rich prior knowledge. To investigate how our model improves performance compared to GPT-4o, we provide a more in-depth discussion in the fine-grained analysis (Section 5.5).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation studies to investigate the effectiveness of the key components in our method. The results are shown in Table  3 .\n\n-w/o LED denotes removing the LED module  and directly using dictionary definitions as its description. It is evident that removing the LED results in a significant 14.3% drop in the model's average wF1 score, highlighting the importance of descriptive information in enhancing emotion representation. In the original model, we use two descriptions (2 Desc.) to help the model fully capture the emotional semantics. To investigate the impact of the number of generated descriptions, we conduct experiments comparing the model's performance with different numbers of descriptions. As shown in Table  3 , with one description (-w 1 Desc.), the average wF1 increases by 2.82% compared to no Desp. However, it still shows an 11.48% drop compared to the original 2 Desc.. With three descriptions (-w 3 Desc.), the average wF1 only slightly increases by 0.33%. This indicates that 2 Desc. are sufficient for the model to fully capture the semantic meaning.\n\n-w/o GSA denotes removing the GSA mechanism and directly using H from Eq. (  3 ) as the final utterance representations. This led to a decrease of 1.06% in the average wF1, demonstrating the positive role of the GSA mechanism in enhancing utterance representations. Since the GSA mechanism benefits from aggregating highly relevant information while reducing the negative impact of distant utterances, we further compare it with using the self-attention mechanism (SA) alone. The results show that the performance drops by 1.53%, and it even performs 0.47% worse than when no mechanism was used (-w/o GSA). This demonstrates that directly using SA for utterance representation learning has a detrimental effect, with the negative impact stemming from distant noise.\n\n-w/o CRF denotes removing the CRF layer and the AVD algorithm, and during the inference phase, it directly uses h see i and h uns j for nearest neighbor search as specified in Eq. (  14 ). The results show a decrease of 6.93% in average wF1, which demonstrates that the AVD algorithm, by leverag-   ing the emotion transition dependencies learned by the CRF layer, plays a crucial role in enhancing the model's performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Hyperparameter Sensitivity",
      "text": "The variance σ in the GSA mechanism controls the attention range. To study the impact of σ on performance, we conducted a sensitivity analysis, as shown in Figure  3 . It can be observed that the best performance is achieved when σ is set to 0.5. As σ increases, the performance gradually decreases and converges. In fact, as σ grows, the Gaussian Self-Attention mechanism gradually degenerates into a standard self-attention mechanism.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Average Performance And Computation Cost With Different Language Models",
      "text": "To investigate the effect of using different pretrained language models and the corresponding computation costs, we conduct experiments and record the average performance and inference costs in Table  4 . Using roberta-base  (Liu et al., 2019)  improves the model's average performance by 0.86%. With the larger versions, Bert and Roberta improve the model's average performance by 2.32% and 1.81%, respectively. However, the average inference time per sample increases by 2.91 ms and 2.92 ms, respectively.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Fine-Grained Analysis",
      "text": "As shown in Figure  4 , we conduct an experiment to demonstrate the fine-grained performance of different methods. Comparing the performance of ProEmoTrans and GPT-4o, we can observe that ProEmoTrans performs better in most unseen emotions. However, as the emotion proportion decreases, ProEmoTrans shows a more noticeable decline in performance. We believe this is due to GPT-4o relying on prior knowledge, while ProE-moTrans depends on the quality of prototype-based representation learning, which makes it more sensitive to the distribution of categories.\n\nRemoving the LED (-w/o LED) causes a performance drop across all unseen emotions, to varying degrees, highlighting the LED's comprehensive contribution. Similarly, removing the CRF (-w/o CRF) also leads to a nearly overall performance decline, but in some cases, it improves performance. For example, in subplot (f), it leads to a 2.45% increase for surprise. This suggests that while the CRF layer optimizes global performance, it may not be ideal for certain local categories.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Visualization",
      "text": "To provide more interpretability, we visualize the embedding space of utterances and unseen emotions on E → I datasets using t-SNE (Van der  Maaten and Hinton, 2008) , as shown in Figure  5 . First, we find that positive emotions (excited and happy) are farther apart from negative emotions (frustrated and angry), while emotions of the same We also collect all the emotion prototype embeddings and compute their cosine similarities. The resulting heatmap is shown in Figure  6 . It can be observed that, first, the cosine similarity is higher between similar emotions (e.g., happy and joy). Second, there is a more pronounced difference in similarity between positive and negative emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "More Additional Experiments",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Analysis On Contrastive Similarity",
      "text": "The contrastive similarity  (Oord et al., 2018)  can effectively measure the difference between two embeddings. To validate its effectiveness, we conducted experiments comparing it with other similarity metrics. The results are shown in Table  5 . When using Euclidean distance, cosine similarity, and dot product, the model's performance decreased by 1.64%, 0.67%, and 1.53%, respectively, which proves the effectiveness of contrastive similarity.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Few-Shot Performance",
      "text": "Our model can also be used for few-shot prediction without any modifications. To investigate the performance of our model in the few-shot setting, we conducted experiments as shown in Table  6 .\n\nTo ensure a fair comparison with the baselines, we follow the 16-shot setting and use weighted macro-F1 as the evaluation metric. The applied baselines include: KEY  (Zhong et al., 2019)",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "Our LLM prompt templates rely on manual design, and their effectiveness has not been verified with more complex emotions. Developing automated prompt-tuning templates would be an interesting avenue for exploration. Additionally, our approach focuses solely on the text modality and does not incorporate multi-modal information, such as facial expressions, which could provide valuable additional information.\n\nA More Details of Experiments Settings The statistics of the unseen emotions under different source and target settings are shown in Table  7 . For example, if we chose I as source dataset and E as target dataset, the unseen emotions are powerful, peaceful, scared, joy, and mad.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.2 Baselines",
      "text": "The details of baselines are as follows:\n\n• DialogueGCN  (Ghosal et al., 2019)  uses a GCN to model the inter-utterance dependency.\n\n• DialogueCRN  (Hu et al., 2021)  is one of the best RNN-based ERC models. They design multiple rounds of reasoning modules to extract and integrate emotional cues.\n\n• DualGAT  (Zhang et al., 2023)  introduces a Dual Graph Attention Network to capture complex dependencies of discourse structure and speakeraware context.\n\n• SACL-LSTM  (Hu et al., 2023)  proposes a supervised adversarial contrastive learning method for learning class-spread structured representations.\n\n• SCCL  (Yang et al., 2023)  proposes a supervised cluster-level contrastive learning method to incorporate measurable emotion prototypes.\n\n• EACL  (Yu et al., 2024)  proposes an emotionanchored contrastive learning framework, which generates more distinguishable utterance representations for similar emotions.\n\n• CPTC  (Xu et al., 2023)  leverages sharable crosstask knowledge from the source task to improve few-shot performance.\n\nWe made the necessary modifications for each baseline to enable zero-shot prediction.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B More Additional Experiments B.1 Results With Parameterized Modules",
      "text": "In supervised settings, previous methods have designed various parameterized modules to help learn better utterance representations. In the zero-shot setting, to validate their effectiveness, we conduct comparative experiments by replacing the Gaussian Self-Attention module in our model with LSTM, GCN, and GAT. The experimental results are shown in Table  8 . It can be observed that the performance is quite weak, which proves that overfitting due to the parameter module severely hinders the generalization performance.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B.2 Utterance-Level Performance",
      "text": "We conducted a comparative experiment on zeroshot ERC at the utterance level, with results shown in Table  9 , where -w utterance-level refers to applying LLM baselines to prompt each individual utterance. Our experiments uncovered some intriguing findings: On the longer dialogue dataset (IEMOCAP, avg. length 52), utterance-level classification significantly outperformed the original conversation-level approach. We believe that excessively long conversations hinder LLM's emotional analysis capability by overwhelming context processing. On the other two datasets (avg. lengths 12 and 9), utterance-level performance was slightly lower than conversation-level. We attribute this to the loss of contextual information, which poses challenges for utterances with ambiguous emotional cues or those that are very brief. For example, the utterance \"That only took me an hour.\" was misclassified as joy at the utterance level, but correctly classified as sad at the conversation level when the broader topic (divorce) was considered. Crucially, our method consistently maintains an advantage across different conversation lengths, despite the observed variations in zero-shot LLM classification performance.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C Details Of Led Generated Descriptions",
      "text": "To eliminate biases introduced by the quality of generated descriptions, we regenerate new descriptions in each of the five random runs. The emotion descriptions generated using the LED module in one of the five runs are shown in Table  10 .",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) shows that the emotion categories in three",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), when",
      "page": 2
    },
    {
      "caption": "Figure 2: The architecture of our proposed EmoTrans.",
      "page": 3
    },
    {
      "caption": "Figure 3: Effects of σ.",
      "page": 7
    },
    {
      "caption": "Figure 3: It can be observed that the best",
      "page": 7
    },
    {
      "caption": "Figure 4: Fine-grained analysis of different methods, with the proportion of unseen emotions also presented.",
      "page": 8
    },
    {
      "caption": "Figure 4: , we conduct an experiment",
      "page": 8
    },
    {
      "caption": "Figure 5: First, we find that positive emotions (excited and",
      "page": 8
    },
    {
      "caption": "Figure 5: t-SNE visualization of utterance and emotion",
      "page": 8
    },
    {
      "caption": "Figure 6: Heatmap of emotion prototype similarities.",
      "page": 8
    },
    {
      "caption": "Figure 6: It can be",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Performance (wF1.) and computation cost",
      "data": [
        {
          "Models": "Proposed ProEmoTrans",
          "E → I M → I\nI → E M → E\nI → M E → M": "37.27\n32.36\n28.34\n20.73\n38.59\n35.64",
          "Average": "32.16"
        },
        {
          "Models": "- w/o LED\n- w 1 Desc.\n- w 3 Desc.",
          "E → I M → I\nI → E M → E\nI → M E → M": "27.68\n7.28\n24.06\n6.31\n22.59\n19.22\n30.46\n9.90\n25.06\n8.74\n24.68\n25.26\n37.56\n33.03\n28.39\n21.22\n37.89\n36.82",
          "Average": "17.86 (14.30↓)\n20.68 (11.48↓)\n32.49 (0.33↑)"
        },
        {
          "Models": "- w/o GSA\n- w SA",
          "E → I M → I\nI → E M → E\nI → M E → M": "36.89\n31.40\n27.00\n19.37\n37.68\n34.26\n36.27\n30.78\n26.47\n19.82\n37.01\n33.45",
          "Average": "31.10 (1.06↓)\n30.63 (1.53↓)"
        },
        {
          "Models": "- w/o CRF",
          "E → I M → I\nI → E M → E\nI → M E → M": "31.22\n19.20\n18.29\n16.59\n33.82\n32.27",
          "Average": "25.23 (6.93↓)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Proposed ProEmoTrans",
          "E → I M → I\nI → E M → E\nI → M E → M": "37.27\n32.36\n28.34\n20.73\n38.59\n35.64",
          "Average": "32.16"
        },
        {
          "Models": "DeepSeek-V3\n-w utterance-level",
          "E → I M → I\nI → E M → E\nI → M E → M": "25.69\n26.26\n24.10\n18.68\n35.15\n29.76\n30.06\n31.52\n23.27\n19.04\n34.86\n29.45",
          "Average": "26.61 (5.55↓)\n28.03 (4.13↓)"
        },
        {
          "Models": "GPT-4o\n-w utterance-level",
          "E → I M → I\nI → E M → E\nI → M E → M": "24.88\n26.10\n24.51\n18.25\n35.28\n29.35\n30.24\n31.25\n24.22\n18.48\n35.07\n29.51",
          "Average": "26.40 (5.76↓)\n28.13 (4.03↓)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "joy",
          "Description": "a feeling of extreme happiness or cheerfulness, es-\npecially related to the acquisition or expectation of\nsomething good.",
          "LED Generated Description": "1. When she received the good news, a bright smile\nspread across her face. 2. At the celebration, laughter\nfilled the room, creating a vibrant atmosphere."
        },
        {
          "Emotion": "happy",
          "Description": "the emotion of being happy; elation.",
          "LED Generated Description": "1. She smiled as she read the message, feeling content\nwith the little surprise.\n2. As the music played, a\nwarm sense of fulfillment washed over her, making\neverything else fade away."
        },
        {
          "Emotion": "surprise",
          "Description": "the feeling that something unexpected has happened.",
          "LED Generated Description": "1. The unexpected turn of events left him speechless,\neyes wide in disbelief. 2. She could hardly believe her\nears when she heard the astonishing news."
        },
        {
          "Emotion": "excited",
          "Description": "having great enthusiasm, passion, and energy.",
          "LED Generated Description": "1. Her heart raced as she opened the envelope con-\ntaining the results.\n2. He couldn’t sit still, eagerly\nanticipating the start of the event."
        },
        {
          "Emotion": "peaceful",
          "Description": "motionless and calm.",
          "LED Generated Description": "1. The gentle sound of the waves lapping against the\nshore filled her with calm. 2. Sitting under the shade\nof the old tree, he felt completely at ease."
        },
        {
          "Emotion": "sad",
          "Description": "emotionally negative and feeling sorrow.",
          "LED Generated Description": "1. He stared out\nthe window, his heart heavy with a\nlingering sense of loss. 2. As she walked through the\nempty hall, a wave of nostalgia washed over her."
        },
        {
          "Emotion": "anger",
          "Description": "displaying or feeling anger.",
          "LED Generated Description": "1. She clenched her fists and glared at him when she\nheard the unfair criticism. 2. His face turned red as he\nconfronted the person who had betrayed his trust."
        },
        {
          "Emotion": "mad",
          "Description": "feeling annoyed.",
          "LED Generated Description": "1. He slammed the door shut, frustration boiling be-\nneath the surface. 2. Her eyes flashed with irritation\nas she listened to the unfair remarks."
        },
        {
          "Emotion": "frustrated",
          "Description": "the feeling of annoyance at impossibility from resis-\ntance or inability to achieve something.",
          "LED Generated Description": "1. She let out a sigh and ran her fingers through her\nhair,\nfeeling exasperated with the situation.\n2. He\nstared at the puzzle pieces scattered on the table, un-\nable to find a solution."
        },
        {
          "Emotion": "scared",
          "Description": "feeling afraid and frightened.",
          "LED Generated Description": "1. A cold sweat broke out on his forehead as he heard\nfootsteps behind him in the dark.\n2.\nShe held her\nbreath, feeling a knot\ntighten in her stomach during\nthe thunderstorm."
        },
        {
          "Emotion": "fear",
          "Description": "a strong, unpleasant emotion or feeling caused by ac-\ntual or perceived danger or threat.",
          "LED Generated Description": "1. In the dark alley, a sudden noise made his heart race\nwith unease. 2. She felt a chill run down her spine as\nshadows flickered around her."
        },
        {
          "Emotion": "powerful",
          "Description": "having, or capable of exerting, power or influence.",
          "LED Generated Description": "1. Standing at\nthe edge of the cliff, she felt an over-\nwhelming sense of strength and determination. 2. The\nspeaker’s voice resonated through the hall, command-\ning everyone’s attention."
        },
        {
          "Emotion": "disgust",
          "Description": "to cause an intense dislike for something.",
          "LED Generated Description": "1. I couldn’t believe it when my teammate ignored my\nadvice during the game. 2. It drove me crazy when the\ninternet kept disconnecting while I was working."
        },
        {
          "Emotion": "neutral",
          "Description": "neither positive nor negative.",
          "LED Generated Description": "1. He sat quietly, showing no particular reaction to the\nevents around him. 2. The room was filled with a quiet\nstillness as everyone focused on their tasks."
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Sébastien Bubeck",
        "Varun Chadrasekaran",
        "Ronen Eldan",
        "Johannes Gehrke",
        "Eric Horvitz",
        "Ece Kamar",
        "Peter Lee",
        "Yin Tat Lee",
        "Yuanzhi Li",
        "Scott Lundberg"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "ZS-BERT: Towards zero-shot relation extraction with attribute representation learning",
      "authors": [
        "Chih-Yao Chen",
        "Cheng-Te Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.272"
    },
    {
      "citation_id": "4",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "5",
      "title": "Basic Emotions, chapter 3",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Basic Emotions, chapter 3",
      "doi": "10.1002/0470013494.ch3"
    },
    {
      "citation_id": "6",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "7",
      "title": "Di-alogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "8",
      "title": "The llama 3 herd of models",
      "authors": [
        "Aaron Grattafiori",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schelten",
        "Alex Vaughan"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "9",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.606"
    },
    {
      "citation_id": "10",
      "title": "Dia-logueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "11",
      "title": "HiGRU: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael Lyu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1037"
    },
    {
      "citation_id": "12",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "13",
      "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "authors": [
        "John Lafferty",
        "Andrew Mccallum",
        "Fernando Pereira"
      ],
      "year": "2001",
      "venue": "Icml"
    },
    {
      "citation_id": "14",
      "title": "Graph based network with contextualized representations of turns in dialogue",
      "authors": [
        "Bongseok Lee",
        "Yong Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.36"
    },
    {
      "citation_id": "15",
      "title": "DiaASQ: A benchmark of conversational aspect-based sentiment quadruple analysis",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Fei Li",
        "Yuhan Wu",
        "Jinsong Zhang",
        "Shengqiong Wu",
        "Jingye Li",
        "Yijiang Liu",
        "Lizi Liao",
        "Tat-Seng Chua",
        "Donghong Ji"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": "10.18653/v1/2023.findings-acl.849"
    },
    {
      "citation_id": "16",
      "title": "AlignRE: An encoding and semantic alignment approach for zero-shot relation extraction",
      "authors": [
        "Zehan Li",
        "Fu Zhang",
        "Jingwei Cheng"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics ACL 2024",
      "doi": "10.18653/v1/2024.findings-acl.174"
    },
    {
      "citation_id": "17",
      "title": "Deepseek-v3 technical report",
      "authors": [
        "Aixin Liu",
        "Bei Feng",
        "Bing Xue",
        "Bingxuan Wang",
        "Bochao Wu",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenyu Zhang",
        "Chong Ruan"
      ],
      "year": "2024",
      "venue": "Deepseek-v3 technical report",
      "arxiv": "arXiv:2412.19437"
    },
    {
      "citation_id": "18",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "19",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "20",
      "title": "Is discourse role important for emotion recognition in conversation?",
      "authors": [
        "Donovan Ong",
        "Jian Su",
        "Bin Chen",
        "Anh Luu",
        "Ashok Narendranath",
        "Yue Li",
        "Shuqi Sun",
        "Yingzhan Lin",
        "Haifeng Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "22",
      "title": "Control globally, understand locally: A global-to-local hierarchical graph network for emotional support conversation",
      "authors": [
        "Wei Peng",
        "Yue Hu",
        "Luxi Xing",
        "Yuqiang Xie",
        "Yajing Sun",
        "Yunpeng Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22",
      "doi": "10.24963/ijcai.2022/600"
    },
    {
      "citation_id": "23",
      "title": "Theories of emotion",
      "authors": [
        "Robert Plutchik",
        "Henry Kellerman"
      ],
      "year": "2013",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "24",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "25",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "26",
      "title": "2022a. Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2022.emnlp-main.347"
    },
    {
      "citation_id": "27",
      "title": "2022b. Emotionflow: Capture the dialogue level emotion transitions",
      "authors": [
        "Xiaohui Song",
        "Liangjun Zang",
        "Rong Zhang",
        "Songlin Hu",
        "Longtao Huang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore",
      "doi": "10.1109/ICASSP43922.2022.9746464"
    },
    {
      "citation_id": "28",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "Efficient cross-task prompt tuning for few-shot conversational emotion recognition",
      "authors": [
        "Yige Xu",
        "Zhiwei Zeng",
        "Zhiqi Shen"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": "10.18653/v1/2023.findings-emnlp.780"
    },
    {
      "citation_id": "31",
      "title": "Qwen2.5-1m technical report",
      "authors": [
        "An Yang",
        "Bowen Yu",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang",
        "Haoyan Huang",
        "Jiandong Jiang",
        "Jianhong Tu",
        "Jianwei Zhang",
        "Jingren Zhou"
      ],
      "year": "2025",
      "venue": "Qwen2.5-1m technical report",
      "arxiv": "arXiv:2501.15383"
    },
    {
      "citation_id": "32",
      "title": "Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "Kailai Yang",
        "Tianlin Zhang",
        "Hassan Alhuzali",
        "Sophia Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2023.3243463"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition in conversation based on a dynamic complementary graph convolutional network",
      "authors": [
        "Zhenyu Yang",
        "Xiaoyang Li",
        "Yuhu Cheng",
        "Tong Zhang",
        "Xuesong Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3360979"
    },
    {
      "citation_id": "34",
      "title": "Emotion-anchored contrastive learning framework for emotion recognition in conversation",
      "authors": [
        "Fangxu Yu",
        "Junjie Guo",
        "Zhen Wu",
        "Xinyu Dai"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2024",
      "doi": "10.18653/v1/2024.findings-naacl.282"
    },
    {
      "citation_id": "35",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "36",
      "title": "DualGATs: Dual graph attention networks for emotion recognition in conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.408"
    },
    {
      "citation_id": "37",
      "title": "RE-matching: A fine-grained semantic matching method for zero-shot relation extraction",
      "authors": [
        "Jun Zhao",
        "Wenyu Zhan",
        "Xin Zhao",
        "Qi Zhang",
        "Tao Gui",
        "Zhongyu Wei",
        "Junzhe Wang",
        "Minlong Peng",
        "Mingming Sun"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of ACL",
      "doi": "10.18653/v1/2023.acl-long.369"
    },
    {
      "citation_id": "38",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on EMNLP and the 9th IJC-NLP (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1016"
    }
  ]
}