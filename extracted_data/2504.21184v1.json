{
  "paper_id": "2504.21184v1",
  "title": "Affecteval: A Modular And Customizable Affective Computing Framework",
  "published": "2025-04-29T21:40:49Z",
  "authors": [
    "Emily Zhou",
    "Khushboo Khatri",
    "Yixue Zhao",
    "Bhaskar Krishnamachari"
  ],
  "keywords": [
    "Affective Computing",
    "Software Framework",
    "AI/ML"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The field of affective computing focuses on recognizing, interpreting, and responding to human emotions, and has broad applications across education, child development, and human health and wellness. However, developing affective computing pipelines remains labor-intensive due to the lack of software frameworks that support multimodal, multi-domain emotion recognition applications. This often results in redundant effort when building pipelines for different applications. While recent frameworks attempt to address these challenges, they remain limited in reducing manual effort and ensuring cross-domain generalizability. We introduce AffectEval, a modular and customizable framework to facilitate the development of affective computing pipelines while reducing the manual effort and duplicate work involved in developing such pipelines. We validate AffectEval by replicating prior affective computing experiments, and we demonstrate that our framework reduces programming effort by up to 90%, as measured by the reduction in raw lines of code.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing is an interdisciplinary field of research for recognizing, interpreting, and responding to human affect with computational methodologies. While stimuli may vary, many affective computing pipelines share the same setup. Oliveira et al.  [35]  outlines the key steps of signal acquisition, signal preprocessing, feature extraction and prediction as the key steps of the affective computing pipeline, which is illustrated in Figure  1 . As a result, several software libraries and frameworks have been developed to support the creation of affective computing pipelines  [11, 31, 34, 35, 42] . However, due to the broad applications of affective computing-which encompasses mental health (MH) disorders  [4, 19, 53] , personalized education  [7] , and aging-related diseases such as Alzheimer's  [44] -these software frameworks are not comprehensive and have additional limitations that create high manual costs and prevent them from being reusable. A comprehensive framework for affective computing should provide all the components necessary to implement a wide range of use cases across domains, signals, and methodologies.\n\nTo guide the development of such a framework, we analyzed existing affective computing tools. Programming languages such as Python  [46]  offer libraries such as scikit-learn  [37] , pyHRV  [18] , and NeuroKit2  [31]  that support parts of the affective computing pipeline. This includes signal preprocessing, feature extraction, and classification. Recent work has also introduced frameworks aimed at being more flexible  [35]  and user-friendly  [34]  to facilitate the development of affective computing pipelines. However, we identify the following main limitations:\n\n(1) Researchers typically need to create pipelines from scratch using various existing libraries and frameworks. This often requires extensive software experience, is time-consuming, and results in duplicated work across applications. (2) Generally, affective computing pipelines are signal-and domain-specific; i.e., they only support a limited set of signals and applications. As a result, it is difficult to reuse code across pipelines or make them generalizable across datasets. (3) There is no standardized interface to compare the performance of individual components across different affective computing techniques.\n\nTo the best of our knowledge, there is no comprehensive framework for the development of end-to-end affective computing systems. To address the aforementioned limitations, such a framework should be modular, customizable, and flexible to support multimodal signals, i.e., from multiple types of sensors and data streams, and multi-domain applications, such as general affect, stress, and depression detection. A framework consisting of individual modules, each responsible for a different component of the affective computing pipeline, provides an easily reusable structure. Users should also be able to customize modules as needed to create new pipelines or adapt existing ones for new applications. They can also compare the performance of different components in a fair playground by modifying their behaviors, such as using different signal preprocessing methods, feature extraction techniques, and classification models.\n\nWe present AffectEval: a modular and customizable affective computing framework that generalizes to more signals and applications than existing frameworks. Our framework is designed under the object-oriented paradigm such that each component is responsible for a separate part of the affective computing pipeline. We program each component with default functionalities that can also be overwritten with custom methods, allowing researchers to easily set up basic end-to-end pipelines or create specialized applications as needed. We primarily focus on the use of time-series physiological signals for affective computing due to their extensive usage in emotion recognition and connection to mental states as suggested by the Somatic Marker Hypothesis  [13] , but AffectEval can be easily extended to support other types of signals. We also establish a standard format for affective computing datasets in order to mitigate the amount of manual setup work required, which we detail in Section 3.1.\n\nAdditionally, we demonstrate the functionality of AffectEval by reproducing the experiments of Schmidt et al.  [40]  and Zhou et al.  [51]  using the Anxiety Phases Dataset (APD)  [41]  and Wearable Stress and Affect Detection dataset (WESAD)  [40] . These datasets were chosen because they are well-established multimodal datasets for stress and affect detection. Both authors conducted different types of affect recognition tasks: Schmidt et al. performed binary and three-class affect classification on WESAD, while Zhou et al. performed binary stress detection on both APD and WESAD. These datasets and experiments are therefore suitable for demonstrating the ease-of-use and reusability of AffectEval, as well as its ability to create signal-and domain-agnostic pipelines. Our AffectEvalbased pipeline's model performance largely matches or exceeds the performance achieved in previous work  [40, 51] , while using up to 90% fewer lines of code. We reuse the pipeline structure across our experiments and incorporate different preprocessing techniques, feature extraction methods, and models using AffectEval's modular design.\n\nIn summary, this paper makes the following contributions:\n\n• We conduct an extensive literature review on existing affective frameworks to motivate our design of AffectEval. Section 2 provides an overview of existing methodologies, software libraries, and pipelines for affective computing. Section 3 describes the architecture of AffectEval and outlines the steps necessary to implement an affective computing pipeline using our framework. Section 4 describes our implementation of AffectEval to replicate Schmidt et al. and Zhou et al's experiments. Section 5 discusses our findings. We conclude the paper in Section 6.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background And Related Work",
      "text": "In this section, we review the potential of affective computing in human-centered applications with a focus on mental healthcare, followed by an overview of existing software libraries and pipelines for affective computing. We identify their strengths, disadvantages, and common components and use them as guidelines for the development of AffectEval.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Affective Computing",
      "text": "Affective computing has shown significant promise in mental health applications, particularly in the diagnosis, tracking, and treatment of various disorders. Recent studies have explored its use in a wide range of mental healthcare applications, especially depression  [53]  and anxiety detection  [22] . Affective computing can also benefit the treatment and care of late-life mood and cognitive disorders, such as depression and Alzheimer's disease, via objective biomarkers such as eye movement and vocal features  [44] .\n\nThe types of signals used in affect detection for health applications vary, each with its own advantages and limitations. Facial analysis and audio processing are popular due to their ease of collection and non-invasive nature  [29] . They can be particularly useful in the detection of emotions such as stress or anxiety in clinical settings, but may be subject to cultural biases and environmental factors. Physiological signals, such as EEG, ECG, and skin conductance, offer a more objective measure of emotional states due to their direct connection to the autonomic nervous system  [30] . Recent works have used physiological signals to perform stress  [17, 27, 40, 52] , and depression  [15, 53]  detection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Investigation Of Existing Affective Computing Pipelines",
      "text": "Based on previous work, an affect recognition system can be broken down into the following main components: signal acquisition, signal pre-processing, feature extraction, feature selection, and classification. Table  1  provides an overview of related work and the components incorporated in each one. Next, we illustrate the tasks carried out by each component in the context of Schmidt et al.'s experiments  [40] .\n\nSignal acquisition is the process of reading in signals to the affect recognition system. This can be from a local database or in real time from more data streams  [3, 48] . In recent years, many benchmark datasets of unimodal and multimodal affect have been released  [49]  to support affective computing research. The methods of emotion elicitation and types of signals collected vary widely. For example, the Wearable Stress and Affect Detection (WESAD) dataset  [40]  contains blood volume pulse (BVP), electrocardiogram (ECG), electrodermal (EDA), electromyography (EMG), respiration (RESP), body temperature (TEMP), and three-axis acceleration (ACC) collected from neutral, stress, and amusement states. DEAP  [25]  contains electroencephalogram (EEG), peripheral physiological signals, and audiovisual recordings from individuals watching music videos. There have also been efforts to develop real-time emotion recognition systems;  [12]  introduced a method for real-time negative affect detection from an ECG signal stream, while  [6]  developed an interactive dialogue system to recognize users' emotion in real-time.\n\nSignal preprocessing is necessary to remove noise and isolate frequency bands for feature extraction.  [40]",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Affective Computing Experiments",
      "text": "Zhu et al.  [52]  ✓  1 : Examples of affective computing experiments and frameworks containing the components of affective computing pipelines described in Oliveira et al.  [35] . AffectEval is the first open-source, modular affective computing framework containing all pipeline components. Figure  1 : Key components of an affective computing pipeline, identified in  [35] , and examples of actions performed by each.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Other Common Preprocessing Techniques Are Removing Baseline Wander And Powerline Noise From Ecg Signals.",
      "text": "Feature extraction is performed in classical feature-based approaches to affect recognition  [7]  using non-DL models, such as Support Vector Machine (SVM)  [47]  and Random Forest  [9] . For examples,  [40]  performed manual feature extraction, obtaining statistical features (mean, median, standard deviation) and high-level physiological features. From ECG signals, heart rate, heart rate variability (HRV), and frequency-domain features were further extracted, and EDA signals were decomposed into their tonic and phasic components. Such high-level features have been found to be indicative of certain emotional states, demonstrating the necessity of performing feature extraction. For example, decreased HRV is associated with elevated anxiety  [26] , and changes in the SCL may reflect changes in arousal  [8] .\n\nFeature selection is often necessary to reduce the dimensionality of the input features  [7] . While this step was not included in  [40] , feature selection and dimensionality reduction can be performed to prevent over-fitting  [52] . Some commonly used feature selection methods include Principal Component Analysis (PCA)  [23] , which reduces a feature set based on its uncorrelated principal components, and Sequential Forward Selection (SFS)  [16] , which is an iterative approach that adds features that provide the most improvement in classifier performance.\n\nClassification is the final step in the pipeline, consisting of training and evaluating the affect recognition model. Both traditional machine learning (ML) and deep learning (DL) methods can be used to perform binary and multi-class classification of affect  [7, 39] . In particular,  [40]  used the Decision Tree (DT), Random Forest (RF), AdaBoost (AB), Linear Discriminant Analysis (LDA), and K-nearest neighbors (KNN) ML algorithms to perform binary stress classification and three-class classification of non-stress, stress, and amusement. Others have performed valence and arousal classification  [25]  based on Russell's circumplex model of affect  [38] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Existing Software Libraries And Pipelines",
      "text": "Existing software libraries and frameworks for affective computing generally support a few, but not all components of the pipeline. For example, Matlab  [21]  offers many packages that provide functions for signal processing, analysis, and visualization. PyTorch  [36]  and Tensorflow  [2]  are open-source frameworks to build machine learning models, but can require complex scripts that are time-consuming to set up. Other libraries focus on pre-processing methods for a small subset of signals, such as biosppy  [11]  and We extend the components identified by  [35]  with Feature Selection and Label Generation, which are useful and often necessary steps for affect recognition across domains. Bolded points indicate preimplemented behaviors.\n\nneurokit  [31]  for physiological signals, including BVP, ECG, EDA, EEG, and EMG, and heartpy  [45]  for ECG and PPG signals.\n\nWhile there exist more comprehensive tools that support more components of the pipeline, they are typically specific to one or very few domains of application. The AffectToolbox  [34]  aims to facilitate the development of affective computing applications for research via a graphical user interface (GUI) framework, removing the need for extensive programming knowledge. It offers signal processing and deep learning methods for video and audio signals, and provides real-time analysis. While it achieves the goal of being accessible for a wider range of users across disciplines, it does not support the use of multimodal physiological signals.\n\nThe metaFERA  [35]  framework provides building blocks to create domain-specific software frameworks for emotion recognition. It consists of independent and reusable modules, providing the highlevel structure of an affective computing pipeline. However, it does not provide default functionalities or support deep learning models, requiring the user to implement their own methods. It is also written in Java, while the majority of libraries used in affective computing are available in Python. This could pose a problem for researchers in this domain, since they are less likely to be familiar with Java.\n\nIn general, existing software tools for affective computing only support a small portion of the pipeline or subset of signals and have limited use cases. As a result, there is a need for an affective computing framework that encompasses the full pipeline from signal acquisition to model evaluation, while being low-effort to implement and easily customizable.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Affecteval'S Design",
      "text": "This section describes the design of AffectEval and its features that address the aforementioned shortcomings of existing affective computing libraries and frameworks. AffectEval is a meta-framework that enables the straightforward construction of end-to-end affective computing pipelines via building blocks defined using individual classes. AffectEval has 3 key characteristics as discussed below, directly addressing existing works' limitations.\n\nModularity: AffectEval adopts a modular architecture, designed based on the common components identified in previous work. It consists of six classes, which we refer to as components: (1) Signal Acquisition, (2) Signal Preprocessor, (3) Feature Extractor, (4) Feature Selector, (5) Label Generator, and (  6 ) Classification. We introduce Feature Selection and Label Generation as new components to the original affective computing pipeline outlined by  [35] . We provide pre-implemented behaviors for each component, which we describe in bolded points in Figure  2 . Components are independent and can be easily modified, and in the case of the Feature Extractor and Feature Selector, omitted as needed. Users instantiate the necessary components, then organize them in an ordered list such that each consecutive component's predefined input and output types are compatible. The required order of components is illustrated in Figure  3 . This list is then passed to the Pipeline, which executes each component's functions in order.\n\nCustomizability: AffectEval allows users to modify and augment its functionalities as needed at both the component-and method-level. Each component has a default implementation that extends from an abstract base class (ABC), but users can choose to implement their own versions of each class as needed. Each predefined component also provides default methods that can be overridden. Listing 1 presents a high-level view of a pipeline created using AffectEval, adapted from our own implementation to replicate Schmidt et al. and Zhou et al. as described in Section 4. Within each component instantiation, parameters such as signal_types, feature_extraction_methods, and models can be specified by the user.\n\nSignal-and Domain Agnostic: AffectEval can be easily extended to support a wide range of signal types across different applications, e.g., multi-class emotion recognition, stress detection, and depression detection. Generally, existing work focuses on a single domain, but the common components across domains can be implemented and reused with AffectEval. For example, Listing 1 is an example pipeline that can be modified to perform various affect recognition tasks on different datasets.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Prerequisites",
      "text": "The structure of the dataset folder and files must follow a standard format  1  in order for the framework to process input signals for affect recognition tasks. The expected directory structure for data files is as follows: {source_folder}/ {subject_ID_1}/ {subject_ID_1}_{phase}_{modality}.{csv} ... {subject_ID_2}/ ... The subject_ID value must be a unique identifier for each subject in the dataset. The phase describes the different phases of the emotion elicitation experiment; typically, affective computing datasets include rest phases to obtain baseline measurements and phases of exposure to various stimuli and stressors. Finally, modality represents the signal type contained in that file, i.e., ECG, EDA, or RESP, and these signal types must be supported by Af-fectEval (currently, ECG, EDA, EMG, RESP, and TEMP signals). The Comma-Separated-Values (CSV) file format, which contains data in a tabular format, is required. The CSV file must contain 2 headers: (1) timestamp, and (2) modality (e.g., ECG, EDA, or EMG). The timestamp column must contain the timestamps for data collection, and the modality column must contain the corresponding measurements, e.g., ECG measurements in millivolts, or EDA measurements in microsiemens.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Signal Acquisition",
      "text": "The Signal Acquisition component reads signals from CSV files from a dataset folder and formats the data into pandas DataFrame objects  [33] . This component returns a dictionary of lists of pandas DataFrames, where each key in the dictionary corresponds to a unique subject, and each list contains all the signals for that subject as DataFrames. The set of supported signals can be easily expanded by defining new signal types in the included metadata file.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Signal Preprocessor",
      "text": "The Signal Preprocessor performs signal denoising and additional preprocessing steps, such as resampling and interpolation. Default preprocessing methods are provided using the biosppy, heartpy, neurokit, and scipy Python libraries, but users may pass in custom methods via the class parameter preprocessing_methods. Specifically, we provide methods for denoising ECG, EDA, EMG, and RESP signals. These include removing baseline wander and powerline noise from ECG and EDA signals and applying filters to remove unnecessary frequencies from EMG and RESP signals. Each DataFrame is processed separately. This component maintains the same data structure as the Signal Acquisition component: a dictionary of lists of DataFrames.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Extractor",
      "text": "The Feature Extractor extracts high-level features from input signals using either default methods provided by the AffectEval framework or user-defined methods passed to the feature_extraction_methods class parameter. We implemented methods to extract features from ECG, EDA, EMG, RESP, and TEMP signals; these include heart rate and heart rate variability metrics from ECG signals, tonic and phasic components from EDA signals, and statistical features. By default, features are averaged across the feature time series for each phase, but users may override this functionality via the calculate_average class parameter. Finally, feature fusion is performed to combine features extracted from different signals. The set of extracted features is concatenated to form a new DataFrame with each feature in a separate column. The output of this component is a dictionary in which keys correspond to subjects and values are each subject's corresponding feature set, organized in a DataFrame.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Selector",
      "text": "The Feature Selector is an optional component that can be inserted after the Feature Extraction component. The default feature selector provided is the SequentialFeatureSelector from scikit-learn, but users may use custom feature selection methods that are compatible with the scikit-learn feature selection module, e.g., users must define fit() and get_feature_names_out() methods. This component automatically identifies categorical features and performs one-hot encoding is automatically performed for categorical features to ensure that these features are compatible with ML models, which require numerical input.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Label Generator",
      "text": "The Label Generator is an additional component we introduced aimed to improve the ease of implementing an affect recognition system. It creates labels based on the features passed from the Feature Extraction or Feature Selection component. AffectEval provides default functions to generate labels based on subject IDs and phase names obtained from the dataset folder. However, to perform affect recognition from annotations or other labels, users are required to implement their own label generator function and pass it to the Label Generator via the label_generation_method class parameter. This function must generate a corresponding label for each feature vector. For example, WESAD provides self-report files for each subject, consisting of one self-report per questionnaire, per experimental phase. In our experiments, we implemented a function to extract specific questionnaire responses and generate binary affect labels, which we detail in Section 4.2.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Classification",
      "text": "The Classification component enables the training and testing of affective computing models. Like the Feature Selection component, this component supports any custom methods compatible with scikit-learn: classifiers must have fit() and predict() methods defined, and cross-validation methods must be compatible with the scikit-learn cross_val_score() method. The default classifier provided is scikit-learn's Support Vector Machine (SVM)  [47] . Users may change the model, cross-validation method, and execution mode (training, testing, or cross-validation) via class parameters. The output of this component is the final output of the pipeline, returning a list of fitted models, ground-truth labels, and model predictions for training and cross-validation modes.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "System Evaluation",
      "text": "To demonstrate the validity and reusability of AffectEval, we build pipelines to replicate various affect classification tasks originally conducted by Schmidt et al.  [40]  and Zhou et al.  [51]  on the following datasets: the Wearable Stress and Affect Dataset (WESAD)  [40]  and the Anxiety Phases Dataset (APD)  [41] . These datasets were chosen because they are often used in affective computing studies, providing many points of comparison for our framework performance. We reproduce the results of Schmidt et al.  [40]  and Zhou et al.  [51]  using 90% and 89% fewer lines of code, respectively. We also demonstrate AffectEval's modularity and customizability by reusing the same pipeline structure across a wide variety of signals, features, and machine learning models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset Overview",
      "text": "Anxiety Phases Dataset (APD). APD consists of electrocardiogram (ECG), electrodermal activity (EDA), and accelerometer (ACC) recordings from 52 subjects across different lab-controlled emotion elicitation phases. These include rest phases (to obtain baseline measurements), anticipation of an upcoming stressor, exposure to the stressor, recovery post-stressor, and a spoken reflection. The two stressors used in the study were a public speaking task in which subjects were instructed to order three topics by difficulty level, then prepare a 3-minute speech on the most difficult topic, and a bug-box task where subjects were instructed to release a fake bug from a small box (without knowing the bug was fake). Subjects reported their anxiety levels using the SUDS  [1]  and LSAS questionnaires  [28]  for each phase of the study. In our reproduction, we found that some subjects did not complete either the speech exposure task or the bug-box exposure task. These subjects were excluded from our experiments.\n\nWearable Stress and Affect Dataset (WESAD). WESAD contains ECG, EDA, ACC, EMG, respiration, and temperature signals from 15 subjects, collected during rest, amusement, stress, and two meditation phases. These signals were collected from both chest-and wrist-based devices, the RespiBAN Professional and Empatica E4, respectively. The emotion elicitation methods used were 11 humorous video clips and the Trier Social Stress Test (TSST). Subjects also reported their affect levels after each phase using the PANAS  [50] , STAI  [32] , SAM  [10] , and SSSQ  [20]  questionnaires.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Affecteval Implementation",
      "text": "We create affective computing pipelines using AffectEval modules to replicate previous work on binary stress detection on APD and WESAD, which is publicly available in the AffectEval GitHub repository as Jupyter Notebook files 2  . In the following sections, we describe the methodologies used and how we implement them with AffectEval. Figure  4  describes the pipeline components we implemented to replicate these experiments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Schmidt Et Al. 2018",
      "text": "Replication. Schmidt et al.  [40]  performed three-class phase classification (baseline vs. stress vs. amusement) and binary stress classification by combining the amusement and baseline conditions to form the non-stress class. A total of 16 modality combinations were evaluated across all chest-and wrist-sensor modalities. In this paper, we replicate their experiments on threeclass and binary classification tasks using all chest-based physiological modalities -ECG, EDA, EMG, respiration (RESP), and temperature (TEMP) -since this set of modalities achieved the highest accuracy in both tasks. Next, we briefly discuss the preprocessing and feature extraction methods used. Physiological signals were segmented using a 60-second window with a 0.25-second overlap. From the raw ECG signals, the following features were extracted: heart rate (HR), mean HR, standard deviation of HR, heart rate variability (HRV), energy in the ultra low, low, high, and ultra-high frequency bands. EDA signals were first filtered with a 5 Hz low-pass filter before statistical features were extracted. Next, signals were decomposed into the tonic and phasic components, also known as the skin conductance level (SCL) and skin conductance response (SCR), respectively. From these components, 9 additional features were extracted. Two sets of features were extracted from EMG signals. First, a high-pass filter was applied to remove the DC component before statistical and frequency-domain features were computed from 5-second windows. Spectral energy was also computed for frequency bands from 0 to 350 Hz. For the second set of features, the raw EMG signal was first filtered with a 50 Hz low-pass filter and segmented in 60-second windows. From each window, peak values and statistical features were calculated. RESP signals were filtered using a band-pass filter with cutoff frequencies of 0.1 and 0.35 Hz. Maxima and minima values and inhalation/exhalation features were extracted. From the raw TEMP signals, statistical features and slope were calculated. The full list of features can be found in Schmidt et al.  [40] .\n\nThe extracted features were concatenated and used as input for classification. The machine learning algorithms used were Decision Tree (DT), Random Forest (RF), AdaBoost (AB), Linear Discriminant Analysis (LDA), and K-nearest neighbors (KNN). Models were evaluated using the leave-one-subject-out (LOSO) cross-validation method, which we implement as a custom cross-validation method using AffectEval's Classification component. To evaluate model performance, accuracy and micro F1-score were calculated.\n\nZhou et al. 2023 Replication. Zhou et al.  [51]  performed binary stress classification on APD, WESAD, and the Continuously Annotated Signals of Emotion (CASE) dataset  [43]  using ECG and EDA signals. Within-and cross-corpus experiments were conducted to evaluate the generalizability of physiological features across stress and high-arousal states. In this paper, we replicate the subset of within-corpus experiments on APD and WESAD.\n\nECG and EDA signals were first denoised using biosppy and neurokit methods. Segmentation was performed using 60-second sliding windows with a 30-second overlap.\n\nFrom ECG signals, the following features were extracted: statistical features (mean, median, standard deviation, and variance), HR, root mean square of successive differences between RR intervals (RMSSD), standard deviation of the inter-beat-interval (SDNN), power in the high (HF) and low frequency (LF) bands, and the ratio of LF to HF. From EDA signals, statistical features, mean SCL, and SCR rate was calculated.\n\nStress labels were obtained from subject self-reports. For APD, the SUDS questionnaire responses were used to generate binary labels for stress. The version of SUDS used in APD ranges from 0 to 100, and the median, 50, was used as a fixed value to binarize the subject self-reports. SUDS scores 50 and above were labeled 1, while scores below 50 were labeled 0. For WESAD, the 6-item STAI questionnaire  [32]  responses were used to generate labels. A dynamic threshold was calculated for each subject by taking the average STAI score across all phases. STAI scores greater than or Figure  4 : The workflow of AffectEval-based implementation to replicate 3-class affect recognition and binary stress detection on WESAD  [40]  and binary stress detection on APD and WESAD  [51] .\n\nequal to this threshold were labeled 1, while scores less than the threshold were labeled 0.\n\nThe classifiers used were Support Vector Machine, LightGBM, Random Forest, XGBoost, and an ensemble of the previous models. A 5-fold cross-validation method was used, and accuracy and AUC score were reported as model evaluation metrics.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Validating Affecteval",
      "text": "We validated AffectEval by replicating  [40, 51]  to the best of our ability, using the same preprocessing methods, physiological features, labels, and classification models outlined by the authors. To migrate existing work to AffectEval, we identified the affective computing components each work used in their experiments: Signal Acquisition, Signal Preprocessing, Feature Extraction, and Classification. Next, we identified the specific preprocessing and feature extraction methods used for each type of signal and implemented them in our AffectEval-based pipeline. In our replication of  [51] , we implemented methods to extract 14 features from ECG and EDA signals; for  [40] , we implemented methods to extract a total of 62 features from WESAD's set of physiological signals.\n\nOur pipelines achieved the same or better accuracies, AUC scores, and F1-scores across all experiments, which are listed in Table  2 . By successfully reproducing previous work  [40, 51] , we show that Af-fectEval is effective and comprehensive, providing components and functionalities to perform a wide variety of multimodal affective computing tasks. In addition, our AffectEval implementation facilitates future discoveries using APD and WESAD under a fair playground by removing all setup work required.\n\nWhile our main goal was to demonstrate the functionality and ease-of-use of AffectEval, we were able to outperform previous work in most experiments, as shown by the bolded values. Discrepancies in model performance can most likely be attributed to preprocessing methods selected from different software libraries and differences in random seeding and model parameters that were not specified in previous work.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Reduction In Effort Using Affecteval",
      "text": "We quantify the reduction in manual effort achieved by using Af-fectEval to build our affect recognition pipelines, which we measure through the number of raw lines of code. Since the original code is not publicly available, we estimate the reduction in the number of lines of code we implemented to perform signal preprocessing, feature extraction, and classification. We focus on these components because the remaining tasks of acquiring and formatting datasets and generating labels are necessary whether or not AffectEval is used. Furthermore, the parameters and custom functions we used to perform signal preprocessing, feature extraction, and classification for these experiments are included as pre-implemented options in AffectEval, mitigating the need for users to implement them from scratch. We compare the total lines of code written to the number of lines of code needed to instantiate pipeline components, excluding the implementation of their behaviors. We estimate that AffectEval reduced the programming effort required to reproduce the experiments in  [40]  by 90% and in  [51]  by 89%, illustrated in Figure  5 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "Our successful reproduction of previous work highlights the contributions of AffectEval as a framework for creating affective computing pipelines. We showed that AffectEval supports multimodal, multi-domain applications and quantify the reduction in manual   work that can be achieved by using AffectEval. This section compares AffectEval to other existing affective computing frameworks and illustrates the role of modularity and customizability in making it a more comprehensive and flexible framework.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Framework Comparison",
      "text": "Existing affective computing frameworks, metaFERA  [35]  and Af-fectToolbox  [34] , were not evaluated by replicating previous work, or across a variety of affective computing tasks. As a result, there is no point of comparison for their ease of use, or demonstration of multimodal and multi-domain capabilities.\n\nTo validate metaFERA, authors created a framework for EDAbased binary affect detection of high vs. low arousal. However, it is unclear how much less manual work the metaFERA-based pipeline requires, compared to implementing such an EDA-based framework from scratch. Additionally, metaFERA does not provide pre-implemented behavior, requiring users to define component behaviors. Therefore, it is likely that metaFERA has a higher manual overhead than AffectEval. While metaFERA is modular and customizable, its effectiveness in reducing manual work and creating multi-domain applications is unclear.\n\nThe main goal of AffectToolbox is to foster a more collaborative environment in affective computing by removing the need for any programming knowledge. It provides a graphical user interface (GUI) to build affective computing pipelines for pleasure, arousal, and dominance classification from audiovisual data streams. While AffectToolbox effectively reduces the manual work involved in creating such pipelines, it is less comprehensive than AffectEval due to its limited support for multimodal signals and affect recognition capabilities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Modularity And Customizability",
      "text": "In comparison, the modularity of AffectEval allows for the reuse of pipeline structure across multimodal, multi-domain affective computing applications, while the individual components can be customized to perform application-specific tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "Through the development and validation of AffectEval, we identify a few remaining limitations and suggest directions for future improvements.\n\nAffectEval primarily supports the use of time-series physiological signals and lacks pre-defined functionalities for image, text, and audio data. Due to their increasing popularity and high performance in affect detection, we prioritized the support of time-series physiological signals. However, facial expression recognition, body gestures, audio, and textual data are non-invasive data streams that are also commonly used in affective computing  [49] . As mentioned in Section 3.2, AffectEval's capabilities can be easily expanded to support other types of signals.\n\nAffectEval does not support real-time emotion recognition or distributed computing. This limits the applicability of AffectEvalbased pipelines in online emotion recognition tasks. Additionally, components such as signal preprocessing, feature extraction, and model training cannot be parallelized, limiting the computing and time efficiency of AffectEval-based pipelines.\n\nThese limitations can be addressed by extending AffectEval's capabilities and list of pre-implemented behaviors. Specifically, we plan to add additional preprocessing methods to support audiovisual and textual data. In addition, a cloud-based or client-server wrapper can be created, enabling AffectEval to support distributed and real-time processing capabilities. Finally, AffectEval can be enhanced with a GUI, similar to  [34] , to further improve its usability and inclusivity by removing the need for programming knowledge.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presented AffectEval, a comprehensive framework to support the development of affective computing pipelines. It addresses key shortcomings of software libraries and tools currently used in affective computing applications, reducing the effort required to set up end-to-end pipelines. AffectEval was validated on previous work, exemplifying the reduction in manual work required while achieving the same or higher affect recognition performance. AffectEval is publicly released, with the goal of facilitating future affective computing research and creating an open-source repository for the community to contribute to.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Key components of an affective computing pipeline,",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of AffectEval components. We extend the components identified by [35] with Feature Selection and Label",
      "page": 4
    },
    {
      "caption": "Figure 2: Components are independent",
      "page": 4
    },
    {
      "caption": "Figure 3: This list is then passed to the Pipeline, which executes",
      "page": 4
    },
    {
      "caption": "Figure 3: Example instantiation of AffectEval. Solid outlines",
      "page": 5
    },
    {
      "caption": "Figure 3: provides an example instantiation of AffectEval and",
      "page": 5
    },
    {
      "caption": "Figure 4: describes the pipeline components we",
      "page": 7
    },
    {
      "caption": "Figure 4: The workflow of AffectEval-based implementation to replicate 3-class affect recognition and binary stress detection",
      "page": 8
    },
    {
      "caption": "Figure 5: Comparison of manual effort required for pipeline",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: provides an overview of related work and the",
      "page": 2
    },
    {
      "caption": "Table 1: Examples of affective computing experiments and frameworks containing the components of affective computing",
      "page": 3
    },
    {
      "caption": "Table 2: By successfully reproducing previous work [40, 51], we show that Af-",
      "page": 8
    },
    {
      "caption": "Table 2: A comparison of AffectEval’s performance to Schmidt et al. [40] and Zhou et al.’s [51] implementations. Abbreviations:",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "146 SUDS: The Subjective Units of Distress Scale",
      "year": "2014",
      "venue": "Concurrent Treatment of PTSD and Substance Use Disorders Using Prolonged Exposure (COPE): Patient Workbook",
      "doi": "10.1093/med:psych/9780199334513.005.0013"
    },
    {
      "citation_id": "2",
      "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems",
      "authors": [
        "Martín Abadi",
        "Ashish Agarwal",
        "Paul Barham",
        "Eugene Brevdo",
        "Zhifeng Chen",
        "Craig Citro",
        "Greg Corrado",
        "Andy Davis",
        "Jeffrey Dean",
        "Matthieu Devin",
        "Sanjay Ghemawat",
        "Ian Goodfellow",
        "Andrew Harp",
        "Geoffrey Irving",
        "Michael Isard",
        "Yangqing Jia",
        "Rafal Jozefowicz",
        "Lukasz Kaiser",
        "Manjunath Kudlur",
        "Josh Levenberg",
        "Dandelion Mané",
        "Rajat Monga",
        "Sherry Moore",
        "Derek Murray",
        "Chris Olah",
        "Mike Schuster",
        "Jonathon Shlens",
        "Benoit Steiner",
        "Ilya Sutskever",
        "Kunal Talwar",
        "Paul Tucker",
        "Vincent Vanhoucke",
        "Vijay Vasudevan",
        "Fernanda Viégas",
        "Oriol Vinyals",
        "Pete Warden",
        "Martin Wattenberg",
        "Martin Wicke",
        "Yuan Yu",
        "Xiaoqiang Zheng"
      ],
      "year": "2015",
      "venue": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems"
    },
    {
      "citation_id": "3",
      "title": "Adapting Software with Affective Computing: A Systematic Review",
      "authors": [
        "Renan Vinicius Aranha",
        "Cléber Gimenez Corrêa",
        "L Fátima",
        "Nunes"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2019.2902379"
    },
    {
      "citation_id": "4",
      "title": "Wipamas Polpakdee, Thapanun Sudhawiyangkul, Ekkarat Boonchieng, and Theerawit Wilaiprasitporn. 2022. Ubiquitous Affective Computing: A Review",
      "authors": [
        "Rawin Assabumrungrat",
        "Soravitt Sangnark",
        "Thananya Charoenpattarawut"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal",
      "doi": "10.1109/JSEN.2021.3138269"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods",
      "authors": [
        "Deger Ayata",
        "Yusuf Yaslan",
        "Mustafa Kamaşak"
      ],
      "year": "2017",
      "venue": "IU-Journal of Electrical & Electronics Engineering"
    },
    {
      "citation_id": "6",
      "title": "Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems",
      "authors": [
        "Dario Bertero",
        "Farhad Bin Siddique",
        "Chien-Sheng Wu",
        "Yan Wan",
        "Ricky Ho Yin Chan",
        "Pascale Fung"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D16-1110"
    },
    {
      "citation_id": "7",
      "title": "A Review, Current Challenges, and Future Possibilities on Emotion Recognition Using Machine Learning and Physiological Signals",
      "authors": [
        "J Patrícia",
        "Chen Bota",
        "Ana Wang",
        "Hugo Fred",
        "Da Plácido",
        "Silva"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2944001"
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Jason Braithwaite",
        "Robert Derrick G Watson",
        "Mickey Jones",
        "Rowe"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "Random Forests",
      "authors": [
        "Leo Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning",
      "doi": "10.1023/a:1010933404324"
    },
    {
      "citation_id": "10",
      "title": "Self-Assessment Manikin",
      "authors": [
        "Teah-Marie Bynion",
        "Matthew Feldner"
      ],
      "year": "2017",
      "venue": "Self-Assessment Manikin",
      "doi": "10.1007/978-3-319-28099-8_77-1"
    },
    {
      "citation_id": "11",
      "title": "BioSPPy: Biosignal Processing in Python",
      "authors": [
        "Carlos Carreiras",
        "Ana Alves",
        "André Lourenço",
        "Filipe Canento",
        "Hugo Silva",
        "Ana Fred"
      ],
      "year": "2015",
      "venue": "BioSPPy: Biosignal Processing in Python"
    },
    {
      "citation_id": "12",
      "title": "A novel ECG-based real-time detection method of negative emotions in wearable applications",
      "authors": [
        "Zi Cheng",
        "Lin Shu",
        "Jinyan Xie",
        "C Philip Chen"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)",
      "doi": "10.1109/SPAC.2017.8304293"
    },
    {
      "citation_id": "13",
      "title": "Somatic markers and the guidance of behavior: Theory and preliminary testing. Frontal Lobe Function and Dysfunction",
      "authors": [
        "Daniel Antonio R Damasio",
        "Hanna Tranel",
        "Damasio"
      ],
      "year": "1991",
      "venue": "Somatic markers and the guidance of behavior: Theory and preliminary testing. Frontal Lobe Function and Dysfunction",
      "doi": "10.1093/oso/9780195062847.003.0011"
    },
    {
      "citation_id": "14",
      "title": "A machine learning model for emotion recognition from physiological signals",
      "authors": [
        "Juan Antonio Domínguez-Jiménez",
        "Kiara Coralia Campo-Landines",
        "Juan Martínez-Santos",
        "Enrique Delahoz",
        "Sonia Contreras-Ortiz"
      ],
      "year": "2020",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "15",
      "title": "Emotion Recognition from Physiological Signal Analysis: A Review",
      "authors": [
        "Maria Egger",
        "Matthias Ley",
        "Sten Hanke"
      ],
      "year": "2019",
      "venue": "The proceedings of AmI, the 2018 European Conference on Ambient Intelligence",
      "doi": "10.1016/j.entcs.2019.04.009"
    },
    {
      "citation_id": "16",
      "title": "Comparative study of techniques for large-scale feature selection* *This work was suported by a SERC grant GR/E 97549",
      "authors": [
        "F Ferri",
        "P Pudil",
        "M Hatef",
        "J Kittler"
      ],
      "year": "1994",
      "venue": "Machine Intelligence and Pattern Recognition",
      "doi": "10.1016/B978-0-444-81892-8.50040-7"
    },
    {
      "citation_id": "17",
      "title": "Olympia Simantiraki, Alexandros Roniotis, and Manolis Tsiknakis",
      "authors": [
        "Giorgos Giannakakis",
        "Dimitris Grigoriadis",
        "Katerina Giannakaki"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2019.2927337"
    },
    {
      "citation_id": "18",
      "title": "pyHRV -Open-Source Python Toolbox for Heart Rate Variability",
      "authors": [
        "Pedro Gomes"
      ],
      "year": "2018",
      "venue": "pyHRV -Open-Source Python Toolbox for Heart Rate Variability"
    },
    {
      "citation_id": "19",
      "title": "A Survey of Affective Computing for Stress Detection: Evaluating technologies in stress detection for better health",
      "authors": [
        "Shalom Greene",
        "Himanshu Thapliyal",
        "Allison Caban-Holt"
      ],
      "year": "2016",
      "venue": "IEEE Consumer Electronics Magazine",
      "doi": "10.1109/MCE.2016.2590178"
    },
    {
      "citation_id": "20",
      "title": "Short Stress State Questionnaire",
      "authors": [
        "William Helton"
      ],
      "year": "2004",
      "venue": "PsycTESTS Dataset",
      "doi": "10.1037/t57758-000"
    },
    {
      "citation_id": "21",
      "title": "MATLAB version: 9.13.0 (R2022b). Natick, Massachusetts",
      "year": "2022",
      "venue": "MATLAB version: 9.13.0 (R2022b). Natick, Massachusetts"
    },
    {
      "citation_id": "22",
      "title": "MMDA: A Multimodal Dataset for Depression and Anxiety Detection",
      "authors": [
        "Yueqi Jiang",
        "Ziyang Zhang",
        "Xiao Sun"
      ],
      "year": "2022",
      "venue": "ICPR 2022 International Workshops and Challenges: Montreal, QC, Canada"
    },
    {
      "citation_id": "24",
      "title": "Principal component analysis: A review and recent developments",
      "authors": [
        "Ian Jolliffe",
        "Jorge Cadima"
      ],
      "year": "2016",
      "venue": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
      "doi": "10.1098/rsta.2015.0202"
    },
    {
      "citation_id": "25",
      "title": "Multi-Domain Feature Fusion for Emotion Classification Using DEAP Dataset",
      "authors": [
        "Muhammad Khateeb",
        "Syed Muhammad Anwar",
        "Majdi Alnowami"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3051281"
    },
    {
      "citation_id": "26",
      "title": "DEAP: A Database for Emotion Analysis Using Physiological Signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Thierry Pun"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "27",
      "title": "Autonomic nervous system activity in emotion: A review",
      "authors": [
        "D Sylvia",
        "Kreibig"
      ],
      "year": "2010",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "28",
      "title": "Multimodal Hierarchical CNN Feature Fusion for Stress Detection",
      "authors": [
        "Radhika Kuttala",
        "Ramanathan Subramanian",
        "Venkata Ramana",
        "Murthy Oruganti"
      ],
      "year": "2023",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2023.3237545"
    },
    {
      "citation_id": "29",
      "title": "Liebowitz Social Anxiety Scale (LSAS)",
      "authors": [
        "M Liebowitz"
      ],
      "year": "1987",
      "venue": "Liebowitz Social Anxiety Scale (LSAS)"
    },
    {
      "citation_id": "30",
      "title": "Affective Computing for Healthcare: Recent Trends, Applications, Challenges, and Beyond",
      "authors": [
        "Yuanyuan Liu",
        "Ke Wang",
        "Lin Wei",
        "Jingying Chen",
        "Yibing Zhan",
        "Dapeng Tao",
        "Zhe Chen"
      ],
      "year": "2024",
      "venue": "Affective Computing for Healthcare: Recent Trends, Applications, Challenges, and Beyond",
      "arxiv": "arXiv:2402.13589[cs.HC"
    },
    {
      "citation_id": "31",
      "title": "Affective computing and medical informatics: state of the art in emotion-aware medical applications",
      "authors": [
        "Andrej Luneski",
        "D Panagiotis",
        "Madga Bamidis",
        "Hitoglou-Antoniadou"
      ],
      "year": "2008",
      "venue": "Stud. Health Technol. Inform"
    },
    {
      "citation_id": "32",
      "title": "NeuroKit2: A Python toolbox for neurophysiological signal processing",
      "authors": [
        "Dominique Makowski",
        "Tam Pham",
        "J Zen",
        "Jan Lau",
        "François Brammer",
        "Hung Lespinasse",
        "Christopher Pham",
        "S Schölzel",
        "Annabel Chen"
      ],
      "year": "2021",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-020-01516-y"
    },
    {
      "citation_id": "33",
      "title": "The development of a six-item short-form of the state scale of the Spielberger State-Trait Anxiety Inventory (STAI)",
      "authors": [
        "M Theresa",
        "Hilary Marteau",
        "Bekker"
      ],
      "year": "1992",
      "venue": "British journal of clinical Psychology"
    },
    {
      "citation_id": "34",
      "title": "Data Structures for Statistical Computing in Python",
      "authors": [
        "Wes Mckinney"
      ],
      "year": "2010",
      "venue": "Proceedings of the 9th Python in Science Conference"
    },
    {
      "citation_id": "35",
      "title": "The AffectToolbox: Affect Analysis for Everyone",
      "authors": [
        "Silvan Mertes",
        "Dominik Schiller",
        "Michael Dietz",
        "Elisabeth André",
        "Florian Lingenfelser"
      ],
      "year": "2024",
      "venue": "The AffectToolbox: Affect Analysis for Everyone",
      "arxiv": "arXiv:2402.15195[cs.HC"
    },
    {
      "citation_id": "36",
      "title": "MetaFERA: A meta-framework for creating emotion recognition frameworks for physiological signals",
      "authors": [
        "João Oliveira",
        "M Soraia",
        "Teresa Alarcão",
        "Manuel Chambel",
        "Fonseca"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-023-15249-5"
    },
    {
      "citation_id": "37",
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga",
        "Alban Desmaison",
        "Andreas Kopf",
        "Edward Yang",
        "Zachary Devito",
        "Martin Raison",
        "Alykhan Tejani",
        "Sasank Chilamkurthy",
        "Benoit Steiner",
        "Lu Fang",
        "Junjie Bai",
        "Soumith Chintala"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "39",
      "title": "A Circumplex Model of Affect",
      "authors": [
        "James Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "40",
      "title": "Emotion Recognition for Everyday Life Using Physiological Signals From Wearables: A Systematic Literature Review",
      "authors": [
        "Stanisław Saganowski",
        "Bartosz Perz",
        "Adam Polak",
        "Przemysław Kazienko"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3176135"
    },
    {
      "citation_id": "41",
      "title": "Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection",
      "authors": [
        "Philip Schmidt",
        "Attila Reiss",
        "Robert Duerichen",
        "Claus Marberger",
        "Kristof Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection",
      "doi": "10.1145/3242969.3242985"
    },
    {
      "citation_id": "42",
      "title": "A Multimodal Dataset and Evaluation for Feature Estimators of Temporal Phases of Anxiety",
      "authors": [
        "Hashini Senaratne",
        "Levin Kuhlmann",
        "Kirsten Ellis",
        "Glenn Melvin",
        "Sharon Oviatt"
      ],
      "year": "2021",
      "venue": "A Multimodal Dataset and Evaluation for Feature Estimators of Temporal Phases of Anxiety",
      "doi": "10.1145/3462244.3479900"
    },
    {
      "citation_id": "43",
      "title": "A method for intelligent allocation of diagnostic testing by leveraging data from Commercial Wearable Devices: A case study on covid-19",
      "authors": [
        "Mobashir Md",
        "Peter Shandhi",
        "Ali Cho",
        "Karnika Roghanizad",
        "Will Singh",
        "Wang",
        "M Oana",
        "Amanda Enache",
        "Rami Stern",
        "Bilge Sbahi",
        "Sean Tatar",
        "Fiscus"
      ],
      "year": "2022",
      "venue": "Digital Medicine",
      "doi": "10.1038/s41746-022-00672-z"
    },
    {
      "citation_id": "44",
      "title": "A dataset of continuous affect annotations and physiological signals for emotion analysis",
      "authors": [
        "Karan Sharma",
        "Claudio Castellini",
        "L Egon",
        "Van Den",
        "Alin Broek",
        "Friedhelm Albu-Schaeffer",
        "Schwenker"
      ],
      "year": "2019",
      "venue": "Scientific data"
    },
    {
      "citation_id": "45",
      "title": "Affective computing for late-life mood and cognitive disorders",
      "authors": [
        "Erin Smith",
        "Eric Storch",
        "Ipsit Vahia",
        "Stephen Wong",
        "Helen Lavretsky",
        "Jeffrey Cummings",
        "Harris Eyre"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychiatry",
      "doi": "10.3389/fpsyt.2021.782183"
    },
    {
      "citation_id": "46",
      "title": "Heart Rate Analysis for Human Factors: Development and Validation of an Open Source Toolkit for Noisy Naturalistic Heart Rate Data",
      "authors": [
        "Haneen Paul Van Gent",
        "Nicole Farah",
        "B Nes",
        "Arem"
      ],
      "year": "2018",
      "venue": "Heart Rate Analysis for Human Factors: Development and Validation of an Open Source Toolkit for Noisy Naturalistic Heart Rate Data"
    },
    {
      "citation_id": "47",
      "title": "Python reference manual. Centrum voor Wiskunde en Informatica Amsterdam",
      "authors": [
        "Guido Van Rossum",
        "Fred Drake"
      ],
      "year": "1995",
      "venue": "Python reference manual. Centrum voor Wiskunde en Informatica Amsterdam"
    },
    {
      "citation_id": "48",
      "title": "Estimation of dependences based on empirical data",
      "authors": [
        "Vladimir Vapnik"
      ],
      "year": "2006",
      "venue": "Estimation of dependences based on empirical data"
    },
    {
      "citation_id": "49",
      "title": "Smart sensor integration: A framework for multimodal emotion recognition in real-time",
      "authors": [
        "Johannes Wagner",
        "Elisabeth André",
        "Frank Jung"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops",
      "doi": "10.1109/ACII.2009.5349571"
    },
    {
      "citation_id": "50",
      "title": "A Systematic Review on Affective Computing: Emotion Models, Databases, and Recent Advances",
      "authors": [
        "Yan Wang",
        "Wei Song",
        "Wei Tao",
        "Antonio Liotta",
        "Dawei Yang",
        "Xinlei Li",
        "Shuyong Gao",
        "Yixuan Sun",
        "Weifeng Ge",
        "Wei Zhang",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "A Systematic Review on Affective Computing: Emotion Models, Databases, and Recent Advances",
      "arxiv": "arXiv:2203.06935[cs.MM"
    },
    {
      "citation_id": "51",
      "title": "Development and validation of brief measures of positive and negative affect: The PANAS scales",
      "authors": [
        "David Watson",
        "Anna Clark",
        "Auke Tellegen"
      ],
      "year": "1988",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.54.6.1063"
    },
    {
      "citation_id": "52",
      "title": "Investigating the Generalizability of Physiological Characteristics of Anxiety",
      "authors": [
        "Emily Zhou",
        "Mohammad Soleymani",
        "Maja Matarić"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": "10.1109/BIBM58861.2023.10385292"
    },
    {
      "citation_id": "53",
      "title": "Multimodal Mild Depression Recognition Based on EEG-EM Synchronization Acquisition Network",
      "authors": [
        "Jing Zhu",
        "Ying Wang",
        "Rong La",
        "Jiawei Zhan",
        "Junhong Niu",
        "Shuai Zeng",
        "Xiping Hu"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2901950"
    },
    {
      "citation_id": "54",
      "title": "Sentiment analysis and affective computing for depression monitoring",
      "authors": [
        "Chiara Zucco",
        "Barbara Calabrese",
        "Mario Cannataro"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 1988-1995",
      "doi": "10.1109/BIBM.2017.8217966"
    }
  ]
}