{
  "paper_id": "2411.09413v2",
  "title": "Detecting Children With Autism Spectrum Disorder Based On Script-Centric Behavior Understanding With Emotional Enhancement",
  "published": "2024-11-14T13:07:19Z",
  "authors": [
    "Wenxing Liu",
    "Yueran Pan",
    "Dong Zhang",
    "Hongzhu Deng",
    "Xiaobing Zou",
    "Ming Li"
  ],
  "keywords": [
    "Autism Spectrum Disorder",
    "Behavior Textualization",
    "Emotion Textualization",
    "Large Language Model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The early diagnosis of autism spectrum disorder (ASD) is critically dependent on systematic observation and analysis of children's social behaviors. While current methodologies predominantly utilize supervised learning approaches, their clinical adoption faces two principal limitations: insufficient ASD diagnostic samples and inadequate interpretability of the detection outcomes. This paper presents a novel zero-shot ASD detection framework based on script-centric behavioral understanding with emotional enhancement, which is designed to overcome the aforementioned clinical constraints. The proposed pipeline automatically converts audio-visual data into structured behavioral text scripts through computer vision techniques, subsequently capitalizing on the generalization capabilities of large language models (LLMs) for zero-shot/few-shot ASD detection. Three core technical contributions are introduced: (1) A multimodal script transcription module transforming behavioral cues into structured textual representations. (2) An emotion textualization module encoding emotional dynamics as the contextual features to augment behavioral understanding. (3) A domain-specific prompt engineering strategy enables the injection of clinical knowledge into LLMs. Our method achieves an F1-score of 95.24% in diagnosing ASD in children with an average age of two years while generating interpretable detection rationales. This work opens up new avenues for leveraging the power of LLMs in analyzing and understanding ASD-related human behavior, thereby enhancing the accuracy of assisted autism diagnosis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "(A) And (B).",
      "text": "The first is based on modular behavioral signal processing. These methods extract the child's behavioral features (e.g., gaze patterns  [17] , head movements  [18] , body motions  [19] , audio prosody and text transcripts  [20] , facial expression and hand gestures  [21] , etc.) and then analyze these features by backend machine learning algorithms. The second is based on end-to-end raw video classification, which directly learns the mapping relationship between video and labels through deep learning models  [22] -  [24] . Both methods provide quantitative and objective diagnostic tools for ASD detection. However, these supervised learning methods need to be trained by a large amount of data, especially for the end-to-end video classification approach. Hence, the scarcity of ASD data limits their accuracy in practice. Moreover, most of the aforementioned methods are limited to making binary predictions and lack detailed explanations supporting those detection outcomes.\n\nIn recent years, researchers have gradually utilized LLMs  [25] -  [27]  to explore new approaches for medical diagnosis. Compared to traditional AI algorithms, LLMs exhibit two primary advantages for clinical applications  [28] -  [30] . First, their ability to leverage extensive medical knowledge enables reasoning and contextual comprehension, forming an interpretable foundation for diagnostic decision-making. This capability establishes a crucial foundation for ensuring the interpretability of diagnostic decisions. Second, their inherent prior knowledge supports zero-shot and few-shot learning, reducing dependence on clinical training data.\n\nWhile LLMs show potential for medical diagnostics, three fundamental challenges persist in processing ASD-specific audio-visual behavioral signals: 1) Modality adaptation gap. Current LLMs excel at text and image processing  [31] -  [36] , but face inherent limitations in accurate and robust cross-modal alignment between audio-video inputs and textual representations  [37] ,  [38] , creating fundamental barriers to biomedical behavioral signal processing. 2) Domain prompt Design. While prompt engineering significantly enhances LLM capabilities for domain-specific tasks  [39] , the systematic design of effective prompts and the optimal infusion of ASDrelated domain knowledge still remain important research questions. 3) Emotional dynamic Modeling. Emotional dynamic is a critical biomarker for ASD detection  [40] -  [42] , a key challenge lies in modeling emotional dynamic as a textual feature that enhances LLMs-based ASD detection.\n\nTherefore, this paper introduces a novel pipeline for ASD detection using LLMs, which can determine ASD and provide explanations. First, to solve the problem of mismatch between audio-visual behavior data and the input modality of LLMs, we convert the video content (e.g. the characters' gestures, head poses, body' movements, facial expressions, speech content, gaze, and etc.) as time-stamped text inspired by movie scripts. Specifically, we use a behavioral transcription module to convert the video content into human behavioral logs, and a script transcription module is designed to process these behavioral logs into natural language texts. Second, to improve the accuracy of LLM for ASD detection, we created a domain prompt module to incorporate ASD domain knowledge. Finally, we design the emotion textualization module to enhance LLMs' understanding of the emotional dynamics in ASD detection. Our main contributions can be summarized as follows:\n\n• To the best of our knowledge, our approach is the first to introduce LLMs for detecting ASD from audio-visual data, laying the foundation for the exploration of LLMs in this domain. • In order to accurately describe the behavioral data and utilize the domain knowledge, we propose a script transcription module and a domain prompt module. They build a bridge between audio-visual data and LLMs, facilitating the development of multimodal ASD detection. • We design an emotion textualization module to add emotional dynamics that are usually ignored in the behavioral script, further enhancing the detection accuracy.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Modular Behavior Signal Processing Methods For Asd Detection",
      "text": "To overcome ASD data scarcity, some approaches leverage cross-domain behavioral signal processing modules for feature extraction, followed by ASD-targeted modeling with domain knowledge. Hashemi et al.  [43]  developed a mobile screening system that employs cinematic stimuli to elicit quantifiable social responses (e.g., name recognition, joint attention, affective reciprocity) for automated ASD classification via behavioral pattern analysis. Negin et al.  [44]  implemented a Bag-of-Visual-Words (BoVW) framework to extract local descriptors from video data, employing multiple machine learning classifiers for automated ASD screening. Zhang et al.  [45]  implemented a 3D spatiotemporal facial analysis pipeline and a few-shot learning strategy to evaluate discriminative facial dynamics for ASD classification. Atyabi et al.  [46]  developed a multimodal integration framework combining behavioral biomarkers-including eye movement scan paths, temporal information and pupil velocity-to differentiate ASD and Typically Developed (TD). Cheng et al.  [15]  developed a computer-aided ASD detection system employing multimodal behavioral signal analysis. The system's multimodal behavioral transcription module and response parser recognizes audio-visual signals to identify child's behaviors, and a backend machine learning model is trained based on the paradigm scores and behavioral features to provide assisted ASD detection. Nie et al.  [47]  formalized child-caregiver interactions through a Computational Interpersonal Communication Model (CICM) grounded in Theory of Mind (ToM), employing Markov decision processes to decode multimodal behavioral signals for early screening. While demonstrating diagnostic potential, these approaches predominantly require substantial domain-specific expertise and customized behavioral signal processing module designs, exhibiting limited generalizability across diverse clinical scenarios.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. End-To-End Video Classification For Asd Detection",
      "text": "Many researchers employ deep learning algorithms to directly model raw videos in an end-to-end manner. Li et al. [48] pioneered an LSTM-based deep learning architecture for automated ASD detection through raw video, specifically targeting discriminative gaze pattern classification between ASD and TD. Pandian et al.  [49]  developed the RGBPose-SlowFast network to automatically detect stereotypical motor behaviors in children with ASD, demonstrating the viability of multi-stream neural networks for automated ASD screening. Wei et al.  [50]  explored a hybrid architecture integrating handcrafted feature extractors with 3D convolutional neural networks for automated detection of stereotyped motor mannerisms in ASD. Chen et al.  [24]  employ Longformer to establish the correlation of facial features in videos over time, aiming to learn a deep representation from dynamic facial data for ASD detection. Asha et al.  [23]  developed a supervised contrastive learning framework to extract crossdataset discriminative feature representations for ASD and TD, ultimately deploying an automated diagnostic classification pipeline leveraging raw video. However, current limitations in ASD audio-visual datasets critically constrain raw video classification techniques that demand extensive training data for reliable behavioral pattern recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Llms Based Methods For Medical Detection",
      "text": "In recent years, LLMs have demonstrated advanced natural language processing capabilities, showing particular promise in clinical diagnostics for developing interpretable decisionmaking systems that balance accuracy and transparency. The Med-PaLM  [51]  is a medical LLM developed by the Google team. Its strong performance in answering medical questions demonstrates the extensive medical knowledge embedded in LLMs. Huatuo GPT  [52]  is an LLM for medical consulting built on the open-source LLaMa-7B model  [53] . The LLM learns structured and unstructured medical knowledge from the Chinese Medical Knowledge Atlas and can diagnose over 3,000 diseases. Through a prompt engineering approach, Medprompt  [54]  demonstrates that general LLMs achieve competitive performance even without domain-specific pretraining. The LLaVA-ASD  [55]  multimodal LLM is designed to detect social and repetitive behaviors through audio-visual cues. While multimodal LLMs can process audio-visual inputs directly, insufficient detail in behavioral descriptions generated by general visual understanding models lead to reduced performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Emotion Based Methods For Asd Detection",
      "text": "The DSM-V  [2] , published by the American Psychiatric Association, is the most authoritative and widely used manual for diagnosing mental disorders, including ASD. Its criteria emphasize significant deficits in the emotional domain, such as lack of facial expression. These challenges stem from impaired emotional expression and regulation mechanisms in ASD  [40] . This has driven the development of computational approaches targeting emotion-related deficits in ASD through computerassisted methodologies. Sarabadani et al.  [56]  proposed a method to recognize emotional states by physiological signals automatically. The study indicated that children with ASD exhibited different responses compared to children with TD when viewing images of the same emotional valence. Piana et al.  [57]  developed an automatic emotion recognition system to support children with ASD in learning emotion recognition and expression through whole-body movements. Prakash et al.  [22]  develop a framework for extracting motor behaviors, emotional states, and facial expressions from child-caregiver interactive videos. This multimodal integration of behavioral Fig.  3 : A two-stage pipeline in the behavioral transcription module. (1) Multi-person identification and localization is used to locate the location information and identify the participant in each frame. (2) Single-person behavior perception is used to perceive the behavioral information of each individual and affective data enables robust diagnostic frameworks for ASD. Rashidan et al.  [58]  verified that appropriate video stimuli can elicit emotional responses in children with ASD and also demonstrated significant differences in emotion regulation between children with ASD and TD.\n\nIn summary, given the limited audio-visual data in the ASD domain, using behavior signal processing modules, including emotion recognition modules, to generate high quality behavior scripts and feeding them to LLMs with ASD domain knowledge for zero-shot or few-shot learning is a very promising area to explore for ASD detection. 3) The emotion textualization module enhances script details by adding appropriate emotional descriptions. 4) The domain prompts module combines script context, system prompt and ASD-related knowledge together to understand human behaviors better. Ultimately, we rely on pretrained LLMs to detect ASD and produce judgments by answering questions based on the script description.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Behavior Transcription Module",
      "text": "Following our previous work  [15] , the behavior transcription module can recognize the position, gesture, body movement, head pose, eye gaze, emotion, speech content of all individuals in each segment by combining audio and image models. As shown in Fig.  3 , the behavioral transcription module is divided into two phases to convert recorded audio-visual data into behavioral logs.\n\nThe first stage, called the multi-person identification and localization stage, has the core objective of localizing and identitying different people in each frame. For each image frame, we first use an instance segmentation model (Solov2  [59] ) to extract human body regions, including bounding boxes and masks. Moreover, it also goes through a face detection model (RetinaFace  [60] ) to localize face bounding boxes. To determine which body bounding box corresponds to the face bounding box, we use the smallest Intersection Over Union (IOU) for matching. Then, the face recognition model (ArcFace  [61] ) IS used to distinguish the identity of characters. If face recognition is possible due to facial occlusion, we\n\nRecord the moment t n of the P n , and the interval of dynamic [t n -0.5s, t n + 0.5s].",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "5:",
      "text": "end if 6: end for 7: while P n and P n+1 have overlap do 8:\n\nMerged into one continuous emotional segment: P n = [t n-start , t n+1-end ]. 9: end while use the person re-identification model (BOTRReID  [62] ) to recognize the identity through body's features.\n\nThe second stage, called the single-person behavior perception stage, processes the behavior of the identified characters separately. The gaze and head pose Estimation model (SYSUGaze  [63] ) are used to localize the direction of a character's head and gaze attention. The emotion recognition model  [64]  is used for three-category facial expression classification (neutral, happy, sad) and continuous valence and arousal emotion regression. The body key points model (HRNet  [65] ) is used to recognize hand-raising movements. The Yolov5 1  model is used to detect the hand region and train a standard ResNet-50 model  [66]  to recognize the 4-class gesture. The Automatic speech recognition model (Kaldi  [67] ) is used to recognize participants' speaking contents.\n\nWe define the audio and video transcription task as a formulation of multiple perception model inference. This is denoted as:\n\nwhere [B] i denotes the behavioral log of the i th frame, f j image denotes the j th image model, f k audio denotes the k th audio model, where (I i ) and (S i ) denote the i th frame of image and audio signals, respectively.\n\nTo ensure a fair comparison with our previous work  [15] , we used the same but older perceptual model. More advanced ones can replace these ones to further the accuracy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Script Transcription Module",
      "text": "Behavioral logs are generated by merging outputs from various models, resulting in data of different formats, such as body coordinates, eye vectors, facial expression types, gesture types, and speech content, etc.. To transform these behavioral logs into text that LLMs can understand, we developed a script transcription module to standardize and unify these diverse outputs into a coherent script.\n\n1) Response Parser: Although the behavioral logs contain information about the actions of each individual in the video, they cannot be directly input into LLMs for analysis. There are two main reasons for this: 1. The perceptual model outputs either categories (e.g., facial expression categories) or values (e.g., gaze angles), and abstract inputs prevent LLMs from understanding the underlining behaviors. 2. Behavioral logs are recorded frame-by-frame, and lengthy inputs can lead to LLMs forgetting crucial information or struggling to detect ASD symptom representations.\n\nTo address these issues, we need to utilize basic response events to simplify the behavioral log. In this paper, each video recording includes six paradigms, namely Response to Name (RN)  [68] , Social Smile (SS)  [69] , Indicating Gesture (IG)  [70] , Responding to Joint Attention (RJA)  [71] , Initiating Joint Attention (IJA)  [72] , and Separation Anxiety (SA)  [15] , these paradigms correspond to those presented in Table  I  and Table  II . As shown in Table  I , We summarize the response events in the above paradigm that are most important for ASD observation. Generally, the observation of each paradigm is based on the following events. We define E as the paradigm's set of all doctor-patient interactions. E 1 represents the event in which the child looks at the target object, called Look at Object. E 2 represents the event in which the child points at the target object, called Point to Object. E 3 denotes the event where the child smiles, called Smile. E 4 denotes the event when the child or doctor speaks, called Speak. E 5 represents the event in which parents or doctors exit the testing studio, called Leave. The following formulation defines the response parsing process:\n\nwhere R records the timestamp of event E occurring in [B].\n\n2) Response Textualization Module: After capturing a response event, we must describe its occurrence and behavior in the text format. The response textualization module can convert the predefined events in the paradigm video into",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ss P3",
      "text": "The doctor greets the child with a passional smile and say hello. response 1 The child made eye contact with the doctor. P4\n\nThe doctor praises the child with a warm smile. response 2 The child look at the doctor and smile .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "P5",
      "text": "The doctor plays a tickle game with smile. She slowly reaches out and gently touches the child. response 3 The child smile but did not look at the doctor.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "P6",
      "text": "With a warm smile, parents entertain their children in whatever way they normally do in their daily lives. response 4 The child bent his head and went on playing with the toy.\n\nresponse 5 The child made eye contact with the doctor without smile.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ig P7",
      "text": "The doctor call the child's name and say \"Look at that flower\". response 1 The child looked up in the direction of the picture.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "P8",
      "text": "The doctor call the child's name and say \"Look at that tree\". response 2 The child keeps his head down and continues to play with his toy.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "P9",
      "text": "The doctor call the child's name and say \"Look at that balloon\" response 3 The child precisely points out the location of the picture. p10\n\nThe doctor call the child's name and say \"Look at that sofa\" response 4 The child roughly points out the location of the picture. response 5 The child turns around and makes eye contact with the doctor. response 6 The child keeps looking at the picture. response 7 Then the kid continue to play with his toy.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Rja P11",
      "text": "The doctor raises his hand and points to the picture of a clock and says, \"Look, there is a clock. what time it is. response 1\n\nThe child turns his head backand then looks to the position of the clock. response 2 The child seek the clock while not finding the correct direction. response 3 The child looked up at the doctor's hand . response 4 The child keeps his head down and continues to play with his toy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ija",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "P12",
      "text": "The wall to the left of the child suddenly displays a yellow bird flapping its wings while a stereo plays the sound of bird. response 1\n\nThe child is attracted to the animation playingand looks at the bird on the left wall.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "P13",
      "text": "The wall to the right of the child suddenly displays a moving riding car while the stereo plays the sound of the car moving. response 2\n\nThe child turns around and makes eye contact with the doctor to share his findings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "P14",
      "text": "A cow wiggling its ears is suddenly displayed on the wall behind the child's right side while the sound is played. response 3\n\nThe child turns around and makes eye contact with the doctor to share his findings. response 4 The child keeps staring at the animation playing on the wall. response 5 The child raises his hand and points to the bird on the wall. response 6 The child lower his head again and continued to play with the toy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sa P15",
      "text": "The parent gets up from their seat, walks past the child, and finally leaves the room. response 1\n\nThe child realizes that the parent has left and gets up and chases him toward the door.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "P16",
      "text": "The parent call the childś name outside the door and say, \"Hi, mom is leaving. You have to play alone. response 2\n\nThe child turns to the direction of the parent but remains seating at the table. response 3 The child keeps his head down and continues to play with his toy. response 4 The parents, the doctor and the child have left the room. response 5 The child lower his head again and continued to play with the toy. textual descriptions. As shown in Fig.  4 , this illustrates the textualization process for the Response to Name paradigm. In the paradigm, the child participant is first guided to play with toys on the desk. Once their attention is engaged, the assessor suddenly calls the child's name from behind. The child can exhibit one of three responses: 1) no response, 2) turning to face the caller, and 3) turning to face the caller and responding verbally. The whole paradigm process is shown as a flowchart, and we select descriptions from the RN paradigm in Table  II  for textualized combinations depending on the instructions and responses. In addition, we found that adding background and gender descriptions before the instruction description can improve diagnostic accuracy. Based on the paradigm's flowchart in our previous work  [15] , Table  II  shows all paradigms' instruction and response descriptions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "3) Emotion Textualization Module:",
      "text": "The DSM-V  [2]  emphasizes that \"ASD children lack social or emotional reciprocity.\" This deficit manifests in their difficulty understanding the emotions of others, being unresponsive, or showing indifference to emotional signals. This could be visually reflected in facial expressions' valence or arousal values. However, the paradigm design in our dataset only considers category emotion classes (e.g., smile or neutral) in the Social Smile (SS) paradigm, as shown in Table  I . To enable LLMs to better understand the deficits in emotional reciprocity, we aim to reflect continuous emotional dynamics in the script. Therefore, we designed an emotion textualization module to capture the emotional dynamic points and transform video segments near these points into textual emotion descriptions.\n\nThe first-order derivative of emotion represents the rate of emotional change and serves as a key feature for understanding emotional dynamics  [73] -  [75] . To capture emotional dynamic points, we approximate the degree of emotion variation using the first-order derivative of the valence value. The detailed process of finding emotional dynamic points is described in Algorithm 1. Let [a 1 , a 2 , . . . , a n ] represent the valence sequence of facial expression, where a n is the valence value of the nth frame. Similarly, let [d 1 , d 2 , . . . , d n ] denote the sequence of first-order derivatives of the valence sequence, where d n represents the first-order derivative of the valence value at the n th frame. We define P as the emotional dynamic point and identify its location based on the emotional dynamic threshold α. Specifically, we identify a P n when d n > 0.2 or d n < -0.2 and define the 1-second video segment before and after this point as a emotional dynamic segment. Notably, a paradigm video may contain multiple emotional dynamic segments. If these segments overlap, they are merged into a single continuous mood segment. Finally, we get the  In transforming emotional fragments into emotional descriptions, it is challenging to manually summarize emotional dynamics into a limited set of discrete emotional events. In recent years, the general video understanding LLMs  [76] -  [78]  has demonstrated a remarkable ability to generate video descriptions, enabling the transformation of video content into emotionally relevant textual descriptions. Given that our data includes both video and audio modalities, we employ the audio-visual LLM model (video-SALMONN  [78] ), to analyze visual frame sequences and audio events, with a primary focus on capturing emotionally relevant content. For segments, we generate textual descriptions of audio-visual content by prompting it with emotion-related queries. Figure  12  illustrates the emotion prompt and the emotion question.  3) The domain prompt incorporates domain knowledge  [79]  and experience into the script descriptions. In this context, domain knowledge refers to the ASD diagnostic criteria, while experience represents the researcher's experiential knowledge in the clinical setup. 4) The format prompt constrains the output results.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Large Language Models",
      "text": "In this paper, we selected eight LLMs with strong performance and reputation: three closed-source models (GPT-4O  [31] , Claude 3.5-sonnet  [32] , Monnshot  [35] ) and five opensource models (LLAMA 3.1  [33] , qwen2-72B-instruct  [34] , DeepSeek-R1-Distill-Qwen-32B  [36] , DeepSeek-R1-Distill-Llama-70B  [36] , DeepSeek-R1-671B  [36] . We input script descriptions and user questions into these LLMs, and the assisted ASD detection results and interpretations were extracted from the answers. For few-shot learning, we only evaluate an opensource model deployed in our server because uploading data to a closed-source API may lead to data leakage. All LLMs are configured for fair experimentation with a max token limit of 1000 and a temperature setting of 0.7.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. The Joint Detection Of Multiple Llms",
      "text": "To further improve detection accuracy and reduce the hallucination from individual LLM, we fuse the outputs of multiple LLMs. The most straightforward strategy is a majority voting method, which we refer to as SCBU-Vote. We selecte five LLMs with high variability (GPT-4O  [31] , Claude 3.5-sonnet  [32] , Monnshot  [35] , qwen2-72B-instruct  [34] , DeepSeek-R1-671B  [36] ), and their predictions were aggregated through a voting mechanism to determine the final result. However, a clear limitation of this approach is that voting only applies to the final detection outcome, without providing a unified or interpretable rationale for the final decision.\n\nInspired by multi-agent medical diagnostics  [80] , we propose a framework for the joint detection of multiple LLMs called SCBU-Agents. The framework is divided into three stages, as shown in",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Dataset",
      "text": "We utilized the multimodal behavioral database in  [15]  to evaluate our method. This database comprises RGB, RGB-D and audio data recorded in a real clinical environment. Specifically, the dataset included eight distinct views of RGB HD video with a resolution of 4096 × 3000 and four different views of depth video with a resolution of 1280 × 720. These synchronized cameras ensured that the subject's behavior response were largely unobstructed.\n\nThe database included 95 participants, comprising 71 children diagnosed with ASD and 24 TD children. Fig.  8  illustrates the age and gender distributions of children with ASD and TD. The ages of ASD and TD participants in the dataset were clustered within the range of 15 to 30 months. The TD group exhibited a more balanced distribution between males and females, whereas the ASD group had a significantly higher proportion of males than females. These characteristics align with the clinical distribution of the data , More details about this dataset can be found in  [15] . For each assessment case, the physician will lead the child participant and parent through the six paradigms, which typically takes 20 to 30 minutes.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Overall Performance",
      "text": "We employ four widely used metrics to evaluate methods in ASD detection: Accuracy (ACC), F1-score (F1), Sensitivity (SN), and Specificity (SP). Accuracy represents the percentage of correct ASD predictions. The F1-score considers both recall and precision, reflecting the overall performance of the methods. Sensitivity indicates the ability to identify ASD cases correctly, while specificity measures the ability to identify non-ASD cases correctly. Higher values for these metrics indicate better method performance.\n\nTable  III  compares the results of different LLMs and the impact of incorporating emotion descriptions. In addition, our proposed zero-shot method SCBU is also compared with the supervised method proposed by Cheng et al. in  [15] . When using a single LLM without emotion descriptions, SCBU-Claude 3.5 (w/o emotion) achieves an F1-score of 91.55%, remarkably close to the supervised learning baseline (92.20%). Similarly, SCBU utilizing other LLMs also achieves comparable accuracy, demonstrating the feasibility of LLMs for ASD detection. When using a single LLM with emotional descrip- tions, SCBU-DeepSeekR1-4O (w/ emotion) achieves an F1score of 94.04%, surpassing the supervised learning baseline. This result indicates that incorporating the emotional dynamic descriptions enhances the distinction between ASD and TD.\n\nAdding appropriate emotional descriptions in behavior scripts can further enhance LLMs' understanding of ASD. Finally, we fused the detection results of multiple LLMs using two strategies: SCBU-Vote and SCBU-Agents. The results of these fusion methods are presented in Table  1 . Both approaches outperform all single LLM method and significantly surpass the supervised method. Notably, SCBU-Agents (w/ emotion) achieved the highest F1-score of 95.24%. Further details of the SCBU-Agents' discussion process and detection rationale are found at https://github.com/lwx0724/ Script-Centric-Behavior-Understanding.\n\nIn addition to achieving high diagnostic accuracy, Fig.  9  presents two examples demonstrating the interpretability of LLMs in autism detection. As shown in Fig.  9  1), The LLMs' response include the reasons for the judgments and the judgment results. The reasons provided by the LLM for its judgments include competencies in social skills, responses to name-calling, smile interactions, stereotypical behaviors, and age factors. The bolded section on the left highlights the child's deficits in social interactions, which align with the DSM-V criteria for ASD diagnosis. As shown in Fig.  9  2), This explains the LLM's detection in TD children. The LLM also considers ASD characteristics such as social and communication skills, attention, eye contact, desire to share, and age. In summary, the LLM's interpretation of the results aligns with human judgment expectations and can serve as an alternative physician reference. The results show that our method can explain the causes and enhance the credibility of   2 The values of valence and arousal range from -1 to 1. 3 The dynamic Frequency is measured in times. assisted detection.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Behavioral Distinction",
      "text": "To verify whether behavioral descriptions can effectively distinguish ASD, Table  IV  demonstrates that all behaviors exhibited significant differences in response latency and response duration (p < 0.05). The mean delay in TD children looking at or pointing to the target object after the doctor's command was shorter than that in ASD children, while the mean duration was more prolonged. Furthermore, the mean latency in chasing ability was also shorter in TD children than in those with ASD. These findings align with ASD characteristics, such as reduced response flexibility and lower concentration in social interactions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Emotional Dynamic Distinction",
      "text": "Valence and arousal are the two core dimensions of emotion, forming the fundamental framework of emotional representation. Valence determines the overall direction of emotion (positive or negative), while arousal describes its intensity  [81] . Tseng et al.  [82]  demonstrated that the range of emotional ratings in the ASD group was consistently limited. As shown in Fig.  10 , both the valence and arousal ranges of the ASD group were significantly smaller than those of the TD group. Therefore, we utilized the differences in valence and arousal values between the ASD and TD groups to identify the emotional dynamic points. Furthermore, a t-test was conducted on our data to determine which measure showed greater statistical significance. Table  IV  presents the significance analysis of valence and arousal. There was a less significant difference in the Arousal minimum between the TD and ASD groups (p = 0.1184). Similarly, for the arousal maximum, the difference was also not high statistically significant (p = 0.0583). In contrast, the TD group had a wider range of valence than the ASD group, and both the valence maximum and minimum differed significantly (P < 0.001). Hence, this paper utilizes valence values to describe the emotional dynamic points in children with ASD.\n\nWhy do the emotional dynamic points contribute to distinguish TD from ASD? Differences in emotional dynamics are demonstrated in Table  IV . Children with ASD exhibit a lower frequency of emotional dynamics compared to TD children, with a statistically significant difference (p < 0.001). Additionally, emotional dynamics in children with ASD occurred later than in TD children, with a significant difference in latency (p < 0.05).",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Ablation Study",
      "text": "General video understanding model Since generic large video understanding models can generate video descriptions,    scripts generated directly by video understanding models and underscores the importance of the transcription module we designed. A possible explanation for this is that behavior script descriptions capture critical responses of children with ASD during the paradigm process. In contrast, descriptions generated by generic video understanding models tend to be overly general, failing to capture the detailed discriminative behavior response. Few-Shot Given the fairness and label leakage concerns associated with closed-source LLMs that upload data, we conducted few-shot experiments on our locally deployed opensource models Qwen2 and DeepseekR1. Qwen2 has a maximum input token limit of 128,000, which constrains the length of the input script. After testing, we set the maximum few-shot number of scripts without emotional descriptions to 20. Since adding emotional descriptions increases the length of a single script, the maximum few-shot number is set as 8.  Emotion thresholds. The emotion threshold setting influences the number of emotional dynamic points. Specifically, a higher threshold results in fewer detected emotional dynamic points. If the number of emotional dynamics is too small, capturing variations in emotional valence becomes difficult. Conversely, an excessive number of dynamic points complicates the ability of LLMs to distinguish ASD from TD. Therefore, striking a balance between these factors requires careful selection of the emotion threshold.   VIII , the domain knowledge, the human experience, and emotion description can improve performance, respectively. Specifically, relying solely on domain knowledge can lead to high sensitivity but low specificity. However, incorporating human experience can effectively constrain the model, resulting in more balanced performance. Furthermore, adding emotional descriptions can comprehensively enhance the ASD detection capability of LLMs. These ablation experiments demonstrate the importance of designing tailored prompts for the ASD detection task. Our method achieved the best performance when all prior information are introduced simultaneously.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "F. Limitations And Discussions",
      "text": "Our method converts audio-visual behavior into textual scripts to detect ASD. This conversion of clinical data into text helps prevent the leakage of patient privacy and facilitates data sharing among peers. Similarly, using text as a medium, our method adds descriptions of emotional dynamic points to the scripts, as if adding valid handcrafted features to existing data. The textual format also enables clinicians' experiences to be translated into behavioral features, making it easier to model practical behavioral markers. Additionally, the LLMs analyze the patient's behavioral script, enabling the model to provide diagnostic explanations, which assist doctors in analyzing the causes of ASD in children.\n\nAlthough our method performs well in ASD detection, it still has limitations in practical scenarios. First, the detection requires both audio and video to be recorded in a controlled environment, as it is essential to ensure that the quality of the data is sufficient for stable behavior analysis. Second, the diagnostic video content is limited to structured paradigms, and robust modules for behavior transcription are still not mature in unstructured settings. Finally, while emotional descriptions enhance the effectiveness of LLMs in ASD detection, they depend on the model's ability to understand audio-visual data. The current general video understanding models still have limitations in psychological analysis. In future work, we aim to enable behavior perception and transcription in unstructured scenes or train a generalized video understanding model explicitly focused on behavior response, so LLMs can better perform behavior assessment tasks in the ASD domain.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this study, we propose a novel zero-shot and few-shot approach for detecting ASD using LLMs. We develop a script transcription Module to convert audio-visual content into scripts. We design a domain prompts module to better leverage prior knowledge of ASD. Furthermore, we added an emotion textualization module to convert videos with intense emotional dynamics into textual descriptions to enhance the quality of behavior. Extensive experimental results demonstrate the effectiveness of our method, showing strong zero-shot and few-shot capabilities. Moreover, LLMs explain the reasoning behind ASD detection, which helps physicians analyze the detection process more effectively. Future research will focus on developing more straightforward and generalized behavioral perception models for video in unconstrained environments or general video understanding models in the ASD domain.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Pipeline comparing (a) behavioral signal processing",
      "page": 1
    },
    {
      "caption": "Figure 1: (a) and (b).",
      "page": 2
    },
    {
      "caption": "Figure 2: The overview of our proposed Script-Centeric Behavior Understanding(SCBD) framework. Behavior Transcription",
      "page": 3
    },
    {
      "caption": "Figure 3: A two-stage pipeline in the behavioral transcription module. (1) Multi-person identification and localization is used",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates the overall framework of our proposed",
      "page": 4
    },
    {
      "caption": "Figure 3: , the behavioral transcription module is divided",
      "page": 4
    },
    {
      "caption": "Figure 4: The textualization process of the Response to Name",
      "page": 5
    },
    {
      "caption": "Figure 4: , this illustrates the",
      "page": 6
    },
    {
      "caption": "Figure 5: The emotion textualization process of the Response to Name paradigm. The blue line indicates the valence value. The",
      "page": 7
    },
    {
      "caption": "Figure 6: The details of the script description. The blue part is",
      "page": 7
    },
    {
      "caption": "Figure 12: illustrates",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the",
      "page": 7
    },
    {
      "caption": "Figure 6: The domain prompt module consists of four",
      "page": 7
    },
    {
      "caption": "Figure 7: This is the framework for the joint detection of multiple LLMs. The process is divided into three stages: (1) Re-analysis",
      "page": 8
    },
    {
      "caption": "Figure 7: (1) Re-analysis and re-detection.",
      "page": 8
    },
    {
      "caption": "Figure 8: illustrates",
      "page": 8
    },
    {
      "caption": "Figure 8: Statistics for ASD and TD groups in the clinical",
      "page": 9
    },
    {
      "caption": "Figure 9: presents two examples demonstrating the interpretability of",
      "page": 9
    },
    {
      "caption": "Figure 9: 2), This explains the LLM’s detection in TD children. The",
      "page": 9
    },
    {
      "caption": "Figure 9: Examples of LLM detection for children with ASD and TD",
      "page": 10
    },
    {
      "caption": "Figure 10: , both the valence and arousal ranges of the ASD",
      "page": 10
    },
    {
      "caption": "Figure 10: Comparison in the ranges of valence and arousal",
      "page": 11
    },
    {
      "caption": "Figure 11: The impact of different emotional thresholds on",
      "page": 12
    },
    {
      "caption": "Figure 12: An example description of an emotional dynamic",
      "page": 12
    },
    {
      "caption": "Figure 11: presents the",
      "page": 12
    },
    {
      "caption": "Figure 12: provides an example of",
      "page": 12
    },
    {
      "caption": "Figure 11: The emotion prompt makes",
      "page": 12
    },
    {
      "caption": "Figure 12: contains a textual description of",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Both approaches outperform all single LLM method and",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Autism spectrum disorder",
      "authors": [
        "C Lord",
        "M Elsabbagh",
        "G Baird",
        "J Veenstra-Vanderweele"
      ],
      "year": "2018",
      "venue": "The lancet"
    },
    {
      "citation_id": "2",
      "title": "The dsm-5: Classification and criteria changes",
      "authors": [
        "D Regier",
        "E Kuhl",
        "D Kupfer"
      ],
      "year": "2013",
      "venue": "World psychiatry"
    },
    {
      "citation_id": "3",
      "title": "The lancet commission on the future of care and clinical research in autism",
      "authors": [
        "C Lord"
      ],
      "year": "2022",
      "venue": "The Lancet"
    },
    {
      "citation_id": "4",
      "title": "Overview of meta-analyses on early intensive behavioral intervention for young children with autism spectrum disorders",
      "authors": [
        "B Reichow"
      ],
      "year": "2012",
      "venue": "J. Autism Dev. Disord"
    },
    {
      "citation_id": "5",
      "title": "Autism diagnostic interviewrevised: A revised version of a diagnostic interview for caregivers of individuals with possible pervasive developmental disorders",
      "authors": [
        "C Lord",
        "M Rutter",
        "A Couteur"
      ],
      "year": "1994",
      "venue": "J. Autism Dev. Disord"
    },
    {
      "citation_id": "6",
      "title": "Austism diagnostic observation schedule: A standardized observation of communicative and social behavior",
      "authors": [
        "C Lord"
      ],
      "year": "1989",
      "venue": "J. Autism Dev. Disord"
    },
    {
      "citation_id": "7",
      "title": "Deep learning with image-based autism spectrum disorder analysis: A systematic review",
      "authors": [
        "M Uddin",
        "M Shahriar",
        "M Mahamood",
        "F Alnajjar",
        "M Pramanik",
        "M Ahad"
      ],
      "year": "2024",
      "venue": "Eng. Appl. Artif. Intell"
    },
    {
      "citation_id": "8",
      "title": "Identifying autism spectrum disorder from resting-state fmri using deep belief network",
      "authors": [
        "Z.-A Huang",
        "Z Zhu",
        "C Yau",
        "K Tan"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst"
    },
    {
      "citation_id": "9",
      "title": "A residual graph convolutional network with spatio-temporal features for autism classification from fmri brain images",
      "authors": [
        "K.-W Park",
        "S.-B Cho"
      ],
      "year": "2023",
      "venue": "Appl. Soft. Comput"
    },
    {
      "citation_id": "10",
      "title": "Spatial-temporal co-attention learning for diagnosis of mental disorders from resting-state fmri data",
      "authors": [
        "R Liu",
        "Z.-A Huang",
        "Y Hu",
        "Z Zhu",
        "K.-C Wong",
        "K Tan"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst"
    },
    {
      "citation_id": "11",
      "title": "Diagnosis of autism disorder based on deep network trained by augmented eeg signals",
      "authors": [
        "H Ardakani",
        "M Taghizadeh",
        "F Shayegh"
      ],
      "year": "2022",
      "venue": "Int. J. Neural Syst"
    },
    {
      "citation_id": "12",
      "title": "Resting-state eeg power differences in autism spectrum disorder: a systematic review and metaanalysis",
      "authors": [
        "W Neo",
        "D Foti",
        "B Keehn",
        "B Kelleher"
      ],
      "year": "2023",
      "venue": "Transl. Psychiatr"
    },
    {
      "citation_id": "13",
      "title": "Developmental trajectories of eeg aperiodic and periodic components in children 2-44 months of age",
      "authors": [
        "C Wilkinson"
      ],
      "year": "2024",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "14",
      "title": "Discriminative few shot learning of facial dynamics in interview videos for autism trait classification",
      "authors": [
        "N Zhang",
        "M Ruan",
        "S Wang",
        "L Paul",
        "X Li"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "15",
      "title": "Computer-aided autism spectrum disorder diagnosis with behavior signal processing",
      "authors": [
        "M Cheng"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "16",
      "title": "Hear me, see me, understand me: Audio-visual autism behavior recognition",
      "authors": [
        "S Deng"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multimedia",
      "doi": "10.1109/TMM.2024.3521838"
    },
    {
      "citation_id": "17",
      "title": "Gaze patterns in children with autism spectrum disorder to emotional faces: Scanpath and similarity",
      "authors": [
        "W Zhou",
        "M Yang",
        "J Tang",
        "J Wang",
        "B Hu"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Neural Syst. Rehabil. Eng"
    },
    {
      "citation_id": "18",
      "title": "Automatic classification of asd children using appearance-based features from videos",
      "authors": [
        "J Li",
        "Z Chen",
        "G Li",
        "G Ouyang",
        "X Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "19",
      "title": "Using 2d video-based pose estimation for automated prediction of autism spectrum disorders in young children",
      "authors": [
        "N Kojovic",
        "S Natraj",
        "S Mohanty",
        "T Maillart",
        "M Schaer"
      ],
      "year": "2021",
      "venue": "Sci Rep"
    },
    {
      "citation_id": "20",
      "title": "Benchmarking children's asr with supervised and self-supervised speech foundation models",
      "authors": [
        "R Fan",
        "N Shankar",
        "A Alwan"
      ],
      "year": "2024",
      "venue": "Proc. Annu. Conf. Int. Speech"
    },
    {
      "citation_id": "21",
      "title": "An automated assessment framework for atypical prosody and stereotyped idiosyncratic phrases related to autism spectrum disorder",
      "authors": [
        "M Li"
      ],
      "year": "2019",
      "venue": "Comput. Speech Lang"
    },
    {
      "citation_id": "22",
      "title": "Computer vision-based assessment of autistic children: Analyzing interactions, emotions, human pose, and life skills",
      "authors": [
        "V Prakash"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "23",
      "title": "Activity-based early autism diagnosis using a multi-dataset supervised contrastive learning approach",
      "authors": [
        "A Rani",
        "Y Verma"
      ],
      "year": "2024",
      "venue": "Proc.-IEEE Winter Conf. Appl. Comput. Vis. (WACV)"
    },
    {
      "citation_id": "24",
      "title": "Identifying children with autism spectrum disorder via transformer-based representation learning from dynamic facial cues",
      "authors": [
        "C Xia",
        "H Chen",
        "J Han",
        "D Zhang",
        "K Li"
      ],
      "year": "2025",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "25",
      "title": "Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis",
      "authors": [
        "C Wu",
        "J Lei",
        "Q Zheng",
        "W Zhao",
        "W Lin",
        "X Zhang",
        "X Zhou",
        "Z Zhao",
        "Y Zhang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis",
      "arxiv": "arXiv:2310.09909"
    },
    {
      "citation_id": "26",
      "title": "A generalist vision-language foundation model for diverse biomedical tasks",
      "authors": [
        "K Zhang",
        "R Zhou",
        "E Adhikarla",
        "Z Yan",
        "Y Liu",
        "J Yu",
        "Z Liu",
        "X Chen",
        "B Davison",
        "H Ren"
      ],
      "year": "2024",
      "venue": "Nature Medicine"
    },
    {
      "citation_id": "27",
      "title": "Large language models to identify social determinants of health in electronic health records",
      "authors": [
        "M Guevara",
        "S Chen",
        "S Thomas",
        "T Chaunzwa",
        "I Franco",
        "B Kann",
        "S Moningi",
        "J Qian",
        "M Goldstein",
        "S Harper"
      ],
      "year": "2024",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "28",
      "title": "A survey of large language models in medicine: Progress, application, and challenge",
      "authors": [
        "H Zhou"
      ],
      "year": "2023",
      "venue": "A survey of large language models in medicine: Progress, application, and challenge",
      "arxiv": "arXiv:2311.05112"
    },
    {
      "citation_id": "29",
      "title": "Large language models in medicine",
      "authors": [
        "A Thirunavukarasu",
        "D Ting",
        "K Elangovan",
        "L Gutierrez",
        "T Tan",
        "D Ting"
      ],
      "year": "2023",
      "venue": "Nat. Med"
    },
    {
      "citation_id": "30",
      "title": "Large language models in medicine: The potentials and pitfalls: A narrative review",
      "authors": [
        "J Omiye",
        "H Gui",
        "S Rezaei",
        "J Zou",
        "R Daneshjou"
      ],
      "year": "2024",
      "venue": "Ann. Intern. Med"
    },
    {
      "citation_id": "31",
      "title": "Gpt-4o system card",
      "authors": [
        "A Hurst"
      ],
      "year": "2024",
      "venue": "Gpt-4o system card",
      "arxiv": "arXiv:2410.21276"
    },
    {
      "citation_id": "32",
      "title": "Claude 3.5 sonnet",
      "authors": [
        "Anthropic"
      ],
      "year": "2024",
      "venue": "Claude 3.5 sonnet"
    },
    {
      "citation_id": "33",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Dubey"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "34",
      "title": "Qwen2 technical report",
      "authors": [
        "A Yang"
      ],
      "year": "2024",
      "venue": "Qwen2 technical report",
      "arxiv": "arXiv:2407.10671"
    },
    {
      "citation_id": "35",
      "title": "",
      "authors": [
        "\" Moonshot",
        "Kimi"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "36",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "D Guo"
      ],
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "arxiv": "arXiv:2501.12948"
    },
    {
      "citation_id": "37",
      "title": "BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Proc. Int. Conf. Mach. Learn. (ICML)"
    },
    {
      "citation_id": "38",
      "title": "Video-LLaMA: An instructiontuned audio-visual language model for video understanding",
      "authors": [
        "H Zhang",
        "X Li",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Video-LLaMA: An instructiontuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "39",
      "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "authors": [
        "J White"
      ],
      "year": "2023",
      "venue": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "arxiv": "arXiv:2302.11382"
    },
    {
      "citation_id": "40",
      "title": "Emotional competence in children with autism: Diagnostic criteria and empirical evidence",
      "authors": [
        "S Begeer",
        "H Koot",
        "C Rieffe",
        "M Terwogt",
        "H Stegge"
      ],
      "year": "2008",
      "venue": "Dev. Rev"
    },
    {
      "citation_id": "41",
      "title": "The role of emotion regulation in autism spectrum disorder",
      "authors": [
        "C Mazefsky"
      ],
      "year": "2013",
      "venue": "J. Am. Acad. Child Adolesc. Psychiatr"
    },
    {
      "citation_id": "42",
      "title": "Technology-assisted emotion recognition for autism spectrum disorder (ASD) children: a systematic literature review",
      "authors": [
        "M Rashidan"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "43",
      "title": "Computer vision analysis for quantification of autism risk behaviors",
      "authors": [
        "J Hashemi"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "44",
      "title": "Visionassisted recognition of stereotype behaviors for early diagnosis of autism spectrum disorders",
      "authors": [
        "F Negin",
        "B Ozyer",
        "S Agahian",
        "S Kacdioglu",
        "G Ozyer"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "45",
      "title": "Discriminative few shot learning of facial dynamics in interview videos for autism trait classification",
      "authors": [
        "N Zhang",
        "M Ruan",
        "S Wang",
        "L Paul",
        "X Li"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "46",
      "title": "Stratification of children with autism spectrum disorder through fusion of temporal information in eye-gaze scan-paths",
      "authors": [
        "A Atyabi"
      ],
      "year": "2023",
      "venue": "ACM Trans. Knowl. Discov. Data"
    },
    {
      "citation_id": "47",
      "title": "Deep learning to interpret autism spectrum disorder behind the camera",
      "authors": [
        "S Chen",
        "M Jiang",
        "Q Zhao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "48",
      "title": "Classifying asd children with lstm based on raw videos",
      "authors": [
        "J Li",
        "Y Zhong",
        "J Han",
        "G Ouyang",
        "X Li",
        "H Liu"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "49",
      "title": "Detecting a child's stimming behaviours for autism spectrum disorder diagnosis using rgbpose-slowfast network",
      "authors": [
        "D Pandian",
        "S Rajagopalan",
        "D Jayagopi"
      ],
      "year": "2022",
      "venue": "Proc. Int. Conf. Image Process. (ICIP)"
    },
    {
      "citation_id": "50",
      "title": "Vision-based activity recognition in children with autismrelated behaviors",
      "authors": [
        "P Wei",
        "D Ahmedt-Aristizabal",
        "H Gammulle",
        "S Denman",
        "M Armin"
      ],
      "year": "2023",
      "venue": "Heliyon"
    },
    {
      "citation_id": "51",
      "title": "Large language models encode clinical knowledge",
      "authors": [
        "K Singhal"
      ],
      "year": "2023",
      "venue": "Nature"
    },
    {
      "citation_id": "52",
      "title": "Huatuo: Tuning llama model with chinese medical knowledge",
      "authors": [
        "H Wang"
      ],
      "year": "2023",
      "venue": "Huatuo: Tuning llama model with chinese medical knowledge",
      "arxiv": "arXiv:2304.06975"
    },
    {
      "citation_id": "53",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "A Zeng"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "arxiv": "arXiv:2210.02414"
    },
    {
      "citation_id": "54",
      "title": "Can generalist foundation models outcompete specialpurpose tuning? Case study in medicine",
      "authors": [
        "H Nori"
      ],
      "year": "2022",
      "venue": "Can generalist foundation models outcompete specialpurpose tuning? Case study in medicine",
      "arxiv": "arXiv:2311.16452"
    },
    {
      "citation_id": "55",
      "title": "Hear me, see me, understand me: Audio-visual autism behavior recognition",
      "authors": [
        "S Deng"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multimedia",
      "doi": "10.1109/TMM.2024.3521838"
    },
    {
      "citation_id": "56",
      "title": "Physiological detection of affective states in children with autism spectrum disorder",
      "authors": [
        "S Sarabadani",
        "L Schudlo",
        "A Samadani",
        "A Kushski"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "57",
      "title": "Effects of computerized emotional training on children with high functioning autism",
      "authors": [
        "S Piana",
        "C Malagoli",
        "M Usai",
        "A Camurri"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "58",
      "title": "Stimuli video quantification based on valencearousal elicitation in children with autism spectrum disorder (asd)",
      "authors": [
        "M Rashidan"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "59",
      "title": "Solov2: Dynamic and fast instance segmentation",
      "authors": [
        "X Wang",
        "R Zhang",
        "T Kong",
        "L Li",
        "C Shen"
      ],
      "year": "2020",
      "venue": "Adv. neural inf. proces. syst. (NeurIPS)"
    },
    {
      "citation_id": "60",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "61",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "J Deng",
        "J Guo",
        "N Xue",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "62",
      "title": "Bag of tricks and a strong baseline for deep person re-identification",
      "authors": [
        "H Luo",
        "Y Gu",
        "X Liao",
        "S Lai",
        "W Jiang"
      ],
      "year": "2019",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops (CVPRW)"
    },
    {
      "citation_id": "63",
      "title": "Accurate head pose estimation using image rectification and a lightweight convolutional neural network",
      "authors": [
        "X Li",
        "D Zhang",
        "M Li",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "64",
      "title": "Joint training on multiple datasets with inconsistent labeling criteria for facial expression recognition",
      "authors": [
        "C Yu",
        "D Zhang",
        "W Zou",
        "M Li"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "65",
      "title": "Deep high-resolution representation learning for human pose estimation",
      "authors": [
        "K Sun",
        "B Xiao",
        "D Liu",
        "J Wang"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "66",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "67",
      "title": "The Kaldi speech recognition toolkit",
      "authors": [
        "D Povey"
      ],
      "year": "2011",
      "venue": "IEEE Workshop Autom. Speech Recogn. Underst. (ASRU)"
    },
    {
      "citation_id": "68",
      "title": "Failure to respond to name is indicator of possible autism spectrum disorder",
      "authors": [
        "J Frohna"
      ],
      "year": "2007",
      "venue": "J. Pediatrics"
    },
    {
      "citation_id": "69",
      "title": "Application of the still-face paradigm in early screening for high-risk autism spectrum disorder in infants and toddlers",
      "authors": [
        "N Qiu"
      ],
      "year": "2020",
      "venue": "Front. Pediatr"
    },
    {
      "citation_id": "70",
      "title": "Early identification of autism spectrum disorders",
      "authors": [
        "L Zwaigenbaum",
        "S Bryson",
        "N Garon"
      ],
      "year": "2013",
      "venue": "Behav. Brain Res"
    },
    {
      "citation_id": "71",
      "title": "Brief report: Preliminary feasibility of the tedi: A novel parent-administered telehealth assessment for autism spectrum disorder symptoms in the first year of life",
      "authors": [
        "M Talbott"
      ],
      "year": "2020",
      "venue": "J. Autism Dev. Disord"
    },
    {
      "citation_id": "72",
      "title": "Early social communication scales (ESCS)",
      "authors": [
        "P Mundy",
        "C Delgado",
        "J Block",
        "M Venezia",
        "A Hogan",
        "J Seibert"
      ],
      "year": "2003",
      "venue": "Early social communication scales (ESCS)"
    },
    {
      "citation_id": "73",
      "title": "Hed: A computational model of affective adaptation and emotion dynamics",
      "authors": [
        "J Steephen"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "74",
      "title": "A multivariate statistical model for emotion dynamics",
      "authors": [
        "T Krone",
        "C Albers",
        "P Kuppens",
        "M Timmerman"
      ],
      "year": "2018",
      "venue": "Emotion"
    },
    {
      "citation_id": "75",
      "title": "Daily life affective dynamics as transdiagnostic predictors of mental health symptoms: An ecological momentary assessment study",
      "authors": [
        "X Zhu"
      ],
      "year": "2024",
      "venue": "J. Affect. Disord"
    },
    {
      "citation_id": "76",
      "title": "Video-xl: Extra-long vision language model for hour-scale video understanding",
      "authors": [
        "Y Shu"
      ],
      "year": "2024",
      "venue": "Video-xl: Extra-long vision language model for hour-scale video understanding",
      "arxiv": "arXiv:2409.14485"
    },
    {
      "citation_id": "77",
      "title": "Qwen2.5-vl technical report",
      "authors": [
        "S Bai"
      ],
      "year": "2025",
      "venue": "Qwen2.5-vl technical report",
      "arxiv": "arXiv:2502.13923"
    },
    {
      "citation_id": "78",
      "title": "video-salmonn: Speech-enhanced audio-visual large language models",
      "authors": [
        "G Sun",
        "W Yu",
        "C Tang",
        "X Chen",
        "T Tan",
        "W Li",
        "L Lu",
        "Z Ma",
        "Y Wang",
        "C Zhang"
      ],
      "year": "2024",
      "venue": "video-salmonn: Speech-enhanced audio-visual large language models",
      "arxiv": "arXiv:2406.15704"
    },
    {
      "citation_id": "79",
      "title": "Assessing the social skills of children with autism spectrum disorder via language-image pre-training models",
      "authors": [
        "W Liu"
      ],
      "year": "2023",
      "venue": "Lect. Notes Comput. Sci. (PRCV)"
    },
    {
      "citation_id": "80",
      "title": "Medagents: Large language models as collaborators for zero-shot medical reasoning",
      "authors": [
        "X Tang"
      ],
      "year": "2024",
      "venue": "Find. Assoc. Comput Linguist. (ACL)"
    },
    {
      "citation_id": "81",
      "title": "The relation between valence and arousal in subjective experience",
      "authors": [
        "P Kuppens",
        "F Tuerlinckx",
        "J Russell",
        "L Barrett"
      ],
      "year": "2013",
      "venue": "Psychol. Bull"
    },
    {
      "citation_id": "82",
      "title": "Wenxing Liu received the master's degree in computer science from the Chongqing University of Technology. He is currently working toward the PhD degree in computer science with Wuhan University",
      "authors": [
        "A Tseng"
      ],
      "year": "2013",
      "venue": "His research interests include ASD diagnosis, ASD assessment, and multimodal large language model"
    }
  ]
}