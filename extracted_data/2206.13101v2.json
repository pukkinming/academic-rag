{
  "paper_id": "2206.13101v2",
  "title": "Speecheq: Speech Emotion Recognition Based On Multi-Scale Unified Datasets And Multitask Learning",
  "published": "2022-06-27T08:11:54Z",
  "authors": [
    "Zuheng Kang",
    "Junqing Peng",
    "Jianzong Wang",
    "Jing Xiao"
  ],
  "keywords": [
    "{kangzuheng896",
    "pengjq",
    "wangjianzong347",
    "xiaojing661}@pingan.com.cn Speech Emotion Recognition",
    "Multi-task Learning",
    "Phoneme Recognition",
    "Gender Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) has many challenges, but one of the main challenges is that each framework does not have a unified standard. In this paper, we propose SpeechEQ, a framework for unifying SER tasks based on a multi-scale unified metric. This metric can be trained by Multitask Learning (MTL), which includes two emotion recognition tasks of Emotion States Category (EIS) and Emotion Intensity Scale (EIS), and two auxiliary tasks of phoneme recognition and gender recognition. For this framework, we build a Mandarin SER dataset -SpeechEQ Dataset (SEQD). We conducted experiments on the public CASIA and ESD datasets in Mandarin, which exhibit that our method outperforms baseline methods by a relatively large margin, yielding 8.0% and 6.5% improvement in accuracy respectively. Additional experiments on IEMOCAP with four emotion categories (i.e., angry, happy, sad, and neutral) also show the proposed method achieves a state-of-the-art of both weighted accuracy (WA) of 78.16% and unweighted accuracy (UA) of 77.47%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions, an \"implicit channel\" that transmits explicit messages  [1] , play a vital role in human communication. Speech is an important carrier of emotion and the easiest way to record information completely. Modern people usually communicate by phone or through APPs. Unfortunately, existing technologies lack standards for speech emotion information. If designed a unified SER framework that automatically recognizes the human emotional state categories (ESC), such as happy, sad, anger, and its emotional intensity scales (EIS) expressed in natural speech, the intelligence systems can better perceive human thoughts and have better interactivity.\n\nRecently, deep-learning-based approaches show great performance in extracting hidden speech information, including age  [2] , facial expression  [3]  and emotion, etc.\n\nThe improvement of network architecture with recurrent neural networks (RNN) and attention mechanism  [4, 5] , or analyzing and fusing the textual modal information  [6, 7]  has demonstrated impressive results in solving SER problem. However, since emotion is a complex psychological process, a single task may not be sufficient to capture complicated emotional features.\n\nUsing a transfer model for different tasks can also improve accuracy. Because it essentially uses external knowledge to make the model more robust. To capture emotional information, researchers combine the power of automatic speech recognition (ASR) with natural language processing (NLP) into the SER network structure to improve model performance  [8, 9] . However, their solutions are to fuse the information of different tasks in the last few layers (high-level features) of the network, which makes the information of different tasks difficult to communicate with each other. In fact, the information of different tasks is intrinsically correlated with each other. In SER, the speed and the way of speaking will affect the expression of human emotions. Therefore, a unified model will comprehensively consider these factors to give a reasonable inference.\n\nMulti-task learning (MTL) uses a shared backbone model to simultaneously optimize multiple objectives in different tasks. The advantage comes from side information and crossregularization of different tasks. At the same time, joint optimization also brings challenges  [10] . In SER, MTL provides an idea for the model to learn multimodal information simultaneously  [9, 11, 12] . However, these methods use only one dataset for training and testing, which leads to a lack of model generalization.\n\nSelf-supervised learning utilizes multiple datasets to generate high-quality speech features  [13, 14]  for SER. However, this method does not fully exploit hidden information in these datasets.\n\nThe released SER datasets have: (1) content-dependent (such as IEMOCAP  [15] ) or content independent (such as RAVDESS  [16] , ESD  [17] , CREMA-D  [18] ); (2) contain EIS (such as IEMOCAP, CREMA-D, RAVDESS) or not (CASIA  [19] ); (3) lack a unified standard for the ESC in each dataset.\n\nTo find this unifying criterion, we took inspiration from psychologists' theories of emotion. One of the most wellknown theories is the emotional wheel proposed by Psychologist Robert Plutchik. He theorized 8 emotions with 24 \"primary\", \"secondary\", and \"tertiary\" dyads  [20] . Later, E. Cambria et al.  [21]  updated his theory and created the hourglass of emotions -he divided emotion into 4 dimensions, but reversed the position of each emotion, such that the intensity of emotion on each dimension could be a numerical value. These theories give us an idea to create a calculable metric for speech emotions. And the main contributions of our works are as follows:\n\n• This paper proposed SpeechEQ, using a multi-scale unified metric, SpeechEQ Metric (SEQM), that unifies all frameworks. It can be trained with an MTL framework to simultaneously perform two emotion recognition tasks, ESC and EIS, and two auxiliary tasks of phoneme recognition and gender recognition;\n\n• We build a Mandarin SER dataset -SpeechEQ dataset (SEQD) to demonstrate that using this metric, the emotion recognition accuracy of each Mandarin SER dataset can be improved;\n\n• The effectiveness of this method is also verified on the public English SER dataset IEMOCAP;  In modeling, since emotion categories are independent of each other, but the discrimination of emotional intensity is vague, sometimes indistinguishable, our proposed model will classify emotions into 9 categories and regress emotional intensity. To create the gold label for training, if the EIS is divided into 3 levels (i.e., low, medium, high), the gold label will be set to 1.5, 2.5, and 3.5 (neutral is 0). If with 2 levels, set them to 2 and 3 respectively. If emotional intensity has been recorded in other forms, rescale to the range of 1 to 4.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speecheq Dataset",
      "text": "To test the effectiveness of the SEQM, we constructed the SpeechEQ Dataset (SEQD), a content-independent Mandarin SER dataset, to help us improve the overall model performance in Mandarin SER tasks. This dataset contains a total of 2.3 hours of speech, 1648 audio clips from 20 speakers (10 males and 10 females) at a sampling rate of 16kHz and a precision of 16 bits. It was recorded with a Huawei phone in a mediumsized conference room at a signal-to-noise ratio (SNR) of about 20dB. The speaker is about 20cm from the microphone, sitting in the center of the room. See Table  1  for details (Neutral is 73). To build this dataset, each speaker independently wrote 3 to 5 sentences describing the emotion for each of the 25 emotions in the SEQM and performed it independently in the tone of talking to someone in everyday conversation. Then, each utterance was judged independently by three judges. If more than one judge judges the recorded speech emotion to be inaccurate, the speaker is notified to re-record until all three judges agree that the emotion is matched.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Structure",
      "text": "For the speech feature extraction, the structure of ECAPA-TDNN  [22]  has efficient design structures such as Res2Net  [23]  and Squeeze Excitation blocks (SE)  [24] . However, it is originally for the speaker recognition task, which results in a smaller receptive field. SER requires more frames to understand contextual information for more comprehensive reasoning, which needs a larger receptive field.  In Figure  2 , in the backbone model, all structures are the same with ECAPA except Res-BiGRU (the residual structure of bidirectional GRU) in the red background, which has been added to SE-Res2Block that extends the receptive field. The backend model is changed to the structure of MTL. In our SER framework, in addition to predicting the category and intensity of emotions, the model also predicts the phonemes in speech and the gender of the speaker as auxiliary tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training And Inference",
      "text": "In the training phase, the generated predictions can have access to the labels via blue, red, orange, and green paths to generate loss functions respectively. For each utterance in the training set, the blue path has access to the gold phoneme sequences labels yp translated from text labels, the red path to the groundtruth gender labels yg, the orange path to the gold ESC labels ye and the green path to the gold EIS labels yeis.\n\nFor phoneme sequence prediction (blue task), the Connectionist Temporal Classification (CTC)  [25]  as the metric between predicted phoneme probability vectors and the given gold phoneme sequence labels. Assuming that the number of frames of speech features is T , the phoneme feature dimension is dp. Thus, the CTC loss for phoneme recognition task is: L [p] = CTC y p , yp , where y p = softmax (ŷp) ∈ P T ×dp\n\n(1) For classification of emotion and gender (red and organce task), the proposed classification tasks are trained with a focal loss (FL)  [26]  with a tunable parameter γ. Assume the gender and ESC feature dimension is dg (=2) and de (=9). The loss for ESC and gender recognition tasks is in Equation  2 , the sign * denotes either g for gender, or e for ESC:\n\nFor regression of EIS (green task), the Lin's Concordance Correlation Coefficient (CCC)  [27]  is used. Let L [eis] denote the loss function for EIS:\n\n, where ŷeis ∈ P\n\nWe introduce a hyper-parameter α, β, η to combine four losses into a single one L. α controls the relative importance of the CCC loss for EIS, β of CTC loss for phoneme recognition, η of classification loss for gender recognition. Finally, the model will be optimized with the following objective w.r.t θ:\n\nIn the training process, the ASR dataset, gender dataset, and emotion dataset are respectively converted to speech features and sent to the network, and their corresponding losses are calculated separately. Finally, combine these losses into one loss to optimize the entire model.\n\nHowever, in EIS tasks, some data do not have labels assigned. For this type of data, here's a label ignore mechanism for unspecified EIS labels by setting a mask, i.e., temporarily setting them to a fake value of -1 when building the gold labels.\n\nDuring training, these values can be captured and masked by replacing any label value in the mini-batch satisfied ŷeis < 0 with the predicted such that there is no error between the predicted value and the gold label value. To prevent the value from exceeding the value range, a clipping function is designed to clip the value to the range of 0 to 4, yeis = clip (ŷeis). The pseudo-code for the detailed training process is in Algorithm 1.\n\nAt inference time, two auxiliary tasks are abandoned, and only emotion recognition tasks with ESC and EIS are retained (yellow background in Figure  2 ). Next, the emotion category softmax can be replaced by the argmax operator, and select the most probable emotion class label as an output.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation",
      "text": "Since ASR data are recorded in a neutral tone at a moderate speech rate, yet the SER data contains various speech rates and intonations, we will use the package of librosa  [28]  to adjust the pitch (randomly select pitch parameter from -3 to 3) and the speed (randomly select speed parameter from 0.8 to 1.3) of each utterance during training. Additionally, all data used for training will be augmented by probabilistically adding noise with an SNR ranging from 10dB to 25dB, and reverberation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In this paper, a total of 2 parts of the experiment will be performed. The purpose of experiment 1 is to verify whether the for i th mini-batch in dataLoader do",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4:",
      "text": "prepare input speech features x (i) g , x (i) p , x (i) e 5:\n\nprepare output gold labels y (i) g , y (i) p , y (i) e , y (i) eis",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "6:",
      "text": "get predictions from multi-datasets:\n\n7: for j th element in mini-batch do",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "12:",
      "text": "if ŷ(i,j) eis < 0 then 13:\n\nend if",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "15:",
      "text": "end for 16:\n\nget FL loss for gender L [g] using Equation  2 17:\n\nget CTC loss for phoneme L [p] using Equation  1 18:\n\nget FL loss for ESC L [e] using Equation  2 19:\n\nget CCC loss for EIS L [eis] using Equation  3 20:\n\nmerge losses with L using Equation  4 21:\n\ncreate and apply gradients with optimizer:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "22:",
      "text": "update model parameters θ;",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "23:",
      "text": "end for MsUD and MTL are helpful to improve the accuracy of each dataset in Mandarin. Experiment 2 confirms that our method is also helpful to the public English dataset -IEMOCAP.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "24: End For",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Metrics",
      "text": "Multiple datasets will be used in these experiments, shown in Table  2 . The dataset for gender recognition uses the merging of all used SER datasets (training set). For phoneme and emotion recognition tasks, they are determined by different experiments. Experiment 1: For the CASIA dataset, we perform a leave-one-fold-out ten-fold cross-validation experiment. For the SEQD dataset, we give a fixed test set of 155 utterances. The ESD dataset uses its given test set (Mandarin part, except Surprise). The MsUD is built by the training part of SEQD, CASIA, ESD (Mandarin part, except Surprise). The phoneme recognition training for Mandarin uses aishell-1  [29] . The phoneme gold label is built by the given transcript translated into INITIALS and FINAL TONE3 by the package pypinyin, then converted to an integer through the phoneme dictionary (dictionary size is 202, with 21 initials, 36 finals, 5 tones, 1 silence). The test metrics for the ESC are weighted accuracy (WA), unweighted accuracy (UA), and the EIS uses the mean square error (MSE). The unweighted accuracy of important emotions (UAi) with neutral, happy, angry, and sad maybe also listed. If the number of test samples for each class is the same (WA=UA), we denote it as Acc. Finally, the average score of each cross-validation result is taken as the final performance score. For datasets that do not have EIS labels (i.e., CASIA and ESD), it is trained using the label ignore mechanism.\n\nExperiment 2: For IEMOCAP, we select 4 emotions (i.e., angry, happy+excited, sad, and neutral) -Table  2 . The experimental setup is the same as the cited system in Table  4 . We use the \"dominance\" value as its EIS, and rescale them from a range of 1 to 5, to a range of 1 to 4. Then, randomly split the train/test into a 4:1 ratio and perform a 5-fold cross-validation. The MsUD is built by important emotions of RAVDESS, CREMA-D, ESD (English part). The phoneme recognition for English uses Librispeech  [30] . Transcripts can be directly translated into phonemes as gold labels via the dictionary CMUDict  [31]  (dictionary size is 85, with 84 phonemes and 1 silence). Unspecified EIS labels are trained with the label ignore mechanism. The test metrics are the same as in Experiment 1.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Hyper-Parameters",
      "text": "For the speech feature, all datasets will be resampled to the sample rate at 16kHz and precision of 16 bits, and use the 80-dimensional Mel Filter-Banks feature extracted using the librosa package with frame size 25ms, hop size 10ms with Hamming window. The channel parameter C in the convolutional layers for the proposed network is 256. The dimension of the bottleneck in the SE-Block and the attention module is set to 128. The scale dimensions in the Res2Block are set to 8. The tunable parameter γ in FL is 10.\n\nIn the first stage, the parameters α, β, η of the combined loss are initially set to be 1, 0.1, 0.1 respectively for fast convergence of each task. All networks are optimized with the Adam optimizer  [32]  with a learning rate of 1e-4 and weight-decay of 1e-5. The mini-batch size for training is 32.\n\nIn the fine-tune stage, both of combined loss parameters β and η gradually decreased from 0.1 to 0.01. Furthermore, the learning rate decreased from 1e-4 to 1e-6.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Results",
      "text": "Experiment 1: The baseline model of CNN+RNN+Attention shares the same backbone model with  [11] . Ablation studies are performed on the SpeechEQ framework with or without MTL or MsUD compared to the baseline model.\n\nThe results are presented in Table  3 , and the EIS value distribution of Neutral, Low, Medium and High for training and testing set of SpeechEQ are shown in the Figure  3a, 3b . Experimental results demonstrate that the feature extractor of SpeechEQ outperforms the baseline model with or without the help of MsUD and MTL. Although adding auxiliary tasks of gender recognition and phoneme recognition improves the accuracy, training models on only a single dataset have limited im-    provement. Without the use of MTL, the accuracy of the model is significantly improved after adding a MsUD. Combining the two approaches, the accuracy of the model is further improved on both models. From this result, it can be seen that training the model with either MsUD or MTL, or their combination, improves the accuracy of each dataset. Meanwhile, the regression task of EIS will also be more convergent. Experiment 2: Compares the evaluation accuracy of training with or without MsUD and MTL, as well as other cited systems trained and tested only with IEMOCAP (results copied directly from the reference) in Table  4 . It is proved that the SpeechEQ framework is also applicable to the English dataset: using MsUD (with external data and labels) and MTL can significantly improve the ESC accuracy of the model, and further improve the state-of-the-art results by about 2%. At the same time, the EIS regression task is more convergent.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose SpeechEQ, a framework for unifying emotion recognition tasks: (1) it unifies all SER frameworks with a multi-scale unified metric -SpeechEQ Metric; (2) it can be trained on an MTL framework to perform not only ESC classification and EIS regression, but also auxiliary tasks of gender recognition and phoneme recognition. Subsequently, we construct a Mandarin SER dataset -SEQD, to demonstrate the effectiveness of our method in improving model performance. Finally, two experiments demonstrate the effectiveness of our method on both Mandarin and English SER tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgement",
      "text": "",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: With this metric, multiple datasets",
      "page": 2
    },
    {
      "caption": "Figure 1: the SpeechEQ Metric for SER.",
      "page": 2
    },
    {
      "caption": "Figure 2: Network topology of the SpeechEQ framework.",
      "page": 2
    },
    {
      "caption": "Figure 2: , in the backbone model, all structures are the",
      "page": 2
    },
    {
      "caption": "Figure 2: ). Next, the emotion category",
      "page": 3
    },
    {
      "caption": "Figure 3: EIS value distributions of Neutral, Low, Medium and",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Ablation study of the proposed model on Mandarin Figure3: EISvaluedistributionsofNeutral,Low,Mediumand",
      "data": [
        {
          "Experiment": "",
          "CASIA": "Acc",
          "ESD": "Acc",
          "SEQD": "WA\nUA\nUAi\nMSE"
        },
        {
          "Experiment": "Baseline\nw/o MTL w/o MsUD",
          "CASIA": "93.27%\n88.52%",
          "ESD": "89.43%\n86.76%",
          "SEQD": "55.83%\n53.87%\n64.24%\n1.04\n46.71%\n45.04%\n52.64%\n0.98"
        },
        {
          "Experiment": "SpeechEQ\nw/o MTL\nw/o MsUD\nw/o MTL w/o MsUD",
          "CASIA": "95.32%\n93.15%\n92.58%",
          "ESD": "96.45% 93.25% 66.14% 65.25% 86.91% 0.68\n92.33%\n90.46%\n89.67%",
          "SEQD": "0.67\n62.47%\n61.56%\n85.45%\n52.24%\n51.30%\n67.20%\n0.73\n51.18%\n50.16%\n55.01%\n0.71"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "3",
      "title": "Towards speaker age estimation with label distribution learning",
      "authors": [
        "S Si",
        "J Wang",
        "J Peng",
        "J Xiao"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Speech2video: Cross-modal distillation for speech to video generation",
      "authors": [
        "S Si",
        "J Wang",
        "X Qu",
        "N Cheng",
        "W Wei",
        "X Zhu",
        "J Xiao"
      ],
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using recurrent neural networks with directional selfattention",
      "authors": [
        "D Li",
        "J Liu",
        "Z Yang",
        "L Sun",
        "Z Wang"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "6",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "A multimodal hierarchical approach to speech emotion recognition from audio and text",
      "authors": [
        "P Singh",
        "R Srivastava",
        "K Rana",
        "V Kumar"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "9",
      "title": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "authors": [
        "S Padi",
        "S Sadjadi",
        "D Manocha",
        "R Sriram"
      ],
      "year": "2022",
      "venue": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "arxiv": "arXiv:2202.08974"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "IEEE Conference of the International Speech Communication Association (IN-TERSPEECH)"
    },
    {
      "citation_id": "11",
      "title": "Multi-task learning with deep neural networks: A survey",
      "authors": [
        "M Crawshaw"
      ],
      "year": "2020",
      "venue": "Multi-task learning with deep neural networks: A survey",
      "arxiv": "arXiv:2009.09796"
    },
    {
      "citation_id": "12",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "IEEE Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "13",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Unsupervised learning approach to feature analysis for automatic speech emotion recognition",
      "authors": [
        "S Eskimez",
        "Z Duan",
        "W Heinzelman"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Variational information bottleneck for effective low-resource audio classification",
      "authors": [
        "S Si",
        "J Wang",
        "H Sun",
        "J Wu",
        "C Zhang",
        "X Qu",
        "N Cheng",
        "L Chen",
        "J Xiao"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "16",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "17",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "19",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "20",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge"
    },
    {
      "citation_id": "21",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "22",
      "title": "The hourglass of emotions",
      "authors": [
        "E Cambria",
        "A Livingstone",
        "A Hussain"
      ],
      "year": "2012",
      "venue": "Cognitive behavioural systems"
    },
    {
      "citation_id": "23",
      "title": "Ecapatdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "Ecapatdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "arxiv": "arXiv:2005.07143"
    },
    {
      "citation_id": "24",
      "title": "Res2net: A new multi-scale backbone architecture",
      "authors": [
        "S.-H Gao",
        "M.-M Cheng",
        "K Zhao",
        "X.-Y Zhang",
        "M.-H Yang",
        "P Torr"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "25",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "27",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE interna"
    },
    {
      "citation_id": "28",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "29",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "30",
      "title": "Aishell-1: An opensource mandarin speech corpus and a speech recognition baseline",
      "authors": [
        "H Bu",
        "J Du",
        "X Na",
        "B Wu",
        "H Zheng"
      ],
      "year": "2017",
      "venue": "the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment"
    },
    {
      "citation_id": "31",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "32",
      "title": "The cmu arctic speech databases",
      "authors": [
        "J Kominek",
        "A Black"
      ],
      "year": "2004",
      "venue": "Fifth ISCA workshop on speech synthesis"
    },
    {
      "citation_id": "33",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "34",
      "title": "Attention-enhanced connectionist temporal classification for discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Speech Communication Association (IN-TERSPEECH)"
    },
    {
      "citation_id": "35",
      "title": "Hgfm: A hierarchical grained and feature model for acoustic emotion recognition",
      "authors": [
        "Y Xu",
        "H Xu",
        "J Zou"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Head fusion: improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset",
      "authors": [
        "M Xu",
        "F Zhang",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    }
  ]
}