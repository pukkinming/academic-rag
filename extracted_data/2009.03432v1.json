{
  "paper_id": "2009.03432v1",
  "title": "Is Everything Fine, Grandma? Acoustic And Linguistic Modeling For Robust Elderly Speech Emotion Recognition",
  "published": "2020-09-07T21:19:16Z",
  "authors": [
    "Gizem Soğancıoğlu",
    "Oxana Verkholyak",
    "Heysem Kaya",
    "Dmitrii Fedotov",
    "Tobias Cadèe",
    "Albert Ali Salah",
    "Alexey Karpov"
  ],
  "keywords": [
    "speech emotion recognition",
    "human-computer interaction",
    "computational paralinguistics",
    "sentiment analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Acoustic and linguistic analysis for elderly emotion recognition is an under-studied and challenging research direction, but essential for the creation of digital assistants for the elderly, as well as unobtrusive telemonitoring of elderly in their residences for mental healthcare purposes. This paper presents our contribution to the INTERSPEECH 2020 Computational Paralinguistics Challenge (ComParE) -Elderly Emotion Sub-Challenge, which is comprised of two ternary classification tasks for arousal and valence recognition. We propose a bimodal framework, where these tasks are modeled using stateof-the-art acoustic and linguistic features, respectively. In this study, we demonstrate that exploiting task-specific dictionaries and resources can boost the performance of linguistic models, when the amount of labeled data is small. Observing a high mismatch between development and test set performances of various models, we also propose alternative training and decision fusion strategies to better estimate and improve the generalization performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "While the state-of-the-art in affective computing and paralinguistic analysis reaches new peaks, research on two subject groups, namely the children and the elderly, lags behind due to scarcity of training resources and difficulty of data collection  [1, 2] . Acoustic characteristics of these groups differ significantly from other age groups mainly found in the available datasets. Models trained on available data do not perform optimally for recognition in extreme age groups. Computational Paralinguistics Challenge (ComParE) 2020  [3, 4]  introduces a novel elderly emotion dataset, where both acoustic signals and speech transcriptions are provided.\n\nRecent ComParE challenges introduced new feature types, such as Bag-of-Audio-Words  [5]  and embeddings from Sequence-to-Sequence Deep Recurrent Neural Networks  [6] . The results of these challenges showed that ensemble systems and alternative feature representations have a great potential for advancing the state-of-the-art. However, the constituents and the combination rules of the ensemble systems must be selected with care. Kaya and colleagues previously applied the Fisher Vector (FV) encoding of acoustic Low-Level Descriptors (LLD) to several paralinguistic tasks including recognition of native language and sincerity  [7] , as well as classification of snoring types  [8]  and eating conditions  [9] . In this work, we use a similar FV encoding for the representation of acoustic features.\n\nIn affective computing and paralinguistics research, it is known that acoustic models perform well for arousal recognition, while providing poorer performance on valence recognition  [10, 11, 12] . Leveraging video and spoken content, when they are available, provides significant improvement on both valence and categorical emotion recognition performance  [12] . Based on our experiences and the relevant literature  [3, 10, 11] , we propose to leverage the linguistic modality for valence and the acoustic modality for arousal in this work. We propose modality-specific ensemble systems for arousal and valence recognition, while investigating the effectiveness of acoustic and linguistic models on both recognition tasks to support our hypothesis. For robust valence modeling, we extract a set of state-of-the-art linguistic features, including TF-IDF (Term Frequency-Inverse Document Frequency), FastText word embeddings, high-level polarity features, and dictionary-based linguistic features in German and English.\n\nThe contribution of this work is manifold. Firstly, we propose a bi-modal framework leveraging the linguistic modality for valence and the acoustic modality for arousal prediction. Secondly, we extract and experiment with a plethora of stateof-the-art acoustic and linguistic feature sets. Thirdly, we investigate strategies for fusion (at both feature and decision levels) and modeling with a high generalization power. We apply the proposed systems and strategies on the ComParE-2020 Elderly Emotion Sub-Challenge and obtain a marked improvement over the challenge test set baselines.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background On Methods",
      "text": "Our task in this work is to predict valence and arousal ratings (in Low, Medium, High levels) of spontaneous narratives (i.e. acoustic and linguistic modalities). The reader is referred to  [3]  for details of the baseline acoustic and linguistic features, as well as the challenge corpora. Here, we briefly provide the background on methods we used from the literature.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fisher Vector Representation For Acoustic Descriptors",
      "text": "The Fisher Vector (FV) encoding  [13]  is a state-of-the-art representation method for representing low level descriptors (LLD) over an image, utterance or video, firstly introduced in the computer vision domain, and successfully applied in paralinguistic analysis  [7, 8] . For the emotion estimation task, we train FVs for the acoustic modality at utterance level. FV requires a background probability model, typically a Gaussian Mixture Model (GMM), trained on LLDs of all the utterances (more on this later) from the training set. Normally, this requires the computation of the Fisher information matrix, which can be approximated in the case when diagonal covariance matrices are used with the GMM. The set of LLDs are de-correlated and projected to a lower dimension using Principal Component Analysis (PCA). Hence, the number of PCA components KP CA and GMM clusters KGMM are the hyper-parameters of the FV encoding.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sentiment Dictionaries",
      "text": "We use two sentiment dictionaries to estimate emotion from language use. These are language specific resources that contain a list of affective words with associated positive or negative scores. The SentiWS dictionary  [14]  contains 3467 German words together with the corresponding inflections and Part-of-Speech (POS) tags. Each word is assigned a single score, which is estimated using frequencies and co-occurrence statistics on a German-language corpus consisting of approximately 100M sentences. The scores are scaled to the range of [-1.0, 1.0] with +1.0 being absolutely positive and -1.0 being absolutely negative. The SentiWordNet dictionary  [15]  is based on WordNet and consists of almost 207K English words (60 times more than SentiWS) together with POS tags. Each word is assigned a positive and a negative score in the range of [0.0, 1.0].",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Supervised Classifiers",
      "text": "Due to their popularity in handling high-dimensional feature vectors (e.g. supra-segmental acoustic features), Support Vector Machines (SVMs) are used in many emotion classification systems. In our ensemble, we additionally employ Kernel Extreme Learning Machines (ELM)  [16]  and Partial Least Squares (PLS) regression  [17] , since these are fast and accurate algorithms that previously produced state-of-the-art results on several speechbased and multimodal tasks  [7, 8] . We obtain kernels from the training data for both PLS and ELM. For handling data imbalance, we employ a variant of ELM dubbed Weighted Kernel ELM (WKELM)  [18]  (and its Kernel PLS counterpart WKPLS that we introduced in [8]), which assign higher weights to minority class instances during model learning.\n\nIn addition to kernel-based classifiers, we employ Gradient Boosting Machines (GBM) in this work. GBMs are a special family of decision tree ensembles, where the K th tree is trained to predict the residual from the former K -1 trees  [19, 20] . In GBMs, tree learning is boosted with instance-wise gradient and the Hessian of the loss function.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "System Development",
      "text": "An overview of the proposed bi-modal arousal and valence recognition system is given in Figure  1 . The main idea is to leverage the strength of different modalities in each task. The details of feature extraction, classification and fusion steps are given in the following subsections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Acoustic Feature Extraction",
      "text": "We extract FV based features that were shown to be effective in former paralinguistic challenges  [7, 8] . As acoustic LLDs, we extract Mel-Frequency Cepstral Coefficients (MFCCs) 0-24 and RASTA-PLP (Perceptual Linear Prediction) cepstrum for 12 th order linear prediction, together with their temporal ∆ coefficients, making an LLD vector of 76 dimensions. The combined LLDs are PCA-projected to preserve 99.9% of the variation in the data. This keeps the dimensionality virtually unchanged (75 dimensions), but de-correlates the data. The number of GMM clusters are optimized using cross-validation (CV).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Linguistic Feature Extraction",
      "text": "Transformer language embeddings (such as BERT  [21] ) are the state-of-the-art in representing linguistic features, and are also a part of the baseline system for the given sub-challenge. However, as seen from the baseline performance, they may show unreliable results on small datasets. Therefore, we test four alternative representations: FastText, dictionary-based, high-level polarity, and TF-IDF features, respectively. We explain these features in dedicated subsections.\n\nIn addition to representations obtained from the original German transcriptions, we propose to extract features from the automatic English translations obtained via Google Translation engine  1  . Analysing English texts is advantageous, since there are more sentiment analysis resources available for English.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fasttext Embeddings",
      "text": "We use the FastText model  [22] , which is a state-of-the-art approach for character level embeddings, producing a semantic vector representation of words in a story. We use pre-trained 100-dimensional English and German word embeddings  [23] , which are trained on Common Crawl 2  , and finetune the pretrained model on our dataset. The story-level vectors are obtained by averaging the 100-dimensional vectors of each word that construct the corresponding story.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dictionary-Based Features",
      "text": "In order to obtain story-level SentiWS scores, an input text is tokenized ignoring the punctuation, and each token is looked up in the dictionary. If it is not found, the list of inflections for each word in the dictionary is checked without POS tag. Usually this results in a single score, but if multiple matches are found, a mean between the scores is accepted as a final score. The output of this process is a sequence of scores for tokens found in the dictionary. The following statistics are applied to the sequence: minimum, maximum, range, mean, sum, and numbers of positive and negative scores.\n\nFor the SentiWordNet representation, an input text is tokenized ignoring the punctuation, and each token is looked up ac-cording to its POS. It is common to see multiple matches since English is rich in homographs and SentiWordNet disambiguates between many of them. The mean score is used as a final score. If a token is not found in the dictionary, the same process is repeated for its lemma using original token's POS. The outputs of this process are two sequences containing positive and negative scores for each token found in the dictionary. Then the sequences are used to calculate two sets of the following statistics: minimum, maximum, range, mean, sum and number of instances. The tokenization, POS tagging and lemmatization were performed using the NLTK Python library  [24] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "High-Level Polarity Features",
      "text": "As polarity and subjectivity features, we use available sentiment analysis tools, namely NLTK Vader  [25] , TextBlob  [26]  and Flair  [27] . Each of these libraries have some strengths and drawbacks in assessing the sentiment of the sentences. For instance, TextBlob is based on a simple pattern analyzer logic and fails to take negation into account in the sentence. However, alongside polarity prediction, it applies subjectivity analysis, which can be considered as a good feature for the valence dimension. Vader is good at handling negation thanks to some heuristics, but performs weakly on unseen words. Flair, which is based on a character-level Long Short-Term Memory (LSTM) network, is good at assessing polarity of unseen words (such as those that result from typos).\n\nTo benefit from the strengths of each approach, we use the predicted polarity and subjectivity probability scores of each library as high-level features for our model. Since English is the common supported language among these tools, we use English transcripts (machine translated from German) to extract these features. Although these methods are designed to work on sentence-level or shorter length of text, we applied them at the story-level yet still obtain a good performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Tf-Idf Model",
      "text": "We extract Term Frequency-Inverse Document Frequency (TF-IDF) representation as an additional linguistic feature. This representation is commonly used in natural language processing  [28] , information retrieval  [29]  and text mining  [30]  tasks. As a vital step of using lexical level features, we apply standard pre-processing methods, such as removal of stop words by using the English/German stop words dictionaries available in the NLTK library  [24] , and stemming by the Porter stemmer  [31] . Afterwards, TF-IDF weights are computed over the set of unigrams and bi-grams.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Model Training And Optimization Strategy",
      "text": "A typical training strategy under the challenge protocol is as follows. We train the models on the challenge training set, optimize the hyper-parameters and the feature types on the development set. Once an optimal setting is found, we combine both and re-train with the optimal setting. Generalization to the sequestered test set depends on many factors, and is difficult to predict. To overcome this problem, we propose to use N-Fold cross-validation (CV) to generate N learners and to fuse their decisions for the test set. This effectively increases the training data, shows the model's performance on the entire set of annotated data points and also reduces the test set error via combining multiple learners. To estimate the real-life (in our case challenge test set) performance, we propose to use a nested N-Fold CV.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Label Fusion Strategies",
      "text": "For fusing class labels, it is common to use majority voting of three or more models. We extend this approach, mainly for breaking ties, and we propose a rule based strategy for fusing two prediction sets. The proposed tie break mechanisms benefit from our domain knowledge, namely the ordinal structure of the target variables and the class distribution.\n\nThe tie break mechanism for three models in a ternary classification uses an idea that in such a tie all three levels are predicted by base models, thus finds the middle way, outputting the medium class. In the case a higher number (> 3) of models are combined, it favors the minority class(es). The tie break mechanism for two models further considers the development set confusion matrices to infer the bias towards majority and minority classes of the two models. A rule set used for combining a prediction set favoring the majority class (P ) and a prediction set favoring the minority classes (S) is given in Algorithm 1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "The Ulm State of Mind Elderly (USOMS-e) database consists of 87 participants (55 f), who reported three narratives each. These are scored for valence and arousal, which are then grouped into three levels. For further details, see  [3] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments With Baseline Feature Sets",
      "text": "Using the conventional train-to-development setting, we obtained the best development set arousal Unweighted Average Recall (UAR) of 44.4% using PCA reduced auDeep-60 features  [6]  with an SVM classifier that yielded a slightly lower test set UAR score of 42.7%. It should be noted that the best test set arousal performance (UAR=50.4%) is obtained using Deep-Spectrum ResNet50  [32]  features, where the development set UAR was 35.0%. Using the baseline feature sets, our best development set valence UAR=56.1% was obtained with the combined BERT  [21]  feature dubbed 'BLAtt' and German POS tags (BLAtt+POS) features modeled with a GBM classifier. This linguistic system yielded a test set score of 42.8%. We attribute this performance mismatch partly to the model optimization/training procedure as discussed in Section 3.3. Hereafter, we follow our pipeline using the proposed features with 4-Fold CV for model training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments With Fv Representation",
      "text": "We carried out FV feature extraction using a number of GMM components, KGMM ∈ {16, 32, 64, 128}. Although there was one annotation for each story, the challenge utterances were provided as chunks of 5 seconds, particularly for acoustic modeling. We hypothesized that summarizing the LLDs over each story and carrying out direct classification on story-wise FVs would yield a better performance than representing chunks and then carrying out decision fusion over each development/test set story. This hypothesis was tested on the training/development set using KGMM =64. We observed that this approach boosted both arousal (from 42.7% to 48.2%) and valence (from 45.8% to 51.0%) UAR performance on the development set. Thus, we conducted remaining experiments using story-wise FVs.\n\nNoting a small number of story-wise instances (87 train + 87 dev = 174), we preferred smaller KGMM if a similar performance is obtained. Using FV representation with 16 GMM components, we obtained a 4-Fold CV overall UAR score of 48.7% (KELM, linear kernel) for arousal and 52.0% for valence (KPLS, linear kernel). We further reduced the dimensionality of FV using PCA and obtained the best UAR performance with 150 and 160 PCA dimensions for arousal (50.1% with KELM, 45.1% with WKELM) and valence (55.3% with KPLS, 53.8% with WKPLS), respectively. The corresponding weighted score fusion gave a 4-Fold CV UAR scores of 51.9% and 58.2% for arousal and valence, respectively. We used this system for our test set submission.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments With Linguistic Features",
      "text": "Using the experimental setup defined in Section 3.3, we evaluated the performance of different combinations of linguistic features, as well as combinations of the models with hard-labelbased majority voting. The linguistic models were trained using SVMs with linear, sigmoid and radial basis function kernels. As can be seen in Table  1 , each proposed linguistic approach performed significantly better than the baseline reported in the challenge paper for valence recognition  [3] . Moreover, a combination of these models at the decision level, namely the Ensemble Model, contributed to the overall performance. On the other hand, as expected, we obtained poor performance using linguistic features for arousal classification.\n\nConsidering the best UAR performances of the baseline system for valence (56% for development and 49% for test) that uses the BERT model  [21] , and those of our FastText features  [22]  (4-fold CV UAR: 46.5%), we observed that using only the complex contextual/semantic word embeddings to represent the story may be insufficient when the amount of labeled data is relatively small. But enriching those features with the knowledge learned from some external resources (such as tonal dictionaries or sentiment analysis tools) that are trained on much larger data sets can greatly improve the performance.\n\nExcept high-level polarity features, we conducted experiments for all models using both English translations and German transcripts. For FastText and TF-IDF models, using only English text gave a slightly better performance than using bilingual text. Thus, for these features, we reported the models that employ feature extraction from English text. Regarding dictionary based features, although the original text was in German, English dictionary-based features performed better than German ones, while fusion of the two followed by feature selection yielded the best performance. Brute-forcing different feature combinations resulted in the following set of optimal dictionary-based features: 2 from SentiWordNet (maximum positive and sum of negative scores) and 3 from SentiWS (minimum, maximum and number of negative scores). This small set of dictionary-based features outperformed the state-of-theart BERT features on the development set by an absolute difference of 16.2%.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "The proposed bi-modal elderly emotion recognition system shows outstanding performance on both arousal and valence recognition tasks with mean UAR=60.6%, beating the baseline mean by an absolute difference of 10.9%. Simple linguistic features proposed in this study dramatically outperform stateof-the-art BERT systems both in terms of accuracy and generalization capability that shows their advantage when using a small dataset. Moreover, they provide explainable results in contrast to \"black-box\" approaches. Linguistic modeling on automatically translated English text shows better performance than the original, because of better resources. Similar results were reported for other languages  [33] . Together with N-Fold CV experimental set-up and careful ensemble building strategy, the proposed system allows obtaining high reliability in terms of performance on a blind test set, which is not always possible with a traditional train/development split. Finally, it is shown that domain and confusion matrix awareness proves to be of considerable practical importance at the final stage of decision-level fusion. Scripts of this work can be found at https://github.com/gizemsogancioglu/elderly-emotion-SC.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "This research was supported by the Russian Science Foundation (project No. 18-11-00145).",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The main idea is to",
      "page": 2
    },
    {
      "caption": "Figure 1: The proposed pipeline for bi-modal elderly speech",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "cadeetobias@gmail.com,",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "a.a.salah@uu.nl,\nkarpov@iias.spb.su"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Abstract",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "to several paralinguistic tasks including recognition of native"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "language and sincerity [7], as well as classiﬁcation of snoring"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Acoustic and linguistic analysis\nfor elderly emotion recogni-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "types [8] and eating conditions [9]. In this work, we use a simi-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "tion is an under-studied and challenging research direction, but",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "lar FV encoding for the representation of acoustic features."
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "essential\nfor\nthe creation of digital assistants\nfor\nthe elderly,",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "In affective computing and paralinguistics\nresearch,\nit\nis"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "as well as unobtrusive telemonitoring of elderly in their\nres-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "known that acoustic models perform well for arousal recogni-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "idences\nfor mental healthcare purposes.\nThis paper presents",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "tion, while providing poorer performance on valence recogni-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "our contribution to the INTERSPEECH 2020 Computational",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "tion [10, 11, 12]. Leveraging video and spoken content, when"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Paralinguistics Challenge (ComParE)\n- Elderly Emotion Sub-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "they are available, provides\nsigniﬁcant\nimprovement on both"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Challenge, which is\ncomprised of\ntwo ternary classiﬁcation",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "valence and categorical emotion recognition performance [12]."
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "tasks\nfor arousal and valence recognition. We propose a bi-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "Based on our experiences and the relevant literature [3, 10, 11],"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "modal\nframework, where these tasks are modeled using state-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "we propose to leverage the linguistic modality for valence and"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "of-the-art acoustic and linguistic features, respectively.\nIn this",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "the acoustic modality for arousal\nin this work. We propose"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "study, we demonstrate that exploiting task-speciﬁc dictionaries",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "modality-speciﬁc\nensemble\nsystems\nfor\narousal\nand valence"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "and resources can boost\nthe performance of linguistic models,",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "recognition, while investigating the effectiveness of acoustic"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "when the amount of labeled data is small. Observing a high mis-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "and linguistic models on both recognition tasks to support our"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "match between development and test set performances of vari-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "hypothesis.\nFor\nrobust valence modeling, we\nextract\na\nset"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "ous models, we also propose alternative training and decision",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "of state-of-the-art\nlinguistic features,\nincluding TF-IDF (Term"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "fusion strategies to better estimate and improve the generaliza-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "Frequency-Inverse Document Frequency), FastText word em-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "tion performance.",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "beddings, high-level polarity features, and dictionary-based lin-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Index Terms:\nspeech emotion recognition, human-computer",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "guistic features in German and English."
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "interaction, computational paralinguistics, sentiment analysis",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "The contribution of this work is manifold. Firstly, we pro-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "pose a bi-modal\nframework leveraging the linguistic modality"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "1.\nIntroduction",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "for valence and the acoustic modality for arousal prediction."
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "Secondly, we extract and experiment with a plethora of state-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "While the state-of-the-art\nin affective computing and paralin-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "of-the-art acoustic and linguistic feature sets. Thirdly, we inves-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "guistic analysis\nreaches new peaks,\nresearch on two subject",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "tigate strategies for fusion (at both feature and decision levels)"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "groups, namely the children and the elderly,\nlags behind due",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "and modeling with a high generalization power. We apply the"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "to scarcity of\ntraining resources and difﬁculty of data collec-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "proposed systems and strategies on the ComParE-2020 Elderly"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "tion [1, 2]. Acoustic characteristics of these groups differ sig-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "Emotion Sub-Challenge and obtain a marked improvement over"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "niﬁcantly from other age groups mainly found in the available",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "the challenge test set baselines."
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "datasets. Models trained on available data do not perform op-",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "timally for recognition in extreme age groups. Computational",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "2. Background on Methods"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Paralinguistics Challenge (ComParE) 2020 [3, 4] introduces a",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "novel elderly emotion dataset, where both acoustic signals and",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "Our task in this work is to predict valence and arousal ratings"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "speech transcriptions are provided.",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "(in Low, Medium, High levels) of spontaneous narratives (i.e."
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Recent ComParE challenges introduced new feature types,",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "acoustic and linguistic modalities). The reader is referred to [3]"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "such\nas\nBag-of-Audio-Words\n[5]\nand\nembeddings\nfrom",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "for details of\nthe baseline acoustic and linguistic features, as"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Sequence-to-Sequence Deep Recurrent Neural Networks\n[6].",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "well as\nthe challenge corpora.\nHere, we brieﬂy provide the"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "The results of these challenges showed that ensemble systems",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "background on methods we used from the literature."
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "and alternative feature representations have a great potential for",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "advancing the state-of-the-art. However,\nthe constituents and",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "2.1. Fisher Vector Representation for Acoustic Descriptors"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "the combination rules of the ensemble systems must be selected",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": ""
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "with care. Kaya and colleagues previously applied the Fisher",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "The Fisher Vector (FV) encoding\n[13] is a state-of-the-art rep-"
        },
        {
          "g.sogancioglu@uu.nl,\noverkholyak@gmail.com,": "Vector (FV) encoding of acoustic Low-Level Descriptors (LLD)",
          "h.kaya@uu.nl,\ndmitrii.fedotov@uni-ulm.de,": "resentation method for representing low level descriptors (LLD)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "puter vision domain, and successfully applied in paralinguis-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "tic analysis\n[7, 8].\nFor\nthe emotion estimation task, we train"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "FVs for\nthe acoustic modality at utterance level.\nFV requires"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "a background probability model,\ntypically a Gaussian Mixture"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "Model\n(GMM),\ntrained on LLDs of all\nthe utterances\n(more"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "on this later) from the training set. Normally,\nthis requires the"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "computation of the Fisher information matrix, which can be ap-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "proximated in the case when diagonal covariance matrices are"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "used with the GMM. The set of LLDs are de-correlated and pro-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "jected to a lower dimension using Principal Component Analy-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "sis (PCA). Hence, the number of PCA components KP CA and"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "GMM clusters KGMM are the hyper-parameters of the FV en-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "coding."
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "2.2.\nSentiment Dictionaries"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "We use two sentiment dictionaries\nto estimate emotion from"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "language use. These are language speciﬁc resources that con-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "tain a list of affective words with associated positive or negative"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "scores.\nThe SentiWS dictionary [14] contains 3467 German"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "words together with the corresponding inﬂections and Part-of-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "Speech (POS) tags. Each word is assigned a single score, which"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "is estimated using frequencies and co-occurrence statistics on"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "a German-language corpus consisting of approximately 100M"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "sentences. The scores are scaled to the range of [-1.0, 1.0] with"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "+1.0 being absolutely positive and -1.0 being absolutely neg-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "ative. The SentiWordNet dictionary [15] is based on WordNet"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "and consists of almost 207K English words (60 times more than"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "SentiWS) together with POS tags. Each word is assigned a pos-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "itive and a negative score in the range of [0.0, 1.0]."
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "2.3.\nSupervised Classiﬁers"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "Due to their popularity in handling high-dimensional\nfeature"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "vectors (e.g. supra-segmental acoustic features), Support Vector"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "Machines (SVMs) are used in many emotion classiﬁcation sys-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "tems. In our ensemble, we additionally employ Kernel Extreme"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "Learning Machines (ELM) [16] and Partial Least Squares (PLS)"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "regression [17], since these are fast and accurate algorithms that"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "previously produced state-of-the-art results on several speech-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "based and multimodal\ntasks [7, 8]. We obtain kernels from the"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "training data for both PLS and ELM. For handling data imbal-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "ance, we employ a variant of ELM dubbed Weighted Kernel"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "ELM (WKELM) [18] (and its Kernel PLS counterpart WKPLS"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "that we introduced in [8]), which assign higher weights to mi-"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "nority class instances during model learning."
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "In addition to kernel-based classiﬁers, we employ Gradient"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "Boosting Machines (GBM)\nin this work. GBMs are a special"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "family of decision tree ensembles, where the K th tree is trained"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "to predict the residual from the former K − 1 trees [19, 20]. In"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "GBMs, tree learning is boosted with instance-wise gradient and"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "the Hessian of the loss function."
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "3.\nSystem Development"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "An overview of\nthe proposed bi-modal\narousal\nand valence"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "recognition system is given in Figure 1.\nThe main idea is to"
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": ""
        },
        {
          "over an image, utterance or video, ﬁrstly introduced in the com-": "leverage the strength of different modalities in each task. The"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "stance, TextBlob is based on a simple pattern analyzer\nlogic",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "and fails to take negation into account\nin the sentence. How-",
          "favoring the minority classes (S) is given in Algorithm 1.": "Algorithm 1 Fusion of two prediction sets {Pi, Si} into {Oi},"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "it applies subjectivity anal-",
          "favoring the minority classes (S) is given in Algorithm 1.": "i=1. . . N, where the class labels {‘L’, ‘M’, ‘H’} are ordinal."
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "ysis, which can be considered as a good feature for the valence",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "if Pi = Si then"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "dimension. Vader is good at handling negation thanks to some",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "Oi ← Pi"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "heuristics, but performs weakly on unseen words. Flair, which",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "else if Pi and Si contain opposite extreme labels then"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "is based on a character-level Long Short-Term Memory (LSTM)",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "Oi ← ‘M’"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "network, is good at assessing the polarity of unseen words (such",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "else if Si contains a minority class label then"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "Oi ← Si"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "To beneﬁt from the strengths of each approach, we use the",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "else"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "predicted polarity and subjectivity probability scores of each",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "Oi ← Pi"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "features for our model.\nSince English is",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "",
          "favoring the minority classes (S) is given in Algorithm 1.": "end if"
        },
        {
          "drawbacks in assessing the sentiment of the sentences. For in-": "the common supported language among these tools, we use En-",
          "favoring the minority classes (S) is given in Algorithm 1.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cording to its POS. It is common to see multiple matches since": "English is rich in homographs and SentiWordNet disambiguates",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "For fusing class labels,\nit\nis common to use majority voting of"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "between many of them. The mean score is used as a ﬁnal score.",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "three or more models. We extend this approach, mainly for"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "If a token is not found in the dictionary, the same process is re-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "breaking ties, and we propose a rule based strategy for fusing"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "peated for\nits lemma using original\ntoken’s POS. The outputs",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "two prediction sets. The proposed tie break mechanisms beneﬁt"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "of this process are two sequences containing positive and neg-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "from our domain knowledge, namely the ordinal structure of the"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "ative scores for each token found in the dictionary.\nThen the",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "target variables and the class distribution."
        },
        {
          "cording to its POS. It is common to see multiple matches since": "sequences are used to calculate two sets of the following statis-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "The tie break mechanism for three models in a ternary clas-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "tics: minimum, maximum,\nrange, mean,\nsum and number of",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "siﬁcation uses an idea that\nin such a tie all\nthree levels are pre-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "instances.\nThe tokenization, POS tagging and lemmatization",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "dicted by base models, thus ﬁnds the middle way, outputting the"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "were performed using the NLTK Python library [24].",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "medium class. In the case a higher number (> 3) of models are"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "combined, it favors the minority class(es). The tie break mecha-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "3.2.3. High-level Polarity Features",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "nism for two models further considers the development set con-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "As polarity and subjectivity features, we use available senti-",
          "3.4. Proposed Label Fusion Strategies": "fusion matrices to infer the bias towards majority and minority"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "ment analysis tools, namely NLTK Vader\n[25], TextBlob [26]",
          "3.4. Proposed Label Fusion Strategies": "classes of the two models. A rule set used for combining a pre-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "and Flair [27]. Each of these libraries have some strengths and",
          "3.4. Proposed Label Fusion Strategies": "diction set favoring the majority class (P ) and a prediction set"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "drawbacks in assessing the sentiment of the sentences. For in-",
          "3.4. Proposed Label Fusion Strategies": "favoring the minority classes (S) is given in Algorithm 1."
        },
        {
          "cording to its POS. It is common to see multiple matches since": "stance, TextBlob is based on a simple pattern analyzer\nlogic",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "and fails to take negation into account\nin the sentence. How-",
          "3.4. Proposed Label Fusion Strategies": "Algorithm 1 Fusion of two prediction sets {Pi, Si} into {Oi},"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "ever, alongside polarity prediction,\nit applies subjectivity anal-",
          "3.4. Proposed Label Fusion Strategies": "i=1. . . N, where the class labels {‘L’, ‘M’, ‘H’} are ordinal."
        },
        {
          "cording to its POS. It is common to see multiple matches since": "ysis, which can be considered as a good feature for the valence",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "if Pi = Si then"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "dimension. Vader is good at handling negation thanks to some",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "Oi ← Pi"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "heuristics, but performs weakly on unseen words. Flair, which",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "else if Pi and Si contain opposite extreme labels then"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "is based on a character-level Long Short-Term Memory (LSTM)",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "Oi ← ‘M’"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "network, is good at assessing the polarity of unseen words (such",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "else if Si contains a minority class label then"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "as those that result from typos).",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "Oi ← Si"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "To beneﬁt from the strengths of each approach, we use the",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "else"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "predicted polarity and subjectivity probability scores of each",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "Oi ← Pi"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "library as high-level\nfeatures for our model.\nSince English is",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "end if"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "the common supported language among these tools, we use En-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "glish transcripts (machine translated from German)\nto extract",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "these features. Although these methods are designed to work",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "on sentence-level or shorter length of text, we applied them at",
          "3.4. Proposed Label Fusion Strategies": "4. Experimental Results"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "the story-level yet still obtain a good performance.",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "The Ulm State of Mind Elderly (USOMS-e) database\ncon-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "sists of 87 participants\n(55 f), who reported three narratives"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "3.2.4.\nTF-IDF Model",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "each. These are scored for valence and arousal, which are then"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "grouped into three levels. For further details, see [3]."
        },
        {
          "cording to its POS. It is common to see multiple matches since": "We extract Term Frequency-Inverse Document Frequency (TF-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "IDF)\nrepresentation as an additional\nlinguistic feature.\nThis",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "4.1. Experiments with Baseline Feature Sets"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "representation is commonly used in natural\nlanguage process-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "ing [28],\ninformation retrieval [29] and text mining [30] tasks.",
          "3.4. Proposed Label Fusion Strategies": "Using the\nconventional\ntrain-to-development\nsetting, we ob-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "As a vital step of using lexical level features, we apply standard",
          "3.4. Proposed Label Fusion Strategies": "tained the best development set arousal Unweighted Average"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "pre-processing methods, such as removal of stop words by us-",
          "3.4. Proposed Label Fusion Strategies": "Recall\n(UAR) of 44.4% using PCA reduced auDeep-60 fea-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "ing the English/German stop words dictionaries available in the",
          "3.4. Proposed Label Fusion Strategies": "tures [6] with an SVM classiﬁer\nthat yielded a slightly lower"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "NLTK library\n[24], and stemming by the Porter stemmer [31].",
          "3.4. Proposed Label Fusion Strategies": "test set UAR score of 42.7%. It should be noted that the best test"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "Afterwards, TF-IDF weights are computed over the set of uni-",
          "3.4. Proposed Label Fusion Strategies": "set arousal performance (UAR=50.4%) is obtained using Deep-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "grams and bi-grams.",
          "3.4. Proposed Label Fusion Strategies": "Spectrum ResNet50 [32]\nfeatures, where the development set"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "UAR was 35.0%. Using the baseline feature sets, our best de-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "3.3. Proposed Model Training and Optimization Strategy",
          "3.4. Proposed Label Fusion Strategies": "velopment set valence UAR=56.1% was obtained with the com-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "bined BERT [21] feature dubbed ‘BLAtt’ and German POS tags"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "A typical\ntraining strategy under\nthe challenge protocol\nis as",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "(BLAtt+POS)\nfeatures modeled with a GBM classiﬁer.\nThis"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "follows. We train the models on the challenge training set, op-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "linguistic system yielded a test\nset\nscore of 42.8%. We at-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "timize the hyper-parameters and the feature types on the devel-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "tribute this performance mismatch partly to the model optimiza-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "opment set. Once an optimal setting is found, we combine both",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "tion/training procedure as discussed in Section 3.3. Hereafter,"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "and re-train with the optimal setting. Generalization to the se-",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "we follow our pipeline using the proposed features with 4-Fold"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "questered test set depends on many factors, and is difﬁcult\nto",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "CV for model training."
        },
        {
          "cording to its POS. It is common to see multiple matches since": "predict. To overcome this problem, we propose to use N-Fold",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "cross-validation (CV)\nto generate N learners and to fuse their",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "",
          "3.4. Proposed Label Fusion Strategies": "4.2. Experiments with FV Representation"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "decisions for the test set. This effectively increases the training",
          "3.4. Proposed Label Fusion Strategies": ""
        },
        {
          "cording to its POS. It is common to see multiple matches since": "data,\nshows the model’s performance on the entire set of an-",
          "3.4. Proposed Label Fusion Strategies": "We carried out FV feature extraction using a number of GMM"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "notated data points and also reduces the test set error via com-",
          "3.4. Proposed Label Fusion Strategies": "components, KGMM ∈ {16, 32, 64, 128}. Although there was"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "bining multiple learners. To estimate the real-life (in our case",
          "3.4. Proposed Label Fusion Strategies": "one annotation for each story, the challenge utterances were pro-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "challenge test set) performance, we propose to use a nested N-",
          "3.4. Proposed Label Fusion Strategies": "vided as chunks of 5 seconds, particularly for acoustic model-"
        },
        {
          "cording to its POS. It is common to see multiple matches since": "Fold CV.",
          "3.4. Proposed Label Fusion Strategies": "ing. We hypothesized that summarizing the LLDs over each"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , each proposed linguistic approach",
      "data": [
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": "System"
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": "System 0"
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": "System 1"
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": "System 2"
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": "System 3"
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": ""
        },
        {
          "baseline relatively by 21.9%.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , each proposed linguistic approach",
      "data": [
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "semble Model: (1, 4, 5) for valence, and (1, 3, 4, 5) for arousal."
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "Features\nDimens.\nValence\nArousal"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "(1) TF-IDF features\n20337\n52.3\n33.8"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "(2) FastText features\n100\n46.5\n31.3"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "(3) Polarity features\n7\n57.0\n40.4"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "(4) FastText+Polarity features\n107\n60.9\n36.3"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "(5) Dictionary-based features\n5\n61.9\n34.4"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "Ensemble Model\n62.3\n-\n38.1"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "4.4. Challenge Test Set Submissions"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "A total of ﬁve test set submissions were evaluated to prevent"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "learning on the sequestered test set. A summary of the test per-"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "formances of the proposed features are given in Table 2. These"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "results are in line with our hypotheses and show that\nlinguis-"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "tic models have a high generalization ability. System 3 arousal"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "performance on the test set also shows that domain-aware rule"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "based fusion of\ntwo prediction sets given in Algorithm 1 dra-"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "matically improves the performance. Reaching an average test"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "set UAR performance of 60.6%, we outperform the challenge"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "baseline relatively by 21.9%."
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "Table 2: Test Set UAR (%) Performances"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "System\nDescription\nValence\nArousal"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "System 0\nChallenge baseline\n49.0\n50.4"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "System 1\nFV based Ensemble\n44.3\n48.8"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "System 2\nLinguistic Ensemble\n63.7\n41.2"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "63.7\n57.5\nSystem 3\nValence: Same as System 2,"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "Arousal: Rule based fusion"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "System 0 + System 1"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "5. Conclusions"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "The\nproposed\nbi-modal\nelderly\nemotion\nrecognition\nsystem"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "shows outstanding performance on both arousal and valence"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "recognition tasks with mean UAR=60.6%, beating the baseline"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "mean by an absolute difference of 10.9%.\nSimple linguistic"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "features proposed in this study dramatically outperform state-"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "of-the-art BERT systems both in terms of accuracy and gen-"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "eralization capability that shows their advantage when using a"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "small dataset. Moreover,\nthey provide explainable results\nin"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "contrast\nto “black-box” approaches.\nLinguistic modeling on"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "automatically translated English text shows better performance"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "than the original, because of better\nresources.\nSimilar\nresults"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "were reported for other languages\n[33]. Together with N-Fold"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "CV experimental\nset-up and careful ensemble building strat-"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "egy,\nthe proposed system allows obtaining high reliability in"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "terms of performance on a blind test set, which is not always"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "possible with a traditional\ntrain/development split.\nFinally,\nit"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "is shown that domain and confusion matrix awareness proves"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "to be of considerable practical\nimportance at\nthe ﬁnal\nstage"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "of decision-level\nfusion.\nScripts of\nthis work can be found at"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "https://github.com/gizemsogancioglu/elderly-emotion-SC."
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "6. Acknowledgements"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": ""
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "This research was supported by the Russian Science Foundation"
        },
        {
          "Table 1: 4-Fold CV Average UAR (%) of Linguistic Models. En-": "(project No. 18-11-00145)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "machine for imbalance learning,” Neurocomputing, vol. 101, pp."
        },
        {
          "7. References": "[1] K. Wang, Q. Zhang, and S. Liao, “A database of elderly emotional",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "229–242, 2013."
        },
        {
          "7. References": "speech,” in Proc. International Symposium on Signal Processing,",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "Biomedical Engineering and Informatics, 2013, pp. 549–553.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[19]\nJ. H. Friedman,\n“Greedy\nfunction\napproximation:\na\ngradient"
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "boosting machine,” Annals of statistics, pp. 1189–1232, 2001."
        },
        {
          "7. References": "[2] H. Kaya, A. A. Salah, A. Karpov, O. Frolova, A. Grigorev, and",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[20]\nT. Chen and C. Guestrin, “Xgboost: A scalable tree boosting sys-"
        },
        {
          "7. References": "E. Lyakso, “Emotion, age, and gender classiﬁcation in childrens",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "tem,” in Proc. 22nd ACM SIGKDD International Conference on"
        },
        {
          "7. References": "speech by humans and machines,” Computer Speech & Language,",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "Knowledge Discovery and Data Mining, 2016, pp. 785–794."
        },
        {
          "7. References": "vol. 46, pp. 268 – 283, 2017.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[21]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-"
        },
        {
          "7. References": "[3] B. W. Schuller, A. Batliner, C. Bergler, E.-M. Messner, A. Hamil-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "training of deep bidirectional\ntransformers\nfor\nlanguage under-"
        },
        {
          "7. References": "ton, S. Amiriparian, A. Baird, G. Rizos, M. Schmitt, L. Stap-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "standing,” arXiv preprint arXiv:1810.04805, 2018."
        },
        {
          "7. References": "pen, H. Baumeister, A. D. MacIntyre, and S. Hantke, “The IN-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "TERSPEECH 2020 Computational Paralinguistics Challenge: El-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[22]\nP. Bojanowski, E. Grave, A.\nJoulin,\nand T. Mikolov,\n“Enrich-"
        },
        {
          "7. References": "derly emotion, Breathing & Masks,” in INTERSPEECH, Shang-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "the\ning word vectors with subword information,” Transactions of"
        },
        {
          "7. References": "hai, China, October 2020, to appear.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "Association for Computational Linguistics, vol. 5, pp. 135–146,"
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "2017."
        },
        {
          "7. References": "[4] M. Markitantov, D. Dresvyanskiy, D. Mamontov, H. Kaya,",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "W. Minker, and A. Karpov, “Ensembling end-to-end deep models",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[23]\nE. Grave, P. Bojanowski, P. Gupta, A.\nJoulin, and T. Mikolov,"
        },
        {
          "7. References": "for computational paralinguistics tasks: ComParE 2020 Mask and",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "“Learning word vectors for 157 languages,” in Proc. International"
        },
        {
          "7. References": "Breathing Sub-challenges,” in INTERSPEECH, Shanghai, China,",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "Conference on Language Resources and Evaluation LREC-2018,"
        },
        {
          "7. References": "October 2020, to appear.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "2018."
        },
        {
          "7. References": "[5] M. Schmitt and B. Schuller, “OpenXBOW:\nintroducing the pas-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "language processing\n[24]\nS. Bird, E. Klein,\nand E. Loper, Natural"
        },
        {
          "7. References": "sau open-source crossmodal bag-of-words toolkit,” The Journal of",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "with Python:\nanalyzing text with the natural\nlanguage toolkit."
        },
        {
          "7. References": "Machine Learning Research, vol. 18, no. 1, pp. 3370–3374, 2017.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "O’Reilly Media, Inc., 2009."
        },
        {
          "7. References": "[6] M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[25] C. J. Hutto and E. Gilbert, “Vader: A parsimonious rule-based"
        },
        {
          "7. References": "B. Schuller, “Audeep: Unsupervised learning of\nrepresentations",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "model for sentiment analysis of social media text,” in Proc. 8th In-"
        },
        {
          "7. References": "from audio with deep recurrent neural networks,” The Journal of",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "ternational AAAI Conference on Weblogs and Social Media, 2014."
        },
        {
          "7. References": "Machine Learning Research, vol. 18, no. 1, p. 63406344, 2017.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[26]\nS. Loria, “Textblob documentation,” Release 0.15, vol. 2, 2018."
        },
        {
          "7. References": "[7] H. Kaya and A. Karpov, “Fusing acoustic feature representations",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[Online]. Available: https://textblob.readthedocs.io/en/dev/"
        },
        {
          "7. References": "for computational paralinguistics tasks,” in INTERSPEECH, San",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[27] A. Akbik, T. Bergmann, D. Blythe, K. Rasul, S. Schweter, and"
        },
        {
          "7. References": "Francisco, USA, 2016, pp. 2046–2050.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "R. Vollgraf, “Flair: An easy-to-use framework for state-of-the-art"
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "nlp,” in Proc. 2019 Conference of the North American Chapter of"
        },
        {
          "7. References": "[8] H. Kaya and A. A. Karpov, “Introducing weighted kernel classi-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "the Association for Computational Linguistics (Demonstrations),"
        },
        {
          "7. References": "ﬁers for handling imbalanced paralinguistic corpora: Snoring, ad-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "2019, pp. 54–59."
        },
        {
          "7. References": "dressee and cold,” in INTERSPEECH, Stockholm, Sweden, 2017,",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "pp. 3527–3531.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[28] B. Trstenjak,\nS. Mikac,\nand D. Donko,\n“KNN with TF-IDF"
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "based framework for text categorization,” Procedia Engineering,"
        },
        {
          "7. References": "[9] H. Kaya, A. A. Karpov, and A. A. Salah, “Fisher vectors with",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "vol. 69, pp. 1356–1364, 2014."
        },
        {
          "7. References": "cascaded normalization for paralinguistic analysis,”\nin INTER-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "SPEECH, Dresden, Germany, 2015, pp. 909–913.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[29] D. Hiemstra, “A probabilistic justiﬁcation for using tf× idf term"
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "weighting in information retrieval,” International Journal on Dig-"
        },
        {
          "7. References": "[10] B. Schuller, “Voice and speech analysis in search of states and",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "ital Libraries, vol. 3, no. 2, pp. 131–139, 2000."
        },
        {
          "7. References": "traits,” in Computer Analysis of Human Behavior, A. A. Salah",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "and T. Gevers, Eds.\nSpringer, 2011, pp. 227–253.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[30]\nL.-P. Jing, H.-K. Huang, and H.-B. Shi, “Improved feature selec-"
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "tion approach TFIDF in text mining,” in Proc. International Con-"
        },
        {
          "7. References": "[11] H. Kaya, D. Fedotov, D. Dresvyanskiy, M. Doyran, D. Mamontov,",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "ference on Machine Learning and Cybernetics, vol. 2.\nIEEE,"
        },
        {
          "7. References": "M. Markitantov, A. A. Akdag Salah, E. Kavcar, A. Karpov, and",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "2002, pp. 944–946."
        },
        {
          "7. References": "A. A. Salah, “Predicting depression and emotions in the cross-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "roads of cultures, para-linguistics, and non-linguistics,” in Proc.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[31] M. F. Porter et al., “An algorithm for sufﬁx stripping.” Program,"
        },
        {
          "7. References": "9th International on Audio/Visual Emotion Challenge and Work-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "vol. 14, no. 3, pp. 130–137, 1980."
        },
        {
          "7. References": "shop, ser. AVEC 19, 2019, p. 2735.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[32]\nS. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag,"
        },
        {
          "7. References": "[12] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emo-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "S. Pugachevskiy, A. Baird, and B. W. Schuller, “Snore sound clas-"
        },
        {
          "7. References": "tion recognition: Features, classiﬁcation schemes, and databases,”",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "siﬁcation using image-based deep spectrum features.” in INTER-"
        },
        {
          "7. References": "Pattern Recognition, vol. 44, no. 3, pp. 572–587, 2011.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "SPEECH, Stockholm, Sweden, 2017, pp. 3512–3516."
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "[33]\nS. Halfon, E. A. Oktay, and A. A. Salah, “Assessing affective di-"
        },
        {
          "7. References": "[13]\nF. Perronnin and C. Dance, “Fisher kernels on visual vocabularies",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "mensions of play in psychodynamic child psychotherapy via text"
        },
        {
          "7. References": "for image categorization,” in Proc. IEEE Conference on Computer",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "analysis,” in International workshop on human behavior under-"
        },
        {
          "7. References": "Vision and Pattern Recognition, Minneapolis, Minnesota, USA,,",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": "standing.\nSpringer, 2016, pp. 15–34."
        },
        {
          "7. References": "2007, pp. 1–8.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "[14] R. Remus, U. Quasthoff, and G. Heyer, “SentiWS - a publicly",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "available German-language resource for sentiment analysis,” in",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "LREC, 2010.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "[15]\nS. Baccianella, A. Esuli, and F. Sebastiani, “Sentiwordnet 3.0: an",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "enhanced lexical resource for sentiment analysis and opinion min-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "ing.” in Lrec, vol. 10, no. 2010, 2010, pp. 2200–2204.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "[16] G.-B. Huang, H. Zhou, X. Ding, and R. Zhang, “Extreme Learn-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "ing Machine for Regression and Multiclass Classiﬁcation,” IEEE",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "Transactions on Systems, Man, and Cybernetics, Part B: Cyber-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "netics, vol. 42, no. 2, pp. 513–529, 2012.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "[17] H. Wold, “Partial least squares,” in Encyclopedia of Statistical Sci-",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "ences, S. Kotz and N. L. Johnson, Eds.\nWiley New York, 1985,",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        },
        {
          "7. References": "pp. 581–591.",
          "[18] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A database of elderly emotional speech",
      "authors": [
        "K Wang",
        "Q Zhang",
        "S Liao"
      ],
      "year": "2013",
      "venue": "Proc. International Symposium on Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Emotion, age, and gender classification in childrens speech by humans and machines",
      "authors": [
        "H Kaya",
        "A Salah",
        "A Karpov",
        "O Frolova",
        "A Grigorev",
        "E Lyakso"
      ],
      "year": "2017",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "4",
      "title": "The IN-TERSPEECH 2020 Computational Paralinguistics Challenge: Elderly emotion, Breathing & Masks",
      "authors": [
        "B Schuller",
        "A Batliner",
        "C Bergler",
        "E.-M Messner",
        "A Hamilton",
        "S Amiriparian",
        "A Baird",
        "G Rizos",
        "M Schmitt",
        "L Stappen",
        "H Baumeister",
        "A Macintyre",
        "S Hantke"
      ],
      "year": "2020",
      "venue": "The IN-TERSPEECH 2020 Computational Paralinguistics Challenge: Elderly emotion, Breathing & Masks"
    },
    {
      "citation_id": "5",
      "title": "Ensembling end-to-end deep models for computational paralinguistics tasks: ComParE 2020 Mask and Breathing Sub-challenges",
      "authors": [
        "M Markitantov",
        "D Dresvyanskiy",
        "D Mamontov",
        "H Kaya",
        "W Minker",
        "A Karpov"
      ],
      "year": "2020",
      "venue": "Ensembling end-to-end deep models for computational paralinguistics tasks: ComParE 2020 Mask and Breathing Sub-challenges"
    },
    {
      "citation_id": "6",
      "title": "OpenXBOW: introducing the passau open-source crossmodal bag-of-words toolkit",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "7",
      "title": "Audeep: Unsupervised learning of representations from audio with deep recurrent neural networks",
      "authors": [
        "M Freitag",
        "S Amiriparian",
        "S Pugachevskiy",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "8",
      "title": "Fusing acoustic feature representations for computational paralinguistics tasks",
      "authors": [
        "H Kaya",
        "A Karpov"
      ],
      "year": "2016",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "9",
      "title": "Introducing weighted kernel classifiers for handling imbalanced paralinguistic corpora: Snoring, addressee and cold",
      "authors": [
        "H Kaya",
        "A Karpov"
      ],
      "year": "2017",
      "venue": "Introducing weighted kernel classifiers for handling imbalanced paralinguistic corpora: Snoring, addressee and cold"
    },
    {
      "citation_id": "10",
      "title": "Fisher vectors with cascaded normalization for paralinguistic analysis",
      "authors": [
        "H Kaya",
        "A Karpov",
        "A Salah"
      ],
      "year": "2015",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "11",
      "title": "Voice and speech analysis in search of states and traits",
      "authors": [
        "B Schuller"
      ],
      "year": "2011",
      "venue": "Computer Analysis of Human Behavior"
    },
    {
      "citation_id": "12",
      "title": "Predicting depression and emotions in the crossroads of cultures, para-linguistics, and non-linguistics",
      "authors": [
        "H Kaya",
        "D Fedotov",
        "D Dresvyanskiy",
        "M Doyran",
        "D Mamontov",
        "M Markitantov",
        "A Akdag Salah",
        "E Kavcar",
        "A Karpov",
        "A Salah"
      ],
      "year": "2019",
      "venue": "Proc. 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "13",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Fisher kernels on visual vocabularies for image categorization",
      "authors": [
        "F Perronnin",
        "C Dance"
      ],
      "year": "2007",
      "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "SentiWS -a publicly available German-language resource for sentiment analysis",
      "authors": [
        "R Remus",
        "U Quasthoff",
        "G Heyer"
      ],
      "year": "2010",
      "venue": "LREC"
    },
    {
      "citation_id": "16",
      "title": "Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining",
      "authors": [
        "S Baccianella",
        "A Esuli",
        "F Sebastiani"
      ],
      "year": "2010",
      "venue": "Lrec"
    },
    {
      "citation_id": "17",
      "title": "Extreme Learning Machine for Regression and Multiclass Classification",
      "authors": [
        "G.-B Huang",
        "H Zhou",
        "X Ding",
        "R Zhang"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics"
    },
    {
      "citation_id": "18",
      "title": "Partial least squares",
      "authors": [
        "H Wold"
      ],
      "year": "1985",
      "venue": "Encyclopedia of Statistical Sciences"
    },
    {
      "citation_id": "19",
      "title": "Weighted extreme learning machine for imbalance learning",
      "authors": [
        "W Zong",
        "G.-B Huang",
        "Y Chen"
      ],
      "year": "2013",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "20",
      "title": "Greedy function approximation: a gradient boosting machine",
      "authors": [
        "J Friedman"
      ],
      "year": "2001",
      "venue": "Annals of statistics"
    },
    {
      "citation_id": "21",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "22",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "23",
      "title": "Enriching word vectors with subword information",
      "authors": [
        "P Bojanowski",
        "E Grave",
        "A Joulin",
        "T Mikolov"
      ],
      "year": "2017",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Learning word vectors for 157 languages",
      "authors": [
        "E Grave",
        "P Bojanowski",
        "P Gupta",
        "A Joulin",
        "T Mikolov"
      ],
      "year": "2018",
      "venue": "Proc. International Conference on Language Resources and Evaluation LREC-2018"
    },
    {
      "citation_id": "25",
      "title": "Natural language processing with Python: analyzing text with the natural language toolkit",
      "authors": [
        "S Bird",
        "E Klein",
        "E Loper"
      ],
      "year": "2009",
      "venue": "Natural language processing with Python: analyzing text with the natural language toolkit"
    },
    {
      "citation_id": "26",
      "title": "Vader: A parsimonious rule-based model for sentiment analysis of social media text",
      "authors": [
        "C Hutto",
        "E Gilbert"
      ],
      "year": "2014",
      "venue": "Proc. 8th International AAAI Conference on Weblogs and Social Media"
    },
    {
      "citation_id": "27",
      "title": "Textblob documentation",
      "authors": [
        "S Loria"
      ],
      "year": "2018",
      "venue": "Textblob documentation"
    },
    {
      "citation_id": "28",
      "title": "Flair: An easy-to-use framework for state-of-the-art nlp",
      "authors": [
        "A Akbik",
        "T Bergmann",
        "D Blythe",
        "K Rasul",
        "S Schweter",
        "R Vollgraf"
      ],
      "year": "2019",
      "venue": "Proc. 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "29",
      "title": "KNN with TF-IDF based framework for text categorization",
      "authors": [
        "B Trstenjak",
        "S Mikac",
        "D Donko"
      ],
      "year": "2014",
      "venue": "Procedia Engineering"
    },
    {
      "citation_id": "30",
      "title": "A probabilistic justification for using tf× idf term weighting in information retrieval",
      "authors": [
        "D Hiemstra"
      ],
      "year": "2000",
      "venue": "International Journal on Digital Libraries"
    },
    {
      "citation_id": "31",
      "title": "Improved feature selection approach TFIDF in text mining",
      "authors": [
        "L.-P Jing",
        "H.-K Huang",
        "H.-B Shi"
      ],
      "year": "2002",
      "venue": "Proc. International Conference on Machine Learning and Cybernetics"
    },
    {
      "citation_id": "32",
      "title": "An algorithm for suffix stripping",
      "authors": [
        "M Porter"
      ],
      "year": "1980",
      "venue": "Program"
    },
    {
      "citation_id": "33",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "A Baird",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "INTER-SPEECH"
    },
    {
      "citation_id": "34",
      "title": "Assessing affective dimensions of play in psychodynamic child psychotherapy via text analysis",
      "authors": [
        "S Halfon",
        "E Oktay",
        "A Salah"
      ],
      "year": "2016",
      "venue": "International workshop on human behavior understanding"
    }
  ]
}