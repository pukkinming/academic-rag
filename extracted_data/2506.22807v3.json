{
  "paper_id": "2506.22807v3",
  "title": "Freqdgt: Frequency-Adaptive Dynamic Graph Networks With Transformer For Cross-Subject Eeg Emotion Recognition",
  "published": "2025-06-28T08:18:05Z",
  "authors": [
    "Yueyang Li",
    "Shengyu Gong",
    "Weiming Zeng",
    "Nizhuan Wang",
    "Wai Ting Siok"
  ],
  "keywords": [
    "Electroencephalography",
    "Emotion",
    "Frequency, Dynamic Graph",
    "Transformer",
    "Disentanglement"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG) serves as a reliable and objective signal for emotion recognition in affective brain-computer interfaces, offering unique advantages through its high temporal resolution and ability to capture authentic emotional states that cannot be consciously controlled. However, cross-subject generalization remains a fundamental challenge due to individual variability, cognitive traits, and emotional responses. We propose FreqDGT, a frequency-adaptive dynamic graph transformer that systematically addresses these limitations through an integrated framework. FreqDGT introduces frequency-adaptive processing (FAP) to dynamically weight emotion-relevant frequency bands based on neuroscientific evidence, employs adaptive dynamic graph learning (ADGL) to learn input-specific brain connectivity patterns, and implements multi-scale temporal disentanglement network (MTDN) that combines hierarchical temporal transformers with adversarial feature disentanglement to capture both temporal dynamics and ensure cross-subject robustness. Comprehensive experiments demonstrate that FreqDGT significantly improves cross-subject emotion recognition accuracy, confirming the effectiveness of integrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical modeling while ensuring robustness to individual differences. The code is available at https://github.com/NZWANG/FreqDGT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Electroencephalography (EEG) has become an important tool for emotion recognition in affective brain-computer interfaces (aBCIs)  [1] , offering high temporal resolution to directly decode neural correlates of emotional states in real time. This non-invasive technique uniquely captures spontaneous brain electrical activities, providing objective insights into emotional processes  [2] . However, cross-subject emotion recognition poses significant challenges for computational modeling due to the inherent complexity of emotional processing systems  [3] . Emotions are complex psychological states that arise from integrated personal experiences, physiological responses and behavioral adaptations to environmental stimuli. At the neural level, they manifest through coordinated activity spanning multiple frequency bands -from slow δ waves involved in deep processing to fast γ oscillations supporting cognitive integrationwith each frequency range making distinct yet interactive contributions to emotional experience  [4] . This complexity stems from three neurobiological challenges. The brain's functional connectivity reorganizes dynamically in response to emotional states, while individual neuroanatomical variations structurally constrain network architectures  [5] . Additionally, substantial inter-individual differences in emotional cognition and expression further compound these challenges. Together, these factors pose fundamental limitations for developing robust cross-subject emotion recognition systems  [6] .\n\nWhile current graph neural networks (GNNs) have advanced spatial modeling of EEG data by treating electrodes as nodes in brain networks, their reliance on fixed adjacency matrices limits their ability to capture the dynamic connectivity patterns that characterize emotional processing  [7] . Current temporal modeling approaches assume uniform time scales, failing to account for emotions' dual temporal nature -both momentary fluctuations and sustained patterns. Most critically, existing cross-subject generalization methods compromise emotion-discriminative features when applying simplistic domain adaptation techniques to reduce inter-subject variability  [8] . These limitations necessitate an integrated framework that concurrently addresses the frequency-specific, spatially dynamic and temporally hierarchical characteristics of emotional neural processing, while maintaining individual neurobiological signatures through robust feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer that integrates adaptive frequency processing, dynamic spatial modeling, and multi-scale temporal disentanglement within a unified framework. The main contributions of this work are:\n\n• We propose a novel mechanism that learns emotion-specific frequency importance through cross-band attention and adaptive weighting, addressing the limitation that emotional states exhibit distinct frequency signatures requiring adaptive rather than uniform processing.\n\n• We develop a multi-level adaptive dynamic graph learning approach that models emotion-dependent brain connectivity at both local and global scales. By overcoming the limitation of fixed adjacency matrices used in conventional methods, our technique accurately captures the dynamic nature of emotional brain network reorganization.\n\n• We introduce an integrated framework that combines hierarchical temporal transformers with adversarial feature disentanglement. This architecture simultaneously models multi-scale temporal dynamics of emotional responses and explicitly separates emotion-related features from subject-specific variations. The result is significantly improved cross-subject generalization while maintaining high discriminative power for emotion classification.\n\nFigure  1 : Overall framework of the FreqDGT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work 2.1 Eeg-Based Emotion Recognition",
      "text": "Early EEG-based emotion recognition approaches established the effectiveness of frequency-domain features such as relative power spectral density (rPSD) and differential entropy (DE) as emotional indicators  [3] , though these methods typically applied uniform processing across frequency bands, failing to account for dynamic variations in spectral importance across different emotional states. The field subsequently advanced with GNNs that model the brain as interconnected networks, with DGCNN pioneering learnable adjacency matrices  [2] , RGNN incorporating neurophysiological constraints  [9] , and GCB-Net enhancing representations through broad learning systems  [10] .\n\nMore recent hybrid approaches have shown particular promise, including EEG-Conformer's combination of CNN and transformer architectures for emotion and motor imagery classification  [11] , and AMDET's transformer-based attention mechanism operating across spectral-spatial-temporal dimensions  [12] . Despite these methodological innovations, current approaches continue to neglect their complex interdependencies that characterize emotional processing. Binary mask for frequency band b",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Subject Generalization",
      "text": "The inherent variability in individual neural patterns and brain structures presents significant challenges for crosssubject generalization. Early domain adaptation approaches focused on statistical alignment techniques  [8] , but these often discarded emotionally relevant information along with individual differences. Recent methodological advances have moved beyond simple alignment strategies. Meta-learning approaches such as MS-MDA develop transferable adaptation strategies to enable rapid personalization  [13] , while contrastive learning approaches such as CLISA leverage the observation that responses to identical stimuli share underlying neural structures despite individual variability, employing contrastive objectives and hyperbolic embeddings respectively to identify these commonalities  [14] . However, these approaches persist in treating inter-subject variability as noise to be minimized rather than as meaningful neurophysiological differences that could improve emotion recognition accuracy when properly modeled and incorporated.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "We introduce FreqDGT, a frequency-adaptive dynamic graph transformer designed to overcome the challenges of crosssubject EEG emotion recognition. The framework begins with frequency-adaptive processing (FAP) that proceeds by dynamically weighting emotion-relevant bands through neuroscience-informed adaptive mechanisms. These frequency features feed into adaptive dynamic graph learning (ADGL), which addresses the fixed connectivity limitation by learning emotion-dependent spatial relationships. Subsequently, multi-scale temporal disentanglement network (MTDN) that combines hierarchical temporal processing with adversarial feature disentanglement, enabling simultaneous capture of emotion-dependent brain connectivity patterns and hierarchical temporal dynamics while ensuring cross-subject generalization.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Frequency-Adaptive Processing",
      "text": "Neuroscientific evidence shows distinct emotional processing roles across different frequency bands  [4] . The FAP module overcomes the limitations of uniform band processing by implementing adaptive weighting mechanisms that dynamically prioritize bands according to their emotional relevance.\n\nThe module operates on rPSD features X ∈ R S×C×F . For each band b ∈ {δ, θ, α, β, γ}, frequency masks M b ∈ {0, 1} F isolate band-specific components through element-wise multiplication. Band energy is computed through spatial aggregation to capture distributed emotional patterns:\n\nWe recognize that emotional states emerge from complex interactions across frequency domains rather than isolated band activations. The concatenated band energies P = [P δ ; P θ ; P α ; P β ; P γ ] ∈ R S×5 are processed through two parallel pathways, cross-band attention and importance weighting.\n\nThe cross-band attention mechanism learns inter-frequency dependencies and the importance network learns adaptive band-specific weights:\n\n) where AttentionNet applies layer normalization followed by two linear transformations with GELU activation, ImporNet consists of two MLPs with GELU activation followed by sigmoid activation.\n\nFor each frequency band b, we apply frequency masking to obtain band-specific features:\n\nwhere M b broadcasts across spatial and temporal dimensions.\n\nThe final integration combines attention and importance weights through element-wise operations:\n\nwhere unsqueeze reshapes the weights to broadcast across channel and frequency dimensions, ensuring adaptive emphasis on emotion-relevant frequency patterns while maintaining signal integrity.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Adaptive Dynamic Graph Learning",
      "text": "The fixed adjacency matrices that fail to capture the dynamic emotional brain connectivity  [8] . The ADGL module addresses this limitation through a novel multi-level dynamic graph construction approach that simultaneously learns emotion-adaptive connectivity patterns at local and global granularities.\n\nGiven the frequency features Y F AP ∈ R S×C×F , we first aggregate temporal information to obtain node representations suitable for graph learning:\n\nEmotional connectivity patterns exhibit both fine-grained local interactions and abstract global relationships, necessitating multi-granularity modeling through networks of varying representational depths. The aggregated node features are mapped through parallel relation networks with shallow and deep architectures: Dynamic adjacency matrices A dynS and A dynD are constructed through pairwise similarity computation between transformed node representations Z S and Z D respectively, with softmax normalization to ensure proper probability distributions. For stable propagation, matrices are symmetrized with self-connections:\n\nwhere D dyn [i, i] = j Ãdyn [i, j] represents the degree matrix. Multi-level graph convolution with K-th order Chebyshev approximation propagates features:\n\nwhere T k ( L) denotes Chebyshev polynomials on scaled Laplacian L = 2L λmax -I and θ\n\nk are learnable parameters. Multi-level graph convolution propagates features through normalized adjacencies:\n\nwhere\n\nD ∈ R dg×dg are learnable transformations, initialized with\n\nThe final representation integrates both connectivity scales and restores temporal dimension:\n\ndynS + H\n\n(2)\n\nwhere ExpandT replicates the spatial features across the temporal dimension.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Scale Temporal Disentanglement Network",
      "text": "Effective EEG emotion recognition requires simultaneous modeling of temporal dynamics across multiple scales and robust generalization across individual differences. We address these challenges through the MTDN, which integrates hierarchical temporal processing with adversarial feature disentanglement. The MTDN captures multi-scale temporal patterns through an novel transformer architecture, then separating emotion-relevant features from subject-specific variations to ensure cross-subject robustness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Scale Temporal Transformer",
      "text": "Emotional responses manifest at multiple timescales, conventional transformers apply uniform attention across temporal dependencies, neglecting the hierarchical nature of emotional dynamics. Our multi-scale temporal transformer addresses this through scale-aware attention mechanisms that explicitly partition attention heads to model distinct temporal granularities within the transformer architecture.\n\nThe multi-scale temporal transformer processes spatio-temporal representations H ADGL ∈ R S×C×dg from the adaptive dynamic graph learning module while preserving spatial information through flattening, yielding H 0 = Flatten spatial (H ADGL ) ∈ R S×(C•dg) . Our design partitions attention heads into scale-specific groups, where each group G s operates at temporal scale s ∈ S = {1, 2, 4, 8}.\n\nFor each scale-specific attention head group, we modify the standard attention computation to incorporate temporal scale awareness. The different head groups focus on different temporal receptive fields. Scale-specific queries, keys, and values are computed through dedicated projections, and attention weights incorporate temporal scale information:\n\nwhere M s represents scale-specific attention masks that constrain attention heads to focus on their designated temporal granularities, and ⊙ denotes element-wise multiplication.\n\nTo quantify temporal pattern significance at each scale, we compute scale-specific energy measures:\n\nThese p s feed into hierarchical attention fusion that weights scale group contributions based on relevance. Scale importance weights α scale [s] are dynamically computed through learnable attention networks processing these measures. Adaptive modulation networks further enhance attention patterns within each scale, producing refined outputs O s,adapted .\n\nThe final multi-scale transformer output integrates all temporal scale representations:\n\nwhere W O is a learnable projection matrix and d h represents the unified output dimension.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Adversarial Disentanglement",
      "text": "Cross-subject generalization faces the challenge that multi-scale temporal features H M ST T ∈ R S×d h inherently contain entangled emotion-relevant information and subject-specific variations that confound model generalization. Rather than treating subject variations as noise to be eliminated, our adversarial disentanglement explicitly models these variations as structured information that can be separated from emotion-relevant features through principled feature space decomposition.\n\nThe disentanglement process begins with temporal aggregation H agg = GlobalAvgPool(H M ST T ) ∈ R d h , followed by dual encoding pathways with complementary objectives. The subject encoder captures individual-specific neural characteristics through learnable subject-dependent transformations following the subject-specific layer approach  [15] :\n\nwhere M s ∈ R ds×d h represents the subject-specific transformation matrix. The emotion encoder employs a multi-layer perceptron to extract emotion-discriminative features: We make no a priori assumptions about emotion representations, instead allowing the adversarial training mechanism to guide the encoder toward learning subject-invariant emotional patterns through competitive optimization.\n\nAdversarial training ensures orthogonal feature spaces through minimax optimization. A discriminator network C adv attempts to predict subject identity from emotion features, while the emotion encoder is simultaneously trained to fool this discriminator. This adversarial mechanism is theoretically grounded in the principle that if emotion features cannot be used to identify subjects, they must contain minimal subject-specific information, thereby achieving the desired invariance property. The FreqDGT objective integrates all components:\n\nwhere L cls represents the emotion classification loss, L adv denotes the adversarial loss for the emotion encoder, and L disc represents the discriminative loss for subject identification.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets And Experimental Protocol",
      "text": "We evaluate FreqDGT on three EEG emotion recognition datasets: SEED  [16] , FACED  [17] , and SEED-IV  [18] .\n\nSEED contains EEG recordings from 15 participants viewing 15 emotion-inducing film clips targeting positive, negative, and neutral states. Subsequently, the participants are tasked with providing self-evaluations regarding their emotional responses, considering dimensions such as valence and arousal. The 62-channel EEG signals were recorded at 1000 Hz and preprocessed with bandpass filtering.\n\nFACED collected by Tsinghua University comprises 123 healthy university students including 75 females who viewed 28 emotion-eliciting video clips averaging 67 seconds across nine categories including amusement, inspiration, joy, tenderness, anger, fear, disgust, sadness and neutral emotions. The 32-channel EEG data sampled at 250 Hz from the final 30 seconds were preprocessed using bandpass filtering.\n\nSEED-IV consists of EEG data from 15 participants (7 males and 8 females) across three sessions, with each session containing 24 trials, corresponding to 2-minute movie clips inducing four emotional states: neutral, sad, fear, and happy.\n\nWe employ subject-independent protocols to assess cross-subject generalization. We use leave-one-subject-out (LOSO) cross-validation with 80/20 train/validation split. SEED and FACED experiments perform binary emotion classification (positive vs. negative), with valence scores binarized at threshold 3.0 for FACED.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details And Metrics",
      "text": "The experiments ran on a single NVIDIA RTX 2080ti GPU. FreqDGT is implemented in PyTorch using AdamW optimizer with learning rate 5e-4 and weight decay 1e-4. The batch size is set to 128 across all datasets. The Chebyshev polynomial order K is 4. To ensure reproducibility, all random seeds are fixed across experiments.\n\nWe report accuracy (ACC) and F1-score (F1) as standard classification metrics, measuring the proportion of correct predictions and the harmonic mean of precision and recall respectively. All results are averaged across folds with standard deviations to assess model stability.  Comprehensive ablation studies in Table  3  reveal the hierarchical importance of each component. MTDN emerges as the most critical module, with its removal causing the largest performance degradation across all datasets, emphasizing that multi-scale temporal modeling and feature disentanglement are fundamental for cross-subject generalization. FAP demonstrates substantial contribution, confirming that adaptive frequency processing effectively captures emotionrelevant spectral patterns. ADGL shows consistent impact, validating that dynamic graph learning captures more emotion-relevant spatial relationships than fixed connectivity approaches. Among sub-components, adversarial training within MTDN proves most significant, indicating that explicit feature disentanglement is crucial for robust cross-subject emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Visualization Analysis Of Freqdgt",
      "text": "Figure  2  provides comprehensive insights into the neurophysiological mechanisms underlying cross-subject emotion recognition from SEED. The frequency-domain analysis reveals distinct spectral signatures between emotional states: positive emotions predominantly engage alpha and beta oscillations associated with approach behaviors and cognitive control, while negative emotions activate θ and γ rhythms linked to threat detection and arousal regulation. The cross-frequency coupling patterns demonstrate sophisticated inter-band interactions that reflect established neurobiological processes in emotional processing  [19] .\n\nThe connectivity analysis unveils emotion-specific brain network configurations that align with contemporary neuroscientific understanding. Positive emotional states exhibit robust fronto-parietal networks with left-hemispheric preference, characteristic of approach-motivated executive control systems. Conversely, negative emotions engage temporo-frontal circuits with enhanced right-hemispheric involvement, consistent with withdrawal-related neural architectures. These findings validate FreqDGT's capacity to capture biologically plausible representations of emotional brain dynamics  [20] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "We introduce FreqDGT, a frequency-adaptive dynamic graph transformer designed to overcome cross-subject EEG emotion recognition challenges through an integrated framework. By combining frequency-adaptive processing, adaptive dynamic graph learning, multi-scale temporal transformers, and adversarial disentanglement networks, FreqDGT effectively models the complex interdependencies among frequency, spatial and temporal aspects of emotional brain responses. Experimental results demonstrate significant improvements in cross-subject generalization, confirming the effectiveness of our unified approach. Future work will explore extending the framework to multimodal emotion recognition and investigating personalization strategies for practical deployment.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall framework of the FreqDGT.",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualization Analysis.",
      "page": 7
    },
    {
      "caption": "Figure 2: provides comprehensive insights into the neurophysiological mechanisms underlying cross-subject emotion",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Electroencephalography (EEG) serves as a reliable and objective signal for emotion recognition in"
        },
        {
          "ABSTRACT": "affective brain-computer interfaces, offering unique advantages through its high temporal resolution"
        },
        {
          "ABSTRACT": "and ability to capture authentic emotional states that cannot be consciously controlled. However,"
        },
        {
          "ABSTRACT": "cross-subject generalization remains a fundamental challenge due to individual variability, cognitive"
        },
        {
          "ABSTRACT": "traits, and emotional responses. We propose FreqDGT, a frequency-adaptive dynamic graph trans-"
        },
        {
          "ABSTRACT": "former that systematically addresses these limitations through an integrated framework. FreqDGT"
        },
        {
          "ABSTRACT": "introduces frequency-adaptive processing (FAP) to dynamically weight emotion-relevant frequency"
        },
        {
          "ABSTRACT": "bands based on neuroscientific evidence, employs adaptive dynamic graph learning (ADGL)"
        },
        {
          "ABSTRACT": "learn input-specific brain connectivity patterns, and implements multi-scale temporal disentangle-"
        },
        {
          "ABSTRACT": "ment network (MTDN) that combines hierarchical temporal transformers with adversarial feature"
        },
        {
          "ABSTRACT": "disentanglement\nto capture both temporal dynamics and ensure cross-subject"
        },
        {
          "ABSTRACT": "prehensive experiments demonstrate that FreqDGT significantly improves cross-subject emotion"
        },
        {
          "ABSTRACT": "recognition accuracy, confirming the effectiveness of integrating frequency-adaptive, spatial-dynamic,"
        },
        {
          "ABSTRACT": "and temporal-hierarchical modeling while ensuring robustness to individual differences. The code is"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "1\nINTRODUCTION"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "Electroencephalography (EEG) has become an important"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "interfaces (aBCIs) [1], offering high temporal resolution to directly decode neural correlates of emotional states in"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "real time. This non-invasive technique uniquely captures spontaneous brain electrical activities, providing objective"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "insights into emotional processes [2]. However, cross-subject emotion recognition poses significant challenges for"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "computational modeling due to the inherent complexity of emotional processing systems [3]. Emotions are complex"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "psychological states that arise from integrated personal experiences, physiological responses and behavioral adaptations"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "to environmental stimuli. At the neural level, they manifest through coordinated activity spanning multiple frequency"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": ""
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "with each frequency range making distinct yet interactive contributions to emotional experience [4]. This complexity"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "stems from three neurobiological challenges. The brain’s functional connectivity reorganizes dynamically in response"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": ""
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "Additionally, substantial inter-individual differences in emotional cognition and expression further compound these"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "challenges. Together, these factors pose fundamental limitations for developing robust cross-subject emotion recognition"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "systems [6]."
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "While current graph neural networks (GNNs) have advanced spatial modeling of EEG data by treating electrodes"
        },
        {
          "Keywords Electroencephalography · Emotion · Frequency, Dynamic Graph · Transformer · Disentanglement.": "as nodes in brain networks,\ntheir"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "connectivity patterns that characterize emotional processing [7]. Current temporal modeling approaches assume uniform"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": ""
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Most critically, existing cross-subject generalization methods compromise emotion-discriminative features when"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "applying simplistic domain adaptation techniques to reduce inter-subject variability [8]. These limitations necessitate an"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "integrated framework that concurrently addresses the frequency-specific, spatially dynamic and temporally hierarchical"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "characteristics of emotional neural processing, while maintaining individual neurobiological signatures through robust"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "integrates adaptive frequency processing, dynamic spatial modeling, and multi-scale temporal disentanglement within a"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "unified framework. The main contributions of this work are:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "integrates adaptive frequency processing, dynamic spatial modeling, and multi-scale temporal disentanglement within a"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "unified framework. The main contributions of this work are:"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "• We propose a novel mechanism that learns emotion-specific frequency importance through cross-band attention"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "and adaptive weighting, addressing the limitation that emotional states exhibit distinct frequency signatures"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "requiring adaptive rather than uniform processing."
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "• We develop a multi-level adaptive dynamic graph learning approach that models emotion-dependent brain"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "connectivity at both local and global scales. By overcoming the limitation of fixed adjacency matrices used"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "in conventional methods, our technique accurately captures the dynamic nature of emotional brain network"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "reorganization."
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "• We introduce an integrated framework that combines hierarchical\ntemporal\ntransformers with adversarial"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "feature disentanglement. This architecture simultaneously models multi-scale temporal dynamics of emotional"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "responses and explicitly separates emotion-related features from subject-specific variations. The result\nis"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "significantly improved cross-subject generalization while maintaining high discriminative power for emotion"
        },
        {
          "feature disentanglement. We therefore propose FreqDGT, a frequency-adaptive dynamic graph transformer\nthat": "classification."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2\nCross-Subject Generalization": "The inherent variability in individual neural patterns and brain structures presents significant challenges for cross-"
        },
        {
          "2.2\nCross-Subject Generalization": "subject generalization.\nEarly domain adaptation approaches focused on statistical alignment\ntechniques [8], but"
        },
        {
          "2.2\nCross-Subject Generalization": "these often discarded emotionally relevant\ninformation along with individual differences. Recent methodological"
        },
        {
          "2.2\nCross-Subject Generalization": "advances have moved beyond simple alignment strategies. Meta-learning approaches such as MS-MDA develop"
        },
        {
          "2.2\nCross-Subject Generalization": "transferable adaptation strategies to enable rapid personalization [13], while contrastive learning approaches such"
        },
        {
          "2.2\nCross-Subject Generalization": "as CLISA leverage the observation that\nresponses to identical stimuli share underlying neural structures despite"
        },
        {
          "2.2\nCross-Subject Generalization": "individual variability, employing contrastive objectives and hyperbolic embeddings respectively to identify these"
        },
        {
          "2.2\nCross-Subject Generalization": "commonalities [14]. However, these approaches persist in treating inter-subject variability as noise to be minimized"
        },
        {
          "2.2\nCross-Subject Generalization": "rather than as meaningful neurophysiological differences that could improve emotion recognition accuracy when"
        },
        {
          "2.2\nCross-Subject Generalization": "properly modeled and incorporated."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "For each frequency band b, we apply frequency masking to obtain band-specific features:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(4)\nXb = Mb ⊙ X ∈ RS×C×F"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "where Mb broadcasts across spatial and temporal dimensions."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "The final integration combines attention and importance weights through element-wise operations:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "5(cid:88) i\n(5)\nYF AP =\nunsqueeze(A:,i ⊙ W:,i, [1, 2]) ⊙ Xbi"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "=1"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "where unsqueeze reshapes the weights to broadcast across channel and frequency dimensions, ensuring adaptive"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "emphasis on emotion-relevant frequency patterns while maintaining signal integrity."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "3.2\nAdaptive Dynamic Graph Learning"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "The fixed adjacency matrices that fail to capture the dynamic emotional brain connectivity [8]. The ADGL module"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "addresses this limitation through a novel multi-level dynamic graph construction approach that simultaneously learns"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "emotion-adaptive connectivity patterns at local and global granularities."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Given the frequency features YF AP ∈ RS×C×F , we first aggregate temporal information to obtain node representations"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "suitable for graph learning:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Y′\n(6)\nF AP = MeanPooltime(YF AP ) ∈ RC×F"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Emotional connectivity patterns exhibit both fine-grained local interactions and abstract global relationships, necessitat-"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "ing multi-granularity modeling through networks of varying representational depths. The aggregated node features are"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "mapped through parallel relation networks with shallow and deep architectures:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(7)\nZS = RS(Y′\nF AP ) ∈ RC×dr"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(8)\nZD = RD(Y′\nF AP ) ∈ RC×dr"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "where RS(·) captures fine-grained local patterns through shallow architecture and RD(·) models coarse-grained global"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "relationships through deep transformations."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Dynamic adjacency matrices AdynS and AdynD are constructed through pairwise similarity computation between"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "transformed node representations ZS and ZD respectively, with softmax normalization to ensure proper probability"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "distributions. For stable propagation, matrices are symmetrized with self-connections:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "1 2\n(9)\nAdynS = D−1/2\n(AdynS + AT\ndynS) + I]D−1/2"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "dynS["
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "1 2\n(10)\nAdynD = D−1/2\n(AdynD + AT\ndynD) + I]D−1/2"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "dynD["
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "˜\nrepresents the degree matrix. Multi-level graph convolution with K-th order\nwhere Ddyn[i, i] = (cid:80)\nAdyn[i, j]"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "j"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Chebyshev approximation propagates features:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "K−1"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "θ(l)"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(cid:88) k\nH(l+1) =\n(11)\nk Tk(˜L)H(l)"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "=0"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "2L\n− I and θ(l)\nare learnable parameters.\nwhere Tk(˜L) denotes Chebyshev polynomials on scaled Laplacian ˜L ="
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "k\nλmax"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Multi-level graph convolution propagates features through normalized adjacencies:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "H(l)"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(12)\nS )\ndynS = ReLU( ˆAdynSH(l−1)\ndynS W(l)"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "H(l)"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(13)\nD )\ndynD = ReLU( ˆAdynDH(l−1)\ndynDW(l)"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "where W(l)"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "F AP .\nS , W(l)\nD ∈ Rdg×dg are learnable transformations, initialized with H(0)\ndynS = H(0)\ndynD = Y′"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "The final representation integrates both connectivity scales and restores temporal dimension:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(cid:18) 1"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(H(2)"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(14)\nHADGL = ExpandT\ndynS + H(2)\ndynD), S"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "2"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "where ExpandT replicates the spatial features across the temporal dimension."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "4"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "3.3\nMulti-Scale Temporal Disentanglement Network"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Effective EEG emotion recognition requires simultaneous modeling of temporal dynamics across multiple scales and"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "robust generalization across individual differences. We address these challenges through the MTDN, which integrates"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "hierarchical temporal processing with adversarial feature disentanglement. The MTDN captures multi-scale temporal"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "patterns through an novel\ntransformer architecture,\nthen separating emotion-relevant features from subject-specific"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "variations to ensure cross-subject robustness."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "3.3.1\nMulti-Scale Temporal Transformer"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Emotional responses manifest at multiple timescales, conventional transformers apply uniform attention across temporal"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "dependencies, neglecting the hierarchical nature of emotional dynamics. Our multi-scale temporal transformer addresses"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "this through scale-aware attention mechanisms that explicitly partition attention heads to model distinct\ntemporal"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "granularities within the transformer architecture."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "The multi-scale temporal\ntransformer processes\nspatio-temporal\nfrom the\nrepresentations HADGL ∈ RS×C×dg"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "adaptive dynamic graph learning module while preserving spatial\ninformation through flattening, yielding H0 ="
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Flattenspatial(HADGL) ∈ RS×(C·dg). Our design partitions attention heads into scale-specific groups, where each group"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Gs operates at temporal scale s ∈ S = {1, 2, 4, 8}."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "For each scale-specific attention head group, we modify the standard attention computation to incorporate temporal"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "scale awareness. The different head groups focus on different temporal receptive fields. Scale-specific queries, keys,"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "and values are computed through dedicated projections, and attention weights incorporate temporal scale information:"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "(cid:19)\n(cid:18) QsKT\ns ⊙ Ms"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "√\n,\n(15)\nAs = softmax\nOs = AsVs"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "dk"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "where Ms represents scale-specific attention masks that constrain attention heads to focus on their designated temporal"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "granularities, and ⊙ denotes element-wise multiplication."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "To quantify temporal pattern significance at each scale, we compute scale-specific energy measures:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "The best results are highlighted with dark gray background and the next best are marked with light gray background."
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": ""
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "Method"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": ""
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "MLP+LSTM"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "DGCNN [2]"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "GCB-Net [10]"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "CLISA [14]"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "Conformer [11]"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "RGNN [9]"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "AMDET [12]"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "PGCN [6]"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "EmT [3]"
        },
        {
          "Table 2: Generalized emotion classification results of different methods on the SEED, SEED-IV and FACED datasets.": "FreqDGT (ours)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmT [3]": "FreqDGT (ours)",
          "0.789 (0.135)": "0.811 (0.035)",
          "0.821 (0.122)": "0.819 (0.093)",
          "0.583 (0.049)": "0.623 (0.089)",
          "0.732 (0.102)": "0.761 (0.072)",
          "-": "0.726 (0.062)"
        },
        {
          "EmT [3]": "We make no a priori assumptions about emotion representations, instead allowing the adversarial training mechanism to",
          "0.789 (0.135)": "",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": ""
        },
        {
          "EmT [3]": "",
          "0.789 (0.135)": "guide the encoder toward learning subject-invariant emotional patterns through competitive optimization.",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": ""
        },
        {
          "EmT [3]": "",
          "0.789 (0.135)": "Adversarial training ensures orthogonal feature spaces through minimax optimization. A discriminator network Cadv",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": ""
        },
        {
          "EmT [3]": "",
          "0.789 (0.135)": "attempts to predict subject identity from emotion features, while the emotion encoder is simultaneously trained to fool",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": ""
        },
        {
          "EmT [3]": "",
          "0.789 (0.135)": "this discriminator. This adversarial mechanism is theoretically grounded in the principle that if emotion features cannot",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": ""
        },
        {
          "EmT [3]": "be used to identify subjects,",
          "0.789 (0.135)": "",
          "0.821 (0.122)": "they must contain minimal subject-specific information,",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": "thereby achieving the desired"
        },
        {
          "EmT [3]": "",
          "0.789 (0.135)": "invariance property. The FreqDGT objective integrates all components:",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": ""
        },
        {
          "EmT [3]": "",
          "0.789 (0.135)": "",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "Ltotal = Lcls(zemo) + λadvLadv + λdiscLdisc",
          "0.732 (0.102)": "",
          "-": "(20)"
        },
        {
          "EmT [3]": "",
          "0.789 (0.135)": "where Lcls represents the emotion classification loss, Ladv denotes the adversarial loss for the emotion encoder, and",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": ""
        },
        {
          "EmT [3]": "",
          "0.789 (0.135)": "Ldisc represents the discriminative loss for subject identification.",
          "0.821 (0.122)": "",
          "0.583 (0.049)": "",
          "0.732 (0.102)": "",
          "-": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Ablation Studies with regard to Components in FreqDGT.": "Component Settings"
        },
        {
          "Table 3: Ablation Studies with regard to Components in FreqDGT.": "w/o Cross-band Attention"
        },
        {
          "Table 3: Ablation Studies with regard to Components in FreqDGT.": "w/o Importance Weighting"
        },
        {
          "Table 3: Ablation Studies with regard to Components in FreqDGT.": "w/o Fixed Adjacency"
        },
        {
          "Table 3: Ablation Studies with regard to Components in FreqDGT.": "w/o Dynamic Learning"
        },
        {
          "Table 3: Ablation Studies with regard to Components in FreqDGT.": "w/o Multi-scale Transformer"
        },
        {
          "Table 3: Ablation Studies with regard to Components in FreqDGT.": "w/o Adversarial Training"
        },
        {
          "Table 3: Ablation Studies with regard to Components in FreqDGT.": "FreqDGT (Full)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "frequency coupling patterns demonstrate sophisticated inter-band interactions that reflect established neurobiological"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "processes in emotional processing [19]."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "The connectivity analysis unveils emotion-specific brain network configurations that align with contemporary neurosci-"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "entific understanding. Positive emotional states exhibit robust fronto-parietal networks with left-hemispheric preference,"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "characteristic of approach-motivated executive control systems. Conversely, negative emotions engage temporo-frontal"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "circuits with enhanced right-hemispheric involvement, consistent with withdrawal-related neural architectures. These"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "findings validate FreqDGT’s capacity to capture biologically plausible representations of emotional brain dynamics [20]."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Project ID: P0049774).": "References"
        },
        {
          "(Project ID: P0049774).": "[1] Zijian Kang, Yueyang Li, Shengyu Gong, et al. Hypergraph multi-modal\nlearning for EEG-based emotion"
        },
        {
          "(Project ID: P0049774).": "recognition in conversation. arXiv preprint arXiv:2502.21154, 2025."
        },
        {
          "(Project ID: P0049774).": "[2] Tengfei Song, Wenming Zheng, Peng Song, et al. EEG emotion recognition using dynamical graph convolutional"
        },
        {
          "(Project ID: P0049774).": "neural networks.\nIEEE Transactions on Affective Computing, 11(3):532–541, 2018."
        },
        {
          "(Project ID: P0049774).": "[3] Yi Ding, Chengxuan Tong, Shuailei Zhang, et al. EmT: A novel transformer for generalized cross-subject eeg"
        },
        {
          "(Project ID: P0049774).": "emotion recognition. arXiv preprint arXiv:2406.18345, 2024."
        },
        {
          "(Project ID: P0049774).": "[4] Yekta Said Can, Bhargavi Mahesh, and Elisabeth André. Approaches, applications, and challenges in physiological"
        },
        {
          "(Project ID: P0049774).": "emotion recognition—a tutorial overview. Proceedings of the IEEE, 111(10):1287–1313, 2023."
        },
        {
          "(Project ID: P0049774).": "[5] Mingyi Sun, Weigang Cui, Shuyue Yu, et al. A dual-branch dynamic graph convolution based adaptive transformer"
        },
        {
          "(Project ID: P0049774).": "feature fusion network for EEG emotion recognition.\nIEEE Transactions on Affective Computing, 13(4):2218–"
        },
        {
          "(Project ID: P0049774).": "2228, 2022."
        },
        {
          "(Project ID: P0049774).": "[6] Ming Jin, Changde Du, Huiguang He, et al. PGCN: Pyramidal graph convolutional network for EEG emotion"
        },
        {
          "(Project ID: P0049774).": "recognition.\nIEEE Transactions on Multimedia, 2024."
        },
        {
          "(Project ID: P0049774).": "[7] Shengyu Gong, Yueyang Li, Zijian Kang, et al. LEREL: Lipschitz continuity-constrained emotion recognition"
        },
        {
          "(Project ID: P0049774).": "ensemble learning for electroencephalography. arXiv preprint arXiv:2504.09156, 2025."
        },
        {
          "(Project ID: P0049774).": "[8] Zhunan Li, Enwei Zhu, Ming Jin, et al. Dynamic domain adaptation for class-aware cross-subject and cross-session"
        },
        {
          "(Project ID: P0049774).": "EEG emotion recognition.\nIEEE Journal of Biomedical and Health Informatics, 26(12):5964–5973, 2022."
        },
        {
          "(Project ID: P0049774).": "[9] Peixiang Zhong, Di Wang, and Chunyan Miao. EEG-based emotion recognition using regularized graph neural"
        },
        {
          "(Project ID: P0049774).": "networks.\nIEEE Transactions on Affective Computing, 13(3):1290–1301, 2020."
        },
        {
          "(Project ID: P0049774).": "[10] Tong Zhang, Xuehan Wang, Xiangmin Xu, et al. GCB-Net: Graph convolutional broad network and its application"
        },
        {
          "(Project ID: P0049774).": "in emotion recognition.\nIEEE Transactions on Affective Computing, 13(1):379–388, 2019."
        },
        {
          "(Project ID: P0049774).": "[11] Yonghao Song, Qingqing Zheng, Bingchuan Liu, et al. EEG Conformer: Convolutional\ntransformer for eeg"
        },
        {
          "(Project ID: P0049774).": "decoding and visualization.\nIEEE Transactions on Neural Systems and Rehabilitation Engineering, 31:710–719,"
        },
        {
          "(Project ID: P0049774).": "2022."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[12] Yongling Xu, Yang Du, Ling Li, et al. AMDET: Attention based multiple dimensions EEG transformer for"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "emotion recognition.\nIEEE Transactions on Affective Computing, 15(3):1067–1077, 2023."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[13] Hao Chen, Ming Jin, Zhunan Li, et al. MS-MDA: Multisource marginal distribution adaptation for cross-subject"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "and cross-session EEG emotion recognition. Frontiers in Neuroscience, 15:778488, 2021."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[14] Xinke Shen, Xianggen Liu, Xin Hu, et al. Contrastive learning of subject-invariant EEG representations for"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "cross-subject emotion recognition.\nIEEE Transactions on Affective Computing, 14(3):2496–2511, 2022."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[15] Yueyang Li, Zijian Kang, Shengyu Gong, et al. Neural-MCRL: Neural multimodal contrastive representation"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "learning for EEG-based visual decoding. arXiv preprint arXiv:2412.17337, 2024."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[16] Wei-Long Zheng and Bao-Liang Lu.\nInvestigating critical frequency bands and channels for EEG-based emotion"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "recognition with deep neural networks.\nIEEE Transactions on autonomous mental development, 7(3):162–175,"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "2015."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[17]\nJingjing Chen, Xiaobin Wang, Chen Huang, et al. A large finer-grained affective computing EEG dataset. Scientific"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "Data, 10(1):740, 2023."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[18] Wei-Long Zheng, Wei Liu, Yifei Lu, et al. Emotionmeter: A multimodal framework for recognizing human"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "emotions.\nIEEE transactions on cybernetics, 49(3):1110–1122, 2018."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[19] William J Ray and Harry W Cole. EEG alpha activity reflects attentional demands, and beta activity reflects"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "emotional and cognitive processes. Science, 228(4700):750–752, 1985."
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "[20] Eddie Harmon-Jones and Philip A Gable. On the role of asymmetric frontal cortical activity in approach and"
        },
        {
          "Accepted by The 2025 International Conference on Machine Intelligence and Nature-InspireD Computing (MIND)": "withdrawal motivation: An updated review of the evidence. Psychophysiology, 55(1):e12879, 2018."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Hypergraph multi-modal learning for EEG-based emotion recognition in conversation",
      "authors": [
        "Zijian Kang",
        "Yueyang Li",
        "Shengyu Gong"
      ],
      "year": "2025",
      "venue": "Hypergraph multi-modal learning for EEG-based emotion recognition in conversation",
      "arxiv": "arXiv:2502.21154"
    },
    {
      "citation_id": "2",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Peng Song"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "EmT: A novel transformer for generalized cross-subject eeg emotion recognition",
      "authors": [
        "Yi Ding",
        "Chengxuan Tong",
        "Shuailei Zhang"
      ],
      "year": "2024",
      "venue": "EmT: A novel transformer for generalized cross-subject eeg emotion recognition",
      "arxiv": "arXiv:2406.18345"
    },
    {
      "citation_id": "4",
      "title": "Approaches, applications, and challenges in physiological emotion recognition-a tutorial overview",
      "authors": [
        "Yekta Said Can",
        "Bhargavi Mahesh",
        "Elisabeth André"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "5",
      "title": "A dual-branch dynamic graph convolution based adaptive transformer feature fusion network for EEG emotion recognition",
      "authors": [
        "Mingyi Sun",
        "Weigang Cui",
        "Shuyue Yu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "PGCN: Pyramidal graph convolutional network for EEG emotion recognition",
      "authors": [
        "Ming Jin",
        "Changde Du",
        "Huiguang He"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "LEREL: Lipschitz continuity-constrained emotion recognition ensemble learning for electroencephalography",
      "authors": [
        "Shengyu Gong",
        "Yueyang Li",
        "Zijian Kang"
      ],
      "year": "2025",
      "venue": "LEREL: Lipschitz continuity-constrained emotion recognition ensemble learning for electroencephalography",
      "arxiv": "arXiv:2504.09156"
    },
    {
      "citation_id": "8",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Zhunan Li",
        "Enwei Zhu",
        "Ming Jin"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "9",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "GCB-Net: Graph convolutional broad network and its application in emotion recognition",
      "authors": [
        "Tong Zhang",
        "Xuehan Wang",
        "Xiangmin Xu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "EEG Conformer: Convolutional transformer for eeg decoding and visualization",
      "authors": [
        "Yonghao Song",
        "Qingqing Zheng",
        "Bingchuan Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "12",
      "title": "AMDET: Attention based multiple dimensions EEG transformer for emotion recognition",
      "authors": [
        "Yongling Xu",
        "Yang Du",
        "Ling Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "MS-MDA: Multisource marginal distribution adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Ming Hao Chen",
        "Zhunan Jin",
        "Li"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "14",
      "title": "Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition",
      "authors": [
        "Xinke Shen",
        "Xianggen Liu",
        "Xin Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Neural multimodal contrastive representation learning for EEG-based visual decoding",
      "authors": [
        "Yueyang Li",
        "Zijian Kang",
        "Shengyu Gong"
      ],
      "year": "2024",
      "venue": "Neural multimodal contrastive representation learning for EEG-based visual decoding",
      "arxiv": "arXiv:2412.17337"
    },
    {
      "citation_id": "16",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "17",
      "title": "A large finer-grained affective computing EEG dataset",
      "authors": [
        "Jingjing Chen",
        "Xiaobin Wang",
        "Chen Huang"
      ],
      "year": "2023",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "18",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng",
        "Wei Liu",
        "Yifei Lu"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "19",
      "title": "EEG alpha activity reflects attentional demands, and beta activity reflects emotional and cognitive processes",
      "authors": [
        "J William",
        "Harry Ray",
        "Cole"
      ],
      "year": "1985",
      "venue": "Science"
    },
    {
      "citation_id": "20",
      "title": "On the role of asymmetric frontal cortical activity in approach and withdrawal motivation: An updated review of the evidence",
      "authors": [
        "Eddie Harmon",
        "Philip Gable"
      ],
      "year": "2018",
      "venue": "Psychophysiology"
    }
  ]
}