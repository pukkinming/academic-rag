{
  "paper_id": "2508.06564v2",
  "title": "Grounding Emotion Recognition With Visual Prototypes: Vega -Revisiting Clip In Merc",
  "published": "2025-08-06T19:43:58Z",
  "authors": [
    "Guanyu Hu",
    "Dimitrios Kollias",
    "Xinyu Yang"
  ],
  "keywords": [
    "‚Ä¢ Information systems ‚Üí Sentiment analysis",
    "‚Ä¢ Computing methodologies ‚Üí Discourse, dialogue and pragmatics",
    "Biometrics Multimodal Emotion Recognition in Conversation",
    "MERC",
    "CLIP",
    "VEGA",
    "Visual Anchors",
    "cognitive theory",
    "multisensory integration"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Emotion Recognition in Conversations remains a challenging task due to the complex interplay of textual, acoustic and visual signals. While recent models have improved performance via advanced fusion strategies, they often lack psychologically meaningful priors to guide multimodal alignment. In this paper, we revisit the use of CLIP and propose a novel Visual Emotion Guided Anchoring (VEGA) mechanism that introduces class-level visual semantics into the fusion and classification process. Distinct from prior work that primarily utilizes CLIP's textual encoder, our approach leverages its image encoder to construct emotion-specific visual anchors based on facial exemplars. These anchors guide unimodal and multimodal features toward a perceptually grounded and psychologically aligned representation space, drawing inspiration from cognitive theories (prototypical emotion categories and multisensory integration). A stochastic anchor sampling strategy further enhances robustness by balancing semantic stability and intra-class diversity. Integrated into a dual-branch architecture with self-distillation, our VEGA-augmented model achieves sota performance on IEMOCAP and MELD. Code is available at: https://github.com/dkollias/VEGA.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversations (ERC) presents unique challenges due to the dynamic and spontaneous nature of dialogues, where individuals express a wide range of emotions that evolve over time  [16, 35, 45, 57] . While early ERC approaches predominantly relied on textual data  [26] , this modality alone often fails to capture the subtlety and variability of emotional expression  [11] . As a result, Multimodal ERC (MERC), which incorporates audio and visual modalities alongside text, has gained growing interest for its potential to model richer emotional cues  [13, 46, 48] .\n\nTo integrate information from multiple modalities, MERC methods commonly adopt fusion strategies such as feature concatenation  [11, 43] , attention mechanisms  [41, 48]  and heterogeneous graph-based approaches  [3, 13, 17] . Recent state-of-the-art models, such as  [53] , have advanced this space by hierarchically integrating modality-specific features and optimizing for emotion classification. While these methods have led to strong performance on benchmark datasets, they remain fundamentally data-driven and lack structured, semantically aligned priors to guide the fusion process. This limits their robustness and generalization, especially when handling ambiguous, noisy, or weakly correlated multimodal inputs, which are common in real-world conversational settings.\n\nIn this paper, we introduce a novel approach that enhances the semantic alignment and robustness of multimodal fusion by integrating CLIP-guided visual anchors, which serve as emotion-specific prototypes encoded from real images. For each of the emotional classes, we select a small set of representative facial images and encode them using CLIP's image encoder. The mean embedding per class is used as a semantic anchor, which guides the alignment of all three modalities (visual, acoustic, textual) within a shared emotion-centric representation space. To enhance generalization and mitigate overfitting to fixed anchors, we introduce a stochastic sampling strategy that alternates between using class-wise center anchors and randomly selected instances during training. This approach balances semantic stability with intra-class variability, effectively serving as semantic augmentation. The sampled anchors are applied to both unimodal and fused representations, promoting discriminative, robust, and semantically coherent feature learning.\n\nImportantly, while most prior approaches that leverage CLIP focus on its textual modality to align features, typically mapping audio or visual inputs to text-derived embeddings (e.g., prompts such as \"this is a sad face\"), we deliberately reverse this trend. These prior methods use text prompts because CLIP's textual encoder offers a powerful, zero-shot compatible representation space and is easier to manipulate through language. However, textual descriptions of emotions are often ambiguous, abstract, and culturally variable, whereas emotional expressions in human faces are more concrete, universal, and visually grounded  [10, 14, 15, 20, 38, 47] . By anchoring fusion around CLIP's visual embeddings, we embrace a more perceptually grounded and psychologically aligned representation of emotion, closer to how humans perceive affective states.\n\nThis design draws on rich insights from cognitive psychology. The use of CLIP-based visual anchors reflects the prototypical emotion theory  [7, 8] , which posits that emotional categories are organized around prototypical exemplars, such as a canonical \"angry\" or \"happy\" face. The use of CLIP-derived anchors mimics this structure by introducing emotion prototypes derived from real-world facial imagery. Moreover, our approach is aligned with the theory of multisensory integration in emotion perception  [6] , which suggests that the human brain aligns information from multiple channels using known emotional schemas. By aligning each modality with shared visual anchors, our method mimics this natural mechanism of emotional cognition. Additionally, affective priming  [9]  and embodied cognition theories  [1]  provide psychological justification: humans often interpret ambiguous emotional signals by referencing mental imagery or contextually induced affective states. Our visual anchors serve a similar role in guiding the model's internal representation toward consistent emotional interpretations.\n\nBy bridging advances in vision-language pretraining with psychologically grounded theories of emotion, our approach offers a new paradigm for MERC that is both more robust and more humanaligned. In summary, our contributions are threefold: To the best of our knowledge, this is the first visually driven use of CLIP in MERC, opening a new and unexplored direction. ‚Ä¢ Psychologically Grounded Design: Our framework is inspired by cognitive and affective psychological theories, such as prototypical emotion categories and multisensory integration, providing semantically meaningful guidance for emotion modeling. ‚Ä¢ State-of-the-Art Performance: Our approach achieves stateof-the-art results on two widely used benchmarks, IEMOCAP and MELD, demonstrating its effectiveness in both fine-grained and diverse conversational emotion recognition settings.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal Emotion Recognition in Conversations (MERC). While traditional ERC has focused on textual input  [33, 35] , its limited emotional expressiveness has driven the development of MERC, which integrates text, audio, and visual modalities  [12, 21-23, 26, 49, 50, 54] . Early fusion approaches  [26]  simply concatenate features, but often struggle with modality imbalance and noise. More advanced strategies leverage attention  [49, 55] , memory  [50] , contextaware modeling  [12, 30] , and semantic refinement  [51, 54]  to better capture inter-modal and temporal dependencies. However, most existing models remain data-driven, lacking structured semantic priors or emotion-level prototypes to guide multimodal alignment. CLIP for Emotion and Multimodal Representation. CLIP  [37]  has demonstrated strong generalization in vision-language tasks via large-scale contrastive pretraining. In emotion recognition, recent works  [27, 36, 56 ] adapt CLIP's textual encoder with emotion prompts for zero-shot classification, yet focus on text-based alignment. These approaches underutilize CLIP's visual encoder and overlook the universality of visual affective cues. Notably, CLIP has not been explored in the context of MERC, leaving a gap in leveraging perceptual visual semantics for multimodal emotional grounding.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In the following, we present MERC, which builds upon the Self-Distillation Transformer  [28]  and comprises the following components: (i) Modality-Specific Encoding: Independently encodes unimodal features from each input modality (text, audio, and visual). (ii) Modality Encoder: Captures both intra-modal and intermodal contextual dependencies using parallel Transformer-based encoders. (iii) Dual-Branch Hierarchical Anchoring and Fusion: Progressively constructs semantically enriched unimodal and multimodal representations via two parallel branches. The supervised branch performs gated feature fusion followed by labelsupervised classification, while the VEGA branch applies CLIPbased Visual Emotion Guided Anchoring (VEGA) to align features with external visual anchors derived from CLIP image embeddings. (iv) Classification and Self-Distillation: Both the supervised branch and the VEGA branch independently perform classification and incorporate self-distillation to enforce cross-modal consistency.\n\nLet a conversation be represented as a sequence of ùëÅ utterances, ùê∂ = {u 1 , u 2 , . . . , u ùëÅ }. Each utterance u ùë° is defined as a tuple of modality-specific inputs:\n\n, where x",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modality-Specific Encoding",
      "text": "Each input modality is first passed through a dedicated encoder to extract utterance-level unimodal features:\n\nwhere F ùëá enc , F ùê¥ enc , and F ùëâ enc refer to RoBERTa, OpenSMILE, and DenseNet-based encoders, respectively. The output h (ùëö) ùë° ‚àà R ùëë ùëö is the initial unimodal representation of utterance u ùë° for modality ùëö.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modality Gated Encoder And Fusion",
      "text": "To capture local temporal patterns, we apply F conv (‚Ä¢), a Temporal Convolution Block (TCB), to extracted features:\n\nwhere: ùëò is the temporal window size and h(ùëö) ùë° ‚àà R ùëë . Next, we inject structural information into each modality stream by adding: (i) positional embeddings p ùë° , which encode the position of each utterance within the conversation; and (ii) speaker embeddings s ùë° , which indicate the speaker identity and help capture speaker-specific emotional dynamics: ƒ•(ùëö)\n\nThese enhanced modality-specific representations are further processed by a set of parallel Transformer (TFM) modules to model both intra-modal and inter-modal contextual dependencies. For each modality ùëö ‚àà {ùëá , ùê¥, ùëâ }, we employ three contextual Transformer modules: (i) an intra-modal Transformer T (ùëö‚Üêùëö) intra , which captures both temporal and contextual dynamics within modality ùëö; (ii) an inter-modal Transformer T (ùëö‚Üêùëö 1 ) inter , which models modality-specific cross-modal interactions from modality ùëö 1 to ùëö; and (iii) another inter-modal Transformer T (ùëö‚Üêùëö 2 ) inter , which encodes complementary information from modality ùëö 2 to ùëö, where {ùëö 1 , ùëö 2 } = {ùëá , ùê¥, ùëâ } \\ {ùëö}. Each module outputs a conversationlevel contextual representation for modality ùëö as follows: z(ùëö) intra = T (ùëö‚Üêùëö) intra ƒ•(ùëö) ; ƒ•(ùëö) ,\n\nz(ùëö) inter1 = T (ùëö‚Üêùëö 1 ) inter ƒ•(ùëö) ; ƒ•(ùëö 1 ) ,\n\nz(ùëö) inter2 = T (ùëö‚Üêùëö 2 ) inter ƒ•(ùëö) ; ƒ•(ùëö 2 ) ,\n\nwhere z(ùëö) intra , z(ùëö) inter1 , z(ùëö) inter2 ‚äÇ R ùëÅ √óùëë . This enables each modality stream to integrate multi-perspective contextual information.\n\nTo further enhance representation quality, we apply a modalityspecific gating mechanism that adaptively filters each context stream based on its semantic relevance. For each z(ùëö) * ,ùë° ‚àà z(ùëö) * , a learnable linear transformation followed by a sigmoid activation is applied:\n\nwhere, * ‚àà {intra, inter1, inter2} and ‚äô denotes element-wise multiplication and ùúé (‚Ä¢) is the sigmoid function. Finally, the gated outputs are concatenated and passed through a modality-specific linear layer F (ùëö) cat (‚Ä¢) to obtain the final unified unimodal representation:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dual Branch For Supervision And Anchoring",
      "text": "After obtaining the final unimodal-level representations, architecture bifurcates into two semantically distinct branches: (i) Supervision Branch: This branch conducts label-supervised classification at both unimodal and multimodal levels. It adaptively integrates the modality-specific representations to produce a final prediction aligned with the ground-truth emotion labels. (ii) VEGA Branch:\n\nThe CLIP-based Visual Emotion Guided Anchoring (VEGA) branch projects both modality-specific and fused representations into the CLIP visual embedding space, aligning them with emotion-specific visual anchors. This branch provides high-level semantic supervision by encouraging the learned features to semantically align with human-interpretable emotion prototypes. The introduction of two parallel branches offers several advantages over a single-branch design: 1) Decoupled Objectives: Visual anchoring and supervised classification are optimized independently, mitigating gradient interference between semantic alignment and task-specific learning. 2) Modularity and Scalability: The VEGA branch can be fine-tuned or extended in isolation, without requiring retraining of the fusion branch, thus enabling flexible experimentation and deployment. 3) Complementary Supervision: While the fusion branch focuses on capturing signal quality and cross-modal interactions, the VEGA branch grounds the learning process in semantically rich visual priors, enhancing robustness under noisy or ambiguous input conditions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Supervision Branch. This Branch Introduces Both Unimodal",
      "text": "and fusion-level supervision to enhance discriminability of modalityspecific representations and the robustness of multimodal fusion.\n\nUnimodal Prediction. Each modality-specific representation z (ùëö)   is passed through a linear classifier\n\nMultimodal Gated Fusion & Prediction. To integrate information across modalities, a soft gating mechanism G(‚Ä¢) is used to assign importance weights to each modality. The fused representation f ‚àà R ùëë is computed as:\n\nThe fusion classifier C fuse (‚Ä¢) is used to produce the prediction:\n\nSupervision Objectives. This branch is optimized using a combination of hard-label and soft-label (self-distillation) objectives.\n\nHard-Label Supervision. Cross-entropy loss is computed for both unimodal and fused predictions with respect to the ground-truth label ùë¶ ‚àà Y:\n\nSelf-Distillation (Soft-Label Supervision). To encourage semantic consistency between unimodal and multimodal representations, we adopt a KL-based distillation loss that aligns unimodal predictions with the fused prediction:\n\nTotal Supervision Loss. The total loss for the Supervision Branch is a weighted combination of classification and distillation objectives:\n\nwhere ùúÜ dist is a tunable hyperparameter controlling the strength of the distillation regularization. VEGA is applied at two levels of representation: (i) Unimodal Anchoring: VEGA guides the semantic projection of each modalityspecific representation, enhancing the emotional discriminability of unimodal features. (ii) Multimodal Anchoring: After fusion, the same anchoring mechanism is applied to refine the fused representation, aligning it with CLIP-derived emotion anchors in a shared semantic space. By enforcing semantic alignment with shared emotion anchors at both unimodal and multimodal level, VEGA promotes feature semantic consistency, strengthens crossmodal grounding, and improves generalization.\n\nThe class-wise Center Anchor is derived by averaging the ùëõ embedding vectors associated with each emotion class:\n\nEach vùëê ‚àà R ùëë anc serves as a visual emotion anchor, capturing highlevel, human-aligned semantic structure. These anchors are used to guide the alignment of modality-specific and fused representations within a shared emotion-centric representation space.\n\nStochastic Anchor Sampling Strategy. To improve the stability and generalization of visual emotion guided anchoring, we propose a simple yet effective stochastic anchor sampling strategy. During training, we alternate between two types of anchors for each emotion class: the Center Anchor (the mean embedding computed as in Eq.(  17 )) and the Random Anchor (a randomly selected instance embedding from the same class as defined in Eq.(  16 )). This stochasticity encourages the model to align features not only with stable semantic prototypes but also with diverse intra-class variations, thereby enhancing robustness and flexibility. This strategy offers two key benefits: (i) Stability -the center anchor serves as a canonical prototype, providing a stable and generalizable semantic reference for alignment, thereby promoting semantic consistency and reducing sensitivity; (ii) Variabilitythe use of randomly selected anchors introduces semantic perturbations, encouraging the model to learn robust associations between diverse intra-class visual representations and their emotion labels. This stochastic alternation between stable and augmentative anchors acts as a form of semantic augmentation, mitigating overfitting to specific anchor instances and enhancing semantic diversity for more flexible and generalizable feature learning.\n\nFormally, at each training iteration, for a given emotion class ùëê, the selected anchor a ùëê ‚àà R ùëë anc is defined as:\n\nTo express the strategy more compactly, we define:\n\nwhere ùëü ‚àº Bernoulli(ùëû) is a binary variable determining the sampling mode. The hyperparameter ùëû ‚àà (0, 1) controls the probability of selecting the center anchor (e.g., ùëû = 0.5 gives equal probability to both types). ùëó ‚àà {1, . . . , ùëõ} denotes a randomly sampled index.\n\nVisual Emotion Guided Anchoring. We apply the VEGA mechanism at both the unimodal and multimodal levels to inject CLIPderived visual emotion anchor priors into the representation space. Unimodal Anchoring. Each modality-specific representation z (ùëö) is first projected into the CLIP-aligned embedding space using a learnable projection module P uni (‚Ä¢):\n\nThe cosine similarity between the projected representation and each emotion anchor a ùëê is computed as:\n\nThese similarity scores are then normalized via softmax to obtain the anchor-based emotion prediction distribution:\n\nMultimodal Anchoring. The fused representation f is projected using a separate fusion-specific module P fuse (‚Ä¢):\n\nIts similarity with each anchor is computed by:\n\nThe normalized fusion-level anchor prediction is given by: ≈∑fuse anc,ùëê = exp(ùë† fuse ùëê )\n\nVEGA Objectives. Similar to the Supervision Branch, VEGA incorporates both hard-label objectives and soft-label (self-distillation) objectives at the unimodal and fusion levels.\n\nModality-Specific Anchoring Loss. For each modality ùëö, we compute the anchor-based classification loss using the cross-entropy between the predicted distribution ≈∑(ùëö) anc and the ground-truth label ùë¶ ‚àà Y:\n\nwhere ≈∑ (ùëö) anc,ùë¶ denotes the predicted probability of the correct class. Multimodal Anchoring Loss. A similar classification loss is defined for the fused representation:\n\nAnchoring Self-Distillation Loss. To promote semantic consistency across modalities in the VEGA-aligned space, we introduce a distillation loss where the fusion prediction ≈∑fuse anc acts as a teacher to guide each modality-specific anchor prediction:\n\nTotal VEGA Loss. The total loss for the VEGA Branch combines the above terms:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overall Objective",
      "text": "The overall training objective combines the Supervision Branch and VEGA Branch losses:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results And Discussion 4.1 Datasets And Evaluations",
      "text": "Datasets. We conduct experiments on IEMOCAP  [2]  and MELD  [34]  datasets. IEMOCAP consists of dyadic conversations between ten professional actors, comprising a total of 153 dialogues and 7,433 utterances. Each utterance is manually segmented and annotated with categorical emotion labels. The dataset is divided into five sessions, with each session featuring a unique pair of speakers. MELD contains 1,433 multi-party conversations and 13,708 utterances, each labeled with one of seven emotion categories. Evaluation Protocol. Following standard practice in previous works  [29] , we split the dataset into training, validation and test  sets. We report class-wise accuracy (ACC) and F1-score (F1), as well as overall accuracy (ACC) and weighted-average F1 (w-F1) to comprehensively evaluate model performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "Our model is implemented in PyTorch and optimized using AdamW. The initial learning rate is 3 √ó 10 -4 with a weight decay of 7 √ó 10 -1 . We use batch size 15, and apply dropout 0.5 in the main encoder and in classification head. The Transformer module is configured with 8 attention heads and a hidden dimension of 1280. For CLIP-guided anchoring, we extract emotion anchors using 35 reference images per class. The CLIP projection head consists of 2 layers with SiLU activation and a dropout rate of 0.4. During training, we employ stochastic anchor sampling with a probability threshold ùëû = 0.2. The weights in the loss have been set empirically as:\n\nanc = ùúÜ anc-dist = 0.6, and ùúÜ dist = 0.9. All reported results are averaged over 10 independent runs with different random seeds.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With State-Of-The-Art (Sota)",
      "text": "We compare our method against SOTA on both IEMOCAP and MELD, as shown in Table  1 . The evaluated methods span a diverse range of paradigms, including early fusion (e.g., MM-DFN), graphbased reasoning (e.g., D 2 GNN, MMGCN, Frame-SCN, GraphSmile), memory-enhanced dialogue modeling (e.g., MGLRA, COGMEN), attention-based architectures (e.g., MKIN-MCL), and transformerbased models (e.g., DQ-Former, DialogueTRM, SDT). To ensure fairness, all methods are evaluated with same multimodal input modalities (text, audio, and visual) whenever applicable.\n\nOur proposed SDT-VEGA consistently outperforms all sota, achieving substantial improvements of 2.07%-11.2% in accuracy and 1.5%-9.89% in F1 score on IEMOCAP, and 2.02%-8.3% in accuracy and 1.75%-9.89% in F1 on MELD.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines And Modality-Agnostic Evaluation",
      "text": "To validate the modality-agnostic and architecture-agnostic nature of VEGA mechanism, we construct three unimodal baselines, each comprising a modality-specific feature extractor followed by a Transformer-based temporal modeling module. The three configurations correspond to visual, textual and acoustic modalities, utilizing DenseNet, RoBERTa and OpenSmile, respectively. For each baseline, we report performance on IEMOCAP under two settings: a standard version and an enhanced version with the VEGA branch. As shown in Table  2 , integrating VEGA consistently improves performance across all modalities and most emotion categories. Specifically: (i) augmenting DenseNet (visual) with VEGA yields a 3.01% gain in F1 score and 1.61% in accuracy; (ii) incorporating VEGA into RoBERTa (text) leads to a 2.29% increase in F1 and 2.47% in accuracy; (iii) applying VEGA to OpenSmile (audio) results in a 2.54% improvement in F1 and 3.33% in accuracy. These results underscore VEGA's general applicability and its effectiveness in enhancing affective representations across diverse modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Architecture-Agnostic Evaluation",
      "text": "To assess the architecture-agnostic nature of VEGA, we integrate it into MSRFG, a graph-based dialogue model that stands in stark contrast to our transformer-based SDT backbone. As reported in Table 3, the VEGA-augmented variant, MSRFG-VEGA, consistently surpasses its baseline counterpart across both datasets. It yields absolute gains of 2.7% in accuracy and 1.9% in F1 on MELD, and 1.8% in accuracy and 1.6% in F1 on IEMOCAP. These results underscore VEGA's strong generalizability and effectiveness when deployed on fundamentally different architectural paradigms.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "To investigate the contribution of individual architectural components in our proposed method, we conduct a series of ablation studies, summarized in Table  4 .\n\nModel Structure: Ablating positional embeddings causes a clear performance drop, reflecting their role in capturing temporal structure. Removing speaker embeddings leads to even greater degradation, underscoring the value of speaker-aware cues. Eliminating intra-modal transformers moderately impairs performance, while removing inter-modal transformers further reduces accuracy, confirming the importance of both modality-specific and cross-modal modeling. These results highlight the complementary contributions of temporal, speaker, and cross-modal cues to emotion recognition.\n\nSingle vs. Dual Branch Design. We compare two fusion-level classification schemes: Single Branch, where CLIP-space projection precedes classification and self-distillation, and Dual Branch, which decouples Supervision and VEGA into separate heads. The dualbranch design yields a notable improvement of 1.63% in accuracy and 1.22% in F1. This gain highlights the benefit of separating optimization paths: Supervision Branch focuses on label discrimination, while VEGA Branch aligns features with external semantics.\n\nImpact of Anchoring and Distillation Losses (1) Without multimodal anchoring (L anc-dist ) without any classification terms causes further deterioration, showing that while anchors provide structure, they cannot substitute for hard-label supervision. These results highlight that anchoring and classification play complementary roles. The full loss objective achieves the best performance by jointly enforcing semantic alignment and decision boundary learning.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Teacher Selection",
      "text": "We investigate the effect of different teacher signals used in the two self-distillation objectives: anchoring-based distillation and classification-based distillation.\n\nWhile both aim to transfer fusion-level semantics to unimodal branches, they rely on distinct supervision signals. We conduct ablation studies by swapping their respective teacher: (1) Anchoringbased distillation with classification teacher. Replacing anchoring teacher with classification logits significantly reduces performance.\n\n(2) Classification-based distillation with anchoring-based teacher. Conversely, using the anchoring fusion distribution to supervise the classification branch leads to further degradation.\n\nVisual Sample Number in Anchor Construction We evaluate the impact of image diversity by varying the number of images used per class to construct anchors. Using only a single image per class yields the worst performance due to limited intra-class coverage, resulting in rigid and unrepresentative anchors. Increasing to 35 images brings a significant improvement by balancing diversity and consistency. However, using 100 or more images slightly degrades performance, likely due to over-diversification introducing noise and assignment instability. Excessively dispersed anchors reduce alignment precision and slow convergence under our similaritythresholded matching strategy.\n\nEffect of Anchor Sampling Threshold ùëû We examine how the anchor sampling threshold ùëû affects performance. Best results are achieved at ùëû = 0.2, which balances exploration of diverse anchors with convergence toward stable assignments. Setting ùëû = 0, which always selecting the center anchor, slightly reduces performance, likely due to limited intra-class variability modeling. Higher values (e.g., ùëû = 0.5) lead to further degradation, as excessive randomness in anchor assignment hampers semantic consistency.\n\nEffect of CLIP Visual Encoder Choice We assess the impact of different CLIP visual encoders on the quality of emotion anchors. Among the tested backbones, ViT-L/14@336 achieves the best performance, outperforming smaller or lower-resolution variants such as ViT-B/16. This improvement can be attributed to two factors: (1) larger model capacity (ViT-L) enables better capture of high-level emotional semantics, and (2) higher input resolution (336px) preserves subtle affective cues (e.g., facial micro-expressions) crucial for emotion grounding. In contrast, lower-capacity or lower-resolution models produce anchors with limited semantic granularity, weakening feature-to-anchor alignment.\n\nEffect of Modality and Modality Combinations We evaluate the contribution of individual modalities and pairwise combinations to overall performance. Among unimodal inputs, text achieves the highest performance, reflecting its strong correlation with emotion semantics. Audio performs moderately, while visual cues alone yield the lowest results. When combining modalities, all pairs show consistent improvement. The best result is obtained from text + audio, indicating complementary strengths, text provides semantic precision, while audio adds prosodic and tonal cues.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we present a novel multimodal emotion recognition framework that integrates data-driven learning with semantically grounded supervision. At the core of our design is Visual Emotion-Guided Anchoring (VEGA), which aligns unimodal and fused representations with emotion semantic priors through a dual-branch architecture. The framework incorporates emotion-level anchors, a",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of SDT-VEGA, comprising of: (i) Modality-Specific Encoding; (ii) Modality Gated Encoder; Dual-Branch",
      "page": 3
    },
    {
      "caption": "Figure 2: t-SNE of the fused feature on IEMOCAP test set.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "S": "P\nS\nP"
        },
        {
          "S": "PS"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Grounded cognition",
      "authors": [
        "Lawrence Barsalou"
      ],
      "year": "2008",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Jie Shao",
        "Shuyuan Zhu",
        "Heng Tao Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Multi-modal graph context extraction and consensus-aware learning for emotion recognition in conversation",
      "authors": [
        "Yijing Dai",
        "Jinxing Li",
        "Yingjian Li",
        "Guangming Lu"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "5",
      "title": "Multimodal decoupled distillation graph neural network for emotion recognition in conversation",
      "authors": [
        "Yijing Dai",
        "Yingjian Li",
        "Dongpeng Chen",
        "Jinxing Li",
        "Guangming Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "6",
      "title": "The perception of emotions by ear and by eye",
      "authors": [
        "Beatrice De",
        "Jean Vroomen"
      ],
      "year": "2000",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "7",
      "title": "An Argument for Basic Emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "8",
      "title": "An Argument for Basic Emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1994",
      "venue": "The Nature of Emotion: Fundamental Questions"
    },
    {
      "citation_id": "9",
      "title": "On the automatic activation of associated evaluations: An overview",
      "authors": [
        "H Russell",
        "Fazio"
      ],
      "year": "2001",
      "venue": "Cognitive Methods in Social Psychology"
    },
    {
      "citation_id": "10",
      "title": "Concept of emotion viewed from a prototype perspective",
      "authors": [
        "Beverley Fehr",
        "James Russell"
      ],
      "year": "1984",
      "venue": "Journal of experimental psychology: General"
    },
    {
      "citation_id": "11",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "12",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Cheng Hu",
        "Hong Mao",
        "Rui Zhang",
        "Jie Zhang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "MM-DFN: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lianxin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Rethinking affect analysis: A protocol for ensuring fairness and consistency",
      "authors": [
        "Guanyu Hu",
        "Dimitrios Kollias",
        "Eleni Papadopoulou",
        "Paraskevi Tzouveli",
        "Jie Wei",
        "Xinyu Yang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "15",
      "title": "Bridging the gap: Protocol towards fair and consistent affect analysis",
      "authors": [
        "Guanyu Hu",
        "Eleni Papadopoulou",
        "Dimitrios Kollias",
        "Paraskevi Tzouveli",
        "Jie Wei",
        "Xinyu Yang"
      ],
      "year": "2024",
      "venue": "2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "16",
      "title": "Robust Facial Reactions Generation: An Emotion-Aware Framework with Modality Compensation",
      "authors": [
        "Guanyu Hu",
        "Jie Wei",
        "Siyang Song",
        "Dimitrios Kollias",
        "Xinyu Yang",
        "Zhonglin Sun",
        "Odysseus Kaloidas"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "17",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "18",
      "title": "Dq-former: Querying transformer with dynamic modality priority for cognitive-aligned multimodal emotion recognition in conversation",
      "authors": [
        "Ye Jing",
        "Xinpei Zhao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain"
      ],
      "year": "2022",
      "venue": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "arxiv": "arXiv:2205.02455"
    },
    {
      "citation_id": "20",
      "title": "Mma-mrnnet: Harnessing multiple models of affect and dynamic masked rnn for precise facial expression intensity estimation",
      "authors": [
        "Dimitrios Kollias",
        "Andreas Psaroudakis",
        "Anastasios Arsenos",
        "Paraskevi Theofilou",
        "Chunchang Shao",
        "Guanyu Hu",
        "Ioannis Patras"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Eric Granger",
        "Marco Pedersoli",
        "Simon Bacon",
        "Alice Baird",
        "Chris Gagne"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "23",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "GraphCFC: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Zhigang Zeng"
      ],
      "year": "2024",
      "venue": "Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition",
      "arxiv": "arXiv:2407.21536"
    },
    {
      "citation_id": "26",
      "title": "Semi-supervised emotion recognition in textual conversation via a context-augmented auxiliary training task",
      "authors": [
        "Yao Li",
        "Xiaohan Mao",
        "Bin Li",
        "Fei Wu",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "27",
      "title": "Multimodal Emotion Recognition with Vision-Language Prompting and Modality Dropout",
      "authors": [
        "Xiaoyi Lin",
        "Qingsong Fan",
        "Xin Zhang",
        "Jianbo Yin"
      ],
      "year": "2023",
      "venue": "Multimodal Emotion Recognition with Vision-Language Prompting and Modality Dropout",
      "arxiv": "arXiv:2409.07078"
    },
    {
      "citation_id": "28",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "DialogueRNN: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "authors": [
        "Yuzhao Mao",
        "Qi Sun",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li",
        "Jianping Shen"
      ],
      "year": "2020",
      "venue": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "arxiv": "arXiv:2010.07637"
    },
    {
      "citation_id": "32",
      "title": "Masked graph learning with recurrent alignment for multimodal emotion recognition in conversation",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Hongen Shao",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Deep Emotion Recognition in Textual Conversations: A Survey",
      "authors": [
        "Patr√≠cia Pereira",
        "Helena Moniz",
        "Joao Carvalho"
      ],
      "year": "2022",
      "venue": "Deep Emotion Recognition in Textual Conversations: A Survey",
      "arxiv": "arXiv:2211.09172"
    },
    {
      "citation_id": "34",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "35",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "36",
      "title": "Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout",
      "authors": [
        "Anbin Qi",
        "Zhongliang Liu",
        "Xinyong Zhou",
        "Jinba Xiao",
        "Fengrun Zhang",
        "Qi Gan",
        "Ming Tao",
        "Gaozheng Zhang",
        "Lu Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pam Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "38",
      "title": "Emotion knowledge: further exploration of a prototype approach",
      "authors": [
        "Phillip Shaver",
        "Judith Schwartz",
        "Donald Kirson",
        "Cary O' Connor"
      ],
      "year": "1987",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "39",
      "title": "Multimodal knowledge-enhanced interactive network with mixed contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xudong Shen",
        "Xianying Huang",
        "Shihao Zou",
        "Xinyi Gan"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "40",
      "title": "Multimodal graph learning with framelet-based stochastic configuration networks for emotion recognition in conversation",
      "authors": [
        "Jiandong Shi",
        "Ming Li",
        "Yuting Chen",
        "Lixin Cui",
        "Lu Bai"
      ],
      "year": "2025",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "41",
      "title": "MultiEMO: An attention-based correlationaware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "Tao Shi",
        "Shao-Lun Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Fuchen Zhang",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "43",
      "title": "Exploration meets exploitation: Multitask learning for emotion recognition based on discrete and dimensional models",
      "authors": [
        "Geng Tu",
        "Jintao Wen",
        "Hao Liu",
        "Sentao Chen",
        "Lin Zheng",
        "Dazhi Jiang"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "44",
      "title": "Adaptive graph learning for multimodal conversational emotion detection",
      "authors": [
        "Geng Tu",
        "Tian Xie",
        "Bin Liang",
        "Hongpeng Wang",
        "Ruifeng Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Multiscale receptive field graph model for emotion recognition in conversations",
      "authors": [
        "Jie Wei",
        "Guanyu Hu",
        "Anh Luu",
        "Xinyu Tuan",
        "Wenjing Yang",
        "Zhu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition",
      "authors": [
        "Jie Wei",
        "Guanyu Hu",
        "Xinyu Yang",
        "Anh Luu",
        "Yizhuo Dong"
      ],
      "year": "2022",
      "venue": "Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition"
    },
    {
      "citation_id": "47",
      "title": "Learning facial expression and body gesture visual information for video emotion recognition",
      "authors": [
        "Jie Wei",
        "Guanyu Hu",
        "Xinyu Yang",
        "Anh Luu",
        "Yizhuo Dong"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "48",
      "title": "Self-adaptive context and modal-interaction modeling for multimodal emotion recognition",
      "authors": [
        "Haozhe Yang",
        "Xianqiang Gao",
        "Jianlong Wu",
        "Tian Gan",
        "Ning Ding",
        "Feijun Jiang",
        "Liqiang Nie"
      ],
      "year": "2023",
      "venue": "Findings of the association for computational linguistics: ACL 2023"
    },
    {
      "citation_id": "49",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "IEEE spoken language technology workshop"
    },
    {
      "citation_id": "50",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Multimodal emotion recognition based on audio and text by using hybrid attention networks",
      "authors": [
        "Shiqing Zhang",
        "Yijiao Yang",
        "Chen Chen",
        "Ruixin Liu",
        "Xin Tao",
        "Wenping Guo",
        "Yicheng Xu",
        "Xiaoming Zhao"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "52",
      "title": "A multi-level alignment and cross-modal unified semantic graph refinement network for conversational emotion recognition",
      "authors": [
        "Xiaoheng Zhang",
        "Weigang Cui",
        "Bin Hu",
        "Yang Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "A cross-modality context fusion and semantic refinement network for emotion recognition in conversation",
      "authors": [
        "Xiaoheng Zhang",
        "Yang Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "54",
      "title": "A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation",
      "authors": [
        "Xiaoheng Zhang",
        "Yang Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.732"
    },
    {
      "citation_id": "55",
      "title": "SWRR: feature map classifier based on sliding window attention and high-response feature reuse for multimodal emotion recognition",
      "authors": [
        "Ziping Zhao",
        "Tian Gao",
        "Haishuai Wang",
        "Bj√∂rn Schuller"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "56",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "Prompting visual-language models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    },
    {
      "citation_id": "57",
      "title": "A facial expressionaware multimodal multi-task learning framework for emotion recognition in multi-party conversations",
      "authors": [
        "Wenjie Zheng",
        "Jianfei Yu",
        "Rui Xia",
        "Shijin Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "58",
      "title": "Improving multimodal fusion with Main Modal Transformer for emotion recognition in conversation",
      "authors": [
        "Shihao Zou",
        "Xianying Huang",
        "Xudong Shen",
        "Hankai Liu"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    }
  ]
}