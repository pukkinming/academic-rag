{
  "paper_id": "2209.10170v1",
  "title": "Fv2Es: A Fully End2End Multimodal System For Fast Yet Effective Video Emotion Recognition Inference",
  "published": "2022-09-21T08:05:26Z",
  "authors": [
    "Qinglan Wei",
    "Xuling Huang",
    "Yuan Zhang"
  ],
  "keywords": [
    "Multimodal",
    "emotion",
    "inference",
    "fully end-toend"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the latest social networks, more and more people prefer to express their emotions in videos through text, speech, and rich facial expressions. Multimodal video emotion analysis techniques can help understand users' inner world automatically based on human expressions and gestures in images, tones in voices, and recognized natural language. However, in the existing research, the acoustic modality has long been in a marginal position as compared to visual and textual modalities. That is, it tends to be more difficult to improve the contribution of the acoustic modality for the whole multimodal emotion recognition task. Besides, although better performance can be obtained by introducing common deep learning methods, the complex structures of these training models always result in low inference efficiency, especially when exposed to highresolution and long-length videos. Moreover, the lack of a fully end-to-end multimodal video emotion recognition system hinders its application. In this paper, we designed a fully multimodal video-to-emotion system (named FV2ES) for fast yet effective recognition inference, whose benefits are threefold: (1) The adoption of the hierarchical attention method upon the sound spectra breaks through the limited contribution of the acoustic modality, and outperforms the existing models' performance on both IEMOCAP and CMU-MOSEI datasets; (2) the introduction of the idea of multi-scale for visual extraction while single-branch for inference brings higher efficiency and maintains the prediction accuracy at the same time; (3) the further integration of data pre-processing into the aligned multimodal learning model allows the significant reduction of computational costs and storage space. Source code will be available in https://github.com/qlwei89/FV2ES if this paper is accepted.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "ITH the rapid development of social networks, more people choose to use videos to express their emotions and opinions. The modern technique of multimodal emotion recognition can help understand these emotions through text, speech, facial expressions, gestures, postures, etc. In the existing multimodal-based video emotion recognition work, we observe three distinct limitations. Firstly, acoustic modality features are usually extracted by the OpenSmile toolkit  [1]  or RNN-based deep learning networks  [2, 3, 4] . Though these global spectrum features or coarse-grained audio information are useful for the overall multimodal recognition task, it seems that the contribution of the acoustic modality is relatively low as compared to visual and textual modalities, which affects the improvement of the video emotion recognition performance. Secondly, 5G networks allow people to easily record and share high-definition videos through their mobile phones anytime and anywhere. Meanwhile, in the era of self-media, the low threshold of video lease has significantly increased the number and the length of videos. However, although recent deep learning networks  [5, 6, 7, 8, 9]  adopted for the visual modality led to better performance, they cannot cope with the abovementioned computing and storage challenges due to the complex structures. Thirdly, throughout the entire field of multimodal video emotion recognition, the research has always stayed at the academic level. Most existing non-endto-end frameworks  [10, 11, 12]  own pipelines with combined models that aim to obtain better performance; however, this kind of slow and incoherent method hindered the application of multimodal video emotion recognition.\n\nIn this paper, we aim to address the above limitations by designing a fully multimodal video-to-emotion system for fast yet effective recognition inference, whose main flow and components are illustrated in Fig.  1 . In this system, the original video is taken as the input. The method of hierarchical attention is used to extract features for each spectral patch of the audio modality to enhance the effect of the audio modality. At the same time, we adopt multi-branch feature learning and single-branch inference structure, so that we can extract the frames' information of visual modality and simplify the inference model. For text modalities, we adopt Albert  [13]  to extract textual features. And we use the basic transformer  [14]  to obtain the visual and acoustic sequential information. Finally, multimodal fusion is performed through a feedforward network. Specifically, through sound spectrum segmentation, intrablock self-attention, and block aggregation processing, we extract the layered spectral features to obtain the internal relationship information of the audio spectrum. At the same time, for video frames, we adopt the structure of multi-branch feature learning and single-branch inference. During training, the advantages of multi-branch feature learning are used to extract the information of video frames. In the inference process, we choose the single-branch structure for prediction, which is simpler and more computationally efficient. We take their respective strengths and realize their connection through parameter migration, which can compress the computational loss and ensure the learning of frame features. Simultaneously, to realize the comprehensive connection between the input video and the procedure of emotion analysis, we integrate the data pre-processing into the multimodal emotion analysis model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "W",
      "text": "Fig.  1 . The data flow of the proposed V2EM.\n\nOur model improves prediction efficiency and promotes the feasibility of industrial application. Experimental results show that the processing of visual and acoustic perception enables our model to significantly outperform the existing multimodal emotion analysis models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Main Contributions And Innovations Of This Paper Include:",
      "text": "‚Ä¢ We successfully migrate the success of Transformer in Vision (ViT) to the audio modality. The hierarchical audio spectrum information breaks through the contribution limitation and effectively improves the performances on two public datasets.\n\n‚Ä¢ The idea of multi-scale visual feature extraction and single-branch reasoning is introduced to ensure the prediction accuracy and improve the efficiency of multimodal emotion analysis.\n\n‚Ä¢ The multimodal learning model that integrates video pre-processing and emotion analysis greatly reduces the computational cost and storage space, making it industrially applicable.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Over the past few years, multimodal emotion recognition has been a widely studied academic topic for various research tasks such as human-machine communication  [30] , action recognition  [31] , online advertising recommendation  [32]  and Video Q&A  [33, 34] . In Table  I , we show almost all of the existing popular multimodal emotion recognition research. From the \"Modality\" column, we can find that all of the multimodal emotion recognition methods considered both visual (V) and textual (T) modalities. It is worth noting that in some studies, researchers studied only V and T modalities while ignoring the acoustic (A) modality  [7, 9] . \"Effect\" in this table refers to the performance effect of three modalities on common datasets and evaluation standards, and it seems that the contribution of the acoustic modality was always the lowest. This kind of low contribution mostly embodies two aspects. On one hand, the emotion recognition performance of modality A is worst compared to modality V and T. For example, in HFU-BERT  [6] , VGGish & SoundNet were used to extract audio information and the acoustic modality played the auxiliary role (F1-scores on the CMU-MOSEI dataset equal 0.74, 0.72, 0.56 for modality V, T, A respectively). On the other hand, it seems more difficult to improve the performance of modality A in multimodal emotion recognition research. For instance, the ablation experiments in  [22]  show that accuracies were increased by 7.4%, 9.6% while only 6.7% on the CMU-MOSEI dataset for modalities V, T, and A respectively.\n\nIn addition, looking at the \"Visual processing\" column in Table  I , we can see that after the year 2015, almost all of the researchers tend to introduce deep learning networks (Cross-modal Transformer  [23] , ResNet50  [6] ) to obtain deeper visual features for better multimodal emotion prediction effect. However, deeper networks own more complex structures, leading to lower computing efficiency for inference and more demands for storage memories. Thus, deep learning inference models may not function well in the current 5G and self-media era, where videos with higher resolution and longer length are leased and shared. This practical problem has yet to be addressed in the existing research.\n\nMoreover, the \"System implementation\" column shows that there is still no attempt to build a multimodal video-to-emotion system to promote practical application, and that may be due to the non-end2end structures. Specifically, in  [10] , late fusion  [11, 12] , and later tensor fusion  [35] , manual feature learning or common pretrained deep learning models were usually combined  [7, 23] . This can result in the unaligned fusion relationship among the V, T, and A modalities in the training and inference procedure. Though Dai W et al.  [5]  made a breakthrough by realizing end-to-end aligned multimodal features learning, this model needed the data pre-processing module and thus it still has not reached the full consistency from data input to emotion prediction.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Methods",
      "text": "The designed video-to-emotion multimodal model (named V2EM) for video emotion recognition inference is illustrated in Fig.  1 . In this model, we input the original video including visual, textual and acoustic modalities. We use hierarchical attention to extract features for each spectral patch of the audio modality. And the RepVGG-based singlebranch inference module is introduced to obtain the frame information of visual modality. For text modalities, we adopt Albert  [13]  to extract features. Then, the basic transformer  [14]  is used to get the visual and acoustic sequential information. Finally, we perform the weighted multimodal fusion with the feedforward network. In the following sections, we give the details of these three main technological innovations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. The Hierarchical-Attention Spectrum Computing Module",
      "text": "In order to bridge the gap between the contributions to the final emotion prediction results of the acoustic modality and the other two modalities, and inspired by the outstanding performance of nesting transformers  [36]  applied in image classification, we design a novel hierarchical-attention spectrum computing module to get fine-grained spectral information. Fig.  2  illustrates the structure of the module proposed in this section. The right panel of this figure looks like a three-layer pyramid, where spectrum maps at various layers are included. And the left panel shows the procedure of generating hierarchical spectrum maps. Specifically, the initial spectrum is obtained through Melscale filter banks. The input of the hierarchical-attention spectrum computing module is a reshaped ùêªùêª √ó ùëäùëä sound spectrum, where ùêªùêª = ùëäùëä. And this input is divided into 16 patches of equal size ùëÜùëÜ √ó ùëÜùëÜ (where\n\n), and these little patches are called the first-layer spectrum maps. After segmentation, we use the Transformer Layer for each spectral patch. Inside the Transformer Layer, we firstly embed the patch of size ùëÜùëÜ √ó ùëÜùëÜ as a ùëëùëë-dimensional vector:\n\nwhere ùêºùêº refers to a patch size of ùëÜùëÜ √ó ùëÜùëÜ.\n\nThen, the basic transformer  [14]  is adopted on each ddimensional vector to extract local self-attention acoustic features. And the LN  [37]  and GELU  [38]  operations are performed on the features to get the output of the first layer:\n\nwhere ùëÇùëÇ 1 represents the output of the first layer, MSA is the multi-head attention of the basic transformer. As shown in Fig.  2 , every four spatially adjacent patches are merged into one block through a 3 √ó 3 CNN followed by LayerNorm  [37]  and a 3 √ó 3 max pooling layer. The aggregated four blocks are termed as the second-layer spectrum maps:\n\nwhere ùêºùêº 2 is the input of the second layer. With regard to the second layer, each block is again fed into the basic transformer  [14]  and then these four blocks are combined. The result is the input of the third layer ùêºùêº 3 , which is passed through the basic transformer to get the final fused spectrum map.\n\nOverall, we extract acoustic hierarchical spectral information through sound spectrum segmentation, intrapatch (block) self-attention, and patch (block)  aggregation. In this way, we take full advantage of the internal relationship of the acoustic data, making up for the limitations of the previous work in which the global and coarse-grained acoustic information is extracted.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. The Repvgg-Based Single-Branch Inference Module",
      "text": "In this sub-section, to make the whole multimodal emotion recognition system better adaptive to future videos in the wild with high resolution and long length, we attempt to deal with the problem of low inference efficiency due to the complex structures of common deep learning models and meanwhile we aim to guarantee the performance.\n\nFor multimodal emotion recognition tasks, we pioneer the use of the multi-branch feature learning and single-branch inference structure proposed in the latest RepVGG technology  [39] . In Fig.  3 , we show the training and the inference models adopted for the visual modality, whose structure is based on 3 √ó 3 convolution, 1 √ó 1 convolution, Identity, and ReLU activation layers  [40] . In the left panel of this figure, we can see that spatial-visual feature is learned mainly through multi-branch and multi-kernel convolutions. As shown in Fig.  3 , for the input image frame, we use three branches to extract multi-dimensional features, including the 3 √ó 3 convolution branch, the 1 √ó 1 convolution branch, and the Identify branch. Then the middle features are fused as the input of the ReLU layer  [40]  to obtain the final result.\n\nThe right panel of Fig.  3  shows the inference procedure, instead of the original clunky multi-branch structure, we choose a purer single-branch structure. And the middle panel shows the details of the process of parameter migration from training models to inference models, in which a three-channel input is taken as an example. Specifically, among the three branches in training models, only the 3 √ó 3 convolution branch is retained while the 1 √ó 1 convolution kernel is transformed to 3 √ó 3 convolution by zero padding. It is worth noting that each convolution here contains a BN layer  [41] .\n\nThe original RepVGG used a model architecture similar to ResNet in order to benchmark the SOTA model. However, the problem we face is that of improving the processing efficiency of video image frames when dealing with a lot of high-definition videos. Thus, we use the multi-branch training and single-branch prediction ideas of RepVGG to achieve this target. And we hope to improve inference efficiency with a simpler structure. Hence, we combine a 3 √ó 3 convolution, a 1 √ó 1 convolution and Identity as one layer, and finally use such six layers as the visual module structure for the target task.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. The Fully Video-To-Emotion System",
      "text": "Although some video sentiment analysis also performs preprocessing operations, they often choose to store modal information files after video processing. Then the model calls the modal information file for inference analysis. In this process, the memory and efficiency waste generated by storage, generation and invocation, we think, unnecessary. We want to solve the \"last mile\" problem by decreasing the unnecessary computational storage waste. Thus, we designed a \"fully video to emotion system\" (FV2ES), which directly interfaces the preprocessed modal information with the V2EM input in order to reduce the waste of previous systems.\n\nIn this subsection, we take the first step to think outside of the existing academic research on multimodal video emotion recognition. Fig.  4  illustrates the data flow of FV2ES, in which the whole process including data uploading, features extraction, and the final emotion prediction is coherent. In FV2ES, long videos are allowed to be taken as the input, and the complete dialogue texts and the audio Mel-spectrogram of the input videos are obtained firstly. At the same time, the image frames need to be preprocessed. Take the videos in the IEMOCAP dataset, for instance. Here the two-person dialogue videos are divided into one-person frames according to the characters. Long videos are too verbose and cause computational overhead. To solve this problem, we divide the whole video into several short video segments. At the same time, we use the timeline to align the data of the three modalities. We select the text, spectrum, and image frames in the same period and take these directly as input to V2EM. Then we can obtain the emotion prediction of this period through V2EM. Finally, the emotional prediction values of multiple short videos are taken as the emotional prediction result of the input video.\n\nBased on the Flask technology, we build FV2ES which is expected to enhance the industrial application value of multimodal video emotion analysis. Users can upload the videos to be analyzed at the front end of the platform while these modality data will be computed at the back end. The obtained emotional scores and the final prediction will be printed on our designed interface.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Experimental Environment And Datasets",
      "text": "The Jiutian artificial intelligence platform is chosen as the experimental platform for this experiment. This is an artificial intelligence innovation platform independently developed by China Mobile, providing open AI services from infrastructure to core capabilities. The experiments in this paper rely on the Tesla V100S-PCIE-32GB GPU and PyTorch v1.8.0.\n\nThere are two public datasets used in our experiments, including the IEMOCAP and the CMU-MOSEI datasets. The IEMOCAP dataset consists of multimodal data of three modalities of video, audio, and text transcription. We select six main categories from the original emotions: anger, happiness, excitement, sadness, frustration, and neutral. And to create a new split for the dataset, we randomly assign 70%, 10%, and 20% of the data to the training, validation, and test sets respectively.\n\nThe CMU-MOSEI dataset also consists of multimodal data of three modalities of vision, audio, and text. Six kinds of labels including happiness, sadness, anger, fear, disgust, and surprise were annotated for the videos. And the dataset contains 250 topics, 3837 videos, 23453 sentences, 1000 narrators, and the total duration reaches 65 hours.\n\nIn subsequent experiments, the same dataset partitioning method was also used for all the baselines.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Implementation Details",
      "text": "As to evaluation indicators, we use Accuracy and F1 score to evaluate our proposed model on the IEMOCAP dataset, while we take weighted accuracy (ùëäùëä ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥ . ) instead of standard accuracy on the CMU-MOSEI dataset. The formula for weighted accuracy isÔºö where ùëÄùëÄ represents total positives, ùëáùëáùëÄùëÄ is the number of true positives, ùêøùêø is total negatives, and ùëáùëáùêøùêø represents the number of true negatives.\n\nIn this paper, we use the following five state-of-the-art models in previous work as the baselines.\n\n‚Ä¢ LF-LSTM: Late fusion using LSTM.\n\n‚Ä¢ LSTMLF-TRANS: Late fusion using Transformer.\n\n‚Ä¢ EmoEmbs  [42] : Using a multimodal transferable model. And pre-trained word embeddings were used to represent textual features. And two mapping functions were built to transfer the embeddings into visual and auditory modalities. Then calculate the distance between the predicted and the target value for each modality, and make predictions based on the distance.\n\n‚Ä¢ MulT  [23] : The structure of the Cross-modal Transformer was used to construct the relationship between different modalities. After obtaining the multimodal fusion information, three sets of features were joined for prediction.\n\n‚Ä¢ FE2E  [5] : Extract visual and audio features using VGG16 and text features using Alert.\n\nMoreover, the Adam optimizer  [43]  is used to train each model. And the binary cross-entropy loss is adopted as the loss function. The hyperparameters such as the learning rate, the epoch and the batch size are set to 4.5e-6, 30 and 8 in our experiments.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Experimental Results And Analysis 1) Recognition Performance",
      "text": "This section demonstrates the advantages of the performance of our proposed V2EM for multimodal video emotion recognition. Comparative Experiments. In Table  II , we show the results of V2EM on the IEMOCAP dataset.\n\nCompared to baselines, the weighted accuracy of V2EM is outstanding (15.46% improved compared to LF-LSTM, 5.20% improved compared to LF-TRANS,10.9% improved compared to EmoEmbs, and 5.30% improved compared to MulT). Such excellent results confirm the superiority of the V2EM structure. Since FE2E performs best in the existing SOTA models and it used the same text processing method as V2EM, we choose FE2E as the reference model to observe the effects of V2EM in the 30 epochs training process. As shown in Fig.  5 , during 30 epochs, the performance of V2EM is significantly better than that of FE2E (1.22% improved), and the good performance is maintained stably. We further evaluate the results of V2EM on the CMU-MOSEI dataset as shown in Table  III  and Fig.  5 . We observe similar trends on this dataset. (10.78% improved compared to LF-LSTM, 9.05% improved compared to LF-TRANS, 5.70% improved compared to EmoEmbs, 4.50% improved compared to MulT and 1.16% improved compared to the best SOTA model FE2E  [5] ).\n\nAblation Experiments. To better explore the effect of the hierarchical-attention spectrum computing module and the RepVGG-based single-branch inference module on multimodal video emotion prediction, we conduct ablation experiments, in which the VGG16 model is used to process the visual and auditory data, and the Alert model is adopted to extract textual features. Then we use the hierarchicalattention spectrum computing module and the RepVGGbased single-branch inference module respectively on the basis of the basic model to observe the prediction performance.  As shown in Table  IV  and Table  V , the introduction of the audio module effectively improves the performance on these two public datasets. On the IEMOCAP dataset, ùëäùëä ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥ . in the V+A(Ours)+T model equals 0.8223, which is 0.38% improved compared to the baseline, and the F1 score is 0.36% improved compared to the baseline. Similarly, on the CMU-MOSEI dataset, the adoption of the audio module makes ùëäùëä ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥ . 1.14% improved. At the same time, the performance of the V+A(Ours)+T model on both datasets is significantly better than that of V(Ours)+A+T, indicating that the audio modality has a greater contribution to the overall multimodal emotion recognition results. Here, it can be seen that the effect of V(Ours)+A+T is not as good as that of baselines. We believe this is due to the relative simplicity of our six-layers visual structure compared to the VGG-16 structure used by the baseline. Thus, there is less performance improvement. However, our original intention of designing the RepVGG-based single-branch inference module is to bring efficiency improvements, which is demonstrated in the subsequent analysis. Thus, its lack of performance is something we can tolerate. Therefore, the above experiments show that the proposed V2EM using the hierarchical-attention spectrum computing module can solve the problem that the contribution of the audio modality is relatively low in the existing multimodal models.\n\nTo make further analysis of the reason for the good performance of the hierarchical-attention spectrum computing module proposed in section III-A, we show a visualization of audio attention maps in Fig.  6 . In this figure, we take the first audio clip of Ses01M_script03_2 in the IEMOCAP dataset as the example. The audio attention maps of each layer are extracted through the self-attention module of three layers. On the far left is the original Mel spectrogram. Reshape Mel Spectrum to get the input of the audio module, as shown in the \"Input\". After being processed by the first Transformer layer of the audio module, the 16 attention maps shown in \"Layer 1\" are obtained. The warm areas represent the focused regions of the attention layers. We can observe that the features of the hierarchical-attention spectrum computing module focus on more subtle internal relationships of audio features in early attention layers. Then, after Aggregation and the second Transformer layer, 4 audio attention maps are obtained as shown in \"Layer 2\". Its warm area becomes larger. At the same time, as shown in \"Layer 3\", the warm area in the attention map obtained after the third Transformer layer is larger. It can be seen that the information on the internal features of the final fusion sound spectrum can be obtained after hierarchical extraction. Therefore, we can say that the proposed audio module can extract more fine-grained, locally high and low audio spectral features, thus leading to the enhancement of the acoustic contribution to the overall multimodal emotion recognition performance.\n\n2) The Inference Efficiency of V2EM Comparative Experiments. In Table  II  and Table  III , we show the prediction speed of V2EM on the IEMOCAP dataset and the CMU-MOSEI dataset. V2EM is the proposed end-to-end model for multimodal video-toemotion prediction. And FE2E is the best end-to-end prediction model in the SOTAs. From these two tables, we can find that the computing efficiency of V2EM is outstanding (On the IEMOCAP dataset, 51.95% improved compared to FE2E while 21.42% improved on the CMU-MOSEI dataset).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Experiments. As Shown In Table Vi And",
      "text": "Table  VII , the application of the RepVGG-based singlebranch inference module effectively improves the inference efficiency on the two datasets. Specifically, on the IEMOCAP dataset, V(Ours)+A+T takes 249.42 seconds to test 1481 samples, and the improvement compared to the baseline is 35.76%. Similarly, on the CMU-MOSEI dataset, V(Ours)+A+T takes 297.62 seconds to test 4188 samples, and the improvement compared to the baseline is 20.92%. Overall, the processing of the visual branch greatly enhanced the model inference efficiency so that it can help cope with today's videos with high resolution and long length. The main reason is due to the function of the single-branch inference structure in the vision processing module proposed in section III-B.\n\nIn Table  VIII , the numbers of parameters involved in each modality of the SOTA FE2E model and our proposed V2EM model are given. It shows that for the visual modality, there are significantly fever parameters due to the adopted single-branch inference structure. And this can help us explain the good effect of the reduced computational complexity and the improved inference efficiency.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "3) The Inference Efficiency Of Fv2Es",
      "text": "On the basis of V2EM, the data pre-processing and the multimodal end-to-end learning model are integrated to achieve FV2ES, in which we dispense with the operations of saving and uploading the preprocessed data of various modalities, aiming to reduce the need for unnecessary data storage space.\n\nIn Table  IX , we compare the inference efficiency results of Pre+FE2E (using independent video preprocessing in advance and the baseline model for emotion prediction), Pre+V2EM (using independent video preprocessing plus our proposed V2EM model), and the designed FV2ES in this paper. It can be seen in Table  IX  that FV2ES greatly enhances the prediction efficiency (63.04% improved compared to Pre+V2EM). Combined with the comparison performances in TABLE II and TABLE III, we can conclude that the proposed FV2ES brings fast yet effective video emotion recognition inference.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion And Discussion",
      "text": "In this paper, we transfer the success of VIT to the audio modality for the multimodal emotion recognition task. Taking layered audio spectrum information into consideration can effectively improve the results of emotion classification, and meanwhile it can also promote the contribution of the acoustic modality compared to the visual and textual modalities for the overall multimodal task. In addition, we introduce the RepVGG-based single-branch inference module for visual frames. In this module, the \"big and whole\" instead of \"small and fragmented\" reasoning structure brings much higher computing efficiency and requires less storage space. And therefore, we expect this vision module to help solve the computational and storage problem caused by a large amount of long-length and high-resolution videos in the 5G and self-media era. Moreover, we designed a fully multimodal video-to-emotion system in which the data pre-processing and the multimodal end-to-end learning model are integrated. This operation leads to the further enhancement of the inference efficiency. Above all, the proposed FV2ES in this paper for fast yet effective video emotion recognition inference is valuable in academic study and also applicable for practical industry.\n\nHowever, FV2ES still has some shortcomings. Fig.  7  illustrates the results of the confusion matrix. We can observe that the method proposed in this paper has the best recognition performance on emotion \"excitement\". The main reason is that the excitement is more often expressed in speech and intonation, and the proposed audio module functions well in extracting fine-grained acoustic information (as shown in the confusion matrix A modality). But for similar emotional expressions like happy and neutral, people express subtle differences in expressions, dialogues, and voices, and our model does not perform well in discriminating these emotions.\n\nAt the same time, the video scenes in the public IEMOCAP and CMU-MOSEI datasets used in this paper are relatively pure instead of in the wild. Specifically, the environment in these videos is quiet and there is almost no noise. Furthermore, there are easily recognizable core characters in the frames. Such clean datasets are still far from the real world. In the future, we will optimize V2EM to improve the performance and migrate FV2ES to short video data in social networks. And we are going to take the visual processing of complex backgrounds, noisy audio, and random text processing into account, so as to promote more links between the multimodal video emotion analysis academic research and industry application.",
      "page_start": 9,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In this system, the",
      "page": 1
    },
    {
      "caption": "Figure 1: The data flow of the proposed V2EM.",
      "page": 2
    },
    {
      "caption": "Figure 1: In this model, we input the original video",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the structure of the module",
      "page": 3
    },
    {
      "caption": "Figure 2: , every four spatially adjacent patches are merged into",
      "page": 3
    },
    {
      "caption": "Figure 2: The structure of the hierarchical-attention spectrum computing module.",
      "page": 4
    },
    {
      "caption": "Figure 3: Illustration of the method of the RepVGG-based single-branch",
      "page": 4
    },
    {
      "caption": "Figure 3: , we show the training and the",
      "page": 4
    },
    {
      "caption": "Figure 3: , for the input image frame, we use three",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the inference procedure,",
      "page": 4
    },
    {
      "caption": "Figure 4: The architecture of FV2ES.",
      "page": 5
    },
    {
      "caption": "Figure 4: illustrates the data flow of FV2ES,",
      "page": 5
    },
    {
      "caption": "Figure 5: , during 30 epochs, the performance of V2EM",
      "page": 6
    },
    {
      "caption": "Figure 5: We observe similar trends on",
      "page": 6
    },
    {
      "caption": "Figure 5: The performance changes of FE2E (year 2021, the best SOTA model), V2EM (ours) on the IEMOCAP dataset and the CMU-MOSEI dataset during",
      "page": 7
    },
    {
      "caption": "Figure 6: The audio attention maps. We take the first audio clip of Ses01M_script03_2 in the IEMOCAP dataset as the example. And we show the attention map",
      "page": 7
    },
    {
      "caption": "Figure 6: In this figure,",
      "page": 8
    },
    {
      "caption": "Figure 7: Confusion matrices for V2EM and the prediction results for each of the three modalities on the IEMOCAP dataset. There are 1481 test samples with",
      "page": 9
    },
    {
      "caption": "Figure 7: illustrates the results of the confusion matrix. We can",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reddy M K, Alku P, Rao K S. Detection of specific language": "impairment in children using glottal source features[J]. IEEE Access,"
        },
        {
          "Reddy M K, Alku P, Rao K S. Detection of specific language": "2020, 8: 15273-15279."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[13] Lan Z, Chen M, Goodman S, et al. Albert: A lite bert for self-": "",
          "Column_2": "supervised learning of language representations[J]. arXiv preprint"
        },
        {
          "[13] Lan Z, Chen M, Goodman S, et al. Albert: A lite bert for self-": "",
          "Column_2": "arXiv:1909.11942, 2019."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Detection of specific language impairment in children using glottal source features[J]",
      "authors": [
        "M K Reddy",
        "P Alku",
        "K Rao"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "2",
      "title": "An RNN-based prosodic information synthesizer for Mandarin text-to-speech[J]",
      "authors": [
        "H Chen",
        "S Hwang",
        "Y Wang"
      ],
      "year": "1998",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "3",
      "title": "Audio-Visual Speech Recognition System Using Recurrent Neural Network",
      "authors": [
        "Y Goh",
        "H Lau",
        "K Lee"
      ],
      "year": "2019",
      "venue": "th International Conference on Information Technology (InCIT)"
    },
    {
      "citation_id": "4",
      "title": "Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss",
      "authors": [
        "Q Zhang",
        "H Lu",
        "H Sak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu"
      ],
      "year": "2021",
      "venue": "Multimodal end-to-end sparse model for emotion recognition",
      "arxiv": "arXiv:2103.09666"
    },
    {
      "citation_id": "6",
      "title": "Multimodal Emotion Recognition Fusion Analysis Adapting BERT With Heterogeneous Feature Unification[J]",
      "authors": [
        "S Lee",
        "D K Han",
        "H Ko"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "Multimodal Machine Translation Enhancement by Fusing Multimodal-attention and Fine-grained Image Features",
      "authors": [
        "L Li",
        "T Tayir"
      ],
      "year": "2021",
      "venue": "2021 IEEE 4th International Conference on Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "8",
      "title": "Multimodal local-global attention network for affective video content analysis",
      "authors": [
        "Y Ou",
        "Z Chen",
        "F Wu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "9",
      "title": "Cross-modal complementary network with hierarchical fusion for multimodal sentiment classification",
      "authors": [
        "C Peng",
        "C Zhang",
        "X Xue"
      ],
      "year": "2021",
      "venue": "Tsinghua Science and Technology"
    },
    {
      "citation_id": "10",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "L Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "11",
      "title": "Select-additive learning: Improving generalization in multimodal sentiment analysis",
      "authors": [
        "H Wang",
        "A Meghawat",
        "L Morency"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "12",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "13",
      "title": "Albert: A lite bert for selfsupervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman"
      ],
      "year": "2019",
      "venue": "Albert: A lite bert for selfsupervised learning of language representations",
      "arxiv": "arXiv:1909.11942"
    },
    {
      "citation_id": "14",
      "title": "Attention is all you need[J]",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "Multimodal human emotion/expression recognition",
      "authors": [
        "L S Chen",
        "T S Huang",
        "T Miyasato"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "16",
      "title": "Multimodal sentiment analysis of spanish online videos[J]",
      "authors": [
        "V P Rosas",
        "R Mihalcea",
        "L Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "17",
      "title": "Deep multimodal learning for affective analysis and retrieval[J]",
      "authors": [
        "L Pang",
        "S Zhu",
        "C Ngo"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "19",
      "title": "Convolutional MKL based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria"
      ],
      "year": "2016",
      "venue": "IEEE 16th international conference on data mining (ICDM)"
    },
    {
      "citation_id": "20",
      "title": "A combined rule-based & machine learning audio-visual emotion recognition approach",
      "authors": [
        "K Seng",
        "L Ang",
        "C Ooi"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Deep spatio-temporal features for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan"
      ],
      "year": "2017",
      "venue": "Deep spatio-temporal features for multimodal emotion recognition"
    },
    {
      "citation_id": "22",
      "title": "Multimodal sentiment analysis via RNN variants",
      "authors": [
        "A Agarwal",
        "A Yadav",
        "D Vishwakarma"
      ],
      "year": "2019",
      "venue": "Data Science & Engineering"
    },
    {
      "citation_id": "23",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y H H Tsai",
        "S Bai",
        "P P Liang"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences"
    },
    {
      "citation_id": "24",
      "title": "An audio-video deep and transfer learning framework for multimodal emotion recognition in the wild",
      "authors": [
        "D Dresvyanskiy",
        "E Ryumina",
        "H Kaya"
      ],
      "year": "2020",
      "venue": "An audio-video deep and transfer learning framework for multimodal emotion recognition in the wild",
      "arxiv": "arXiv:2010.03692"
    },
    {
      "citation_id": "25",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Z Sun",
        "P Sarma",
        "W Sethares"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Cross Attentional Audio-Visual Fusion for Dimensional Emotion Recognition",
      "authors": [
        "G Praveen",
        "E Granger",
        "P Cardinal"
      ],
      "venue": "/2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "27",
      "title": "Real-Time Video Emotion Recognition based on Reinforcement Learning and Domain Knowledge",
      "authors": [
        "K Zhang",
        "Y Li",
        "J Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "28",
      "title": "A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition",
      "authors": [
        "Praveen Rajasekar",
        "Carneiro De Melo",
        "W Ullah"
      ],
      "year": "2022",
      "venue": "A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition",
      "arxiv": "arXiv:2203.14779"
    },
    {
      "citation_id": "29",
      "title": "Mt-Tcct: Multi-Task Learning for Multimodal Emotion Recognition",
      "authors": [
        "Y Wang",
        "Z Chen",
        "S Chen"
      ],
      "venue": "Mt-Tcct: Multi-Task Learning for Multimodal Emotion Recognition"
    },
    {
      "citation_id": "30",
      "title": "Novel Bidirectional Multimodal System for Affective Human-Robot Engagement",
      "authors": [
        "T Applewhite",
        "V J Zhong",
        "R Dornberger"
      ],
      "venue": "IEEE Symposium Series on Computational Intelligence (SSCI)"
    },
    {
      "citation_id": "31",
      "title": "Spatiotemporal multimodal learning with 3D CNNs for video action recognition",
      "authors": [
        "H Wu",
        "X Ma",
        "Y Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "32",
      "title": "Predicting content similarity via multimodal modeling for video-in-video advertising",
      "authors": [
        "X Song",
        "B Xu",
        "Y Jiang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "33",
      "title": "Long-term video question answering via multimodal hierarchical memory attentive networks",
      "authors": [
        "T Yu",
        "J Yu",
        "Z Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "34",
      "title": "Video dialog via multi-grained convolutional self-attention context multi-modal networks",
      "authors": [
        "M Gu",
        "Z Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "35",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "36",
      "title": "Aggregating nested transformers",
      "authors": [
        "Z Zhang",
        "H Zhang",
        "L Zhao"
      ],
      "year": "2021",
      "venue": "Aggregating nested transformers",
      "arxiv": "arXiv:2105.12723"
    },
    {
      "citation_id": "37",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "38",
      "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units"
    },
    {
      "citation_id": "39",
      "title": "Repvgg: Making vgg-style convnets great again",
      "authors": [
        "X Ding",
        "X Zhang",
        "N Ma"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Deep sparse rectifier neural networks",
      "authors": [
        "X Glorot",
        "A Bordes",
        "Y Bengio"
      ],
      "year": "2011",
      "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings"
    },
    {
      "citation_id": "41",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "PMLR"
    },
    {
      "citation_id": "42",
      "title": "Modality-transferable emotion embeddings for low-resource multimodal emotion recognition",
      "authors": [
        "W Dai",
        "Z Liu",
        "T Yu"
      ],
      "year": "2020",
      "venue": "Modality-transferable emotion embeddings for low-resource multimodal emotion recognition",
      "arxiv": "arXiv:2009.09629"
    },
    {
      "citation_id": "43",
      "title": "A method for stochastic optimization",
      "authors": [
        "D P Kingma",
        "J Ba",
        "Adam"
      ],
      "year": "2014",
      "venue": "A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    }
  ]
}