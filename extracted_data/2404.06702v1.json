{
  "paper_id": "2404.06702v1",
  "title": "What Is Learnt By The Learnable Front-End (Leaf)? Adapting Per-Channel Energy Normalisation (Pcen) To Noisy Conditions",
  "published": "2024-04-10T03:19:03Z",
  "authors": [
    "Hanyu Meng",
    "Vidhyasaharan Sethu",
    "Eliathamby Ambikairajah"
  ],
  "keywords": [
    "learnable audio front-end",
    "adaptive front-end",
    "pre-channel energy normalization",
    "speech signal classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "There is increasing interest in the use of the LEArnable Frontend (LEAF) in a variety of speech processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting, speech-based emotion recognition and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt. Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions. This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The speech front-end is a crucial component in speech signal classification systems, and has been the focus of research for many decades. The Mel filterbank  [1]  leads to representations inspired by our understanding of human perception, and is arguably the most widely used front-end across a range of different speech applications. With the advent of deep learning systems, there has been an interest in end-to-end systems that attempt to learn the optimal transformations to extract information from speech waveforms for any target application  [2] [3] [4] [5] . More recently, focus has shifted to learnable front-ends that constraints the architecture of the front-end but allows the model parameters to be learnt in conjunction with the backend  [6] [7] [8] [9] [10] [11] [12] . They have been employed in a wide range of applications, including but not limited to speaker verification  [7] , spoofing detection  [9] , and emotion recognition  [8] .\n\nA number of general-purposed learnable front-ends such as Time-Domain Filterbank (TD-Fbank)  [12] , SincNet  [10] , CGCNN  [11]  and LEArnable Front-end (LEAF)  [6]  have been developed recently. Among these, LEAF stands out as having fewer parameters and higher reported accuracy in tasks such as audio events classification. Its universal representation generated from raw speech signals has also made it applicable in other audio-related tasks, such as medical acoustic signal feature learning  [13] , analog acoustic recognition  [14] , bird activity detection  [15] , speaker verification  [16] , and limitedvocabulary speech recognition tasks  [17] . Despite these successes, little is known about what is exactly learnt by LEAF from speech signals.\n\nAnalyses of the LEAF model have suggested that one of its components, Per-Channel Energy Normalization (PCEN) plays an important role in effectively compensating for the impact of environmental noise on speech intelligibility  [18] [19] [20] [21] . PCEN has also been widely applied in acoustic scene classification  [22]  and long-distance bioacoustic event detection  [23] . However, beyond this, there have been limited analyses and insights in the operation of LEAF. To address this shortcoming, we investigate which components of LEAF learn during model training and to what extent.\n\nIn this paper, we demonstrate that only the PCEN layer of the broader LEAF model learns during training and there is no observable change in the characteristics of any of the other components away from their initial values. Following this, we leverage our findings to develop a noise adaptation strategy whereby only the PCEN layer of LEAF is adapted using a small amount of noisy data to enable the LEAF model to be used under noisy conditions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "What Is Learnt By Leaf?",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Leaf",
      "text": "LEAF is a general-purpose audio front-end designed for audio event classification  [6] . It mainly comprises three learnable and one non-learnable elements: Spectral Decomposition, Energy Estimation (non-learnable), Smoothing, and Dynamic Range Compression. As depicted in Figure  1 , a frame of speech with M samples passes through a parallel filterbank of N Gabor filters  [24] , which is initialised to be equally spaced along the mel-frequency scale. During training, both the centre frequencies (fi) and bandwidths (BWi) of all filters can be learnt. The filterbank is followed by energy estimation implemented by a sample-wise squaring operation, which in turn is smoothed using a low pass filter. The low pass filtering is implemented as a pooling operation comprising of a Gaussian Low-pass Filters (LPF), an approach that has been shown to be effective with 2D features  [25] . The learnable parameter is the standard deviation σi (equivalently its bandwidth) of the Gaussian LPF in each frequency channel.\n\nFinally, the per channel energy compression (PCEN) acts as an input energy level-dependent gain controller that computes appropriate gains to enhance or attenuate signals in each frequency channel  [26] . The PCEN layer for the i th frequency channel takes as input the smoothed energy estimates produced by the low pass filter layer, E[n, i], and is formulated as:\n\nwhere M [n, i] is the smoothed version of the input represen-tation achieved by a first-order infinite impulse response (IIR) filter as expressed in Equation  2 with a learnable smoothing factor, si.\n\nAs illustrated in Figure  1 , the PCEN layer has four learnable parameters (si, αi, δi, γi) per frequency channel.\n\nFigure  1 : An overview of LEAF (the input speech frame contains M samples).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Analysis Of Leaf Parameters: Experimental Setup",
      "text": "To investigate what is learnt by LEAF, we train LEAF on three of the tasks where its performance has been perviously reported: emotion recognition, keyword spotting, and language identification  [6] . The tasks were chosen to represent a range of speech processing applications as well as prediction accuracies ranging from quite high to somewhat low (refer Table  1 ). Following this, we compare the spectral and compression characteristics of the three learnable components of LEAF before and after training.\n\nFor these analyses, we replicate the dataset settings reported in  [27] . The keyword spotting task is trained on Speech-CommandsV2 dataset  [28] , emotion recognition experiments were carried out on CREMA-D dataset  [29] , and we use Voxforge  [30]  for language identification. All three datasets are sampled at 16kHz. For all three tasks, we utilise Efficent-NetB0  [31]  as the back-end, which is a lightweight Convolution Neural Network (CNN) based classification network.\n\nPrior to training, the LEAF model was initialised as described in  [6] . Specifically, we initialised the 40 Gabor filters using the mel-scale and set the input window size to 401 samples with a hop size of 160 samples, corresponding to 25ms for audio sampled at 16kHz. The Gaussian LPFs were initialised with a standard deviation of 0.4 for all channels. The initial values of the PCEN layer parameters in each frequency channel were set to α = 0.96, γ = 2, δ = 2, and s = 0.04.\n\nFor training the models, we utilised the ADAM optimiser with a fixed learning rate of 10 -4 , and employed mini-batches of size 256. We set the input sequence length for SpeechCom-mandsV2 and Voxforge to 1 second, and for CREMA-D as 3 seconds, based on the audio files durations in each dataset. To ensure consistent loudness range across different recordings, we rescaled the raw speech signals to a range between 15 dB Sound Pressure Level (SPL) and 30 dB SPL. In the test phase, we adopted the approach in  [27] , and compute predictions for non-overlapping one-second segments and averaging the logits across the entire recording.\n\nTable  1  presents the test accuracy for all tasks with four different LEAF model settings. The \"Untrained\" settings indicate that all parts of LEAF were frozen and the initial values were not updated during training. The \"Filter Trained\" settings indicate that Gabor filters and Gaussian low pass filters were set to be trainable, and their parameters would be updated during model training (but the PCEN layer parameters would not be updated). Conversly, the \"PCEN Trained\" settings refer to the condition where only the PCEN layer parameters were trained and the Gabor filter and Gaussian low pass filter parameters were kept unchanged from their initial values. Finally, \"Fully Trained\" settings refer to the standard setting where all parameters are trainable.\n\nFrom the table, the first interesting observation that stands out is that there is little difference between the various trained and untrained versions of LEAF across all three tasks. It is worth noting that for the emotion recognition task, where the untrained LEAF shows the highest accuracy, we used the data partitioning reported in  [27] , whereby the data was partitioned by shuffling speaker groups and segmenting the data to ensure speaker independence in each partition. For the other two speech tasks, we used the data partitions as per the original dataset release. These results prompt the question \"What is learnt by the LEAF model?\". Specifically, through our analyses, we aim to answer the question: Which element of the model has undergone the most significant changes as a result of the training?",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Analysis Of Leaf Parameters: Results",
      "text": "Extracting the weights from the fully trained and untrained LEAF models, we compute the centre frequencies and bandwidths of the Gabor filters as well as magnitude responses of the Gaussian low pass filters and compare them to each other (trained vs untrained across all three tasks). We also reconstruct the compression function of the PCEN layers and compare them to each other. These comparisons are shown in Figure  2 (a)-2(f). a) Gabor Filterbank: Figures  2(a ) and 2(b) represent the changes in centre frequencies and bandwidths of each filter of the 40 Gabor filters across three tasks as well as the initial values. No appreciable deviations from the intial values can be observed for any of the trained models in any of the three tasks. These results strongly indicate that the initial Gabor filters may be optimal and learning does not help.\n\nb) Gaussian Lowpass Filterbank: In Figure  2 (c), we plot the frequency response of all 40 Gaussian lowpass filters for all three tasks. Once again, there appears to be no appreciable deviation from the initial values in the pass band of the low pass filters. This again suggests there may be no benefit to learning these filters. Taken together these results suggest that of three potentially learnable components of the LEAF model, the Gabor filterbank for spectral decomposition, the Gaussian low pass filter for energy smoothing and the PCEN which offers dynamic range compression, only the PCEN layers appear to be actually learning anything. Consequently, constraining the learning to only this layer of the LEAF model would significantly cut down the number of trainable parameters in the model, leading to more efficient learning. This in turn suggests a noise adaptation scheme involving a small set of noisy speech samples might enable a LEAF model trained on clean speech to be employed in noisy conditions. This hypothesis is explored in the rest of the paper.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Adapting Pcen To Noisy Environments",
      "text": "We explore the hypothesis that adapting or tuning the PCEN layer in LEAF can enhance the accuracy of speech classification in noisy environments using a limited amount of noisy adaptation data. To test this hypothesis, we compare the performance of a system with a PCEN-adapted LEAF model to that of one trained on clean speech. For reference we also include a model trained entirely on a large amount of noisy data in the compari-son 1  .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Setups",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Partition",
      "text": "Of the three speech procesing tasks, we choose the emotion recognition task to test the proposed PCEN adaptation scheme since it had the lowest accuracy. The CREMA-D dataset for speech emotion recognition consists of 91 speakers and 6 emotions  [29] , with each speaker having an almost equal number of utterances. When partitioning CREMA-D for this set of experiments, we use different partitions from those used in section 2.2. Specifically, we split the data based on sentences to ensure that each partition contained utterances from each speaker in order to minimise the impact of speaker variability on the results. As illustrated in Figure  3 , we used 9 sentences for training, 1 sentence for validation, and the remaining 2 sentences for testing. Consequently, the training set contained 5811 recordings, validation set contained 545, and the test set contained 1086. For adaptation, we selected one sentence from the training set (comprising of 546 recordings) as the adaptation data to which we add different types of noise (white noise and babble noise) at different Signal-to-Noise Ratios (SNRs).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "For this experiment, we compared four models that had the same structure as the system used in the experiments reported in section 2.2, but with varying training setups as illustrated in",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "To verify the hypothesis proposed at the beginning of Section 3, we tested the adaptation of PCEN using both stationary and non-stationary noise.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gaussian Noise Adaptation",
      "text": "The left pane in Figure  5  shows the classification accuracy for all four models tested by adding different levels of Gaussian noise to the clean test data in order to obtain SNR in the range of 0 dB to 20 dB. The results suggest that training the model with Gaussian noise helps the model learn the pattern of noise and improves its robustness. Further, the models trained on only clean data perform poorly when exposed to Gaussian noise. However, after adapting the PCEN layer with a small amount of noisy data, the impact of noise on accuracy can be somewhat mitigated as can be seen by comparing the performance Before Adaptation (BA model) to that after adaptation (PA model).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Babble Noise Adaptation",
      "text": "We also repeated the above experiment using babble noise instead of white noise. To simulate babble noise, we followed the data augmentation approach used in  [32]  and used the MU-SAN dataset, which contains 60 hours of speech from 12 languages  [33] . Specifically, we randomly selected three speech recordings from MUSAN, mixed them together, and added them to the clean signal to simulate babble noise with SNR ranging from 0 dB to 20 dB. The right pane in Figure  5  shows how the accuracy of the four models changes under different levels of babble noise.\n\nThe results suggest that training with noisy data is not as effective under babble noise conditions. This might be due to the greater similarity between noise and speech in this case. However, we can observe from the graph that adapting PCEN with babble noise may be quite effective in allowing the model to be used under noisy conditions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we sought to answer the question 'What is learnt by LEAF?', by comparing the extent of change in the characteristics of the different learnable components of the LEAF model from their initial values prior to training. This analysis was repeated on multiple speech processing tasks, and consistently our analyses revealed that only the PCEN layer changes in response to training. The filterbank and low pass filters employed for spectral decomposition and spectral energy smoothing remained unchanged for all three tasks. This suggests that the actual learning in the LEArnable Front-end occurs within a much lower dimensional subspace of the parameter space of the model. Following this, we developed a model adaptation scheme constrained to this subspace (PCEN layer only) using a small amount of noisy training data to adapt a LEAF trained on clean speech to operate more effectively in noisy conditions.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , a frame of speech",
      "page": 1
    },
    {
      "caption": "Figure 1: , the PCEN layer has four learnable",
      "page": 2
    },
    {
      "caption": "Figure 1: An overview of LEAF (the input speech frame contains",
      "page": 2
    },
    {
      "caption": "Figure 2: (c), we plot",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualising learnt information of LEAF across three tasks.",
      "page": 3
    },
    {
      "caption": "Figure 3: , we used 9 sentences for training, 1",
      "page": 3
    },
    {
      "caption": "Figure 4: These models are:",
      "page": 3
    },
    {
      "caption": "Figure 3: Data partitions in the CREMA-D dataset for PCEN",
      "page": 4
    },
    {
      "caption": "Figure 4: Overview of the PCEN adaptation experimental setup.",
      "page": 4
    },
    {
      "caption": "Figure 5: shows the classification accuracy for",
      "page": 4
    },
    {
      "caption": "Figure 5: Accuracy of PCEN adaptation experiments on the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: ). Following",
      "page": 2
    },
    {
      "caption": "Table 1: presents the test accuracy for all tasks with four dif-",
      "page": 2
    },
    {
      "caption": "Table 1: Classification accuracy of LEAF models (mean ±",
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The relation of pitch to frequency: A revised scale",
      "authors": [
        "S Stevens",
        "J Volkmann"
      ],
      "year": "1940",
      "venue": "The American Journal of Psychology"
    },
    {
      "citation_id": "3",
      "title": "Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation",
      "authors": [
        "Y Luo",
        "N Mesgarani"
      ],
      "year": "2019",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "4",
      "title": "On the comparison of popular end-to-end models for large scale speech recognition",
      "authors": [
        "J Li",
        "Y Wu",
        "Y Gaur",
        "C Wang",
        "R Zhao",
        "S Liu"
      ],
      "year": "2020",
      "venue": "On the comparison of popular end-to-end models for large scale speech recognition",
      "arxiv": "arXiv:2005.14327"
    },
    {
      "citation_id": "5",
      "title": "Towards end-to-end speech recognition with recurrent neural networks",
      "authors": [
        "A Graves",
        "N Jaitly"
      ],
      "year": "2014",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "6",
      "title": "Joint speaker encoder and neural back-end model for fully end-toend automatic speaker verification with multiple enrollment utterances",
      "authors": [
        "C Zeng",
        "X Miao",
        "X Wang",
        "E Cooper",
        "J Yamagishi"
      ],
      "year": "2022",
      "venue": "Joint speaker encoder and neural back-end model for fully end-toend automatic speaker verification with multiple enrollment utterances",
      "arxiv": "arXiv:2209.00485"
    },
    {
      "citation_id": "7",
      "title": "Leaf: A learnable frontend for audio classification",
      "authors": [
        "N Zeghidour",
        "O Teboul",
        "F Quitry",
        "M Tagliasacchi"
      ],
      "year": "2021",
      "venue": "Leaf: A learnable frontend for audio classification",
      "arxiv": "arXiv:2101.08596"
    },
    {
      "citation_id": "8",
      "title": "Learnable sparse filterbank for speaker verification",
      "authors": [
        "J Peng",
        "R Gu",
        "L Mošner",
        "O Plchot",
        "L Burget",
        "J Černockỳ"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "9",
      "title": "Multimodal transformer with learnable frontend and self attention for emotion recognition",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Fastaudio: A learnable audio front-end for spoof speech detection",
      "authors": [
        "Q Fu",
        "Z Teng",
        "J White",
        "M Powell",
        "D Schmidt"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "12",
      "title": "Cgcnn: Complex gabor convolutional neural network on raw speech",
      "authors": [
        "P.-G Noé",
        "T Parcollet",
        "M Morchid"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Learning filterbanks from raw speech for phone recognition",
      "authors": [
        "N Zeghidour",
        "N Usunier",
        "I Kokkinos",
        "T Schaiz",
        "G Synnaeve",
        "E Dupoux"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Deep feature learning for medical acoustics",
      "authors": [
        "A Poirè",
        "F Simonetta",
        "S Ntalampiras"
      ],
      "year": "2022",
      "venue": "Artificial Neural Networks and Machine Learning-ICANN 2022: 31st International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "15",
      "title": "Peaf: Learnable power efficient analog acoustic features for audio recognition",
      "authors": [
        "B Bergsma",
        "M Yang",
        "M Cernak"
      ],
      "year": "2021",
      "venue": "Peaf: Learnable power efficient analog acoustic features for audio recognition",
      "arxiv": "arXiv:2110.03715"
    },
    {
      "citation_id": "16",
      "title": "Learnable acoustic frontends in bird activity detection",
      "authors": [
        "M Anderson",
        "N Harte"
      ],
      "year": "2022",
      "venue": "Learnable acoustic frontends in bird activity detection",
      "arxiv": "arXiv:2210.00889"
    },
    {
      "citation_id": "17",
      "title": "Learnable frequency filters for speech feature extraction in speaker verification",
      "authors": [
        "J Li",
        "Y Tian",
        "T Lee"
      ],
      "year": "2022",
      "venue": "Learnable frequency filters for speech feature extraction in speaker verification",
      "arxiv": "arXiv:2206.07563"
    },
    {
      "citation_id": "18",
      "title": "Learnable filter-banks for cnn-based audio applications",
      "authors": [
        "H Peic Tukuljac",
        "B Ricaud",
        "N Aspert",
        "L Colbois"
      ],
      "year": "2022",
      "venue": "Proceedings of the Northern Lights Deep Learning Workshop 2022"
    },
    {
      "citation_id": "19",
      "title": "Per-channel energy normalization: Why and how",
      "authors": [
        "V Lostanlen",
        "J Salamon",
        "M Cartwright",
        "B Mcfee",
        "A Farnsworth",
        "S Kelling",
        "J Bello"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "20",
      "title": "Sound event detection in urban audio with single and multi-rate pcen",
      "authors": [
        "C Ick",
        "B Mcfee"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Realtime denoising and dereverberation wtih tiny recurrent u-net",
      "authors": [
        "H.-S Choi",
        "S Park",
        "J Lee",
        "H Heo",
        "D Jeon",
        "K Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Learning to detect dysarthria from raw speech",
      "authors": [
        "J Millet",
        "N Zeghidour"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Domain generalization on efficient acoustic scene classification using residual normalization",
      "authors": [
        "B Kim",
        "S Yang",
        "J Kim",
        "S Chang"
      ],
      "year": "2021",
      "venue": "Domain generalization on efficient acoustic scene classification using residual normalization",
      "arxiv": "arXiv:2111.06531"
    },
    {
      "citation_id": "24",
      "title": "Longdistance detection of bioacoustic events with per-channel energy normalization",
      "authors": [
        "V Lostanlen",
        "K Palmer",
        "E Knight",
        "C Clark",
        "H Klinck",
        "A Farnsworth",
        "T Wong",
        "J Cramer",
        "J Bello"
      ],
      "year": "2019",
      "venue": "Longdistance detection of bioacoustic events with per-channel energy normalization",
      "arxiv": "arXiv:1911.00417"
    },
    {
      "citation_id": "25",
      "title": "Theory of communication. part 1: The analysis of information",
      "authors": [
        "D Gabor"
      ],
      "year": "1946",
      "venue": "Journal of the Institution of Electrical Engineers-part III: radio and communication engineering"
    },
    {
      "citation_id": "26",
      "title": "Making convolutional networks shift-invariant again",
      "authors": [
        "R Zhang"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "27",
      "title": "Trainable frontend for robust and far-field keyword spotting",
      "authors": [
        "Y Wang",
        "P Getreuer",
        "T Hughes",
        "R Lyon",
        "R Saurous"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Efficientleaf: A faster learnable audio frontend of questionable use",
      "authors": [
        "J Schlüter",
        "G Gutenbrunner"
      ],
      "year": "2022",
      "venue": "2022 30th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "29",
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "authors": [
        "P Warden"
      ],
      "year": "2018",
      "venue": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "arxiv": "arXiv:1804.03209"
    },
    {
      "citation_id": "30",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "31",
      "title": "Multiclass language identification using deep learning on spectral images of audio signals",
      "authors": [
        "S Revay",
        "M Teschke"
      ],
      "year": "2019",
      "venue": "Multiclass language identification using deep learning on spectral images of audio signals",
      "arxiv": "arXiv:1905.04348"
    },
    {
      "citation_id": "32",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "33",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "34",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "D Snyder",
        "G Chen",
        "D Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    }
  ]
}