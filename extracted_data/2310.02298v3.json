{
  "paper_id": "2310.02298v3",
  "title": "Prompting Audios Using Acoustic Properties For Emotion Representation",
  "published": "2023-10-03T13:06:58Z",
  "authors": [
    "Hira Dhamyal",
    "Benjamin Elizalde",
    "Soham Deshmukh",
    "Huaming Wang",
    "Bhiksha Raj",
    "Rita Singh"
  ],
  "keywords": [
    "Emotion Audio Retrieval",
    "EAR",
    "Speech Emotion Recognition",
    "SER",
    "contrastive language-audio pre-training",
    "acoustic properties",
    "prompt generation",
    "prompt augmentation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are usually described using discrete labels like 'angry', or 'happy' following psychological models like the Plutchik wheel of emotion  [1]  or Ekman's model of emotion  [2] . Although these frameworks are extremely popular and provide ease of modeling, they do not fully capture the diversity in emotion expression. This makes using such discrete representations sub-optimal for downstream tasks.\n\nUnderstanding the source of diversity in emotion expression is the key to formulating more accurate emotion representations. There are many sources of diversity in emotion, like the speaker, culture, and context, among other factors  [3, 4] . Labeling two instances of emotion with the same label of say 'anger', ignores the intricacies of the expression of anger. Therefore, we believe it is important to represent the fine-grained characteristics of emotion.\n\nThese fine-grained characteristics of emotions can be better captured by the flexibility that natural language provides. In general, such descriptions can describe the low-level information in the audio like the acoustic properties or they can describe the high-level information like who is expressing the emotion and what the context is. Humans often use affective language to casually describe emotion in speech, for example, 'An angry man shouting loudly'. In this example 'loudness' has a direct acoustic correlate 'intensity' which can be used to form a description e.g. 'this is the sound of high-intensity anger'.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "This Work Was Done When The First Author Was An Intern At Microsoft",
      "text": "The choice of natural language description affects the high dimensional representation learned from the text, hence it is very important to choose the right description for the emotion. This leads to the question: How do we describe an emotion using natural language and how can a model learn it?\n\nIn this work, we propose a method to describe the emotion in audio by using the low-level information in the audio. Previous research shows that there are numerous acoustic correlates of emotion  [5] [6] [7] . These acoustic correlates include measurements like the average pitch, intensity, speech rate, and articulation rate. We extract these correlates from each utterance and use them to form the description in an automatic and scalable way. We call descriptions generated in this manner 'acoustic prompts'.\n\nGiven these acoustic prompts, we train models that associate them with corresponding audio by fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model  [8, 9] . CLAP uses contrastive learning to associate the audio and their descriptions and yields state-of-the-art performance in learning audio concepts with natural language descriptions. We then evaluate this fine-tuned model on downstream tasks.\n\nWe evaluate on Emotion Audio Retrieval (EAR) and Speech Emotion Recognition (SER). SER is a well-known task defined as given a speech utterance, determine the emotion present in the utterance  [4, 10] . The task of EAR is not a commonly performed task. There are tangential works e.g.  [11, 12]  which examine retrieval of music audios, however, this task has not been explored for speech emotion. We believe that EAR is an important task to address since it can be useful in speech forensics, recommendation systems, search engines, social media, etc. Since emotions are also indicators of certain events, EAR methods can help in retrieving hate speech, and violence from audio. We show that the acoustic prompts improve the model's performance in EAR significantly; Precision@K is consistently better for various values of K. We also find that in SER, the model performance improves. Specifically, recognition performance improves 3.8% relative on Ravdess dataset. In a fine-tuning classification setup, we observe 3.7% improvement on Ravdess.\n\nIn summary, the contributions of this paper are as follows:\n\n1. In this work, we propose a unified framework to train emotion representation model through audio-text contrastive learning. We explore ways to generate emotion prompts for speech, grounded in acoustic properties of pitch, intensity, speech rate, and articulation rate.\n\n2. We introduce the task of text-based audio retrieval for emotion (not done before as far as we know) and show that our proposed prompts significantly improve performance on this task.\n\n3. We show improvements in two tasks; SER and EAR on a model trained on multiple emotion datasets.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "Fig.  1  shows the Contrastive Language-Audio Pretraining (CLAP) model -the backbone architecture used in this paper. The audio-text pairs are passed through an audio encoder and a text encoder respectively. Let fa(.) represent the audio encoder and ft(.) represent the text encoder. For a batch of N:\n\nwhere Xa ∈ R N ×V are the audio representations of dimensionality V , and Xt ∈ R N ×U are the text representations of dimensionality U .\n\nWe brought audio and text representations into a joint multimodal space of dimension d by using a projection layer:\n\nwhere Ea ∈ R N ×d , Et ∈ R N ×d , La and Lt are the linear projections for audio and text respectively. Now that the audio and text embeddings (Ea, Et) are comparable, we can measure similarity:\n\nwhere τ is a temperature parameter to scale the range of logits. The similarity matrix C ∈ R N ×N has N correct pairs in the diagonal and N 2 -N incorrect pairs in the off-diagonal. The loss can be calculated as:\n\nwhere\n\n) along text and audio axis respectively. We used this symmetric cross-entropy loss (L) over the similarity matrix to jointly train the audio and text encoders along with their linear projections.\n\nIn this paper, we chose this architecture because it yields SoTA performance in learning audio concepts with natural language descriptions. We use log Mel spectrograms from the audios, sampled at 44K Hz, as input to the audio encoder -CNN14  [13] , which is pretrained on 2M audio clips from AudioSet. The text encoder is BERT uncased. The audio encodings are of 1024 dimensional from the HuggingFace library  [14] , whereas text encodings are 768 dimensional. Both encodings are then projected into a joint multimodal space of dimension 1024. Both audio and text encoders are frozen in our experiments, but the projection layers are learnable. We use PyTorch to implement the model architecture. The model is trained with 0.0001 learning rate, batch size of 128, for 30 epochs using Adam optimizer.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We use 6 Emotion Datasets (ED) in this setup, see Table  1 . The literature using these many datasets for emotion tasks are rare. The original CLAP model is trained with audio-text pairs sourced from three audio captioning datasets: ClothoV2  [15] , AudioCaps  [16] , MACS  [17] , and one sound event dataset: FSD50K  [18] . Altogether they are referred to as 4D henceforth. All the datasets used are publicly available.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Prompt Generation",
      "text": "For all the emotion datasets being used, we only have the discrete class labels no associated descriptions. Therefore, we devise a scalable and automatic prompting method that is based on the acoustic properties of the speech audios. There are numerous acoustic correlates of emotion therefore, we hypothesize that including this information in the prompts would benefit downstream emotion tasks. We construct the prompts in the manner described below: Class label Prompt The simplest description for each audio can be the class label, i.e. audio with the discrete true label of 'anger' will be labeled as 'anger'. We use this as the baseline prompt to compare against the proposed prompts.\n\nPitch Prompt Pitch is known to be affected by emotion, lower pitch is related to negative emotions like fear and high pitch is related to positive emotions like happiness or surprise  [6] . We bin pitch into four bins, since pitch is naturally sex-specific i.e. low-male pitch (< 132.5 Hz), high-male pitch (> 132.5 Hz, < 180 Hz), low-female pitch (> 180 Hz, < 210 Hz) and high-female pitch (> 210 Hz) However, we also experiment with binning into two classes, based on a cutoff of 170 Hz. The cutoffs are obtained from the average numbers for vocal pitch reported in the literature  [24] . The prompt is set as 'bin-class emotion-class', an example of which is 'low pitch anger' (without sex information) or 'low male pitch anger' (otherwise).\n\nIntensity Prompt Intensity is known to be affected by emotion, low intensity is linked with negative emotions like sadness or melancholy and high intensity is linked with joy or excitement  [6] . We bin the average intensity over the audio clip in two bins, low and high intensity at 60 dB  [25] . The cutoffs are based on average intensity numbers reported for human speech in literature. The same rule as pitch prompt is followed to form the intensity prompt, an example of which is 'high intensity anger'.\n\nSpeech-rate Prompt It has been observed that faster-spoken speech is linked with highly potent emotions such as anger and happiness whilst slower speech is linked with sadness, disgust, and boredom  [5] . Speech rate is calculated by extracting the number of syllables spoken divided by the total duration of the audio clip. We use 3.12 syllables/sec as the cutoff to bin the speech rate into two bins, low and high speech rate  [26] . An example of a speech-rate prompt is 'high speech rate anger'.\n\nArticulation-rate Prompt Similarly to speech rate, fast articulation rate is linked with emotions of interest, fear, or happiness; whereas slow articulation rate is indicative of sadness and disgust  [5] . The articulation rate is calculated as the total number of syllables divided by the total phonation time. We bin the audio into low and high articulation rate at the cutoff of 4 syllables/sec  [26] . An example of articulation-rate prompt is 'high articulation rate anger'. Even though speech and articulation rate are similar concepts, speech rate captures speaker-specific information in the form of the number of pauses and hesitation whereas articulation rate ignores such information. Prompt Augmentation To combine all 5 prompts, we pair an audio clip independently with each acoustic prompt. Thus, one audio clip will result in 5 pairs used for training our model. Note: we also tried making one prompt with all the acoustic properties combined together. However, this does not perform as well as when the prompts are paired separately with a given audio.\n\nTable  2  shows all the acoustic prompts that are used in this work. We calculate the pitch and intensity using Librosa  [27]  and we calculate speech rate and articulation rate using Praat  [28] . Note: Other methods to select thresholds (used in prompt creation) like datasetspecific thresholds showed little effect on the final results, therefore we choose to use the literature-inspired thresholds.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Audio Retrieval",
      "text": "We evaluate our trained models for the task of emotion audio retrieval (EAR). With the increasing sizes of audio databases, being able to search such databases for specific types of audio is important.   3 ) the model trained with our acoustic prompting method using prompt augmentation.\n\nThe first three columns in Table  3  show the results when the queries are among the four emotion classes, i.e. happy, sad, angry, and neutral and the collection consists of IEMOCAP dataset. Row 1 model is trained on only 4 audio captioning datasets. Rows 2 and 3 models are trained on 5 emotion datasets, not including IEMOCAP. For a given query, the model outputs top K audios whose audio embeddings have the highest cosine similarity to the text embedding of the query.\n\nWe observe that the model trained on acoustic prompts performs significantly better for all the precision@K metrics. This shows that training the model with acoustic prompts is resulting in betterlearned emotion representations.\n\nFurthermore, we also access whether the trained model learns associations between the acoustic properties and the speech emotion. We test this in a similar framework as in the last experiment. The queries are made similar to the prompts as shown in Table  2 .\n\nThe rest of the columns in Table  3  show the results of audio retrieval when queries are from the acoustic prompts. We calculate precision@K for each acoustic prompt shown on the columns. From the results, we observe that the model trained on the proposed acoustic prompting method performs best in all cases. The takeaway here is that our model is able to retrieve audio significantly better when trained using acoustic prompt augmentation. The precision@K numbers are comparable to numbers observed in audio retrieval tasks  [29] . The results suggest that we can introduce even more elaborate descriptions for each audio at training time and the model will learn associations and be able to retrieve audios with those descriptions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "To evaluate how the acoustic prompts would help in SER, we perform the following two experiments. The first is a zero-shot like setup where we leave one dataset out, which is used during the testing stage. The second is a fine-tuning setup where the model from the first setup is fine-tuned on the left-out dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Leave One Out",
      "text": "This setup evaluates how well a model trained on a pre-defined set of classes generalizes to a new dataset, which might have same or different sets of classes. Out of the 6 emotion datasets, we leave one out for testing and train the model on the other 5 emotion datasets.\n\nTable  3 : Precision@K achieved under different training conditions and prompt settings. The rows show three different models. The first row is the baseline CLAP model. The second and third rows are models trained on 5 emotion datasets, not including the IEMOCAP dataset. The second row is when the prompts used for training are the emotion class labels (CL) of the audios and the third row is when the prompts are acoustic prompts. PA refers to Prompt-Augmentation The queries here are the acoustic prompts also shown in Table  2 . The model trained with acoustic prompt augmentation (PA) is consistently better.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Class Label Queries",
      "text": "Pitch Queries Intensity Queries Speech Rate Queries Articulation Rate Queries P@1 P@5 P@10 P@1 P@5 P@10 P1 P@5 P@10 P@1 P@5 P@10 P@1 P@5 P@10 4D Therefore the training and testing datasets are completely different.\n\nIn the case where Ravdess is the testing dataset, 'calm' class is not represented in any of the other training datasets and is a zero-shot classification result. We train 5 different models shown in the rows of Table  4 . There are two main takeaways from this experiment. Firstly adding Emotion datasets in the training stage helps the performance on the leftout emotion dataset. This can be observed in the second column where the performance improves from 15.99% to 22.88%.\n\nSecondly using acoustic prompt augmentation (PA) is not helping in the fine-tuning setup. We believe this is because there is a distribution shift in the training and testing datasets, which effects the acoustics and hence the acoustic prompts. For example, 'high intensity anger' prompt might not be prevalent in the training datasets but is present in the testing dataset. This harms the transferability of the learned acoustic prompts to a completely new dataset. Note that the SoTA performance for this evaluation setup is not found in literature because the general evaluation setup is when the dataset is present in both training and testing sets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Finetune",
      "text": "In this experiment, we fine-tune the model from the previous stage on the left-out dataset.\n\nThe results for SER are shown in the last column of Table  4 . We observe that when using acoustic prompt augmentation, we get the best accuracy metric. We see improvement in performance by absolute 3.77%, from 68.69% to 72.46%.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Prompt Analysis",
      "text": "To evaluate which of the proposed acoustic prompts is better, we apply the trained model on SER with a smaller setup as in the last experiment, where the testing dataset is present in the training dataset.\n\nThe model is trained 6 different times, where each time the description associated with emotion audios are varied. Among the 6, 1 uses the class label prompt and 4 uses the acoustic prompts as described in Section 3.2, and 1 uses the prompt augmentation -which combines all the acoustic prompts. We train the model on 4 audio captioning datasets and 1 emotion dataset. The left part of Figure  2  shows the performance achieved when the model is trained on the training set (including 4D and Ravdess) and tested on the testing set of Ravdess. We observe that among the 4 acoustic prompts, the pitch prompt gives the best performance. The second-best performance is achieved by the intensity prompt, followed by speech rate and then articulation rate. Secondly, we observe that overall acoustic prompt augmentation is giving the best performance in both datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Limitations And Conclusion",
      "text": "There are certain limitations to our work. Firstly, we use only four acoustic properties, however, there are other acoustic properties that are effected by emotion and should be explored. Secondly for each prompt, we create 2 or 4 bins per acoustic property, while these bins could be more fine-grained. Our future study will include work in alleviating the need for thresholding and relying on data-centric methods of binning the prompts.\n\nThis work performs SER and EAR using the audios and their automatically generated descriptions. We use the acoustics of emotions to prompt the audios, in fact, there can be more complicated descriptions, invoking the semantics, environment, and context among other factors. We envision that as methods of describing emotions become more complicated, our ability to model emotions will become better. The acoustic properties we extract include pitch, intensity, speech rate, and articulation rate extracted from the audio. We find that among the acoustic prompts, pitch prompt is the best performing. Overall for EAR when we do acoustic prompt augmentation, we achieve consistently better Precision@K metric. For SER, we also achieve an improvement in performance in Ravdess by 3.8% in the finetuning setup.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The left part of the image shows model training. Given a batch of N audio-text pairs, the model trains the audio and text encoders to learn their",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the Contrastive Language-Audio Pretraining (CLAP)",
      "page": 2
    },
    {
      "caption": "Figure 2: Accuracy achieved using different acoustic prompts on Ravdess.",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the performance achieved",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 3: show the results when the",
      "data": [
        {
          "Property": "Class label (CL)",
          "Prompt": "• {emotion}"
        },
        {
          "Property": "Pitch",
          "Prompt": "•\nhigh female pitch {emotion}\n•\nlow female pitch {emotion}\n•\nhigh male pitch {emotion}\n•\nlow male pitch {emotion}"
        },
        {
          "Property": "Intensity",
          "Prompt": "•\nhigh intensity {emotion}\n•\nlow intensity {emotion}"
        },
        {
          "Property": "Speech rate",
          "Prompt": "•\nhigh speech rate {emotion}\n•\nlow speech rate {emotion}"
        },
        {
          "Property": "Articulation rate",
          "Prompt": "•\nhigh articulation rate {emotion}\n•\nlow articulation rate {emotion}"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The emotions",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1991",
      "venue": "The emotions"
    },
    {
      "citation_id": "3",
      "title": "Are there basic emotions?",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions?"
    },
    {
      "citation_id": "4",
      "title": "Detecting gender differences in perception of emotion in crowdsourced data",
      "authors": [
        "Shahan Ali Memon",
        "Hira Dhamyal",
        "Oren Wright",
        "Daniel Justice",
        "Vijaykumar Palat",
        "William Boler",
        "Bhiksha Raj",
        "Rita Singh"
      ],
      "year": "2019",
      "venue": "Detecting gender differences in perception of emotion in crowdsourced data",
      "arxiv": "arXiv:1910.11386"
    },
    {
      "citation_id": "5",
      "title": "The phonetic bases of vocal expressed emotion: Natural versus acted",
      "authors": [
        "Hira Dhamyal",
        "Shahan Ali Memon",
        "Bhiksha Raj",
        "Rita Singh"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "6",
      "title": "Communicating emotion: The role of prosodic features",
      "authors": [
        "Robert Frick"
      ],
      "year": "1985",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "7",
      "title": "Acoustic concomitants of emotional dimensions: Judging affect from synthesized tone sequences",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "1972",
      "venue": "Acoustic concomitants of emotional dimensions: Judging affect from synthesized tone sequences"
    },
    {
      "citation_id": "8",
      "title": "Emotions and multilingualism",
      "authors": [
        "Aneta Pavlenko"
      ],
      "year": "2005",
      "venue": "Emotions and multilingualism"
    },
    {
      "citation_id": "9",
      "title": "Clap: Learning audio concepts from natural language supervision",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Mahmoud Ismail",
        "Huaming Wang"
      ],
      "year": "2022",
      "venue": "Clap: Learning audio concepts from natural language supervision",
      "arxiv": "arXiv:2206.04769"
    },
    {
      "citation_id": "10",
      "title": "Audio retrieval with wavtext5k and clap training",
      "authors": [
        "Soham Deshmukh",
        "Benjamin Elizalde",
        "Huaming Wang"
      ],
      "year": "2022",
      "venue": "Audio retrieval with wavtext5k and clap training",
      "arxiv": "arXiv:2209.14275"
    },
    {
      "citation_id": "11",
      "title": "Positional encoding for capturing modality specific cadence for emotion detection",
      "authors": [
        "Hira Dhamyal",
        "Bhiksha Raj",
        "Rita Singh"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "12",
      "title": "Research on emotional semantic retrieval of attention mechanism oriented to audio-visual synesthesia",
      "authors": [
        "Weixing Wang",
        "Qianqian Li",
        "Jingwen Xie",
        "Ningfeng Hu",
        "Ziao Wang",
        "Ning Zhang"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "Emomv: Affective music-video correspondence learning datasets for classification and retrieval",
      "authors": [
        "Ha Thi",
        "Phuong Thao",
        "Gemma Roig",
        "Dorien Herremans"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "14",
      "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Qiuqiang Kong",
        "Yin Cao",
        "Turab Iqbal",
        "Yuxuan Wang",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771"
    },
    {
      "citation_id": "16",
      "title": "Clotho: an audio captioning dataset",
      "authors": [
        "Konstantinos Drossos",
        "Samuel Lipping",
        "Tuomas Virtanen"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "AudioCaps: Generating Captions for Audios in The Wild",
      "authors": [
        "Chris Dongjoo Kim",
        "Byeongchang Kim",
        "Hyunmin Lee",
        "Gunhee Kim"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "18",
      "title": "What is the ground truth? reliability of multi-annotator data for audio tagging",
      "authors": [
        "Irene Martín",
        "Annamaria Mesaros"
      ],
      "year": "2021",
      "venue": "2021 29th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "19",
      "title": "Fsd50k: An open dataset of human-labeled sound events",
      "authors": [
        "Eduardo Fonseca",
        "Xavier Favory",
        "Jordi Pons",
        "Frederic Font",
        "Xavier Serra"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "23",
      "title": "Crema-d: Crowdsourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "24",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "25",
      "title": "The frequency range of the voice fundamental in the speech of male and female adults",
      "authors": [
        "Hartmut Traunmüller",
        "Anders Eriksson"
      ],
      "year": "1995",
      "venue": "Unpublished manuscript"
    },
    {
      "citation_id": "26",
      "title": "Attending at a low intensity increases impulsivity in an auditory sustained attention to response task",
      "authors": [
        "Hettie Roebuck",
        "Kun Guo",
        "Patrick Bourke"
      ],
      "year": "2015",
      "venue": "Perception"
    },
    {
      "citation_id": "27",
      "title": "Speech rate in parkinson's disease: A controlled study",
      "authors": [
        "Martínez-Sánchez",
        "J Jjg Meilán",
        "C Carro",
        "L Gómez Íñiguez",
        "Millian-Morell",
        "T Im Pujante Valverde",
        "López-Alburquerque",
        "López"
      ],
      "year": "2016",
      "venue": "Neurología (English Edition)"
    },
    {
      "citation_id": "28",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Eric Mcvicar",
        "Oriol Battenberg",
        "Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "29",
      "title": "Praat",
      "venue": "Praat"
    },
    {
      "citation_id": "30",
      "title": "Improving content-based audio retrieval by vocal imitation feedback",
      "authors": [
        "Bongjun Kim",
        "Bryan Pardo"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "A proposal for multimodal emotion recognition using aural transformers and action units on ravdess dataset",
      "authors": [
        "Cristina Luna-Jiménez",
        "Ricardo Kleinlein",
        "David Griol",
        "Zoraida Callejas",
        "Juan Montero",
        "Fernando Fernández-Martínez"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    }
  ]
}