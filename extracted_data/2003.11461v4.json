{
  "paper_id": "2003.11461v4",
  "title": "Emotion Recognition From Gait Analyses: Current Research And Future Directions",
  "published": "2020-03-13T08:22:33Z",
  "authors": [
    "Shihao Xu",
    "Jing Fang",
    "Xiping Hu",
    "Edith Ngai",
    "Wei Wang",
    "Yi Guo",
    "Victor C. M. Leung"
  ],
  "keywords": [
    "Emotion recognition",
    "gait analysis",
    "intelligent computation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human gait refers to a daily motion that represents not only mobility, but it can also be used to identify the walker by either human observers or computers. Recent studies reveal that gait even conveys information about the walker's emotion. Individuals in different emotion states may show different gait patterns. The mapping between various emotions and gait patterns provides a new source for automated emotion recognition. Compared to traditional emotion detection biometrics, such as facial expression, speech, and physiological parameters, gait is remotely observable, more difficult to imitate, and requires less cooperation from the subject. These advantages make gait a promising source for emotion detection. This article reviews current research on gait-based emotion detection, particularly on how gait parameters can be affected by different emotion states and how the emotion states can be recognized through distinct gait patterns. We focus on the detailed methods and techniques applied in the whole process of emotion recognition: data collection, preprocessing, and classification. At last, we discuss possible future developments of efficient and effective gaitbased emotion recognition using the state of the art techniques in intelligent computation and big data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "H UMAN gait is a manner of walking of individuals. It describes a common but important daily motion through which observers can learn much useful information about the walker. In clinical terms, apart from the detection of movement abnormalities, observation of gait patterns also provides diagnostic clues for multiple neurological disorders such as cerebral palsy, Parkinson's disease, and Rett syndrome  [1] ,  [2] ,  [3] ,  [4]  in an early stage. Clinical gait analysis therefore plays a more and more important role in medical care which may prevent patients from permanent damage. It becomes a well developed tool for quantitative assessment of gait disturbance, which can be applied to functional diagnosis, treatment planning and monitoring of disease progression  [5] ,  [6] . In addition, gait provides useful social knowledge to the observers. Research has revealed that human observers are able to recognize themselves and other people that they are familiar with even from the point-light depicted or impoverished gait patterns  [7] ,  [8] , indicating that gait is unique. The identity information is embedded in the gait signature, which has been considered as a unique biometric identification tool. With the advacements of computer vision and big data analysis, gait recognition has been widely employed for various security applications  [9] ,  [10] .\n\nFurthermore, it was suggested that emotion expression is embedded in the body languages including gait and postural features  [11] ,  [12] ,  [13] ,  [14] ,  [15] . Indeed, people in different emotional states show distinct gait kinematics  [16] ,  [17] . For instance, studies found that depressed individuals show different gait patterns, including slower gaits, smaller stride size, shorter double limb support, and cycle duration, in contrast to the control group  [18] ,  [19] . According to the previous studies, human observers are able to identify emotions based on the gait  [12] . These findings indicate that gait can be considered as a potential informative source for emotion perception and recognition.\n\nHuman emotion is heavily involved in cognitive process and social interaction. In recent years, automated emotion recognition becomes a growing field along with the development of computing techniques. It supports numerous applications in inter-personal and human-computer interaction such as customer services, interactive gaming and e-learning, etc.\n\nHowever, current research on emotion recognition mainly focused on facial expression  [20] ,  [21] , speech (including linguistic and acoustic features)  [22]  and physiological signals (e.g. electroencephalography, electromyography, heart rate, blood volume pulse, etc.)  [23] ,  [24] ,  [25] ,  [26] , while relatively few studies investigate the association between emotion and full-body movements. Gait analysis shows apparent advantages over the prevailing modalities, which makes it a promising tool for emotion recognition. We summarize these advantages as follows.\n\n• Human bodies are relatively large and have multiple degrees of freedom. Unlike facial expression data which must be collected by cameras in very short distance. For monitoring gait patterns, the subjects are allowed to be relatively far from the cameras. The practical distance can be up to several meters while most other biometrics are no longer observable or provide very low resolution  [27] ,  [28] .\n\n• Walking usually requires less consciousness and intention from the walker. Thereby it is not vulnerable to active manipulation and imitation. Ekman  [29]  has pointed out that the head expresses the nature of emotions while the body shows information about the emotional intensity. • The data collection process of gait patterns requires less cooperation from the subject, so that his or her behaviour is less interfered and closer to the normal state in real life. With a goal of exploring the opportunities and facilitating the development of this emerging field, we systematically review the literatures related to emotion detection based on gait. A recent survey  [30]  discussed how emotion affects gait for patients with Parkinson disease. Different from that survey, our study focuses on a generic target group with physically and mentally healthy individuals in this work. Another survey by Stephens-Fripp et al.  [31]  focused on emotion detection based on gait and posture, but it discussed gait and posture together and emphasized more on the posture part whereas our current paper is all around gait and shares the details of how gait can be affected by emotions and the process of gaitbased emotion detection. Moreover, we also introduce emotion models in current emotion theory and dynamics of gait. It is not only informative, but also essential for computer scientists to integrate theories into their design of the automated emotion recognition system since this field is highly interdisciplinary.\n\nThis survey is organized as follows: Section II and III give some background on different models of emotion, gait initiation and gait cycle. Section IV discusses the impact of emotion on the gait initiation and the gait cycle. Section V presents the details of gait based emotion recognition including data collection, preprocessing, and classification. In Section VI, we propose some future directions on gait based emotion recognition with machine learning and data analysis. Finally, Section VII gives the conclusion on this topic.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Models Of Emotion",
      "text": "Human emotion is a diverse complex. It can be reflected by a clear happy representation like laughter or smile to extreme sadness with tears. Emotion is hard to be defined comprehensively since it is identified in context-dependent situations.\n\nThere are three common models to describe and measure emotional states in scientific analysis: 1) distinct categories; 2) pleasure-arousal-dominance model (PAD model); and 3) appraisal approach. The model based on distinct categories is simple and intuitive, which consists of six discrete emotions including happiness, sadness, fear, anger, surprise, and disgust. They are often linked to human facial expression  [32] . The PAD model is a continuous space divided by three orthogonal axes shown in Fig.  1 . The pleasure dimension identifies the positivity-negativity of emotional states, the arousal dimension evaluates the degree of physical activity and mental alertness of emotional state, and the dominance dimension indicates to what extend the emotional state is under control or lack of control  [33] . Each type of emotion has its place in the three dimensional space including the former mentioned distinct emotions. For instance, sadness is defined as negative pleasure, positive arousal, and negative dominance according to the mapping rules between distinct emotions and the PAD model  [34] ,  [35] . This model has been used to study nonverbal communication like body language in psychology  [36] . The third model is appraisal model which describes how emotions develop, influence, and are influenced by interaction with the circumstances  [37] ,  [38] . Ortony et al.  [39]  applied this appraisal theory to their models and found out that the parameters in the environment such as the events or objects may affect the emotion strength. Because of the complexity of individuals' evaluations (appraisals or estimates) on events that cause specific reactions in themselves, the appraisal method is less often applied for recognition of emotional states than the two former models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Gait",
      "text": "Human gait refers to a periodical forward task that requires precise cooperation of the neural and musculoskeletal systems to achieve dynamic balance. This requirement is so crucial since the human kinematic system controls the advancing with the support from the two feet. In particular, when only one foot provides standing, the body is in a state of imbalance and it needs a complicated mechanism to make accurate and safe foot trajectory. In the next two subsections, we will talk about the gait initiation and the gait cycle since both of them would be affected by emotion.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Gait Initiation",
      "text": "As an individual starts to move from the static state, he or she is doing gait initiation. In this initiation, anticipatory postural adjustments (APA) plays an important role in breaking the erect posture and transferring the body center of gravity to the stepping foot  [41] . It is a movement involving the swing of a leg forward and leading to imbalance. This imbalance is a result of a swift lateral weight shift and is for generating enough advancing power.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Gait Cycle",
      "text": "One gait cycle is measured from one heel-strike to another heel-strike with the same heel  [42] , which can be described by two cyclic features, phases and events. Both features can be divided into percentage by several key action points (see Fig.  2 ). Two main phases are shown in the gait cycle: In the stance phase, touching the ground with one foot is called single limb stance. When both feet are on ground, it is called double support. Following the stance phase, the target foot is going to swing and followed by midswing and terminal swing. Gait can be divided into eight events or subphases. Fig.  2  shows an example of gait cycle, which starts with initial contact of the right foot in the stance phase. In the initial contact, the crotch extends to a large angle and the knee flexes. Next is the loading response. It is a progress that transfers the body weight from the left leg to the right. Midstance shows the right leg is on the ground whereas the left leg is in motion and the weight is being transferred between them. When the right heel starts to lift and before the left foot lands on the Fig.  1 . The pleasure-arousal-dominance space for emotions  [33][40] . The center of each ellipsoid is the mean and the radius is the standard deviation of each emotional state.\n\nground, the individual is in terminal stance. Once the left toe touches the ground following the terminal stance, the weight is again uphold by both limbs and the preswing phase begins. Passing through the right toe-off (initial swing) event, the body weight is transferred from the right to the left this time (the midswing phase). As soon as the right heel strikes again, the whole gait cycle is completed and the walker prepares for next cycle.\n\nFig.  3  summarizes the emotion models and the gait cycle that we discussed above. It shows the general relationship between emotion and gait, and gives an overview on the content of this paper. It includes the aforementioned content in Section II and Section III, namely the models for describing emotion and the components of gait, respectively. It also contains a brief representation of Section IV and Section V with the upper part listing the gait parameters that would be influenced by emotions and the bottom part showing the process of gait based emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Emotional Impact On Gait A. On Gait Initiation",
      "text": "Appetitive approach and defensive avoidance are two basic motivational mechanisms for fundamentally organizing emotion in the light of the biphasic theory of emotion  [43] . Based on this theory, studies  [44] ,  [45] ,  [46] ,  [47] ,  [48]  have been conducted to explore the emotion effects on gait initiation. Theses studies were usually conducted by giving congruent (CO) tasks and incongruent (IC) tasks to the walkers. In the CO tasks, the participants were asked to show approaching when sensing the pleasant stimuli or they should express avoidance responding to the unpleasant stimuli. IC tasks were the opposite to the CO tasks as the participants were asked to make approach movements for the unpleasant stimuli or they needed to performed avoidance at the moment they perceived the pleasant stimuli  [49] . Apparently, the IC tasks were related to the human emotional conflict. To elicit participants' emotions, the pleasant or unpleasant images from the International Affective Picture System (IAPS) were used as visual stimulus and force plates were used to record the ground reaction forces for the movements.\n\nWe summarize the experimental setups and results from the research studies related to emotion impacts on gait initiation in Table  I . In  [44] , the specific paradigm was \"go\" or \"nogo\" for volunteers. The \"nogo\" response (i.e., volunteers were not supposed to move) was related to the stimulus of neutral images showing only an object whereas the \"go\" response (i.e., volunteers should approach or avoid) was for pleasant or unpleasant pictures corresponding to CO or IC tasks. Volunteers were asked to perform responding action as soon as possible after the onset of the images. In  [45] , participants were asked to make either an anterior step (approach) or a posterior step (withdrawal) as soon as the image was presented and to remain static till it disappeared. The experiment in  [46]  studied the influence of a change in the delay between picture onset and the appearance of \"go\" action for checking people's reaction time. The changes in the delay had two conditions, namely the short condition (the word \"go\" showed 500ms after image onset) and the long condition (the word \"go\" showed 3000ms after image onset). Research  [47]  aimed at gait initiation as soon as the participants saw the image (i.e., onset) or as soon as the image disappeared (offset). In clinical studies, the experiment in  [48]  focused on gait initiation in patients with Parkinson's disease and the patients were asked to make an initial step with their preferred legs after the image offset and to keep walking at their self-selected pace.\n\nStudies mentioned above revealed that an individual's emotion can affect gait initiation. When the participants encounter emotional conflicts, they seemed to pose a defensive response for the IC tasks leading to longer reaction time (RT) and shorter step length compared to CO tasks shown in Table  I. This is inspiring because these features learned from gait initiation show significant differences between people with positive and negative emotions. The analysis of gait initiation may become a potential method to recognize human's emotion in the future.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. On Gait Cycle",
      "text": "In this subsection, a few studies would be shared for showing the performances and characteristics of emotive gaits (see Table  III ). In Montepare's investigation  [50] , ten female observers viewed the gaits of five walkers with four various emotions (i.e., sadness, anger, happiness, and pride) in order to determine the walkers' emotions and report specific gait features observed. Note that the walkers' heads were not recorded in the gaits to prevent the facial confusion of emotion perception. The investigation showed that gait patterns with different emotions could be identified better than chance level with mean accuracy of 56%, 90%, 74%, 94% for pride, anger, happiness, and sadness respectively. As for the gait features which differentiated emotions, the angry gaits were relatively more heavyfooted than the other gaits, while the sad gaits had less arm swing compared with the other gaits. It also turned out that proud and angry gaits had longer stride lengths than happy or sad gaits. Finally, happy gaits performed faster in pace than the other gaits.\n\nSimilarly, thirty observers used Effort-Shape method  [51]  to rate the qualitative gait movements of sixteen walkers expressing five emotions (i.e., joy, contentment, anger, sadness, and neutral) in Gross's work  [52] . The Effort-Shape analysis involved four factors evaluating the effort in the walker's movements (i.e., space, time, energy, flow) and two factors described the shape of the body (i.e., torso shape and limb shape) which are shown in Table  II . Each factor was rated from 1 to 5. For the instance of flow, the left anchor was \"free, relaxed, uncontrolled\" and the right anchor was \"Bound, tense, controlled\". The three intermediate points in the scale acted a transition between the left and right anchor qualities.\n\nResults revealed that the sad gait was featured as having a contracted torso shape, contracted limb shape, indirect space, light energy, and slow time. The angry gait was regarded as having expanded limb shape, direct space, forceful energy, fast time, and tense flow. The joy gait had common characteristics with the angry gait in limb shape, energy, and time, but there was a more expanded torso shape and more relaxed flow for the joy gait than the anger gait. The content gait might look like the joy gait but the former one had more contracted limb shape, less direct space, lighter energy and slower time than the latter one. When it came to the gait pattern in neutral emotion state, it was similar to the sad gait, however, it had a more expanded torso and limb shapes, more direct space, more forceful energy and faster time than the sad gait. Another report from human observers for perceiving walkers' emotions showed that there were significant differences in gait patterns among people with various emotions such as happiness with a bouncing gait, sadness with a slow slouching pace, anger with a fast stomping gait and fear with fast and short gait  [53] .\n\nThrough data analytics, the features in gaits carrying various emotions could be studied through kinematic analysis based on the gait data. In  [54] , two types of features, 1) posture features and 2) movement features, had been explored to determine which features were the most important in various emotional expression through analyzing the gait trajectory data. For both types of features mentioned above, flexion angles of eleven major joints (head, spine, pelvis and left and right shoulder, elbow, hip and knee joints) have been averaged over the gait cycle. In terms of the posture features, the evident results were the reduced head angle for sad walking, and increased elbow angles for fear and anger while walking. As for movement features, the happy and angry gaits were linked to increased joint amplitudes whereas sadness and fear showed a reduction in joint-angle amplitudes. In addition, the study compared the emotional gaits with neutral gaits whose speeds were matched to the former ones (with overall velocity difference < 15%) and it figured out that the dynamics of the emotion-specific features cannot be explained by changes in gait velocity. In Barliya's study  [55] , the gait speed was shown to be a crucial parameter that would be affected by various emotions. The amplitude of thigh elevation angles differed from those in the neutral gait for all affections except sadness. Anger was showing more frequently oriented intersegmental plane than others.\n\nThrough the kinematic analysis of emotional (i.e., happy, angry, fearful, and sad ) point-light walkers, Halovic and Kroos  [53]  found out that both happy and angry gaits showed long strides with increased arm movement but angry strides had a faster cadence. Walkers feeling fear were with fast and short strides. Sad walkers had slow short strides gaits representing the slowest walking pace. The fearful and sad gaits both had less arm movement but the former one mainly moved their lower arms whilst the latter one had the entire arms movement.\n\nStudies  [56] ,  [57]  applying eight-camera optoelectronic motion capture system focused on smoothness of linear and angular movements of body and limbs to explore the emotive impact on gaits. In the vertical direction, the smoothness of movements increased with angry and joyful emotions in the whole body center-of-mass, head, thorax and pelvis compared to sadness. In the anterior-posterior direction, neutral, angry, and joyful emotions only had increased movement smoothness for the head compared to sadness. In angular movements, anger's movement smoothness in the hip and ankle increased compared to that of sadness. Smoothness in the shoulder increased for anger and joy emotions compared to sadness.\n\nResearch in  [18] ,  [58]  further analyzed the gaits of patients with depression compared wtih the control group. It found that depressed patients showed lower velocity, reduced limb swing and less vertical head movements.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Gait Analysis For Emotion Recognition",
      "text": "There is a long way to go towards the ultimate goal, namely, automated emotion recognition through gait patterns. However, there are important observations based on previous studies  [59] ,  [60] ,  [61] . Automated process may be possible by aggregating data from previous observations supported by big data analysis and machine learning. In this subsection, we discuss some details of current research on gait based emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Gait Data Collection",
      "text": "Gait can be captured in digital format (i.e. by cameras, force plates) for data analysis. As the technology is advancing, digital devices have increased sensing quality and become more intelligent. In the field of emotion detection based on gait, the force platform was very useful for recording the anteroposterior velocity and displacement of center of foot pressure  [62] . The infrared light barrier system functioned as well for measuring gait velocity  [58] ,  [62] . More prevailing was the motion analysis systems (e.g., Vicon and Helen Hayes) that could capture precise coordinates information of markers by taping them over people  [54] ,  [55] ,  [63] ,  [18] ,  [52] ,  [64] ,  [65] ,  [66] . As another convenient and non-wearable device, Microsoft Kinect, showed up. It was first for interactive games, and then it could be used for motion analysis due to its representation of human skeleton  [67] ,  [68] . Fig.  4  shows an individual's skeleton consisting of marking 25 joints, which were displayed by Kinect V2 device. The joints were figured out based on the three dimension coordinates derived from depth image. In addition, Kinect can provide other types of video, such as RGB video and infrared video  [69] ,  [70] . In recent years, intelligent wearable devices got a lot of attention not only in the market, but they could be used in gait pattern analysis for identifying people's emotion since the accelerometer in the devices collect movement data  [71] ,  [72] ,  [73] . In the work of Chiu  [74] , the mobile phone was used for capturing the gait of people and then the video data was sent to a server for pose estimation and emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Preprocessing",
      "text": "Before feeding data to the classifiers or performing correlation analysis, the raw data should be preprocessed to obtain significant and concise features for computation efficiency and performance. Data preprocessing is a crucial step that may include many substeps such as information filtering that helps to remove noise artifacts and burrs and to perform data transformation, feature selection, and so on. In the following paragraph, we will present some of the preprocessing techniques.\n\n1) Filtering To smooth the data and get rid of the noise for the marker trajectories of an individual's walking, studies  [56] ,  [57] ,  [52] ,  [64]  used low-pass Butterworth filter with a cut-off frequency of 6 Hz. Butterworth filter is considered as a maximally flat magnitude filter  [75]   since the frequency response in the passband has no ripples and rolls off towards zero in the stopband  [76] . Sliding window Gaussian filtering is another common way to eliminate some noises or high-frequency components like jitters. Mathematically, a discrete Gaussian filter transfers the input signal by convolution with a Gaussian function in which there is a significant parameter called standard deviation and the standard deviation is key to designing a Gaussian kernel of fixed length. In studies  [67] ,  [68] , the Gaussian kernel was 1 16  [1, 4, 6, 4, 1]  for filtering three dimensional coordinates of joints of walkers .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "2) Data Transformation",
      "text": "Processing the data in the time domain may not be the most effective method. In most of the time, data transformation to other domains, like the frequency domain or time-frequency domain, is favorable, which can make the understanding of the data more thorough. One classical and popular method is the discrete Fourier transform (DFT)  [77] . Discrete Fourier transform derives frequency information of the data, providing the Fourier coefficients that can feature the data. This transform can be applied in three dimensional gait data recorded by Microsoft Kinect devices  [67] ,  [68] ,  [78]  Another method outweighing discrete Fourier transform is the discrete wavelet transform (DWT) that represents the frequency and location (location in time) information, which has been applied in gait studies  [79] ,  [80] ,  [81] . In DWT, selecting a wavelet that best suited the analysis is crucial and the choice of the best wavelet is generally based on the the signal characteristics and the applications. For example, for data compression, the wavelet is supposed to represent the largest amount of information with as few coefficients as possible. Many wavelets have been proposed ranging from the Haar, Daubechies, Coiflet, Symmlet, and Mexican Hat to Morlet wavelets and they possess various properties that meet with the needs of the work. For instance, the Daubechies 4 (Db4) is for dealing with signals that have linear approximation over four samples whereas Db6 aims at quadratic approximations over six samples  [82] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "3) Feature Selection",
      "text": "Investigations have been conducted to provide qualitative and quantitative evaluations of multiple features in the human gait. The way to choose the parameters of interest depends on specific application domain. As an example in sports, Electromyography (EMG) signal recorded from muscles was exploited to build a model for determining muscle force-time histories when an individual was walking  [83] .\n\n• Spatiotemporal Features The gait cycle contains many useful spatiotemporal quantities such as gait velocity, stride and step lengths, step width, single/double support and swing period, phases, rhythm (number of steps per time unit), foot placement through the measurement of time and scale.\n\n• Kinematic Features Gait data of the joints or skeleton collected by marker trajectories, Kinect cameras or video-based pose estimation, are helpful for the measurements of kinematic features, which are not limited to joint angles, angular range of motion, displacement and velocity on basis of various axes  [55] . Research in  [54]  explored the crucial kinematic features related to emotion from gait. It indicated that limb flexion is a key feature for expression of anger and fear on gait whereas head inclination corresponded to sadness dominantly. More specific adoption of kinematic features could be found in the next section.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Emotion Recognition",
      "text": "There have been several efforts to classify a walker's emotion through their gait data (see Table  IV ). In  [62] , Janssen et al. used the Kistler force platform to record the ground reaction forces of walkers who were sad, angry or happy through recalling specific occasion when they felt the corresponding emotion. After a short period of elicitation of various emotions, volunteers were asked to walk about 7m at a self-determined gait velocity on the force platform. The ground forces of gait after normalization by amplitude and time were separated into two parts, training set and test set. The training part was put into a supervised Multilayer Perceptrons (MLP) with 200-111-22 (input-hidden-output) neurons (one output neuron per participant). The test part was for the validation that MLP model achieved 95.3% accuracy of recognizing each individual. With the help of unsupervised Self-organizing Maps (SOM), the average emotion recognition rate was 80.8%.\n\nOmlor et al.  [63]  tried to classify emotions expressed by walkers with attached markers upon joints. They presented an original non-linear source separation method that efficiently dealt with temporal delays of signals. This technique showed superiority to PCA and Independent Component Correlation Algorithm (ICA). The combination of this method and sparse multivariate regression figured out spatio-temporal primitives that were specific for different emotions in gait. The stunning part is that this method approximated movement trajectories very accurately up to 97% based on three learned spatiotemporal source signals.\n\nIn  [65] ,  [84] , feature vectorization (i.e computing the autocorrelation matrix) has been applied in gait trajectories. Four professional actors feeling neutral, joy, anger, sadness, and fear respectively repeated five times walks in a straight line with joint-attached 41 markers of a Vicon motion capture system. In the vector analysis, the angles, position, velocity, and acceleration of joints have been explored for detecting emotions. The study found out that lower torso motion, waist, and head angles could significantly characterize the emotion expression of walkers whereas the leg and arm biased the detection. More importantly with the utilization of the weight on vector, the emotion recognition has been improved to a total average accuracy of 78% for a given volunteer.\n\nKarg et al. studied thoroughly affect recognition based on gait patterns  [66] . Statistical parameters of joint angle trajectories and modeling joint angle trajectories by eigenpostures were two ways to extract features in the research. The former consists of three parts. The first part was calculating the velocity, stride length and cadence (VSC). The second part was figuring minimum, mean, and maximum of significant joint angles (i.e., neck angle, shoulder angle, thorax angle) including VSC whereas the third part applied the same method as the one in the second part to all joint angles. These latter two parts were processed independently by PCA, kernel PCA (KPCA), linear discriminant analysis (LDA), and general discriminant analysis (GDA) respectively. The other way based on eigenpostures was to establish the matrix W that contained the mean posture, four eigenpostures, four frequencies and three phase shifts (later referred to as PCA-FT-PCA) as shown in Fig.  5 . Given two kinds of features above, for recognition, the next step was to feed them to the classifiers Naive Bayes, Nearest Neighbor (NN) and SVM respectively. The essence of this research was to further explore the interindividual and person-dependent recognition as well as the recognition based on the PAD (pleasure, arousal, dominance) model  [85] . The best accuracy of affect recognition was achieved, 93%, based on the estimated identity as shown in Table  IV . four eigenpostures P j , four frequencies f j , and three phase shifts φ j .  [66]  Research in  [67] ,  [68]  both used Microsoft Kinect v2 sensor, a low-cost and portable sensor to recognize emotional state through people's 1-minute gaits in straight line walking back and forth after they watched a 3-minute video clips that may cause various emotion elicitation. There were 59 actors experiencing happiness, anger, and neutral state respectively. Kinect camera captured the gait data in the format of 3D coordinate of 25 joints. After data preprocessing (data segmentation, low-pass filtering, coordinates translation and coordinate difference), the selected joints were then featured in the time and frequency domain. After that they were finally fed into classifiers including the LDA, Naive Bayes, Decision Tree and SVM respectively  [67]  (see Fig.  6 ). The distinct improvements were achieved by PCA with selecting the useful major features that the highest accuracy were 88% using SVM for happiness between happiness and neutral, 80% using Naive Bayes for neutral between anger and neutral and 83% using SVM for anger between happiness and anger whilst Li et al.  [68]  used Naive Bayes, Random Forest, SVM and SMO respectively to the skeletons data and the highest accuracy were 80.51% , 79.66%, 55.08% with Naive Bayes (for anger and neutral) , Naive Bayes (for happiness and neutral) , Random Forests (for happiness and anger) accordingly. In  [71] , gait pattern was tracked by a customized smart bracelet with built-in accelerometer, which recorded three dimensional acceleration data from different body positions such as right wrist and ankle. In order to remove unexpected waking vibrations and reduce data redundancy, they preprocessed the time series data by using moving average filter and sliding window. Then, with the calculation of skewness, kurtosis and standard deviation of each one of three axes, the temporal domain features were selected. What is more, they used Power Spectral Density (PSD) and Fast Fourier Transform (FFT) to extract temporal and frequency domain features. After this part, the final number of features was 114 (38 features on each axis), and they were fed into 4 different algorithms (Decision Tree, SVM, Random Forest, and Random Tree) respectively. In the eventual result, SVM achieved the highest accuracy of classification, 88.5 %, 91.3%, 88.5%, respectively for happiness and neutral, anger and neutral, happiness and anger. Results indicated that it was efficient to recognize human emotions (happiness, neutral and angry) with gait data recorded by a wearable device.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Preprocessing Joints Selestion",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Features In Frequency",
      "text": "In the investigation by Juan et al.  [72] , the accelerometer data from the walker's smart watch has been used to identify emotion. Fifty participants were divided into groups to perform three tasks: 1)watch movies and walk 2)listen to music and walk 3) listen to music while walking. Movies or music were for eliciting participants happiness or sadness for the later tasks. Either after stimulation or engaging in it, volunteers walked in a 250m S-shaped corridor in a round trip while wearing a smart watch . When it comes to feature selection, the accelerometer data would be divided by sliding windows and features came from each window as a vector. Then Random forest and Logistic Regression handled the classification for happiness and sadness based on the feature vectors, and the majority of accuracies ranged from 60% to 80% compared to the baseline 50%.\n\nChiu et al. aimed at emotion recognition using mobile devices  [74] . Eleven male undergraduate students were recruited to show emotive walks feeling five emotions (i.e., joy, anger, sadness, relaxation, and neutrality) with about ten seconds for each walk when a mobile phone camera was recording from the left side of the subject. After data collection, videos were fed into OpenPose model to extract 18 joints positions in pixel xy-coordinate format for each frame. Then three main features 1) euclidean features 2)angular features and 3) speed were calculated. They were euclidean distances between joint positions (i.e., two hands, left hand and left hip, two knees, left knee and left hip, two elbows, two feet) normalized by the height of the bounding box surrounding subject in pixels and angles of joint positions (i.e., two arms, left and right elbow flexion, two legs, left and right knee flexion, vertical angle of the front thigh, angle between front thigh and torso, vertical angle of head inclination, angle between the head and torso) as well as several types of speed (i.e., average total speed, average each step speed, standard deviation of each step speed, maximum and minimum of each step speed). Then features were used for training six models respectively (i.e., SVM, Multi-layer Perceptron, Decision Tree, Naive Bayes, Random Forest, and Logistic Regression). All the computations were done in a server and the result would be sent back to the client side, the mobile phone. Evaluation results presented that single SVM achieved the highest accuracy of 62.1% in classification compared with other classifiers, while human observers achieved 72% accuracy.\n\nIn another research of gait based emotion recognition  [86] , seven subjects' five emotive gaits (i.e., happiness, sadness, anger, fear, neutrality) with 20 seconds duration in each walking sequence were recorded in skeleton format by Kinect v2.0 camera. Geometric and kinematic features were calculated using Laban Movement Analysis (LMA)  [51]  and those features acted as LMA components from four aspects Body, Effort, Shape, and Space. Then the binary chromosome based genetic algorithm was adopted to determine a subset of features that gained the maximum accuracy of four classifiers (i.e., SVM, KNN, Decision Tree, LDA) for emotion recognition. Furthermore, the four classifiers were combined with Score-level and Rank-level Fusion algorithms to boost the overall performance. At last the Score-level Fusion method helped to achieve 80% emotion recognition accuracy, which outperformed any single one of those four models.\n\nMore recently, Shao et al.  [87]  claimed that detecting people at risk of depression in a non-contact and effective method is important. For this purpose, they designed a multimodal gait analysis-based depression detection method that considers both skeleton modality and silhouette modality. In their method, a skeleton feature set including spatiotemporal features and kinematics features are adopted for depression description. Similarly, Lu et al.  [88]  proposed an integrated gait assessment framework to assess the risk of depression based on the collection and analysis of multimedia data. Specifically, they represented the rigid body based on kinetic energy and potential energy. Then, the fast Fourier transform is used to analyze these two energies based on the frequency domain. To improve the efficiency of the gait-based method, Rao et al. explored the person re-identification issue via gait features within 3D skeleton sequences using a new selfsupervised learning paradigm  [89] ,  [90] ,  [91] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Future Directions",
      "text": "In this paper, we have gained insight into how emotions may influence on gaits and learned different techniques for emotion recognition. The development of this field has a lot of potentials, especially with the advancement of intelligent computation. Based on the current research, we highlight and discuss the future research directions as below.\n\nNon-contact Capturing. Different gait-based analyses depend on different devices that capture and measure relevant gait parameters. There are lots of capturing systems and they might be divided into three subsets: wearable sensors attached onto human body, floor sensors and sensors for image or video processing  [92] . It is no doubt about the validity of these types of sensors used in gait detection and measurement as numerous investigations have already demonstrated. Although wearable sensors are very precise to record the trajectories of motion because of their relatively large number of markers and the high sampling rate, they may lead to uncomfortable feeling and unnatural motion as they have to be directly attached to subject's body. Secondly, force platform requires to be paved on the ground which means it is not advisable to upgrade to large scale due to the expense. On the other hand, as the number of the surveillance cameras increase in various public places such as streets, supermarkets, stations, airports and offices, the advantage of using non-contact gait measuring based on image or video becomes so tremendous. It can achieve big data for training comprehensive models to identify the walkers' emotion in order to avoid the overfitting problem  [93] . In addition, it is a more natural way to record people's gaits because there is no disturbance to their emotional representation compared to putting markers on their body. Another new technology, depth sensor  [94]  which can be integrated into RGB sensor, has become popular. It is a remarkable way to measure the distance of things with x, y, z coordinates data. The studies for identifying emotion on the basis of gait through RGB-D information are still rare, which presumably will become one of breakthrough in the future.\n\nIntelligent Classification. From Table  IV , we find that there is no study in the field of gait-based emotion recognition that takes advantage of deep learning techniques, which probably would become one of the future breakthroughs. Deep learning algorithms help to achieve great success in artificial intelligence. In particular, the convolutional neural networks (CNN) have gained massive achievements for image-based task and the recurrent neural networks (RNN) is able to deal with sequence-based problems  [95] . As for gait recognition, deep CNNs can help a lot in classification as well as dealing with cross-view variance  [96]  whereas RNN can tackle with the temporal pattern of the people's gait. Furthermore, the combination of different deep neural networks may yield better outcome since only one single network could not ensure comprehensive solution. For instance, The network C3D+ConvLSTM is hybrid type that fuses CNN and LSTM together. It can lead to good performance for motion recognition  [97] . As the gait based emotion recognition is intrinsically a classification task, it should take full advantage of the fusion of various deep neural networks. Furthermore, if the process is supposed to be achieved massively based on RGB or RGB-D information like surveillance in public, there will be some problems to be handled. For example, many subjects will be captured simultaneously in the view which leads to The key solution is to do parallel target segmentation  [98] . At the same time, the segmentation should be intelligent enough to figure out if the target remains the same identity from the beginning, which still depends on intelligent computation. Apparently, how to make full use of computational intelligence for boosting the classification accuracy is becoming momentous. Large-scale Data Sets. Along with the development of computation intelligence, big data is desperately required for deep network training in order to facilitate the generalization and robustness of the models. To establish a large and comprehensive data set for gait based emotion recognition, several elements should be taken into account. Firstly, the number of participants should be large enough. The second point is the variety of participants. They should include different genders, wide range of age, various height and weight, with or without carrying things etc. The next point would be the emotion elicitation by volunteers. Participants producing approximate emotion expression is quite important because it leads to clear and fine labels for model training. Thus, finding an appropriate way to elicit volunteers' emotion naturally is a significant aspect. Getting volunteers to walk in various spatial contexts is another requirement, since walking on different contexts such as on sand or lawn may influence the gait. Last but not least, it is advisable to collect data through different views of cameras and in different formats for enriching the information and training more generalized models. The HumanID Gait Challenge data set may act as great template to learn for establishing the emotion-gait related data set. It consists of 1,870 sequences from 122 subjects spanning five covariates including change in viewing angle, change in shoe type, change in walking surface, carrying or not carrying a briefcase, and elapsed time between sequences being compared  [99] . These requirements discussed above are not easy to accomplish but are necessary for building a large scale and comprehensive data set.\n\nOnline Emotion Prediction. If the requirements above are fulfilled, as an ultimate goal, online emotion prediction based on human gait is possible. However, online prediction requires data analysis to be run in a continuous way. Different from object detection which can be achieved almost real-time  [100] , online emotion prediction based on gait has to consider the historic steps since the gait cycle has a duration. Therefore, there is a high demand on computation capability. It is usually time-consuming if the neural network is deep, so real-time recognition could be hard to be ensured. Thus, to develop more efficient methods for online emotion recognition with less computing resources would be a challenging and important task.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vii. Conclusions",
      "text": "This survey paper has provided a comprehensive study on the kinematics in emotional gaits and reviewed emerging studies on gait-based emotion recognition. It has discussed technical details in gait analysis for emotion recognition, including data collection, data preprocessing, feature selection, dimension reduction, and classification. We want to conclude that gait is a practical modality for emotion recognition, apart from its existing functions of motion abnormality detection, neural system disorder diagnosis, and walker identification for security reasons. Gait analysis is able to fill the gaps in automated emotion recognition when neither speech nor facial expression is feasible in long-distance observation. The technique of gait analysis is of great practical value even though it still has a lot of research challenges. Fortunately, there are more and more non-invasive, cost-effective, and unobtrusive sensors available nowadays, like surveillance cameras installed in public, which make the collection of big data possible. Coupling with the development of intelligent computation, gait-based emotion recognition has great potential to be fully automated and improved to a higher level to support a broader range of applications.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The pleasure dimension identiﬁes the",
      "page": 2
    },
    {
      "caption": "Figure 2: ). Two main phases are shown in the gait cycle: In the",
      "page": 2
    },
    {
      "caption": "Figure 1: The pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each",
      "page": 3
    },
    {
      "caption": "Figure 3: summarizes the emotion models and the gait cycle",
      "page": 3
    },
    {
      "caption": "Figure 2: Diagram of gait cycle. The right foot (red) shows the gait cycle which can be parted into stance and swing in terms of phase. The stance can be",
      "page": 4
    },
    {
      "caption": "Figure 3: Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait initiation and cycle can be inﬂuenced",
      "page": 4
    },
    {
      "caption": "Figure 4: 25 joints of human generated by Kinect",
      "page": 6
    },
    {
      "caption": "Figure 5: Given two kinds of features above, for recognition,",
      "page": 9
    },
    {
      "caption": "Figure 5: Component description of Wk. It consists of the mean posture Pmean,",
      "page": 9
    },
    {
      "caption": "Figure 6: ). The distinct improvements",
      "page": 9
    },
    {
      "caption": "Figure 6: Procedure of emotion recognition based on Kinect. Joints selections",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract—Human gait refers to a daily motion that represents": "not only mobility, but\nit can also be used to identify the walker",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "treatment planning and monitoring of disease progression [5],"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "by either human observers or computers. Recent\nstudies reveal",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "[6].\nIn addition, gait provides useful social knowledge to the"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "that gait even conveys\ninformation about\nthe walker’s emotion.",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "observers. Research has revealed that human observers are able"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "Individuals\nin different emotion states may show different gait",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "patterns. The mapping between various emotions and gait pat-",
          "gait disturbance, which can be applied to functional diagnosis,": "to recognize themselves and other people that they are familiar"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "terns provides a new source for automated emotion recognition.",
          "gait disturbance, which can be applied to functional diagnosis,": "with even from the point-light depicted or\nimpoverished gait"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "Compared to traditional\nemotion detection biometrics,\nsuch as",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "patterns\n[7],\n[8],\nindicating that gait\nis unique. The identity"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "facial\nexpression,\nspeech, and physiological parameters, gait\nis",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "information is embedded in the gait signature, which has been"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "remotely observable, more difﬁcult\nto imitate, and requires\nless",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "considered as a unique biometric identiﬁcation tool. With the"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "cooperation\nfrom the\nsubject. These\nadvantages make\ngait\na",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "promising\nsource\nfor\nemotion\ndetection. This\narticle\nreviews",
          "gait disturbance, which can be applied to functional diagnosis,": "advacements of\ncomputer vision and big data\nanalysis, gait"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "current\nresearch on gait-based emotion detection, particularly",
          "gait disturbance, which can be applied to functional diagnosis,": "recognition\nhas\nbeen widely\nemployed\nfor\nvarious\nsecurity"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "on how gait parameters\ncan be\naffected by different\nemotion",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "applications [9],\n[10]."
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "states\nand how the\nemotion states\ncan be\nrecognized through",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "Furthermore,\nit was\nsuggested that\nemotion expression is"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "distinct\ngait patterns. We\nfocus\non the detailed methods\nand",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "embedded in the body languages\nincluding gait and postural"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "techniques applied in the whole process of emotion recognition:",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "features [11], [12], [13], [14], [15]. Indeed, people in different"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "data\ncollection,\npreprocessing,\nand\nclassiﬁcation. At\nlast, we",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "discuss possible future developments of efﬁcient and effective gait-",
          "gait disturbance, which can be applied to functional diagnosis,": "emotional states show distinct gait kinematics [16],\n[17]. For"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "based emotion recognition using the state of\nthe art\ntechniques",
          "gait disturbance, which can be applied to functional diagnosis,": "instance, studies found that depressed individuals show differ-"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "in intelligent computation and big data.",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "ent gait patterns,\nincluding slower gaits,\nsmaller\nstride size,"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "Index Terms—Emotion recognition,\ngait\nanalysis,\nintelligent",
          "gait disturbance, which can be applied to functional diagnosis,": "shorter double limb support, and cycle duration,\nin contrast\nto"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "computation.",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "the control group [18], [19]. According to the previous studies,"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "human observers are able to identify emotions based on the"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "gait\n[12]. These ﬁndings indicate that gait can be considered"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "I.\nINTRODUCTION",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "as a potential\ninformative source for emotion perception and"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "is a manner of walking of\nindividuals.\nIt",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "H UMAN gait",
          "gait disturbance, which can be applied to functional diagnosis,": "recognition."
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "describes a common but\nimportant daily motion through",
          "gait disturbance, which can be applied to functional diagnosis,": "Human emotion is heavily involved in cognitive process and"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "which\nobservers\ncan\nlearn much\nuseful\ninformation\nabout",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "social\ninteraction.\nIn recent years, automated emotion recog-"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "the walker.\nIn\nclinical\nterms,\napart\nfrom the\ndetection\nof",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "nition becomes a growing ﬁeld along with the development"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "movement\nabnormalities,\nobservation\nof\ngait\npatterns\nalso",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "of\ncomputing techniques.\nIt\nsupports numerous\napplications"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "provides diagnostic clues\nfor multiple neurological disorders",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "in\ninter-personal\nand\nhuman-computer\ninteraction\nsuch\nas"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "such as cerebral palsy, Parkinson’s disease, and Rett syndrome",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "customer services,\ninteractive gaming and e-learning, etc."
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "[1],\n[2],\n[3],\n[4]\nin\nan\nearly\nstage. Clinical\ngait\nanalysis",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "However, current\nresearch on emotion recognition mainly"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "therefore plays\na more\nand more\nimportant\nrole\nin medical",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "focused\non\nfacial\nexpression\n[20],\n[21],\nspeech\n(including"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "care which may prevent patients\nfrom permanent damage.\nIt",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "linguistic and acoustic features)[22] and physiological signals"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "becomes a well developed tool\nfor quantitative assessment of",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "(e.g.\nelectroencephalography,\nelectromyography,\nheart\nrate,"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "blood volume pulse,\netc.)\n[23],\n[24],\n[25],\n[26], while\nrela-"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "(Shihao Xu and Jing Fang are co-ﬁrst authors.)\n(Corresponding authors:",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "Xiping Hu; Wei Wang.)",
          "gait disturbance, which can be applied to functional diagnosis,": "tively few studies\ninvestigate\nthe\nassociation between emo-"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "S. Xu and X. Hu are with the School of Information Science and Engineer-",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "tion and full-body movements. Gait analysis\nshows apparent"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "advantages over\nthe prevailing modalities, which makes\nit a"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "huxp@lzu.edu.cn).",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "J. Fang\nis with Shenzhen\nInstitutes\nof Advanced Technology, Chinese",
          "gait disturbance, which can be applied to functional diagnosis,": "promising tool\nfor emotion recognition. We summarize these"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "Academy of Sciences, Shenzhen 518055, China (e-mail: jing.fang@siat.ac.cn)",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "advantages as follows."
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "W. Wang\nis\nwith\nthe\nSchool\nof\nIntelligent\nSystems\nEngineer-",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "",
          "gait disturbance, which can be applied to functional diagnosis,": "• Human\nbodies\nare\nrelatively\nlarge\nand\nhave multiple"
        },
        {
          "Abstract—Human gait refers to a daily motion that represents": "ing,\nSun\nYat-sen\nUniversity,\nShenzhen,\nGuangdong\n518107,\n(e-mail:",
          "gait disturbance, which can be applied to functional diagnosis,": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": ""
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "huxp@lzu.edu.cn)."
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "J. Fang\nis with Shenzhen\nInstitutes\nof Advanced Technology, Chinese"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "Academy of Sciences, Shenzhen 518055, China (e-mail: jing.fang@siat.ac.cn)"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": ""
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "W. Wang\nis\nwith\nthe\nSchool\nof\nIntelligent\nSystems\nEngineer-"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": ""
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "ing,\nSun\nYat-sen\nUniversity,\nShenzhen,\nGuangdong\n518107,\n(e-mail:"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "wangw328@mail.sysu.edu.cn)."
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "E. Ngai\nis with the Department of Electrical and Electronic Engineering,"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": ""
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "The University of Hong Kong, Hong Kong SAR (e-mail: chngai@eee.hku.hk)."
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": ""
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "Y\n. Guo\nis with\nthe Second Clinical Medical College,\nJinan University,"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "Shenzhen 518055, China (e-mail: xuanyi guo@163.com)."
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "V\n. C. M. Leung is with the College of Computer Science and Software"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": ""
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "Engineering, Shenzhen University, Shenzhen 518055, China, and also with the"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": ""
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "Department of Electrical and Computer Engineering,\nthe University of British"
        },
        {
          "ing, Lanzhou University, Gansu 730000, China (e-mail: xushh16@lzu.edu.cn;": "Columbia, Vancouver, B.C. V6T 1Z4, Canada (e-mail: vleung@ieee.org)."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "positive\narousal,\nand\nnegative\ndominance\naccording\nto\nthe"
        },
        {
          "2": "mapping rules between distinct emotions and the PAD model"
        },
        {
          "2": "[34],\n[35]. This model\nhas\nbeen\nused\nto\nstudy\nnonverbal"
        },
        {
          "2": "communication\nlike\nbody\nlanguage\nin\npsychology[36]. The"
        },
        {
          "2": "third model\nis\nappraisal model which\ndescribes\nhow emo-"
        },
        {
          "2": "tions\ndevelop,\ninﬂuence,\nand\nare\ninﬂuenced\nby\ninteraction"
        },
        {
          "2": "with the\ncircumstances[37],\n[38]. Ortony et\nal.\n[39]\napplied"
        },
        {
          "2": "this appraisal\ntheory to their models and found out\nthat\nthe"
        },
        {
          "2": "parameters\nin the environment\nsuch as\nthe events or objects"
        },
        {
          "2": ""
        },
        {
          "2": "may affect\nthe emotion strength. Because of the complexity of"
        },
        {
          "2": ""
        },
        {
          "2": "individuals’ evaluations (appraisals or estimates) on events that"
        },
        {
          "2": ""
        },
        {
          "2": "cause speciﬁc reactions in themselves,\nthe appraisal method is"
        },
        {
          "2": ""
        },
        {
          "2": "less often applied for recognition of emotional states than the"
        },
        {
          "2": ""
        },
        {
          "2": "two former models."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "III. GAIT"
        },
        {
          "2": ""
        },
        {
          "2": "Human gait refers to a periodical forward task that requires"
        },
        {
          "2": ""
        },
        {
          "2": "precise cooperation of the neural and musculoskeletal systems"
        },
        {
          "2": ""
        },
        {
          "2": "to achieve dynamic balance. This\nrequirement\nis\nso crucial"
        },
        {
          "2": ""
        },
        {
          "2": "since the human kinematic system controls the advancing with"
        },
        {
          "2": ""
        },
        {
          "2": "the support from the two feet. In particular, when only one foot"
        },
        {
          "2": ""
        },
        {
          "2": "provides standing,\nthe body is in a state of\nimbalance and it"
        },
        {
          "2": ""
        },
        {
          "2": "needs\na\ncomplicated mechanism to make\naccurate\nand safe"
        },
        {
          "2": ""
        },
        {
          "2": "foot\ntrajectory. In the next\ntwo subsections, we will\ntalk about"
        },
        {
          "2": ""
        },
        {
          "2": "the gait\ninitiation and the gait cycle since both of them would"
        },
        {
          "2": ""
        },
        {
          "2": "be affected by emotion."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "A. Gait\nInitiation"
        },
        {
          "2": ""
        },
        {
          "2": "As\nan individual\nstarts\nto move\nfrom the\nstatic\nstate, he"
        },
        {
          "2": "or\nshe is doing gait\ninitiation.\nIn this\ninitiation, anticipatory"
        },
        {
          "2": "postural adjustments (APA) plays an important role in breaking"
        },
        {
          "2": "the erect posture and transferring the body center of gravity to"
        },
        {
          "2": "the stepping foot\n[41].\nIt\nis a movement\ninvolving the swing"
        },
        {
          "2": "of a leg forward and leading to imbalance. This imbalance is"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "emotional state."
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "to the human emotional conﬂict. To elicit participants’ emo-\nground,\nthe individual\nis in terminal stance. Once the left\ntoe"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "tions,\nthe pleasant or unpleasant\nimages from the International\ntouches the ground following the terminal stance,\nthe weight"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "Affective Picture System (IAPS) were used as visual stimulus\nis again uphold by both limbs and the preswing phase begins."
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "Passing through the right toe-off (initial swing) event, the body\nand force plates were used to record the ground reaction forces"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "for\nthe movements.\nweight\nis\ntransferred from the right\nto the left\nthis\ntime (the"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "midswing phase). As soon as the right heel strikes again,\nthe"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "We summarize the experimental setups and results from the"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "whole gait cycle is completed and the walker prepares for next"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "research studies related to emotion impacts on gait\ninitiation"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "cycle."
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "in Table I.\nIn [44],\nthe speciﬁc paradigm was “go” or “nogo”"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "Fig. 3 summarizes\nthe emotion models and the gait cycle"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "for volunteers. The “nogo” response (i.e., volunteers were not"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "that we\ndiscussed\nabove.\nIt\nshows\nthe\ngeneral\nrelationship"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "supposed\nto move) was\nrelated\nto\nthe\nstimulus\nof\nneutral"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "between\nemotion\nand\ngait,\nand\ngives\nan\noverview on\nthe"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "images\nshowing only an object whereas\nthe\n“go”\nresponse"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "content of\nthis paper.\nIt\nincludes\nthe aforementioned content"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "(i.e., volunteers\nshould approach or\navoid) was\nfor pleasant"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "in Section II and Section III, namely the models for describing"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "or\nunpleasant\npictures\ncorresponding\nto CO or\nIC tasks."
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "emotion\nand\nthe\ncomponents\nof\ngait,\nrespectively.\nIt\nalso"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "Volunteers were asked to perform responding action as\nsoon"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "contains a brief\nrepresentation of Section IV and Section V"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "as possible after\nthe onset of\nthe images.\nIn [45], participants"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "with\nthe\nupper\npart\nlisting\nthe\ngait\nparameters\nthat would"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "were\nasked to make\neither\nan anterior\nstep (approach) or\na"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "be inﬂuenced by emotions and the bottom part\nshowing the"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "posterior step (withdrawal) as soon as the image was presented"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "process of gait based emotion recognition."
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "and\nto\nremain\nstatic\ntill\nit\ndisappeared. The\nexperiment\nin"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "[46]\nstudied the inﬂuence of a change in the delay between"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "IV. EMOTIONAL IMPACT ON GAIT"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "picture onset and the appearance of “go” action for checking"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "A. On Gait\nInitiation\npeople‘s\nreaction\ntime. The\nchanges\nin\nthe\ndelay\nhad\ntwo"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "conditions, namely the short condition (the word “go” showed"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "Appetitive approach and defensive avoidance are two basic"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "500ms after\nimage onset)\nand the\nlong condition (the word"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "motivational mechanisms\nfor\nfundamentally organizing emo-"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "“go” showed 3000ms after\nimage onset). Research[47] aimed"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "tion in the light of the biphasic theory of emotion [43]. Based"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "at gait initiation as soon as the participants saw the image (i.e.,"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "on this\ntheory,\nstudies\n[44],\n[45],\n[46],\n[47],\n[48] have been"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "onset) or as soon as the image disappeared (offset). In clinical"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "conducted to explore\nthe\nemotion effects on gait\ninitiation."
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "studies,\nthe\nexperiment\nin [48]\nfocused on gait\ninitiation in"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "Theses\nstudies were usually conducted by giving congruent"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "patients with Parkinson’s disease and the patients were asked"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "(CO)\ntasks and incongruent\n(IC)\ntasks to the walkers.\nIn the"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "to make an initial step with their preferred legs after the image"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "CO tasks,\nthe participants were\nasked to show approaching"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "offset and to keep walking at\ntheir self-selected pace."
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "when\nsensing\nthe\npleasant\nstimuli\nor\nthey\nshould\nexpress"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "avoidance responding to the unpleasant stimuli. IC tasks were\nStudies mentioned above revealed that an individual’s emo-"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "tion can affect gait\ninitiation. When the participants encounter\nthe opposite to the CO tasks as the participants were asked to"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "emotional conﬂicts,\nthey seemed to pose a defensive response\nmake approach movements for\nthe unpleasant stimuli or\nthey"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "for\nthe\nIC tasks\nleading\nto\nlonger\nreaction\ntime\n(RT)\nand\nneeded to performed avoidance at\nthe moment\nthey perceived"
        },
        {
          "Fig. 1.\nThe pleasure-arousal-dominance space for emotions[33][40]. The center of each ellipsoid is the mean and the radius is the standard deviation of each": "shorter\nstep\nlength\ncompared\nto CO tasks\nshown\nin Table\nthe pleasant stimuli [49]. Apparently, the IC tasks were related"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "by emotion.\nIn the opposite, emotion could be recognized through the gait pattern. PAD: Pleasure-arousal-dominance."
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "I. This\nis\ninspiring because these features\nlearned from gait\ndifferent emotions could be identiﬁed better than chance level"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "initiation\nshow signiﬁcant\ndifferences\nbetween\npeople with\nwith mean accuracy of 56%, 90%, 74%, 94% for pride, anger,"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "positive and negative emotions. The analysis of gait\ninitiation\nhappiness, and sadness\nrespectively. As\nfor\nthe gait\nfeatures"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "may become a potential method to recognize human’s emotion\nwhich differentiated emotions,\nthe angry gaits were relatively"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "in the future.\nmore heavyfooted than the other gaits, while the sad gaits had"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "less arm swing compared with the other gaits.\nIt also turned"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "out\nthat proud and angry gaits had longer stride lengths than"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "B. On Gait Cycle"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "happy or\nsad gaits. Finally, happy gaits performed faster\nin"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "In\nthis\nsubsection,\na\nfew studies would\nbe\nshared\nfor\npace than the other gaits."
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "showing the performances and characteristics of emotive gaits"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "(see Table III).\nIn Montepare’s\ninvestigation[50],\nten female\nSimilarly,\nthirty observers used Effort-Shape method [51]"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "observers viewed the gaits of ﬁve walkers with four various\nto\nrate\nthe\nqualitative\ngait movements\nof\nsixteen walkers"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "emotions (i.e., sadness, anger, happiness, and pride)\nin order\nexpressing ﬁve emotions (i.e., joy, contentment, anger, sadness,"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "to determine\nthe walkers’\nemotions\nand report\nspeciﬁc gait\nand neutral)\nin Gross’s work [52]. The Effort-Shape analysis"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "features\nobserved. Note\nthat\nthe walkers’\nheads were\nnot\ninvolved\nfour\nfactors\nevaluating\nthe\neffort\nin\nthe walker’s"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "recorded in the gaits to prevent the facial confusion of emotion\nmovements\n(i.e.,\nspace,\ntime,\nenergy, ﬂow)\nand two factors"
        },
        {
          "Fig. 3. Diagram of general relationship between gait and emotion. Three models are used to describe emotion. The gait\ninitiation and cycle can be inﬂuenced": "perception. The investigation showed that gait patterns with\ndescribed the\nshape of\nthe body (i.e.,\ntorso shape\nand limb"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": "RESEARCHES ON IMPACTS OF EMOTION ON GAIT INITIATION."
        },
        {
          "TABLE I": "Ref\nNumber of participants\nCO tasks\nIC tasks\nResults"
        },
        {
          "TABLE I": "Longer RT in IC than CO trials"
        },
        {
          "TABLE I": "15 (age 20-32,\nInitiate gait after\nInitiate gait after\nand the amplitude of early"
        },
        {
          "TABLE I": "[44]"
        },
        {
          "TABLE I": "9 females)\npleasant\nimage onset\nunpleasant\nimage onset\npostural modiﬁcations"
        },
        {
          "TABLE I": "was reduced in IC trials."
        },
        {
          "TABLE I": "Approach after\nApproach after\nUnpleasant\nimages caused"
        },
        {
          "TABLE I": "30 (mean age 22.3 years,\npleasant\nimage onset\nunpleasant\nimage onset\nan initial “freezing” response with"
        },
        {
          "TABLE I": "[45]"
        },
        {
          "TABLE I": "16 females)\nor withdraw after\nor withdraw after\nanalyses of\nthe preparation,"
        },
        {
          "TABLE I": "unpleasant\nimage onset\npleasant\nimage onset\ninitiation,and execution of steps."
        },
        {
          "TABLE I": "Initiate gait once\nInitiate gait once"
        },
        {
          "TABLE I": "the word “go”\nthe word “go”\nMotor\nresponses were faster\nfor"
        },
        {
          "TABLE I": "19 (age 18-26 years,"
        },
        {
          "TABLE I": "appeared 500 ms\nappeared 500 ms\npleasant pictures than unpleasant ones\n[46]"
        },
        {
          "TABLE I": "11 females)"
        },
        {
          "TABLE I": "or 3000 ms after\nor 3000 ms after\nin the short delay of 500 ms."
        },
        {
          "TABLE I": "pleasant\nimage onset\nunpleasant\nimage onset"
        },
        {
          "TABLE I": "Gait was initiated faster"
        },
        {
          "TABLE I": "Initiate gait after\nInitiate gait after"
        },
        {
          "TABLE I": "27 (mean age 28.7 years,\nwith pleasant\nimages at onset and"
        },
        {
          "TABLE I": "pleasant\nimage onset or\nunpleasant\nimage onset\n[47]"
        },
        {
          "TABLE I": "16 females)\nfaster with unpleasant\nimages"
        },
        {
          "TABLE I": "unpleasant\nimage offset\nor pleasant\nimage offset"
        },
        {
          "TABLE I": "at offset with analyses of COP and COG."
        },
        {
          "TABLE I": "For PD patients and healthy older adults,"
        },
        {
          "TABLE I": "threatening pictures speeded the GI"
        },
        {
          "TABLE I": "26 patients"
        },
        {
          "TABLE I": "Initiate gait and walk\nInitiate gait and walk\nand approach-oriented emotional pictures,"
        },
        {
          "TABLE I": "(age 55-80 years,3 females)"
        },
        {
          "TABLE I": "after approach-oriented\nafter withdrawal-oriented\ncompared to withdrawal-oriented pictures,\n[48]"
        },
        {
          "TABLE I": "25 normals"
        },
        {
          "TABLE I": "emotional picture onset\nemotional picture onset\nfacilitated the anticipatory postural"
        },
        {
          "TABLE I": "(age 55-80 years,3 females)"
        },
        {
          "TABLE I": "adjustments of gait\ninitiation with analyses"
        },
        {
          "TABLE I": "of RT and COP."
        },
        {
          "TABLE I": "CO: Congruent.\nIC:\nIncongurent. RT: Reaction time. COP: Center-of-pressure. COG: Center-of-gravity. GI: Gait\ninitiation. PD: Parkinson’s disease."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of RT and COP.": "initiation. PD: Parkinson’s disease."
        },
        {
          "of RT and COP.": "types of\nfeatures mentioned above, ﬂexion angles of eleven"
        },
        {
          "of RT and COP.": "major\njoints\n(head,\nspine, pelvis and left and right\nshoulder,"
        },
        {
          "of RT and COP.": "elbow, hip and knee joints) have been averaged over\nthe gait"
        },
        {
          "of RT and COP.": "cycle. In terms of the posture features, the evident results were"
        },
        {
          "of RT and COP.": "the reduced head angle for sad walking, and increased elbow"
        },
        {
          "of RT and COP.": "angles\nfor\nfear\nand anger while walking. As\nfor movement"
        },
        {
          "of RT and COP.": "features,\nthe happy and angry gaits were linked to increased"
        },
        {
          "of RT and COP.": "joint amplitudes whereas sadness and fear showed a reduction"
        },
        {
          "of RT and COP.": "in joint-angle amplitudes. In addition,\nthe study compared the"
        },
        {
          "of RT and COP.": "emotional gaits with neutral gaits whose speeds were matched"
        },
        {
          "of RT and COP.": "to the former ones\n(with overall velocity difference < 15%)"
        },
        {
          "of RT and COP.": "and it ﬁgured out\nthat\nthe dynamics of\nthe emotion-speciﬁc"
        },
        {
          "of RT and COP.": "features cannot be explained by changes\nin gait velocity.\nIn"
        },
        {
          "of RT and COP.": "Barliya’s study [55],\nthe gait speed was shown to be a crucial"
        },
        {
          "of RT and COP.": "parameter\nthat would be\naffected by various\nemotions. The"
        },
        {
          "of RT and COP.": "amplitude\nof\nthigh\nelevation\nangles\ndiffered\nfrom those\nin"
        },
        {
          "of RT and COP.": "the neutral gait\nfor all affections except\nsadness. Anger was"
        },
        {
          "of RT and COP.": "showing more frequently oriented intersegmental plane than"
        },
        {
          "of RT and COP.": "others."
        },
        {
          "of RT and COP.": "Through the kinematic\nanalysis of\nemotional\n(i.e., happy,"
        },
        {
          "of RT and COP.": "angry, fearful, and sad ) point-light walkers, Halovic and Kroos"
        },
        {
          "of RT and COP.": "[53]\nfound out\nthat both happy and angry gaits showed long"
        },
        {
          "of RT and COP.": "strides with increased arm movement but angry strides had a"
        },
        {
          "of RT and COP.": "faster cadence. Walkers feeling fear were with fast and short"
        },
        {
          "of RT and COP.": "strides. Sad walkers had slow short strides gaits representing"
        },
        {
          "of RT and COP.": "the slowest walking pace. The fearful and sad gaits both had"
        },
        {
          "of RT and COP.": ""
        },
        {
          "of RT and COP.": "less\narm movement but\nthe\nformer one mainly moved their"
        },
        {
          "of RT and COP.": ""
        },
        {
          "of RT and COP.": "lower arms whilst the latter one had the entire arms movement."
        },
        {
          "of RT and COP.": ""
        },
        {
          "of RT and COP.": "Studies\n[56],\n[57]\napplying\neight-camera\noptoelectronic"
        },
        {
          "of RT and COP.": "motion capture system focused on smoothness of\nlinear and"
        },
        {
          "of RT and COP.": "angular movements of body and limbs to explore the emotive"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": "QUALITIES ASSOCIATED WITH EFFORT-SHAPE FACTORS[52]."
        },
        {
          "TABLE II": "Left-anchor qualitiesa"
        },
        {
          "TABLE II": "Contracted, bowed, shrinking"
        },
        {
          "TABLE II": "Moves close to body, contracted"
        },
        {
          "TABLE II": "Indirect, wandering, diffuse"
        },
        {
          "TABLE II": "Light, delicate, buoyant"
        },
        {
          "TABLE II": "Sustained,\nleisurely, slow"
        },
        {
          "TABLE II": "Free,\nrelaxed, uncontrolled"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "depressed patients showed lower velocity, reduced limb swing"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "and less vertical head movements."
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "1. Head"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "2. Neck"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "3. SpineShoulder"
        },
        {
          "with depression compared wtih the control group. It found that": "4. ShoulderLeft"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "5. ShoulderRight"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "6. ElbowLeft"
        },
        {
          "with depression compared wtih the control group. It found that": "7. ElbowRight"
        },
        {
          "with depression compared wtih the control group. It found that": "8. WristLeft"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "9. WristRight"
        },
        {
          "with depression compared wtih the control group. It found that": "10. ThumbLeft"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "11. ThumbRight"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "12. HandLeft"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "13. HandRight"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "14. HandTipLeft"
        },
        {
          "with depression compared wtih the control group. It found that": "15. HandTipRight"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "16. SpineMid"
        },
        {
          "with depression compared wtih the control group. It found that": "17. SpineBase"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "18. HipLeft"
        },
        {
          "with depression compared wtih the control group. It found that": "19. HipRight"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "20. KneeLeft"
        },
        {
          "with depression compared wtih the control group. It found that": "21. KneeRight"
        },
        {
          "with depression compared wtih the control group. It found that": "22. AnkleLeft"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "23. AnkleRight"
        },
        {
          "with depression compared wtih the control group. It found that": "24. FootLeft"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        },
        {
          "with depression compared wtih the control group. It found that": "25. FootRight"
        },
        {
          "with depression compared wtih the control group. It found that": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a Score = 1.": "b Score = 5."
        },
        {
          "a Score = 1.": "impact on gaits.\nIn the vertical direction,\nthe smoothness of"
        },
        {
          "a Score = 1.": "movements\nincreased with angry and joyful emotions\nin the"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "whole body center-of-mass, head,\nthorax and pelvis compared"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "to sadness.\nIn the anterior-posterior direction, neutral, angry,"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "and joyful emotions only had increased movement smoothness"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "for\nthe\nhead\ncompared\nto\nsadness.\nIn\nangular movements,"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "anger’s movement smoothness in the hip and ankle increased"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "compared\nto\nthat\nof\nsadness.\nSmoothness\nin\nthe\nshoulder"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "increased for anger and joy emotions compared to sadness."
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "Research in [18], [58] further analyzed the gaits of patients"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "with depression compared wtih the control group. It found that"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "depressed patients showed lower velocity, reduced limb swing"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "and less vertical head movements."
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "1. Head"
        },
        {
          "a Score = 1.": "1"
        },
        {
          "a Score = 1.": "2. Neck"
        },
        {
          "a Score = 1.": "2"
        },
        {
          "a Score = 1.": "3. SpineShoulder"
        },
        {
          "a Score = 1.": "3\n4. ShoulderLeft"
        },
        {
          "a Score = 1.": "5"
        },
        {
          "a Score = 1.": "4"
        },
        {
          "a Score = 1.": "5. ShoulderRight"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "6. ElbowLeft"
        },
        {
          "a Score = 1.": "7. ElbowRight"
        },
        {
          "a Score = 1.": "6\n7\n8. WristLeft"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "9. WristRight\n16"
        },
        {
          "a Score = 1.": "10. ThumbLeft"
        },
        {
          "a Score = 1.": "8"
        },
        {
          "a Score = 1.": "9"
        },
        {
          "a Score = 1.": "11. ThumbRight"
        },
        {
          "a Score = 1.": "10\n11"
        },
        {
          "a Score = 1.": "12. HandLeft"
        },
        {
          "a Score = 1.": "12\n13"
        },
        {
          "a Score = 1.": "17"
        },
        {
          "a Score = 1.": "13. HandRight"
        },
        {
          "a Score = 1.": "18\n19\n14\n15"
        },
        {
          "a Score = 1.": "14. HandTipLeft"
        },
        {
          "a Score = 1.": "15. HandTipRight"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "16. SpineMid"
        },
        {
          "a Score = 1.": "17. SpineBase"
        },
        {
          "a Score = 1.": "21\n20"
        },
        {
          "a Score = 1.": "18. HipLeft"
        },
        {
          "a Score = 1.": "19. HipRight"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "20. KneeLeft"
        },
        {
          "a Score = 1.": "21. KneeRight"
        },
        {
          "a Score = 1.": "22. AnkleLeft"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "23\n22\n23. AnkleRight"
        },
        {
          "a Score = 1.": "24. FootLeft"
        },
        {
          "a Score = 1.": "24\n25"
        },
        {
          "a Score = 1.": "25. FootRight"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "Fig. 4.\n25 joints of human generated by Kinect"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "V. GAIT ANALYSIS FOR EMOTION RECOGNITION"
        },
        {
          "a Score = 1.": ""
        },
        {
          "a Score = 1.": "There\nis\na\nlong way\nto\ngo\ntowards\nthe\nultimate\ngoal,"
        },
        {
          "a Score = 1.": "namely, automated emotion recognition through gait patterns."
        },
        {
          "a Score = 1.": "However,\nthere are important observations based on previous"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "helps to remove noise artifacts and burrs and to perform data": ""
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": "feature selection, and so on.\nIn the following"
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": "some of\nthe preprocessing tech-"
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": ""
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": "Filtering"
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": "To\nsmooth\nthe\ndata\nand\nget\nrid\nof\nthe\nnoise\nfor\nthe"
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": "marker\ntrajectories of\nan individual’s walking,\nstudies"
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": "[56],\n[57],\n[52],\n[64] used low-pass Butterworth ﬁlter"
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": "with a cut-off\nfrequency of 6 Hz. Butterworth ﬁlter\nis"
        },
        {
          "helps to remove noise artifacts and burrs and to perform data": "considered\nas\na maximally ﬂat magnitude ﬁlter\n[75]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": "RESEARCHES ON IMPACTS OF EMOTION ON GAIT CYCLE."
        },
        {
          "TABLE III": "Methods"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "5 female observers"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "30 (15 females) observers"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "with Effort-Shape method"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "34 (19 females) observers"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "and the kinematic analysis"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Average ﬂexion angles,"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "nonlinear mixture model,"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "and sparse regression"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "The intersegmental"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "law of coordination"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Measuring spatiotemporal"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "gait parameters"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "and smoothness of"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "linear movements"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Analysing"
        },
        {
          "TABLE III": "forward/backward"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "movements, VT movements,"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "and lateral movements"
        },
        {
          "TABLE III": "of all body segments"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Measuring spatiotemporal"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "gait parameters"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "• Kinematic Features"
        },
        {
          "8": "Gait\ndata\nof\nthe\njoints\nor\nskeleton\ncollected\nby"
        },
        {
          "8": "marker\ntrajectories, Kinect cameras or video-based"
        },
        {
          "8": "pose estimation, are helpful\nfor\nthe measurements"
        },
        {
          "8": "of kinematic features, which are not\nlimited to joint"
        },
        {
          "8": "angles, angular\nrange of motion, displacement and"
        },
        {
          "8": "velocity on basis of various axes[55]. Research in"
        },
        {
          "8": "[54] explored the crucial kinematic features related"
        },
        {
          "8": "to emotion from gait. It indicated that limb ﬂexion is"
        },
        {
          "8": "a key feature for expression of anger and fear on gait"
        },
        {
          "8": "whereas head inclination corresponded to sadness"
        },
        {
          "8": "dominantly. More\nspeciﬁc\nadoption\nof\nkinematic"
        },
        {
          "8": "features could be found in the next section."
        },
        {
          "8": ""
        },
        {
          "8": "C. Emotion Recognition"
        },
        {
          "8": "There\nhave\nbeen\nseveral\nefforts\nto\nclassify\na walker’s"
        },
        {
          "8": "emotion\nthrough\ntheir\ngait\ndata\n(see\nTable\nIV).\nIn\n[62],"
        },
        {
          "8": "Janssen et\nal. used the Kistler\nforce platform to record the"
        },
        {
          "8": "ground reaction forces of walkers who were\nsad,\nangry or"
        },
        {
          "8": "happy through recalling speciﬁc occasion when they felt\nthe"
        },
        {
          "8": "corresponding emotion. After a short period of elicitation of"
        },
        {
          "8": "various emotions, volunteers were asked to walk about 7m at a"
        },
        {
          "8": "self-determined gait velocity on the force platform. The ground"
        },
        {
          "8": "forces of gait after normalization by amplitude and time were"
        },
        {
          "8": "separated into two parts,\ntraining set and test set. The training"
        },
        {
          "8": "part was put\ninto a supervised Multilayer Perceptrons (MLP)"
        },
        {
          "8": "with\n200-111-22\n(input-hidden-output)\nneurons\n(one\noutput"
        },
        {
          "8": "neuron per participant). The test part was\nfor\nthe validation"
        },
        {
          "8": "that MLP model\nachieved\n95.3% accuracy\nof\nrecognizing"
        },
        {
          "8": "each individual. With the help of unsupervised Self-organizing"
        },
        {
          "8": "Maps (SOM), the average emotion recognition rate was 80.8%."
        },
        {
          "8": "Omlor et al.\n[63]\ntried to classify emotions expressed by"
        },
        {
          "8": "walkers with attached markers upon joints. They presented an"
        },
        {
          "8": "original non-linear\nsource\nseparation method that\nefﬁciently"
        },
        {
          "8": "dealt with temporal delays of signals. This technique showed"
        },
        {
          "8": "superiority to PCA and Independent Component Correlation"
        },
        {
          "8": "Algorithm (ICA). The combination of this method and sparse"
        },
        {
          "8": "multivariate regression ﬁgured out spatio-temporal primitives"
        },
        {
          "8": "that were speciﬁc for different emotions in gait. The stunning"
        },
        {
          "8": "part\nis\nthat\nthis method approximated movement\ntrajectories"
        },
        {
          "8": "very\naccurately\nup\nto\n97% based\non\nthree\nlearned\nspatio-"
        },
        {
          "8": "temporal source signals."
        },
        {
          "8": "In [65],\n[84],\nfeature vectorization (i.e computing the auto-"
        },
        {
          "8": "correlation matrix) has been applied in gait\ntrajectories. Four"
        },
        {
          "8": "professional\nactors\nfeeling neutral,\njoy,\nanger,\nsadness,\nand"
        },
        {
          "8": "fear\nrespectively repeated ﬁve times walks\nin a straight\nline"
        },
        {
          "8": "with\njoint-attached\n41 markers\nof\na Vicon motion\ncapture"
        },
        {
          "8": "system.\nIn the vector analysis,\nthe angles, position, velocity,"
        },
        {
          "8": "and acceleration of\njoints have been explored for detecting"
        },
        {
          "8": "emotions. The study found out\nthat\nlower torso motion, waist,"
        },
        {
          "8": "and head angles could signiﬁcantly characterize the emotion"
        },
        {
          "8": "expression\nof walkers whereas\nthe\nleg\nand\narm biased\nthe"
        },
        {
          "8": "detection. More importantly with the utilization of the weight"
        },
        {
          "8": "on vector,\nthe\nemotion recognition has been improved to a"
        },
        {
          "8": "total average accuracy of 78% for a given volunteer."
        },
        {
          "8": "Karg et al.\nstudied thoroughly affect\nrecognition based on"
        },
        {
          "8": "gait patterns [66]. Statistical parameters of\njoint angle trajec-"
        },
        {
          "8": "tories and modeling joint angle trajectories by eigenpostures"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "happiness and anger) accordingly."
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "Features"
        },
        {
          "9": "in time"
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "Joints \nDimension"
        },
        {
          "9": "Classifiers\nPreprocessing"
        },
        {
          "9": "selestion\nreduction"
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "Features"
        },
        {
          "9": ""
        },
        {
          "9": "in"
        },
        {
          "9": "frequency"
        },
        {
          "9": ""
        },
        {
          "9": "Fig. 6.\nProcedure of emotion recognition based on Kinect. Joints selections"
        },
        {
          "9": ""
        },
        {
          "9": "includes 2 wrists, 2 knees and 2 ankles coordinates. Features in time domain"
        },
        {
          "9": "were mean\nand\nvariance\nof\nstride,\nperiod,\nvelocity,\nand\nheight whereas"
        },
        {
          "9": "frequency features were the amplitudes and phases of\ntop 20 frequencies."
        },
        {
          "9": ""
        },
        {
          "9": "In [71], gait pattern was\ntracked by a\ncustomized smart"
        },
        {
          "9": "bracelet with built-in accelerometer, which recorded three di-"
        },
        {
          "9": "mensional acceleration data from different body positions such"
        },
        {
          "9": "as right wrist and ankle. In order to remove unexpected waking"
        },
        {
          "9": "vibrations and reduce data redundancy,\nthey preprocessed the"
        },
        {
          "9": "time\nseries data by using moving average ﬁlter\nand sliding"
        },
        {
          "9": "window. Then, with the calculation of skewness, kurtosis and"
        },
        {
          "9": "standard deviation of\neach one of\nthree\naxes,\nthe\ntemporal"
        },
        {
          "9": ""
        },
        {
          "9": "domain features were selected. What is more, they used Power"
        },
        {
          "9": "Spectral Density (PSD) and Fast Fourier Transform (FFT)\nto"
        },
        {
          "9": "extract\ntemporal\nand\nfrequency\ndomain\nfeatures. After\nthis"
        },
        {
          "9": "part,\nthe ﬁnal number of\nfeatures was 114 (38 features on"
        },
        {
          "9": ""
        },
        {
          "9": "each\naxis),\nand\nthey were\nfed\ninto\n4\ndifferent\nalgorithms"
        },
        {
          "9": "(Decision Tree, SVM, Random Forest,\nand Random Tree)"
        },
        {
          "9": ""
        },
        {
          "9": "respectively. In the eventual result, SVM achieved the highest"
        },
        {
          "9": "accuracy of classiﬁcation, 88.5 %, 91.3%, 88.5%, respectively"
        },
        {
          "9": ""
        },
        {
          "9": "for happiness and neutral, anger and neutral, happiness and"
        },
        {
          "9": "anger. Results\nindicated\nthat\nit was\nefﬁcient\nto\nrecognize"
        },
        {
          "9": ""
        },
        {
          "9": "human emotions (happiness, neutral and angry) with gait data"
        },
        {
          "9": "recorded by a wearable device."
        },
        {
          "9": "In the investigation by Juan et al.\n[72],\nthe accelerometer"
        },
        {
          "9": "data from the walker‘s smart watch has been used to identify"
        },
        {
          "9": "emotion. Fifty participants were divided into groups to perform"
        },
        {
          "9": "three tasks: 1)watch movies and walk 2)listen to music and"
        },
        {
          "9": "walk 3) listen to music while walking. Movies or music were"
        },
        {
          "9": "for\neliciting\nparticipants\nhappiness\nor\nsadness\nfor\nthe\nlater"
        },
        {
          "9": "tasks. Either\nafter\nstimulation or\nengaging in it, volunteers"
        },
        {
          "9": "walked in a 250m S-shaped corridor\nin a\nround trip while"
        },
        {
          "9": "wearing a smart watch . When it comes to feature selection, the"
        },
        {
          "9": "accelerometer data would be divided by sliding windows and"
        },
        {
          "9": "features came from each window as a vector. Then Random"
        },
        {
          "9": "forest and Logistic Regression handled the classiﬁcation for"
        },
        {
          "9": "happiness and sadness based on the feature vectors, and the"
        },
        {
          "9": "majority of accuracies ranged from 60% to 80% compared to"
        },
        {
          "9": "the baseline 50%."
        },
        {
          "9": "Chiu\net\nal.\naimed\nat\nemotion\nrecognition\nusing mobile"
        },
        {
          "9": "devices[74]. Eleven male\nundergraduate\nstudents were\nre-"
        },
        {
          "9": "cruited\nto\nshow emotive walks\nfeeling ﬁve\nemotions\n(i.e.,"
        },
        {
          "9": "joy,\nanger,\nsadness,\nrelaxation,\nand\nneutrality) with\nabout"
        },
        {
          "9": "ten\nseconds\nfor\neach walk when\na mobile\nphone\ncamera"
        },
        {
          "9": "was\nrecording from the\nleft\nside of\nthe\nsubject. After data"
        },
        {
          "9": "collection, videos were fed into OpenPose model\nto extract 18"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "emotion recognition. The development of\nthis ﬁeld has a lot"
        },
        {
          "10": "of potentials, especially with the advancement of\nintelligent"
        },
        {
          "10": "computation. Based on the current research, we highlight and"
        },
        {
          "10": "discuss the future research directions as below."
        },
        {
          "10": "Non-contact Capturing. Different\ngait-based\nanalyses de-"
        },
        {
          "10": "pend on different devices\nthat capture and measure relevant"
        },
        {
          "10": "gait parameters. There are lots of capturing systems and they"
        },
        {
          "10": "might be divided into three subsets: wearable sensors attached"
        },
        {
          "10": "onto\nhuman\nbody, ﬂoor\nsensors\nand\nsensors\nfor\nimage\nor"
        },
        {
          "10": "video processing[92]. It\nis no doubt about\nthe validity of these"
        },
        {
          "10": "types of\nsensors used in gait detection and measurement as"
        },
        {
          "10": "numerous investigations have already demonstrated. Although"
        },
        {
          "10": "wearable sensors are very precise to record the trajectories of"
        },
        {
          "10": "motion because of\ntheir\nrelatively large number of markers"
        },
        {
          "10": "and the high sampling rate,\nthey may lead to uncomfortable"
        },
        {
          "10": "feeling\nand\nunnatural motion\nas\nthey\nhave\nto\nbe\ndirectly"
        },
        {
          "10": "attached to subject’s body. Secondly,\nforce platform requires"
        },
        {
          "10": "to be paved on the ground which means\nit\nis not advisable"
        },
        {
          "10": "to upgrade\nto large\nscale due\nto the\nexpense. On the other"
        },
        {
          "10": "hand, as\nthe number of\nthe surveillance cameras\nincrease in"
        },
        {
          "10": "various public places\nsuch as\nstreets,\nsupermarkets,\nstations,"
        },
        {
          "10": "airports and ofﬁces,\nthe advantage of using non-contact gait"
        },
        {
          "10": "measuring based on image or video becomes so tremendous."
        },
        {
          "10": "It can achieve big data for\ntraining comprehensive models to"
        },
        {
          "10": "identify the walkers’\nemotion in order\nto avoid the overﬁt-"
        },
        {
          "10": "ting problem [93].\nIn addition,\nit\nis\na more natural way to"
        },
        {
          "10": "record people’s gaits because there is no disturbance to their"
        },
        {
          "10": "emotional representation compared to putting markers on their"
        },
        {
          "10": "body. Another new technology, depth sensor\n[94] which can"
        },
        {
          "10": "be\nintegrated into RGB sensor, has become popular.\nIt\nis\na"
        },
        {
          "10": "remarkable way to measure the distance of things with x, y, z"
        },
        {
          "10": "coordinates data. The studies\nfor\nidentifying emotion on the"
        },
        {
          "10": "basis of gait\nthrough RGB-D information are still rare, which"
        },
        {
          "10": "presumably will become one of breakthrough in the future."
        },
        {
          "10": "Intelligent Classiﬁcation. From Table IV, we ﬁnd that\nthere"
        },
        {
          "10": "is\nno\nstudy\nin\nthe ﬁeld\nof\ngait-based\nemotion\nrecognition"
        },
        {
          "10": "that\ntakes advantage of deep learning techniques, which prob-"
        },
        {
          "10": "ably would become one of\nthe\nfuture breakthroughs. Deep"
        },
        {
          "10": "learning algorithms help to achieve great success in artiﬁcial"
        },
        {
          "10": "intelligence.\nIn particular,\nthe convolutional neural networks"
        },
        {
          "10": "(CNN)\nhave\ngained massive\nachievements\nfor\nimage-based"
        },
        {
          "10": "task and the recurrent neural networks (RNN)\nis able to deal"
        },
        {
          "10": "with sequence-based problems\n[95]. As\nfor gait\nrecognition,"
        },
        {
          "10": "deep CNNs can help a lot\nin classiﬁcation as well as dealing"
        },
        {
          "10": "with cross-view variance [96] whereas RNN can tackle with"
        },
        {
          "10": "the\ntemporal\npattern\nof\nthe\npeople’s\ngait. Furthermore,\nthe"
        },
        {
          "10": "combination\nof\ndifferent\ndeep\nneural\nnetworks may\nyield"
        },
        {
          "10": "better\noutcome\nsince\nonly\none\nsingle\nnetwork\ncould\nnot"
        },
        {
          "10": "ensure\ncomprehensive\nsolution.\nFor\ninstance, The\nnetwork"
        },
        {
          "10": "C3D+ConvLSTM is hybrid type that\nfuses CNN and LSTM"
        },
        {
          "10": "together. It can lead to good performance for motion recogni-"
        },
        {
          "10": "tion [97]. As the gait based emotion recognition is intrinsically"
        },
        {
          "10": "a\nclassiﬁcation\ntask,\nit\nshould\ntake\nfull\nadvantage\nof\nthe"
        },
        {
          "10": "fusion of various deep neural networks. Furthermore,\nif\nthe"
        },
        {
          "10": "process is supposed to be achieved massively based on RGB"
        },
        {
          "10": ""
        },
        {
          "10": "or RGB-D information like surveillance in public,\nthere will"
        },
        {
          "10": "be some problems to be handled. For example, many subjects"
        },
        {
          "10": "will be\ncaptured simultaneously in the view which leads\nto"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": "STUDIES ON EMOTION RECOGNITION BASED ON GAIT."
        },
        {
          "TABLE IV": "Method/Classiﬁer"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Multilayer Perceptron,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Self-organizing Maps"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Original non-linear"
        },
        {
          "TABLE IV": "source separation,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Sparse"
        },
        {
          "TABLE IV": "Multivariate Regression"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Similarity index"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "KNN,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Naive Bayes,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "SVM"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "LDA,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Naive Bayes,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "SVM"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "with PCA features"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Naive Bayes,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Random Forest,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "SVM,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "SMO"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Decision Tree,"
        },
        {
          "TABLE IV": "SVM,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Random Forest,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Random Tree"
        },
        {
          "TABLE IV": "with PCA features"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Random Forest,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Logistic Regression"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "SVM,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Decision Tree,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Naive Bayes,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Random Forest,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Logistic Regression,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Multilayer Perceptron"
        },
        {
          "TABLE IV": "KNN,SVM,LDA,"
        },
        {
          "TABLE IV": "Decision Tree,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Genetic algorithm,"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Score-level Fusion,"
        },
        {
          "TABLE IV": "Rank-level Fusion"
        },
        {
          "TABLE IV": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "that gait\nis a practical modality for emotion recognition, apart"
        },
        {
          "12": "from its existing functions of motion abnormality detection,"
        },
        {
          "12": "neural\nsystem disorder\ndiagnosis,\nand walker\nidentiﬁcation"
        },
        {
          "12": "for\nsecurity reasons. Gait analysis\nis able to ﬁll\nthe gaps\nin"
        },
        {
          "12": "automated emotion recognition when neither speech nor facial"
        },
        {
          "12": "expression is feasible in long-distance observation. The tech-"
        },
        {
          "12": "nique of gait analysis is of great practical value even though"
        },
        {
          "12": "it still has a lot of\nresearch challenges. Fortunately,\nthere are"
        },
        {
          "12": "more and more non-invasive, cost-effective, and unobtrusive"
        },
        {
          "12": "sensors available nowadays, like surveillance cameras installed"
        },
        {
          "12": "in\npublic, which make\nthe\ncollection\nof\nbig\ndata\npossible."
        },
        {
          "12": "Coupling with\nthe\ndevelopment\nof\nintelligent\ncomputation,"
        },
        {
          "12": "gait-based emotion recognition has great potential\nto be fully"
        },
        {
          "12": "automated and improved to a higher level\nto support a broader"
        },
        {
          "12": "range of applications."
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": "REFERENCES"
        },
        {
          "12": ""
        },
        {
          "12": "[1]\nJ. R. Gage,\n“Gait\nanalysis. An\nessential\ntool\nin\nthe\ntreatment\nof"
        },
        {
          "12": ""
        },
        {
          "12": "cerebral palsy.” Clinical orthopaedics and related research, no. 288,"
        },
        {
          "12": ""
        },
        {
          "12": "pp. 126–34, mar 1993.\n[Online]. Available: http://www.ncbi.nlm.nih."
        },
        {
          "12": "gov/pubmed/8458125"
        },
        {
          "12": "[2] K.\nJellinger,\nD.\nArmstrong,\nH.\nY\n.\nZoghbi,\nand\nA.\nK.\nPercy,"
        },
        {
          "12": ""
        },
        {
          "12": "“Neuropathology of Rett\nsyndrome.” Acta neuropathologica, vol. 76,"
        },
        {
          "12": ""
        },
        {
          "12": "no. 2, pp. 142–58, 1988. [Online]. Available: http://www.ncbi.nlm.nih."
        },
        {
          "12": "gov/pubmed/2900587"
        },
        {
          "12": "[3]\nJ.\nJankovic,\n“Parkinson’s\ndisease:\nclinical\nfeatures\nand\ndiagnosis.”"
        },
        {
          "12": ""
        },
        {
          "12": "Journal of neurology, neurosurgery, and psychiatry, vol. 79, no. 4, pp."
        },
        {
          "12": ""
        },
        {
          "12": "368–76,\napr 2008.\n[Online]. Available: http://www.ncbi.nlm.nih.gov/"
        },
        {
          "12": "pubmed/18344392"
        },
        {
          "12": "[4]\nZ. You, Z. You, Y. Li, S. Zhao, H. Ren,\nand X. Hu,\n“Alzheimer’s"
        },
        {
          "12": ""
        },
        {
          "12": "disease distinction based on gait feature analysis,” in 2020 IEEE Inter-"
        },
        {
          "12": ""
        },
        {
          "12": "national Conference on E-health Networking, Application & Services"
        },
        {
          "12": "(HEALTHCOM).\nIEEE, 2021, pp. 1–6."
        },
        {
          "12": "[5] R.\nBaker, A.\nEsquenazi, M. G.\nBenedetti,\nand K. Desloovere,"
        },
        {
          "12": ""
        },
        {
          "12": "European\njournal\nof\nphysical\nand\n“Gait\nanalysis:\nclinical\nfacts.”"
        },
        {
          "12": ""
        },
        {
          "12": "rehabilitation medicine, vol. 52, no. 4, pp. 560–74, aug 2016. [Online]."
        },
        {
          "12": "Available: http://www.ncbi.nlm.nih.gov/pubmed/27618499"
        },
        {
          "12": "[6] G. Giorgi, A. Saracino,\nand F. Martinelli,\n“Using\nrecurrent\nneural"
        },
        {
          "12": ""
        },
        {
          "12": "networks for continuous authentication through gait analysis,” Pattern"
        },
        {
          "12": ""
        },
        {
          "12": "Recognition Letters, vol. 147, pp. 157–163, 2021."
        },
        {
          "12": "[7]\nJ. E. Cutting and L. T. Kozlowski, “Recognizing friends by their walk:"
        },
        {
          "12": "the Psychonomic\nGait perception without familiarity cues,” Bulletin of"
        },
        {
          "12": ""
        },
        {
          "12": "Society, vol. 9, no. 5, pp. 353–356, may 1977.\n[Online]. Available:"
        },
        {
          "12": ""
        },
        {
          "12": "http://link.springer.com/10.3758/BF03337021"
        },
        {
          "12": "[8]\nF. Loula, S. Prasad, K. Harber, and M. Shiffrar, “Recognizing people"
        },
        {
          "12": "Journal of\nexperimental psychology. Human\nfrom their movement.”"
        },
        {
          "12": ""
        },
        {
          "12": "perception and performance, vol. 31, no. 1, pp. 210–220,\nfeb 2005."
        },
        {
          "12": ""
        },
        {
          "12": "[Online]. Available: http://www.ncbi.nlm.nih.gov/pubmed/15709874"
        },
        {
          "12": "[9] A. Kale, A.\nSundaresan, A.\nRajagopalan, N.\nCuntoor, A.\nRoy-"
        },
        {
          "12": "Chowdhury, V. Kruger, and R. Chellappa, “Identiﬁcation of Humans"
        },
        {
          "12": ""
        },
        {
          "12": "IEEE\nTransactions\non\nImage\nUsing Gait,”\nProcessing,\nvol.\n13,"
        },
        {
          "12": ""
        },
        {
          "12": "no.\n9,\npp.\n1163–1173,\nsep\n2004.\n[Online].\nAvailable:\nhttp:"
        },
        {
          "12": "//ieeexplore.ieee.org/document/1323098/"
        },
        {
          "12": "[10] N. Boulgouris, D. Hatzinakos, and K. Plataniotis, “Gait\nrecognition: a"
        },
        {
          "12": "challenging signal processing technology for biometric identiﬁcation,”"
        },
        {
          "12": ""
        },
        {
          "12": "IEEE Signal Processing Magazine, vol. 22, no. 6, pp. 78–90, 2005."
        },
        {
          "12": "[Online]. Available: http://ieeexplore.ieee.org/document/1550191/"
        },
        {
          "12": "[11] G.\nCastellano,\nS.\nD.\nVillalba,\nand\nA.\nCamurri,\n“Recognising"
        },
        {
          "12": "Human Emotions\nfrom Body Movement and Gesture Dynamics,” in"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "variety of participants. They should include different genders,": "wide range of age, various height and weight, with or without",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "carrying\nthings\netc. The\nnext\npoint would\nbe\nthe\nemotion",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "REFERENCES"
        },
        {
          "variety of participants. They should include different genders,": "elicitation by volunteers. Participants producing approximate",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[1]\nJ. R. Gage,\n“Gait\nanalysis. An\nessential\ntool\nin\nthe\ntreatment\nof"
        },
        {
          "variety of participants. They should include different genders,": "emotion expression is quite important because it\nleads to clear",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "cerebral palsy.” Clinical orthopaedics and related research, no. 288,"
        },
        {
          "variety of participants. They should include different genders,": "and ﬁne labels for model training. Thus, ﬁnding an appropriate",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "pp. 126–34, mar 1993.\n[Online]. Available: http://www.ncbi.nlm.nih."
        },
        {
          "variety of participants. They should include different genders,": "way\nto\nelicit\nvolunteers’\nemotion\nnaturally\nis\na\nsigniﬁcant",
          "range of applications.": "gov/pubmed/8458125"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[2] K.\nJellinger,\nD.\nArmstrong,\nH.\nY\n.\nZoghbi,\nand\nA.\nK.\nPercy,"
        },
        {
          "variety of participants. They should include different genders,": "aspect. Getting volunteers to walk in various spatial contexts",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "“Neuropathology of Rett\nsyndrome.” Acta neuropathologica, vol. 76,"
        },
        {
          "variety of participants. They should include different genders,": "is\nanother\nrequirement,\nsince walking on different\ncontexts",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "no. 2, pp. 142–58, 1988. [Online]. Available: http://www.ncbi.nlm.nih."
        },
        {
          "variety of participants. They should include different genders,": "such as on sand or\nlawn may inﬂuence the gait. Last but not",
          "range of applications.": "gov/pubmed/2900587"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[3]\nJ.\nJankovic,\n“Parkinson’s\ndisease:\nclinical\nfeatures\nand\ndiagnosis.”"
        },
        {
          "variety of participants. They should include different genders,": "least,\nit\nis advisable to collect data through different views of",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Journal of neurology, neurosurgery, and psychiatry, vol. 79, no. 4, pp."
        },
        {
          "variety of participants. They should include different genders,": "cameras and in different formats for enriching the information",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "368–76,\napr 2008.\n[Online]. Available: http://www.ncbi.nlm.nih.gov/"
        },
        {
          "variety of participants. They should include different genders,": "and\ntraining more\ngeneralized models. The HumanID Gait",
          "range of applications.": "pubmed/18344392"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[4]\nZ. You, Z. You, Y. Li, S. Zhao, H. Ren,\nand X. Hu,\n“Alzheimer’s"
        },
        {
          "variety of participants. They should include different genders,": "Challenge\ndata\nset may\nact\nas\ngreat\ntemplate\nto\nlearn\nfor",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "disease distinction based on gait feature analysis,” in 2020 IEEE Inter-"
        },
        {
          "variety of participants. They should include different genders,": "establishing the\nemotion-gait\nrelated data\nset.\nIt\nconsists of",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "national Conference on E-health Networking, Application & Services"
        },
        {
          "variety of participants. They should include different genders,": "1,870 sequences from 122 subjects spanning ﬁve covariates in-",
          "range of applications.": "(HEALTHCOM).\nIEEE, 2021, pp. 1–6."
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[5] R.\nBaker, A.\nEsquenazi, M. G.\nBenedetti,\nand K. Desloovere,"
        },
        {
          "variety of participants. They should include different genders,": "cluding change in viewing angle, change in shoe type, change",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "European\njournal\nof\nphysical\nand\n“Gait\nanalysis:\nclinical\nfacts.”"
        },
        {
          "variety of participants. They should include different genders,": "in walking surface, carrying or not carrying a briefcase, and",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "rehabilitation medicine, vol. 52, no. 4, pp. 560–74, aug 2016. [Online]."
        },
        {
          "variety of participants. They should include different genders,": "elapsed time between sequences being compared [99]. These",
          "range of applications.": "Available: http://www.ncbi.nlm.nih.gov/pubmed/27618499"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[6] G. Giorgi, A. Saracino,\nand F. Martinelli,\n“Using\nrecurrent\nneural"
        },
        {
          "variety of participants. They should include different genders,": "requirements discussed above are not easy to accomplish but",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "networks for continuous authentication through gait analysis,” Pattern"
        },
        {
          "variety of participants. They should include different genders,": "are necessary for building a\nlarge\nscale\nand comprehensive",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Recognition Letters, vol. 147, pp. 157–163, 2021."
        },
        {
          "variety of participants. They should include different genders,": "data set.",
          "range of applications.": "[7]\nJ. E. Cutting and L. T. Kozlowski, “Recognizing friends by their walk:"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "the Psychonomic\nGait perception without familiarity cues,” Bulletin of"
        },
        {
          "variety of participants. They should include different genders,": "Online Emotion Prediction.\nIf\nthe requirements above are",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Society, vol. 9, no. 5, pp. 353–356, may 1977.\n[Online]. Available:"
        },
        {
          "variety of participants. They should include different genders,": "fulﬁlled, as an ultimate goal, online emotion prediction based",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "http://link.springer.com/10.3758/BF03337021"
        },
        {
          "variety of participants. They should include different genders,": "on human gait is possible. However, online prediction requires",
          "range of applications.": "[8]\nF. Loula, S. Prasad, K. Harber, and M. Shiffrar, “Recognizing people"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Journal of\nexperimental psychology. Human\nfrom their movement.”"
        },
        {
          "variety of participants. They should include different genders,": "data analysis\nto be run in a continuous way. Different\nfrom",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "perception and performance, vol. 31, no. 1, pp. 210–220,\nfeb 2005."
        },
        {
          "variety of participants. They should include different genders,": "object detection which can be achieved almost real-time [100],",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[Online]. Available: http://www.ncbi.nlm.nih.gov/pubmed/15709874"
        },
        {
          "variety of participants. They should include different genders,": "online emotion prediction based on gait has\nto consider\nthe",
          "range of applications.": "[9] A. Kale, A.\nSundaresan, A.\nRajagopalan, N.\nCuntoor, A.\nRoy-"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Chowdhury, V. Kruger, and R. Chellappa, “Identiﬁcation of Humans"
        },
        {
          "variety of participants. They should include different genders,": "historic steps\nsince the gait cycle has a duration. Therefore,",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "IEEE\nTransactions\non\nImage\nUsing Gait,”\nProcessing,\nvol.\n13,"
        },
        {
          "variety of participants. They should include different genders,": "there is a high demand on computation capability. It\nis usually",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "no.\n9,\npp.\n1163–1173,\nsep\n2004.\n[Online].\nAvailable:\nhttp:"
        },
        {
          "variety of participants. They should include different genders,": "time-consuming if\nthe neural network is deep,\nso real-time",
          "range of applications.": "//ieeexplore.ieee.org/document/1323098/"
        },
        {
          "variety of participants. They should include different genders,": "recognition could be hard to be ensured. Thus, to develop more",
          "range of applications.": "[10] N. Boulgouris, D. Hatzinakos, and K. Plataniotis, “Gait\nrecognition: a"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "challenging signal processing technology for biometric identiﬁcation,”"
        },
        {
          "variety of participants. They should include different genders,": "efﬁcient methods\nfor\nonline\nemotion\nrecognition with\nless",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "IEEE Signal Processing Magazine, vol. 22, no. 6, pp. 78–90, 2005."
        },
        {
          "variety of participants. They should include different genders,": "computing resources would be\na\nchallenging and important",
          "range of applications.": "[Online]. Available: http://ieeexplore.ieee.org/document/1550191/"
        },
        {
          "variety of participants. They should include different genders,": "task.",
          "range of applications.": "[11] G.\nCastellano,\nS.\nD.\nVillalba,\nand\nA.\nCamurri,\n“Recognising"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Human Emotions\nfrom Body Movement and Gesture Dynamics,” in"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Affective Computing and Intelligent\nInteraction.\nBerlin, Heidelberg:"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Springer Berlin Heidelberg,\n2007,\npp.\n71–82.\n[Online]. Available:"
        },
        {
          "variety of participants. They should include different genders,": "VII. CONCLUSIONS",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "http://link.springer.com/10.1007/978-3-540-74889-2{ }7"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[12]\nJ. M. Montepare, S. B. Goldstein, and A. Clausen, “The identiﬁcation"
        },
        {
          "variety of participants. They should include different genders,": "This\nsurvey\npaper\nhas\nprovided\na\ncomprehensive\nstudy",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "of emotions\nfrom gait\ninformation,” Journal of Nonverbal Behavior,"
        },
        {
          "variety of participants. They should include different genders,": "on the kinematics\nin emotional gaits and reviewed emerging",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "vol.\n11,\nno.\n1,\npp.\n33–42,\n1987.\n[Online].\nAvailable:\nhttp:"
        },
        {
          "variety of participants. They should include different genders,": "studies on gait-based emotion recognition.\nIt has discussed",
          "range of applications.": "//link.springer.com/10.1007/BF00999605"
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "[13] M. Coulson, “Attributing Emotion to Static Body Postures: Recognition"
        },
        {
          "variety of participants. They should include different genders,": "technical\ndetails\nin\ngait\nanalysis\nfor\nemotion\nrecognition,",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "Accuracy, Confusions, and Viewpoint Dependence,” Journal of Non-"
        },
        {
          "variety of participants. They should include different genders,": "including data collection, data preprocessing, feature selection,",
          "range of applications.": ""
        },
        {
          "variety of participants. They should include different genders,": "",
          "range of applications.": "verbal Behavior, vol. 28, no. 2, pp. 117–139, 2004. [Online]. Available:"
        },
        {
          "variety of participants. They should include different genders,": "dimension reduction, and classiﬁcation. We want\nto conclude",
          "range of applications.": "http://link.springer.com/10.1023/B:JONB.0000023655.25550.be"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13": "[33]\nS. Zhang, Z. Wu, H. M. Meng, and L. Cai, “Facial expression synthesis"
        },
        {
          "13": "based on emotion dimensions for affective talking avatar,” in Modeling"
        },
        {
          "13": "machine emotions for realizing intelligence.\nSpringer, 2010, pp. 109–"
        },
        {
          "13": "132."
        },
        {
          "13": "[34]\nJ. A. Mikels, B. L. Fredrickson, G. R. Larkin, C. M. Lindberg, S.\nJ."
        },
        {
          "13": "Maglio, and P. A. Reuter-Lorenz, “Emotional category data on images"
        },
        {
          "13": "research\nfrom the\ninternational\naffective\npicture\nsystem,” Behavior"
        },
        {
          "13": "methods, vol. 37, no. 4, pp. 626–630, 2005."
        },
        {
          "13": "[35]\nJ. D. Morris,\n“Observations: Sam:\nthe\nself-assessment manikin;\nan"
        },
        {
          "13": "efﬁcient cross-cultural measurement of emotional\nresponse,” Journal"
        },
        {
          "13": "of advertising research, vol. 35, no. 6, pp. 63–68, 1995."
        },
        {
          "13": "[36] A. Mehrabian, Nonverbal communication.\nRoutledge, 2017."
        },
        {
          "13": "[37]\nE. T. Rolls, Emotion explained.\nOxford University Press, USA, 2005."
        },
        {
          "13": "[38] K. R. Scherer, “Appraisal\ntheory,” Handbook of cognition and emotion,"
        },
        {
          "13": "pp. 637–663, 1999."
        },
        {
          "13": "cognitive\nstructure of\n[39] A. Ortony, G. L. Clore,\nand A. Collins, The"
        },
        {
          "13": "emotions.\nCambridge university press, 1990."
        },
        {
          "13": "[40] X. Li, H. Zhou, S. Song, T. Ran,\nand X. Fu,\n“The\nreliability and"
        },
        {
          "13": "validity of\nthe\nchinese version of\nabbreviated pad emotion scales,”"
        },
        {
          "13": "International Conference\non Affective Computing\nand\nIntelligent\nin"
        },
        {
          "13": "Interaction.\nSpringer, 2005, pp. 513–518."
        },
        {
          "13": "[41]\nS. Bouisset and M. Zattara, “Biomechanical study of the programming"
        },
        {
          "13": "of anticipatory postural adjustments associated with voluntary move-"
        },
        {
          "13": "ment,” journal of Biomechanics, vol. 20, no. 8, pp. 735–742, 1987."
        },
        {
          "13": "[42] C. L. Vaughan, B. L. Davis, C.\nJeremy et al., “Dynamics of human"
        },
        {
          "13": "gait,” 1999."
        },
        {
          "13": "[43]\nP.\nJ. Lang, R. F. Simons, M. Balaban, and R. Simons, Attention and"
        },
        {
          "13": "orienting: Sensory and motivational processes.\nPsychology Press,"
        },
        {
          "13": "2013."
        },
        {
          "13": "[44]\nT. G´elat, L. Coudrat,\nand A. Le Pellec,\n“Gait\ninitiation is\naffected"
        },
        {
          "13": "during emotional conﬂict,” Neuroscience letters, vol. 497, no. 1, pp."
        },
        {
          "13": "64–67, 2011."
        },
        {
          "13": "[45]\nJ. Stins and P. Beek, “Organization of voluntary stepping in response"
        },
        {
          "13": "to emotion-inducing pictures,” Gait & posture, vol. 34, no. 2, pp. 164–"
        },
        {
          "13": "168, 2011."
        },
        {
          "13": "[46]\nT. G´elat and C. F. Chapus, “Reaction time in gait\ninitiation depends on"
        },
        {
          "13": "the time available for affective processing,” Neuroscience letters, vol."
        },
        {
          "13": "609, pp. 69–73, 2015."
        },
        {
          "13": "[47]\nJ. F. Stins, L. M. van Gelder, L. M. Oudenhoven,\nand P.\nJ. Beek,"
        },
        {
          "13": "“Biomechanical organization of gait\ninitiation depends on the timing"
        },
        {
          "13": "of affective processing,” Gait & posture, vol. 41, no. 1, pp. 159–163,"
        },
        {
          "13": "2015."
        },
        {
          "13": "[48] K. M. Naugle, C. J. Hass, D. Bowers, and C. M. Janelle, “Emotional"
        },
        {
          "13": "state\naffects\ngait\ninitiation\nin\nindividuals with\nparkinson’s\ndisease,”"
        },
        {
          "13": "Cognitive, Affective, & Behavioral Neuroscience, vol. 12, no. 1, pp."
        },
        {
          "13": "207–219, 2012."
        },
        {
          "13": "[49] M. Chen and J. A. Bargh, “Consequences of automatic evaluation: Im-"
        },
        {
          "13": "mediate behavioral predispositions to approach or avoid the stimulus,”"
        },
        {
          "13": "Personality and social psychology bulletin, vol. 25, no. 2, pp. 215–224,"
        },
        {
          "13": "1999."
        },
        {
          "13": "[50]\nJ. M. Montepare, S. B. Goldstein, and A. Clausen, “The identiﬁcation"
        },
        {
          "13": "of emotions\nfrom gait\ninformation,” Journal of Nonverbal Behavior,"
        },
        {
          "13": "vol. 11, no. 1, pp. 33–42, 1987."
        },
        {
          "13": "[51] R. Laban\nand L. Ullmann,\n“The mastery\nof movement.” Creative"
        },
        {
          "13": "Activities, p. 200, 1971."
        },
        {
          "13": "[52] M. M. Gross, E. A. Crane, and B. L. Fredrickson, “Effort-shape and"
        },
        {
          "13": "kinematic\nassessment of bodily expression of\nemotion during gait,”"
        },
        {
          "13": "Human movement science, vol. 31, no. 1, pp. 202–221, 2012."
        },
        {
          "13": "[53]\nS. Halovic\nand C. Kroos,\n“Not\nall\nis\nnoticed: Kinematic\ncues\nof"
        },
        {
          "13": "emotion-speciﬁc gait,” Human movement\nscience, vol. 57, pp. 478–"
        },
        {
          "13": "488, 2018."
        },
        {
          "13": "[54] C. L. Roether, L. Omlor, A. Christensen, and M. A. Giese, “Critical"
        },
        {
          "13": "features\nfor\nthe perception of emotion from gait,” Journal of vision,"
        },
        {
          "13": "vol. 9, no. 6, pp. 15–15, 2009."
        },
        {
          "13": "[55] A. Barliya, L. Omlor, M. A. Giese, A. Berthoz, and T. Flash, “Expres-"
        },
        {
          "13": "sion of emotion in the kinematics of\nlocomotion,” Experimental brain"
        },
        {
          "13": "research, vol. 225, no. 2, pp. 159–176, 2013."
        },
        {
          "13": "[56] G. E. Kang and M. M. Gross, “Emotional\ninﬂuences on sit-to-walk in"
        },
        {
          "13": "healthy young adults,” Human movement science, vol. 40, pp. 341–351,"
        },
        {
          "13": "2015."
        },
        {
          "13": "[57]\nE. Kang\nand M. M. Gross,\n“The\neffect\nof\nemotion\non movement"
        },
        {
          "13": "Journal of biome-\nsmoothness during gait\nin healthy young adults,”"
        },
        {
          "13": "chanics, vol. 49, no. 16, pp. 4022–4027, 2016."
        },
        {
          "13": "[58] M. R. Lemke, T. Wendorff, B. Mieth, K. Buhl, and M. Linnemann,"
        },
        {
          "13": "“Spatiotemporal gait patterns during over ground locomotion in major"
        },
        {
          "13": "Journal\nof\npsychiatric\ndepression\ncompared with\nhealthy\ncontrols,”"
        },
        {
          "13": "research, vol. 34, no. 4-5, pp. 277–283, 2000."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "[83]\nS. C. White and D. A. Winter, “Predicting muscle forces in gait\nfrom"
        },
        {
          "14": "emg signals and musculotendon kinematics,” Journal of Electromyog-"
        },
        {
          "14": "raphy and Kinesiology, vol. 2, no. 4, pp. 217–231, 1992."
        },
        {
          "14": "[84] G. Venture,\n“Human\ncharacterization\nand\nemotion\ncharacterization"
        },
        {
          "14": "2010 Annual\nInternational Conference\nof\nthe\nIEEE\nfrom gait,”\nin"
        },
        {
          "14": "Engineering in Medicine and Biology, Aug 2010, pp. 1292–1295."
        },
        {
          "14": "[85]\nJ. A. Russell and A. Mehrabian, “Evidence for a three-factor\ntheory"
        },
        {
          "14": "of emotions,” Journal of\nresearch in Personality, vol. 11, no. 3, pp."
        },
        {
          "14": "273–294, 1977."
        },
        {
          "14": "[86]\nF. Ahmed, B.\nSieu,\nand M. L. Gavrilova,\n“Score\nand\nrank-level"
        },
        {
          "14": "fusion for emotion recognition using genetic algorithm,” in 2018 IEEE"
        },
        {
          "14": ""
        },
        {
          "14": "17th International Conference on Cognitive Informatics & Cognitive"
        },
        {
          "14": ""
        },
        {
          "14": "Computing (ICCI* CC).\nIEEE, 2018, pp. 46–53."
        },
        {
          "14": ""
        },
        {
          "14": "[87] W. Shao, Z. You, L. Liang, X. Hu, C. Li, W. Wang, and B. Hu, “A"
        },
        {
          "14": ""
        },
        {
          "14": "IEEE\nmulti-modal gait\nanalysis-based depression detection system,”"
        },
        {
          "14": ""
        },
        {
          "14": "Journal of Biomedical and Health Informatics, pp. 1–1, 202."
        },
        {
          "14": ""
        },
        {
          "14": "[88] H. Lu, S. Xu, X. Hu, E. Ngai, Y. Guo, W. Wang, and B. Hu, “Post-"
        },
        {
          "14": ""
        },
        {
          "14": "graduate student depression assessment by multimedia gait analysis,”"
        },
        {
          "14": ""
        },
        {
          "14": "IEEE MultiMedia, pp. 1–1, 2022."
        },
        {
          "14": ""
        },
        {
          "14": "[89] H. Rao, S. Wang, X. Hu, M. Tan, Y. Guo, J. Cheng, X. Liu, and B. Hu,"
        },
        {
          "14": ""
        },
        {
          "14": "“A self-supervised gait encoding approach with locality-awareness for"
        },
        {
          "14": ""
        },
        {
          "14": "IEEE Transactions\non\n3d\nskeleton\nbased\nperson\nre-identiﬁcation,”"
        },
        {
          "14": ""
        },
        {
          "14": "Pattern Analysis and Machine Intelligence, pp. 1–1, 2021."
        },
        {
          "14": ""
        },
        {
          "14": "[90] H. Rao, S. Wang, X. Hu, M. Tan, H. Da, J. Cheng, and B. Hu, “Self-"
        },
        {
          "14": ""
        },
        {
          "14": "supervised gait encoding with locality-aware attention for person re-"
        },
        {
          "14": ""
        },
        {
          "14": "the Twenty-Ninth International Con-\nidentiﬁcation,” in Proceedings of"
        },
        {
          "14": ""
        },
        {
          "14": "ference on International Joint Conferences on Artiﬁcial\nIntelligence,"
        },
        {
          "14": ""
        },
        {
          "14": "2021, pp. 898–905."
        },
        {
          "14": ""
        },
        {
          "14": "[91] H. Rao, X. Hu, J. Cheng, and B. Hu, “Sm-sge: A self-supervised multi-"
        },
        {
          "14": ""
        },
        {
          "14": "scale skeleton graph encoding framework for person re-identiﬁcation,”"
        },
        {
          "14": ""
        },
        {
          "14": "the 29th ACM International Conference on Multi-\nin Proceedings of"
        },
        {
          "14": ""
        },
        {
          "14": "media, 2021, pp. 1812–1820."
        },
        {
          "14": "[92] A. Muro-De-La-Herran, B. Garcia-Zapirain, and A. Mendez-Zorrilla,"
        },
        {
          "14": "“Gait analysis methods: An overview of wearable and non-wearable"
        },
        {
          "14": "systems, highlighting clinical applications,” Sensors, vol. 14, no. 2, pp."
        },
        {
          "14": "3362–3394, 2014."
        },
        {
          "14": "[93] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,"
        },
        {
          "14": "no. 7553, p. 436, 2015."
        },
        {
          "14": "[94]\nZ. Zhang, “Microsoft kinect sensor and its effect,” IEEE multimedia,"
        },
        {
          "14": "vol. 19, no. 2, pp. 4–10, 2012."
        },
        {
          "14": "[95]\nP. Wang, W. Li, P. Ogunbona, J. Wan, and S. Escalera, “Rgb-d-based"
        },
        {
          "14": "human motion recognition with deep learning: A survey,” Computer"
        },
        {
          "14": "Vision and Image Understanding, 2018."
        },
        {
          "14": "[96]\nZ. Wu, Y. Huang, L. Wang, X. Wang, and T. Tan, “A comprehensive"
        },
        {
          "14": "study on cross-view gait based human identiﬁcation with deep cnns,”"
        },
        {
          "14": "IEEE Transactions on Pattern Analysis & Machine Intelligence, no. 2,"
        },
        {
          "14": "pp. 209–226, 2017."
        },
        {
          "14": "[97] G. Zhu, L. Zhang, P. Shen, and J. Song, “Multimodal gesture recog-"
        },
        {
          "14": "nition using 3-d convolution and convolutional\nlstm,”\nIEEE Access,"
        },
        {
          "14": "vol. 5, pp. 4517–4524, 2017."
        },
        {
          "14": "[98] H.\nZacharatos, C. Gatzoulis,\nand Y.\nL. Chrysanthou,\n“Automatic"
        },
        {
          "14": "emotion recognition based on body movement analysis: a survey,” IEEE"
        },
        {
          "14": ""
        },
        {
          "14": "computer graphics and applications, vol. 34, no. 6, pp. 35–45, 2014."
        },
        {
          "14": ""
        },
        {
          "14": "[99]\nS. Sarkar, P.\nJ. Phillips, Z. Liu,\nI. R. Vega, P. Grother,\nand K. W."
        },
        {
          "14": ""
        },
        {
          "14": "Bowyer, “The humanid gait challenge problem: Data sets, performance,"
        },
        {
          "14": ""
        },
        {
          "14": "IEEE transactions\non\npattern\nanalysis\nand machine\nand\nanalysis,”"
        },
        {
          "14": ""
        },
        {
          "14": "intelligence, vol. 27, no. 2, pp. 162–177, 2005."
        },
        {
          "14": ""
        },
        {
          "14": "[100]\nS. Ren, K. He, R. Girshick, and J. Sun, “Faster\nr-cnn: Towards\nreal-"
        },
        {
          "14": ""
        },
        {
          "14": "time object detection with region proposal networks,” in Advances in"
        },
        {
          "14": ""
        },
        {
          "14": "neural\ninformation processing systems, 2015, pp. 91–99."
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An essential tool in the treatment of cerebral palsy",
      "authors": [
        "J Gage"
      ],
      "year": "1993",
      "venue": "Clinical orthopaedics and related research"
    },
    {
      "citation_id": "2",
      "title": "Neuropathology of Rett syndrome",
      "authors": [
        "K Jellinger",
        "D Armstrong",
        "H Zoghbi",
        "A Percy"
      ],
      "year": "1988",
      "venue": "Acta neuropathologica"
    },
    {
      "citation_id": "3",
      "title": "Parkinson's disease: clinical features and diagnosis",
      "authors": [
        "J Jankovic"
      ],
      "year": "2008",
      "venue": "Journal of neurology, neurosurgery, and psychiatry"
    },
    {
      "citation_id": "4",
      "title": "Alzheimer's disease distinction based on gait feature analysis",
      "authors": [
        "Z You",
        "Z You",
        "Y Li",
        "S Zhao",
        "H Ren",
        "X Hu"
      ],
      "year": "2021",
      "venue": "2020 IEEE International Conference on E-health Networking, Application & Services (HEALTHCOM)"
    },
    {
      "citation_id": "5",
      "title": "Gait analysis: clinical facts",
      "authors": [
        "R Baker",
        "A Esquenazi",
        "M Benedetti",
        "K Desloovere"
      ],
      "year": "2016",
      "venue": "European journal of physical and rehabilitation medicine"
    },
    {
      "citation_id": "6",
      "title": "Using recurrent neural networks for continuous authentication through gait analysis",
      "authors": [
        "G Giorgi",
        "A Saracino",
        "F Martinelli"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "7",
      "title": "Recognizing friends by their walk: Gait perception without familiarity cues",
      "authors": [
        "J Cutting",
        "L Kozlowski"
      ],
      "year": "1977",
      "venue": "Bulletin of the Psychonomic Society",
      "doi": "10.3758/BF03337021"
    },
    {
      "citation_id": "8",
      "title": "Recognizing people from their movement",
      "authors": [
        "F Loula",
        "S Prasad",
        "K Harber",
        "M Shiffrar"
      ],
      "year": "2005",
      "venue": "Journal of experimental psychology. Human perception and performance"
    },
    {
      "citation_id": "9",
      "title": "Identification of Humans Using Gait",
      "authors": [
        "A Kale",
        "A Sundaresan",
        "A Rajagopalan",
        "N Cuntoor",
        "A Roy-Chowdhury",
        "V Kruger",
        "R Chellappa"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "10",
      "title": "Gait recognition: a challenging signal processing technology for biometric identification",
      "authors": [
        "N Boulgouris",
        "D Hatzinakos",
        "K Plataniotis"
      ],
      "year": "2005",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "11",
      "title": "Recognising Human Emotions from Body Movement and Gesture Dynamics",
      "authors": [
        "G Castellano",
        "S Villalba",
        "A Camurri"
      ],
      "year": "2007",
      "venue": "Affective Computing and Intelligent Interaction",
      "doi": "10.1007/978-3-540-74889-2{_}7"
    },
    {
      "citation_id": "12",
      "title": "The identification of emotions from gait information",
      "authors": [
        "J Montepare",
        "S Goldstein",
        "A Clausen"
      ],
      "year": "1987",
      "venue": "Journal of Nonverbal Behavior",
      "doi": "10.1007/BF00999605"
    },
    {
      "citation_id": "13",
      "title": "Attributing Emotion to Static Body Postures: Recognition Accuracy, Confusions, and Viewpoint Dependence",
      "authors": [
        "M Coulson"
      ],
      "year": "2004",
      "venue": "Journal of Nonverbal Behavior",
      "doi": "10.1023/B:JONB.0000023655.25550.be"
    },
    {
      "citation_id": "14",
      "title": "Bodily expression of emotion",
      "authors": [
        "H Wallbott"
      ],
      "year": "1998",
      "venue": "European Journal of Social Psychology",
      "doi": "10.1002/{%}28SICI{%}291099-0992{%}281998110{%}2928{%}3A6{%}3C879{%}3A{%}3AAID-EJSP901{%}3E3.0.CO{%}3B2-W"
    },
    {
      "citation_id": "15",
      "title": "Human gait analysis in neurodegenerative diseases: a review",
      "authors": [
        "G Cicirelli",
        "D Impedovo",
        "V Dentamaro",
        "R Marani",
        "G Pirlo",
        "T D'orazio"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "16",
      "title": "Effort-Shape and kinematic assessment of bodily expression of emotion during gait",
      "authors": [
        "M Gross",
        "E Crane",
        "B Fredrickson"
      ],
      "year": "2012",
      "venue": "Human Movement Science"
    },
    {
      "citation_id": "17",
      "title": "Feature-level fusion approaches based on multimodal eeg data for depression recognition",
      "authors": [
        "H Cai",
        "Z Qu",
        "Z Li",
        "Y Zhang",
        "X Hu",
        "B Hu"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "18",
      "title": "Embodiment of sadness and depression gait patterns associated with dysphoric mood",
      "authors": [
        "J Michalak",
        "N Troje",
        "J Fischer",
        "P Vollmar",
        "T Heidenreich",
        "D Schulte"
      ],
      "year": "2009",
      "venue": "Psychosomatic medicine"
    },
    {
      "citation_id": "19",
      "title": "Emotion-aware cognitive system in multi-channel cognitive radio ad hoc networks",
      "authors": [
        "X Hu",
        "J Cheng",
        "M Zhou",
        "B Hu",
        "X Jiang",
        "Y Guo",
        "K Bai",
        "F Wang"
      ],
      "year": "2018",
      "venue": "IEEE Communications Magazine"
    },
    {
      "citation_id": "20",
      "title": "Automatic Analysis of Facial Affect: A Survey of Registration, Representation, and Recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Automatic facial expression analysis: a survey",
      "authors": [
        "B Fasel",
        "J Luettin"
      ],
      "year": "2003",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition system using short-term monitoring of physiological signals",
      "authors": [
        "K Kim",
        "S Bang",
        "S Kim"
      ],
      "year": "2004",
      "venue": "Medical & Biological Engineering & Computing",
      "doi": "10.1007/BF02344719"
    },
    {
      "citation_id": "24",
      "title": "Early indicators of vulnerability to depression: The role of rumination and heart rate variability",
      "authors": [
        "T Moretta",
        "S Benvenuti"
      ],
      "year": "2022",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "25",
      "title": "Mechanisms of respiratory depression induced by the combination of buprenorphine and diazepam in rats",
      "authors": [
        "D Vodovar",
        "L Chevillard",
        "F Caillé",
        "P Risède",
        "G Pottier",
        "S Auvity",
        "B Mégarbane",
        "N Tournier"
      ],
      "year": "2022",
      "venue": "British Journal of Anaesthesia"
    },
    {
      "citation_id": "26",
      "title": "Prediction of physical frailty in orthogeriatric patients using sensor insole-based gait analysis and machine learning algorithms: Cross-sectional study",
      "authors": [
        "M Kraus",
        "M Saller",
        "S Baumbach",
        "C Neuerburg",
        "U Stumpf",
        "W Böcker",
        "A Keppler"
      ],
      "year": "2022",
      "venue": "JMIR Medical Informatics"
    },
    {
      "citation_id": "27",
      "title": "Automatic Temporal Segment Detection and Affect Recognition From Face and Body Display",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "28",
      "title": "Perception of the smile and other emotions of the body and face at different distances",
      "authors": [
        "R Walk",
        "K Walters"
      ],
      "year": "1988",
      "venue": "Bulletin of the Psychonomic Society"
    },
    {
      "citation_id": "29",
      "title": "HEAD AND BODY CUES IN THE JUDGMENT OF EMOTION: A REFORMULATION",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1967",
      "venue": "Perceptual and Motor Skills",
      "doi": "10.2466/pms.1967.24.3.711"
    },
    {
      "citation_id": "30",
      "title": "Relationships between gait and emotion in parkinson's disease: A narrative review",
      "authors": [
        "L Avanzino",
        "G Lagravinese",
        "G Abbruzzese",
        "E Pelosin"
      ],
      "year": "2018",
      "venue": "Gait & posture"
    },
    {
      "citation_id": "31",
      "title": "Automatic affect perception based on body gait and posture: A survey",
      "authors": [
        "B Stephens-Fripp",
        "F Naghdy",
        "D Stirling",
        "G Naghdy"
      ],
      "year": "2017",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "32",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "American psychologist"
    },
    {
      "citation_id": "33",
      "title": "Facial expression synthesis based on emotion dimensions for affective talking avatar",
      "authors": [
        "S Zhang",
        "Z Wu",
        "H Meng",
        "L Cai"
      ],
      "year": "2010",
      "venue": "Modeling machine emotions for realizing intelligence"
    },
    {
      "citation_id": "34",
      "title": "Emotional category data on images from the international affective picture system",
      "authors": [
        "J Mikels",
        "B Fredrickson",
        "G Larkin",
        "C Lindberg",
        "S Maglio",
        "P Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "35",
      "title": "Observations: Sam: the self-assessment manikin; an efficient cross-cultural measurement of emotional response",
      "authors": [
        "J Morris"
      ],
      "year": "1995",
      "venue": "Journal of advertising research"
    },
    {
      "citation_id": "36",
      "title": "Nonverbal communication",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2017",
      "venue": "Nonverbal communication"
    },
    {
      "citation_id": "37",
      "title": "Emotion explained",
      "authors": [
        "E Rolls"
      ],
      "year": "2005",
      "venue": "Emotion explained"
    },
    {
      "citation_id": "38",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "K Scherer"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "39",
      "title": "The cognitive structure of emotions",
      "authors": [
        "A Ortony",
        "G Clore",
        "A Collins"
      ],
      "year": "1990",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "40",
      "title": "The reliability and validity of the chinese version of abbreviated pad emotion scales",
      "authors": [
        "X Li",
        "H Zhou",
        "S Song",
        "T Ran",
        "X Fu"
      ],
      "year": "2005",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "41",
      "title": "Biomechanical study of the programming of anticipatory postural adjustments associated with voluntary movement",
      "authors": [
        "S Bouisset",
        "M Zattara"
      ],
      "year": "1987",
      "venue": "journal of Biomechanics"
    },
    {
      "citation_id": "42",
      "title": "Dynamics of human gait",
      "authors": [
        "C Vaughan",
        "B Davis",
        "C Jeremy"
      ],
      "year": "1999",
      "venue": "Dynamics of human gait"
    },
    {
      "citation_id": "43",
      "title": "Attention and orienting: Sensory and motivational processes",
      "authors": [
        "P Lang",
        "R Simons",
        "M Balaban",
        "R Simons"
      ],
      "year": "2013",
      "venue": "Attention and orienting: Sensory and motivational processes"
    },
    {
      "citation_id": "44",
      "title": "Gait initiation is affected during emotional conflict",
      "authors": [
        "T Gélat",
        "L Coudrat",
        "A Pellec"
      ],
      "year": "2011",
      "venue": "Neuroscience letters"
    },
    {
      "citation_id": "45",
      "title": "Organization of voluntary stepping in response to emotion-inducing pictures",
      "authors": [
        "J Stins",
        "P Beek"
      ],
      "year": "2011",
      "venue": "Gait & posture"
    },
    {
      "citation_id": "46",
      "title": "Reaction time in gait initiation depends on the time available for affective processing",
      "authors": [
        "T Gélat",
        "C Chapus"
      ],
      "year": "2015",
      "venue": "Neuroscience letters"
    },
    {
      "citation_id": "47",
      "title": "Biomechanical organization of gait initiation depends on the timing of affective processing",
      "authors": [
        "J Stins",
        "L Van Gelder",
        "L Oudenhoven",
        "P Beek"
      ],
      "year": "2015",
      "venue": "Gait & posture"
    },
    {
      "citation_id": "48",
      "title": "Emotional state affects gait initiation in individuals with parkinson's disease",
      "authors": [
        "K Naugle",
        "C Hass",
        "D Bowers",
        "C Janelle"
      ],
      "year": "2012",
      "venue": "Cognitive, Affective, & Behavioral Neuroscience"
    },
    {
      "citation_id": "49",
      "title": "Consequences of automatic evaluation: Immediate behavioral predispositions to approach or avoid the stimulus",
      "authors": [
        "M Chen",
        "J Bargh"
      ],
      "year": "1999",
      "venue": "Personality and social psychology bulletin"
    },
    {
      "citation_id": "50",
      "title": "The identification of emotions from gait information",
      "authors": [
        "J Montepare",
        "S Goldstein",
        "A Clausen"
      ],
      "year": "1987",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "51",
      "title": "The mastery of movement",
      "authors": [
        "R Laban",
        "L Ullmann"
      ],
      "year": "1971",
      "venue": "Creative Activities"
    },
    {
      "citation_id": "52",
      "title": "Effort-shape and kinematic assessment of bodily expression of emotion during gait",
      "authors": [
        "M Gross",
        "E Crane",
        "B Fredrickson"
      ],
      "year": "2012",
      "venue": "Human movement science"
    },
    {
      "citation_id": "53",
      "title": "Not all is noticed: Kinematic cues of emotion-specific gait",
      "authors": [
        "S Halovic",
        "C Kroos"
      ],
      "year": "2018",
      "venue": "Human movement science"
    },
    {
      "citation_id": "54",
      "title": "Critical features for the perception of emotion from gait",
      "authors": [
        "C Roether",
        "L Omlor",
        "A Christensen",
        "M Giese"
      ],
      "year": "2009",
      "venue": "Journal of vision"
    },
    {
      "citation_id": "55",
      "title": "Expression of emotion in the kinematics of locomotion",
      "authors": [
        "A Barliya",
        "L Omlor",
        "M Giese",
        "A Berthoz",
        "T Flash"
      ],
      "year": "2013",
      "venue": "Experimental brain research"
    },
    {
      "citation_id": "56",
      "title": "Emotional influences on sit-to-walk in healthy young adults",
      "authors": [
        "G Kang",
        "M Gross"
      ],
      "year": "2015",
      "venue": "Human movement science"
    },
    {
      "citation_id": "57",
      "title": "The effect of emotion on movement smoothness during gait in healthy young adults",
      "authors": [
        "E Kang",
        "M Gross"
      ],
      "year": "2016",
      "venue": "Journal of biomechanics"
    },
    {
      "citation_id": "58",
      "title": "Spatiotemporal gait patterns during over ground locomotion in major depression compared with healthy controls",
      "authors": [
        "M Lemke",
        "T Wendorff",
        "B Mieth",
        "K Buhl",
        "M Linnemann"
      ],
      "year": "2000",
      "venue": "Journal of psychiatric research"
    },
    {
      "citation_id": "59",
      "title": "Towards intelligent decision making in emotion-aware applications",
      "authors": [
        "J Wang",
        "S Wang",
        "Y Guo",
        "X Hu",
        "X Li",
        "J Cheng"
      ],
      "year": "2019",
      "venue": "Proceedings of the 52nd Hawaii International Conference on System Sciences"
    },
    {
      "citation_id": "60",
      "title": "Multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network",
      "authors": [
        "W Sheng",
        "X Li"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "61",
      "title": "Deep learning analysis of mobile physiological, environmental and location sensor data for emotion detection",
      "authors": [
        "E Kanjo",
        "E Younis",
        "C Ang"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "62",
      "title": "Recognition of emotions in gait patterns by means of artificial neural nets",
      "authors": [
        "D Janssen",
        "W Schöllhorn",
        "J Lubienetzki",
        "K Fölling",
        "H Kokenge",
        "K Davids"
      ],
      "year": "2008",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "63",
      "title": "Extraction of spatio-temporal primitives of emotional body expressions",
      "authors": [
        "L Omlor",
        "M Giese"
      ],
      "year": "2007",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "64",
      "title": "The influences of emotional intensity for happiness and sadness on walking",
      "authors": [
        "M Destephe",
        "T Maruyama",
        "M Zecca",
        "K Hashimoto",
        "A Takanishi"
      ],
      "year": "2013",
      "venue": "Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual International Conference of the IEEE"
    },
    {
      "citation_id": "65",
      "title": "Recognizing emotions conveyed by human gait",
      "authors": [
        "G Venture",
        "H Kadone",
        "T Zhang",
        "J Grèzes",
        "A Berthoz",
        "H Hicheur"
      ],
      "year": "2014",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "66",
      "title": "Recognition of affect based on gait patterns",
      "authors": [
        "M Karg",
        "K Kuhnlenz",
        "M Buss"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "67",
      "title": "Identifying emotions from noncontact gaits information based on microsoft kinects",
      "authors": [
        "B Li",
        "C Zhu",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "68",
      "title": "Emotion recognition using kinect motion capture data of human gaits",
      "authors": [
        "S Li",
        "L Cui",
        "C Zhu",
        "B Li",
        "N Zhao",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ"
    },
    {
      "citation_id": "69",
      "title": "A gait assessment framework for depression detection using kinect sensors",
      "authors": [
        "T Wang",
        "C Li",
        "C Wu",
        "C Zhao",
        "J Sun",
        "H Peng",
        "X Hu",
        "B Hu"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "70",
      "title": "A new skeletal representation based on gait for depression detection",
      "authors": [
        "H Lu",
        "W Shao",
        "E Ngai",
        "X Hu",
        "B Hu"
      ],
      "year": "2021",
      "venue": "2020 IEEE International Conference on E-health Networking, Application & Services (HEALTHCOM)"
    },
    {
      "citation_id": "71",
      "title": "Emotion recognition based on customized smart bracelet with built-in accelerometer",
      "authors": [
        "Z Zhang",
        "Y Song",
        "L Cui",
        "X Liu",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ"
    },
    {
      "citation_id": "72",
      "title": "Emotion recognition using smart watch sensor data: Mixed-design study",
      "authors": [
        "J Quiroz",
        "E Geangu",
        "M Yong"
      ],
      "year": "2018",
      "venue": "JMIR mental health"
    },
    {
      "citation_id": "73",
      "title": "Prototypical contrast and reverse prediction: Unsupervised skeleton based action recognition",
      "authors": [
        "S Xu",
        "H Rao",
        "X Hu",
        "J Cheng",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "74",
      "title": "Emotion recognition through gait on mobile devices",
      "authors": [
        "M Chiu",
        "J Shu",
        "P Hui"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)"
    },
    {
      "citation_id": "75",
      "title": "On the theory of filter amplifiers",
      "authors": [
        "S Butterworth"
      ],
      "year": "1930",
      "venue": "Wireless Engineer"
    },
    {
      "citation_id": "76",
      "title": "Electronic filter simulation & design",
      "authors": [
        "G Bianchi",
        "R Sorrentino"
      ],
      "year": "2007",
      "venue": "Electronic filter simulation & design"
    },
    {
      "citation_id": "77",
      "title": "Digital signal processing: principles algorithms and applications",
      "authors": [
        "J Proakis"
      ],
      "year": "2001",
      "venue": "Digital signal processing: principles algorithms and applications"
    },
    {
      "citation_id": "78",
      "title": "Self-esteem recognition based on gait pattern using kinect",
      "authors": [
        "B Sun",
        "Z Zhang",
        "X Liu",
        "B Hu",
        "T Zhu"
      ],
      "year": "2017",
      "venue": "Gait & posture"
    },
    {
      "citation_id": "79",
      "title": "Wavelet-based characterization of gait signal for neurological abnormalities",
      "authors": [
        "E Baratin",
        "L Sugavaneswaran",
        "K Umapathy",
        "C Ioana",
        "S Krishnan"
      ],
      "year": "2015",
      "venue": "Gait & posture"
    },
    {
      "citation_id": "80",
      "title": "Discrete wavelet transform: a tool in smoothing kinematic data",
      "authors": [
        "A Ismail",
        "S Asfour"
      ],
      "year": "1999",
      "venue": "Journal of biomechanics"
    },
    {
      "citation_id": "81",
      "title": "Classification of gait patterns in the time-frequency domain",
      "authors": [
        "M Nyan",
        "F Tay",
        "K Seah",
        "Y Sitoh"
      ],
      "year": "2006",
      "venue": "Journal of biomechanics"
    },
    {
      "citation_id": "82",
      "title": "Singularity detection and processing with wavelets",
      "authors": [
        "S Mallat",
        "W Hwang"
      ],
      "year": "1992",
      "venue": "IEEE transactions on information theory"
    },
    {
      "citation_id": "83",
      "title": "Predicting muscle forces in gait from emg signals and musculotendon kinematics",
      "authors": [
        "S White",
        "D Winter"
      ],
      "year": "1992",
      "venue": "Journal of Electromyography and Kinesiology"
    },
    {
      "citation_id": "84",
      "title": "Human characterization and emotion characterization from gait",
      "authors": [
        "G Venture"
      ],
      "year": "2010",
      "venue": "2010 Annual International Conference of the IEEE Engineering in Medicine and Biology"
    },
    {
      "citation_id": "85",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "86",
      "title": "Score and rank-level fusion for emotion recognition using genetic algorithm",
      "authors": [
        "F Ahmed",
        "B Sieu",
        "M Gavrilova"
      ],
      "year": "2018",
      "venue": "2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI* CC)"
    },
    {
      "citation_id": "87",
      "title": "A multi-modal gait analysis-based depression detection system",
      "authors": [
        "W Shao",
        "Z You",
        "L Liang",
        "X Hu",
        "C Li",
        "W Wang",
        "B Hu"
      ],
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "88",
      "title": "Postgraduate student depression assessment by multimedia gait analysis",
      "authors": [
        "H Lu",
        "S Xu",
        "X Hu",
        "E Ngai",
        "Y Guo",
        "W Wang",
        "B Hu"
      ],
      "year": "2022",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "89",
      "title": "A self-supervised gait encoding approach with locality-awareness for 3d skeleton based person re-identification",
      "authors": [
        "H Rao",
        "S Wang",
        "X Hu",
        "M Tan",
        "Y Guo",
        "J Cheng",
        "X Liu",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "90",
      "title": "Selfsupervised gait encoding with locality-aware attention for person reidentification",
      "authors": [
        "H Rao",
        "S Wang",
        "X Hu",
        "M Tan",
        "H Da",
        "J Cheng",
        "B Hu"
      ],
      "year": "2021",
      "venue": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence"
    },
    {
      "citation_id": "91",
      "title": "Sm-sge: A self-supervised multiscale skeleton graph encoding framework for person re-identification",
      "authors": [
        "H Rao",
        "X Hu",
        "J Cheng",
        "B Hu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "92",
      "title": "Gait analysis methods: An overview of wearable and non-wearable systems, highlighting clinical applications",
      "authors": [
        "A Muro-De-La-Herran",
        "B Garcia-Zapirain",
        "A Mendez-Zorrilla"
      ],
      "year": "2014",
      "venue": "Gait analysis methods: An overview of wearable and non-wearable systems, highlighting clinical applications"
    },
    {
      "citation_id": "93",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "94",
      "title": "Microsoft kinect sensor and its effect",
      "authors": [
        "Z Zhang"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "95",
      "title": "Rgb-d-based human motion recognition with deep learning: A survey",
      "authors": [
        "P Wang",
        "W Li",
        "P Ogunbona",
        "J Wan",
        "S Escalera"
      ],
      "year": "2018",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "96",
      "title": "A comprehensive study on cross-view gait based human identification with deep cnns",
      "authors": [
        "Z Wu",
        "Y Huang",
        "L Wang",
        "X Wang",
        "T Tan"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "97",
      "title": "Multimodal gesture recognition using 3-d convolution and convolutional lstm",
      "authors": [
        "G Zhu",
        "L Zhang",
        "P Shen",
        "J Song"
      ],
      "year": "2017",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "98",
      "title": "Automatic emotion recognition based on body movement analysis: a survey",
      "authors": [
        "H Zacharatos",
        "C Gatzoulis",
        "Y Chrysanthou"
      ],
      "year": "2014",
      "venue": "IEEE computer graphics and applications"
    },
    {
      "citation_id": "99",
      "title": "The humanid gait challenge problem: Data sets, performance, and analysis",
      "authors": [
        "S Sarkar",
        "P Phillips",
        "Z Liu",
        "I Vega",
        "P Grother",
        "K Bowyer"
      ],
      "year": "2005",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "100",
      "title": "Faster r-cnn: Towards realtime object detection with region proposal networks",
      "authors": [
        "S Ren",
        "K He",
        "R Girshick",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    }
  ]
}