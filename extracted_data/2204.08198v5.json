{
  "paper_id": "2204.08198v5",
  "title": "Utnlp At Semeval-2022 Task 6: A Comparative Analysis Of Sarcasm Detection Using Generative-Based And Mutation-Based Data Augmentation",
  "published": "2022-04-18T07:25:27Z",
  "authors": [
    "Amirhossein Abaskohi",
    "Arash Rasouli",
    "Tanin Zeraati",
    "Behnam Bahrak"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sarcasm is a term that refers to the use of words to mock, irritate, or amuse someone. It is commonly used on social media. The metaphorical and creative nature of sarcasm presents a significant difficulty for sentiment analysis systems based on affective computing. The methodology and results of our team, UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in this paper. We put different models, and data augmentation approaches to the test and report on which one works best. The tests begin with traditional machine learning models and progress to transformer-based and attention-based models. We employed data augmentation based on data mutation and data generation. Using RoBERTa and mutation-based data augmentation, our best approach achieved an F1sarcastic of 0.38 in the competition's evaluation phase. After the competition, we fixed our model's flaws and achieved an F1-sarcastic of 0.414.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Billions of internet users use social networks not only to stay in touch with friends, meet new people, and share user-generated content but also to express their opinions on a wide range of topics using a variety of methods such as posting comments, videos, photos, etc. with specific groups of people  (Tungthamthiti et al., 2016) . In these platforms, users could submit information on whatever topic they wanted, with no restrictions on the sort of content they may share. The lack of constraints and individuals' anonymity on these networks led to humorous sarcastic texts.\n\nBecause sarcasm indicates sentiment, detecting sarcasm in a text is critical for anticipating the text's accurate sentiment, making sarcasm detection a valuable tool with multiple applications in * equal contribution domains such as security, health, services, product evaluations, and sales. Sarcasm detection is an essential aspect of creative language comprehension  (Veale et al., 2019)  and online opinion mining  (Kannangara, 2018) . Even for humans, identifying sarcasm is difficult due to heavily contextualized expressions  (Walker et al., 2012) . There are few labeled data resources for sarcasm detection. Any available texts that can be collected (for example, Tweets) contain many issues, such as an evolving dictionary of slang words and abbreviations, requiring many hours of human annotation to prepare the data for any potential use. Furthermore, the nature of sarcasm identification adds to the task's difficulty, as sarcasm may be considered relative and varies significantly across people, depending on a variety of criteria such as the context, area, time, and events surrounding the statement.\n\nIn an attempt to solve this issue, we participated in SemEval-2022 shared task 6  (Abu Farha et al., 2022) , which aims to recognize whether a tweet is sarcastic or not. Our contributions are as follows:\n\n1. We experiment with simple machine learning models like Support Vector Machine (SVM) and various word encodings.\n\n2. To discover the optimum data preprocessing method, we tested the effect of various data preprocessing.\n\n3. We put several data augmentation techniques to the test. mutation-based data augmentation, our top result gets an F1-sarcastic of 0.38. However, we obtain better outcomes, with a 0.414 F1-sarcastic after fixing the problems of our proposed method. The rest of this paper is organized as follows. In Section 2, we discuss the related work, Section 3 introduces the dataset. In Sections 4 and 5, we present our methodology and results, respectively. Finally Section 6 concludes the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "We give a quick review of previous works on sarcasm detection in this part, followed by works on data augmentation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sarcasm Detection On Twitter",
      "text": "Sarcasm detection has been represented as a binary classification issue, with most tweets labeled with specific hashtags (e.g., #sarcasm, #sarcastic) being considered sarcastic. Many techniques in various languages have been proposed using this framework.\n\nIn  (Davidov et al., 2010) , Semi-supervised sarcasm detection experiments were done using a Twitter dataset (5.9 million tweets) and 66,000 Amazon product evaluations. On the product review dataset, they acquired an F-measure of 0.83. On the Twitter dataset, they obtained an F-measure of 0.55 using 5-fold cross-validation on their k-Nearest Neighbor (kNN) like classifier.  (González-Ibánez et al., 2011)  used 900 messages from Twitter sorted into three groups (sarcastic, positive sentiment, and negative sentiment). To find sarcastic tweets, they utilized the hashtags #sarcasm and #sarcastic. SVM with Sequential Minimum Optimization (SMO) and logistic regression were employed as classifiers. The best accuracy for the sarcastic class was 0.65.  (Reyes et al., 2012)  presented elements to capture ambiguity, polarity, unexpectedness, and emotive situations in figurative language. F1-sarcastic of 0.65 was the best result in categorizing irony and general tweets.\n\nThe representativeness and significance of conceptual elements have been investigated in  (Reyes et al., 2013) . Punctuation marks, emoticons, quotations, capitalized words, lexicon-based features, character n-grams, skip-grams, and polarity skipgrams are all examples of these characteristics. Each of the four categories (irony, comedy, education, and politics) in their corpus has 10,000 tweets.\n\nUsing the Naive Bayes and decision trees algorithms, they evaluated two distributional scenarios: balanced distribution and unbalanced distribution (25% ironic tweets and 75% tweets from the three non-ironic categories). The decision trees classified the balanced distribution with an F1-sarcastic of 0.72 and the unbalanced distribution with an F1-sarcastic of 0.53.\n\nOne sort of sarcasm identified by  (Riloff et al., 2013)  is the difference between a good mood and a bad scenario. Using a bootstrapping approach, the authors gathered collections of positive sentiment phrases and negative circumstance words from sarcastic tweets. They suggested a method for classifying tweets as sarcastic if they contain a positive predictive close to a negative context phrase. They used a SVM classifier using unigrams and bigrams as features to evaluate a human-annotated dataset of 3000 tweets (23% sarcastic), getting an F1-sarcastic of 0.48. The F1-sarcastic of the hybrid strategy, which combined the findings of the SVM classifier with their baseline method, was 0.51.  (Lukin and Walker, 2017)  used bootstrapping, syntactic patterns, and a high precision classifier to classify sarcasm and nastiness in online chats. On their snark dataset, they got an F1-sarcastic of 0.57.\n\nIn  (Oprea and Magdy, 2019) , LSTM, Att-LSTM, CNN, SIARN, MIARN, 3CNN, and Dense-LSTM models were used to assess the task dataset that was introduced in  (Oprea and Magdy, 2019) , which is an unbalanced dataset and labeled by the tweets' writers. Using Multi-Dimension Intra-Attention (MIARN)  (Tay et al., 2018)  Network, they could get an F-score of 0.364.\n\nIn  (Guo et al., 2021) , the Latent Optimized Adversarial Neural Transfer (LOANT) model was suggested as a novel latent-optimized adversarial neural transfer model for cross-domain sarcasm detection. LOANT surpasses classical adversarial neural transfer, multitask learning, and meta-learning baselines using stochastic gradient descent (SGD) with a one-step look-ahead and sets a new state-ofthe-art F-score of 0.4101 on the iSarcasm dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Augmentation",
      "text": "Natural Language Processing(NLP) encompasses a wide range of tasks, from text categorization to question answering, but no matter what you do, the quantity of data you have to train your model has a significant influence on the model's performance.\n\nUsing the data you already have, data augmentation techniques are used to produce extra, synthetic data. Augmentation techniques are widely used in computer vision applications, but they may also be used in natural language processing.\n\nIn the instance of Twitter, (Van Hee et al., 2018) and  (Ilić et al., 2018)  found that adding more data from the same domain did not improve the performance for recognizing sarcasm and irony. Although their result is not general for all sarcasm detection tasks and the result of data augmentation depends on the data and augmentation method.\n\n(Lee et al., 2020)'s idea is to make a new datapoint out of the context sequence [c1, c2\" cn] and label it \"NOT SARCASM.\" The sequence could not be identified as \"SARCASM\" without the answer [r1]. They believe that the newly created negative samples will aid the model in focusing on the link between the response [r1] and its contexts  [c1, c2, cn] . They also create positive samples using back-translation procedures  (Berard et al., 2019; Zheng et al., 2019)  in French, Spanish, and Dutch to balance out the quantity of negative examples.\n\nIn  (Feng et al., 2020)  different data augmentation methods were tested on Yelp Reviews dataset  (Yel, 2014)  for GPT-2 generative model  (Radford et al., 2019) . They used \"Random Insertion, Deletion, & Swap\", \"Semantic Text Exchange (STE)\", \"Synthetic Noise\", and \"Keyword Replacement\". They showed in some case data augmentation could help them to reach better performance.\n\nThis paper is the first to look at generative-based and mutation-based data augmentation strategies in sarcasm detection.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "We mostly used the iSarcasm  (Oprea and Magdy, 2019)  dataset in this study. In specific experiments, we integrated the primary dataset with various secondary datasets, including the Sarcasm Headlines Dataset  (Misra and Arora, 2019)  and Sentiment140 dataset  (Go et al., 2009)  to increase the quantity of data and compensate for the lack of sarcastic data. For each dataset, the details are further discussed. It is worthy to mention that all of the supplementary datasets we included had a negative impact on our model's performance. We believe this was the result of a different data gathering method. Because to the differing labeling process and domain, the distribution diverged from that of iSarcasm. As a result, the following sections are solely depen-dent on the iSarcasm dataset, with no other datasets being used.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Main Task Dataset: Isarcasm",
      "text": "According to  (Oprea and Magdy, 2019) , the sarcasm labeling using hashtags to build datasets captures just the sarcasm that the annotators were able to detect, leaving out the intended sarcasm. When the author intends for the content to be sarcastic, it is called intended sarcasm. The iSarcasm dataset includes 4484 tweets: 3707 non-sarcastic and 777 sarcastic. Because some tweets had been erased, we only had access to 3469 tweets for the job. The unbalanced dataset and the scarcity of sarcastic data were two of the most significant issues we encountered. Table  1  displays some of the dataset's annotated remarks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sarcasm Headlines Dataset",
      "text": "Sarcasm Headlines Dataset  (Misra and Arora, 2019; Misra and Grover, 2021)  was gathered from two news websites. It is beneficial since it overcomes the constraints of Twitter datasets due to noise. As the second edition of this dataset includes more data and a greater variety of data than the first version, we chose the second version.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Sentiment140 Dataset",
      "text": "We needed to compensate for the limited data to train our model successfully. As a result, we chose the sentiment140 dataset  (Go et al., 2009)  because it has a large quantity of data and is based on Twitter. The sentiment tweet message is labeled using an automated classification approach in this dataset. The accuracy is more than 80% when using a machine learning algorithm.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Methodology",
      "text": "In this study we examined and analyzed various models and data augmentation strategies for sarcasm detection. First, we go through data augmentation methods; then, we discuss the structure and hyperparameters of these models in this section. The codes of all models are available on GitHub 1  .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Augmentation",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Generator-Based",
      "text": "For this augmentation method, we used GPT-2  (Radford et al., 2019)  generative model to generate  4000 tweets for both sarcastic and non-sarcastic classes. Then we selected 2000 tweets of each class randomly to increase dataset quantity and have more sarcastic samples.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Mutation-Based",
      "text": "We used three distinct ways to change the data in this method: eliminating, replacing with synonyms, and shuffling. These processes were used in the following order: shuffling, deleting, and replacing. The removal and replacement were carried out systematically. We used the words' roots to create a synonym dictionary. Synonym dictionary is created by scarping the Thesaurus website 2  . When a term was chosen to be swapped with its synonyms, we chose one of the synonyms randomly (Figure  1 ). We tried each combination of these processes to find the best data augmentation combination (a total of seven).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Models",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Support Vector Machine (Svm)",
      "text": "We utilized SVM to discover the optimal approaches for dataset preprocessing and word embeddings. For data augmentation, we employed both generator-based and mutation-based methods. We also put other data preprocessing approaches to the test, such as link removal, emoji removal, stop word removal, stemming, and lemmatizing. We utilized TF-IDF, Word2Vec  (Mikolov et al., 2013) , and BERT  (Devlin et al., 2018)  for word embedding. We found that using a regularization value of 10 and a Radial Basis Function (RBF) kernel, BERT word embedding, and no data preprocessing will give us the best results.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Lstm-Based Methods",
      "text": "We begin with the intuition that a memory model can help us reach a better result. So we started with Long Short Term Memory (LSTM) model  (Hochreiter and Schmidhuber, 1997) . We used one LSTM layer followed by time distributed dense layer. We repeated these two layers one more time, and then we used another LSTM layer followed by two dense layers. This model and all of the following models in this section were trained in 10 epochs.\n\nIn addition, we used Bidirectional Long Short Term Memory (BLSTM). Using bidirectional will run the inputs in two directions, one from past to future and the other from future to past. We used one BLSTM layer for this network, followed by a time-distributed dense layer. We repeated these two layers one more time, and then we used another BLSTM layer followed by two dense layers. Furthermore, we combined LSTM and BLSTM with Convolutional Neural Networks (CNNs). CNN layers for feature extraction on input data are paired with LSTM to facilitate sequence prediction in the CNN-LSTM architecture. Although this model is often employed for video datasets,  (Rehman et al., 2019)  demonstrated that it could perform better in sentiment analysis tasks. We used three 1D convolutional layers followed by a 1D global max-pooling layer for the convolutional part. We used these layers at the end of LSTM-based networks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Bert-Based Methods",
      "text": "The use of bidirectional training of transformer and a prominent attention mode for language modeling is BERT's fundamental technological breakthrough  (Devlin et al., 2018) . The researchers describe a new Masked Language Model (MLM) approach that permits bidirectional training in previously tricky models. They found that bidirectionally trained language models can have a better understanding of language context and flow than unidirectional ones.\n\nRobustly Optimized BERT or RoBERTa has a nearly identical architecture to BERT, however, the researchers made some minor adjustments to its architecture and training technique to enhance the results on BERT architecture  (Liu et al., 2019) .\n\nWe used both RoBERTa with twitter-robertabase, which has been trained on near 58 million tweets and finetuned for sentiment analysis with the TweetEval benchmark and BERT with bert-base from Huggingface  (Wolf et al., 2019) . For both models, we employed five epochs, batch size of 32, 500 warmup steps, and a weight decay of 0.01.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Attention-Based Methods",
      "text": "One of the most important achievements in deep learning research in the recent decade is the attention mechanism  (Vaswani et al., 2017) . The Encoder-Decoder model's restriction of encoding the input sequence to one fixed-length vector to decode each output time step is addressed via an attention mechanism. This difficulty is thought to be more prevalent when decoding extended sequences.\n\nWe start with the assumption that if a model with an attention layer is trained to identify sarcasm at the sentence level, the sarcastic words will be the ones the attention layer learns to value. As a result, we added an attention layer to our LSTM-based and BERT-based models. The results will be discussed further.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Google'S T5",
      "text": "Google's T5  (Raffel et al., 2019)  text-to-text model outperformed the human baseline on the GLUE, SQuAD, and CNN/Daily Mail datasets and earned a remarkable 88.9 on the SuperGLUE language benchmark.\n\nWe fine-tuned T5 for our problem and dataset by giving the sarcastic label the target and the tweets as the source. We used two epochs, batch size of 4,",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "In this section we report the results of our models introduced in Section 4. It's important to note that after the competition, we discovered that none of our preprocessing strategies improved the performance of our model. So we were able to get an F1-sarcastic of 0.414 without using any preprocessing methods, which was 0.034 higher than our performance in the competition, which was based on the best combination of preprocessing methods.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Support Vector Machine (Svm)",
      "text": "The optimum augmentation technique, preprocessing method, and word embedding were all determined using the SVM model. Without any augmentation, BERT obtained the greatest F1-sarcastic of 0.2862, compared to 0.2541 and 0.0924 for Word2Vec and TF-IDF, respectively.\n\nWe have also looked at several ways of data augmentation. The F1-sarcastics for shuffling with replacing words, only word elimination, just shuffling, and shuffling with word elimination were the highest in the mutation-based augmentation (Table 2). We also tried these data augmentation and",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Lstm-Based Methods",
      "text": "LSTM obtained an F1-sarcastic of 0.2176 using BERT word embeddings, mutation-based data augmentation, and no preprocessing, whereas BLSTM's F1-sarcastic was 0.2439 using BERT word embeddings, mutation-based data augmentation, and no preprocessing. By adding CNN layers, the F1-sarcastic of the LSTM was increased to 0.2453, and the BLSTM was increased to 0.2751. The CNN model's F1-sarcastic was 0.2263.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Bert-Based Methods",
      "text": "We employed a mutation-based data augmentation approach with no preprocessing for BERT-based procedures. We got an F1-sarcastic of 0.323 using BERT. We achieved our best result with RoBERTa with an F1-sarcastic of 0.414, which was better than  LOANT (Guo et al., 2021)  model on the same dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Attention-Based Methods",
      "text": "Adding attention layers to this job was not helpful, and it decreased our models' performance. RoBERTa's F1-sarcastic dropped to 0.2959 using the attention layer. LSTM model with the attention layer earned an F1-sarcastic of 0.2145. The F1-sarcastic of BLSTM with attention layer was 0.2336.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Google'S T5",
      "text": "Based on the hyperparameters listed in the Section 4, our F1-sarcastic for this model is 0.4038. However, we believe that we may get better results by increasing the tokenization max length, increasing the batch size, and utilizing the t5-large pre-trained model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we reviewed and contrasted a number of sarcasm detection methods. To improve the performance of our model, we experimented with two different types of augmentation. In the job of sarcasm detection, we observed that mutation-based data augmentation can assist us in achieving better results than generative-based data augmentation.\n\nAdditionally, we tested with other deep-learning techniques, including RNN and BERT-based models. Our best system, an ensemble model, has an F1-sarcastic of 0.414.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Effect of shufﬂing, word elimination, and re-",
      "page": 4
    },
    {
      "caption": "Figure 1: ). We tried each combination of these processes",
      "page": 4
    },
    {
      "caption": "Figure 2: Fine-tuning T5 model for sarcasm detection",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "University of Tehran, Tehran, Iran": "t.zeraati,\nbahrak}@ut.ac.ir"
        },
        {
          "University of Tehran, Tehran, Iran": "domains such as security, health, services, prod-"
        },
        {
          "University of Tehran, Tehran, Iran": "uct evaluations, and sales. Sarcasm detection is an"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "essential aspect of creative language comprehen-"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "sion (Veale et al., 2019) and online opinion mining"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "(Kannangara, 2018). Even for humans, identifying"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "sarcasm is difﬁcult due to heavily contextualized"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "expressions (Walker et al., 2012). There are few"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "labeled data resources for sarcasm detection. Any"
        },
        {
          "University of Tehran, Tehran, Iran": "available texts that can be collected (for example,"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "Tweets) contain many issues, such as an evolving"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "dictionary of slang words and abbreviations, requir-"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "ing many hours of human annotation to prepare"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "the data for any potential use.\nFurthermore,\nthe"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "nature of sarcasm identiﬁcation adds to the task’s"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "difﬁculty, as sarcasm may be considered relative"
        },
        {
          "University of Tehran, Tehran, Iran": "and varies signiﬁcantly across people, depending"
        },
        {
          "University of Tehran, Tehran, Iran": "on a variety of criteria such as the context, area,"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "time, and events surrounding the statement."
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "In an attempt to solve this issue, we participated"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "in SemEval-2022 shared task 6 (Abu Farha et al.,"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "2022), which aims to recognize whether a tweet is"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        },
        {
          "University of Tehran, Tehran, Iran": "sarcastic or not. Our contributions are as follows:"
        },
        {
          "University of Tehran, Tehran, Iran": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "0.414."
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "1\nIntroduction"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "Billions of internet users use social networks not"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "only to stay in touch with friends, meet new peo-"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "ple, and share user-generated content but also to"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "express their opinions on a wide range of topics"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "using a variety of methods such as posting com-"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "ments, videos, photos, etc. with speciﬁc groups of"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "people (Tungthamthiti et al., 2016). In these plat-"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "forms, users could submit information on whatever"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "topic they wanted, with no restrictions on the sort"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "of content they may share. The lack of constraints"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "and individuals’ anonymity on these networks led"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "to humorous sarcastic texts."
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "Because sarcasm indicates sentiment, detecting"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "sarcasm in a text\nis critical\nfor anticipating the"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "text’s accurate sentiment, making sarcasm detec-"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": "tion a valuable tool with multiple applications in"
        },
        {
          "model’s ﬂaws and achieved an F1-sarcastic of": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "mutation-based data augmentation, our top result": "gets an F1-sarcastic of 0.38. However, we obtain",
          "Using the Naive Bayes and decision trees algo-": "rithms, they evaluated two distributional scenarios:"
        },
        {
          "mutation-based data augmentation, our top result": "better outcomes, with a 0.414 F1-sarcastic after",
          "Using the Naive Bayes and decision trees algo-": "balanced distribution and unbalanced distribution"
        },
        {
          "mutation-based data augmentation, our top result": "ﬁxing the problems of our proposed method.",
          "Using the Naive Bayes and decision trees algo-": "(25% ironic tweets and 75% tweets from the three"
        },
        {
          "mutation-based data augmentation, our top result": "The rest of\nthis paper\nis organized as follows.",
          "Using the Naive Bayes and decision trees algo-": "non-ironic categories). The decision trees classi-"
        },
        {
          "mutation-based data augmentation, our top result": "In Section 2, we discuss the related work, Section",
          "Using the Naive Bayes and decision trees algo-": "ﬁed the balanced distribution with an F1-sarcastic"
        },
        {
          "mutation-based data augmentation, our top result": "3 introduces the dataset.\nIn Sections 4 and 5, we",
          "Using the Naive Bayes and decision trees algo-": "of 0.72 and the unbalanced distribution with an"
        },
        {
          "mutation-based data augmentation, our top result": "present our methodology and results, respectively.",
          "Using the Naive Bayes and decision trees algo-": "F1-sarcastic of 0.53."
        },
        {
          "mutation-based data augmentation, our top result": "Finally Section 6 concludes the paper.",
          "Using the Naive Bayes and decision trees algo-": "One sort of sarcasm identiﬁed by (Riloff et al.,"
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "2013) is the difference between a good mood and"
        },
        {
          "mutation-based data augmentation, our top result": "2\nRelated Work",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "a bad scenario. Using a bootstrapping approach,"
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "the authors gathered collections of positive sen-"
        },
        {
          "mutation-based data augmentation, our top result": "We give a quick review of previous works on sar-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "timent phrases and negative circumstance words"
        },
        {
          "mutation-based data augmentation, our top result": "casm detection in this part, followed by works on",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "from sarcastic tweets. They suggested a method for"
        },
        {
          "mutation-based data augmentation, our top result": "data augmentation.",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "classifying tweets as sarcastic if they contain a pos-"
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "itive predictive close to a negative context phrase."
        },
        {
          "mutation-based data augmentation, our top result": "2.1\nSarcasm Detection on Twitter",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "They used a SVM classiﬁer using unigrams and"
        },
        {
          "mutation-based data augmentation, our top result": "Sarcasm detection has been represented as a bi-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "bigrams as features to evaluate a human-annotated"
        },
        {
          "mutation-based data augmentation, our top result": "nary classiﬁcation issue, with most tweets labeled",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "dataset of 3000 tweets (23% sarcastic), getting an"
        },
        {
          "mutation-based data augmentation, our top result": "with speciﬁc hashtags (e.g., #sarcasm, #sarcastic)",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "F1-sarcastic of 0.48. The F1-sarcastic of the hybrid"
        },
        {
          "mutation-based data augmentation, our top result": "being considered sarcastic. Many techniques in",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "strategy, which combined the ﬁndings of the SVM"
        },
        {
          "mutation-based data augmentation, our top result": "various languages have been proposed using this",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "classiﬁer with their baseline method, was 0.51."
        },
        {
          "mutation-based data augmentation, our top result": "framework.",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "(Lukin and Walker, 2017) used bootstrapping,"
        },
        {
          "mutation-based data augmentation, our top result": "In (Davidov et al., 2010), Semi-supervised sar-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "syntactic patterns, and a high precision classiﬁer"
        },
        {
          "mutation-based data augmentation, our top result": "casm detection experiments were done using a",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "to classify sarcasm and nastiness in online chats."
        },
        {
          "mutation-based data augmentation, our top result": "Twitter dataset\n(5.9 million tweets) and 66,000",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "On their snark dataset, they got an F1-sarcastic of"
        },
        {
          "mutation-based data augmentation, our top result": "Amazon product evaluations. On the product re-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "0.57."
        },
        {
          "mutation-based data augmentation, our top result": "view dataset, they acquired an F-measure of 0.83.",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "In (Oprea and Magdy, 2019), LSTM, Att-LSTM,"
        },
        {
          "mutation-based data augmentation, our top result": "On the Twitter dataset, they obtained an F-measure",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "CNN, SIARN, MIARN, 3CNN, and Dense-LSTM"
        },
        {
          "mutation-based data augmentation, our top result": "of 0.55 using 5-fold cross-validation on their k-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "models were used to assess the task dataset that was"
        },
        {
          "mutation-based data augmentation, our top result": "Nearest Neighbor (kNN) like classiﬁer.",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "introduced in (Oprea and Magdy, 2019), which is"
        },
        {
          "mutation-based data augmentation, our top result": "(González-Ibánez et al., 2011) used 900 mes-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "an unbalanced dataset and labeled by the tweets’"
        },
        {
          "mutation-based data augmentation, our top result": "sages from Twitter sorted into three groups (sarcas-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "writers. Using Multi-Dimension Intra-Attention"
        },
        {
          "mutation-based data augmentation, our top result": "tic, positive sentiment, and negative sentiment). To",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "(MIARN) (Tay et al., 2018) Network, they could"
        },
        {
          "mutation-based data augmentation, our top result": "ﬁnd sarcastic tweets, they utilized the hashtags #sar-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "get an F-score of 0.364."
        },
        {
          "mutation-based data augmentation, our top result": "casm and #sarcastic. SVM with Sequential Mini-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "In (Guo et al., 2021), the Latent Optimized Ad-"
        },
        {
          "mutation-based data augmentation, our top result": "mum Optimization (SMO) and logistic regression",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "versarial Neural Transfer (LOANT) model was sug-"
        },
        {
          "mutation-based data augmentation, our top result": "were employed as classiﬁers. The best accuracy for",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "gested as a novel latent-optimized adversarial neu-"
        },
        {
          "mutation-based data augmentation, our top result": "the sarcastic class was 0.65.",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "ral transfer model for cross-domain sarcasm detec-"
        },
        {
          "mutation-based data augmentation, our top result": "(Reyes et al., 2012) presented elements to cap-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "tion. LOANT surpasses classical adversarial neu-"
        },
        {
          "mutation-based data augmentation, our top result": "ture ambiguity, polarity, unexpectedness, and emo-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "ral transfer, multitask learning, and meta-learning"
        },
        {
          "mutation-based data augmentation, our top result": "tive situations in ﬁgurative language. F1-sarcastic",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "baselines using stochastic gradient descent (SGD)"
        },
        {
          "mutation-based data augmentation, our top result": "of 0.65 was the best result in categorizing irony and",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "with a one-step look-ahead and sets a new state-of-"
        },
        {
          "mutation-based data augmentation, our top result": "general tweets.",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "the-art F-score of 0.4101 on the iSarcasm dataset."
        },
        {
          "mutation-based data augmentation, our top result": "The representativeness and signiﬁcance of con-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "ceptual elements have been investigated in (Reyes",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "",
          "Using the Naive Bayes and decision trees algo-": "2.2\nData Augmentation"
        },
        {
          "mutation-based data augmentation, our top result": "et al., 2013). Punctuation marks, emoticons, quo-",
          "Using the Naive Bayes and decision trees algo-": ""
        },
        {
          "mutation-based data augmentation, our top result": "tations, capitalized words, lexicon-based features,",
          "Using the Naive Bayes and decision trees algo-": "Natural Language Processing(NLP) encompasses"
        },
        {
          "mutation-based data augmentation, our top result": "character n-grams, skip-grams, and polarity skip-",
          "Using the Naive Bayes and decision trees algo-": "a wide range of tasks, from text categorization to"
        },
        {
          "mutation-based data augmentation, our top result": "grams are all examples of\nthese characteristics.",
          "Using the Naive Bayes and decision trees algo-": "question answering, but no matter what you do, the"
        },
        {
          "mutation-based data augmentation, our top result": "Each of the four categories (irony, comedy, educa-",
          "Using the Naive Bayes and decision trees algo-": "quantity of data you have to train your model has a"
        },
        {
          "mutation-based data augmentation, our top result": "tion, and politics) in their corpus has 10,000 tweets.",
          "Using the Naive Bayes and decision trees algo-": "signiﬁcant inﬂuence on the model’s performance."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Using the data you already have, data augmenta-": "tion techniques are used to produce extra, synthetic",
          "dent on the iSarcasm dataset, with no other datasets": "being used."
        },
        {
          "Using the data you already have, data augmenta-": "data. Augmentation techniques are widely used in",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "3.1\nMain Task Dataset: iSarcasm"
        },
        {
          "Using the data you already have, data augmenta-": "computer vision applications, but they may also be",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "used in natural language processing.",
          "dent on the iSarcasm dataset, with no other datasets": "According to (Oprea and Magdy, 2019),\nthe sar-"
        },
        {
          "Using the data you already have, data augmenta-": "In the instance of Twitter, (Van Hee et al., 2018)",
          "dent on the iSarcasm dataset, with no other datasets": "casm labeling using hashtags to build datasets cap-"
        },
        {
          "Using the data you already have, data augmenta-": "and (Ili´c et al., 2018) found that adding more data",
          "dent on the iSarcasm dataset, with no other datasets": "tures just the sarcasm that the annotators were able"
        },
        {
          "Using the data you already have, data augmenta-": "from the same domain did not\nimprove the per-",
          "dent on the iSarcasm dataset, with no other datasets": "to detect, leaving out the intended sarcasm. When"
        },
        {
          "Using the data you already have, data augmenta-": "formance for recognizing sarcasm and irony. Al-",
          "dent on the iSarcasm dataset, with no other datasets": "the author intends for the content to be sarcastic, it"
        },
        {
          "Using the data you already have, data augmenta-": "though their result\nis not general for all sarcasm",
          "dent on the iSarcasm dataset, with no other datasets": "is called intended sarcasm. The iSarcasm dataset"
        },
        {
          "Using the data you already have, data augmenta-": "detection tasks and the result of data augmentation",
          "dent on the iSarcasm dataset, with no other datasets": "includes 4484 tweets: 3707 non-sarcastic and 777"
        },
        {
          "Using the data you already have, data augmenta-": "depends on the data and augmentation method.",
          "dent on the iSarcasm dataset, with no other datasets": "sarcastic. Because some tweets had been erased,"
        },
        {
          "Using the data you already have, data augmenta-": "(Lee et al., 2020)’s idea is to make a new data-",
          "dent on the iSarcasm dataset, with no other datasets": "we only had access to 3469 tweets for the job. The"
        },
        {
          "Using the data you already have, data augmenta-": "point out of the context sequence [c1, c2„ cn] and",
          "dent on the iSarcasm dataset, with no other datasets": "unbalanced dataset and the scarcity of sarcastic"
        },
        {
          "Using the data you already have, data augmenta-": "label\nit \"NOT SARCASM.\" The sequence could",
          "dent on the iSarcasm dataset, with no other datasets": "data were two of\nthe most signiﬁcant\nissues we"
        },
        {
          "Using the data you already have, data augmenta-": "not be identiﬁed as \"SARCASM\" without the an-",
          "dent on the iSarcasm dataset, with no other datasets": "encountered. Table 1 displays some of the dataset’s"
        },
        {
          "Using the data you already have, data augmenta-": "swer\n[r1].\nThey believe that\nthe newly created",
          "dent on the iSarcasm dataset, with no other datasets": "annotated remarks."
        },
        {
          "Using the data you already have, data augmenta-": "negative samples will aid the model in focusing on",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "3.2\nSarcasm Headlines Dataset"
        },
        {
          "Using the data you already have, data augmenta-": "the link between the response [r1] and its contexts",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "[c1, c2, cn]. They also create positive samples us-",
          "dent on the iSarcasm dataset, with no other datasets": "Sarcasm Headlines Dataset\n(Misra\nand Arora,"
        },
        {
          "Using the data you already have, data augmenta-": "ing back-translation procedures(Berard et al., 2019;",
          "dent on the iSarcasm dataset, with no other datasets": "2019; Misra and Grover, 2021) was gathered from"
        },
        {
          "Using the data you already have, data augmenta-": "Zheng et al., 2019) in French, Spanish, and Dutch",
          "dent on the iSarcasm dataset, with no other datasets": "two news websites.\nIt\nis beneﬁcial since it over-"
        },
        {
          "Using the data you already have, data augmenta-": "to balance out the quantity of negative examples.",
          "dent on the iSarcasm dataset, with no other datasets": "comes the constraints of Twitter datasets due to"
        },
        {
          "Using the data you already have, data augmenta-": "In (Feng et al., 2020) different data augmentation",
          "dent on the iSarcasm dataset, with no other datasets": "noise. As the second edition of\nthis dataset\nin-"
        },
        {
          "Using the data you already have, data augmenta-": "methods were tested on Yelp Reviews dataset(Yel,",
          "dent on the iSarcasm dataset, with no other datasets": "cludes more data and a greater variety of data than"
        },
        {
          "Using the data you already have, data augmenta-": "2014) for GPT-2 generative model(Radford et al.,",
          "dent on the iSarcasm dataset, with no other datasets": "the ﬁrst version, we chose the second version."
        },
        {
          "Using the data you already have, data augmenta-": "2019). They used \"Random Insertion, Deletion, &",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "3.3\nSentiment140 Dataset"
        },
        {
          "Using the data you already have, data augmenta-": "Swap\", \"Semantic Text Exchange (STE)\", \"Syn-",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "thetic Noise\", and \"Keyword Replacement\". They",
          "dent on the iSarcasm dataset, with no other datasets": "We needed to compensate for the limited data to"
        },
        {
          "Using the data you already have, data augmenta-": "showed in some case data augmentation could help",
          "dent on the iSarcasm dataset, with no other datasets": "train our model successfully. As a result, we chose"
        },
        {
          "Using the data you already have, data augmenta-": "them to reach better performance.",
          "dent on the iSarcasm dataset, with no other datasets": "the sentiment140 dataset (Go et al., 2009) because"
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "it has a large quantity of data and is based on Twit-"
        },
        {
          "Using the data you already have, data augmenta-": "This paper is the ﬁrst to look at generative-based",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "ter. The sentiment tweet message is labeled using"
        },
        {
          "Using the data you already have, data augmenta-": "and mutation-based data augmentation strategies",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "an automated classiﬁcation approach in this dataset."
        },
        {
          "Using the data you already have, data augmenta-": "in sarcasm detection.",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "The accuracy is more than 80% when using a ma-"
        },
        {
          "Using the data you already have, data augmenta-": "3\nDataset",
          "dent on the iSarcasm dataset, with no other datasets": "chine learning algorithm."
        },
        {
          "Using the data you already have, data augmenta-": "We mostly used the iSarcasm (Oprea and Magdy,",
          "dent on the iSarcasm dataset, with no other datasets": "4\nMethodology"
        },
        {
          "Using the data you already have, data augmenta-": "2019) dataset in this study. In speciﬁc experiments,",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "In this study we examined and analyzed various"
        },
        {
          "Using the data you already have, data augmenta-": "we integrated the primary dataset with various sec-",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "models and data augmentation strategies for sar-"
        },
        {
          "Using the data you already have, data augmenta-": "ondary datasets, including the Sarcasm Headlines",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "casm detection. First, we go through data augmen-"
        },
        {
          "Using the data you already have, data augmenta-": "Dataset (Misra and Arora, 2019) and Sentiment140",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "tation methods; then, we discuss the structure and"
        },
        {
          "Using the data you already have, data augmenta-": "dataset (Go et al., 2009) to increase the quantity of",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "hyperparameters of\nthese models in this section."
        },
        {
          "Using the data you already have, data augmenta-": "data and compensate for the lack of sarcastic data.",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "The codes of all models are available on GitHub1."
        },
        {
          "Using the data you already have, data augmenta-": "For each dataset, the details are further discussed.",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "It is worthy to mention that all of the supplemen-",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "4.1\nData Augmentation"
        },
        {
          "Using the data you already have, data augmenta-": "tary datasets we included had a negative impact",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "4.1.1\nGenerator-based"
        },
        {
          "Using the data you already have, data augmenta-": "on our model’s performance. We believe this was",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "For\nthis augmentation method, we used GPT-2"
        },
        {
          "Using the data you already have, data augmenta-": "the result of a different data gathering method. Be-",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "(Radford et al., 2019) generative model to generate"
        },
        {
          "Using the data you already have, data augmenta-": "cause to the differing labeling process and domain,",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "the distribution diverged from that of iSarcasm. As",
          "dent on the iSarcasm dataset, with no other datasets": ""
        },
        {
          "Using the data you already have, data augmenta-": "",
          "dent on the iSarcasm dataset, with no other datasets": "1https://github.com/AmirAbaskohi/SemEval2022-Task6-"
        },
        {
          "Using the data you already have, data augmenta-": "a result,\nthe following sections are solely depen-",
          "dent on the iSarcasm dataset, with no other datasets": "Sarcasm-Detection"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "True bliss is laying in an ice": "cold bath during the hottest part of the year"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "Figure 1: Effect of shufﬂing, word elimination, and re-"
        },
        {
          "True bliss is laying in an ice": "placing with synonyms on a tweet sample."
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "4000 tweets for both sarcastic and non-sarcastic"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "classes.\nThen we selected 2000 tweets of each"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "class randomly to increase dataset quantity and"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "have more sarcastic samples."
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "4.1.2\nMutation-based"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "We used three distinct ways to change the data in"
        },
        {
          "True bliss is laying in an ice": "this method: eliminating, replacing with synonyms,"
        },
        {
          "True bliss is laying in an ice": "and shufﬂing. These processes were used in the"
        },
        {
          "True bliss is laying in an ice": "following order: shufﬂing, deleting, and replacing."
        },
        {
          "True bliss is laying in an ice": "The removal and replacement were carried out sys-"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "tematically. We used the words’ roots to create a"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "synonym dictionary. Synonym dictionary is cre-"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "ated by scarping the Thesaurus website2. When a"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "term was chosen to be swapped with its synonyms,"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "we chose one of the synonyms randomly (Figure"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "1). We tried each combination of these processes"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "to ﬁnd the best data augmentation combination (a"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "total of seven)."
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "4.2\nModels"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "4.2.1\nSupport Vector Machine (SVM)"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "We\nutilized SVM to\ndiscover\nthe\noptimal\nap-"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "proaches for dataset preprocessing and word em-"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "beddings.\nFor data augmentation, we employed"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "both generator-based and mutation-based methods."
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "We also put other data preprocessing approaches to"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "the test, such as link removal, emoji removal, stop"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "word removal, stemming, and lemmatizing. We"
        },
        {
          "True bliss is laying in an ice": ""
        },
        {
          "True bliss is laying in an ice": "2https://www.thesaurus.com"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: F1-sarcastic and accuracy for different data",
      "data": [
        {
          "unidirectional ones.": "",
          "augmentation methods": "word embedding and no preprocessing.",
          "on SVM model with BERT": ""
        },
        {
          "unidirectional ones.": "Robustly Optimized BERT or RoBERTa has a",
          "augmentation methods": "",
          "on SVM model with BERT": ""
        },
        {
          "unidirectional ones.": "nearly identical architecture to BERT, however, the",
          "augmentation methods": "",
          "on SVM model with BERT": ""
        },
        {
          "unidirectional ones.": "researchers made some minor adjustments to its",
          "augmentation methods": "Data Augmentation",
          "on SVM model with BERT": "F1-sarcastic\nAccuracy"
        },
        {
          "unidirectional ones.": "architecture and training technique to enhance the",
          "augmentation methods": "Shufﬂing",
          "on SVM model with BERT": "0.305\n0.7471"
        },
        {
          "unidirectional ones.": "",
          "augmentation methods": "Shufﬂing + Replacing",
          "on SVM model with BERT": "0.301\n0.741"
        },
        {
          "unidirectional ones.": "results on BERT architecture (Liu et al., 2019).",
          "augmentation methods": "",
          "on SVM model with BERT": ""
        },
        {
          "unidirectional ones.": "",
          "augmentation methods": "Shufﬂing + Removing",
          "on SVM model with BERT": "0.306\n0.747"
        },
        {
          "unidirectional ones.": "We used both RoBERTa with twitter-roberta-",
          "augmentation methods": "",
          "on SVM model with BERT": ""
        },
        {
          "unidirectional ones.": "",
          "augmentation methods": "Removing",
          "on SVM model with BERT": "0.301\n0.747"
        },
        {
          "unidirectional ones.": "base, which has been trained on near 58 million",
          "augmentation methods": "GPT-2",
          "on SVM model with BERT": "0.292\n0.675"
        },
        {
          "unidirectional ones.": "tweets and ﬁnetuned for sentiment analysis with the",
          "augmentation methods": "",
          "on SVM model with BERT": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: F1-sarcastic and accuracy for different data",
      "data": [
        {
          "4.2.3\nBERT-based Methods": "The use of bidirectional training of transformer and"
        },
        {
          "4.2.3\nBERT-based Methods": "a prominent attention mode for language model-"
        },
        {
          "4.2.3\nBERT-based Methods": "ing is BERT’s fundamental\ntechnological break-"
        },
        {
          "4.2.3\nBERT-based Methods": "through (Devlin et al., 2018). The researchers de-"
        },
        {
          "4.2.3\nBERT-based Methods": "scribe a new Masked Language Model (MLM) ap-"
        },
        {
          "4.2.3\nBERT-based Methods": "proach that permits bidirectional\ntraining in pre-"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "viously tricky models.\nThey found that bidirec-"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "tionally trained language models can have a better"
        },
        {
          "4.2.3\nBERT-based Methods": "understanding of language context and ﬂow than"
        },
        {
          "4.2.3\nBERT-based Methods": "unidirectional ones."
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "Robustly Optimized BERT or RoBERTa has a"
        },
        {
          "4.2.3\nBERT-based Methods": "nearly identical architecture to BERT, however, the"
        },
        {
          "4.2.3\nBERT-based Methods": "researchers made some minor adjustments to its"
        },
        {
          "4.2.3\nBERT-based Methods": "architecture and training technique to enhance the"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "results on BERT architecture (Liu et al., 2019)."
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "We used both RoBERTa with twitter-roberta-"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "base, which has been trained on near 58 million"
        },
        {
          "4.2.3\nBERT-based Methods": "tweets and ﬁnetuned for sentiment analysis with the"
        },
        {
          "4.2.3\nBERT-based Methods": "TweetEval benchmark and BERT with bert-base"
        },
        {
          "4.2.3\nBERT-based Methods": "from Huggingface (Wolf et al., 2019). For both"
        },
        {
          "4.2.3\nBERT-based Methods": "models, we employed ﬁve epochs, batch size of 32,"
        },
        {
          "4.2.3\nBERT-based Methods": "500 warmup steps, and a weight decay of 0.01."
        },
        {
          "4.2.3\nBERT-based Methods": "4.2.4\nAttention-based Methods"
        },
        {
          "4.2.3\nBERT-based Methods": "One of the most\nimportant achievements in deep"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "learning research in the recent decade is the at-"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "tention mechanism (Vaswani et al., 2017).\nThe"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "Encoder-Decoder model’s restriction of encoding"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "the input sequence to one ﬁxed-length vector\nto"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "decode each output\ntime step is addressed via an"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "attention mechanism.\nThis difﬁculty is thought"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "to be more prevalent when decoding extended se-"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "quences."
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "We start with the assumption that if a model with"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "an attention layer is trained to identify sarcasm at"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "the sentence level, the sarcastic words will be the"
        },
        {
          "4.2.3\nBERT-based Methods": "ones the attention layer learns to value. As a result,"
        },
        {
          "4.2.3\nBERT-based Methods": "we added an attention layer to our LSTM-based and"
        },
        {
          "4.2.3\nBERT-based Methods": "BERT-based models. The results will be discussed"
        },
        {
          "4.2.3\nBERT-based Methods": "further."
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "4.2.5\nGoogle’s T5"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "Google’s T5 (Raffel et al., 2019) text-to-text model"
        },
        {
          "4.2.3\nBERT-based Methods": "outperformed the human baseline on the GLUE,"
        },
        {
          "4.2.3\nBERT-based Methods": "SQuAD, and CNN/Daily Mail datasets and earned"
        },
        {
          "4.2.3\nBERT-based Methods": "a remarkable 88.9 on the SuperGLUE language"
        },
        {
          "4.2.3\nBERT-based Methods": "benchmark."
        },
        {
          "4.2.3\nBERT-based Methods": "We ﬁne-tuned T5 for our problem and dataset by"
        },
        {
          "4.2.3\nBERT-based Methods": "giving the sarcastic label the target and the tweets"
        },
        {
          "4.2.3\nBERT-based Methods": ""
        },
        {
          "4.2.3\nBERT-based Methods": "as the source. We used two epochs, batch size of 4,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Best results for each model using iSarcasm increasingthetokenizationmaxlength,increasing",
      "data": [
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "dataset and mutation-based data augmentation."
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "Model\nF1-sarcastic\nAccuracy"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "SVM\n0.3064\n0.7478"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "LSTM-based\n0.2751\n0.7251"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "BERT-based\n0.414\n0.8634"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "Attention-based\n0.2959\n0.7793"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "Google’s T5\n0.4038\n0.8124"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "GPT-2 data augmentation on RoBERTa because"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "the results were close, and we found that merely"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "word removal was the best data augmentation. The"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "following results are based on no data preprocess-"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "ing, BERT word embedding, and mutation-based"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "data augmentation utilizing only word removal."
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "5.2\nLSTM-based Methods"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "LSTM obtained\nan F1-sarcastic\nof\n0.2176\nus-"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "ing BERT word embeddings, mutation-based data"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "augmentation,\nand\nno\npreprocessing, whereas"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "BLSTM’s F1-sarcastic was 0.2439 using BERT"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "word embeddings, mutation-based data augmenta-"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "tion, and no preprocessing. By adding CNN layers,"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "the F1-sarcastic of\nthe LSTM was\nincreased to"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "0.2453, and the BLSTM was increased to 0.2751."
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "The CNN model’s F1-sarcastic was 0.2263."
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "5.3\nBERT-based Methods"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "We employed a mutation-based data augmentation"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "approach with no preprocessing for BERT-based"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "procedures. We got an F1-sarcastic of 0.323 using"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "BERT. We achieved our best result with RoBERTa"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "with an F1-sarcastic of 0.414, which was better"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "than LOANT (Guo et al., 2021) model on the same"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "dataset."
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "5.4\nAttention-based Methods"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "Adding attention layers to this job was not help-"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "ful,\nand it decreased our models’ performance."
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "RoBERTa’s F1-sarcastic dropped to 0.2959 using"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "the attention layer. LSTM model with the atten-"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "tion layer earned an F1-sarcastic of 0.2145. The"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "F1-sarcastic of BLSTM with attention layer was"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "0.2336."
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "5.5\nGoogle’s T5"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "Based on the hyperparameters listed in the Section"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "4, our F1-sarcastic for this model is 0.4038. How-"
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": ""
        },
        {
          "Table 3: Best\nresults for each model using iSarcasm": "ever, we believe that we may get better results by"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Nina Wacholder. 2011.\nIdentifying sarcasm in twit-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "and Waqar Ali. 2019.\nA hybrid cnn-lstm model"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "the 49th An-\nter:\na closer\nlook.\nIn Proceedings of",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "for\nimproving\naccuracy\nof movie\nreviews\nsenti-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "nual Meeting of\nthe Association for Computational",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "ment analysis. Multimedia Tools and Applications,"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Linguistics: Human Language Technologies, pages",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "78(18):26597–26613."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "581–586.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Antonio Reyes,\nPaolo Rosso,\nand Davide Buscaldi."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Xu Guo, Boyang Li, Han Yu, and Chunyan Miao. 2021.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "2012.\nFrom humor\nrecognition to irony detection:"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Latent-optimized adversarial neural\ntransfer for sar-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Data &\nThe ﬁgurative language of\nsocial media."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "casm detection. arXiv preprint arXiv:2104.09261.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Knowledge Engineering, 74:1–12."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Sepp\nHochreiter\nand\nJürgen\nSchmidhuber.\n1997.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Antonio Reyes, Paolo Rosso,\nand Tony Veale. 2013."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Long\nshort-term memory.\ncomputation,",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "A multidimensional\napproach\nfor\ndetecting\nirony"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "9(8):1735–1780.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Language\nresources\nand\nin\ntwitter.\nevaluation,"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Suzana Ili´c, Edison Marrese-Taylor,\nJorge A Balazs,",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "47(1):239–268."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "and Yutaka Matsuo.\n2018.\nDeep\ncontextualized",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "word representations for detecting sarcasm and irony.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Ellen Riloff, Ashequl Qadir,\nPrafulla Surve, Lalin-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "arXiv preprint arXiv:1809.09795.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "dra De Silva, Nathan Gilbert, and Ruihong Huang."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "2013. Sarcasm as contrast between a positive senti-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Sandeepa Kannangara. 2018. Mining twitter\nfor ﬁne-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "the\nment and negative situation.\nIn Proceedings of"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "grained political opinion polarity classiﬁcation,\nide-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "2013 conference on empirical methods\nin natural"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "ology detection and sarcasm detection.\nIn Proceed-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "language processing, pages 704–714."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "ings of\nthe Eleventh ACM International Conference",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "on Web Search and Data Mining, pages 751–752.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui,\nand Jian"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Hankyol Lee, Youngjae Yu,\nand Gunhee Kim. 2020.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Su. 2018.\nReasoning with sarcasm by reading in-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Augmenting\ndata\nfor\nsarcasm detection with\nun-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "between. arXiv preprint arXiv:1805.02856."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "arXiv\npreprint\nlabeled\nconversation\ncontext.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "arXiv:2006.06259.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Piyoros Tungthamthiti, Kiyoaki Shirai, and Masnizah"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Mohd. 2016. Recognition of sarcasm in microblog-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "ging\nbased\non\nsentiment\nanalysis\nand\ncoherence"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis,",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Journal of Natural Language Pro-\nidentiﬁcation."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Luke\nZettlemoyer,\nand Veselin\nStoyanov.\n2019.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "cessing, 23(5):383–405."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Roberta: A robustly optimized bert pretraining ap-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "proach. arXiv preprint arXiv:1907.11692.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Cynthia Van Hee, Els Lefever, and Véronique Hoste."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "2018.\nSemeval-2018 task 3:\nIrony detection in en-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Stephanie Lukin and Marilyn Walker. 2017.\nReally?",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "glish tweets.\nIn Proceedings of The 12th Interna-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "well. apparently bootstrapping improves the perfor-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "tional Workshop on Semantic Evaluation, pages 39–"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "mance of sarcasm and nastiness classiﬁers for online",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "50."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "dialogue. arXiv preprint arXiv:1708.08572.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Tomas Mikolov, Kai Chen, Greg Corrado,\nand Jef-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "frey Dean.\n2013.\nEfﬁcient\nestimation\nof word",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "arXiv\npreprint\nrepresentations\nin\nvector\nspace.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "arXiv:1301.3781.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "information process-\nyou need. Advances in neural"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "ing systems, 30."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Rishabh Misra and Prahal Arora. 2019.\nSarcasm de-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "tection using hybrid neural network. arXiv preprint",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Tony Veale, F Amílcar Cardoso, and Rafael Pérez y"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "arXiv:1908.07414.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Pérez. 2019.\nSystematizing creativity: A computa-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Rishabh Misra and Jigyasa Grover. 2021.\nSculpting",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "tional view.\nIn Computational Creativity, pages 1–"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "data for ml: The ﬁrst act of machine learning.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "19. Springer."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Silviu Oprea\nand Walid Magdy.\n2019.\nisarcasm:",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Marilyn Walker, Jean E Fox Tree, Pranav Anand, Rob"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "arXiv\npreprint\nA dataset\nof\nintended\nsarcasm.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Abbott, and Joseph King. 2012.\nA corpus\nfor\nre-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "arXiv:1911.03123.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "search on deliberation and debate.\nIn Proceedings"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "of the Eighth International Conference on Language"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Resources and Evaluation (LREC’12), pages 812–"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Dario Amodei,\nIlya Sutskever,\net\nal. 2019.\nLan-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "817."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "guage models are unsupervised multitask learners.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "OpenAI blog, 1(8):9.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": ""
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Thomas Wolf, Lysandre Debut, Victor Sanh,\nJulien"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "Chaumond, Clement Delangue, Anthony Moi, Pier-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "ric Cistac, Tim Rault, Rémi Louf, Morgan Fun-"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "Wei Li, and Peter J Liu. 2019. Exploring the limits",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "towicz,\net\nal. 2019.\nHuggingface’s\ntransformers:"
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "of transfer learning with a uniﬁed text-to-text\ntrans-",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "arXiv\nState-of-the-art natural\nlanguage processing."
        },
        {
          "Roberto González-Ibánez,\nSmaranda Muresan,\nand": "former. arXiv preprint arXiv:1910.10683.",
          "Anwar Ur Rehman, Ahmad Kamran Malik, Basit Raza,": "preprint arXiv:1910.03771."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Renjie Zheng, Hairong Liu, Mingbo Ma, Baigong": "Zheng, and Liang Huang. 2019."
        },
        {
          "Renjie Zheng, Hairong Liu, Mingbo Ma, Baigong": "translation with\ndomain\nsensitive"
        },
        {
          "Renjie Zheng, Hairong Liu, Mingbo Ma, Baigong": ""
        },
        {
          "Renjie Zheng, Hairong Liu, Mingbo Ma, Baigong": "report. arXiv preprint arXiv:1906.08393."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Yelp. yelp open dataset",
      "venue": "Yelp. yelp open dataset"
    },
    {
      "citation_id": "2",
      "title": "SemEval-2022 Task 6: iSarcas-mEval, Intended Sarcasm Detection in English and Arabic",
      "authors": [
        "Ibrahim Abu Farha",
        "Silviu Oprea",
        "Steven Wilson",
        "Walid Magdy"
      ],
      "year": "2022",
      "venue": "Proceedings of the 16th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "3",
      "title": "Naver labs europe's systems for the wmt19 machine translation robustness task",
      "authors": [
        "Alexandre Berard",
        "Ioan Calapodescu",
        "Claude Roux"
      ],
      "year": "2019",
      "venue": "Naver labs europe's systems for the wmt19 machine translation robustness task",
      "arxiv": "arXiv:1907.06488"
    },
    {
      "citation_id": "4",
      "title": "Semi-supervised recognition of sarcasm in twitter and amazon",
      "authors": [
        "Dmitry Davidov",
        "Oren Tsur",
        "Ari Rappoport"
      ],
      "year": "2010",
      "venue": "Proceedings of the fourteenth conference on computational natural language learning"
    },
    {
      "citation_id": "5",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "6",
      "title": "Genaug: Data augmentation for finetuning text generators",
      "authors": [
        "Varun Steven Y Feng",
        "Dongyeop Gangal",
        "Teruko Kang",
        "Eduard Mitamura",
        "Hovy"
      ],
      "year": "2020",
      "venue": "Genaug: Data augmentation for finetuning text generators",
      "arxiv": "arXiv:2010.01794"
    },
    {
      "citation_id": "7",
      "title": "Twitter sentiment classification using distant supervision",
      "authors": [
        "Alec Go",
        "Richa Bhayani",
        "Lei Huang"
      ],
      "year": "2009",
      "venue": "CS224N project report"
    },
    {
      "citation_id": "8",
      "title": "Identifying sarcasm in twitter: a closer look",
      "authors": [
        "Roberto González-Ibánez",
        "Smaranda Muresan",
        "Nina Wacholder"
      ],
      "year": "2011",
      "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "9",
      "title": "Latent-optimized adversarial neural transfer for sarcasm detection",
      "authors": [
        "Boyang Xu Guo",
        "Han Li",
        "Chunyan Yu",
        "Miao"
      ],
      "year": "2021",
      "venue": "Latent-optimized adversarial neural transfer for sarcasm detection",
      "arxiv": "arXiv:2104.09261"
    },
    {
      "citation_id": "10",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "11",
      "title": "Deep contextualized word representations for detecting sarcasm and irony",
      "authors": [
        "Suzana Ilić",
        "Edison Marrese-Taylor",
        "Jorge Balazs",
        "Yutaka Matsuo"
      ],
      "year": "2018",
      "venue": "Deep contextualized word representations for detecting sarcasm and irony",
      "arxiv": "arXiv:1809.09795"
    },
    {
      "citation_id": "12",
      "title": "Mining twitter for finegrained political opinion polarity classification, ideology detection and sarcasm detection",
      "authors": [
        "Sandeepa Kannangara"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining"
    },
    {
      "citation_id": "13",
      "title": "Augmenting data for sarcasm detection with unlabeled conversation context",
      "authors": [
        "Hankyol Lee",
        "Youngjae Yu",
        "Gunhee Kim"
      ],
      "year": "2020",
      "venue": "Augmenting data for sarcasm detection with unlabeled conversation context",
      "arxiv": "arXiv:2006.06259"
    },
    {
      "citation_id": "14",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "15",
      "title": "Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue",
      "authors": [
        "Stephanie Lukin",
        "Marilyn Walker"
      ],
      "year": "2017",
      "venue": "Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue",
      "arxiv": "arXiv:1708.08572"
    },
    {
      "citation_id": "16",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "Tomas Mikolov",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "17",
      "title": "Sarcasm detection using hybrid neural network",
      "authors": [
        "Rishabh Misra",
        "Prahal Arora"
      ],
      "year": "2019",
      "venue": "Sarcasm detection using hybrid neural network",
      "arxiv": "arXiv:1908.07414"
    },
    {
      "citation_id": "18",
      "title": "Sculpting data for ml: The first act of machine learning",
      "authors": [
        "Rishabh Misra",
        "Jigyasa Grover"
      ],
      "year": "2021",
      "venue": "Sculpting data for ml: The first act of machine learning"
    },
    {
      "citation_id": "19",
      "title": "isarcasm: A dataset of intended sarcasm",
      "authors": [
        "Silviu Oprea",
        "Walid Magdy"
      ],
      "year": "2019",
      "venue": "isarcasm: A dataset of intended sarcasm",
      "arxiv": "arXiv:1911.03123"
    },
    {
      "citation_id": "20",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "21",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2019",
      "venue": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "arxiv": "arXiv:1910.10683"
    },
    {
      "citation_id": "22",
      "title": "A hybrid cnn-lstm model for improving accuracy of movie reviews sentiment analysis",
      "authors": [
        "Ahmad Anwar Ur Rehman",
        "Basit Malik",
        "Waqar Raza",
        "Ali"
      ],
      "year": "2019",
      "venue": "A hybrid cnn-lstm model for improving accuracy of movie reviews sentiment analysis"
    },
    {
      "citation_id": "23",
      "title": "From humor recognition to irony detection: The figurative language of social media",
      "authors": [
        "Antonio Reyes",
        "Paolo Rosso",
        "Davide Buscaldi"
      ],
      "year": "2012",
      "venue": "Data & Knowledge Engineering"
    },
    {
      "citation_id": "24",
      "title": "A multidimensional approach for detecting irony in twitter. Language resources and evaluation",
      "authors": [
        "Antonio Reyes",
        "Paolo Rosso",
        "Tony Veale"
      ],
      "year": "2013",
      "venue": "A multidimensional approach for detecting irony in twitter. Language resources and evaluation"
    },
    {
      "citation_id": "25",
      "title": "Sarcasm as contrast between a positive sentiment and negative situation",
      "authors": [
        "Ellen Riloff",
        "Ashequl Qadir",
        "Prafulla Surve",
        "Lalindra Silva",
        "Nathan Gilbert",
        "Ruihong Huang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "26",
      "title": "Reasoning with sarcasm by reading inbetween",
      "authors": [
        "Yi Tay",
        "Anh Luu",
        "Siu Tuan",
        "Jian Hui",
        "Su"
      ],
      "year": "2018",
      "venue": "Reasoning with sarcasm by reading inbetween",
      "arxiv": "arXiv:1805.02856"
    },
    {
      "citation_id": "27",
      "title": "Recognition of sarcasm in microblogging based on sentiment analysis and coherence identification",
      "authors": [
        "Piyoros Tungthamthiti",
        "Kiyoaki Shirai",
        "Masnizah Mohd"
      ],
      "year": "2016",
      "venue": "Journal of Natural Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Semeval-2018 task 3: Irony detection in english tweets",
      "authors": [
        "Cynthia Van Hee",
        "Els Lefever",
        "Véronique Hoste"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Systematizing creativity: A computational view",
      "authors": [
        "Tony Veale",
        "F Amílcar Cardoso",
        "Rafael Pérez Y Pérez"
      ],
      "year": "2019",
      "venue": "Computational Creativity"
    },
    {
      "citation_id": "31",
      "title": "A corpus for research on deliberation and debate",
      "authors": [
        "Marilyn Walker",
        "Jean Fox Tree",
        "Pranav Anand",
        "Rob Abbott",
        "Joseph King"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "32",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771"
    },
    {
      "citation_id": "33",
      "title": "Robust machine translation with domain sensitive pseudo-sources: Baidu-osu wmt19 mt robustness shared task system report",
      "authors": [
        "Renjie Zheng",
        "Hairong Liu",
        "Mingbo Ma",
        "Baigong Zheng",
        "Liang Huang"
      ],
      "year": "2019",
      "venue": "Robust machine translation with domain sensitive pseudo-sources: Baidu-osu wmt19 mt robustness shared task system report",
      "arxiv": "arXiv:1906.08393"
    }
  ]
}