{
  "paper_id": "2312.00799v1",
  "title": "Hveegnet: Exploiting Hierarchical Vaes On Eeg Data For Neuroscience Applications",
  "published": "2023-11-20T15:36:31Z",
  "authors": [
    "Giulia Cisotto",
    "Alberto Zancanaro",
    "Italo F. Zoppis",
    "Sara L. Manzoni"
  ],
  "keywords": [
    "EEG",
    "VAE",
    "variational autoencoder",
    "latent representation",
    "motor imagery"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the recent success of artificial intelligence in neuroscience, a number of deep learning (DL) models were proposed for classification, anomaly detection, and pattern recognition tasks in electroencephalography (EEG). EEG is a multi-channel time-series that provides information about the individual brain activity for diagnostics, neuro-rehabilitation, and other applications (including emotions recognition). Two main issues challenge the existing DL-based modeling methods for EEG: the high variability between subjects and the low signalto-noise ratio making it difficult to ensure a good quality in the EEG data. In this paper, we propose two variational autoencoder models, namely vEEGNet-ver3 and hvEEGNet, to target the problem of high-fidelity EEG reconstruction. We properly designed their architectures using the blocks of the well-known EEGNet as the encoder, and proposed a loss function based on dynamic time warping. We tested the models on the public Dataset 2a -BCI Competition IV, where EEG was collected from 9 subjects and 22 channels. hvEEGNet was found to reconstruct the EEG data with very high-fidelity, outperforming most previous solutions (including our vEEGNet-ver3 ). Furthermore, this was consistent across all subjects. Interestingly, hvEEGNet made it possible to dis- * Corresponding author.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The first quantitative analysis of an electroencephalography (EEG) signal dates back to the pioneering work of Hans Berger that, in the late Twenties  (1929) , took a Fourier transform of an EEG signal to quantify the spectral distribution of the brain activity under different physiological and stimulation conditions  [Berger (1929) ]. Since then, a vast literature flourished and obtained very successful achievements in the modeling and classification of EEG data for different clinical and research applications  [Teplan et al. (2002) ]. From the very first quantitative Berger's analyses, a number of different methods were proposed, with machine learning (ML)-based models reaching the highest popularity for, e.g., pattern recognition, classification, and compression tasks  [Hosseini et al. (2020) ]. Among many others, the classification of motor imagery (MI), i.e., the brain activity corresponding to the imagination of moving one specific body segment, has been largely used in basic neuroscience to understand brain mechanisms  [Kaiser et al. (2012) ], as well as to drive brain-computer interface (BCI) systems  [Kodama et al. (2023) ] and robots  [Beraldo et al. (2022) ] to support neuro-rehabilitation.\n\nDespite the large body of literature already produced, EEG modelling still suffers from three major issues: (1) this particular time-series has a very fast dynamics (in the range of milliseconds) making it prone to interferences from many possible sources of noise, (2) it displays a high inter-subject as well as an inherent within-subject variability, and (3) when used in more ecological environments, poor reliability is often a problem. Standard ML models proved to be relatively good in several tasks  [Hosseini et al. (2020) ], but they still lack the flexibility to generalize over different subjects or sessions, due to the rigid feature extraction step which is typically performed based on a-priori knowledge of the domain experts, or some simple (first or second-order) statistical description of the data. Also, there is still no gold-standard pre-processing to be applied.\n\nFinally, when models are embedded on portable and lower-quality EEG devices for usage in more ecological settings, e.g., in new Internet of things scenarios  [Munari et al. (2023) ] for continuous monitoring, their performance rapidly degrade  [Anders & Arnrich (2022) ]. Nonetheless, the state-of-the-art (SOTA) solutions for several processing tasks in EEG are still based on standard ML.\n\nMore recently, deep learning (DL)-based models have been increasingly employed and could often outperform the SOTA methods. As an example, in the case of MI, filter-bank common spatial pattern (FBCSP) has been employed as a reference method for years  [Ang et al. (2008) ]. However, the so-called EEG-Net DL-based architecture, proposed in 2016  [Lawhern et al. (2016) ] and its later variants were able to achieve higher performance, with less pre-processing effort and no a-priori knowledge needed  [Zancanaro et al. (2021) ]. Nevertheless, the investigation on the potentialities of new architectures is still open  [Lotte et al. (2018) ]. A critical issue in these methods is the dependency on the training set, as  Gyori et al. (2022)  recently pointed out in the domain of magnetic resonance imaging data: a training dataset of poor quality, as well as a training set distributed in a non-representative way might induce biases in the final model and, consequently, to poor results in the task the model is expected to perform e.g., classification or anomaly detection. At the same time, in the neuroscience domain it is fairly difficult to certainly exclude the above-mentioned conditions  [Pion-Tonachini et al. (2019) ].\n\nThen, to enhance models' capability in classifying, recognising anomalies as well as automatically denoising large EEG datasets, training a DL model to optimally reconstruct EEG data and to provide an effective latent representation has been recently recognized by the literature as an effective pre-processing step.\n\nIn this paper, we propose vEEGNet version 3 (vEEGNet-ver3) and hierarchical vEEGNet (hvEEGNet), two DL-based models consisting of a variational autoencoder (VAE) architecture that aims at reconstructing the EEG data with high-fidelity. In this process, a latent representation is obtained, which could then be used, e.g., to train a classifier or to generate new data samples. More specifically, vEEGNet-ver3 is an improvement over our previous architectures  [Zancanaro et al. (2023, under review) ] with a significant modification of the loss function, while hvEEGNet represents our best model consisting in a hierarchical version of the VAE which allowed us to achieve almost perfect reconstruction of the EEG data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "State Of The Art",
      "text": "To contextualize our study, we report here that related work that addressed both the reconstruction of EEG signals via DL, with the extraction of a latent representation for this kind of data, and those which proposed autoencoder models to detect anomalies in EEG data.\n\nOne of the most interesting works on the topic is  Bethge et al. (2022) , where the authors proposed EEG2VEC, i.e., a VAE-based architecture developed to encode emotions-related EEG signals in the latent space of the VAE. The authors succeeded in reconstructing low-frequency components of the original EEG signals by using the VAE latent representations. Unfortunately, the higher frequency components could not be reconstructed. Moreover, the reconstructed signals appeared to be largely attenuated (with amplitudes often in the order of half the original one). According to the authors' explanation, this can be due to the particular design of the decoder which might have introduced aliasing and artefacts. These results are in line with those we found in our previous work  [Zancanaro et al. (2023) ]: we proposed a new DL-based architecture named vEEGNet-ver1, where we used EEGNet (a popular architecture that has tailored a convolutional neural networks (CNN) to specifically process EEG data  [Lawhern et al. (2016) ]), as an encoder and its mirrored architecture as decoder in a VAE model. We evaluated its classification and reconstruction performance on a public dataset (containing MI-related EEG data) and found that only low-frequency components could be recovered, while achieving state-of-the-art performance in classification. In contrast with  Bethge et al. (2022) , we were able to explain this sub-optimal behaviour as the clear effect of the filters applied at the first block of the architecture: they simply have the effect of smoothing the signal, so that the information related to higher frequency components is not further propagated along the DL network, thus making them not available anymore for the reconstruction. Nevertheless, we recognized the reconstructed low-frequency component as the motor related cortical potential (MRCP), a well-known neurophysiological behaviour associated with movements initiation  [Bressan et al. (2021) ;  Ofner et al. (2019) ]. In  Al-Marridi et al. (2018) , the authors implemented a convolutional autoencoder to compress and reconstruct MI-EEG signals (from two public datasets, including the one used in our own work). They evaluated the trade-off between the compression ratio, computed as the ratio between the size of the raw signal and the size of the autoencoder latent representation, and the reconstruction quality, measured in terms of percent root mean square distortion (PRD). The authors proved the good ability of the convolutional autoencoder to reconstruct single channel EEG signals with a relatively high compression ratio, e.g., compression ratio up to 98% with PRD of 1.33%. However, the authors did not discuss about the quality of the dataset and reported representative performance, only. Thus, no further insights on the relationship between model training, reconstruction performance and input data quality can be retrieved.  Dasan & Gnanaraj (2022)  proposed a multi-branch denoising autoencoder to jointly compress EEG-ECG-EMG signals, assuming them acquired in a mobile-health scenario with the aim to ensure continual learning, i.e., continuous fine tuning using incoming data during real-time health monitoring. Each signal modality (EEG, ECG, EMG) was independently pre-processed, and then a joint latent representation was obtained to compress the signals. The authors showed the trade-off between compression ratio and reconstruction quality using three (independent) public datasets. They provided an example of reconstructed EEG, EMG, and ECG signal, where the reconstruction appeared to be very reliable. However, the targeted EEG signal was acquired from one only sensor using a portable device, i.e., the signal was of low quality and poorly variable, thus most probably making the reconstruction easier. Finally, they used different metrics to quantify the quality of the reconstructed signal, e.g., the reconstruction quality index.\n\nHowever, it was defined w.r.t. the compression ratio, thus not applicable to other reconstruction-targeting scenarios.  Khan et al. (2023)  used a shallow autoencoder to obtain an encoded representation with low dimensionality (8 to 64) of a single-channel EEG data to be used in the classification of epileptic versus healthy EEG data (using a k-nearest neighbors (kNN) and a support vector machine (SVM) classifier and a public dataset  [Tran et al. (2022) ]). They achieved very high values for the accuracy (over 97%), with very high sensitivity (mostly over 96%) as well as specificity (over 96%). Also, the reconstruction quality was showed in two representative EEG signals, with very high fidelity. Unfortunately, the authors did not report the power spectrum of the original EEG signals, thus making it difficult to fully ensure a reproducibility of these good performance on other, more complex (i.e., with larger bandwidth), EEG data. Also, the proposed architecture was proved to be very efficient in a channel-wise reconstruction: nevertheless, in many applications, multiple channels should be processed altogether. Then, further investigations should be needed to explore a more compact solution to obtain an encoded representation from all available EEG channels.\n\nMoreover, autoencoders offer the inherent possibility to be used as anomaly detectors  [Pang et al. (2021) ]: in fact, they are trained, in an unsupervised way, to learn the distribution of the normal data that corresponds to the optimal reconstruction performance. After training, the model is used to reconstruct any new sample: when the latter is sufficiently out of the expected distribution, the model shows a large reconstruction error, signalling an anomaly. Autoencodersbased anomaly detection has been proved effective also for EEG data, when the latter are affected by different kinds of pathologies. In  Emami et al. (2019a) , an autoencoder was used to detect epileptic seizures on a private dataset with 24 subjects  [Emami et al. (2019b) ]. To identify anomalies they set a threshold on the reconstruction error and the EEG samples exceeding it were labelled as anomalous. A 100% accuracy in seizure detection could be obtained in 22 subjects out of 24. In  Ortiz et al. (2020) , the authors used an autoencoder-based architecture to detect dyslexia on a public dataset  [De Vos et al. (2017) ]. They first extracted a number of EEG features (in time and frequency domain), and then trained an autoencoder to reconstruct the time-series of such features. The difference, i.e., the residual, between the input and the reconstructed time-series was used as to feed an SVM classifier, aimed at distinguishing between healthy and dyslexic individuals. This solution achieved an accuracy of 96%, sensitivity of 86%, specificity of 100%, area under the curve (AUC) of 92%. In both cases, the model training relied only on data from healthy and clean EEG data. After training, the EEG samples corresponding to the largest reconstruction error values were labelled as anomalous.\n\nHowever, in the case of EEG data, the definition of normality can be very challenging: it is fairly difficult to have a certified clean dataset, even though the subjects are healthy. In fact, an EEG sample could be considered as anomalous both for the presence of a pathology, but also because of any noise and interference that might occur during recordings (e.g., the so-called artefacts)  [Gabardi et al. (2023) ]. Therefore, it would be more realistic to train an autoencoder model on a mixture of clean and noisy data, in line with some other literature (not necessarily addressing biological data). For instance, in Zhou & Paffenroth (2017), the authors proposed a robust autoencoder, i.e., a combination of a robust PCA (RPCA) and an autoencoder, where the autoencoder was used for data projection in the (reduced) principal components space (in place of the usual linear projection). Unfortunately, there is a limited literature on this kind of autoencoders, as confirmed by a recent survey  [Al-amri et al. (2021) ]. In  Xing et al. (2020) , the authors proposed a combination of an evolving spiking neural network and a Boltzmann machine to identify anomalies in a multimedia data stream. The proposed training algorithm was able to localize and ignore any random noise that could corrupt the training data.  Wambura et al. (2020)  suggested to jointly use a CNN and a long-short term memory (LSTM) to forecast future trends and reconstruct past trends of different types of data stream. They were able to accurately predict time-series related to three real-world scenarios, i.e., web traffic in Wikipedia, price trends of the avocado fruit, and temperature series in a city. In  Dong & Japkowicz (2018) , a model composed by an ensemble of autoencoders was employed to identify anomalies in data streams. The authors claimed that the training algorithm made the presence of noisy samples in the training data not statistically significant, thus ensuring model's robustness to noise. In  Qiu et al. (2019) , an architecture made by the sequence of a CNN, an LSTM, a feed-forward neural network (FFNN), and a softmax layer was proposed to identify anomalies. Interestingly, a VAE was preliminarily used to over-sample the dataset, before training the classifier (i.e., the FFNN with the softmax layer). The model was tested on the AIOps-KPI public dataset  [Li et al. (2022) ], achieving an accuracy of 77% (KP1), 75% (KP2), 83% (KP3), and 75% (KP4).\n\nNevertheless, to the best of our knowledge, this kind of approaches has never been applied to EEG data, yet.\n\nAnother body of literature presents DL architectures that are not specifically targeted to reconstruct EEG data, but to extract, via autoencoders, the most relevant features to improve classification. In  Qiu et al. (2018) , the authors proposed a denoising sparse autoencoder, i.e., an autoencoder that imposes a sparsity condition on the latent space, as a feature learner and then used the learnt representation as the input of a linear regression model to detect different types of epileptic seizures. They successfully tested their solution on a public dataset  [Andrzejak et al. (2002) ] to classify either two EEG classes, i.e., healthy vs epileptic, or three classes, i.e., healthy, ictal and inter-ictal EEG data (in both cases, the reported accuracy was 100%). In  Wang et al. (2020) , an EEGNetbased autoencoder was employed as a feature extractor from a high-density EEG device aimed at acquiring evoked potentials  [Lascano et al. (2017) ] during a repeated pain stimulation with a laser at different energy levels. Here, the authors opted for an intense pre-processing which included a filtering step in the frequency range 1 to 30 Hz, segmentation in 1.5 s-epochs, each one taken from 0.5 s before the stimuls until 1 s after it, baseline correction (using the pre-stimulus period as the baseline), independent component analysis (ICA) decomposition to remove eye-movement related artifacts, and down-sampling from 1000 to 250 Hz. The features extracted from the autoencoder fed four different ML classifiers, i.e., a kNN, a SVM, a linear discriminant analysis (LDA), and a logistic regression model, with the aim of detecting 10 levels of pain.\n\nInterestingly, several latent space sizes were tested, and the size of 64, used with a logistic regression classifier, resulted as the best solution. An accuracy of 74.6 ± 11.2% was obtained (with a chance level of 10%), thus largely outperforming the alternative solution using principal component analysis (PCA) as feature extractor (accuracy equal to 59.9±19%). In  Liu et al. (2020) , a CNN was used as feature extractor from EEG, i.e., the output of the network was used to feed an autoencoder with the goal of compressing and reconstructing the original EEG data. The CNN and the autoencoder were trained together, with the autoencoder forcing the CNN to extract the most significant features. After training, the obtained features were fed to a FFNN that acted as a classifier and it was separately trained. They tested the model on the DEAP  [Koelstra et al. (2012) ] and the SEED  [Zheng & Lu (2015) ] public emotions-related EEG datasets. In the DEAP dataset, they achieved an accuracy of 89.49% in the valence category and an accuracy of 92.86% in the arousal one (each category has 2 labels, high and low). For the SEED dataset, they achieved an accuracy 96.77%, on the 3 classes of the dataset (positive, neutral, negative).\n\nThus, some fundamental challenges still emerge from the SOTA review to be solved including (1) high reconstruction error or generation of traces that are not faithful to the original signal, (2) lack of focus on the reconstruction even if the architecture have the capacity to favor classification taks, and (3) no dedicated investigations on the impact of the input EEG data quality on the training of DL models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Materials And Methods",
      "text": "In this section, we present the basic modules as well as the overall architecture of our proposed models, i.e., vEEGNet-ver3 and hvEEGNet. Furthermore, we describe the metrics and the methodologies we employed to evaluate our models.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Variational Autoencoder",
      "text": "The common overall architecture of our both models is the VAE.\n\nUnlike traditional autoencoders, i.e., producing a deterministic encoding for each input, VAE is able to learn a probabilistic mapping between the input data and a latent space, which is additionally learned as a structured latent representation  [Kingma & Welling (2013 , 2019) ]. Given the observed data x and assuming z to be the latent variables, with a proper training, a VAE learns the variational distribution q ϕ (z|x) as well as the generative distribution p θ (x|z), using a pair of (deep) neural networks (acting as the encoder and the decoder), parametrized by ϕ and θ, respectively  [Blei et al. (2017) ]. The training loss function, denoted as L V AE , accounts for the sum of two different contributions:\n\nthe Kullback-Leibler divergence between the variational distribution q ϕ (z|x) and the posterior distribution p θ (x|z), denoted as L KL , and the reconstruction error, denoted as L R , which forces the decoded samples to approximate the initial inputs. Thus, the loss function adopted for the VAE is\n\nwhere µ i and σ 2 i are the predicted mean and variance values of the corresponding i-th latent component of z. Assuming normal distribution as a prior for the sample distribution in the latent space, it is possible to rewrite eq. 1 as follows\n\nwhere µ i and σ 2 i are the predicted mean and variance values of the corresponding i-th latent component of z.\n\nIn this work, we adopted this basic architecture to propose vEEGNet-ver3.\n\nThe details characterizing our specific VAE are reported in Section 3.1.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Hierarchical Vae",
      "text": "A hierarchical VAE  [Vahdat & Kautz (2021) ] is the evolution of a standard VAE enriched by a hierarchical latent space, i.e., multiple layers implementing a latent space each. In fact, standard VAEs suffer from the lack of accuracy in details reconstruction, given by the trade-off between the reconstruction loss and the Kullback-Leibler divergence contributions, thus generating the tendency to generate slightly approximated data (e.g., blurred images), only. Hierarchical VAEs attempt to solve this problem by using multiple latent spaces, where each of them is trained to encode different levels of detail in the input data. Assuming a model with L latent spaces, its loss function can be written as\n\nwhere\n\nwith q ϕ (z l |, z <l ) = l-1 i=1 q ϕ (z l |x, z <i ) as the approximate posterior up to the (l-1) level and the conditional in each prior p(z l |z <l ) and approximate posterior q ϕ (z l |x, z <i ) is represented as a factorial normal distribution. The symbol z <i is taken from the original paper  [Vahdat & Kautz (2021) ] and it means that the random variable is conditioned by the output of all latent spaces from 1 to i.\n\nIn this work, we adopted this basic architecture to propose hvEEGNet. The details characterizing our specific hierarchical VAE are reported in Section 3.2.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Veegnet -Ver3",
      "text": "Fig.  1  represents the schematic architecture of this simple VAE model. As any conventional VAE, it consists of an encoder, a latent space, and a decoder. However, inspired by the work of  Lawhern et al. (2016) , we designed the encoder as the popular EEGNet architecture, i.e., with the three processing blocks: in the first block, a horizontal convolution (that imitates the conventional temporal filtering) is followed by a batch normalization. In the second block, a vertical convolution, acting as a spatial filter, is applied. This operation is then followed by an activation and an average pooling step. The third, and last, block performs a separable convolution with a horizontal kernel, followed by an activation and an average pooling step. We always used, as in  Lawhern et al. (2016) , the exponential linear unit (ELU) activation function. At the output of the third block, the obtained D × C × T tensor is further transformed by means of a sampling layer which applies a convolution with a 1 × 1 kernel, thus doubling its depth size, resulting in a 2D × C × T tensor. Finally, the latter is projected onto the latent space (i.e., of dimension N = D • C • T ). In line with other previous works  [Zancanaro et al. (2023) ;  Kingma & Welling (2013) ], the first N elements of the depth map were intended as the marginal means (µ) and the second N elements as the marginal log-variances (σ) of the Gaussian distribution represented in the latent space. Then, to reconstruct the EEG data, the latent space z 0 is sampled using the reparametrization trick, as follows:\n\nTo note, this architecture is very similar to other previous architectures proposed by the authors in  Zancanaro et al. (2023)  and in Zancanaro et al.\n\n(under review). However, it introduces a few significant novelties that leads this new model to perform much better than the older ones. The most relevant novelty is that the reconstruction error L R of the VAE loss function expressed by eq. 1 was here quantified by the dynamic time warping (DTW) similarity score  [Sakoe & Chiba (1978) ], i.e., replacing the more standard mean square error (MSE). DTW leads to a more suitable measure of the similarity between two time-series  [Bankó & Abonyi (2012) ], thus allowing the model better learn to reconstruct EEG data. In fact, DTW is known to be more robust to nonlinear transformations of time-series  [Huang & Jansen (1985) ], thus capturing the similarity between two time-series even in presence of time shrinkage or dilatation, i.e., warpings. This cannot be achieved by MSE, which is highly sensitive to noise, i.e., the error computed by MSE rapidly increases when small modifications are applied to time-series.\n\nIn brief, given two time-series a(i) and b(j), where i, j = 1, 2, ..., T (i.e., for simplicity, we consider two series with the same length), DTW is a time-series alignment algorithm that extensively searches for the best match between them, by following a five-step procedure:\n\n1. The cost matrix W is initialized, with each row i associated with the corresponding amplitude value of the first time-series a(T -i + 1), while each column j associated with the corresponding amplitude value of the second time-series b(j).\n\n2. Starting from position W(0, 0), the value of each matrix element is com-\n\n3. The optimal warping path is identified as the minimum cost path in W, starting from the element W(1, T ), i.e., the upper right corner, ending to the element W(T, 1). 4. the array d is formed by taking the values of W included in the optimal warping path. Note that d might have a different (i.e., typically longer) length compared to the two original time-series, as a single element of one series could be associated with multiple elements of the other. 5. Finally, the normalized DTW score is computed as\n\nwhere K is the length of the array d. To note, normalization was not applied during the models' training (to keep this contribution in the range of the other loss function contributions). Whereas, during the performance evaluation, we used the normalized score. Nevertheless, this difference did not induce criticisms, as all segments share the same length.\n\nFinally, the projection onto the latent space was also modified w.r.t. our previous implementations: earlier, the tensor obtained by the convolutional layers was flattened into a vector and projected through a FFNN into a 2N -size vector, with N the dimension of the latent space. Then, the first N elements were interpreted as mean values and the second N elements as the log-variance values, respectively, of the distribution encoded in the latent space. Now, we apply a 1 × 1 convolution to the output of the encoder, thus obtaining a depth map whose first half is taken as the mean and the second half as the log-variance of the distribution of the latent space.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Hveegnet",
      "text": "As we observed sub-optimal reconstruction results with vEEGNet-ver3 and in line with other literature on computer vision  [Vahdat & Kautz (2021) ], we developed a new architecture, called hvEEGNet, to overcome the remaining issues of vEEGNet-ver3. The most relevant change in hvEEGNet w.r.t. vEEGNet-ver3 is its hierarchical architecture with three different latent spaces, namely z 1 , z 2 , and z 3 , with z 1 being the deepest one. Each of them is located at the output of each main block of the encoder, i.e., after the temporal convolution (Te) block (z 3 ), after the spatial convolution (Sp) block (z 2 ), and after the separable convolution (SC) block (z 1 ). The input to the decoder's Sp block is now given by the linear combination (i.e., the sum) of the SC block's output and the sampled data from z 2 . Similarly, the input to the decoder's Te block is obtained by the sum of the Sp block's output and the sampled data from z 3 . Incidentally, but significantly, it is worth noting that we kept here using the DTW algorithm to compute the reconstruction loss L R (with reference to eq. 5). The main structure of the model is depicted in Fig.  2 .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Outlier Identification",
      "text": "As our architectures implement completely self-supervised models, we have the opportunity to use them as anomaly detectors. In line with the vast majority of related work (as introduced in Section 2), in the present study, we define as an outlier any sample (i.e., EEG segment) that is very poorly reconstructed.\n\nThis, in turn, is verified by large values of the DTW similarity score between the reconstructed EEG sample and the original one. To identify such samples, we decided to use the kNN algorithm  [Cover & Hart (1967) ]. kNN is an unsupervised ML algorithm that computes the distance between every sample and its k-th nearest neighbour (with k properly chosen). All samples in the dataset are sorted w.r.t. increasing values of such distance. Those points whose distance (from their k-th nearest neighbour) exceeds a user-defined threshold are labelled as outliers.\n\nIn this study, we used our hvEEGNet model, only, to identify the outliers.\n\nBefore applying kNN, we performed two pre-processing steps: we computed the DTW similarity score for the EEG segment (i.e., channel-and repetition-wise).  To note, by definition (see eq. 5), the score is normalized by the number of time points in the series (even though all time-series have fixed length in this work).\n\nFor each training run, we built the following matrix E:  EEG segment (i.e., one row in matrix E). This allowed us to identify two types of outliers: (1) repetitions where all (or, the majority of the) channels were affected by some mild to severe problem, or (2) repetitions where only one (or, a few) channel was highly anomalous. Both are very common situations that might occur during neuroscience experiments  [Teplan et al. (2002) ].",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Implementation",
      "text": "We employed PyTorch to implement all pre-processing steps and to design and train our models.\n\nTo implement the newly proposed loss function, i.e., including the DTW computation, we exploited the soft-DTW loss function CUDA time-efficient implementation 1  [Maghoumi (2020);  Maghoumi et al. (2021) ]. In fact, the original DTW algorithm is quite time-consuming and employs a minimum function that is non-differentiable. Then, in  Cuturi & Blondel (2017) , a modification of the original algorithm was proposed to specifically be used in DL models, i.e., to be differentiable, thus suitable as a loss function. Then, CUDA was employed to make it time-efficient, too. Also, it is worth noting that DTW works with 1D time-series. However, our models aimed to reconstruct multi-channel EEG time-series. Then, during training, we computed the channel-wise DTW similarity score between the original and the reconstructed EEG segment. Then, in the loss function, we added the contribution coming from the sum of all channel-wise DTW scores.\n\nThe models were trained using the free cloud service offered by Google Colab, based on Nvidia Tesla T4 GPU. The hyperparameters were set as follows: batch size to 30, learning rate to 0.01, the number of epochs to 80, an exponential learning rate scheduler with γ set to 0.999. 20 training runs for each subject, were performed, in order to better evaluate the stability of the models training and the error trend along the epochs. The total number of parameters of the vEEGNet-ver3 is 4992 and the state dictionary (i.e., including all parameter weights) is 40 kB-weight. The total number of parameters of the hvEEGNet model is 8224, with 5456 of them to define the encoder, and the remaining 2768 for the decoder. Note that the higher number of parameters in the decoder is due to the sampling layers that operate on the three different latent spaces. The state dictionary of the parameter weights is about 56 kB.\n\nFinally, for the kNN algorithm for outliers detection (see Section 3.3), we employed the well-known knee method in the implementation given by the kneed python package  [Satopaa et al. (2011) ] to find the threshold distance to actually mark some samples as outliers.\n\nTo foster an open science approach to scientific research, we made our code available on GitHub 2  .",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Performance Evaluation",
      "text": "In this work, we evaluated our models in a within-subject scenario [Zanca- For visual inspection, we compared in a single plot the time domain representations of the original EEG segment and its corresponding reconstructed one.\n\nAlso, we computed the Welch's spectrogram  [Welch (1967) ] (in the implementation provided by the Python Scipy package 3  ) with the following parameters:\n\nHann's window of 500 time points with 250 time points overlap between consecutive segments.\n\nThen, to train and test our models (both vEEGNet-ver3 and hvEEGNet), Finally, the reconstruction ability of our models, after proper training (i.e., 80 epochs), was evaluated on the test set, too, by means of the same normalized DTW similarity score.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Dataset",
      "text": "To validate our model we used the Dataset 2a of BCI Competition IV  [Blankertz et al. (2007) ]. The dataset was downloaded using the MOABB tool  [Jayaram & Barachant (2018) ] and it is composed by the 22-channel EEG recordings of 9 subjects while they repeatedly performed four different MI tasks: imagining the movement of the right hand, left hand, feet or tongue. Each repetition consists of about 2 s fixation cross task, where a white cross appeared on a black screen and the subject needed to fix it and relax (as much as possible). Then, a 1.25 s cue allowed the subject to start imagining the required movement. The cue was displayed as an arrow pointing either left, right, up, or down, to indicate the corresponding task to perform, i.e., either left hand, right hand, tongue, or feet MI. MI was mantained until the fixation cross disappeared from the screen (for 3 s). A random inter-trial interval of a few seconds was applied (to avoid subjects habituation and expectation). Then, several repetitions of each type of MI were required to be performed. The order to repeat the different MI tasks was randomized to avoid habituation. The timeline of the experimental paradigm is depicted in Fig.  3 . A total number of 576 trials (or, repetitions) was collected from each individual subject and made available as a public dataset, namely the Dataset 2a of BCI Competition IV. The EEG data were recorded with a sampling frequency of 250 Hz and the authors filtered the data with a 0.5 -100Hz band-pass filter and a notch filter at 50 Hz (accordingly to the experimental records associated with the public dataset). We kept these settings as they were, to be in line with the literature  [Lawhern et al. (2016) ] and to be consistent with our previous studies  [Zancanaro et al. (2021 [Zancanaro et al. ( , 2023, under review), under review) ].\n\nAs explained in Section 3.4, we adopted the pre-defined 50/50 training/test split on the dataset and thus, for each subject, we obtained 260 EEG segments for the training set, 28 for the validation set, and 288 for the test set. To note, the dataset was perfectly balanced in terms of stratification of the different subjects in all splits.\n\nWe performed segmentation and, for each MI repetition, we extracted a 4 s (22-channel) EEG segment. The piece of EEG was selected in the most active MI part of the repetition, i.e., from 2 to 6 s, in order to isolate the most apparent brain behaviour related to the MI process.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Reconstruction Performance",
      "text": "In this section we show the performance of vEEGNet-ver3 and hvEEGNet, and we discuss to what extent the new loss function (with the DTW contribution) and the hierarchical architecture influenced the reconstruction performance.\n\nFirst, we visually inspect the output from our two models.  be observed, hvEEGNet is much better in reconstructing the EEG segment, and this can be very clearly appreciated in both domains. However, it is worth mentioning that vEEGNet-ver3 brought a large improvement w.r.t. its previous versions (i.e., vEEGNet-1  [Zancanaro et al. (2023) ] and vEEGNet-2  [Zancanaro et al. (under review) ]) as well as to other recently proposed architectures in the literature  [Bethge et al. (2022) ]. In fact, from our previous work  [Zancanaro et al. (2023) ], we noticed that the model trained with a reconstruction error based on MSE was capable of reconstructing slow components, only, while now with vEEGNet-ver3 the reconstructed signal has a much broader spectrum, with higher frequency components. Therefore, we can conclude that our choice to train the models using a loss function where the reconstruction error is quantified by the DTW made a significant improvement. Nevertheless, we can also infer that the hierarchical architecture has a relevant influence in the ability of the model to reconstruct the signal with high-fidelity, as one might expect from the literature on VAEs as applied to reduce blurry effects in the reconstructed images  [Vahdat & Kautz (2021) ].\n\nTo more systematically compare the results from the two architectures, we filled Table  3  with all subject-wise performance of both models, after training  Later, we will deepen the investigation of these two cases providing a reasonable explanation for this problem.  Computer vision literature has already shown that the hierarchical architecture made the VAE models able to generate more detailed images, i.e., more effective in learning and generating high frequency components  [Razavi et al. (2019) ;  Prost et al. (2022) ]. Here, the use of more than one latent space seemed to have similarly allowed hvEEGNet to better learn the underlying distribution of the data, and consequently greatly improved the reconstruction performance. This is also confirmed by Fig.  6 , where it is possible to see how the contributions of the three different latent spaces influenced the reconstruction performance of the model. As it can be observed, the deepest latent space (z 1 ) can quite follow the original signal, has a similar dynamics (check also the power spectrum in Fig.  6 ), but suffers from some time shifts and amplitude mismatches. Still, this result is better than the vEEGNet-ver3 output, even though sampling from z 1 in hvEEGNet could have similarities with sampling from z 0 in vEEGNet-ver3 (e.g., much faster components can be recovered from z 1 , but not from z 0 ).\n\nThen, sampling from more superficial (i.e., detailed) latent spaces produces an increasingly better reconstruction quality: when sampling from z 2 (including   the effect from the deepest latent space z 1 ), amplitude mismatches are less frequent compared to the previous case, and the power spectrum is very similar to the original one. Finally, when sampling from z 2 , the reconstruction is almost perfect, with minimal amplitude incongruences and time shifts.\n\nHowever, we found cases where hvEEGNet dramatically failed in reconstructing the original EEG data. Also, there were cases in which the same number of training epochs was not enough for the hvEEGNet model to reconstruct a particular subject. These two issues are discussed in the following, with additional investigations.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Training Behaviour Vs Training Set: Investigations On Hveegnet",
      "text": "hvEEGNet should be trained until the DTW is small enough to guarantee optimal reconstruction. We performed multiple (about 20) training runs with 80 epochs each, to evaluate the statistical behaviour of the model's training in different subjects. We also computed the average normalized DTW similarity score and its standard deviation across multiple runs and could show, for each subject, separately, the number of epochs at which that average is low enough and the standard deviation stabilizes, at the same time. Fig.  7  displays the average (and standard deviation) DTW-based error for an increasing number of epochs for each subject during training. We can observe that the DTW-based error clearly decreases as the number of epochs increases, as expected. Then, for all subjects, 80 epochs are enough to obtain almost perfect reconstruction.\n\nHowever, we also clearly noted that the time (no. epochs) needed to reach that point highly varies from subject to subject. For example, S3 reaches an optimal model very rapidly, in about 15 epochs: we can see that the training of an hvEEGNet model starts with an average DTW error of 38 and a large standard deviation of 12, then it fastly decreases in its mean and variability, reaching a stable average of 5 and a very small standard deviation in 15 epochs.\n\nA completely different case is represented by S9: here, the average beginning error is smaller than the S3's one, but the standard deviation is much larger.\n\nAlso, it takes much more -on average -to the model to adapt to this subject and reach a stable and optimal model (at about 60 epochs). Therefore, we have just empirically proved that there is a relationship between the training time (i.e., the number of epochs needed to reach an optimal model) and the distribution of the input training set that cannot be overlooked  [Gyori et al. (2022) ].\n\nAnother relevant case to discuss is the dramatic fail of the hvEEGNet model in reconstructing some -rare -specific EEG segments. We found four anomalous training runs where the model failed, i.e., two for S4, one for S5, and another for S8. We further analyzed all segments in these three subjects and discovered that the model fail was due to problems of saturation that happened during the acquisition step of the EEG data (those segments had not been removed from the public available dataset). This, in turn, led the DTW score to assume extremely high values, i.e., the model to significantly fail the reconstruction.   tematic investigation of the relationship between the models training and the characteristics of the input data.\n\nIn the following, we investigate the performance of the hvEEGNet model, when sufficiently trained (i.e., for a number of epochs that varies from subject to subject), and we explore its ability to identify other anomalies as well as reconstructing clean EEG segments.",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "Hveegnet As Anomaly Detector",
      "text": "Once the hvEEGNet model is properly trained, we can look into its ability to identify outliers. To be conservative, for all subjects, we searched for outliers in the test set with the hvEEGNet model trained for 80 epochs. As described in Section 3.3, we employed the kNN algorithm on the matrix E given by all values obtained by averaging the normalized DTW scores across the training runs for each pair task repetition-channel (subject-wise). Here, E is a 288 × 22 matrix.\n\nThen, we applied kNN over E to find out any possible outliers. We implemented the algorithm using the scikit-learn Python package  [Pedregosa et al. (2011) ],\n\nwith default settings and the number of nearest neighbours (k) equal to 15.\n\nWe empirically found that 15 was a good trade-off between the stability of the results and the expected proximity among all samples in the dataset. Also, note that each sample of this matrix is characterized by 22 dimensions, and the kNN algorithm worked in such high-dimensional space to find proximity among points as well as outliers.    In all panels, i.e., for all subjects, from Fig.  10  we can easily distinguish two phases in the training behaviour: in the first part of the training, as expected, the average reconstruction error is progressively reduced (black line). This typically corresponds to a few outliers (red line) significantly contributing to the global average error (dark green line). In the second phase, i.e., after the model has reached a stable performance (in terms of average reconstruction error), the number of outliers starts to vary with the average global error and the outliers average error remaining quite small. This can be intuitively explained by the fact that the EEG segments are generally well reconstructed and small variations on the error are enough to make the corresponding EEG segment to be considered an outlier by the kNN algorithm.\n\nTherefore, we decided to deepen the investigation on the transition point to check if a subject-independent characterization of the training behaviour can be obtained, and to verify the opportunity to stop the training at that point.\n\nFirst, we empirically defined the transition point as the number of epochs where the global average error showed the maximal slope (an elbow point), with low standard deviation, and the number of identified outliers was about to suddenly increase. For example, for S1 the transition point was identified at 30 epochs, while for S9 at 40 epochs. To note, the earliest transition point was found in S3 and S5 at 20 epochs, while the latest transition point was found in S4 at 45 epochs. Second, we re-evaluated the performance of our hvEEGNet model on the test set with the training stopped at the transition point. Table  4  reports the average (and standard deviation) reconstruction error at the subject-specific transition point. We can observe that the reconstruction performance are similar to the performance obtained for an extensively trained model (see Table  3  for the comparison). Thus, we can conclude that our hvEEGNet model could reach very high-fidelity reconstruction in a short time, lower than 30 minutes (approx.\n\ntime needed to train the model for 50 epochs, as reported in Section 4.5).\n\nUsually, as already discussed in Section 2, when using autoencoder architectures to identify outliers, the model is trained on normal data  [Ortiz et al. (2020) ] and anomalies result from the model's largest errors  [Pang et al. (2021) ].\n\nAnyway, in more ecological acquisition scenarios  [Muharemi et al. (2019) ] and, frequently, when the human is in-the-loop  [Straetmans et al. (2022) ], anomaly detectors can be successfully trained on a mixture of clean and noisy data, too  [Al-amri et al. (2021) ]. Anyway, for EEG data, normality cannot be easily defined and it is quite challenging to ensure a dataset to be anomaly-free.\n\nReconstruction error Subject id. artefacts. In fact, it is well-known  [Buzsaki & Draguhn (2004) ] that the typical power spectrum of a clean EEG acquired from a healthy subject follows a 1/f shape, with other relevant components (contributing as visible peaks) at the center of band of the α band (approx. 10 Hz) and of the β band (approx.\n\n20 Hz, generally less visible). In its upper panels, Fig.  11  shows an example of clean dataset (from S1). Whereas, the lower panels report the power spectra of S2 and S5. It was decisive to visualize these spectra to realize that S2 has a normal (average) power spectrum in his/her training set, while highly noisy power spectrum in his/her test set. Furthermore, it could be easily recognized that the large power contribution in other frequency ranges (e.g., higher than 50 Hz) is possibly be due to muscular activity that was simultaneously recorded by the EEG electrodes during the test session  [Chen et al. (2019) ]. A similar situation was found for S5: again, all data coming from the test set were clearly corrupted by the 50 Hz power supply. We might only guess that, for some reason, the notch filter at 50 Hz (see Section 4.1) was not actually applied for this subject during the second recording session, i.e., the test session.\n\nThis finally explained why our hvEEGNet model dramatically failed at reconstructing S2 and S5 in their test sets, while keeping very satisfactory performance in the training phase. Furthermore, this might also motivate why the large majority of the related work classifying (i.e., with DL models, at least) this public dataset found the worst results on S2 and S5  [Zancanaro et al. (2023) ].  include the approximation of the DTW function itself with a neural network, as recently proposed by  Lerogeron et al. (2023a,b) . The present work still suffers from a number of limitations, e.g., the investigation of the balance between the different components of the training loss and their impact of the training course and quality. Their investigation and solution have been left to further studies in favor of a few relevant take-home messages that can be robustly supported by the results available so far. As one of many possible future perspectives, hvEEGNet will be tested on different EEG datasets, including different types of anomalies, to prove its generalizability and the extent to which it can identify either artefactual or pathological EEG data.",
      "page_start": 31,
      "page_end": 38
    },
    {
      "section_name": "Computational Complexity",
      "text": "",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we targeted the problem of EEG reconstruction using two We tested our models on the public, and very popular, Dataset 2a -BCI Competition IV, where a 22-channel EEG dataset was collected from 9 subjects repeatedly performing MI of the right hand, the left hand, the feet, and the tongue. Our results showed that hvEEGNet brings to a very high-fidelity reconstruction, outperforming our previous solutions as well as the other stateof-the-art models. This outcome was also consistent across all subjects and repetitions. Furthermore, we deeply investigated the reconstruction fidelity across every individual subject of the dataset, and found that hvEEGNet failed to reconstruct some EEG data belonging to the test sessions of S2 and S5. However, we realized that this failure could not be addressed to the poorness of the model, but rather to the input data quality. Interestingly, we were able to identify specific EEG segments and channels where the raw EEG data were corrupted (e.g., by saturation during the acquisition phase). Therefore, hvEEGNet could be effectively employed as an automatic detector for noisy (e.g., artefactual) EEG segments, thus supporting the domain experts. Also, with this deep investigation, we have finally provided an explanation for the high variability of classification results that are regularly found in the literature, when any ML and DL-based models are applied to this dataset.\n\nThis work opens new fundamental research questions, i.e., regarding the relationship between the DL-based models training (including its effectiveness and possible biases) and the quality of the input data, still very poorly investigated in this application domain. This approach intends to adhere to the best scientific methodological practises of new AI methods applied to the medical domain, in line with  Cabitza & Campagner (2021) . In the future, hvEEGNet and the",
      "page_start": 38,
      "page_end": 39
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: represents the schematic architecture of this simple VAE model. As",
      "page": 12
    },
    {
      "caption": "Figure 1: Schematic architecture of our model called vEEGNet-ver3. The encoder block is",
      "page": 12
    },
    {
      "caption": "Figure 2: Figure 2: Schematic architecture of our model called hvEEGNet. The encoder block is formed",
      "page": 15
    },
    {
      "caption": "Figure 3: The timeline of the experimental paradigm (modified from Blankertz et al. (2007)).",
      "page": 21
    },
    {
      "caption": "Figure 3: A total number of 576 trials (or, repetitions) was",
      "page": 21
    },
    {
      "caption": "Figure 4: shows an example of two raw",
      "page": 21
    },
    {
      "caption": "Figure 4: Two representative EEG segments from the Dataset 2a from S3 (time range limited",
      "page": 22
    },
    {
      "caption": "Figure 5: Comparison of the reconstruction performance in the time and frequency domain",
      "page": 23
    },
    {
      "caption": "Figure 6: , where it is possible to see how the contributions",
      "page": 25
    },
    {
      "caption": "Figure 6: ), but suffers from some time shifts and amplitude mismatches. Still,",
      "page": 25
    },
    {
      "caption": "Figure 6: Reconstruction as obtained at different points of the hierarchy both in time and",
      "page": 26
    },
    {
      "caption": "Figure 5: The first three rows represent",
      "page": 26
    },
    {
      "caption": "Figure 7: displays the",
      "page": 27
    },
    {
      "caption": "Figure 7: Reconstruction error (within-subject, across-EEG channels). Box markers show the",
      "page": 28
    },
    {
      "caption": "Figure 7: shows the model training behaviour along the epochs both with and without",
      "page": 29
    },
    {
      "caption": "Figure 8: Example of saturated trials in the training set leading to very high DTW values.",
      "page": 30
    },
    {
      "caption": "Figure 9: shows three representative examples of EEG segments",
      "page": 31
    },
    {
      "caption": "Figure 7: has shown that the hvEEGNet model can reach very low",
      "page": 31
    },
    {
      "caption": "Figure 9: Three representative examples of EEG segments (belonging to the test set) marked",
      "page": 32
    },
    {
      "caption": "Figure 10: , where the global average error of the whole training",
      "page": 33
    },
    {
      "caption": "Figure 10: Relationship between training effectiveness and outliers identification ability w.r.t.",
      "page": 33
    },
    {
      "caption": "Figure 10: we can easily distinguish two",
      "page": 33
    },
    {
      "caption": "Figure 11: It shows the",
      "page": 35
    },
    {
      "caption": "Figure 11: shows an example of",
      "page": 36
    },
    {
      "caption": "Figure 11: Average power spectra for training set (panels on the left side) and test set (panels",
      "page": 37
    }
  ],
  "tables": [
    {
      "caption": "Table 1: reports the details of both vEEGNet-ver3 and hvEEGNet architec-",
      "data": [
        {
          "Parameters name": "",
          "vEEGNet-ver3": "Kernel\nIn. dep.\nOut. dep.\nNotes",
          "hvEEGNet": "Kernel\nIn. dep.\nOut. dep.\nNotes"
        },
        {
          "Parameters name": "First block\n(temporal filter)",
          "vEEGNet-ver3": "(1, 128)\n1\n8\nDepth-wise convolution\n-\n-\n-\nDefault parameters",
          "hvEEGNet": "(1, 128)\n1\n8\nDepth-wise convolution\n-\n-\n-\nDefault parameters"
        },
        {
          "Parameters name": "Second block\n(spatial filter)",
          "vEEGNet-ver3": "(1, 22)\n8\n16\nDepth-wise convolution\n-\n-\n-\nDefault Parameters\n-\n-\n-\nELU\n(1, 4)\n-\n-\n-\n-\n-\n-\np = 0.5",
          "hvEEGNet": "(1, 22)\n8\n16\nDepth-wise convolution\n-\n-\n-\nDefault Parameters\n-\n-\n-\nELU\n-\n-\n-\nNo pooling used\n-\n-\n-\np = 0.5"
        },
        {
          "Parameters name": "Third Block\n(Separable Convolutoin)",
          "vEEGNet-ver3": "(1, 32)\n16\n16\nDepth-wise convolution\n(1, 1)\n16\n16\nPointwise convolution\nDefault parameters\n-\n-\n-\nELU\n(1, 8)\n-\n-\n-\n-\n-\n-\np = 0.5",
          "hvEEGNet": "(1, 32)\n16\n16\nDepth-wise convolution\n(1, 1)\n16\n16\nPointwise convolution\nDefault parameters\n-\n-\n-\nELU\n(1, 10)\n-\n-\n-\n-\n-\n-\np = 0.5"
        },
        {
          "Parameters name": "Sample layer",
          "vEEGNet-ver3": "(1,1)\n16\n32\nPointwise convolution",
          "hvEEGNet": "(1,1)\n16\n32\nPointwise convolution"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 2: Parameters’ values of the vEEGNet-ver3’s and hvEEGNet’s decoder, respectively.",
      "data": [
        {
          "Parameters name": "",
          "vEEGNet-ver3": "Kernel\nIn. dep.\nOut. dep.\nNotes",
          "hvEEGNet": "Kernel\nIn. dep.\nOut. Dep.\nNotes"
        },
        {
          "Parameters name": "Third Block\n(Separable Convolutoin)",
          "vEEGNet-ver3": "-\n-\n-\np = 0.5\n(1, 8)\n-\n-\n-\n-\n-\n-\nELU\nDefault parameters\n(1, 1)\n16\n16\nPointwise convolution\n(1, 32)\n16\n16\nDepth-wise convolution",
          "hvEEGNet": "-\n-\n-\np = 0.5\n(1, 10)\n-\n-\n-\n-\n-\n-\nELU\nDefault parameters\n(1, 1)\n16\n16\nPointwise convolution\n(1, 32)\n16\n16\nDepth-wise convolution"
        },
        {
          "Parameters name": "Second block\n(spatial filter)",
          "vEEGNet-ver3": "-\n-\n-\np = 0.5\n(1, 4)\n-\n-\n-\n-\n-\n-\nELU\n-\n-\n-\nDefault parameters\n(1, 22)\n8\n16\nDepth-wise convolution",
          "hvEEGNet": "-\n-\n-\np = 0.5\n-\n-\n-\nNo pooling used\n-\n-\n-\nELU\n-\n-\n-\nDefault parameters\n(1, 22)\n8\n16\nDepth-wise convolution"
        },
        {
          "Parameters name": "First block\n(temporal filter)",
          "vEEGNet-ver3": "-\n-\n-\nDefault parameters\n(1, 128)\n1\n8\nDepth-wise convolution",
          "hvEEGNet": "-\n-\n-\nDefault parameters\n(1, 128)\n1\n8\nDepth-wise convolution"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Subject id.": "1\n2\n3\n4\n5\n6\n7\n8\n9",
          "vEEGNet-ver3\nTrain\nTest": "18.41±6.26\n22.99±22.52\n18.06±8.88\n128.05±162.16\n48.34±14.19\n41.35±34.16\n18.1±21.66\n18.01±12.51\n17.38±15.16\n49.09±9.92\n32.92±13.28\n29.01±12.49\n13.49±3.67\n12.37±2.85\n42.13±21.19\n48.5±12.36\n36.45±21.33\n33.87±8.12",
          "hvEEGNet\nTrain\nTest": "1.16±0.36\n2.3±1.84\n1.7±0.81\n60.81±65.84\n1.87±0.62\n4.96±5.81\n3.59±7.44\n1.51±1.13\n1.01±0.45\n15.67±3.95\n1.76±0.72\n1.87±0.61\n1.02±0.28\n0.9±0.33\n4.07±1.61\n5.46±1.65\n2.01±1.23\n1.91±0.57"
        },
        {
          "Subject id.": "AVG",
          "vEEGNet-ver3\nTrain\nTest": "27.25\n42.58",
          "hvEEGNet\nTrain\nTest": "2.02\n10.6"
        },
        {
          "Subject id.": "STD",
          "vEEGNet-ver3\nTrain\nTest": "13.96\n30.79",
          "hvEEGNet\nTrain\nTest": "1.5\n9.08"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table 4: Average (± standard deviation) reconstruction error for hvEEGNet in the test set,",
      "data": [
        {
          "Subject id.": "1\n2\n3\n4\n5\n6\n7\n8\n9",
          "Transition point\n[epoch no.]": "30\n45\n20\n45\n20\n25\n30\n30\n40",
          "Reconstruction error\nTrain\nTest": "3.0±0.81\n4.68±3.55\n1.73±0.82\n60.01±65.89\n2.79±0.79\n3.58±1.53\n2.18±1.76\n2.19±1.71\n5.97±2.5\n36.29±6.44\n2.01±0.95\n3.37±1.02\n1.86±0.68\n1.42±0.43\n5.06±1.92\n6.49±1.82\n4.51±6.1\n3.58±0.8"
        }
      ],
      "page": 35
    },
    {
      "caption": "Table 5: shows the inference time, i.e. the time the model used to encode",
      "data": [
        {
          "Batch size\n[no. of EEG segments]": "1\n10\n100\n288 (all)",
          "CPU 1\n[s]": "0.13 ± 0.05\n0.83 ± 0.13\n5.91 ± 1.45\n10.69 ± 1.87",
          "GPU 1\n[ms]": "8 ± 0.9\n23.3 ± 17.2\n63.7 ± 10.3\n182.7 ± 30.1",
          "CPU 2\n[s]": "0.05 ± 0.003\n0.33 ± 0.03\n5.02 ± 0.49\n14.73 ± 0.44",
          "GPU 2\n[ms]": "3.24 ± 0.26\n10.45 ±1.58\n62.15 ± 4.95\n179.16 ± 18.22"
        }
      ],
      "page": 38
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A review of machine learning and deep learning techniques for anomaly detection in iot data",
      "authors": [
        "R Al-Amri",
        "R Murugesan",
        "M Man",
        "A Abdulateef",
        "M Al-Sharafi",
        "A Alkahtani"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "2",
      "title": "Convolutional autoencoder approach for eeg compression and reconstruction in m-health systems",
      "authors": [
        "A Al-Marridi",
        "A Mohamed",
        "A Erbad"
      ],
      "year": "2018",
      "venue": "2018 14th International Wireless Communications & Mobile Computing Conference (IWCMC)",
      "doi": "10.1109/IWCMC.2018.8450511"
    },
    {
      "citation_id": "3",
      "title": "Wearable electroencephalography and multimodal mental state classification: A systematic literature review",
      "authors": [
        "C Anders",
        "B Arnrich"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "4",
      "title": "Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state",
      "authors": [
        "R Andrzejak",
        "K Lehnertz",
        "F Mormann",
        "C Rieke",
        "P David",
        "C Elger"
      ],
      "year": "2002",
      "venue": "Physical review. E, Statistical, nonlinear, and soft matter physics",
      "doi": "10.1103/PhysRevE.64.061907"
    },
    {
      "citation_id": "5",
      "title": "Filter bank common spatial pattern (fbcsp) in brain-computer interface",
      "authors": [
        "K Ang",
        "Z Chin",
        "H Zhang",
        "C Guan"
      ],
      "year": "2008",
      "venue": "IEEE"
    },
    {
      "citation_id": "6",
      "title": "Correlation based dynamic time warping of multivariate time series",
      "authors": [
        "Z Bankó",
        "J Abonyi"
      ],
      "year": "2012",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "7",
      "title": "Shared intelligence for robot teleoperation via bmi",
      "authors": [
        "G Beraldo",
        "L Tonin",
        "J Millán",
        "E Menegatti"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Human-Machine Systems",
      "doi": "10.1109/THMS.2021.3137035"
    },
    {
      "citation_id": "8",
      "title": "On the eeg in humans",
      "authors": [
        "H Berger"
      ],
      "year": "1929",
      "venue": "Arch. Psychiatr. Nervenkr"
    },
    {
      "citation_id": "9",
      "title": "Eeg2vec: Learning affective eeg representations via variational autoencoders",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "T Grosse-Puppendahl",
        "M Kari",
        "L Chuang",
        "O Özdenizci",
        "A Schmidt"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Systems, Man, and Cybernetics",
      "doi": "10.1109/SMC53654.2022.9945517"
    },
    {
      "citation_id": "10",
      "title": "The non-invasive Berlin Brain-Computer Interface: Fast acquisition of effective performance in untrained subjects",
      "authors": [
        "B Blankertz",
        "G Dornhege",
        "M Krauledat",
        "K.-R Müller",
        "G Curio"
      ],
      "year": "2007",
      "venue": "NeuroImage",
      "doi": "10.1016/j.neuroimage.2007.01.051"
    },
    {
      "citation_id": "11",
      "title": "Variational inference: A review for statisticians",
      "authors": [
        "D Blei",
        "A Kucukelbir",
        "J Mcauliffe"
      ],
      "year": "2017",
      "venue": "Journal of the American statistical Association"
    },
    {
      "citation_id": "12",
      "title": "Deep learning-based classification of fine hand movements from low frequency eeg",
      "authors": [
        "G Bressan",
        "G Cisotto",
        "G Müller-Putz",
        "S Wriessnegger"
      ],
      "year": "2021",
      "venue": "Future Internet"
    },
    {
      "citation_id": "13",
      "title": "Neuronal oscillations in cortical networks",
      "authors": [
        "G Buzsaki",
        "A Draguhn"
      ],
      "year": "2004",
      "venue": "science"
    },
    {
      "citation_id": "14",
      "title": "The need to separate the wheat from the chaff in medical informatics: Introducing a comprehensive checklist for the (self)-assessment of medical ai studies",
      "authors": [
        "F Cabitza",
        "A Campagner"
      ],
      "year": "2021",
      "venue": "The need to separate the wheat from the chaff in medical informatics: Introducing a comprehensive checklist for the (self)-assessment of medical ai studies"
    },
    {
      "citation_id": "15",
      "title": "Removal of muscle artifacts from the eeg: A review and recommendations",
      "authors": [
        "X Chen",
        "X Xu",
        "A Liu",
        "S Lee",
        "X Chen",
        "X Zhang",
        "M Mckeown",
        "Z Wang"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "16",
      "title": "Real-time detection of eeg electrode displacement for brain-computer interface applications",
      "authors": [
        "G Cisotto",
        "P Silvano"
      ],
      "year": "2015",
      "venue": "Proceedings of 5th International Conference on Wireless Communications, Vehicular Technology, Information Theory and Aerospace & Electronic Systems (Wireless VITAE)"
    },
    {
      "citation_id": "17",
      "title": "Nearest neighbor pattern classification",
      "authors": [
        "T Cover",
        "P Hart"
      ],
      "year": "1967",
      "venue": "IEEE transactions on information theory"
    },
    {
      "citation_id": "18",
      "title": "Soft-dtw: a differentiable loss function for time-series",
      "authors": [
        "M Cuturi",
        "M Blondel"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "Joint ecg-emg-eeg signal compression and reconstruction with incremental multimodal autoencoder approach",
      "authors": [
        "E Dasan",
        "R Gnanaraj"
      ],
      "year": "2022",
      "venue": "Circuits, Systems, and Signal Processing",
      "doi": "10.1007/s00034-022-02071-x"
    },
    {
      "citation_id": "20",
      "title": "A longitudinal study investigating neural processing of speech envelope modulation rates in children with (a family risk for) dyslexia",
      "authors": [
        "A De Vos",
        "S Vanvooren",
        "J Vanderauwera",
        "P Ghesquière",
        "J Wouters"
      ],
      "year": "2017",
      "venue": "Cortex",
      "doi": "10.1016/j.cortex.2017.05.007"
    },
    {
      "citation_id": "21",
      "title": "Threaded ensembles of autoencoders for stream learning",
      "authors": [
        "Y Dong",
        "N Japkowicz"
      ],
      "year": "2018",
      "venue": "Computational Intelligence"
    },
    {
      "citation_id": "22",
      "title": "A simple system for detection of eeg artifacts in polysomnographic recordings",
      "authors": [
        "P Durka",
        "H Klekowicz",
        "K Blinowska",
        "W Szelenberger",
        "S Niemcewicz"
      ],
      "year": "2003",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "23",
      "title": "Autoencoding of long-term scalp electroencephalogram to detect epileptic seizure for diagnosis support system",
      "authors": [
        "A Emami",
        "N Kunii",
        "T Matsuo",
        "T Shinozaki",
        "K Kawai",
        "H Takahashi"
      ],
      "year": "2019",
      "venue": "Computers in Biology and Medicine",
      "doi": "10.1016/j.compbiomed.2019.05.025"
    },
    {
      "citation_id": "24",
      "title": "Seizure detection by convolutional neural network-based analysis of scalp electroencephalography plot images",
      "authors": [
        "A Emami",
        "N Kunii",
        "T Matsuo",
        "T Shinozaki",
        "K Kawai",
        "H Takahashi"
      ],
      "year": "2019",
      "venue": "NeuroImage: Clinical",
      "doi": "10.1016/j.nicl.2019.101684"
    },
    {
      "citation_id": "25",
      "title": "A multi-artifact eeg denoising by frequency-based deep learning",
      "authors": [
        "M Gabardi",
        "A Saibene",
        "F Gasparini",
        "D Rizzo",
        "F Stella"
      ],
      "year": "2023",
      "venue": "A multi-artifact eeg denoising by frequency-based deep learning",
      "arxiv": "arXiv:2310.17335"
    },
    {
      "citation_id": "26",
      "title": "Automatic removal of various artifacts from eeg signals using combined methods",
      "authors": [
        "J Gao",
        "Y Yang",
        "J Sun",
        "G Yu"
      ],
      "year": "2010",
      "venue": "Journal of Clinical Neurophysiology"
    },
    {
      "citation_id": "27",
      "title": "Training data distribution significantly impacts the estimation of tissue microstructure with machine learning",
      "authors": [
        "N Gyori",
        "M Palombo",
        "C Clark",
        "H Zhang",
        "D Alexander"
      ],
      "year": "2022",
      "venue": "Magnetic Resonance in Medicine",
      "doi": "10.1002/mrm.29014"
    },
    {
      "citation_id": "28",
      "title": "A review on machine learning for eeg signal processing in bioengineering",
      "authors": [
        "M.-P Hosseini",
        "A Hosseini",
        "K Ahi"
      ],
      "year": "2020",
      "venue": "IEEE reviews in biomedical engineering"
    },
    {
      "citation_id": "29",
      "title": "Eeg waveform analysis by means of dynamic time-warping",
      "authors": [
        "H.-C Huang",
        "B Jansen"
      ],
      "year": "1985",
      "venue": "International journal of bio-medical computing"
    },
    {
      "citation_id": "30",
      "title": "Moabb: trustworthy algorithm benchmarking for bcis",
      "authors": [
        "V Jayaram",
        "A Barachant"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "31",
      "title": "Relationship between electrical brain responses to motor imagery and motor impairment in stroke",
      "authors": [
        "V Kaiser",
        "I Daly",
        "F Pichiorri",
        "D Mattia",
        "G Müller-Putz",
        "C Neuper"
      ],
      "year": "2012",
      "venue": "Stroke"
    },
    {
      "citation_id": "32",
      "title": "A shallow autoencoder framework for epileptic seizure detection in eeg signals",
      "authors": [
        "G Khan",
        "N Khan",
        "M Altaf",
        "Q Abbasi"
      ],
      "year": "2023",
      "venue": "Sensors",
      "doi": "10.3390/s23084112"
    },
    {
      "citation_id": "33",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "34",
      "title": "An introduction to variational autoencoders",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2019",
      "venue": "An introduction to variational autoencoders",
      "arxiv": "arXiv:1906.02691"
    },
    {
      "citation_id": "35",
      "title": "Thirty-minute motor imagery exercise aided by eeg sensorimotor rhythm neurofeedback enhances morphing of sensorimotor cortices: a double-blind sham-controlled study",
      "authors": [
        "M Kodama",
        "S Iwama",
        "M Morishige",
        "J Ushiba"
      ],
      "year": "2023",
      "venue": "Cerebral Cortex"
    },
    {
      "citation_id": "36",
      "title": "Deap: A database for emotion analysis ;using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "37",
      "title": "Clinical evoked potentials in neurology: a review of techniques and indications",
      "authors": [
        "A Lascano",
        "P Lalive",
        "M Hardmeier",
        "P Fuhr",
        "M Seeck"
      ],
      "year": "2017",
      "venue": "Neurosurgery & Psychiatry"
    },
    {
      "citation_id": "38",
      "title": "EEGNet: A compact convolutional network for EEG-based Brain-Computer Interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2016",
      "venue": "Journal of Neural Engineering",
      "doi": "10.1088/1741-2552/aace8c"
    },
    {
      "citation_id": "39",
      "title": "Learning an autoencoder to compress eeg signals via a neural network based approximation of dtw",
      "authors": [
        "H Lerogeron",
        "R Picot-Clémente",
        "L Heutte",
        "A Rakotomamonjy"
      ],
      "year": "2023",
      "venue": "Procedia Computer Science",
      "doi": "10.1016/j.procs.2023.08.183"
    },
    {
      "citation_id": "40",
      "title": "International Neural Network Society Workshop on Deep Learning Innovations and Applications",
      "venue": "International Neural Network Society Workshop on Deep Learning Innovations and Applications",
      "doi": "10.1016/j.procs.2023.08.183"
    },
    {
      "citation_id": "41",
      "title": "Approximating dynamic time warping with a convolutional neural network on eeg data",
      "authors": [
        "H Lerogeron",
        "R Picot-Clémente",
        "A Rakotomamonjy",
        "L Heutte"
      ],
      "year": "2023",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2023.05.012"
    },
    {
      "citation_id": "42",
      "title": "Densely feature fusion based on convolutional neural networks for motor imagery EEG classification",
      "authors": [
        "D Li",
        "J Wang",
        "J Xu",
        "X Fang"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2941867"
    },
    {
      "citation_id": "43",
      "title": "Constructing large-scale real-world benchmark datasets for aiops",
      "authors": [
        "Z Li",
        "N Zhao",
        "S Zhang",
        "Y Sun",
        "P Chen",
        "X Wen",
        "M Ma",
        "D Pei"
      ],
      "year": "2022",
      "venue": "Constructing large-scale real-world benchmark datasets for aiops",
      "arxiv": "arXiv:2208.03938"
    },
    {
      "citation_id": "44",
      "title": "Eegbased emotion classification using a deep neural network and sparse autoencoder",
      "authors": [
        "J Liu",
        "G Wu",
        "Y Luo",
        "S Qiu",
        "S Yang",
        "W Li",
        "Y Bi"
      ],
      "year": "2020",
      "venue": "Frontiers in Systems Neuroscience, 14",
      "doi": "10.3389/fnsys.2020.00043"
    },
    {
      "citation_id": "45",
      "title": "A review of classification algorithms for eeg-based brain-computer interfaces: a 10 year update",
      "authors": [
        "F Lotte",
        "L Bougrain",
        "A Cichocki",
        "M Clerc",
        "M Congedo",
        "A Rakotomamonjy",
        "F Yger"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "46",
      "title": "Deep Recurrent Networks for Gesture Recognition and Synthesis",
      "authors": [
        "M Maghoumi"
      ],
      "year": "2020",
      "venue": "Deep Recurrent Networks for Gesture Recognition and Synthesis"
    },
    {
      "citation_id": "47",
      "title": "Deepnag: Deep nonadversarial gesture generation",
      "authors": [
        "M Maghoumi",
        "E Taranta",
        "J Laviola"
      ],
      "year": "2021",
      "venue": "26th International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "48",
      "title": "Machine learning approaches for anomaly detection of water quality on a real-world data set",
      "authors": [
        "F Muharemi",
        "D Logofȃtu",
        "F Leon"
      ],
      "year": "2019",
      "venue": "Journal of Information and Telecommunication"
    },
    {
      "citation_id": "49",
      "title": "Local or edge/cloud processing for data freshness",
      "authors": [
        "A Munari",
        "T Cola",
        "L Badia"
      ],
      "year": "2023",
      "venue": "Local or edge/cloud processing for data freshness"
    },
    {
      "citation_id": "50",
      "title": "Attempted arm and hand movements can be decoded from low-frequency eeg from persons with spinal cord injury",
      "authors": [
        "P Ofner",
        "A Schwarz",
        "J Pereira",
        "D Wyss",
        "R Wildburger",
        "G Müller-Putz"
      ],
      "year": "2019",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "51",
      "title": "Dyslexia diagnosis by eeg temporal and spectral descriptors: An anomaly detection approach",
      "authors": [
        "A Ortiz",
        "F Martinez-Murcia",
        "J Luque",
        "A Giménez",
        "R Morales-Ortega",
        "J Ortega"
      ],
      "year": "2020",
      "venue": "International Journal of Neural Systems"
    },
    {
      "citation_id": "52",
      "title": "Deep learning for anomaly detection: A review",
      "authors": [
        "G Pang",
        "C Shen",
        "L Cao",
        "A Hengel"
      ],
      "year": "2021",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3439950"
    },
    {
      "citation_id": "53",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "54",
      "title": "Iclabel: An automated electroencephalographic independent component classifier, dataset, and website",
      "authors": [
        "L Pion-Tonachini",
        "K Kreutz-Delgado",
        "S Makeig"
      ],
      "year": "2019",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "55",
      "title": "Diverse super-resolution with pretrained deep hiererarchical vaes",
      "authors": [
        "J Prost",
        "A Houdard",
        "N Papadakis",
        "A Almansa"
      ],
      "year": "2022",
      "venue": "Diverse super-resolution with pretrained deep hiererarchical vaes",
      "arxiv": "arXiv:2205.10347"
    },
    {
      "citation_id": "56",
      "title": "Kpi-tsad: A time-series anomaly detector for kpi monitoring in cloud applications",
      "authors": [
        "J Qiu",
        "Q Du",
        "C Qian"
      ],
      "year": "2019",
      "venue": "Symmetry",
      "doi": "10.3390/sym11111350"
    },
    {
      "citation_id": "57",
      "title": "Denoising sparse autoencoder-based ictal eeg classification",
      "authors": [
        "Y Qiu",
        "W Zhou",
        "N Yu",
        "P Du"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "doi": "10.1109/TNSRE.2018.2864306"
    },
    {
      "citation_id": "58",
      "title": "Generating diverse high-fidelity images with vq-vae-2",
      "authors": [
        "A Razavi",
        "A Van Den Oord",
        "O Vinyals"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "59",
      "title": "MI-EEGNET: A novel convolutional neural network for motor imagery classification",
      "authors": [
        "M Riyad",
        "M Khalil",
        "A Adib"
      ],
      "year": "2021",
      "venue": "Journal of Neuroscience Methods",
      "doi": "10.1016/j.jneumeth.2020.109037"
    },
    {
      "citation_id": "60",
      "title": "Learning temporal information for brain-computer interface using convolutional neural networks",
      "authors": [
        "S Sakhavi",
        "C Guan",
        "S Yan"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "61",
      "title": "Dynamic programming algorithm optimization for spoken word recognition",
      "authors": [
        "H Sakoe",
        "S Chiba"
      ],
      "year": "1978",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "Finding a \"kneedle\" in a haystack: Detecting knee points in system behavior. In 2011 31st International Conference on Distributed Computing Systems Workshops",
      "authors": [
        "V Satopaa",
        "J Albrecht",
        "D Irwin",
        "B Raghavan"
      ],
      "year": "2011",
      "venue": "Finding a \"kneedle\" in a haystack: Detecting knee points in system behavior. In 2011 31st International Conference on Distributed Computing Systems Workshops",
      "doi": "10.1109/ICDCSW.2011.20"
    },
    {
      "citation_id": "63",
      "title": "Deep learning with convolutional neural networks for EEG decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human Brain Mapping",
      "doi": "10.1002/hbm.23730"
    },
    {
      "citation_id": "64",
      "title": "Neural tracking to go: auditory attention decoding and saliency detection with mobile eeg",
      "authors": [
        "L Straetmans",
        "B Holtze",
        "S Debener",
        "M Jaeger",
        "B Mirkovic"
      ],
      "year": "2022",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "65",
      "title": "Fundamentals of eeg measurement",
      "authors": [
        "M Teplan"
      ],
      "year": "2002",
      "venue": "Measurement science review"
    },
    {
      "citation_id": "66",
      "title": "Application of machine learning in epileptic seizure detection",
      "authors": [
        "L Tran",
        "H Tran",
        "T Le",
        "T Huynh",
        "H Tran",
        "S Dao"
      ],
      "year": "2022",
      "venue": "Diagnostics",
      "doi": "10.3390/diagnostics12112879"
    },
    {
      "citation_id": "67",
      "title": "Nvae: A deep hierarchical variational autoencoder",
      "authors": [
        "A Vahdat",
        "J Kautz"
      ],
      "year": "2021",
      "venue": "Nvae: A deep hierarchical variational autoencoder",
      "arxiv": "arXiv:2007.03898"
    },
    {
      "citation_id": "68",
      "title": "Long-range forecasting in featureevolving data streams. Knowledge-Based Systems",
      "authors": [
        "S Wambura",
        "J Huang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Long-range forecasting in featureevolving data streams. Knowledge-Based Systems"
    },
    {
      "citation_id": "69",
      "title": "An autoencoder-based approach to predict subjective pain perception from high-density evoked eeg potentials",
      "authors": [
        "J Wang",
        "M Wei",
        "L Zhang",
        "G Huang",
        "Z Liang",
        "L Li",
        "Z Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Engineering in Medicine and Biology Society",
      "doi": "10.1109/EMBC44109.2020.9176644"
    },
    {
      "citation_id": "70",
      "title": "The use of fast fourier transform for the estimation of power spectra: a method based on time averaging over short, modified periodograms",
      "authors": [
        "P Welch"
      ],
      "year": "1967",
      "venue": "IEEE Transactions on audio and electroacoustics"
    },
    {
      "citation_id": "71",
      "title": "Identifying data streams anomalies by evolving spiking restricted boltzmann machines",
      "authors": [
        "L Xing",
        "K Demertzis",
        "J Yang"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "72",
      "title": "(under review). veegnet: learning latent representations to reconstruct eeg raw data via variational autoencoders",
      "authors": [
        "A Zancanaro",
        "G Cisotto",
        "S Manzoni",
        "I Zoppis"
      ],
      "venue": "(under review). veegnet: learning latent representations to reconstruct eeg raw data via variational autoencoders"
    },
    {
      "citation_id": "73",
      "title": "CNN-based approaches for cross-subject classification in motor imagery: From the state-of-the-art to DynamicNet",
      "authors": [
        "A Zancanaro",
        "G Cisotto",
        "J Paulo",
        "G Pires",
        "U Nunes"
      ],
      "year": "2021",
      "venue": "2021 IEEE Conference on"
    }
  ]
}