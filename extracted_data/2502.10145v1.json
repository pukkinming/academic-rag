{
  "paper_id": "2502.10145v1",
  "title": "Interpretable Concept-Based Deep Learning Framework For Multimodal Human Behavior Modeling",
  "published": "2025-02-14T13:15:21Z",
  "authors": [
    "Xinyu Li",
    "Marwa Mahmoud"
  ],
  "keywords": [
    "Explainable AI",
    "multimodal learning",
    "affective computing",
    "facial expression recognition",
    "human-human interaction High Engagement Reason: Model Focuses on No Domain-specific Insights High Contribution of Forward Gaze Smile Clasped Hands Light Laughter Multimodal Input Black Box Map-based XAI Multimodal Input Attention-Guided Concept Model Multimodal Input"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the contemporary era of intelligent connectivity, Affective Computing (AC), which enables systems to recognize, interpret, and respond to human behavior states, has become an integrated part of many AI systems. As one of the most critical components of responsible AI and trustworthiness in all human-centered systems, explainability has been a major concern in AC. Particularly, the recently released EU General Data Protection Regulation requires any high-risk AI systems to be sufficiently interpretable, including biometric-based systems and emotion recognition systems widely used in the affective computing field. Existing explainable methods often compromise between interpretability and performance. Most of them focus only on highlighting key network parameters without offering meaningful, domain-specific explanations to the stakeholders. Additionally, they also face challenges in effectively co-learning and explaining insights from multimodal data sources. To address these limitations, we propose a novel and generalizable framework, namely the Attention-Guided Concept Model (AGCM), which provides learnable conceptual explanations by identifying what concepts that lead to the predictions and where they are observed. AGCM is extendable to any spatial and temporal signals through multimodal concept alignment and co-learning, empowering stakeholders with deeper insights into the model's decision-making process. We validate the efficiency of AGCM on well-established Facial Expression Recognition benchmark datasets while also demonstrating its generalizability on more complex real-world human behavior understanding applications. We believe that AGCM's flexibility and extensibility lay a solid foundation for developing future interpretable and trustworthy models in downstream affective computing applications, including in mental health, psychiatry, education, automotive, and security, offering both competitive performance and domainspecific explanations.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Affective Computing (AC) aims to develop models and systems that recognize, interpret, and respond to human behavior states. As a human-centered design, explainability and transparency have become critical concerns in AC applications  [18] . The EU AI Act  [28]  and the newly proposed General Data Protection Regulation (GDPR) in 2024  [20]  mandates that high-risk AI systems, including biometric-based systems and emotion recognition systems widely used in the affective computing field, must be sufficiently transparent to allow stakeholders from cross-disciplinary area to comprehend the decision-making process of the framework. Enhancing explainability in AC models not only offers extra insights into AI predictions but also ensures fair, trustworthy, and accountable outcomes in sensitive applications like education, healthcare, and security systems.  [32, 56] .\n\nThere is an increasing interest in developing interpretable or eXplainable Artificial Intelligence (XAI) to improve model transparency in AC. As shown in Fig.  1 (b ), approaches such as post-hoc explanations  [26, 38, 42]  and map-based methods  [11, 21, 24]  have emerged to address this need. However, these techniques primarily focus on identifying important regions or parameters within deep neural networks, rather than providing an explicit, causal explanation for the predictions. This limitation is especially pronounced in AC, where opposing facial Action Units, like AU12 (Lip Corner Puller) associated with positive emotion and AU15 (Lip Corner Depressor) linked to negative emotion, can occur in the same facial region. Meanwhile, the alignment and co-learning from multimodal sources pose even greater challenges for these approaches due to the inherently different properties of multimodal knowledge. Therefore, they often face a trade-off between performance and interpretability, which, in high-risk XAI, may undermine the system's trustworthiness  [43] .\n\nConsider a common question: How would a human expert explain their prediction of an individual with highly conversational engagement? They would likely point to the activation of specific facial muscles, such as the zygomatic major, indicating engaged smiles, a strong positive indicator of engagement. Meanwhile, the forward gaze direction, proper gesture, body language, and audio indicators can also be used to recognize engagement. Thus, a good explanation from an AC model should address two key aspects: what indicators or concepts (e.g., facial muscle activations) contribute to the prediction, and where these concepts are observed. Furthermore, the importance of multimodal learning is self-evident in real-world AC applications  [1, 15, 53] . Training and interpreting AC models with multimodal alignment and co-learning is another key challenge in affective XAI  [7] .\n\nAs shown in Fig.  1  (c), in this paper, we propose an interpretable concept-based framework: the Attention-Guided Concept Model (AGCM), which localizes and learns the key indicators during training and then makes the final prediction according to the contribution of these intermediate concepts. This framework incorporates spatial concept information and multimodal concept fusion within a powerful attention-based architecture, combining the advantage of both domain-specific explanation and state-of-the-art performance. In summary, the main contributions of this paper are as follows:\n\n1) We propose a concept-based interpretable framework for AC applications, namely the Attention-Guided Concept Model (AGCM), which provides both learnable multimodal conceptual explanations and spatial visual concept localization, quantifying the contribution of individual concepts to the predicted affective label. 2) To address the challenge of multimodal concept alignment and co-learning, AGCM introduces an extendable sequential multimodal concept fusion, which can be easily expanded to any spatial-temporal signal. This approach accounts for temporal and contextual information between input modalities, demonstrating the adaptability to other discrete or continuous signals. 3) We qualitatively and quantitatively evaluate the proposed framework on three large-scale FER datasets: RAF-DB, AffectNet, and Aff-Wild2, demonstrating that AGCM outperforms previous interpretable models and achieves competitive performance compared to state-of-the-art black-box models. Moreover, the experiment shows that AGCM offers a human-interpretable explanation grounded in domain-specific knowledge. 4) To demonstrate the generalizability of AGCM on complex real-world AC applications, we conduct extensive experiments on the human-human interaction dataset, validating its ability to provide explainable and accurate prediction in downstream AC applications. We provide a video demonstration in the supplementary material to offer additional insights into the prediction process and its explainability.\n\nII. RELATED WORK In this section, we examine two primary machine learning approaches commonly used in affective computing: featurebased models and end-to-end models. We then discuss recent advancements in explainable affective computing, emphasizing their contribution and limitation to model transparency and interpretability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Feature And End-To-End Models In Affective Computing",
      "text": "Discriminative AC focuses on mapping human-centered data to emotion-related labels, employing two primary approaches: feature-based models and end-to-end models.\n\nFeature-based models  [5, 47]  rely on manually extracted features derived from raw data, which are then used to train machine learning models to establish the relationship between features and labels. The strength of this approach lies in the interpretability of the features, which are often humanunderstandable and can provide valuable behavioral insights  [13] . Additionally, feature-based models typically operate on structured, tabular data, offering a computationally efficient solution  [14] . However, the reliance on handcrafted features may omit potentially important information embedded in the raw data, causing inevitable information loss  [18, 60] . Furthermore, decoupling feature extraction from model training may introduce limitations, such as overfitting, particularly due to the structured nature of the input data  [12] .\n\nEnd-to-end models  [33] , on the other hand, learn directly from raw data, eliminating the need for manual feature engineering. Fully leveraging the representational power of deep neural networks, these models are particularly effective when trained on large datasets. However, their strength is also their weakness: the opacity of their learned representations often leads to what is referred to as the \"black-box\" problem, making these models difficult to interpret as they lack human-understandable intermediate representations  [60] . This challenge persists in multi-task learning, where models are designed to predict multiple task labels simultaneously, such as emotion and AUs. Despite their multi-task design, emotion and AU predictions are learned independently, leaving the model as a black box, where the predicted AUs cannot explain the predicted emotions.\n\nAs shown in Fig.  2 , in this work, we propose a hybrid approach, integrating the strengths of the well-understandable feature-based model and the state-of-the-art black-box models through concept-based learning, where each concept serves as an embedded neural representation of the feature. This approach retains the interpretability inherent in feature-based models while harnessing the robust learning capabilities of end-to-end neural networks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Xai In Affective Computing",
      "text": "Recent efforts to enhance the explainability of affective computing models have largely relied on post-hoc, map-based visualizations, and concept-based learning. Fig.  2 . Feature-based approaches offer inherent interpretability and are easily understood by humans, while end-to-end models deliver state-of-theart learning capabilities. This work seeks to integrate the strengths of both methods through a concept-based framework, which achieves a balance between high explainability and robust performance. Unlike traditional features, concepts are not static values. They serve as the neural embeddings of features that are trainable within the ML framework, spontaneously quantifying the contribution of individual concepts to the task label.\n\nPost-hoc approaches  [26, 38, 42, 51]  retrospectively analyze the parameter importance of pre-trained black-box models after deployment. These methods attempt to explain the model by manipulating parameters in specific parts of the network to check their impact on the final prediction. Map-based approaches  [11, 24]  are another common method used to interpret black-box models, typically highlighting the regions where the model focuses its attention. However, both of these approaches primarily focus on the importance scores within the neural network, without offering additional, domainrelevant information for experts. This limitation is particularly evident in AC, where conflicting indicators, such as AU12 (Lip Corner Puller) signaling positive emotion and AU15 (Lip Corner Depressor) indicating negative emotion, may appear in the same facial region. Therefore, simply presenting the weight importance or model attention provides little insight for domain experts like psychologists to understand the AI decision-making process. Furthermore, the distinct properties of multimodal data make incorporating multimodal alignment and co-learning in post-hoc or map-based XAI methods even more challenging, taking the risk of losing either accuracy or interpretability.\n\nRecent attempts on concept-based models  [35, 57]  try to encapsulate specific, human-understandable features through concept embeddings C that are learned in a fully supervised manner. These models learn the mapping X → C → Z, where x ∈ X represents the raw image pixels and z ∈ Z represents the task labels. Specifically, a concept generator G generates concept embeddings, denoted as ĉ = G(x), with ĉ ∈ C representing the learned concepts within a bottleneck layer C. Subsequently, a facial expression predictor y maps the concept embeddings to task labels ẑ ∈ Z, where ẑ = y(ĉ). While concept-based models offer a more interpretable framework than map-based approaches, ongoing research is focused on integrating this explainable architecture with multimodal learning and performance-optimized strategies  [35] . Moreover, a key challenge lies in integrating spatial explanations, which reveal where the model is focusing, with concept-based explanations, which clarify what contributes to the prediction. Achieving this synergy is essential for enhancing both the interpretability and practical utility of models in high-stakes applications.\n\nTable  I",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methods",
      "text": "This section provides a detailed overview of the proposed Attention-Guided Concept Model (AGCM). We begin by detailing the selection and generation of multimodal concepts, a critical step before deploying any concept-based explainable model. Next, we focus on the visual modality, as it is the most widely used and complex modality, uniquely supporting explanations of what concepts contribute to predictions and where they are observed. Finally, we describe the multimodal architecture, addressing the challenges of multimodal alignment and co-learning. Using the audio-visual modality as an example, we demonstrate the framework's functionality and highlight its extendability to other signal-based modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Multimodal Concept Selection & Generation",
      "text": "The selection of concepts or features plays a pivotal role in producing accurate and explainable results, whether in interpretable concept-based models or traditional feature-based models. In terms of explainability, the concept functionsimilar to features -acts as a key representation of the underlying data. Moreover, concepts must explicitly capture attributes that are highly relevant and meaningful to the task at hand. For example, in object detection, attributes such as color and shape are critical, while in bird classification, features like wing morphology or bill structure provide significant insights.\n\nExplaining spatial signals, such as those in the visual modality, involves two key aspects: spatial explanations and conceptual insights, which are particularly critical in explainable medical analysis  [9]  and affective XAI  [11] . To address this, AGCM integrates spatial concepts, enabling the model to learn not only what to focus on but also where to focus.\n\nFor the conceptual explanations (the what question), key features such as facial muscle movements, gaze direction, and head pose are important for assessing and interpreting an individual's affective state  [3, 10, 35] . To address spatial explanations (the where question), patch-level attention maps are trained alongside each concept in an end-to-end, fully supervised manner. This method allows the model to associate concept contributions with their exact spatial locations, thereby enhancing both interpretability and overall performance.\n\nSince manually annotated attention maps are not always available for large-scale datasets, the spatial maps are localized based on facial landmarks  [11, 37] . In this paper, we utilize an open-source landmark detector  [49]  for automatic landmark detection. According to the landmark locations, Regions of Interest (ROI) maps are generated for all AUs, which are subsequently used to supervise the spatial concept attention map throughout model training.\n\nTo integrate these ROI maps into our transformer-based concept learning framework, they are transferred into patchlevel representations, PatchMaps[i], by performing average interpolation, as described in (1). Here, AUMaps[i](x 1 , y 1 ) denotes the value of the i-th input map at position (x 1 , y 1 ). The terms x ′ and y ′ correspond to the patch indices in the x and y dimensions, while S x and S y denote the respective scaling factors.\n\n(1) Fig.  3  presents an example of a patch-level AU map generated using landmark detection and average interpolation. In this map, patches with lighter colors indicate regions of higher importance, effectively highlighting the ROI for each AU. These maps are utilized as part of the ground truth to guide the model's concept learning process via a concept map loss, ensuring the model's focus aligns with the actual spatial regions of interest during training.\n\nOther than spatial signals, temporal signals such as audio, Electrocardiogram (ECG), and Electroencephalogram (EEG) are often perceived as less complex in terms of dimensionality since they typically vary along a single axis (time). For these signals, stakeholders often prioritize conceptual insights (the what question) over spatial interpretation. Temporal dependencies (the where question in time) are naturally addressed by mechanisms like attention models or recurrence in sequential architectures, which excel at capturing temporal relationships.\n\nUsing the widely used audio modality as an example, acoustic indicators such as pitch, loudness, and speech rate and their variations provide critical information by capturing subtle vocal variations that reflect emotional or cognitive states directly tied to the affective labels  [2, 6, 22, 41, 54] . Providing conceptual insights into the decision-making process is essential for explaining predictions derived from these temporal signals.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Visual Attention-Guided Concept Learning",
      "text": "Given the complexity and the inherent differences between the spatial visual signal and other temporal signals, AGCM first focuses only on training the visual concept through attention-guided concept learning. This architecture leverages\n\nFig.  3 . Example of patch-level AU map generated using landmark detection and average interpolation.\n\nspatial concept supervision and concept attention to interpret the model's decision-making process by determining not only what key concepts contribute the most to the prediction but also where these concepts appear. As illustrated in Fig.  4 , the proposed Attention-Guided Concept Model (AGCM) is designed to enhance both the accuracy and explainability of the concept-based models. The model begins by processing the input facial image x through a transformer backbone φ(•), which converts the image into a patch-level representation. This representation effectively captures local and global features by dividing the image into patches and is essential for subsequent processing.\n\nThe core component of AGCM is the Attention-Guided Concept Generator (ACG), which integrates two attention mechanisms: Multi-scale Spatial Attention (MSA) and Channel Attended Concept Mapping (CACM). The MSA block focuses on spatial features at multiple scales, enabling the model to capture both fine-grained and coarse details within the image. For example, recognizing the concept of the cheek region may require a broader attention area compared to the eye region. To achieve this, three MSA heads are employed to capture diverse spatial patterns, each generating a concept attention map âi . These maps are then weighted and summed to produce a final concept attention map, which is utilized to update the concept map loss during training.\n\nComplementing the spatial attention, CACM enhances the model's focus along the channel dimension. By applying attention to the most informative feature channels, CACM ensures robust feature selection across multiple channels, which is crucial for accurately interpreting complex facial expressions.\n\nThe proposed framework also includes a concept probability generator p(•) that computes the probability of each activated concept. This mechanism facilitates concept supervision by quantifying the contribution of individual concepts to the predicted label. Importantly, ACG considers both activated and inactivated concept embeddings because the absence of certain concepts (e.g., deactivation of AUs) can also provide valuable information about one's facial expressions. The i-th predicted activated concepts, ĉ+ i , and inactivated concepts, ĉi , are weighted by their respective probabilities from p(•). The probability score p indicates the likelihood that the activated concept contributes to the final prediction. These are then concatenated and passed to the task predictor y(•), which is The MSA block focuses on spatial features at multiple scales, enhancing the model's ability to capture both fine and coarse details. For instance, the concept of the cheek region may benefit from a larger attention area compared to the eye region. Three MSA heads are used to capture diverse spatial patterns within an image, each generating a concept attention map âi . These maps are weighted and summed to produce the final concept attention map, which is used to update the concept map loss during training. CACM further improves the model's focus on the most informative features along the channel dimension, ensuring robust feature selection across multiple channels. A concept probability generator p(•) computes the probability of each activated concept, facilitating concept supervision by showing the contribution of individual concepts to the predicted label. Notably, ACG considers both activated and inactivated concept embeddings, as the absence of certain concepts (e.g., AUs) can provide additional information about a subject's facial expression. The predicted activated concepts, ĉ+ i , and inactivated concepts, ĉi , are weighted by their respective probabilities from p(•), then concatenated and passed to the one-layer fully-connected task predictor y(•) to generate the final task label t. During loss computation, the model optimizes its performance using the task loss, concept probability loss, and concept map loss associated with the spatial concept attention, ensuring a strong explainability of the model's decision-making process giving not only what key concepts contribute the most to the prediction but also where these concepts appear. a one-layer fully connected network, to generate the final task label t. Therefore, it is designed to be adaptable and expandable to any discrete or continuous concepts, given that appropriate concept annotations are available.\n\nDuring loss computation, the model optimizes performance through a combination of losses: task loss, L t , concept probability loss, L c , and concept map loss, L m , associated with spatial concept attention. The task loss, L t , is computed using Cross Entropy (CE), while the concept probability loss, L c , is derived from the sum of Binary Cross Entropy (BCE) across all concepts. Instead of relying on Mean Square Error (MSE), the concept map loss, L m , uses Cosine Similarity (sim) to emphasize spatial pattern alignment rather than strict value matching. Therefore, the total loss, L, is formulated as:\n\nHere, t is the ground truth task label, c i is the label of the i-th concept, and a i is the i-th concept attention map, while n denotes the total number of used concepts. This comprehensive optimization strategy ensures that the AGCM framework achieves high accuracy while maintaining explainability in its predictions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Expandable Multimodal Agcm Concept Fusion",
      "text": "Alignment, fusion, and co-learning are three primary challenges in multimodal learning, involving the ability to identify, combine, and transfer knowledge across different modalities  [7] , particularly in the context of interpretable AC  [18] . After training the visual concept branch in the first stage, AGCM integrates visual information with any other temporal modalities through concept fusion. In this work, we demonstrate AGCM is an expandable multimodal architecture, using the most commonly used audio-visual fusion as an example, which involves identifying audio information using an acoustic concept generator and joining and transferring knowledge via a late fusion concept-label classifier.\n\nAs shown in Fig.  5 , the fusion stage builds upon the visualbased branch from the previous stage. During the fusion stage, the task predictor from the visual branch is removed, transforming it into a Visual Attention-Guided Concept Generator. This visual generator is responsible for extracting and predicting key visual concepts, including AUs, gaze direction, and head poses. To ensure stability and reliability in visual concept prediction, the parameters of the visual branch are frozen, preventing further modifications during the audio-visual training phase. This approach allows the model to harness pre-learned visual knowledge without overfitting, facilitating robust integrated learning across diverse input modalities.\n\nIn parallel with the visual concept branch, the fusion stage introduces an audio brunch with an Acoustic Concept Generator (ACG) to process the audio input. This generator identifies relevant audio information using an acoustic feature extractor, denoted as G(•). These features are then mapped into activated (ĉ + i ) and inactivated (ĉ - i ) acoustic concept embeddings. The probability of activation for each concept is computed through an acoustic concept probability generator p(•), which quantifies the likelihood of each acoustic concept processes the audio input, generating activated (ĉ + i ) and inactivated (ĉ - i ) acoustic concept embeddings via an acoustic feature extractor G(•). The probability of each concept's activation is computed using an acoustic concept probability generator p(•). The acoustic concept embeddings are concatenated with their corresponding visual concept set and passed through a sequential bottleneck layer ĉ0 , ...ĉ k , where k represents the number of samples in the sequence. For a given video clip, it is assumed that acoustic concepts are shared across all frames. A sequence-to-sequence label predictor y(•) is then used to capture the contextual relationships between frames to generate the final by-frame task label. Importantly, the AGCM framework is inherently extendable to other temporal modalities by adding additional branches to accommodate new data inputs, as long as the appropriate data and annotations are available.\n\nbeing present in the input.\n\nFor downstream applications, the audio branch can be replaced or expanded to incorporate other temporal modalities, such as Electrocardiogram (ECG), Electroencephalogram (EEG), or Electrodermal Activity (EDA), provided the appropriate data and annotations are available.\n\nOnce the visual and temporal concepts are extracted, they are aligned and concatenated to form a unified multimodal representation. In this architecture, a key assumption is made: for a given video clip, temporal concepts are shared across all frames. This allows the model to maintain temporal coherence in the audio stream while aligning it with frame-specific visual features. The bottleneck layer serves to compress the multimodal information, ensuring that only the most relevant aspects of the fused representation are retained for further processing. The concatenated concepts are then passed through a sequential bottleneck layer, denoted as ĉ0 , ..., ĉk , where k represents the number of samples in the sequence.\n\nTo capture the temporal and contextual relationships between frames, the fusion branch employs a sequence-tosequence concept-label predictor y(•), using a transformer architecture. This predictor is designed to handle sequential data, leveraging the temporal dependencies between consecutive frames in a video. By utilizing sequential learning, the model effectively integrates and co-learns multimodal information across time, improving the accuracy of by-frame predictions. This is particularly important for tasks where affective signals evolve over time, such as conversational engagement estimation or mental health assessment.\n\nThe final task label is generated on a per-frame basis, with the model predicting the affective state for each frame in the video sequence. The combination of multimodal concept embeddings allows the VA-AGCM to provide robust and accurate predictions, as it captures a wider range of cues that contribute to affective behavior. Notably, the AGCM framework is readily extendable to other temporal modalities by incorporating additional branches for new data inputs.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Evaluation And Results",
      "text": "Given the intricate nature and wide-ranging applications of AC tasks, we initially employed Facial Expression Recognition (FER) in both visual and audio-visual settings to validate the efficacy of our proposed AGCM framework, considering its well-established datasets and baseline models. We quantitatively evaluate the task and concept-level performance of AGCM on three large-scale FER datasets, and provide qualitative visualizations of the visual and multimodal conceptual explanations, demonstrating the framework's robustness through occlusion experiments and an ablation study.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "We employ three popular benchmark datasets, including RAF-DB and AffectNet with visual modality and Aff-Wild2 with audio-visual data.\n\nRAF-DB  [34]  is a widely-used static FER dataset sourced from the internet, containing 6 basic emotion labels (Surprise, Disgust, Fear, Happiness, Sadness, Anger), and a Neutral label. The dataset includes 12,271 images in the training set and 3,068 images for testing.\n\nAffectNet  [40]  is one of the largest FER datasets, comprising 420,000 facial images annotated with categorical emotion labels. We utilize AffectNet-8, which consists of 291,651 manually labeled images with 8-class emotion labels (Neutral, Happy, Angry, Sad, Fear, Surprise, Disgust, and Contempt). In addition, we employ AffectNet-7, which contains 287,401 images annotated with seven emotion labels (excluding Contempt). The test set contains approximately 3,500 images.\n\nAff-Wild2  [30]  is a large-scale in-the-wild dataset specifically designed for FER and AU detection. It includes over 2.7 million frames from 564 videos with 554 subjects. We use the by-frame FER subset which is manually labeled with 8-class discrete emotions (Neutral, Anger, Disgust, Fear, Happiness, Sadness, Surprise, Other). It also provides manual annotation of 12 AUs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Concept Generation Setup",
      "text": "AGCM is designed to be flexible and extendable to all kinds of discrete or continuous concepts, provided suitable concept annotations are available. In this work, we use the most commonly used audio-visual pair as an example. For the audio concepts, pitch, pitch variation, pitch stability (Jitter), loudness, loudness variation, and speech rate are used. For the visual modality, AUs, gaze direction, and head pose are used.\n\nUnlike AUs, which are binary in nature (activated or inactivated), gaze, head pose and acoustic concepts are continuous and must be mapped into a probability space to fit the conceptbased framework. Specifically, gaze concepts are defined as the degree of direct forward gaze in both horizontal and vertical planes, where 1 represents directly looking forward and 0 indicates looking elsewhere. Head pose concepts capture deviations in yaw (head shake) and pitch (head nod). These gaze and head pose concepts are scaled to the range [0, 1] to fit within the concept probability generator, and corresponding heatmaps are generated based on facial landmarks, similar to Section III-A.\n\nAll acoustic concept labels are normalized to the range [0, 1] before AGCM training. To ensure alignment with the visual concepts, the video data is split into one-second clips (FPS=30), with a 33ms stride applied to capture temporal information effectively. For clips containing complete silence, both pitch and loudness are set to 0, indicating no contribution from the audio modality. Variations in loudness and pitch are calculated using their first-order derivatives, representing the rate of change for these acoustic features, while the Jitter is inherently a percentage. For videos featuring multiple speakers, the audio track for each subject will be individually separated to minimize noise and ensure clarity.\n\nFurthermore, the AGCM framework is flexible and can incorporate other temporal modalities with continuous or discrete values, provided the appropriate data and annotations are available.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Implementation Details",
      "text": "Our experimental setup is summarized as follows: AGCM utilized a pre-trained Vision Transformer as the backbone feature extractor  [19] . Similar to  [35] , the backbone was pretrained on VGGFace2  [16]  for the facial recognition task. After pre-training, the classification header was removed and replaced with the AGCM workflow. Facial images were cropped from the video dataset using the InsightFace detector  [4] . To prevent overfitting, the preprocessing stage incorporated random data augmentation techniques, including horizontal flipping, random rotation, and random erasing.\n\nFor datasets lacking AU annotations, we utilized OpenFace 2.0  [8]  to automatically extract 18 Action Units (AUs), which served as intermediary concepts in our proposed framework. All models were trained for 100 epochs, with early stopping to avoid overfitting, and optimized using the Adam optimizer (learning rate set to 0.0001). The AGCM generated concepts using a Dropout rate of 0.01 and Leaky-ReLU activation. The concept probability and map loss weights were set to 1, ensuring a balanced focus on both conceptual explanation and task prediction.\n\nAGCM used HuBERT  [27]  feature extractor for the audio input. During concept fusion, the learning of the vision branch was frozen, and the Acoustic Concept Generator (ACG) was fine-tuned for 100 epochs, with early stopping (learning rate set to 0.0001). All experiments were conducted on a workstation equipped with dual 48GB Nvidia RTX 6000 Ada GPUs, running a Linux-based PyTorch environment. For quantitative performance evaluation, we report the average performance over four random seeds.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Evaluating Visual-Based Agcm",
      "text": "Given the complexity and necessity of determining not only what key concepts contribute the most to the prediction but also where these concepts appear, we begin by evaluating the visual branch on RAF-DB and AffectNet. To assess the efficiency of the proposed AGCM framework against the previous feature-based and explainable models, we compared this work with a feature-based model, end-to-end map-based explainable models (with CNN and ViT backbones), previous conceptbased explainable models, and the state-of-the-art black-box model without explicit model explainability.\n\nThe feature-based model uses only handcrafted features (e.g., AUs) as input, and a 3-layer Fully Connected (FC) neural network with ReLU activation, matching the complexity of AGCM's task predictor.\n\nTable  II  presents the overall accuracy of various models on RAF-DB, AffectNet-7, and AffectNet-8. The proposed AGCM framework achieves the highest accuracy across all datasets, with 94.40% on RAF-DB, 69.45% on AffectNet-7, and 65.62% on AffectNet-8. These results demonstrate AGCM's significant improvement over the classic feature-based methods, particularly on RAF-DB (+27.36%) and AffectNet-8 (+28.51%). AGCM also outperforms state-of-the-art blackbox transformer models including S2D  [17]  and Poster++  [39] , providing gains of 1.83% on RAF-DB and 1.86% on AffectNet-8 compared to S2D. This highlights AGCM's ability to match and exceed black-box model performance while maintaining conceptual explainability. Furthermore, AGCM demonstrates superior results compared to interpretable mapbased approaches, with a 3.37% improvement on RAF-DB and over 4% on AffectNet. When compared to the previous concept-based model  [35] , AGCM shows consistent gains across all datasets, benefiting from its spatial concept and attention learning.\n\nTable  III  presents the class-wise performance comparison between the proposed AGCM framework and the blackbox transformer-based Poster++ model  [39]  on RAF-DB and AffectNet-8. The results clearly demonstrate the effectiveness of AGCM in delivering a more balanced performance across all FER classes than Poster++, resulting in higher average accuracy on both datasets. The result shows the efficiency of considering conceptual prior knowledge, such as AUs and ROI maps, into the training process to quantify the individual concept's contribution towards predicting the label.\n\nOn RAF-DB, AGCM consistently outperforms Poster++ across nearly all emotion classes, particularly in challenging  Similarly, on AffectNet-8, AGCM provides improved accuracy in most categories, including notable gains in Anger (+5.85%), Disgust (+3.58%), and Happy (+3.02%). Although Poster++ marginally outperforms AGCM in the Sad and Surprise categories, AGCM still delivers a more balanced overall performance, as evidenced by the higher average accuracy (+1.84%).\n\nThe consistent class-wise improvements offered by AGCM highlight its ability to maintain strong performance across both datasets, even in the presence of class imbalance and data variability. More importantly, AGCM not only surpasses Poster++ in terms of average accuracy but also achieves these gains while preserving the model's interpretability, which is essential for applications requiring both performance and transparency.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Evaluating Multimodal Agcm",
      "text": "AGCM framework is designed to be expandable to multimodal inputs and concepts. In this work, we use the most com- monly used audio-visual dataset as an example, demonstrating the AGCM's capacity for aligning and co-learning information from spatial and temporal modalities.\n\nTo evaluate the overall performance of the AGCM framework in a multimodal context, we conducted comprehensive experiments using the audio-visual Aff-Wild2 dataset.\n\nTable  IV  presents the performance comparison in terms of the average F-1 score on the Aff-Wild2 dataset. The proposed AGCM framework consistently outperforms feature-based, map-based, and concept-based interpretable models. Notably, AGCM in a multimodal setting achieves competitive results compared to state-of-the-art black-box models that leverage multimodal data, while maintaining conceptual explainability.\n\nSpecifically, AGCM attains an F-1 score of 47.52% by combining visual and audio inputs, outperforming visual-only AGCM (+2.57%) and CEM (+4.92%), showing that generally it works better in the multimodal setting. In comparison to feature-based models, AGCM demonstrates a significant improvement (+22.25%), emphasizing the effectiveness of concept-level multimodal alignment and co-learning. While CNN-based map models  [11]  show stronger performance among map-based approaches, they still lag behind AGCM (by 3.03%) and AGCM (by 5.6%).\n\nThe black-box MMAE model  [58]  achieves the highest F-1 score of 48.93%, largely due to its use of a pre-trained transformer (Masked Autoencoder or MAE), which is computationally expensive, time-consuming, and lacks interpretability. In contrast, the competitive results of AGCM highlight its ability to deliver robust performance while offering interpretability, which is a key advantage over black-box methods, even in the real-world multimodal context.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "F. Concept Efficiency",
      "text": "The efficiency of predicted concepts is a critical metric for both performance as well as explainability. To evaluate the reliability of learned concept representations, we employ the Concept Alignment Score (CAS)  [57] , which measures how well the predicted concepts align with their corresponding ground truth labels. Unlike traditional accuracy, which struggles with defining thresholds between \"activated\" and \"inactivated\" concepts, CAS uses homogeneity scores and clustering algorithms to assess the proximity of predicted concepts to ground truth, providing a more robust measure of concept alignment.\n\nAs shown in Table  V , models without concept supervision (No Concept) serve as a baseline for comparison. The proposed framework in visual (AGCM-V) and audio-visual (AGCM-AV) contexts outperform the previous CEM models  [35] , which give higher CAS across all datasets, indicating their superior ability to learn meaningful and aligned concepts for both visual and audio modalities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "G. Human Interpretable Conceptual Explanation",
      "text": "In addition to achieving competitive performance compared to black-box deep learning models, a significant advantage of concept-based frameworks lies in their ability to offer clear, human-interpretable conceptual explanations grounded in domain-specific knowledge, making them accessible to even non-AI experts.\n\n1) Spatial Conceptual Explanation: Compared to the mapbased approaches that only give one activation map as an explanation, AGCM combines the advantage of both conceptbased and map-based models, which not only identifies where the model focuses during inference but also explains what specific facial behaviors the model is focusing on.\n\nFig.  6  illustrates the spatial concept explanations generated by the proposed AGCM for a facial image classified as  \"Happiness\" from the AffectNet test set. During inference, AGCM produces attention maps for all relevant concepts and assigns probability scores based on the areas of the face highlighted in the maps. Concepts with higher probabilities, such as AU12 (Lip Corner Puller), are identified as making a significant contribution to the final classification, while those with lower probabilities, such as AU28 (Lip Suck), are effectively suppressed by the concept generator, reducing their influence on the predicted label. Compared to the map-based XAI that gives only a single attention map as the explanation, as in Fig.  1 , the proposed model focuses on every possible expression indicator all over the facial region and then assigns the concept score to further indicate its contribution to a specific affective label, efficiently overcoming the trade-off between explainability and performance.\n\nTo simplify the visualization of the overall conceptual explanation, we proposed a weighted concept attention map ᾱ that combines i-th predicted attention heatmaps αi with its corresponding concept probability pi , as given in  (3) . Here, Norm represents the min-max normalization, n is the total number of concepts, and I(p i ≥ ρ) is an indicator function that includes only concepts with probabilities exceeding the threshold ρ. We set ρ = 0.5 to visualize all activated concepts.\n\nFig.  7  shows examples randomly selected from the Af-fectNet and RAF-DB test sets, illustrating the prediction of emotion labels alongside the top-4 concept probabilities (%) and corresponding weighted concept attention visualizations. The AGCM framework accurately predicts class labels and provides insightful conceptual explanations through activated concept probabilities and attention heatmaps.\n\nIn the \"Happy\" example, AU6 (Cheek Raiser), AU12 (Lip Corner Puller), and AU14 (Dimpler) are all strong indicators of happiness. AGCM efficiently focuses on the relevant facial areas while highlighting the contributions of these concepts. For the \"Anger\" expression, the model emphasizes AU4 (Brow",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "H. Robustness Of The Explanation",
      "text": "To further evaluate the robustness of the model's explanations, we stress-test AGCM to explore its ability to handle challenging scenarios. Facial occlusion is a common challenge in real-world affective signal processing applications, particularly in in-the-wild datasets, where the subjects may wear VR glasses, causing upper-face occlusion, or masks, leading to lower-face occlusion. These occlusions present difficulties for affective computing, especially when providing conceptual or map-based explanations. The proposed AGCM framework addresses this limitation by generating weighted concept attention maps, which improve both the performance and the interpretability.\n\nTo simulate real-world occlusion scenarios, we selected images from the Aff-Wild2 test set and manually occluded either the upper or lower face regions, re-evaluating the performance of the well-trained AGCM framework.\n\nAs shown in Fig.  9 , we randomly selected samples with varying facial expressions, lighting conditions, and angles, then removed either the upper or lower face regions. Using the same well-trained AGCM model, we re-evaluated the predicted emotion labels, representative top concept probabilities, and the corresponding weighted concept attention maps. After occlusion, AGCM still accurately predicts the emotion by focusing on the unobstructed facial regions. In the \"Happy\" examples, the model shifts attention away from AU6 (Cheek Raiser), which is occluded and focuses more on AU12 (Lip Corner Puller), resulting in a correct prediction despite the occlusion. Similarly, in the \"Surprise\" example, AGCM downweights the contribution of the occluded AU26 (Jaw Drop) and instead focuses on AU2 (Outer Brow Raiser), another strong indicator of surprise. These results demonstrate AGCM's robustness in handling occluded facial images while maintaining accurate and interpretable predictions.\n\nHand-over-face occlusion presents an even more complex challenge than occlusion caused by glasses and masks, as the hand can often be misinterpreted as part of the face during Fig.  10 . Example of hand-over-face occlusion, with the predicted facial expression label, top-4 concept probability predictions (%), and weighted concept attention visualization. AGCM accurately focuses on the nonoccluded regions and predicts the task label based on the available concepts, demonstrating its robustness in handling facial expressions with hand-overface occlusion. model inference because one's hands often share similar textures with the face. To evaluate AGCM's performance in such scenarios, we selected additional samples from the Aff-Wild2 dataset, which contains instances of hand-over-face occlusion.\n\nFig.  10  shows test images featuring hand-over-face occlusion. Despite these occlusions, AGCM generates accurate emotion predictions by leveraging a few key concepts. For instance, in the \"Surprise\" example, even though the lowerface concepts are occluded, the model identifies high probabilities for upper-face indicators AU1 (Inner Brow Raiser) and AU2, leading to a correct prediction. Similarly, in the \"Happy\" example, AU6 (Cheek Raiser) alone is sufficient for the model to make this accurate prediction.\n\nThese stress-testing results demonstrate that AGCM effectively handles partial face occlusion and hand-over-face occlusion by focusing on unobstructed regions and leveraging spatial concept learning to emphasize visible concepts during training. This capability highlights AGCM's robust, conceptaware spatial explanations, enabling reliable predictions even in challenging scenarios.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "I. Ablation Study",
      "text": "Compared to the previous concept-based approaches, the proposed AGCM framework introduces four main components, including Multi-scale Spatial Attention (MSA), Multihead Attention (MHA), Cannel Attended Concept Mapping (CACM), and Concept Map Loss (CML). As the evaluation of multimodal concept fusion has been given in Section IV-E, this section provides an ablation study to show the efficiency of the visual-based AGCM framework.\n\nTable  VI  presents the ablation study for the visual-based AGCM framework on RAF-DB and AffectNet-8. The baseline model without any components achieves 90.47% on RAF-DB and 62.58% on AffectNet-8. Adding Multi-scale Spatial Attention (MSA) improves performance significantly, reaching 92.84% and 62.99%. Introducing Multi-head Attention (MHA) further boosts accuracy to 93.26% and 63.10%, while Channel Attended Concept Mapping (CACM) provides a slight improvement to 93.31% and 63.46%. Finally, the full AGCM with Concept Map Loss (CML) achieves the best results, 94.40% on RAF-DB and 65.62% on AffectNet-8, demonstrating the cumulative benefit of these components in enhancing accuracy while maintaining explainability.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "V. Agcm For Interpretable Engagement Estimation",
      "text": "The generalizability of the framework to downstream applications is essential for establishing a trustworthy AC system. Real-life affective signal processing is inherently more ambiguous, complex, and diverse compared to the well-defined FER task. One good example is human-human interactions, where the conversational engagement score is designed to measure the level and rate of engagement between participants, illustrating the broader and more nuanced requirements of realworld AC applications.\n\nIn this section, we use the NOvice eXpert Interaction (NOXI) dataset, a large-scale, well-annotated human-human interaction dataset with the engagement label, to illustrate AGCM's generalization capacity in real-world AC contexts. We conduct both qualitative and quantitative evaluations, demonstrating that AGCM achieves robust performance by automatically identifying key indicators and highlighting essential concepts.\n\nNOXI  [15]  is designed for the analysis of human interaction in real-world, cross-cultural settings. It includes video recordings of novice-expert interactions in eight languages (English, French, German, Spanish, Indonesian, Arabic, Dutch, and Italian), with AU capture via Microsoft Kinect  [59] . The dataset spans over 50 hours of video and is annotated with a by-frame engagement score ranging from 0 to 1. For our experiments, we utilize 76 videos (over 1.5 million frames) for training and 20 videos (over 500,000 frames) for testing.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "A. Generalizing Agcm For Engagement Estimation",
      "text": "AGCM is highly generalizable to downstream AC applications by simply adjusting the configuration of the final task predictor. For example, in FER tasks, a classification header is utilized, whereas in continuous signal prediction tasks, a regression header is employed. This flexibility allows AGCM to adapt a wide range of affective computing applications.\n\nTable  VII  shows the performance comparison of various models in terms of the Concordance Correlation Coefficient (CCC) on the Noxi dataset for continuous engagement estimation. CCC is used to evaluate continuous tasks by measuring the agreement between predicted and true values, accounting for both correlation and accuracy, making it ideal for engagement estimation tasks. The proposed multimodal AGCM framework with audio-visual concept fusion again outperforms feature-based and previous concept-based models, showing its outstanding state-of-the-art performance in downstream realworld engagement estimation tasks.\n\nIn the unimodal setting, AGCM with visual concepts achieves a CCC score of 0.59, marking a substantial improvement over the unimodal CEM (+0.11) and feature-based model (+0.23). This result highlights the advantages of spatial concept learning while underscoring the limitations of featurebased models in addressing the complexities of affective signal processing.\n\nIn the multimodal context, AGCM attains a performance of 0.80, demonstrating the significant benefits of co-learning multimodal knowledge. This is particularly valuable in complex real-world AC applications, such as engagement estimation, where multiple modalities are essential for capturing and understanding nuanced human behavior.\n\nAlthough the black-box S2S model  [55]  slightly outperforms AGCM with a CCC of 0.03, AGCM underscores its ability to approximate state-of-the-art results while maintaining interpretability. The attention map-based models  [11]  are not well-suited to this task, as they rely on predefined mappings between AUs and labels, which are not available  for continuous engagement estimation. Additionally, TS-CAM  [21] , which is restricted to the visual modality, also performs poorly in engagement estimation. Meanwhile, the Concept Alignment Score (CAS), as shown in Table  V , illustrates that the AGCM framework with audiovisual co-learning not only maintains competitive performance compared to state-of-the-art black-box deep learning models but also delivers accurate conceptual explanations.\n\nTherefore, this sophisticated interpretable framework maintains competitive performance without compromise. By simply configuring the AGCM classifier, the performance evaluation on engagement estimation demonstrates the strong generalizability of AGCM to a wide range of downstream applications beyond FER, making it both powerful and accessible for diverse affective computing tasks.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "B. Agcm Explainability In Engagement Estimation",
      "text": "Explainability becomes even more crucial in downstream AC applications compared to FER, given the inherent complexity of human behavior. In tasks such as engagement estimation, delivering domain-specific explanations is vital for non-AI stakeholders to understand and interpret the decisionmaking process.\n\n1) Explaining Engagement Transitions: To show AGCM's explanation and prediction capabilities in human-human engagement estimation, Fig.  11  presents an example from the NOXI dataset. This sample, randomly selected to cover approximately 60 seconds of data, highlights engagement transitions between listening, distraction, and interaction.\n\nAt the beginning of the sequence, the subject actively listens with a direct gaze toward the speaker, as indicated by the high intensity of the conceptual direct gaze. In conversationbased engagement estimation, gaze direction and head pose are critical concepts for predicting and explaining engagement scores. Since the acoustic input is not prominent during the listening phase, the intensities of acoustic concepts remain low, which is expected as the audio track of each subject is recorded separately in this dataset.\n\nWhen distraction occurs, the subject shifts attention to a phone call or another person, causing the intensities of direct gaze and forward head pose concepts to decrease, which in turn lowers the engagement score. When the subject looks down with eyes only partly open, as evidenced by the activation of AU45 (Blink), the predicted engagement score reaches its lowest point, signifying that the subject's attention is fully disengaged from the conversation.\n\nAs the distraction ends and the subject re-engages with the speaker, positive facial expressions, such as AU12, become prominent, associated with increased engagement during interactions  [23, 44] . The intensities of gaze and head pose concepts increase, and acoustic concepts begin to register, indicating the subject's regained focus and positive emotional feedback. This highlights the subject's re-engagement in the conversation.  6  AGCM effectively leverages these multimodal concepts to capture subtle changes in engagement states and ensure robust conceptual explainability during inference, demonstrating its strong generalizability to downstream AC applications with complex behavioral labels, extending beyond the scope of FER.\n\n2) Explaining Complex Human Behaviours: Real-world behavioral states are more complex than facial expressions. Higher-level affective states that share similar feature representations usually introduce ambiguity in task predictions, especially in feature-based models.\n\nThe proposed AGCM framework has an inherent advantage in differentiating states by autonomously learning and capturing both the concept-aware similarities and distinctions between these affective states. Take an example of human-human interaction, real-world applications often involve complex affective states that are more ambiguous and abstract compared to discrete emotions, such as distraction and cognitive load  [31] .\n\nFig.  12  provides an example of engagement estimation in the presence of distraction and cognitive Load. Distraction occurs when the subject's gaze drifts away, indicating mental disengagement. Conversely, cognitive load happens when the subject looks away while remaining engaged in processing information. In feature-based affective computing models, these complex behaviors which share similar feature representations, can introduce ambiguity in task predictions.\n\nThus, the proposed AGCM framework provides robust learning and explainability, even in complex behavioral states such as distraction and cognitive load. This demonstrates its effectiveness in capturing nuanced affective states, providing enhanced generalizability to complex downstream AC applications that are difficult to tackle using conventional methods.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion & Future Work",
      "text": "In this paper, we introduce the Attention-Guided Concept Model (AGCM), a multimodal concept-based interpretable framework that provides conceptual explanations of what concepts contribute to the predictions and where they are observed. AGCM is highly extendable to various spatialtemporal modalities, effectively addressing the challenges of multimodal alignment, fusion, and co-learning. The framework demonstrates strong generalizability and flexibility, making it well-suited for diverse real-world AC applications.\n\nWe first validate the model's effectiveness in achieving both high performance and robust explanation through qualitative and quantitative evaluations on well-established FER datasets. Then, we demonstrate the generalizability of the AGCM framework to other complex real-world AC applications by extensive experiments on the human-human interaction task. We believe that AGCM establishes a foundation for creating future interpretable systems in downstream AC applications, such as psychology, psychiatry, digital behavior, and Human-Computer Interaction, with competitive performance and human-interpretable explanation.\n\nAGCM leverages the strengths of both feature-based models and deep black-box models to offer interpretable, highperformance predictions. However, explainability in affective computing remains an evolving area of research. We posit that model explanations should be tailored to end-users, such as psychologists and cognitive scientists. Therefore, we plan to incorporate a human-in-the-loop approach for affective XAI to further enhance model usability. Additionally, while AGCM is trained on large datasets, exploring XAI fairness in terms of gender, cultural, and age biases presents an interesting avenue for further investigation. In this paper, we assess various forms of occlusion using the Aff-Wild2 dataset; future improvements could be achieved by fine-tuning AGCM on occlusion-specific datasets to better handle such challenges. Generating textbased explanations via Large Language Models (LLM) may also give users extra insights. However, given the inherent complexity of LLMs, it is imperative to employ appropriate knowledge distillation techniques, particularly for crossdisciplinary stakeholders. The use of handcrafted features, such as AU detections, has been ongoing for decades. These approaches mainly focus on automatically mapping the facial representation to a single numerical value, without fully accounting for the complexity of one's affective state. Like in most of the featurebased approaches, relying solely on these numerical values for intricate AC tasks risks overlooking other emotion-related information conveyed by the subject, potentially degrading performance. Similarly, in multi-task learning-for instance, simultaneously predicting AU and expression-each classification head optimizes independently, rather than fostering mutually beneficial learning that emphasizes the relevance of AUs to facial expressions.\n\nIn contrast, as illustrated in Fig.  13 , the proposed AGCM framework enhances both model explainability and performance by bridging this gap. It employs an end-to-end learning strategy that quantifies the contributions of underlying emotion-related indicators to the final task prediction. By design, AGCM naturally advances traditional feature-based and multi-task AC approaches, where feature representations are either static or insufficient as explanations for task predictions. Previous studies have demonstrated that embedding size can impact the task performance of concept-based frameworks  [57] . The optimal concept size may vary depending on the task. In this work, we use an embedding size of 16 for all FER tasks and 32 for engagement estimation tasks. Fig.  14  shows the task performance across various embedding sizes. For both applications, performance initially improves with increasing embedding size. However, once the",
      "page_start": 13,
      "page_end": 16
    },
    {
      "section_name": "B. Embedding Size Ablation Study",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "C. Comparing End-To-End And By-Step Agcm",
      "text": "To further assess the efficiency of the AGCM framework, we compare the end-to-end and by-step training strategies. In by-step AGCM, the model first optimizes a mapping function from the raw input to all intermediate concept scores. If the concepts include only AUs, this phase operates similarly to an AU detector, generating activation probabilities for all AUs. These AU probabilities are then combined with the embeddings in a subsequent optimization step to predict the final facial expression label separately.\n\nIn by-step AGCM, the neural embeddings of intermediate concepts are not trainable during task learning. The parameter optimization treats the concept and task loss separately. This approach contrasts with end-to-end training, where a unified push-pull joint loss is employed to enhance both concept explainability and task performance simultaneously.\n\nTable  VIII  presents a performance comparison between the end-to-end and by-step AGCM training strategies. Compared to the end-to-end approach, the by-step training strategy results in performance degradation across all datasets, with particularly notable declines in the multimodal AGCM framework, where separately learning concepts can lead to significant information loss from the raw data. Thus, we posit that jointly learning the concept and task label enhances both model explainability and task performance by compelling the model to explicitly supervise human-understandable features derived from domain-specific prior knowledge.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "D. Expanded Discussion Of Agcm And Map-Based Xai",
      "text": "Map-based XAI was originally designed for general ML tasks like object localization, where attention heatmaps serve as effective tools to indicate object locations  [21] . In affective signal processing, however, spatial concept explanations offer significant advantages over map-based XAI by providing domain-specific insights alongside task performance improvements. Simply presenting an attention heatmap over a face region offers minimal value for domain experts in AC applications. For instance, two opposing indicators, AU12 (Lip Corner Puller) and AU15 (Lip Corner Depressor), appear in the same region of the face, making it insufficient to rely solely on attention maps for emotion interpretation. Instead, conceptual explanations that explicitly indicate the activation and contribution of specific AUs provide a more natural and informative approach to AC tasks.\n\nRecent map-based FER work  [11]  uses pre-generated AU maps based on emotion labels to guide model learning, depending on a strict mapping between AUs and facial expressions. For example, for images labeled as \"happiness,\" this approach restricts the model's focus strictly to the AU6 and AU12 regions, regardless of whether these specific AUs are activated, ignoring other facial information that may contribute to the expression. This rigid mapping not only degrades performance but also proves limiting in downstream AC applications, such as engagement estimation or mental health assessment, where there is no clear mapping between AUs and affective labels.\n\nFig.  15  compares explanations provided by our proposed AGCM with those from two map-based XAI methods  [11, 21] . The attention heatmaps from the map-based XAI approaches appear similar across different expression labels, offering insufficient interpretability for high-stakes AC applications. In contrast, AGCM not only localizes each AU but also quantifies its contribution to the final prediction, delivering richer insights into model predictions while achieving state-of-the-art task performance.",
      "page_start": 17,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Difference between the black-box models, current eXplainable AI",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), approaches such",
      "page": 1
    },
    {
      "caption": "Figure 1: (c), in this paper, we propose an",
      "page": 2
    },
    {
      "caption": "Figure 2: , in this work, we propose a hybrid",
      "page": 2
    },
    {
      "caption": "Figure 2: Feature-based approaches offer inherent interpretability and are",
      "page": 3
    },
    {
      "caption": "Figure 3: presents an example of a patch-level AU map",
      "page": 4
    },
    {
      "caption": "Figure 3: Example of patch-level AU map generated using landmark detection",
      "page": 4
    },
    {
      "caption": "Figure 4: , the proposed Attention-Guided",
      "page": 4
    },
    {
      "caption": "Figure 4: The architecture of our proposed Attention-Guided Concept Model (AGCM) for the spatial visual modality. The model uses a transformer backbone",
      "page": 5
    },
    {
      "caption": "Figure 5: , the fusion stage builds upon the visual-",
      "page": 5
    },
    {
      "caption": "Figure 5: In the multimodal fusion stage, the pre-learned visual branch functions as a Visual Attention-Guided Concept Generator. The parameters of the",
      "page": 6
    },
    {
      "caption": "Figure 6: illustrates the spatial concept explanations generated",
      "page": 9
    },
    {
      "caption": "Figure 6: AGCM offers human-interpretable and intuitive explanations by",
      "page": 9
    },
    {
      "caption": "Figure 1: , the proposed model focuses on every possible",
      "page": 9
    },
    {
      "caption": "Figure 7: shows examples randomly selected from the Af-",
      "page": 9
    },
    {
      "caption": "Figure 7: Example of the facial expression label prediction, top-4 concept",
      "page": 10
    },
    {
      "caption": "Figure 8: illustrates an example of FER prediction on the",
      "page": 10
    },
    {
      "caption": "Figure 8: AGCM facilitates both the co-learning and interpretation of mul-",
      "page": 10
    },
    {
      "caption": "Figure 9: , we randomly selected samples with",
      "page": 10
    },
    {
      "caption": "Figure 9: Example of the face occlusion samples, with the expression label",
      "page": 11
    },
    {
      "caption": "Figure 10: Example of hand-over-face occlusion, with the predicted facial",
      "page": 11
    },
    {
      "caption": "Figure 10: shows test images featuring hand-over-face oc-",
      "page": 11
    },
    {
      "caption": "Figure 11: Example of the engagement estimation, gaze, head pose direction",
      "page": 12
    },
    {
      "caption": "Figure 11: presents an example from the",
      "page": 12
    },
    {
      "caption": "Figure 12: provides an example of engagement estimation in",
      "page": 13
    },
    {
      "caption": "Figure 12: Example of the engagement estimation for distraction and cognitive",
      "page": 13
    },
    {
      "caption": "Figure 13: (a) Feature-based models rely on manual feature preprocessing using external automatic toolkits, such as OpenFace, which operate outside the",
      "page": 16
    },
    {
      "caption": "Figure 13: , the proposed AGCM",
      "page": 16
    },
    {
      "caption": "Figure 14: Task performance evaluation (%) with different embedding sizes.",
      "page": 16
    },
    {
      "caption": "Figure 14: shows the task performance across various em-",
      "page": 16
    },
    {
      "caption": "Figure 15: compares explanations provided by our proposed",
      "page": 17
    },
    {
      "caption": "Figure 15: Explanation examples of map-based TS-CAM [21], attention map-based FER (Att-Map) [11], and the proposed AGCM framework. In addition to all",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RAF-DB": "AGCM\nPOST++",
          "AffectNet-8": "AGCM\nPOST++"
        },
        {
          "RAF-DB": "94.53\n88.27\n82.43\n71.88\n87.50\n68.92\n97.47\n97.22\n93.51\n92.89\n90.58\n89.51\n-\n-\n93.68\n92.06",
          "AffectNet-8": "66.05\n60.20\n61.58\n58.00\n63.00\n63.00\n79.42\n76.40\n66.80\n65.01\n65.60\n62.99\n64.08\n59.52\n62.76\n60.60"
        },
        {
          "RAF-DB": "91.23\n85.97",
          "AffectNet-8": "65.61\n63.77"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Surprise\nSurprise": "",
          "Happy\nHappy": "",
          "Happy": "Happy",
          "Neutral\nNeutral": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A multi-modal stacked ensemble model for bipolar disorder classification",
      "authors": [
        "N Abaeikoupaei",
        "H Osman"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Modeling prosodic differences for speaker recognition",
      "authors": [
        "A Adami"
      ],
      "year": "2007",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "3",
      "title": "Recognizing emotion from facial expressions: psychological and neurological mechanisms",
      "authors": [
        "R Adolphs"
      ],
      "year": "2002",
      "venue": "Behavioral and Cognitive Neuroscience Reviews"
    },
    {
      "citation_id": "4",
      "title": "Killing two birds with one stone: Efficient and robust training of face recognition cnns by partial fc",
      "authors": [
        "X An",
        "J Deng",
        "J Guo",
        "Z Feng",
        "X Zhu",
        "J Yang",
        "T Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Affective action and interaction recognition by multi-view representation learning from handcrafted low-level skeleton features",
      "authors": [
        "D Avola",
        "M Cascio",
        "L Cinque",
        "A Fagioli",
        "G Foresti"
      ],
      "year": "2022",
      "venue": "International journal of neural systems"
    },
    {
      "citation_id": "6",
      "title": "Vocal expression of emotion: Acoustic properties of speech are associated with emotional intensity and context",
      "authors": [
        "M Owren"
      ],
      "year": "1995",
      "venue": "Psychological science"
    },
    {
      "citation_id": "7",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "9",
      "title": "A case-based interpretable deep learning model for classification of mass lesions in digital mammography",
      "authors": [
        "A Barnett",
        "F Schwartz",
        "C Tao",
        "C Chen",
        "Y Ren",
        "J Lo",
        "C Rudin"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Affective evaluations of objects are influenced by observed gaze direction and emotional expression",
      "authors": [
        "A Bayliss",
        "A Frischen",
        "M Fenske",
        "S Tipper"
      ],
      "year": "2007",
      "venue": "Cognition"
    },
    {
      "citation_id": "11",
      "title": "Guided interpretable facial expression recognition via spatial action unit cues",
      "authors": [
        "S Belharbi",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2024",
      "venue": "Guided interpretable facial expression recognition via spatial action unit cues",
      "arxiv": "arXiv:2402.00281"
    },
    {
      "citation_id": "12",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Y Bengio",
        "A Courville",
        "P Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "13",
      "title": "Comparing handcrafted features and deep neural representations for domain generalization in human activity recognition",
      "authors": [
        "N Bento",
        "J Rebelo",
        "M Barandas",
        "A Carreiro",
        "A Campagner",
        "F Cabitza",
        "H Gamboa"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition at a distance: The robustness of machine learning based on hand-crafted facial features vs deep learning models",
      "authors": [
        "C Bisogni",
        "L Cimmino",
        "M De Marsico",
        "F Hao",
        "F Narducci"
      ],
      "year": "2023",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "15",
      "title": "The noxi database: multimodal recordings of mediated novice-expert interactions",
      "authors": [
        "A Cafaro",
        "J Wagner",
        "T Baur",
        "S Dermouche",
        "M Torres",
        "C Pelachaud",
        "E André",
        "M Valstar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "16",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "17",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Y Chen",
        "J Li",
        "S Shan",
        "M Wang",
        "R Hong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Toward explainable affective computing: A review",
      "authors": [
        "K Cortiñas-Lorenzo",
        "G Lacey"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "19",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "20",
      "title": "Regulation (eu) 2024/1689 on eu artificial intelligence act",
      "authors": [
        "Eur-Lex"
      ],
      "year": "2024",
      "venue": "Regulation (eu) 2024/1689 on eu artificial intelligence act"
    },
    {
      "citation_id": "21",
      "title": "Ts-cam: Token semantic coupled attention map for weakly supervised object localization",
      "authors": [
        "W Gao",
        "F Wan",
        "X Pan",
        "Z Peng",
        "Q Tian",
        "Z Han",
        "B Zhou",
        "Q Ye"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "22",
      "title": "The distinction between integral and separable dimensions: evidence for the integrality of pitch and loudness",
      "authors": [
        "J Grau",
        "D Nelson"
      ],
      "year": "1988",
      "venue": "Journal of experimental psychology. General"
    },
    {
      "citation_id": "23",
      "title": "Facial and bodily expressions of emotional engagement",
      "authors": [
        "S Greipl",
        "K Bernecker",
        "M Ninaus"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "24",
      "title": "Interpretable emotion classification using temporal convolutional models",
      "authors": [
        "M Gund",
        "A Bharadwaj",
        "I Nwogu"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "25",
      "title": "Hossain. Tca-net: Triplet concatenated-attentional network for multimodal engagement estimation",
      "authors": [
        "H He",
        "D Wang",
        "M Hasan",
        "T Gedeon"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "26",
      "title": "Unraveling ml models of emotion with nova: Multi-level explainable ai for non-experts",
      "authors": [
        "A Heimerl",
        "K Weitz",
        "T Baur",
        "E André"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "28",
      "title": "The landscape of facial processing applications in the context of the european ai act and the development of trustworthy systems",
      "authors": [
        "I Hupont",
        "S Tolan",
        "H Gunes",
        "E Gómez"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "29",
      "title": "Good vibrations: A review of vocal expressions of positive emotions",
      "authors": [
        "R Kamiloglu",
        "A Fischer",
        "D Sauter"
      ],
      "year": "2020",
      "venue": "Psychonomic bulletin & review"
    },
    {
      "citation_id": "30",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "31",
      "title": "Gaze-based signatures of mind wandering during real-world scene processing",
      "authors": [
        "K Krasich",
        "R Mcmanus",
        "S Hutt",
        "M Faber",
        "S Mello",
        "J Brockmole"
      ],
      "year": "2018",
      "venue": "Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "32",
      "title": "Opacity, transparency, and the ethics of affective computing",
      "authors": [
        "M Kumar",
        "A Aijaz",
        "O Chattar",
        "J Shukla",
        "R Mutharaju"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "Unlocking the black box: Conceptbased modeling for interpretable affective computing applications",
      "authors": [
        "X Li",
        "M Mahmoud"
      ],
      "year": "2024",
      "venue": "2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "36",
      "title": "A unified approach to facial affect analysis: the mae-face visual representation",
      "authors": [
        "B Ma",
        "W Zhang",
        "F Qiu",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "37",
      "title": "Landmarkbased facial feature construction and action unit intensity prediction",
      "authors": [
        "J Ma",
        "X Li",
        "Y Ren",
        "R Yang",
        "Q Zhao"
      ],
      "year": "2021",
      "venue": "Mathematical Problems in Engineering"
    },
    {
      "citation_id": "38",
      "title": "Towards interpretable facial emotion recognition",
      "authors": [
        "S Malik",
        "P Kumar",
        "B Raman"
      ],
      "year": "2021",
      "venue": "Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing"
    },
    {
      "citation_id": "39",
      "title": "Poster++: A simpler and stronger facial expression recognition network",
      "authors": [
        "J Mao",
        "R Xu",
        "X Yin",
        "Y Chang",
        "B Nie",
        "A Huang",
        "Y Wang"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Anger recognition in speech using acoustic and linguistic cues",
      "authors": [
        "T Polzehl",
        "A Schmitt",
        "F Metze",
        "M Wagner"
      ],
      "year": "2011",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "42",
      "title": "explaining the predictions of any classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "43",
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "authors": [
        "C Rudin"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Classifying emotions and engagement in online learning based on a single facial expression recognition neural network",
      "authors": [
        "A Savchenko",
        "L Savchenko",
        "I Makarov"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Facial expression recognition with adaptive frame rate based on multiple testing correction",
      "authors": [
        "A Savchenko"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning, ICML'23"
    },
    {
      "citation_id": "46",
      "title": "Hierarchical attention network with progressive feature fusion for facial expression recognition",
      "authors": [
        "H Tao",
        "Q Duan"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "47",
      "title": "Feature extraction with handcrafted methods and convolutional neural networks for facial emotion recognition",
      "authors": [
        "E Tsalera",
        "A Papadakis",
        "M Samarakou",
        "I Voyiatzis"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "48",
      "title": "Dctm: Dilated convolutional transformer model for multimodal engagement estimation in conversation",
      "authors": [
        "V Tu",
        "V Huynh",
        "H.-J Yang",
        "S.-H Kim",
        "S Nawaz",
        "K Nandakumar",
        "M Zaheer"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "Deep high-resolution representation learning for visual recognition",
      "authors": [
        "J Wang",
        "K Sun",
        "T Cheng",
        "B Jiang",
        "C Deng",
        "Y Zhao",
        "D Liu",
        "Y Mu",
        "M Tan",
        "X Wang"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "50",
      "title": "Distract your attention: Multi-head cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2023",
      "venue": "Biomimetics"
    },
    {
      "citation_id": "51",
      "title": "Enhancing sound texture in cnn-based acoustic scene classification",
      "authors": [
        "Y Wu",
        "T Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "52",
      "title": "Cl-transfer: Collaborative learning based transformer for facial expression recognition with masked reconstruction",
      "authors": [
        "Y Yang",
        "L Hu",
        "C Zu",
        "J Zhang",
        "Y Hou",
        "Y Chen",
        "J Zhou",
        "L Zhou",
        "Y Wang"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "53",
      "title": "D-vlog: Multimodal vlog dataset for depression detection",
      "authors": [
        "J Yoon",
        "C Kang",
        "S Kim",
        "J Han"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Detecting user engagement in everyday conversations",
      "authors": [
        "C Yu",
        "P Aoki",
        "A Woodruff"
      ],
      "year": "2004",
      "venue": "Detecting user engagement in everyday conversations"
    },
    {
      "citation_id": "55",
      "title": "Sliding window seq2seq modeling for engagement estimation",
      "authors": [
        "J Yu",
        "K Lu",
        "M Jing",
        "Z Liang",
        "B Zhang",
        "J Sun",
        "J Liang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "56",
      "title": "Bridging computer and education sciences: a systematic review of automated emotion recognition in online learning environments",
      "authors": [
        "S Yu",
        "A Androsov",
        "H Yan",
        "Y Chen"
      ],
      "year": "2024",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "57",
      "title": "Concept embedding models: Beyond the accuracy-explainability trade-off",
      "authors": [
        "M Zarlenga",
        "B Pietro",
        "C Gabriele",
        "M Giuseppe",
        "F Giannini",
        "M Diligenti",
        "S Zohreh",
        "P Frederic",
        "S Melacci",
        "W Adrian"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "58",
      "title": "Multi-modal facial affective analysis based on masked autoencoder",
      "authors": [
        "W Zhang",
        "B Ma",
        "F Qiu",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "59",
      "title": "Microsoft kinect sensor and its effect",
      "authors": [
        "Z Zhang"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "60",
      "title": "Affective computing for large-scale heterogeneous multimedia data: A survey",
      "authors": [
        "S Zhao",
        "S Wang",
        "M Soleymani",
        "D Joshi",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "61",
      "title": "Ceprompt: Crossmodal emotion-aware prompting for facial expression recognition",
      "authors": [
        "H Zhou",
        "S Huang",
        "F Zhang",
        "C Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "62",
      "title": "Leveraging tcn and transformer for effective visual-audio fusion in continuous emotion recognition",
      "authors": [
        "W Zhou",
        "J Lu",
        "Z Xiong",
        "W Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}